<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000884">
<title confidence="0.995791">
A Model of Discourse Predictions in Human Sentence Processing
</title>
<author confidence="0.98618">
Amit Dubey and Frank Keller and Patrick Sturt
</author>
<affiliation confidence="0.989486">
Human Communication Research Centre, University of Edinburgh
</affiliation>
<address confidence="0.984879">
10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.998962">
{amit.dubey,frank.keller,patrick.sturt}@ed.ac.uk
</email>
<sectionHeader confidence="0.993901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999832">
This paper introduces a psycholinguistic
model of sentence processing which combines
a Hidden Markov Model noun phrase chun-
ker with a co-reference classifier. Both mod-
els are fully incremental and generative, giv-
ing probabilities of lexical elements condi-
tional upon linguistic structure. This allows
us to compute the information theoretic mea-
sure of surprisal, which is known to correlate
with human processing effort. We evaluate
our surprisal predictions on the Dundee corpus
of eye-movement data show that our model
achieve a better fit with human reading times
than a syntax-only model which does not have
access to co-reference information.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994172">
Recent research in psycholinguistics has seen a
growing interest in the role of prediction in sentence
processing. Prediction refers to the fact that the hu-
man sentence processor is able to anticipate upcom-
ing material, and that processing is facilitated when
predictions turn out to be correct (evidenced, e.g.,
by shorter reading times on the predicted word or
phrase). Prediction is presumably one of the factors
that contribute to the efficiency of human language
understanding. Sentence processing is incremental
(i.e., it proceeds on a word-by-word basis); there-
fore, it is beneficial if unseen input can be antici-
pated and relevant syntactic and semantic structure
constructed in advance. This allows the processor to
save time and makes it easier to cope with the con-
stant stream of new input.
Evidence for prediction has been found in a range
of psycholinguistic processing domains. Semantic
prediction has been demonstrated by studies that
show anticipation based on selectional restrictions:
listeners are able to launch eye-movements to the
predicted argument of a verb before having encoun-
tered it, e.g., they will fixate an edible object as soon
as they hear the word eat (Altmann and Kamide,
1999). Semantic prediction has also been shown in
the context of semantic priming: a word that is pre-
ceded by a semantically related prime or by a seman-
tically congruous sentence fragment is processed
faster (Stanovich and West, 1981; Clifton et al.,
2007). An example for syntactic prediction can be
found in coordinate structures: readers predict that
the second conjunct in a coordination will have the
same syntactic structure as the first conjunct (Fra-
zier et al., 2000). In a similar vein, having encoun-
tered the word either, readers predict that or and a
conjunct will follow it (Staub and Clifton, 2006).
Again, priming studies corroborate this: Compre-
henders are faster at naming words that are syntacti-
cally compatible with prior context, even when they
bear no semantic relationship to it (Wright and Gar-
rett, 1984).
Predictive processing is not confined to the sen-
tence level. Recent experimental results also provide
evidence for discourse prediction. An example is the
study by van Berkum et al. (2005), who used a con-
text that made a target noun highly predictable, and
found a mismatch effect in the ERP (event-related
brain potential) when an adjective appeared that was
inconsistent with the target noun. An example is (we
give translations of their Dutch materials):
</bodyText>
<listItem confidence="0.681840333333333">
(1) The burglar had no trouble locating the secret
family safe.
a. Of course, it was situated behind a
</listItem>
<page confidence="0.983041">
304
</page>
<note confidence="0.8069835">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 304–312,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.988637386666667">
bigneu but unobtrusive paintingneu.
b. Of course, it was situated behind a
bigcom but unobtrusive bookcasecom.
Here, the adjective big, which can have neutral or
common gender in Dutch, is consistent with the pre-
dicted noun painting in (1-a), but inconsistent with it
in (1-b), leading to a mismatch ERP on big in (1-b)
but not in (1-a).
Previous results on discourse effects in sentence
processing can also be interpreted in terms of pre-
diction. In a classical paper, Altmann and Steed-
man (1988) demonstrated that PP-attachment pref-
erences can change through discourse context: if the
context contains two potential referents for the tar-
get NP, then NP-attachment of a subsequent PP is
preferred (to disambiguate between the two refer-
ents), while if the context only contains one target
NP, VP-attachment is preferred (as there is no need
to disambiguate). This result (and a large body of
related findings) is compatible with an interpretation
in which the processor predicts upcoming syntactic
attachment based on the presence of referents in the
preceding discourse.
Most attempts to model prediction in human lan-
guage processing have focused on syntactic pre-
diction. Examples include Hale’s (2001) surprisal
model, which relates processing effort to the con-
ditional probability of the current word given the
previous words in the sentence. This approach has
been elaborated by Demberg and Keller (2009) in a
model that explicitly constructs predicted structure,
and includes a verification process that incurs ad-
ditional processing cost if predictions are not met.
Recent work has attempted to integrate semantic
and discourse prediction with models of syntactic
processing. This includes Mitchell et al.’s (2010)
approach, which combines an incremental parser
with a vector-space model of semantics. However,
this approach only provides a loose integration of
the two components (through simple addition of
their probabilities), and the notion of semantics used
is restricted to lexical meaning approximated by
word co-occurrences. At the discourse level, Dubey
(2010) has proposed a model that combines an incre-
mental parser with a probabilistic logic-based model
of co-reference resolution. However, this model
does not explicitly model discourse effects in terms
of prediction, and again only proposes a loose in-
tegration of co-reference and syntax. Furthermore,
Dubey’s (2010) model has only been tested on two
experimental data sets (pertaining to the interaction
of ambiguity resolution with context), no broad cov-
erage evaluation is available.
The aim of the present paper is to overcome these
limitations. We propose a computational model that
captures discourse effects on syntax in terms of pre-
diction. The model comprises a co-reference com-
ponent which explicitly stores discourse mentions
of NPs, and a syntactic component which adjust
the probabilities of NPs in the syntactic structure
based on the mentions tracked by the discourse com-
ponent. Our model is HMM-based, which makes
it possible to efficiently process large amounts of
data, allowing an evaluation on eye-tracking cor-
pora, which has recently become the gold-standard
in computational psycholinguistics (e.g., Demberg
and Keller 2008; Frank 2009; Boston et al. 2008;
Mitchell et al. 2010).
The paper is structured as follows: In Section 2,
we describe the co-reference and the syntactic mod-
els and evaluate their performance on standard data
sets. Section 3 presents an evaluation of the overall
model on the Dundee eye-tracking corpus. The pa-
per closes with a comparison with related work and
a general discussion in Sections 4 and 5.
</bodyText>
<sectionHeader confidence="0.979283" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999977">
This model utilises an NP chunker based upon a hid-
den Markov model (HMM) as an approximation to
syntax. Using a simple model such as an HMM fa-
cilitates the integration of a co-reference component,
and the fact that the model is generative is a prereq-
uisite to using surprisal as our metric of interest (as
surprisal require the computation of prefix probabil-
ities). The key insight in our model is that human
sentence processing is, on average, facilitated when
a previously-mentioned discourse entity is repeated.
This facilitation depends upon keeping track of a list
of previously-mentioned entities, which requires (at
the least) shallow syntactic information, yet the fa-
cilitation itself is modeled primarily as a lexical phe-
nomenon. This allows a straightforward separation
of concerns: shallow syntax is captured using the
HMM’s hidden states, whereas the co-reference fa-
</bodyText>
<page confidence="0.961842">
305
</page>
<bodyText confidence="0.9901725">
However, using Bayes’ theorem we can compute:
cilitation is modeled using the HMM’s emissions.
The vocabulary of hidden states is described in Sec-
tion 2.1 and the emission distribution in Section 2.2
</bodyText>
<equation confidence="0.998708">
P(tag|word)P(word)
P(word|tag) = (1)
P(tag)
</equation>
<subsectionHeader confidence="0.814594">
2.1 Syntactic Model
</subsectionHeader>
<bodyText confidence="0.9999661">
A key feature of the co-reference component of our
model (described below) is that syntactic analysis
and co-reference resolution happen simultaneously.
This could potentially slow down the syntactic anal-
ysis, which tends to already be quite slow for ex-
haustive surprisal-based incremental parsers. There-
fore, rather than using full parsing, we use an HMM-
based NP chunker which allows for a fast analysis.
NP chunking is sufficient to extract NP discourse
mentions and, as we show below, surprisal values
computed using HMM chunks provide a useful fit
on the Dundee eye-movement data.
To allow the HMM to handle possessive construc-
tions as well as NP with simple modifiers and com-
plements, the HMM decodes NP subtrees with depth
of 2, by encoding the start, middle and end of a
syntactic category X as ‘(X’, ‘X’ and ‘X)’, respec-
tively. To reduce an explosion in the number of
states, the category begin state ‘(X’ only appears at
the rightmost lexical token of the constituent’s left-
most daughter. Likewise, ‘X)’ only appears at the
leftmost lexical token of the constituent’s rightmost
daughter. An example use of this state vocabulary
can be seen in Figure 1. Here, a small degree of re-
cursion allows for the NP ((new york city’s) general
obligation fund) to be encoded, with the outer NP’s
left bracket being ‘announced’ at the token ’s, which
is the rightmost lexical token of the inner NP. Hid-
den states also include part-of-speech (POS) tags,
allowing simultaneous POS tagging. In the exam-
ple given in Figure 1, the full state can be read by
listing the labels written above a word, from top to
bottom. For example, the full state associated with
’s is (NP-NP)-POS. As ’s can also be a contraction
of is, another possible state for ’s is VBZ (without
recursive categories as we are only interested in NP
chunks).
The model uses unsmoothed bi-gram transition
probabilities, along with a maximum entropy dis-
tribution to guess unknown word features. The re-
sulting distribution has the form P(tag|word) and is
therefore unsuitable for computing surprisal values.
which is what we need for surprisal. The pri-
mary information from this probability comes from
P(tag|word), however, reasonable estimates of
P(tag) and P(word) are required to ensure the prob-
ability distribution is proper. P(tag) may be esti-
mated on a parsed treebank. P(word), the probabil-
ity of a particular unseen word, is difficult to esti-
mate directly. Given that our training data contains
approximately 106 words, we assume that this prob-
ability must be bounded above by 10−6. As an ap-
proximation, we use this upper bound as the proba-
bility of P(word).
Training The chunker is trained on sections 2–
22 of the Wall Street Journal section of the Penn
Treebank. CoNLL 2000 included chunking as a
shared task, and the results are summarized by Tjong
Kim Sang and Buchholz (2000). Our chunker is not
comparable to the systems in the shared task for sev-
eral reasons: we use more training data, we tag si-
multaneously (the CoNLL systems used gold stan-
dard tags) and our notion of a chunk is somewhat
more complex than that used in CoNLL. The best
performing chunker from CoNLL 2000 achieved an
F-score of 93.5%, and the worst performing system
an F-score of 85.8%. Our chunker achieves a com-
parable F-score of 85.5%, despite the fact that it si-
multaneously tags and chunks, and only uses a bi-
gram model.
</bodyText>
<subsectionHeader confidence="0.882882">
2.2 Co-Reference Model
</subsectionHeader>
<bodyText confidence="0.99997">
In a standard HMM, the emission probabilities are
computed as P(wi|si) where wi is the ith word and si
is the ith state. In our model, we replace this with a
choice between two alternatives:
</bodyText>
<equation confidence="0.986516">
{ λPseen before(wi|si)
P(wi|si) = (1 − λ)Pdiscourse new(wi|si) (2)
</equation>
<bodyText confidence="0.9999908">
The ‘discourse new’ probability distribution is the
standard HMM emission distribution. The ‘seen be-
fore’ distribution is more complicated. It is in part
based upon caching language models. However, the
contents of the cache are not individual words but
</bodyText>
<page confidence="0.989405">
306
</page>
<figure confidence="0.724558">
(NP NP NP NP)
(NP NP) (NP NP NP NP) NP (NP NP NP)
77 NN IN NNP NNP NNP POS 77 NN NNS VBN RP DT NN NN
strong demand for new york city ’s general obligation bonds propped up the municipal market
</figure>
<figureCaption confidence="0.998863">
Figure 1: The chunk notation of a tree from the training data.
</figureCaption>
<figure confidence="0.889554857142857">
Variable Type
l,l0 List of trie nodes
w,wi Words
t Tag
n,n0 Trie nodes
l ← List(root of mention trie)
for w ← w0 to wn do
</figure>
<equation confidence="0.6862955">
l0 ← l
l ← 0/
</equation>
<bodyText confidence="0.9202245">
Clear tag freq array ft
Clear word freq array fwt
for t ∈ tag set do
for n ∈ l0 do
</bodyText>
<equation confidence="0.757475">
ft(t) ← ft(t)+FreqOf(n,t)
n0 ← Getchild(w,t)
if n0 =60/then
fwt(t) ← fwt(t)+FreqOf(n0,w,t)
l ← n0 :: l
end if
end for
end for
Pseen before(w|t) = ft(t)/ fwt(t)
end for
</equation>
<figureCaption confidence="0.990342">
Figure 2: Looking up entries from the NP Cache
</figureCaption>
<bodyText confidence="0.969017525">
rather a collection of all NPs mentioned so far in the
document.
Using a collection of NPs rather than individual
words complicates the decoding process. If m is the
size of a document, and n is the size of the current
sentence, decoding occurs in O(mn) time as opposed
to O(n), as the collection of NPs needs to be ac-
cessed at each word. However, we do not store the
NPs in a list, but rather a trie. This allows decoding
to occur in O(nlogm) time, which we have found
to be quite fast in practise. The algorithm used to
keep track of currently active NPs is presented in
Figure 2. This shows how the distribution Pseen before
is updated on a word-by-word basis. At the end of
each sentence, the NPs of the Viterbi parse are added
to the mention trie after having their leading arti-
cles stripped. A weakness of the algorithm is that
mentions are only added on a sentence-by-sentence
basis (disallowing within-sentence references). Al-
though the algorithm is intended to find whole-string
matches, in practise, it will count any NP whose pre-
fix matches as being co-referent.
A consequence of Equation 2 is that co-reference
resolution is handled at the same time as HMM de-
coding. Whenever the ‘seen before’ distribution is
applied, an NP is co-referent with one occurring ear-
lier. Likewise, whenever the ‘discourse new’ dis-
tribution is applied, the NP is not co-referent with
any NP appearing previously. As one choice or the
other is made during decoding, the decoder there-
fore also selects a chain of co-referent entities. Gen-
erally, for words which have been used in this dis-
course, the magnitude of probabilities in the ‘seen
before’ distribution are much higher than in the ‘dis-
course new’ distribution. Thus, there is a strong
bias to classify NPs which match word-for-word as
being co-referent. There remains a possibility that
the model primarily captures lexical priming, rather
than co-reference. However, we note that string
match is a strong indicator of two NPs being corefer-
</bodyText>
<page confidence="0.996807">
307
</page>
<bodyText confidence="0.99587627184466">
ent (cf. Soon et al. 2001), and, moreover, the match- will tend to re-fixate on a more central viewing lo-
ing is done on an NP-by-NP basis, which is more cation), (b) Right Bounded reading time, which in-
suitable for finding entity coreference, rather than a cludes all fixations on a word before moving to the
word-by-word basis, which would be more suitable right of the word (i.e., re-fixations after moving left
for lexical priming. are included), and (c) Second Pass, which includes
An appealing side-effect of using a simple co- any re-fixation on a word after looking at any other
reference decision rule which is applied incremen- word (be it to the left or the right of the word of inter-
tally is that it is relatively simple to incremen- est). We found that the model performed similarly
tally compute the transitive closure of co-reference across all these reading time metrics, we therefore
chains, resulting in the entity sets which are then only report results for Total Time.
used in evaluation. As mentioned above, reading measures are hy-
The co-reference model only has one free param- pothesised to correlate with Surprisal, which is de-
eter, λ, which is estimated from the ACE-2 corpus. fined as:
The estimate is computed by counting how often a S(wt) = −log(P(wt|w1...wt1) (3)
repeated NP actually is discourse new. In the current We compute the surprisal scores for the syntax-only
implementation of the model, λ is constant through- HMM, which does not have access to co-reference
out the test runs. However, λ could possibly be information (henceforth referred to as ‘HMM’)
a function of the previous discourse, allowing for and the full model, which combines the syntax-
more complicated classification probabilities. only HMM with the co-reference model (henceforth
3 Evaluation ‘HMM+Ref’). To determine if our Dundee corpus
3.1 Data simulations provide a reasonable model of human
Our evaluation experiments were conducted upon sentence processing, we perform a regression anal-
the Dundee corpus (Kennedy et al., 2003), which ysis with the Dundee corpus reading time measure
contains the eye-movement record of 10 participants as the dependent variable and the surprisal scores as
each reading 2,368 sentences of newspaper text. the independent variable.
This data set has previously been used by Demberg To account for noise in the corpus, we also use
and Keller (2008) and Frank (2009) among others. a number of additional explanatory variables which
3.2 Evaluation are known to strongly influence reading times.
Eye tracking data is noisy for a number of rea- These include the logarithm of the frequency of a
sons, including the fact that experimental partici- word (measured in occurrences per million) and the
pants can look at any word which is currently dis- length of a word in letters. Two additional explana-
played. While English is normally read in a left- tory variables were available in the Dundee corpus,
to-right manner, readers often skip words or make which we also included in the regression model.
regressions (i.e., look at a word to the left of the These were the position of a word on a line, and
one they are currently fixating). Deviations from which line in a document a word appeared in. As
a strict left-to-right progression of fixations moti- participants could only view one line at a time (i.e.,
vate the need for several different measures of eye one line per screen), these covariates are known as
movement. The model presented here predicts the line position and screen position, respectively.
Total Time that participants spent looking at a re- All the covariates, including the surprisal es-
gion, which includes any re-fixations after looking timates, were centered before including them in
away. In addition to total time, other possible mea- the regression model. Because the HMM and
sures include (a) First Pass, which measures the ini- HMM+Ref surprisal values are highly collinear, the
tial fixation and any re-fixations before looking at HMM+Ref surprisal values were added as residuals
any other word (this occurs, for instance, if the eye of the HMM surprisal values.
initially lands at the start of a long word – the eye In a normal regression analysis, one must either
308 assume that participants or the particular choice of
items add some randomness to the experiment, and mally short or abnormally long fixation durations.
either each participant’s responses for all items must 3.3 Results
be averaged (treating participants as a random fac- The result of the model comparison on Total Time
tor), or all participant’s responses for each item is reading data is summarised in Table 1. To allow this
averaged (treating items as a random factor). How- work to be compared with other models, the lower
ever, in the present analysis we utilise a mixed ef- part of the table gives the abosolute AIC, BIC and
fects model, which allows both items and partici- log likelihood of the baseline model, while the upper
pants to be treated as random factors.1 part gives delta AIC, BIC and log likelihood scores
The are a number of criteria which can be used of pairs of models.
to test the efficacy of one regression model over an- We found that both the HMM and HMM+Ref
other. These include the Aikake Information Cri- provide a significantly better fit with the reading
terion (AIC), the Bayesian Information Criterion time data than the Baseline model; all three crite-
(BIC), which trade off model fit and number of ria agree: AIC and BIC lower than for the base-
model parameters (lower scores are better). It is also line, and log-likelihood is higher. Moreover, the
common to compare the log-likelihood of the mod- HMM+Ref model provides a significantly better fit
els (higher log-likelihood is better), in which case a than the HMM model, which demonstrates the bene-
χ2 can be used to evaluate if a model offers a sig- fit of co-reference information for modeling reading
nificantly better fit, given the number of parameters times. Again, all three measures provide the same
is uses. We test three models: (i) a baseline, with result.
only low-level factors as independent variables; (ii) Table 2 corroborates this result. It list the
the HMM model, with the baseline factors plus sur- mixed-model coefficients for the HMM+Ref model
prisal computed by the syntax-only HMM; and (iii) and shows that all factors are significant predic-
the HMM+Ref model which includes the raw sur- tors, including both HMM surprisal and residualized
prisal values of the syntax-only HMM and the sur- HMM+Ref surprisal.
prisal of the HMM+Ref models as computed as a 4 Related Work
residual of the HMM surprisal score. We compare There have been few computational models of hu-
the HMM and HMM+Ref to the baseline, and the man sentence processing that have incorporated
HMM+Ref model against the HMM model. a referential or discourse-level component. Niv
Some of the data needed to be trimmed. If, due to (1994) proposed a parsing model based on Com-
data sparsity, the surprisal of a word goes to infinity binatory Categorial Grammar (Steedman, 2001), in
for one of the models, we entirely remove that word which referential information was used to resolve
from the analysis. This occurred seven times form syntactic ambiguities. The model was able to cap-
the HMM+Ref model, but did not occur at all with ture effects of referential information on syntactic
the HMM model. Some of the eye-movement data garden paths (Altmann and Steedman, 1988). This
was trimmed, as well. Fixations on the first and last model differs from that proposed in the present pa-
words of a line were excluded, as were tracklosses. per, as it is intended to capture psycholinguistic pref-
However, we did not trim any items due to abnor- erences in a qualitative manner, whereas the aim
of the present model is to provide a quantitative
fit to measures of processing difficulty. Moreover,
the model was not based on a large-scale grammar,
and was not tested on unrestricted text. Spivey and
Tanenhaus (1998) proposed a sentence processing
model that examined the effects of referential infor-
mation, as well as other constraints, on the resolu-
tion of ambiguous sentences. Unlike Niv (1994),
1We assume that each participant and item bias the reading
time of the experiment. Such an analysis is known as having
random intercepts of participant and item. It is also possible
to assume a more involved analysis, known as random slopes,
where the participants and items bias the slope of the predictor.
The model did not converge when using random intercept and
slopes on both participant and item. If random slopes on items
were left out, the HMM regression model did converge, but not
the HMM+Ref model. As the HMM+Ref is the model of inter-
est random slopes were left out entirely to allow a like-with-like
comparison between the HMM and HMM+Ref regression mod-
els.
</bodyText>
<table confidence="0.948187571428571">
309
From To 0 AIC 0 BIC 0 logLik x2 Significance
Baseline HMM -80 -69 41 82.112 p &lt; .001
Baseline HMM+Ref -99 -89 51 101.54 p &lt; .001
HMM HMM+Ref -19 -8 11 21.424 p &lt; .001
Model AIC BIC logLik
Baseline 10567789 10567880 -5283886
</table>
<tableCaption confidence="0.993124">
Table 1: Model comparison (upper part) and absolute scores for the Baseline model (lower part)
</tableCaption>
<table confidence="0.999928375">
Coefficient Estimate Std Error t-value
(Intercept) 991.4346 23.7968 41.66
log(Word Frequency) -55.3045 1.4830 -37.29
Word Length 128.6216 1.4677 87.63
Screen Position -1.7769 0.1326 -13.40
Line Position 10.1592 0.7387 13.75
HMM 12.1287 1.3366 9.07
HMM+Ref 19.2772 4.1627 4.63
</table>
<tableCaption confidence="0.982861">
Table 2: Coefficients of the HMM+Ref model on Total Reading Times. Note that t &gt; 2 indicates that the factor in
question is a significant predictor.
</tableCaption>
<bodyText confidence="0.9999367">
Spivey and Tanenhaus’s (1998) model was specifi-
cally designed to provide a quantitative fit to reading
times. However, the model lacked generality, being
designed to deal with only one type of sentence. In
contrast to both of these earlier models, the model
proposed here aims to be general enough to provide
estimated reading times for unrestricted text. In fact,
as far as we are aware, the present paper represents
the first wide-coverage model of human parsing that
has incorporated discourse-level information.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="acknowledgments">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99999545">
The primary finding of this work is that incorporat-
ing discourse information such as co-reference into
an incremental probabilistic model of sentence pro-
cessing has a beneficial effect on the ability of the
model to predict broad-coverage human parsing be-
haviour.
Although not thoroughly explored in this paper,
our finding is related to an ongoing debate about the
structure of the human sentence processor. In par-
ticular, the model of Dubey (2010), which also sim-
ulates the effect of discourse on syntax, is aimed at
examining interactivity in the human sentence pro-
cessor. Interactivity describes the degree to which
human parsing is influenced by non-syntactic fac-
tors. Under the weakly interactive hypothesis, dis-
course factors may prune or re-weight parses, but
only when assuming the strongly interactive hypoth-
esis would we argue that the sentence processor pre-
dicts upcoming material due to discourse factors.
Dubey found that a weakly interactive model sim-
ulated a pattern of results in an experiment (Crrodner
et al., 2005) which was previously believed to pro-
vide evidence for the strongly interactive hypothesis.
However, as Dubey does not provide broad-coverage
parsing results, this leaves open the possibility that
the model cannot generalise beyond the experiments
expressly modeled in Dubey (2010).
The model presented here, on the other hand,
is not only broad-coverage but could also be de-
scribed as a strongly interactive model. The strong
interactivity arises because co-reference resolution
is strongly tied to lexical generation probabilities,
which are part of the syntactic portion of our model.
This cannot be achieve in a weakly interactive
model, which is limited to pruning or re-weighting
of parses based on discourse information. As our
analysis on the Dundee corpus showed, the lexical
probabilities (in the form of HMM+Ref surprisal)
are key to improving the fit on eye-tracking data.
We therefore argue that our results provide evidence
</bodyText>
<page confidence="0.992758">
310
</page>
<bodyText confidence="0.9999391">
against a weakly interactive approach, which may be
sufficient to model individual phenomena (as shown
by Dubey 2010), but is unlikely to be able to match
the broad-coverage result we have presented here.
We also note that psycholinguistic evidence for dis-
course prediction (such as the context based lexi-
cal prediction shown by van Berkum et al. 2005,
see Section 1) is also evidence for strong interac-
tivity; prediction goes beyond mere pruning or re-
weighting and requires strong interactivity.
</bodyText>
<sectionHeader confidence="0.97468" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.828748592592592">
Gerry Altmann and Mark Steedman. Interaction
with context during human sentence processing.
Cognition, 30:191–238, 1988.
Gerry T. M. Altmann and Yuki Kamide. Incremen-
tal interpretation at verbs: Restricting the domain
of subsequent reference. Cognition, 73:247–264,
1999.
Marisa Ferrara Boston, John T. Hale, Reinhold
Kliegl, and Shravan Vasisht. Surprising parser
actions and reading difficulty. In Proceedings of
ACL-08:HLT, Short Papers, pages 5–8, 2008.
Charles Clifton, Adrian Staub, and Keith Rayner.
Eye movement in reading words and sentences.
In R V Gompel, M Fisher, W Murray, and R L
Hill, editors, Eye Movements: A Window in Mind
and Brain, pages 341–372. Elsevier, 2007.
Vera Demberg and Frank Keller. Data from eye-
tracking corpora as evidence for theories of syn-
tactic processing complexity. Cognition, 109:
192–210, 2008.
Vera Demberg and Frank Keller. A computational
model of prediction in human parsing: Unifying
locality and surprisal effects. In Proceedings of
the 29th meeting of the Cognitive Science Society
(CogSci-09), 2009.
Amit Dubey. The influence of discourse on syntax:
A psycholinguistic model of sentence processing.
</bodyText>
<reference confidence="0.989563745098039">
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL
2010), Uppsala, Sweden, 2010.
Stefan Frank. Surprisal-based comparison between
a symbolic and a connectionist model of sentence
processing. In 31st Annual Conference of the
Cognitive Science Society (COGSCI2009), Ams-
terdam, The Netherlands, 2009.
Lyn Frazier, Alan Munn, and Charles Clifton. Pro-
cessing coordinate structure. Journal of Psy-
cholinguistic Research, 29:343–368, 2000.
Daniel J. Grodner, Edward A. F. Gibson, and Du-
ane Watson. The influence of contextual constrast
on syntactic processing: Evidence for strong-
interaction in sentence comprehension. Cogni-
tion, 95(3):275–296, 2005.
John T. Hale. A probabilistic earley parser as a psy-
cholinguistic model. In In Proceedings of the Sec-
ond Meeting of the North American Chapter of
the Asssociation for Computational Linguistics,
2001.
A. Kennedy, R. Hill, and J. Pynte. The dundee cor-
pus. In Proceedings of the 12th European confer-
ence on eye movement, 2003.
Jeff Mitchell, Mirella Lapata, Vera Demberg, and
Frank Keller. Syntactic and semantic factors in
processing difficulty: An integrated measure. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, Uppsala,
Sweden, 2010.
M. Niv. A psycholinguistically motivated parser for
CCG. In Proceedings of the 32nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-94), pages 125–132, Las Cruces, NM,
1994.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. A ma-
chine learning approach to coreference resolution
of noun phrases. Computational Linguistics, 27
(4):521–544, 2001.
M. J. Spivey and M. K. Tanenhaus. Syntactic am-
biguity resolution in discourse: Modeling the ef-
fects of referential context and lexical frequency.
Journal of Experimental Psychology: Learning,
Memory and Cognition, 24(6):1521–1543, 1998.
Kieth E. Stanovich and Richard F. West. The effect
of sentence context on ongoing word recognition:
Tests of a two-pricess theory. Journal of Exper-
imental Psychology: Human Perception and Per-
formance, 7:658–672, 1981.
Adrian Staub and Charles Clifton. Syntactic predic-
tion in language comprehension: Evidence from
</reference>
<page confidence="0.984644">
311
</page>
<reference confidence="0.999222111111111">
either ...or. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 32:425–436,
2006.
Mark Steedman. The Syntactic Process. Bradford
Books, 2001.
Erik F. Tjong Kim Sang and Sabine Buchholz. In-
troduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL-
2000, pages 127–132. Lisbon, Portugal, 2000.
Jos J. A. van Berkum, Colin M. Brown, Pienie Zwit-
serlood, Valesca Kooijman, and Peter Hagoort.
Anticipating upcoming words in discourse: Evi-
dence from erps and reading times. Journal of Ex-
perimental Psychology: Learning, Memory and
Cognition, 31(3):443–467, 2005.
Barton Wright and Merrill F. Garrett. Lexical deci-
sion in sentences: Effects of syntactic structure.
Memory and Cognition, 12:31–45, 1984.
</reference>
<page confidence="0.998663">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.374495">
<title confidence="0.808155">A Model of Discourse Predictions in Human Sentence Processing Dubey Keller</title>
<affiliation confidence="0.790868">Human Communication Research Centre, University of</affiliation>
<address confidence="0.827431">10 Crichton Street, Edinburgh EH8 9AB,</address>
<email confidence="0.996811">amit.dubey@ed.ac.uk</email>
<email confidence="0.996811">frank.keller@ed.ac.uk</email>
<email confidence="0.996811">patrick.sturt@ed.ac.uk</email>
<abstract confidence="0.998264625">This paper introduces a psycholinguistic model of sentence processing which combines a Hidden Markov Model noun phrase chunker with a co-reference classifier. Both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. This allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. We evaluate our surprisal predictions on the Dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010),</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5445" citStr="(2010)" startWordPosition="848" endWordPosition="848">n language processing have focused on syntactic prediction. Examples include Hale’s (2001) surprisal model, which relates processing effort to the conditional probability of the current word given the previous words in the sentence. This approach has been elaborated by Demberg and Keller (2009) in a model that explicitly constructs predicted structure, and includes a verification process that incurs additional processing cost if predictions are not met. Recent work has attempted to integrate semantic and discourse prediction with models of syntactic processing. This includes Mitchell et al.’s (2010) approach, which combines an incremental parser with a vector-space model of semantics. However, this approach only provides a loose integration of the two components (through simple addition of their probabilities), and the notion of semantics used is restricted to lexical meaning approximated by word co-occurrences. At the discourse level, Dubey (2010) has proposed a model that combines an incremental parser with a probabilistic logic-based model of co-reference resolution. However, this model does not explicitly model discourse effects in terms of prediction, and again only proposes a loose</context>
<context position="25679" citStr="(2010)" startWordPosition="4225" endWordPosition="4225">as we are aware, the present paper represents the first wide-coverage model of human parsing that has incorporated discourse-level information. 5 Discussion The primary finding of this work is that incorporating discourse information such as co-reference into an incremental probabilistic model of sentence processing has a beneficial effect on the ability of the model to predict broad-coverage human parsing behaviour. Although not thoroughly explored in this paper, our finding is related to an ongoing debate about the structure of the human sentence processor. In particular, the model of Dubey (2010), which also simulates the effect of discourse on syntax, is aimed at examining interactivity in the human sentence processor. Interactivity describes the degree to which human parsing is influenced by non-syntactic factors. Under the weakly interactive hypothesis, discourse factors may prune or re-weight parses, but only when assuming the strongly interactive hypothesis would we argue that the sentence processor predicts upcoming material due to discourse factors. Dubey found that a weakly interactive model simulated a pattern of results in an experiment (Crrodner et al., 2005) which was prev</context>
</contexts>
<marker>2010</marker>
<rawString>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>In 31st Annual Conference of the Cognitive Science Society (COGSCI2009),</booktitle>
<location>Amsterdam, The</location>
<contexts>
<context position="6954" citStr="Frank 2009" startWordPosition="1073" endWordPosition="1074">s. We propose a computational model that captures discourse effects on syntax in terms of prediction. The model comprises a co-reference component which explicitly stores discourse mentions of NPs, and a syntactic component which adjust the probabilities of NPs in the syntactic structure based on the mentions tracked by the discourse component. Our model is HMM-based, which makes it possible to efficiently process large amounts of data, allowing an evaluation on eye-tracking corpora, which has recently become the gold-standard in computational psycholinguistics (e.g., Demberg and Keller 2008; Frank 2009; Boston et al. 2008; Mitchell et al. 2010). The paper is structured as follows: In Section 2, we describe the co-reference and the syntactic models and evaluate their performance on standard data sets. Section 3 presents an evaluation of the overall model on the Dundee eye-tracking corpus. The paper closes with a comparison with related work and a general discussion in Sections 4 and 5. 2 Model This model utilises an NP chunker based upon a hidden Markov model (HMM) as an approximation to syntax. Using a simple model such as an HMM facilitates the integration of a co-reference component, and </context>
<context position="17492" citStr="Frank (2009)" startWordPosition="2866" endWordPosition="2867">ion ‘HMM+Ref’). To determine if our Dundee corpus 3.1 Data simulations provide a reasonable model of human Our evaluation experiments were conducted upon sentence processing, we perform a regression analthe Dundee corpus (Kennedy et al., 2003), which ysis with the Dundee corpus reading time measure contains the eye-movement record of 10 participants as the dependent variable and the surprisal scores as each reading 2,368 sentences of newspaper text. the independent variable. This data set has previously been used by Demberg To account for noise in the corpus, we also use and Keller (2008) and Frank (2009) among others. a number of additional explanatory variables which 3.2 Evaluation are known to strongly influence reading times. Eye tracking data is noisy for a number of rea- These include the logarithm of the frequency of a sons, including the fact that experimental partici- word (measured in occurrences per million) and the pants can look at any word which is currently dis- length of a word in letters. Two additional explanaplayed. While English is normally read in a left- tory variables were available in the Dundee corpus, to-right manner, readers often skip words or make which we also inc</context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>Stefan Frank. Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In 31st Annual Conference of the Cognitive Science Society (COGSCI2009), Amsterdam, The Netherlands, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
<author>Alan Munn</author>
<author>Charles Clifton</author>
</authors>
<title>Processing coordinate structure.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<contexts>
<context position="2614" citStr="Frazier et al., 2000" startWordPosition="398" endWordPosition="402">d argument of a verb before having encountered it, e.g., they will fixate an edible object as soon as they hear the word eat (Altmann and Kamide, 1999). Semantic prediction has also been shown in the context of semantic priming: a word that is preceded by a semantically related prime or by a semantically congruous sentence fragment is processed faster (Stanovich and West, 1981; Clifton et al., 2007). An example for syntactic prediction can be found in coordinate structures: readers predict that the second conjunct in a coordination will have the same syntactic structure as the first conjunct (Frazier et al., 2000). In a similar vein, having encountered the word either, readers predict that or and a conjunct will follow it (Staub and Clifton, 2006). Again, priming studies corroborate this: Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to it (Wright and Garrett, 1984). Predictive processing is not confined to the sentence level. Recent experimental results also provide evidence for discourse prediction. An example is the study by van Berkum et al. (2005), who used a context that made a target noun highly predict</context>
</contexts>
<marker>Frazier, Munn, Clifton, 2000</marker>
<rawString>Lyn Frazier, Alan Munn, and Charles Clifton. Processing coordinate structure. Journal of Psycholinguistic Research, 29:343–368, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel J Grodner</author>
<author>Edward A F Gibson</author>
<author>Duane Watson</author>
</authors>
<title>The influence of contextual constrast on syntactic processing: Evidence for stronginteraction in sentence comprehension.</title>
<date>2005</date>
<journal>Cognition,</journal>
<volume>95</volume>
<issue>3</issue>
<marker>Grodner, Gibson, Watson, 2005</marker>
<rawString>Daniel J. Grodner, Edward A. F. Gibson, and Duane Watson. The influence of contextual constrast on syntactic processing: Evidence for stronginteraction in sentence comprehension. Cognition, 95(3):275–296, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John T Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model. In</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Asssociation for Computational Linguistics,</booktitle>
<marker>Hale, 2001</marker>
<rawString>John T. Hale. A probabilistic earley parser as a psycholinguistic model. In In Proceedings of the Second Meeting of the North American Chapter of the Asssociation for Computational Linguistics, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kennedy</author>
<author>R Hill</author>
<author>J Pynte</author>
</authors>
<title>The dundee corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th European conference on eye movement,</booktitle>
<contexts>
<context position="17123" citStr="Kennedy et al., 2003" startWordPosition="2804" endWordPosition="2807"> the model, λ is constant through- HMM, which does not have access to co-reference out the test runs. However, λ could possibly be information (henceforth referred to as ‘HMM’) a function of the previous discourse, allowing for and the full model, which combines the syntaxmore complicated classification probabilities. only HMM with the co-reference model (henceforth 3 Evaluation ‘HMM+Ref’). To determine if our Dundee corpus 3.1 Data simulations provide a reasonable model of human Our evaluation experiments were conducted upon sentence processing, we perform a regression analthe Dundee corpus (Kennedy et al., 2003), which ysis with the Dundee corpus reading time measure contains the eye-movement record of 10 participants as the dependent variable and the surprisal scores as each reading 2,368 sentences of newspaper text. the independent variable. This data set has previously been used by Demberg To account for noise in the corpus, we also use and Keller (2008) and Frank (2009) among others. a number of additional explanatory variables which 3.2 Evaluation are known to strongly influence reading times. Eye tracking data is noisy for a number of rea- These include the logarithm of the frequency of a sons,</context>
</contexts>
<marker>Kennedy, Hill, Pynte, 2003</marker>
<rawString>A. Kennedy, R. Hill, and J. Pynte. The dundee corpus. In Proceedings of the 12th European conference on eye movement, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Syntactic and semantic factors in processing difficulty: An integrated measure.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6997" citStr="Mitchell et al. 2010" startWordPosition="1079" endWordPosition="1082">del that captures discourse effects on syntax in terms of prediction. The model comprises a co-reference component which explicitly stores discourse mentions of NPs, and a syntactic component which adjust the probabilities of NPs in the syntactic structure based on the mentions tracked by the discourse component. Our model is HMM-based, which makes it possible to efficiently process large amounts of data, allowing an evaluation on eye-tracking corpora, which has recently become the gold-standard in computational psycholinguistics (e.g., Demberg and Keller 2008; Frank 2009; Boston et al. 2008; Mitchell et al. 2010). The paper is structured as follows: In Section 2, we describe the co-reference and the syntactic models and evaluate their performance on standard data sets. Section 3 presents an evaluation of the overall model on the Dundee eye-tracking corpus. The paper closes with a comparison with related work and a general discussion in Sections 4 and 5. 2 Model This model utilises an NP chunker based upon a hidden Markov model (HMM) as an approximation to syntax. Using a simple model such as an HMM facilitates the integration of a co-reference component, and the fact that the model is generative is a </context>
</contexts>
<marker>Mitchell, Lapata, Demberg, Keller, 2010</marker>
<rawString>Jeff Mitchell, Mirella Lapata, Vera Demberg, and Frank Keller. Syntactic and semantic factors in processing difficulty: An integrated measure. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Niv</author>
</authors>
<title>A psycholinguistically motivated parser for CCG.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94),</booktitle>
<pages>125--132</pages>
<location>Las Cruces, NM,</location>
<contexts>
<context position="23279" citStr="Niv (1994)" startWordPosition="3830" endWordPosition="3831">t pawords of a line were excluded, as were tracklosses. per, as it is intended to capture psycholinguistic prefHowever, we did not trim any items due to abnor- erences in a qualitative manner, whereas the aim of the present model is to provide a quantitative fit to measures of processing difficulty. Moreover, the model was not based on a large-scale grammar, and was not tested on unrestricted text. Spivey and Tanenhaus (1998) proposed a sentence processing model that examined the effects of referential information, as well as other constraints, on the resolution of ambiguous sentences. Unlike Niv (1994), 1We assume that each participant and item bias the reading time of the experiment. Such an analysis is known as having random intercepts of participant and item. It is also possible to assume a more involved analysis, known as random slopes, where the participants and items bias the slope of the predictor. The model did not converge when using random intercept and slopes on both participant and item. If random slopes on items were left out, the HMM regression model did converge, but not the HMM+Ref model. As the HMM+Ref is the model of interest random slopes were left out entirely to allow a</context>
</contexts>
<marker>Niv, 1994</marker>
<rawString>M. Niv. A psycholinguistically motivated parser for CCG. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94), pages 125–132, Las Cruces, NM, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<pages>4--521</pages>
<contexts>
<context position="15113" citStr="Soon et al. 2001" startWordPosition="2478" endWordPosition="2481">ly. As one choice or the other is made during decoding, the decoder therefore also selects a chain of co-referent entities. Generally, for words which have been used in this discourse, the magnitude of probabilities in the ‘seen before’ distribution are much higher than in the ‘discourse new’ distribution. Thus, there is a strong bias to classify NPs which match word-for-word as being co-referent. There remains a possibility that the model primarily captures lexical priming, rather than co-reference. However, we note that string match is a strong indicator of two NPs being corefer307 ent (cf. Soon et al. 2001), and, moreover, the match- will tend to re-fixate on a more central viewing loing is done on an NP-by-NP basis, which is more cation), (b) Right Bounded reading time, which insuitable for finding entity coreference, rather than a cludes all fixations on a word before moving to the word-by-word basis, which would be more suitable right of the word (i.e., re-fixations after moving left for lexical priming. are included), and (c) Second Pass, which includes An appealing side-effect of using a simple co- any re-fixation on a word after looking at any other reference decision rule which is applied</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. C. Y. Lim. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27 (4):521–544, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Spivey</author>
<author>M K Tanenhaus</author>
</authors>
<title>Syntactic ambiguity resolution in discourse: Modeling the effects of referential context and lexical frequency.</title>
<date>1998</date>
<journal>Journal of Experimental Psychology: Learning, Memory and Cognition,</journal>
<volume>24</volume>
<issue>6</issue>
<contexts>
<context position="23098" citStr="Spivey and Tanenhaus (1998)" startWordPosition="3800" endWordPosition="3803">ctic the HMM model. Some of the eye-movement data garden paths (Altmann and Steedman, 1988). This was trimmed, as well. Fixations on the first and last model differs from that proposed in the present pawords of a line were excluded, as were tracklosses. per, as it is intended to capture psycholinguistic prefHowever, we did not trim any items due to abnor- erences in a qualitative manner, whereas the aim of the present model is to provide a quantitative fit to measures of processing difficulty. Moreover, the model was not based on a large-scale grammar, and was not tested on unrestricted text. Spivey and Tanenhaus (1998) proposed a sentence processing model that examined the effects of referential information, as well as other constraints, on the resolution of ambiguous sentences. Unlike Niv (1994), 1We assume that each participant and item bias the reading time of the experiment. Such an analysis is known as having random intercepts of participant and item. It is also possible to assume a more involved analysis, known as random slopes, where the participants and items bias the slope of the predictor. The model did not converge when using random intercept and slopes on both participant and item. If random slo</context>
</contexts>
<marker>Spivey, Tanenhaus, 1998</marker>
<rawString>M. J. Spivey and M. K. Tanenhaus. Syntactic ambiguity resolution in discourse: Modeling the effects of referential context and lexical frequency. Journal of Experimental Psychology: Learning, Memory and Cognition, 24(6):1521–1543, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kieth E Stanovich</author>
<author>Richard F West</author>
</authors>
<title>The effect of sentence context on ongoing word recognition: Tests of a two-pricess theory.</title>
<date>1981</date>
<journal>Journal of Experimental Psychology: Human Perception and Performance,</journal>
<volume>7</volume>
<contexts>
<context position="2372" citStr="Stanovich and West, 1981" startWordPosition="360" endWordPosition="363">rediction has been found in a range of psycholinguistic processing domains. Semantic prediction has been demonstrated by studies that show anticipation based on selectional restrictions: listeners are able to launch eye-movements to the predicted argument of a verb before having encountered it, e.g., they will fixate an edible object as soon as they hear the word eat (Altmann and Kamide, 1999). Semantic prediction has also been shown in the context of semantic priming: a word that is preceded by a semantically related prime or by a semantically congruous sentence fragment is processed faster (Stanovich and West, 1981; Clifton et al., 2007). An example for syntactic prediction can be found in coordinate structures: readers predict that the second conjunct in a coordination will have the same syntactic structure as the first conjunct (Frazier et al., 2000). In a similar vein, having encountered the word either, readers predict that or and a conjunct will follow it (Staub and Clifton, 2006). Again, priming studies corroborate this: Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to it (Wright and Garrett, 1984). Predi</context>
</contexts>
<marker>Stanovich, West, 1981</marker>
<rawString>Kieth E. Stanovich and Richard F. West. The effect of sentence context on ongoing word recognition: Tests of a two-pricess theory. Journal of Experimental Psychology: Human Perception and Performance, 7:658–672, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Staub</author>
<author>Charles Clifton</author>
</authors>
<title>Syntactic prediction in language comprehension: Evidence from either ...or.</title>
<date>2006</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<volume>32</volume>
<contexts>
<context position="2750" citStr="Staub and Clifton, 2006" startWordPosition="423" endWordPosition="426"> and Kamide, 1999). Semantic prediction has also been shown in the context of semantic priming: a word that is preceded by a semantically related prime or by a semantically congruous sentence fragment is processed faster (Stanovich and West, 1981; Clifton et al., 2007). An example for syntactic prediction can be found in coordinate structures: readers predict that the second conjunct in a coordination will have the same syntactic structure as the first conjunct (Frazier et al., 2000). In a similar vein, having encountered the word either, readers predict that or and a conjunct will follow it (Staub and Clifton, 2006). Again, priming studies corroborate this: Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to it (Wright and Garrett, 1984). Predictive processing is not confined to the sentence level. Recent experimental results also provide evidence for discourse prediction. An example is the study by van Berkum et al. (2005), who used a context that made a target noun highly predictable, and found a mismatch effect in the ERP (event-related brain potential) when an adjective appeared that was inconsistent with the t</context>
</contexts>
<marker>Staub, Clifton, 2006</marker>
<rawString>Adrian Staub and Charles Clifton. Syntactic prediction in language comprehension: Evidence from either ...or. Journal of Experimental Psychology: Learning, Memory, and Cognition, 32:425–436, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<publisher>Bradford Books,</publisher>
<contexts>
<context position="22169" citStr="Steedman, 2001" startWordPosition="3645" endWordPosition="3646">al and residualized prisal values of the syntax-only HMM and the sur- HMM+Ref surprisal. prisal of the HMM+Ref models as computed as a 4 Related Work residual of the HMM surprisal score. We compare There have been few computational models of huthe HMM and HMM+Ref to the baseline, and the man sentence processing that have incorporated HMM+Ref model against the HMM model. a referential or discourse-level component. Niv Some of the data needed to be trimmed. If, due to (1994) proposed a parsing model based on Comdata sparsity, the surprisal of a word goes to infinity binatory Categorial Grammar (Steedman, 2001), in for one of the models, we entirely remove that word which referential information was used to resolve from the analysis. This occurred seven times form syntactic ambiguities. The model was able to capthe HMM+Ref model, but did not occur at all with ture effects of referential information on syntactic the HMM model. Some of the eye-movement data garden paths (Altmann and Steedman, 1988). This was trimmed, as well. Fixations on the first and last model differs from that proposed in the present pawords of a line were excluded, as were tracklosses. per, as it is intended to capture psycholing</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. The Syntactic Process. Bradford Books, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL2000,</booktitle>
<pages>127--132</pages>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="11345" citStr="Sang and Buchholz (2000)" startWordPosition="1798" endWordPosition="1801">) are required to ensure the probability distribution is proper. P(tag) may be estimated on a parsed treebank. P(word), the probability of a particular unseen word, is difficult to estimate directly. Given that our training data contains approximately 106 words, we assume that this probability must be bounded above by 10−6. As an approximation, we use this upper bound as the probability of P(word). Training The chunker is trained on sections 2– 22 of the Wall Street Journal section of the Penn Treebank. CoNLL 2000 included chunking as a shared task, and the results are summarized by Tjong Kim Sang and Buchholz (2000). Our chunker is not comparable to the systems in the shared task for several reasons: we use more training data, we tag simultaneously (the CoNLL systems used gold standard tags) and our notion of a chunk is somewhat more complex than that used in CoNLL. The best performing chunker from CoNLL 2000 achieved an F-score of 93.5%, and the worst performing system an F-score of 85.8%. Our chunker achieves a comparable F-score of 85.5%, despite the fact that it simultaneously tags and chunks, and only uses a bigram model. 2.2 Co-Reference Model In a standard HMM, the emission probabilities are compu</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. Introduction to the conll-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL2000, pages 127–132. Lisbon, Portugal, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos J A van Berkum</author>
<author>Colin M Brown</author>
<author>Pienie Zwitserlood</author>
<author>Valesca Kooijman</author>
<author>Peter Hagoort</author>
</authors>
<title>Anticipating upcoming words in discourse: Evidence from erps and reading times.</title>
<date>2005</date>
<journal>Journal of Experimental Psychology: Learning, Memory and Cognition,</journal>
<volume>31</volume>
<issue>3</issue>
<marker>van Berkum, Brown, Zwitserlood, Kooijman, Hagoort, 2005</marker>
<rawString>Jos J. A. van Berkum, Colin M. Brown, Pienie Zwitserlood, Valesca Kooijman, and Peter Hagoort. Anticipating upcoming words in discourse: Evidence from erps and reading times. Journal of Experimental Psychology: Learning, Memory and Cognition, 31(3):443–467, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barton Wright</author>
<author>Merrill F Garrett</author>
</authors>
<title>Lexical decision in sentences: Effects of syntactic structure.</title>
<date>1984</date>
<journal>Memory and Cognition,</journal>
<volume>12</volume>
<contexts>
<context position="2965" citStr="Wright and Garrett, 1984" startWordPosition="456" endWordPosition="460">faster (Stanovich and West, 1981; Clifton et al., 2007). An example for syntactic prediction can be found in coordinate structures: readers predict that the second conjunct in a coordination will have the same syntactic structure as the first conjunct (Frazier et al., 2000). In a similar vein, having encountered the word either, readers predict that or and a conjunct will follow it (Staub and Clifton, 2006). Again, priming studies corroborate this: Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to it (Wright and Garrett, 1984). Predictive processing is not confined to the sentence level. Recent experimental results also provide evidence for discourse prediction. An example is the study by van Berkum et al. (2005), who used a context that made a target noun highly predictable, and found a mismatch effect in the ERP (event-related brain potential) when an adjective appeared that was inconsistent with the target noun. An example is (we give translations of their Dutch materials): (1) The burglar had no trouble locating the secret family safe. a. Of course, it was situated behind a 304 Proceedings of the 2011 Conferenc</context>
</contexts>
<marker>Wright, Garrett, 1984</marker>
<rawString>Barton Wright and Merrill F. Garrett. Lexical decision in sentences: Effects of syntactic structure. Memory and Cognition, 12:31–45, 1984.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>