<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.986869">
Active Learning by Labeling Features
</title>
<author confidence="0.985885">
Gregory Druck Burr Settles Andrew McCallum
</author>
<affiliation confidence="0.9864645">
Dept. of Computer Science Dept. of Biostatistics &amp; Dept. of Computer Science
University of Massachusetts Medical Informatics University of Massachusetts
</affiliation>
<note confidence="0.633394">
Amherst, MA 01003 Dept. of Computer Sciences Amherst, MA 01003
gdruck@cs.umass.edu University of Wisconsin mccallum@cs.umass.edu
Madison, WI 53706
</note>
<email confidence="0.918844">
bsettles@cs.wisc.edu
</email>
<sectionHeader confidence="0.992376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998765625">
Methods that learn from prior informa-
tion about input features such as general-
ized expectation (GE) have been used to
train accurate models with very little ef-
fort. In this paper, we propose an ac-
tive learning approach in which the ma-
chine solicits “labels” on features rather
than instances. In both simulated and real
user experiments on two sequence label-
ing tasks we show that our active learning
method outperforms passive learning with
features as well as traditional active learn-
ing with instances. Preliminary experi-
ments suggest that novel interfaces which
intelligently solicit labels on multiple fea-
tures facilitate more efficient annotation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99971465">
The application of machine learning to new prob-
lems is slowed by the need for labeled training
data. When output variables are structured, an-
notation can be particularly difficult and time-
consuming. For example, when training a condi-
tional random field (Lafferty et al., 2001) to ex-
tract fields such as rent, contact, features, and utilities
from apartment classifieds, labeling 22 instances
(2,540 tokens) provides only 66.1% accuracy.1
Recent work has used unlabeled data and lim-
ited prior information about input features to boot-
strap accurate structured output models. For ex-
ample, both Haghighi and Klein (2006) and Mann
and McCallum (2008) have demonstrated results
better than 66.1% on the apartments task de-
scribed above using only a list of 33 highly dis-
criminative features and the labels they indicate.
However, these methods have only been applied
in scenarios in which the user supplies such prior
knowledge before learning begins.
</bodyText>
<footnote confidence="0.670541">
1Averaged over 10 randomly selected sets of 22 instances.
</footnote>
<bodyText confidence="0.999944634146342">
In traditional active learning (Settles, 2009), the
machine queries the user for only the labels of in-
stances that would be most helpful to the machine.
This paper proposes an active learning approach in
which the user provides “labels” for input features,
rather than instances. A labeled input feature de-
notes that a particular input feature, for example
the word call, is highly indicative of a particular
label, such as contact. Table 1 provides an excerpt
of a feature active learning session.
In this paper, we advocate using generalized
expectation (GE) criteria (Mann and McCallum,
2008) for learning with labeled features. We pro-
vide an alternate treatment of the GE objective
function used by Mann and McCallum (2008) and
a novel speedup to the gradient computation. We
then provide a pool-based feature active learning
algorithm that includes an option to skip queries,
for cases in which a feature has no clear label.
We propose and evaluate feature query selection
algorithms that aim to reduce model uncertainty,
and compare to several baselines. We evaluate
our method using both real and simulated user ex-
periments on two sequence labeling tasks. Com-
pared to previous approaches (Raghavan and Al-
lan, 2007), our method can be used for both classi-
fication and structured tasks, and the feature query
selection methods we propose perform better.
We use experiments with simulated labelers on
real data to extensively compare feature query se-
lection algorithms and evaluate on multiple ran-
dom splits. To make these simulations more re-
alistic, the effort required to perform different la-
beling actions is estimated from additional exper-
iments with real users. The results show that ac-
tive learning with features outperforms both pas-
sive learning with features and traditional active
learning with instances.
In the user experiments, each annotator actively
labels instances, actively labels features one at a
time, and actively labels batches of features orga-
</bodyText>
<page confidence="0.98792">
81
</page>
<note confidence="0.996631">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 81–90,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.916260407407407">
accuracy 46.5 → 60.5
feature label
PHONE* contact
call contact
deposit rent
month rent
pets restrict.
lease rent
appointment contact
parking features
EMAIL* contact
information contact
accuracy 60.5 → 67.1
feature label
water utilities
close neighbor.
garbage utilities
included utilities
features
shopping neighbor.
bart neighbor.
downtown neighbor.
TIME* contact
bath size
to the difference between the empirical expecta-
tion of Fj and the model expectation of Fj, where
Fj(y, x) = Ei fj(yi, yi+1, x, i).
</bodyText>
<equation confidence="0.997831333333333">
∂ L(θ) = E˜p(x,y)[Fj(y, x)]
∂θj
− E˜p(x)[Ep(y|x;θ)[Fj(y, x)]].
</equation>
<bodyText confidence="0.951955363636364">
We also include a zero-mean variance σ2 = 10
Gaussian prior on parameters in all experiments.2
Table 1: Two iterations of feature active learning.
Each table shows the features labeled, and the re-
sulting change in accuracy. Note that the word in-
cluded was labeled as both utilities and features, and
that * denotes a regular expression feature.
nized using a “grid” interface. The results support
the findings of the simulated experiments and pro-
vide evidence that the “grid” interface can facili-
tate more efficient annotation.
</bodyText>
<sectionHeader confidence="0.958207" genericHeader="introduction">
2 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.999411888888889">
In this section we describe the underlying proba-
bilistic model for all methods in this paper. We
focus on sequence labeling, though the described
methods could be applied to other structured out-
put or classification tasks. We model the proba-
bility of the label sequence y E Yn conditioned
on the input sequence x E Xn, p(y|x; θ) using
first-order linear-chain conditional random fields
(CRFs) (Lafferty et al., 2001). This probability is
</bodyText>
<equation confidence="0.997602">
p(y|x; θ) = 1Z exp ( θjfj(yi, yi+1, x, i)),
x i j
</equation>
<bodyText confidence="0.999302333333333">
where Zx is the partition function and feature
functions fj consider the entire input sequence
and at most two consecutive output variables.
The most probable output sequence and transition
marginal distributions can be computed using vari-
ants of Viterbi and forward-backward.
Provided a training data distribution ˜p, we es-
timate CRF parameters by maximizing the condi-
tional log likelihood of the training data.
</bodyText>
<equation confidence="0.961756">
L(θ) = E˜p(x,y)[log p(y|x; θ)]
</equation>
<bodyText confidence="0.9999385">
We use numerical optimization to maximize L(θ),
which requires the gradient of L(θ) with respect
to the parameters. It can be shown that the par-
tial derivative with respect to parameter j is equal
</bodyText>
<subsectionHeader confidence="0.999732">
2.1 Learning with missing labels
</subsectionHeader>
<bodyText confidence="0.9999645">
The training set may contain partially labeled se-
quences. Let z denote missing labels. We esti-
mate parameters with this data by maximizing the
marginal log-likelihood of the observed labels.
</bodyText>
<equation confidence="0.987011">
LMML(θ) = E˜p(x,y)[log p(y, z|x; θ)]
z
</equation>
<bodyText confidence="0.999954444444445">
We refer to this training method as maximum
marginal likelihood (MML); it has also been ex-
plored by Quattoni et al. (2007).
The gradient of LMML(θ) can also be written
as the difference of two expectations. The first is
an expectation over the empirical distribution of x
and y, and the model distribution of z. The second
is a double expectation over the empirical distribu-
tion of x and the model distribution of y and z.
</bodyText>
<equation confidence="0.966762666666667">
∂θj
∂ LMML(θ) = E˜p(x,y)[Ep(z|y,x;θ)[Fj(y, z, x)]]
− E˜p(x)[Ep(y,z|x;θ)[Fj(y, z, x)]].
</equation>
<bodyText confidence="0.99987675">
We train models using LMML(θ) with expected
gradient (Salakhutdinov et al., 2003).
To additionally leverage unlabeled data, we
compare with entropy regularization (ER). ER
adds a term to the objective function that en-
courages confident predictions on unlabeled data.
Training of linear-chain CRFs with ER is de-
scribed by Jiao et al. (2006).
</bodyText>
<sectionHeader confidence="0.997521" genericHeader="method">
3 Generalized Expectation Criteria
</sectionHeader>
<bodyText confidence="0.998103142857143">
In this section, we give a brief overview of gen-
eralized expectation criteria (GE) (Mann and Mc-
Callum, 2008; Druck et al., 2008) and explain how
we can use GE to learn CRF parameters with esti-
mates of feature expectations and unlabeled data.
GE criteria are terms in a parameter estimation
objective function that express preferences on the
</bodyText>
<footnote confidence="0.735038">
210 is a default value that works well in many settings.
</footnote>
<page confidence="0.999094">
82
</page>
<bodyText confidence="0.999417105263158">
value of a model expectation of some function.
Given a score function S, an empirical distribution
˜p(x), a model distribution p(yJx; θ), and a con-
straint function Gk(x, y), the value of a GE crite-
rion is 9(θ) = S(E˜p(x)[Ep(y|x;θ)[Gk(x,y)]]).
GE provides a flexible framework for parameter
estimation because each of these elements can take
an arbitrary form. The most important difference
between GE and other parameter estimation meth-
ods is that it does not require a one-to-one cor-
respondence between constraint functions Gk and
model feature functions. We leverage this flexi-
bility to estimate parameters of feature-rich CRFs
with a very small set of expectation constraints.
Constraint functions Gk can be normalized so
that the sum of the expectations of a set of func-
tions is 1. In this case, S may measure the di-
vergence between the expectation of the constraint
function and a target expectation
</bodyText>
<equation confidence="0.999258">
9(θ) = ˆGk log(E[Gk(x, y)]), (1)
</equation>
<bodyText confidence="0.9953804">
where E[Gk(x, y)] = E˜p(x)[Ep(y|x;θ)[Gk(x, y)]].
It can be shown that the partial derivative of
9(θ) with respect to parameter j is proportional to
the predicted covariance between the model fea-
ture function Fj and the constraint function Gk.3
</bodyText>
<equation confidence="0.997375">
(E˜p(x) [Ep(y|x;θ)[Fj(x, y)Gk(x, y)]
− Ep(y|x;θ)[Fj(x, y)]Ep(y|x;θ)[Gk(x, y)]]I
</equation>
<bodyText confidence="0.999993142857143">
The partial derivative shows that GE learns pa-
rameter values for model feature functions based
on their predicted covariance with the constraint
functions. GE can thus be interpreted as a boot-
strapping method that uses the limited training sig-
nal to learn about parameters for related model
feature functions.
</bodyText>
<subsectionHeader confidence="0.999981">
3.1 Learning with feature-label distributions
</subsectionHeader>
<bodyText confidence="0.997163285714286">
Mann and McCallum (2008) apply GE to a linear-
chain, first-order CRF. In this section we provide
an alternate treatment that arrives at the same ob-
jective function from the general form described
in the previous section.
Often, feature functions in a first-order linear-
chain CRF f are binary, and are the conjunction
</bodyText>
<footnote confidence="0.87705">
3If we use squared error for S, the partial derivative is the
covariance multiplied by 2( ˆGk − E[Gk(x, y)]).
</footnote>
<bodyText confidence="0.546846">
of an observational test q(x, i) and a label pair test
1{yi=y0,yi+1=y00}.4
</bodyText>
<equation confidence="0.7688">
f(yi, yi+1, x, i) = 1{yi=y0,yi+1=y00}q(x, i)
</equation>
<bodyText confidence="0.999888090909091">
The constraint functions Gk we use here decom-
pose and operate similarly, except that they only
include a test for a single label. Single label con-
straints are easier for users to estimate and make
GE training more efficient. Label transition struc-
ture can be learned automatically from single la-
bel constraints through the covariance-based pa-
rameter update of Equation 2. For convenience,
we can write Gyk to denote the constraint func-
tion that combines observation test k with a test
for label y. We also add a normalization constant
</bodyText>
<equation confidence="0.966012666666667">
Ck = E˜p(x)[Ei qk(x, i)],
i Ck
1 1{yi=y}qk(x,i)
</equation>
<bodyText confidence="0.999887068965517">
Under this construction the expectation of Gyk is
the predicted conditional probability that the label
at some arbitrary position i is y when the observa-
tional test at i succeeds, ˜p(yi =yJqk(x, i)=1; θ).
If we have a set of constraint functions {Gyk :
y E Y}, and we use the score function in Equa-
tion 1, then the GE objective function specifies the
minimization of the KL divergence between the
model and target distributions over labels condi-
tioned on the success of the observational test. In
general the objective function will consist of many
such KL divergence penalties.
Computing the first term of the covariance in
Equation 2 requires a marginal distribution over
three labels, two of which will be consecutive, but
the other of which could appear anywhere in the
sequence. We can compute this marginal using
the algorithm of Mann and McCallum (2008). As
previously described, this algorithm is O(nJYJ3)
for a sequence of length n. However, we make
the following novel observation: we do not need
to compute the extra lattices for feature label pairs
with ˆGyk = 0, since this makes Equation 2 equal
to zero. In Mann and McCallum (2008), probabil-
ities were smoothed so that by ˆGyk &gt; 0. If we
assume that only a small number of labels m have
non-zero probability, then the time complexity of
the gradient computation is O(nmJYJ2). In this
paper typically 1 GmG 4, while JYJ is 11 or 13.
</bodyText>
<footnote confidence="0.90355">
4We this notation for an indicator function that returns 1
if the condition in braces is satisfied, and 0 otherwise.
</footnote>
<equation confidence="0.9988962">
∂ ∂θ 9(θ) =
7
E [Gk (x, y)] (2)
ˆGk x
ˆGk. Gyk(x, y) = �
</equation>
<page confidence="0.968214">
83
</page>
<bodyText confidence="0.999921777777778">
In experiments in this paper, using this optimiza-
tion does not significantly affect final accuracy.
We use numerical optimization to estimate
model parameters. In general GE objective func-
tions are not convex. Consequently, we initial-
ize 0th-order CRF parameters using a sliding win-
dow logistic regression model trained with GE.
We also include a Gaussian prior on parameters
with Q2 = 10 in the objective function.
</bodyText>
<subsectionHeader confidence="0.999984">
3.2 Learning with labeled features
</subsectionHeader>
<bodyText confidence="0.999995166666667">
The training procedure described above requires
a set of observational tests or input features with
target distributions over labels. Estimating a dis-
tribution could be a difficult task for an annotator.
Consequently, we abstract away from specifying
a distribution by allowing the user to assign labels
to features (c.f. Haghighi and Klein (2006) , Druck
et al. (2008)). For example, we say that the word
feature call has label contact. A label for a feature
simply indicates that the feature is a good indicator
of the label. Note that features can have multiple
labels, as does included in the active learning ses-
sion shown in Table 1. We convert an input feature
with a set of labels L into a distribution by assign-
ing probability 1/|L |for each l ∈ L and probabil-
ity 0 for each l ∈/ L. By assigning 0 probability to
labels l ∈/ L, we can use the speed-up described in
the previous section.
</bodyText>
<subsectionHeader confidence="0.700541">
3.3 Related Work
</subsectionHeader>
<bodyText confidence="0.999935513513514">
Other proposed learning methods use labeled fea-
tures to label unlabeled data. The resulting
partially-labeled corpus can be used to train a CRF
by maximizing MML. Similarly, prototype-driven
learning (PDL) (Haghighi and Klein, 2006) opti-
mizes the joint marginal likelihood of data labeled
with prototype input features for each label. Ad-
ditional features that indicate similarity to the pro-
totypes help the model to generalize. In a previ-
ous comparison between GE and PDL (Mann and
McCallum, 2008), GE outperformed PDL without
the extra similarity features, whose construction
may be problem-specific. GE also performed bet-
ter when supplied accurate label distributions.
Additionally, both MML and PDL do not natu-
rally generalize to learning with features that have
multiple labels or distributions over labels, as in
these scenarios labeling the unlabeled data is not
straightforward. In this paper, we attempt to ad-
dress this problem using a simple heuristic: when
there are multiple choices for a token’s label, sam-
ple a label. In Section 5 we use this heuristic with
MML, but in general obtain poor results.
Raghavan and Allan (2007) also propose sev-
eral methods for learning with labeled features,
but in a previous comparison GE gave better re-
sults (Druck et al., 2008). Additionally, the gen-
eralization of these methods to structured output
spaces is not straightforward. Chang et al. (2007)
present an algorithm for learning with constraints,
but this method requires users to set weights by
hand. We plan to explore the use of the recently
developed related methods of Bellare et al. (2009),
Grac¸a et al. (2008), and Liang et al. (2009) in fu-
ture work. Druck et al. (2008) provide a survey
of other related methods for learning with labeled
input features.
</bodyText>
<sectionHeader confidence="0.991555" genericHeader="method">
4 Active Learning by Labeling Features
</sectionHeader>
<bodyText confidence="0.998487045454545">
Feature active learning, presented in Algorithm 1,
is a pool-based active learning algorithm (Lewis
and Gale, 1994) (with a pool of features rather
than instances). The novel components of the
algorithm are an option to skip a query and the
notion that skipping and labeling have different
costs. The option to skip is important when us-
ing feature queries because a user may not know
how to label some features. In each iteration the
model is retrained using the train procedure, which
takes as input a set of labeled features C and un-
labeled data distribution ˜p. For the reasons de-
scribed in Section 3.3, we advocate using GE for
the train procedure. Then, while the iteration cost
c is less than the maximum cost cmax, the feature
query q that maximizes the query selection met-
ric 0 is selected. The accept function determines
whether the labeler will label q. If q is labeled, it
is added to the set of labeled features C, and the
label cost clabel is added to c. Otherwise, the skip
cost cskip is added to c. This process continues for
N iterations.
</bodyText>
<subsectionHeader confidence="0.995863">
4.1 Feature query selection methods
</subsectionHeader>
<bodyText confidence="0.999973333333333">
In this section we propose feature query selection
methods 0. Queries with a higher scores are con-
sidered better candidates. Note again that by fea-
tures we mean observational tests qk(x, i). It is
also important to note these are not feature selec-
tion methods since we are determining the features
for which supervisory feedback will be most help-
ful to the model, rather than determining which
features will be part of the model.
</bodyText>
<page confidence="0.994614">
84
</page>
<table confidence="0.8665312">
Algorithm 1 Feature Active Learning
Input: empirical distribution ˜p, initial feature constraints
C, label cost clabel, skip cost cskip, max cost per iteration
cmax, max iterations N
Output: model parameters 0
</table>
<equation confidence="0.880269428571428">
for i = 1 to N do
0 = train(˜p, C)
c = 0
while c &lt; cmax do
q = argmaxqk O(qk)
if accept(q) then
C = C U label(q)
c = c + clabel
else
c = c + cskip
end if
end while
end for
0 = train(˜p, C)
</equation>
<bodyText confidence="0.999929833333333">
We propose to select queries that provide the
largest reduction in model uncertainty. We notate
possible responses to a query qk as ˆg. The Ex-
pected Information Gain (EIG) of a query is the
expectation of the reduction in model uncertainty
over all possible responses. Mathematically, IG is
</bodyText>
<equation confidence="0.975782">
φEIG(qk) = Ep(ˆg|qk;θ)[E˜p(x)[H(p(y|x; θ)−
H(p(y|x; θˆg)]],
</equation>
<bodyText confidence="0.999983176470589">
where θˆg are the new model parameters if the re-
sponse to qk is ˆg. Unfortunately, this method is
computationally intractable. Re-estimating θˆg will
typically involve retraining the model, and do-
ing this for each possible query-response pair is
prohibitively expensive for structured output mod-
els. Computing the expectation over possible re-
sponses is also difficult, as in this paper users may
provide a set of labels for a query, and more gen-
erally gˆ could be a distribution over labels.
Instead, we propose a tractable strategy for re-
ducing model uncertainty, motivated by traditional
uncertainty sampling (Lewis and Gale, 1994). We
assume that when a user responds to a query, the
reduction in uncertainty will be equal to the To-
tal Uncertainty (TU), the sum of the marginal en-
tropies at the positions where the feature occurs.
</bodyText>
<equation confidence="0.9767375">
φTU(qk) = 1: 1: qk(xi, j)H(p(yj|xi; θ))
i j
</equation>
<bodyText confidence="0.99979375">
Total uncertainty, however, is highly biased to-
wards selecting frequent features. A mean un-
certainty variant, normalized by the feature’s
count, would tend to choose very infrequent fea-
tures. Consequently we propose a tradeoff be-
tween the two extremes, called weighted uncer-
tainty (WU), that scales the mean uncertainty by
the log count of the feature in the corpus.
</bodyText>
<equation confidence="0.947212">
φWU(qk) = log(Ck)φTU(qk)
</equation>
<bodyText confidence="0.999700285714286">
Finally, we also suggest an uncertainty-based met-
ric called diverse uncertainty (DU) that encour-
ages diversity among queries by multiplying TU
by the mean dissimilarity between the feature and
previously labeled features. For sequence labeling
tasks, we can measure the relatedness of features
using distributional similarity.5
</bodyText>
<equation confidence="0.981785333333333">
1:
φDU(qk) = φTU(qk) 1 1−sim(qk, qj)
|S |jEC
</equation>
<bodyText confidence="0.9994466">
We contrast the notion of uncertainty described
above with another type of uncertainty: the en-
tropy of the predicted label distribution for the fea-
ture, or expectation uncertainty (EU). As above
we also multiply by the log feature count.
</bodyText>
<equation confidence="0.971463">
φEU(qk) = log(Ck)H(˜p(yi = y|qk(x, i)=1; θ))
</equation>
<bodyText confidence="0.999795923076923">
EU is flawed because it will have a large value for
non-discriminative features.
The methods described above require the model
to be retrained between iterations. To verify that
this is necessary, we compare against query selec-
tion methods that only consider the previously la-
beled features. First, we consider a feature query
selection method called coverage (cov) that aims
to select features that are dissimilar from existing
labeled features, increasing the labeled features’
“coverage” of the feature space. In order to com-
pensate for choosing very infrequent features, we
multiply by the log count of the feature.
</bodyText>
<equation confidence="0.980733666666667">
1:
φcov(qk) = log(Ck) 1 1 − sim(qk, qj)
|S |jEC
</equation>
<bodyText confidence="0.999901666666667">
Motivated by the feature query selection method
of Tandem Learning (Raghavan and Allan, 2007)
(see Section 4.2 for further discussion), we con-
sider a feature selection metric similarity (sim)
that is the maximum similarity to a labeled fea-
ture, weighted by the log count of the feature.
</bodyText>
<equation confidence="0.746897166666667">
φsim(qk) = log(Ck) max
jEC
5sim(qk, qj) returns the cosine similarity between context
vectors of words occurring in a window of f3.
Ck .
sim(qk, qj)
</equation>
<page confidence="0.993246">
85
</page>
<bodyText confidence="0.9979708">
Features similar to those already labeled are likely
to be discriminative, and therefore likely to be la-
beled (rather than skipped). However, insufficient
diversity may also result in an inaccurate model,
suggesting that coverage should select more use-
ful queries than similarity.
Finally, we compare with several passive base-
lines. Random (rand) assigns scores to features
randomly. Frequency (freq) scores input features
using their frequency in the training data.
</bodyText>
<equation confidence="0.9906425">
Ofreq(qk) = � � qk(Xi,j)
i j
</equation>
<bodyText confidence="0.997288857142857">
Top LDA (LDA) selects the top words from 50
topics learned from the unlabeled data using la-
tent Dirichlet allocation (LDA) (Blei et al., 2003).
More specifically, the words w generated by each
topic t are ranked using the conditional probability
p(wIt). The word feature is assigned its maximum
rank across all topics.
</bodyText>
<equation confidence="0.9992475">
OLDA(qk) = max
t
</equation>
<bodyText confidence="0.999330333333333">
This method will select useful features if the top-
ics discovered are relevant to the task. A similar
heuristic was used by Druck et al. (2008).
</bodyText>
<sectionHeader confidence="0.818358" genericHeader="method">
4.2 Related Work
</sectionHeader>
<bodyText confidence="0.998823421052632">
Tandem Learning (Raghavan and Allan, 2007) is
an algorithm that combines feature and instance
active learning for classification. The algorithm it-
eratively queries the user first for instance labels,
then for feature labels. Feature queries are selected
according to their co-occurrence with important
model features and previously labeled features. As
noted in Section 3.3, GE is preferable to the meth-
ods Tandem Learning uses to learn with labeled
features. We address the mixing of feature and in-
stance queries in Section 4.3.
In order to better understand differences in fea-
ture query selection methodology, we proposed a
feature query selection method motivated6 by the
method used in Tandem Learning in Section 4.1.
However, this method performs poorly in the ex-
periments in Section 5.
Liang et al. (2009) simultaneously developed
a method for learning with and actively selecting
</bodyText>
<footnote confidence="0.98004425">
6The query selection method of Raghavan and Allan
(2007) requires a stack that is modified between queries
within each iteration. Here query scores are only updated
after each iteration of labeling.
</footnote>
<bodyText confidence="0.999950592592593">
measurements, or target expectations with associ-
ated noise. The measurement selection method
proposed by Liang et al. (2009) is based on
Bayesian experimental design and is similar to
the expected information gain method described
above. Consequently this method is likely to be
intractable for real applications. Note that Liang
et al. (2009) only use this method in synthetic ex-
periments, and instead use a method similar to to-
tal uncertainty for experiments in part-of-speech
tagging. Unlike the experiments presented in this
paper, Liang et al. (2009) conduct only simulated
active learning experiments and do not consider
skipping queries.
Sindhwani (Sindhwani et al., 2009) simultane-
ously developed an active learning method that
queries for both instance and feature labels that
are then used in a graph-based learning algorithm.
They find that querying certain features outper-
forms querying uncertain features, but this is likely
because their query selection method is similar
to the expectation uncertainty method described
above, and consequently non-discriminative fea-
tures may be queried often (see also the discus-
sion in Section 4.1). It is also not clear how this
graph-based training method would generalize to
structured output spaces.
</bodyText>
<subsectionHeader confidence="0.995189">
4.3 Expectation Constraint Active Learning
</subsectionHeader>
<bodyText confidence="0.999974142857143">
Throughout this paper, we have focussed on label-
ing input features. However, the proposed meth-
ods generalize to queries for expectation estimates
of arbitrary functions, for example queries for the
label distributions for input features, labels for in-
stances (using a function that is non-zero only for
a particular instance), partial labels for instances,
and class priors. The uncertainty-based query se-
lection methods described in Section 4.1 apply
naturally to these new query types. Importantly
this framework would allow principled mixing of
different query types, instead of alternating be-
tween them as in Tandem Learning (Raghavan and
Allan, 2007). When mixing queries, it will be
important to use different costs for different an-
notation types (Vijayanarasimhan and Grauman,
2008), and estimate the probability of obtaining a
useful response to a query. We plan to pursue these
directions in future work. This idea was also pro-
posed by Liang et al. (2009), but no experiments
with mixed active learning were presented.
</bodyText>
<equation confidence="0.545948">
rankLDA(qk, t)
</equation>
<page confidence="0.994631">
86
</page>
<sectionHeader confidence="0.989765" genericHeader="method">
5 Simulated User Experiments
</sectionHeader>
<bodyText confidence="0.99409393877551">
In this section we experiment with an automated
oracle labeler. When presented an instance query,
the oracle simply provides the true labels. When
presented a feature query, the oracle first decides
whether to skip the query. We have found that
users are more likely to label features that are rel-
evant for only a few labels. Therefore, the oracle
labels a feature if the entropy of its per occurrence
label expectation, H(˜p(yi = y|qk(x, i) = 1; B)) ≤
0.7. The oracle then labels the feature using a
heuristic: label the feature with the label whose
expectation is highest, as well as any label whose
expectation is at least half as large.
We estimate the effort of different labeling ac-
tions with preliminary experiments in which we
observe users labeling data for ten minutes. Users
took an average of 4 seconds to label a feature, 2
seconds to skip a feature, and 0.7 seconds to la-
bel a token. We setup experiments such that each
iteration simulates one minute of labeling by set-
ting cmax = 60, cskip = 2 and clabel = 4. For
instance active learning, we use Algorithm 1 but
without the skip option, and set clabel = 0.7. We
use N = 10 iterations, so the entire experiment
simulates 10 minutes of annotation time. For ef-
ficiency, we consider the 500 most frequent unla-
beled features in each iteration. To start, ten ran-
domly selected seed labeled features are provided.
We use random (rand) selection, uncertainty
sampling (US) (using sequence entropy, normal-
ized by sequence length) and information den-
sity (ID) (Settles and Craven, 2008) to select in-
stance queries. We use Entropy Regularization
(ER) (Jiao et al., 2006) to leverage unlabeled in-
stances.7 We weight the ER term by choosing the
best8 weight in {10−3,10−2,10−1,1,10} multi-
plied by #labeled
#unlabeled for each data set and query se-
lection method. Seed instances are provided such
that the simulated labeling time is equivalent to la-
beling 10 features.
We evaluate on two sequence labeling tasks.
The apartments task involves segmenting 300
apartment classified ads into 11 fields including
features, rent, neighborhood, and contact. We use
the same feature processing as Haghighi and Klein
(2006), with the addition of context features in a
window of f3. The cora references task is to ex-
tract 13 BibTeX fields such as author and booktitle
</bodyText>
<footnote confidence="0.9906805">
7Results using self-training instead of ER are similar.
8As measured by test accuracy, giving ER an advantage.
</footnote>
<table confidence="0.9986530625">
method apartments cora
mean final mean final
ER rand 48.1 53.6 75.9 81.1
ER US 51.7 57.9 76.0 83.2
ER ID 51.4 56.9 75.9 83.1
MML rand 47.7 51.2 58.6 64.6
MML WU 57.6 60.8 61.0 66.2
GE rand 59.0 64.8* 77.6 83.7
GE freq 66.5* 71.6* 68.6 79.8
GE LDA 65.7* 71.4* 74.9 85.0
GE cov 68.2*† 72.6* 73.5 83.3
GE sim 57.8 65.9* 67.1 79.2
GE EU 66.5* 71.6* 68.6 79.8
GE TU 70.1*† 73.6*† 76.9 88.2*†
GE WU 71.6*† 74.6*† 80.3*† 88.1*†
GE DU 70.5*† 74.4*† 78.4* 87.5*†
</table>
<tableCaption confidence="0.990715">
Table 2: Mean and final token accuracy results.
</tableCaption>
<bodyText confidence="0.920678777777778">
A * or † denotes that a GE method significantly
outperforms all non-GE or passive GE methods,
respectively. Bold entries significantly outperform
all others. Methods in italics are passive.
from 500 research paper references. We use a stan-
dard set of word, regular expressions, and lexicon
features, as well as context features in a window
of f3. All results are averaged over ten random
80:20 splits of the data.
</bodyText>
<subsectionHeader confidence="0.612974">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.99990565">
Table 2 presents mean (across all iterations) and
final token accuracy results. On the apartments
task, GE methods greatly outperform MML9 and
ER methods. Each uncertainty-based GE method
also outperforms all passive GE methods. On the
cora task, only GE with weighted uncertainty sig-
nificantly outperforms ER and passive GE meth-
ods in terms of mean accuracy, but all uncertainty-
based GE methods provide higher final accuracy.
This suggests that on the cora task, active GE
methods are performing better in later iterations.
Figure 1, which compares the learning curves of
the best performing methods of each type, shows
this phenomenon. Further analysis reveals that the
uncertainty-based methods are choosing frequent
features that are more likely to be skipped than
those selected randomly in early iterations.
We next compare with the results of related
methods published elsewhere. We cannot make
claims about statistical significance, but the results
</bodyText>
<footnote confidence="0.928295">
9Only the best MML results are shown.
</footnote>
<page confidence="0.999308">
87
</page>
<bodyText confidence="0.9999685">
illustrate the competitiveness of our method. The
74.6% final accuracy on apartments is higher than
any result obtained by Haghighi and Klein (2006)
(the highest is 74.1%), higher than the supervised
HMM results reported by Grenager et al. (2005)
(74.4%), and matches the results of Mann and Mc-
Callum (2008) with GE with more accurate sam-
pled label distributions and 10 labeled examples.
Chang et al. (2007) only obtain better results than
88.2% on cora when using 300 labeled examples
(two hours of estimated annotation time), 5000 ad-
ditional unlabeled examples, and extra test time in-
ference constraints. Note that obtaining these re-
sults required only 10 simulated minutes of anno-
tation time, and that GE methods are provided no
information about the label transition matrix.
</bodyText>
<sectionHeader confidence="0.989513" genericHeader="method">
6 User Experiments
</sectionHeader>
<bodyText confidence="0.999943">
Another advantage of feature queries is that fea-
ture names are concise enough to be browsed,
rather than considered individually. This allows
the design of improved interfaces that can further
increase the speed of feature active learning. We
built a prototype interface that allows the user to
quickly browse many candidate features. The fea-
tures are split into groups of five features each.
Each group contains features that are related, as
measured by distributional similarity. The features
within each group are sorted according to the ac-
tive learning metric. This interface, displayed in
Figure 3, may be useful because features in the
same group are likely to have the same label.
We conduct three types of experiments. First, a
user labels instances selected by information den-
sity, and models are trained using ER. The in-
stance labeling interface allows the user to label
tokens quickly by extending the current selection
one token at a time and only requiring a single
keystroke to label an entire segment. Second,
the user labels features presented one-at-a-time by
weighted uncertainty, and models are trained us-
ing GE. To aid the user in understanding the func-
tion of the feature quickly, we provide several ex-
amples of the feature occurring in context and the
model’s current predicted label distribution for the
feature. Finally, the user labels features organized
using the grid interface described in the previous
paragraph. Weighted uncertainty is used to sort
feature queries within each group, and GE is used
to train models. Each iteration of labeling lasts
two minutes, and there are five iterations. Retrain-
ing with ER between iterations takes an average
of 5 minutes on cora and 3 minutes on apart-
ments. With GE, the retraining times are on av-
erage 6 minutes on cora and 4 minutes on apart-
ments. Consequently, even when viewed with to-
tal time, rather than annotation time, feature active
learning is beneficial. While waiting for models to
retrain, users can perform other tasks.
Figure 2 displays the results. User 1 labeled
apartments data, while Users 2 and 3 labeled cora
data. User 1 was able to obtain much better results
with feature labeling than with instance labeling,
but performed slightly worse with the grid inter-
face than with the serial interface. User 1 com-
mented that they found the label definitions for
apartments to be imprecise, so the other experi-
ments were conducted on the cora data. User 2
obtained better results with feature labeling than
instance labeling, and obtained higher mean ac-
curacy with the grid interface. User 3 was much
better at labeling features than instances, and per-
formed especially well using the grid interface.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999876875">
We proposed an active learning approach in which
features, rather than instances, are labeled. We
presented an algorithm for active learning with
features and several feature query selection meth-
ods that approximate the expected reduction in
model uncertainty of a feature query. In simu-
lated experiments, active learning with features
outperformed passive learning with features, and
uncertainty-based feature query selection outper-
formed other baseline methods. In both simulated
and real user experiments, active learning with
features outperformed passive and active learning
with instances. Finally, we proposed a new label-
ing interface that leverages the conciseness of fea-
ture queries. User experiments suggested that this
grid interface can improve labeling efficiency.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999468222222222">
We thank Kedar Bellare for helpful discussions and Gau-
rav Chandalia for providing code. This work was supported
in part by the Center for Intelligent Information Retrieval
and the Central Intelligence Agency, the National Security
Agency and National Science Foundation under NSF grant
#IIS-0326249. The second author was supported by a grant
from National Human Genome Research Institute. Any opin-
ions, findings and conclusions or recommendations are the
authors’ and do not necessarily reflect those of the sponsor.
</bodyText>
<page confidence="0.99413">
88
</page>
<figure confidence="0.998389055555555">
apartments
simulated annotation time (minutes)
cora
simulated annotation time (minutes)
80
75
70
token accuracy
65
60
55
50
ER + uncertainty
MML + weighted uncertainty
GE + frequency
GE + weighted uncertainty
2 4 6 8 10
45
40
35
90
85
80
token accuracy
65
60
45
2 4 6 8 10
ER + uncertainty
MML + weighted uncertainty
GE + random
GE + weighted uncertainty
75
70
55
50
</figure>
<figureCaption confidence="0.99907">
Figure 1: Token accuracy vs. time for best performing ER, MML, passive GE, and active GE methods.
</figureCaption>
<figure confidence="0.996316914893617">
user 1 − apartments user 2 − cora user 3 − cora
65
70
60
65
55
60
50
55
45
40
50
35
45
30
40
token accuracy
token accuracy
token accuracy
25
20
35
30
10
ER + information density
GE + weighted uncertainty (serial)
GE + weighted uncertainty (grid)
2 4 6 8
10
85
80
75
70
65
60
55
50
ER + information density
40 GE + weighted uncertainty (serial)
GE + weighted uncertainty (grid)
352 4 6 8 10
45
15 ER + information density
GE + weighted uncertainty (serial)
GE + weighted uncertainty (grid)
52 4 6 8 10
annotation time (minutes) annotation time (minutes) annotation time (minutes)
</figure>
<figureCaption confidence="0.98946775">
Figure 2: User experiments with instance labeling and feature labeling with the serial and grid interfaces.
Figure 3: Grid feature labeling interface. Boxes on the left contain groups of features that appear in
similar contexts. Features in the same group often receive the same label. On the right, the model’s
current expectation and occurrences of the selected feature in context are displayed.
</figureCaption>
<page confidence="0.996716">
89
</page>
<bodyText confidence="0.853616">
Ruslan Salakhutdinov, Sam Roweis, and Zoubin
Ghahramani. 2003. Optimization with em and
expectation-conjugate-gradient. In ICML, pages
672–679.
</bodyText>
<sectionHeader confidence="0.878403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995412661290323">
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning with
expectation constraints. In UAI.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
Journal of Machine Learning Research, 3:2003.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL, pages 280–287.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In SIGIR.
Joao Grac¸a, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20. MIT Press.
Trond Grenager, Dan Klein, and Christopher D. Man-
ning. 2005. Unsupervised learning of field segmen-
tation models for information extraction. In ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In HTL-NAACL.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In ACL, pages
209–216.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
David D. Lewis and William A. Gale. 1994. A sequen-
tial algorithm for training text classifiers. In SIGIR,
pages 3–12, New York, NY, USA. Springer-Verlag
New York, Inc.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In ICML.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In ACL.
A. Quattoni, S. Wang, L.-P Morency, M. Collins, and
T. Darrell. 2007. Hidden conditional random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 29:1848–1852, October.
Hema Raghavan and James Allan. 2007. An interac-
tive algorithm for asking and incorporating feature
feedback into support vector machines. In SIGIR,
pages 79–86.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP.
Burr Settles. 2009. Active learning literature survey.
Technical Report 1648, University of Wisconsin -
Madison.
Vikas Sindhwani, Prem Melville, and Richard D.
Lawrence. 2009. Uncertainty sampling and trans-
ductive experimental design for active dual supervi-
sion. In ICML.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2008. Multi-level active prediction of useful image
annotations for recognition. In NIPS.
</reference>
<page confidence="0.99864">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.419008">
<title confidence="0.999973">Active Learning by Labeling Features</title>
<author confidence="0.999922">Gregory Burr Andrew</author>
<affiliation confidence="0.99997">Dept. of Computer Dept. of Biostatistics Dept. of Computer University of Massachusetts Medical University of Massachusetts</affiliation>
<address confidence="0.999898">Amherst, MA Dept. of Computer Amherst, MA 01003</address>
<email confidence="0.813684">gdruck@cs.umass.eduUniversityofmccallum@cs.umass.edu</email>
<author confidence="0.449469">WI Madison</author>
<email confidence="0.999281">bsettles@cs.wisc.edu</email>
<abstract confidence="0.997530058823529">Methods that learn from prior informaabout input features such as generalexpectation (GE) been used to train accurate models with very little ef- In this paper, we propose an aclearning in which the masolicits “labels” on than instances. In both simulated and real user experiments on two sequence labeling tasks we show that our active learning method outperforms passive learning with features as well as traditional active learning with instances. Preliminary experiments suggest that novel interfaces which intelligently solicit labels on multiple features facilitate more efficient annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Gregory Druck</author>
<author>Andrew McCallum</author>
</authors>
<title>Alternating projections for learning with expectation constraints.</title>
<date>2009</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="15292" citStr="Bellare et al. (2009)" startWordPosition="2482" endWordPosition="2485">ultiple choices for a token’s label, sample a label. In Section 5 we use this heuristic with MML, but in general obtain poor results. Raghavan and Allan (2007) also propose several methods for learning with labeled features, but in a previous comparison GE gave better results (Druck et al., 2008). Additionally, the generalization of these methods to structured output spaces is not straightforward. Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by hand. We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Grac¸a et al. (2008), and Liang et al. (2009) in future work. Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. 4 Active Learning by Labeling Features Feature active learning, presented in Algorithm 1, is a pool-based active learning algorithm (Lewis and Gale, 1994) (with a pool of features rather than instances). The novel components of the algorithm are an option to skip a query and the notion that skipping and labeling have different costs. The option to skip is important when using feature queries because a user may not know how to la</context>
</contexts>
<marker>Bellare, Druck, McCallum, 2009</marker>
<rawString>Kedar Bellare, Gregory Druck, and Andrew McCallum. 2009. Alternating projections for learning with expectation constraints. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>John Lafferty</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--2003</pages>
<contexts>
<context position="21444" citStr="Blei et al., 2003" startWordPosition="3516" endWordPosition="3519">those already labeled are likely to be discriminative, and therefore likely to be labeled (rather than skipped). However, insufficient diversity may also result in an inaccurate model, suggesting that coverage should select more useful queries than similarity. Finally, we compare with several passive baselines. Random (rand) assigns scores to features randomly. Frequency (freq) scores input features using their frequency in the training data. Ofreq(qk) = � � qk(Xi,j) i j Top LDA (LDA) selects the top words from 50 topics learned from the unlabeled data using latent Dirichlet allocation (LDA) (Blei et al., 2003). More specifically, the words w generated by each topic t are ranked using the conditional probability p(wIt). The word feature is assigned its maximum rank across all topics. OLDA(qk) = max t This method will select useful features if the topics discovered are relevant to the task. A similar heuristic was used by Druck et al. (2008). 4.2 Related Work Tandem Learning (Raghavan and Allan, 2007) is an algorithm that combines feature and instance active learning for classification. The algorithm iteratively queries the user first for instance labels, then for feature labels. Feature queries are </context>
</contexts>
<marker>Blei, Ng, Jordan, Lafferty, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Guiding semi-supervision with constraint-driven learning.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>280--287</pages>
<contexts>
<context position="15091" citStr="Chang et al. (2007)" startWordPosition="2448" endWordPosition="2451">ls or distributions over labels, as in these scenarios labeling the unlabeled data is not straightforward. In this paper, we attempt to address this problem using a simple heuristic: when there are multiple choices for a token’s label, sample a label. In Section 5 we use this heuristic with MML, but in general obtain poor results. Raghavan and Allan (2007) also propose several methods for learning with labeled features, but in a previous comparison GE gave better results (Druck et al., 2008). Additionally, the generalization of these methods to structured output spaces is not straightforward. Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by hand. We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Grac¸a et al. (2008), and Liang et al. (2009) in future work. Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. 4 Active Learning by Labeling Features Feature active learning, presented in Algorithm 1, is a pool-based active learning algorithm (Lewis and Gale, 1994) (with a pool of features rather than instances). The novel components of the</context>
<context position="30009" citStr="Chang et al. (2007)" startWordPosition="4910" endWordPosition="4913">ndomly in early iterations. We next compare with the results of related methods published elsewhere. We cannot make claims about statistical significance, but the results 9Only the best MML results are shown. 87 illustrate the competitiveness of our method. The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al. (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. Chang et al. (2007) only obtain better results than 88.2% on cora when using 300 labeled examples (two hours of estimated annotation time), 5000 additional unlabeled examples, and extra test time inference constraints. Note that obtaining these results required only 10 simulated minutes of annotation time, and that GE methods are provided no information about the label transition matrix. 6 User Experiments Another advantage of feature queries is that feature names are concise enough to be browsed, rather than considered individually. This allows the design of improved interfaces that can further increase the spe</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. Guiding semi-supervision with constraint-driven learning. In ACL, pages 280–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="7736" citStr="Druck et al., 2008" startWordPosition="1219" endWordPosition="1222">el distribution of y and z. ∂θj ∂ LMML(θ) = E˜p(x,y)[Ep(z|y,x;θ)[Fj(y, z, x)]] − E˜p(x)[Ep(y,z|x;θ)[Fj(y, z, x)]]. We train models using LMML(θ) with expected gradient (Salakhutdinov et al., 2003). To additionally leverage unlabeled data, we compare with entropy regularization (ER). ER adds a term to the objective function that encourages confident predictions on unlabeled data. Training of linear-chain CRFs with ER is described by Jiao et al. (2006). 3 Generalized Expectation Criteria In this section, we give a brief overview of generalized expectation criteria (GE) (Mann and McCallum, 2008; Druck et al., 2008) and explain how we can use GE to learn CRF parameters with estimates of feature expectations and unlabeled data. GE criteria are terms in a parameter estimation objective function that express preferences on the 210 is a default value that works well in many settings. 82 value of a model expectation of some function. Given a score function S, an empirical distribution ˜p(x), a model distribution p(yJx; θ), and a constraint function Gk(x, y), the value of a GE criterion is 9(θ) = S(E˜p(x)[Ep(y|x;θ)[Gk(x,y)]]). GE provides a flexible framework for parameter estimation because each of these elem</context>
<context position="13150" citStr="Druck et al. (2008)" startWordPosition="2122" endWordPosition="2125"> convex. Consequently, we initialize 0th-order CRF parameters using a sliding window logistic regression model trained with GE. We also include a Gaussian prior on parameters with Q2 = 10 in the objective function. 3.2 Learning with labeled features The training procedure described above requires a set of observational tests or input features with target distributions over labels. Estimating a distribution could be a difficult task for an annotator. Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006) , Druck et al. (2008)). For example, we say that the word feature call has label contact. A label for a feature simply indicates that the feature is a good indicator of the label. Note that features can have multiple labels, as does included in the active learning session shown in Table 1. We convert an input feature with a set of labels L into a distribution by assigning probability 1/|L |for each l ∈ L and probability 0 for each l ∈/ L. By assigning 0 probability to labels l ∈/ L, we can use the speed-up described in the previous section. 3.3 Related Work Other proposed learning methods use labeled features to l</context>
<context position="14968" citStr="Druck et al., 2008" startWordPosition="2430" endWordPosition="2433">distributions. Additionally, both MML and PDL do not naturally generalize to learning with features that have multiple labels or distributions over labels, as in these scenarios labeling the unlabeled data is not straightforward. In this paper, we attempt to address this problem using a simple heuristic: when there are multiple choices for a token’s label, sample a label. In Section 5 we use this heuristic with MML, but in general obtain poor results. Raghavan and Allan (2007) also propose several methods for learning with labeled features, but in a previous comparison GE gave better results (Druck et al., 2008). Additionally, the generalization of these methods to structured output spaces is not straightforward. Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by hand. We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Grac¸a et al. (2008), and Liang et al. (2009) in future work. Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. 4 Active Learning by Labeling Features Feature active learning, presented in Algorithm 1, is a pool-based ac</context>
<context position="21780" citStr="Druck et al. (2008)" startWordPosition="3574" endWordPosition="3577">scores to features randomly. Frequency (freq) scores input features using their frequency in the training data. Ofreq(qk) = � � qk(Xi,j) i j Top LDA (LDA) selects the top words from 50 topics learned from the unlabeled data using latent Dirichlet allocation (LDA) (Blei et al., 2003). More specifically, the words w generated by each topic t are ranked using the conditional probability p(wIt). The word feature is assigned its maximum rank across all topics. OLDA(qk) = max t This method will select useful features if the topics discovered are relevant to the task. A similar heuristic was used by Druck et al. (2008). 4.2 Related Work Tandem Learning (Raghavan and Allan, 2007) is an algorithm that combines feature and instance active learning for classification. The algorithm iteratively queries the user first for instance labels, then for feature labels. Feature queries are selected according to their co-occurrence with important model features and previously labeled features. As noted in Section 3.3, GE is preferable to the methods Tandem Learning uses to learn with labeled features. We address the mixing of feature and instance queries in Section 4.3. In order to better understand differences in featur</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>Gregory Druck, Gideon Mann, and Andrew McCallum. 2008. Learning from labeled features using generalized expectation criteria. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Grac¸a</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
</authors>
<title>Expectation maximization and posterior constraints.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20.</booktitle>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press.</publisher>
<marker>Grac¸a, Ganchev, Taskar, 2008</marker>
<rawString>Joao Grac¸a, Kuzman Ganchev, and Ben Taskar. 2008. Expectation maximization and posterior constraints. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Unsupervised learning of field segmentation models for information extraction.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="29848" citStr="Grenager et al. (2005)" startWordPosition="4882" endWordPosition="4885">his phenomenon. Further analysis reveals that the uncertainty-based methods are choosing frequent features that are more likely to be skipped than those selected randomly in early iterations. We next compare with the results of related methods published elsewhere. We cannot make claims about statistical significance, but the results 9Only the best MML results are shown. 87 illustrate the competitiveness of our method. The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al. (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. Chang et al. (2007) only obtain better results than 88.2% on cora when using 300 labeled examples (two hours of estimated annotation time), 5000 additional unlabeled examples, and extra test time inference constraints. Note that obtaining these results required only 10 simulated minutes of annotation time, and that GE methods are provided no information about the label transition matrix. 6 User Experiments Another advantage of feature queries is that fea</context>
</contexts>
<marker>Grenager, Klein, Manning, 2005</marker>
<rawString>Trond Grenager, Dan Klein, and Christopher D. Manning. 2005. Unsupervised learning of field segmentation models for information extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In HTL-NAACL.</booktitle>
<contexts>
<context position="1695" citStr="Haghighi and Klein (2006)" startWordPosition="249" endWordPosition="252">oduction The application of machine learning to new problems is slowed by the need for labeled training data. When output variables are structured, annotation can be particularly difficult and timeconsuming. For example, when training a conditional random field (Lafferty et al., 2001) to extract fields such as rent, contact, features, and utilities from apartment classifieds, labeling 22 instances (2,540 tokens) provides only 66.1% accuracy.1 Recent work has used unlabeled data and limited prior information about input features to bootstrap accurate structured output models. For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. However, these methods have only been applied in scenarios in which the user supplies such prior knowledge before learning begins. 1Averaged over 10 randomly selected sets of 22 instances. In traditional active learning (Settles, 2009), the machine queries the user for only the labels of instances that would be most helpful to the machine. This paper proposes an active learning approach in which the</context>
<context position="13128" citStr="Haghighi and Klein (2006)" startWordPosition="2117" endWordPosition="2120"> objective functions are not convex. Consequently, we initialize 0th-order CRF parameters using a sliding window logistic regression model trained with GE. We also include a Gaussian prior on parameters with Q2 = 10 in the objective function. 3.2 Learning with labeled features The training procedure described above requires a set of observational tests or input features with target distributions over labels. Estimating a distribution could be a difficult task for an annotator. Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006) , Druck et al. (2008)). For example, we say that the word feature call has label contact. A label for a feature simply indicates that the feature is a good indicator of the label. Note that features can have multiple labels, as does included in the active learning session shown in Table 1. We convert an input feature with a set of labels L into a distribution by assigning probability 1/|L |for each l ∈ L and probability 0 for each l ∈/ L. By assigning 0 probability to labels l ∈/ L, we can use the speed-up described in the previous section. 3.3 Related Work Other proposed learning methods use</context>
<context position="27421" citStr="Haghighi and Klein (2006)" startWordPosition="4474" endWordPosition="4477">8) to select instance queries. We use Entropy Regularization (ER) (Jiao et al., 2006) to leverage unlabeled instances.7 We weight the ER term by choosing the best8 weight in {10−3,10−2,10−1,1,10} multiplied by #labeled #unlabeled for each data set and query selection method. Seed instances are provided such that the simulated labeling time is equivalent to labeling 10 features. We evaluate on two sequence labeling tasks. The apartments task involves segmenting 300 apartment classified ads into 11 fields including features, rent, neighborhood, and contact. We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of f3. The cora references task is to extract 13 BibTeX fields such as author and booktitle 7Results using self-training instead of ER are similar. 8As measured by test accuracy, giving ER an advantage. method apartments cora mean final mean final ER rand 48.1 53.6 75.9 81.1 ER US 51.7 57.9 76.0 83.2 ER ID 51.4 56.9 75.9 83.1 MML rand 47.7 51.2 58.6 64.6 MML WU 57.6 60.8 61.0 66.2 GE rand 59.0 64.8* 77.6 83.7 GE freq 66.5* 71.6* 68.6 79.8 GE LDA 65.7* 71.4* 74.9 85.0 GE cov 68.2*† 72.6* 73.5 83.3 GE sim 57.8 65.9* 67.1 79.2 GE EU 66.5* 71.6* </context>
<context position="29750" citStr="Haghighi and Klein (2006)" startWordPosition="4866" endWordPosition="4869">ns. Figure 1, which compares the learning curves of the best performing methods of each type, shows this phenomenon. Further analysis reveals that the uncertainty-based methods are choosing frequent features that are more likely to be skipped than those selected randomly in early iterations. We next compare with the results of related methods published elsewhere. We cannot make claims about statistical significance, but the results 9Only the best MML results are shown. 87 illustrate the competitiveness of our method. The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al. (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. Chang et al. (2007) only obtain better results than 88.2% on cora when using 300 labeled examples (two hours of estimated annotation time), 5000 additional unlabeled examples, and extra test time inference constraints. Note that obtaining these results required only 10 simulated minutes of annotation time, and that GE methods are provided no information abou</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In HTL-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jiao</author>
<author>Shaojun Wang</author>
<author>Chi-Hoon Lee</author>
<author>Russell Greiner</author>
<author>Dale Schuurmans</author>
</authors>
<title>Semisupervised conditional random fields for improved sequence segmentation and labeling.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>209--216</pages>
<contexts>
<context position="7571" citStr="Jiao et al. (2006)" startWordPosition="1192" endWordPosition="1195">ion over the empirical distribution of x and y, and the model distribution of z. The second is a double expectation over the empirical distribution of x and the model distribution of y and z. ∂θj ∂ LMML(θ) = E˜p(x,y)[Ep(z|y,x;θ)[Fj(y, z, x)]] − E˜p(x)[Ep(y,z|x;θ)[Fj(y, z, x)]]. We train models using LMML(θ) with expected gradient (Salakhutdinov et al., 2003). To additionally leverage unlabeled data, we compare with entropy regularization (ER). ER adds a term to the objective function that encourages confident predictions on unlabeled data. Training of linear-chain CRFs with ER is described by Jiao et al. (2006). 3 Generalized Expectation Criteria In this section, we give a brief overview of generalized expectation criteria (GE) (Mann and McCallum, 2008; Druck et al., 2008) and explain how we can use GE to learn CRF parameters with estimates of feature expectations and unlabeled data. GE criteria are terms in a parameter estimation objective function that express preferences on the 210 is a default value that works well in many settings. 82 value of a model expectation of some function. Given a score function S, an empirical distribution ˜p(x), a model distribution p(yJx; θ), and a constraint functio</context>
<context position="26881" citStr="Jiao et al., 2006" startWordPosition="4390" endWordPosition="4393">2 and clabel = 4. For instance active learning, we use Algorithm 1 but without the skip option, and set clabel = 0.7. We use N = 10 iterations, so the entire experiment simulates 10 minutes of annotation time. For efficiency, we consider the 500 most frequent unlabeled features in each iteration. To start, ten randomly selected seed labeled features are provided. We use random (rand) selection, uncertainty sampling (US) (using sequence entropy, normalized by sequence length) and information density (ID) (Settles and Craven, 2008) to select instance queries. We use Entropy Regularization (ER) (Jiao et al., 2006) to leverage unlabeled instances.7 We weight the ER term by choosing the best8 weight in {10−3,10−2,10−1,1,10} multiplied by #labeled #unlabeled for each data set and query selection method. Seed instances are provided such that the simulated labeling time is equivalent to labeling 10 features. We evaluate on two sequence labeling tasks. The apartments task involves segmenting 300 apartment classified ads into 11 fields including features, rent, neighborhood, and contact. We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of f3. T</context>
</contexts>
<marker>Jiao, Wang, Lee, Greiner, Schuurmans, 2006</marker>
<rawString>Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. 2006. Semisupervised conditional random fields for improved sequence segmentation and labeling. In ACL, pages 209–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1355" citStr="Lafferty et al., 2001" startWordPosition="197" endWordPosition="200">r experiments on two sequence labeling tasks we show that our active learning method outperforms passive learning with features as well as traditional active learning with instances. Preliminary experiments suggest that novel interfaces which intelligently solicit labels on multiple features facilitate more efficient annotation. 1 Introduction The application of machine learning to new problems is slowed by the need for labeled training data. When output variables are structured, annotation can be particularly difficult and timeconsuming. For example, when training a conditional random field (Lafferty et al., 2001) to extract fields such as rent, contact, features, and utilities from apartment classifieds, labeling 22 instances (2,540 tokens) provides only 66.1% accuracy.1 Recent work has used unlabeled data and limited prior information about input features to bootstrap accurate structured output models. For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. However, these methods have only been applied in scenarios in </context>
<context position="5746" citStr="Lafferty et al., 2001" startWordPosition="892" endWordPosition="895"> using a “grid” interface. The results support the findings of the simulated experiments and provide evidence that the “grid” interface can facilitate more efficient annotation. 2 Conditional Random Fields In this section we describe the underlying probabilistic model for all methods in this paper. We focus on sequence labeling, though the described methods could be applied to other structured output or classification tasks. We model the probability of the label sequence y E Yn conditioned on the input sequence x E Xn, p(y|x; θ) using first-order linear-chain conditional random fields (CRFs) (Lafferty et al., 2001). This probability is p(y|x; θ) = 1Z exp ( θjfj(yi, yi+1, x, i)), x i j where Zx is the partition function and feature functions fj consider the entire input sequence and at most two consecutive output variables. The most probable output sequence and transition marginal distributions can be computed using variants of Viterbi and forward-backward. Provided a training data distribution ˜p, we estimate CRF parameters by maximizing the conditional log likelihood of the training data. L(θ) = E˜p(x,y)[log p(y|x; θ)] We use numerical optimization to maximize L(θ), which requires the gradient of L(θ) </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In SIGIR,</booktitle>
<pages>3--12</pages>
<publisher>Springer-Verlag</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="15614" citStr="Lewis and Gale, 1994" startWordPosition="2535" endWordPosition="2538">alization of these methods to structured output spaces is not straightforward. Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by hand. We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Grac¸a et al. (2008), and Liang et al. (2009) in future work. Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. 4 Active Learning by Labeling Features Feature active learning, presented in Algorithm 1, is a pool-based active learning algorithm (Lewis and Gale, 1994) (with a pool of features rather than instances). The novel components of the algorithm are an option to skip a query and the notion that skipping and labeling have different costs. The option to skip is important when using feature queries because a user may not know how to label some features. In each iteration the model is retrained using the train procedure, which takes as input a set of labeled features C and unlabeled data distribution ˜p. For the reasons described in Section 3.3, we advocate using GE for the train procedure. Then, while the iteration cost c is less than the maximum cost</context>
<context position="18406" citStr="Lewis and Gale, 1994" startWordPosition="3024" endWordPosition="3027">e the new model parameters if the response to qk is ˆg. Unfortunately, this method is computationally intractable. Re-estimating θˆg will typically involve retraining the model, and doing this for each possible query-response pair is prohibitively expensive for structured output models. Computing the expectation over possible responses is also difficult, as in this paper users may provide a set of labels for a query, and more generally gˆ could be a distribution over labels. Instead, we propose a tractable strategy for reducing model uncertainty, motivated by traditional uncertainty sampling (Lewis and Gale, 1994). We assume that when a user responds to a query, the reduction in uncertainty will be equal to the Total Uncertainty (TU), the sum of the marginal entropies at the positions where the feature occurs. φTU(qk) = 1: 1: qk(xi, j)H(p(yj|xi; θ)) i j Total uncertainty, however, is highly biased towards selecting frequent features. A mean uncertainty variant, normalized by the feature’s count, would tend to choose very infrequent features. Consequently we propose a tradeoff between the two extremes, called weighted uncertainty (WU), that scales the mean uncertainty by the log count of the feature in </context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In SIGIR, pages 3–12, New York, NY, USA. Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning from measurements in exponential families.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="15339" citStr="Liang et al. (2009)" startWordPosition="2491" endWordPosition="2494">el. In Section 5 we use this heuristic with MML, but in general obtain poor results. Raghavan and Allan (2007) also propose several methods for learning with labeled features, but in a previous comparison GE gave better results (Druck et al., 2008). Additionally, the generalization of these methods to structured output spaces is not straightforward. Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by hand. We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Grac¸a et al. (2008), and Liang et al. (2009) in future work. Druck et al. (2008) provide a survey of other related methods for learning with labeled input features. 4 Active Learning by Labeling Features Feature active learning, presented in Algorithm 1, is a pool-based active learning algorithm (Lewis and Gale, 1994) (with a pool of features rather than instances). The novel components of the algorithm are an option to skip a query and the notion that skipping and labeling have different costs. The option to skip is important when using feature queries because a user may not know how to label some features. In each iteration the model </context>
<context position="22610" citStr="Liang et al. (2009)" startWordPosition="3705" endWordPosition="3708">labels, then for feature labels. Feature queries are selected according to their co-occurrence with important model features and previously labeled features. As noted in Section 3.3, GE is preferable to the methods Tandem Learning uses to learn with labeled features. We address the mixing of feature and instance queries in Section 4.3. In order to better understand differences in feature query selection methodology, we proposed a feature query selection method motivated6 by the method used in Tandem Learning in Section 4.1. However, this method performs poorly in the experiments in Section 5. Liang et al. (2009) simultaneously developed a method for learning with and actively selecting 6The query selection method of Raghavan and Allan (2007) requires a stack that is modified between queries within each iteration. Here query scores are only updated after each iteration of labeling. measurements, or target expectations with associated noise. The measurement selection method proposed by Liang et al. (2009) is based on Bayesian experimental design and is similar to the expected information gain method described above. Consequently this method is likely to be intractable for real applications. Note that L</context>
<context position="25145" citStr="Liang et al. (2009)" startWordPosition="4091" endWordPosition="4094"> instances, and class priors. The uncertainty-based query selection methods described in Section 4.1 apply naturally to these new query types. Importantly this framework would allow principled mixing of different query types, instead of alternating between them as in Tandem Learning (Raghavan and Allan, 2007). When mixing queries, it will be important to use different costs for different annotation types (Vijayanarasimhan and Grauman, 2008), and estimate the probability of obtaining a useful response to a query. We plan to pursue these directions in future work. This idea was also proposed by Liang et al. (2009), but no experiments with mixed active learning were presented. rankLDA(qk, t) 86 5 Simulated User Experiments In this section we experiment with an automated oracle labeler. When presented an instance query, the oracle simply provides the true labels. When presented a feature query, the oracle first decides whether to skip the query. We have found that users are more likely to label features that are relevant for only a few labels. Therefore, the oracle labels a feature if the entropy of its per occurrence label expectation, H(˜p(yi = y|qk(x, i) = 1; B)) ≤ 0.7. The oracle then labels the feat</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning from measurements in exponential families. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning of conditional random fields.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1724" citStr="Mann and McCallum (2008)" startWordPosition="254" endWordPosition="257">chine learning to new problems is slowed by the need for labeled training data. When output variables are structured, annotation can be particularly difficult and timeconsuming. For example, when training a conditional random field (Lafferty et al., 2001) to extract fields such as rent, contact, features, and utilities from apartment classifieds, labeling 22 instances (2,540 tokens) provides only 66.1% accuracy.1 Recent work has used unlabeled data and limited prior information about input features to bootstrap accurate structured output models. For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. However, these methods have only been applied in scenarios in which the user supplies such prior knowledge before learning begins. 1Averaged over 10 randomly selected sets of 22 instances. In traditional active learning (Settles, 2009), the machine queries the user for only the labels of instances that would be most helpful to the machine. This paper proposes an active learning approach in which the user provides “labels” for i</context>
<context position="7715" citStr="Mann and McCallum, 2008" startWordPosition="1214" endWordPosition="1218">ribution of x and the model distribution of y and z. ∂θj ∂ LMML(θ) = E˜p(x,y)[Ep(z|y,x;θ)[Fj(y, z, x)]] − E˜p(x)[Ep(y,z|x;θ)[Fj(y, z, x)]]. We train models using LMML(θ) with expected gradient (Salakhutdinov et al., 2003). To additionally leverage unlabeled data, we compare with entropy regularization (ER). ER adds a term to the objective function that encourages confident predictions on unlabeled data. Training of linear-chain CRFs with ER is described by Jiao et al. (2006). 3 Generalized Expectation Criteria In this section, we give a brief overview of generalized expectation criteria (GE) (Mann and McCallum, 2008; Druck et al., 2008) and explain how we can use GE to learn CRF parameters with estimates of feature expectations and unlabeled data. GE criteria are terms in a parameter estimation objective function that express preferences on the 210 is a default value that works well in many settings. 82 value of a model expectation of some function. Given a score function S, an empirical distribution ˜p(x), a model distribution p(yJx; θ), and a constraint function Gk(x, y), the value of a GE criterion is 9(θ) = S(E˜p(x)[Ep(y|x;θ)[Gk(x,y)]]). GE provides a flexible framework for parameter estimation becau</context>
<context position="9651" citStr="Mann and McCallum (2008)" startWordPosition="1525" endWordPosition="1528">l derivative of 9(θ) with respect to parameter j is proportional to the predicted covariance between the model feature function Fj and the constraint function Gk.3 (E˜p(x) [Ep(y|x;θ)[Fj(x, y)Gk(x, y)] − Ep(y|x;θ)[Fj(x, y)]Ep(y|x;θ)[Gk(x, y)]]I The partial derivative shows that GE learns parameter values for model feature functions based on their predicted covariance with the constraint functions. GE can thus be interpreted as a bootstrapping method that uses the limited training signal to learn about parameters for related model feature functions. 3.1 Learning with feature-label distributions Mann and McCallum (2008) apply GE to a linearchain, first-order CRF. In this section we provide an alternate treatment that arrives at the same objective function from the general form described in the previous section. Often, feature functions in a first-order linearchain CRF f are binary, and are the conjunction 3If we use squared error for S, the partial derivative is the covariance multiplied by 2( ˆGk − E[Gk(x, y)]). of an observational test q(x, i) and a label pair test 1{yi=y0,yi+1=y00}.4 f(yi, yi+1, x, i) = 1{yi=y0,yi+1=y00}q(x, i) The constraint functions Gk we use here decompose and operate similarly, excep</context>
<context position="11616" citStr="Mann and McCallum (2008)" startWordPosition="1858" endWordPosition="1861">functions {Gyk : y E Y}, and we use the score function in Equation 1, then the GE objective function specifies the minimization of the KL divergence between the model and target distributions over labels conditioned on the success of the observational test. In general the objective function will consist of many such KL divergence penalties. Computing the first term of the covariance in Equation 2 requires a marginal distribution over three labels, two of which will be consecutive, but the other of which could appear anywhere in the sequence. We can compute this marginal using the algorithm of Mann and McCallum (2008). As previously described, this algorithm is O(nJYJ3) for a sequence of length n. However, we make the following novel observation: we do not need to compute the extra lattices for feature label pairs with ˆGyk = 0, since this makes Equation 2 equal to zero. In Mann and McCallum (2008), probabilities were smoothed so that by ˆGyk &gt; 0. If we assume that only a small number of labels m have non-zero probability, then the time complexity of the gradient computation is O(nmJYJ2). In this paper typically 1 GmG 4, while JYJ is 11 or 13. 4We this notation for an indicator function that returns 1 if t</context>
<context position="14190" citStr="Mann and McCallum, 2008" startWordPosition="2304" endWordPosition="2307">∈/ L. By assigning 0 probability to labels l ∈/ L, we can use the speed-up described in the previous section. 3.3 Related Work Other proposed learning methods use labeled features to label unlabeled data. The resulting partially-labeled corpus can be used to train a CRF by maximizing MML. Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. Additional features that indicate similarity to the prototypes help the model to generalize. In a previous comparison between GE and PDL (Mann and McCallum, 2008), GE outperformed PDL without the extra similarity features, whose construction may be problem-specific. GE also performed better when supplied accurate label distributions. Additionally, both MML and PDL do not naturally generalize to learning with features that have multiple labels or distributions over labels, as in these scenarios labeling the unlabeled data is not straightforward. In this paper, we attempt to address this problem using a simple heuristic: when there are multiple choices for a token’s label, sample a label. In Section 5 we use this heuristic with MML, but in general obtain</context>
<context position="29909" citStr="Mann and McCallum (2008)" startWordPosition="4892" endWordPosition="4896">ty-based methods are choosing frequent features that are more likely to be skipped than those selected randomly in early iterations. We next compare with the results of related methods published elsewhere. We cannot make claims about statistical significance, but the results 9Only the best MML results are shown. 87 illustrate the competitiveness of our method. The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al. (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. Chang et al. (2007) only obtain better results than 88.2% on cora when using 300 labeled examples (two hours of estimated annotation time), 5000 additional unlabeled examples, and extra test time inference constraints. Note that obtaining these results required only 10 simulated minutes of annotation time, and that GE methods are provided no information about the label transition matrix. 6 User Experiments Another advantage of feature queries is that feature names are concise enough to be browsed, rather than cons</context>
</contexts>
<marker>Mann, McCallum, 2008</marker>
<rawString>Gideon Mann and Andrew McCallum. 2008. Generalized expectation criteria for semi-supervised learning of conditional random fields. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Quattoni</author>
<author>S Wang</author>
<author>L-P Morency</author>
<author>M Collins</author>
<author>T Darrell</author>
</authors>
<title>Hidden conditional random fields.</title>
<date>2007</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>29--1848</pages>
<contexts>
<context position="6844" citStr="Quattoni et al. (2007)" startWordPosition="1074" endWordPosition="1077">ng data. L(θ) = E˜p(x,y)[log p(y|x; θ)] We use numerical optimization to maximize L(θ), which requires the gradient of L(θ) with respect to the parameters. It can be shown that the partial derivative with respect to parameter j is equal 2.1 Learning with missing labels The training set may contain partially labeled sequences. Let z denote missing labels. We estimate parameters with this data by maximizing the marginal log-likelihood of the observed labels. LMML(θ) = E˜p(x,y)[log p(y, z|x; θ)] z We refer to this training method as maximum marginal likelihood (MML); it has also been explored by Quattoni et al. (2007). The gradient of LMML(θ) can also be written as the difference of two expectations. The first is an expectation over the empirical distribution of x and y, and the model distribution of z. The second is a double expectation over the empirical distribution of x and the model distribution of y and z. ∂θj ∂ LMML(θ) = E˜p(x,y)[Ep(z|y,x;θ)[Fj(y, z, x)]] − E˜p(x)[Ep(y,z|x;θ)[Fj(y, z, x)]]. We train models using LMML(θ) with expected gradient (Salakhutdinov et al., 2003). To additionally leverage unlabeled data, we compare with entropy regularization (ER). ER adds a term to the objective function th</context>
</contexts>
<marker>Quattoni, Wang, Morency, Collins, Darrell, 2007</marker>
<rawString>A. Quattoni, S. Wang, L.-P Morency, M. Collins, and T. Darrell. 2007. Hidden conditional random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29:1848–1852, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hema Raghavan</author>
<author>James Allan</author>
</authors>
<title>An interactive algorithm for asking and incorporating feature feedback into support vector machines.</title>
<date>2007</date>
<booktitle>In SIGIR,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="3303" citStr="Raghavan and Allan, 2007" startWordPosition="508" endWordPosition="512">for learning with labeled features. We provide an alternate treatment of the GE objective function used by Mann and McCallum (2008) and a novel speedup to the gradient computation. We then provide a pool-based feature active learning algorithm that includes an option to skip queries, for cases in which a feature has no clear label. We propose and evaluate feature query selection algorithms that aim to reduce model uncertainty, and compare to several baselines. We evaluate our method using both real and simulated user experiments on two sequence labeling tasks. Compared to previous approaches (Raghavan and Allan, 2007), our method can be used for both classification and structured tasks, and the feature query selection methods we propose perform better. We use experiments with simulated labelers on real data to extensively compare feature query selection algorithms and evaluate on multiple random splits. To make these simulations more realistic, the effort required to perform different labeling actions is estimated from additional experiments with real users. The results show that active learning with features outperforms both passive learning with features and traditional active learning with instances. In</context>
<context position="14830" citStr="Raghavan and Allan (2007)" startWordPosition="2406" endWordPosition="2409">ed PDL without the extra similarity features, whose construction may be problem-specific. GE also performed better when supplied accurate label distributions. Additionally, both MML and PDL do not naturally generalize to learning with features that have multiple labels or distributions over labels, as in these scenarios labeling the unlabeled data is not straightforward. In this paper, we attempt to address this problem using a simple heuristic: when there are multiple choices for a token’s label, sample a label. In Section 5 we use this heuristic with MML, but in general obtain poor results. Raghavan and Allan (2007) also propose several methods for learning with labeled features, but in a previous comparison GE gave better results (Druck et al., 2008). Additionally, the generalization of these methods to structured output spaces is not straightforward. Chang et al. (2007) present an algorithm for learning with constraints, but this method requires users to set weights by hand. We plan to explore the use of the recently developed related methods of Bellare et al. (2009), Grac¸a et al. (2008), and Liang et al. (2009) in future work. Druck et al. (2008) provide a survey of other related methods for learning</context>
<context position="20460" citStr="Raghavan and Allan, 2007" startWordPosition="3355" endWordPosition="3358">rained between iterations. To verify that this is necessary, we compare against query selection methods that only consider the previously labeled features. First, we consider a feature query selection method called coverage (cov) that aims to select features that are dissimilar from existing labeled features, increasing the labeled features’ “coverage” of the feature space. In order to compensate for choosing very infrequent features, we multiply by the log count of the feature. 1: φcov(qk) = log(Ck) 1 1 − sim(qk, qj) |S |jEC Motivated by the feature query selection method of Tandem Learning (Raghavan and Allan, 2007) (see Section 4.2 for further discussion), we consider a feature selection metric similarity (sim) that is the maximum similarity to a labeled feature, weighted by the log count of the feature. φsim(qk) = log(Ck) max jEC 5sim(qk, qj) returns the cosine similarity between context vectors of words occurring in a window of f3. Ck . sim(qk, qj) 85 Features similar to those already labeled are likely to be discriminative, and therefore likely to be labeled (rather than skipped). However, insufficient diversity may also result in an inaccurate model, suggesting that coverage should select more usefu</context>
<context position="21841" citStr="Raghavan and Allan, 2007" startWordPosition="3583" endWordPosition="3586">put features using their frequency in the training data. Ofreq(qk) = � � qk(Xi,j) i j Top LDA (LDA) selects the top words from 50 topics learned from the unlabeled data using latent Dirichlet allocation (LDA) (Blei et al., 2003). More specifically, the words w generated by each topic t are ranked using the conditional probability p(wIt). The word feature is assigned its maximum rank across all topics. OLDA(qk) = max t This method will select useful features if the topics discovered are relevant to the task. A similar heuristic was used by Druck et al. (2008). 4.2 Related Work Tandem Learning (Raghavan and Allan, 2007) is an algorithm that combines feature and instance active learning for classification. The algorithm iteratively queries the user first for instance labels, then for feature labels. Feature queries are selected according to their co-occurrence with important model features and previously labeled features. As noted in Section 3.3, GE is preferable to the methods Tandem Learning uses to learn with labeled features. We address the mixing of feature and instance queries in Section 4.3. In order to better understand differences in feature query selection methodology, we proposed a feature query se</context>
<context position="24836" citStr="Raghavan and Allan, 2007" startWordPosition="4039" endWordPosition="4042"> focussed on labeling input features. However, the proposed methods generalize to queries for expectation estimates of arbitrary functions, for example queries for the label distributions for input features, labels for instances (using a function that is non-zero only for a particular instance), partial labels for instances, and class priors. The uncertainty-based query selection methods described in Section 4.1 apply naturally to these new query types. Importantly this framework would allow principled mixing of different query types, instead of alternating between them as in Tandem Learning (Raghavan and Allan, 2007). When mixing queries, it will be important to use different costs for different annotation types (Vijayanarasimhan and Grauman, 2008), and estimate the probability of obtaining a useful response to a query. We plan to pursue these directions in future work. This idea was also proposed by Liang et al. (2009), but no experiments with mixed active learning were presented. rankLDA(qk, t) 86 5 Simulated User Experiments In this section we experiment with an automated oracle labeler. When presented an instance query, the oracle simply provides the true labels. When presented a feature query, the or</context>
</contexts>
<marker>Raghavan, Allan, 2007</marker>
<rawString>Hema Raghavan and James Allan. 2007. An interactive algorithm for asking and incorporating feature feedback into support vector machines. In SIGIR, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
</authors>
<title>An analysis of active learning strategies for sequence labeling tasks.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="26798" citStr="Settles and Craven, 2008" startWordPosition="4376" endWordPosition="4379">s such that each iteration simulates one minute of labeling by setting cmax = 60, cskip = 2 and clabel = 4. For instance active learning, we use Algorithm 1 but without the skip option, and set clabel = 0.7. We use N = 10 iterations, so the entire experiment simulates 10 minutes of annotation time. For efficiency, we consider the 500 most frequent unlabeled features in each iteration. To start, ten randomly selected seed labeled features are provided. We use random (rand) selection, uncertainty sampling (US) (using sequence entropy, normalized by sequence length) and information density (ID) (Settles and Craven, 2008) to select instance queries. We use Entropy Regularization (ER) (Jiao et al., 2006) to leverage unlabeled instances.7 We weight the ER term by choosing the best8 weight in {10−3,10−2,10−1,1,10} multiplied by #labeled #unlabeled for each data set and query selection method. Seed instances are provided such that the simulated labeling time is equivalent to labeling 10 features. We evaluate on two sequence labeling tasks. The apartments task involves segmenting 300 apartment classified ads into 11 fields including features, rent, neighborhood, and contact. We use the same feature processing as Ha</context>
</contexts>
<marker>Settles, Craven, 2008</marker>
<rawString>Burr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2009</date>
<tech>Technical Report 1648,</tech>
<institution>University of Wisconsin -Madison.</institution>
<contexts>
<context position="2128" citStr="Settles, 2009" startWordPosition="319" endWordPosition="320">.1 Recent work has used unlabeled data and limited prior information about input features to bootstrap accurate structured output models. For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. However, these methods have only been applied in scenarios in which the user supplies such prior knowledge before learning begins. 1Averaged over 10 randomly selected sets of 22 instances. In traditional active learning (Settles, 2009), the machine queries the user for only the labels of instances that would be most helpful to the machine. This paper proposes an active learning approach in which the user provides “labels” for input features, rather than instances. A labeled input feature denotes that a particular input feature, for example the word call, is highly indicative of a particular label, such as contact. Table 1 provides an excerpt of a feature active learning session. In this paper, we advocate using generalized expectation (GE) criteria (Mann and McCallum, 2008) for learning with labeled features. We provide an </context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>Burr Settles. 2009. Active learning literature survey. Technical Report 1648, University of Wisconsin -Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Prem Melville</author>
<author>Richard D Lawrence</author>
</authors>
<title>Uncertainty sampling and transductive experimental design for active dual supervision.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="23564" citStr="Sindhwani et al., 2009" startWordPosition="3849" endWordPosition="3852">se. The measurement selection method proposed by Liang et al. (2009) is based on Bayesian experimental design and is similar to the expected information gain method described above. Consequently this method is likely to be intractable for real applications. Note that Liang et al. (2009) only use this method in synthetic experiments, and instead use a method similar to total uncertainty for experiments in part-of-speech tagging. Unlike the experiments presented in this paper, Liang et al. (2009) conduct only simulated active learning experiments and do not consider skipping queries. Sindhwani (Sindhwani et al., 2009) simultaneously developed an active learning method that queries for both instance and feature labels that are then used in a graph-based learning algorithm. They find that querying certain features outperforms querying uncertain features, but this is likely because their query selection method is similar to the expectation uncertainty method described above, and consequently non-discriminative features may be queried often (see also the discussion in Section 4.1). It is also not clear how this graph-based training method would generalize to structured output spaces. 4.3 Expectation Constraint</context>
</contexts>
<marker>Sindhwani, Melville, Lawrence, 2009</marker>
<rawString>Vikas Sindhwani, Prem Melville, and Richard D. Lawrence. 2009. Uncertainty sampling and transductive experimental design for active dual supervision. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudheendra Vijayanarasimhan</author>
<author>Kristen Grauman</author>
</authors>
<title>Multi-level active prediction of useful image annotations for recognition.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="24970" citStr="Vijayanarasimhan and Grauman, 2008" startWordPosition="4059" endWordPosition="4062">rary functions, for example queries for the label distributions for input features, labels for instances (using a function that is non-zero only for a particular instance), partial labels for instances, and class priors. The uncertainty-based query selection methods described in Section 4.1 apply naturally to these new query types. Importantly this framework would allow principled mixing of different query types, instead of alternating between them as in Tandem Learning (Raghavan and Allan, 2007). When mixing queries, it will be important to use different costs for different annotation types (Vijayanarasimhan and Grauman, 2008), and estimate the probability of obtaining a useful response to a query. We plan to pursue these directions in future work. This idea was also proposed by Liang et al. (2009), but no experiments with mixed active learning were presented. rankLDA(qk, t) 86 5 Simulated User Experiments In this section we experiment with an automated oracle labeler. When presented an instance query, the oracle simply provides the true labels. When presented a feature query, the oracle first decides whether to skip the query. We have found that users are more likely to label features that are relevant for only a </context>
</contexts>
<marker>Vijayanarasimhan, Grauman, 2008</marker>
<rawString>Sudheendra Vijayanarasimhan and Kristen Grauman. 2008. Multi-level active prediction of useful image annotations for recognition. In NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>