<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001941">
<title confidence="0.998155">
Multi-Task Active Learning for Linguistic Annotations
</title>
<author confidence="0.999666">
Roi Reichart1* Katrin Tomanek2* Udo Hahn2 Ari Rappoport1
</author>
<affiliation confidence="0.9932125">
1Institute of Computer Science
Hebrew University of Jerusalem, Israel
</affiliation>
<email confidence="0.98896">
{roiri|arir}@cs.huji.ac.il
</email>
<affiliation confidence="0.8017375">
2Jena University Language &amp; Information Engineering (JULIE) Lab
Friedrich-Schiller-Universit¨at Jena, Germany
</affiliation>
<email confidence="0.896885">
{katrin.tomanek|udo.hahn}@uni-jena.de
</email>
<sectionHeader confidence="0.997892" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995575">
We extend the classical single-task active
learning (AL) approach. In the multi-task ac-
tive learning (MTAL) paradigm, we select ex-
amples for several annotation tasks rather than
for a single one as usually done in the con-
text of AL. We introduce two MTAL meta-
protocols, alternating selection and rank com-
bination, and propose a method to implement
them in practice. We experiment with a two-
task annotation scenario that includes named
entity and syntactic parse tree annotations on
three different corpora. MTAL outperforms
random selection and a stronger baseline, one-
sided example selection, in which one task is
pursued using AL and the selected examples
are provided also to the other task.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829785714286">
Supervised machine learning methods have success-
fully been applied to many NLP tasks in the last few
decades. These techniques have demonstrated their
superiority over both hand-crafted rules and unsu-
pervised learning approaches. However, they re-
quire large amounts of labeled training data for every
level of linguistic processing (e.g., POS tags, parse
trees, or named entities). When, when domains
and text genres change (e.g., moving from common-
sense newspapers to scientific biology journal arti-
cles), extensive retraining on newly supplied train-
ing material is often required, since different do-
mains may use different syntactic structures as well
as different semantic classes (entities and relations).
</bodyText>
<note confidence="0.65076">
* Both authors contributed equally to this work.
</note>
<bodyText confidence="0.999918147058824">
Consequently, with an increasing coverage of a
wide variety of domains in human language tech-
nology (HLT) systems, we can expect a growing
need for manual annotations to support many kinds
of application-specific training data.
Creating annotated data is extremely labor-
intensive. The Active Learning (AL) paradigm
(Cohn et al., 1996) offers a promising solution to
deal with this bottleneck, by allowing the learning
algorithm to control the selection of examples to
be manually annotated such that the human label-
ing effort be minimized. AL has been successfully
applied already for a wide range of NLP tasks, in-
cluding POS tagging (Engelson and Dagan, 1996),
chunking (Ngai and Yarowsky, 2000), statistical
parsing (Hwa, 2004), and named entity recognition
(Tomanek et al., 2007).
However, AL is designed in such a way that it se-
lects examples for manual annotation with respect to
a single learning algorithm or classifier. Under this
AL annotation policy, one has to perform a separate
annotation cycle for each classifier to be trained. In
the following, we will refer to the annotations sup-
plied for a classifier as the annotations for a single
annotation task.
Modern HLT systems often utilize annotations re-
sulting from different tasks. For example, a machine
translation system might use features extracted from
parse trees and named entity annotations. For such
an application, we obviously need the different an-
notations to reside in the same text corpus. It is not
clear how to apply the single-task AL approach here,
since a training example that is beneficial for one
task might not be so for others. We could annotate
</bodyText>
<page confidence="0.975819">
861
</page>
<note confidence="0.696833">
Proceedings ofACL-08: HLT, pages 861–869,
</note>
<page confidence="0.495188">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999981164179105">
the same corpus independently by the two tasks and
merge the resulting annotations, but that (as we show
in this paper) would possibly yield sub-optimal us-
age of human annotation efforts.
There are two reasons why multi-task AL, and
by this, a combined corpus annotated for various
tasks, could be of immediate benefit. First, annota-
tors working on similar annotation tasks (e.g., con-
sidering named entities and relations between them),
might exploit annotation data from one subtask for
the benefit of the other. If for each subtask a sepa-
rate corpus is sampled by means of AL, annotators
will definitely lack synergy effects and, therefore,
annotation will be more laborious and is likely to
suffer in terms of quality and accuracy. Second, for
dissimilar annotation tasks – take, e.g., a compre-
hensive HLT pipeline incorporating morphological,
syntactic and semantic data – a classifier might re-
quire features as input which constitute the output
of another preceding classifier. As a consequence,
training such a classifier which takes into account
several annotation tasks will best be performed on
a rich corpus annotated with respect to all input-
relevant tasks. Both kinds of annotation tasks, simi-
lar and dissimilar ones, constitute examples of what
we refer to as multi-task annotation problems.
Indeed, there have been efforts in creating re-
sources annotated with respect to various annotation
tasks though each of them was carried out indepen-
dently of the other. In the general language UPenn
annotation efforts for the WSJ sections of the Penn
Treebank (Marcus et al., 1993), sentences are anno-
tated with POS tags, parse trees, as well as discourse
annotation from the Penn Discourse Treebank (Milt-
sakaki et al., 2008), while verbs and verb arguments
are annotated with Propbank rolesets (Palmer et al.,
2005). In the biomedical GENIA corpus (Ohta et
al., 2002), scientific text is annotated with POS tags,
parse trees, and named entities.
In this paper, we introduce multi-task active
learning (MTAL), an active learning paradigm for
multiple annotation tasks. We propose a new AL
framework where the examples to be annotated are
selected so that they are as informative as possible
for a set of classifiers instead of a single classifier
only. This enables the creation of a single combined
corpus annotated with respect to various annotation
tasks, while preserving the advantages of AL with
respect to the minimization of annotation efforts.
In a proof-of-concept scenario, we focus on two
highly dissimilar tasks, syntactic parsing and named
entity recognition, study the effects of multi-task AL
under rather extreme conditions. We propose two
MTAL meta-protocols and a method to implement
them for these tasks. We run experiments on three
corpora for domains and genres that are very differ-
ent (WSJ: newspapers, Brown: mixed genres, and
GENIA: biomedical abstracts). Our protocols out-
perform two baselines (random and a stronger one-
sided selection baseline).
In Section 2 we introduce our MTAL framework
and present two MTAL protocols. In Section 3 we
discuss the evaluation of these protocols. Section
4 describes the experimental setup, and results are
presented in Section 5. We discuss related work in
Section 6. Finally, we point to open research issues
for this new approach in Section 7.
</bodyText>
<sectionHeader confidence="0.986336" genericHeader="introduction">
2 A Framework for Multi-Task AL
</sectionHeader>
<bodyText confidence="0.999886333333333">
In this section we introduce a sample selection
framework that aims at reducing the human anno-
tation effort in a multiple annotation scenario.
</bodyText>
<subsectionHeader confidence="0.984922">
2.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.999505">
To measure the efficiency of selection methods, we
define the training quality TQ of annotated mate-
rial 5 as the performance p yielded with a reference
learner X trained on that material: TQ(X, 5) = p.
A selection method can be considered better than an-
other one if a higher TQ is yielded with the same
amount of examples being annotated.
Our framework is an extension of the Active
Learning (AL) framework (Cohn et al., 1996)). The
original AL framework is based on querying in an it-
erative manner those examples to be manually anno-
tated that are most useful for the learner at hand. The
TQ of an annotated corpus selected by means of AL
is much higher than random selection. This AL ap-
proach can be considered as single-task AL because
it focuses on a single learner for which the examples
are to be selected. In a multiple annotation scenario,
however, there are several annotation tasks to be ac-
complished at once and for each task typically a sep-
arate statistical model will then be trained. Thus, the
goal of multi-task AL is to query those examples for
</bodyText>
<page confidence="0.996029">
862
</page>
<bodyText confidence="0.9150825">
human annotation that are most informative for all
learners involved.
</bodyText>
<subsectionHeader confidence="0.994785">
2.2 One-Sided Selection vs. Multi-Task AL
</subsectionHeader>
<bodyText confidence="0.999996037037037">
The naive approach to select examples in a multiple
annotation scenario would be to perform a single-
task AL selection, i.e., the examples to be annotated
are selected with respect to one of the learners only.1
In a multiple annotation scenario we call such an ap-
proach one-sided selection. It is an intrinsic selec-
tion for the reference learner, and an extrinsic selec-
tion for all the other learners also trained on the an-
notated material. Obviously, a corpus compiled with
the help of one-sided selection will have a good TQ
for that learner for which the intrinsic selection has
taken place. For all the other learners, however, we
have no guarantee that their TQ will not be inferior
than the TQ of a random selection process.
In scenarios where the different annotation tasks
are highly dissimilar we can expect extrinsic selec-
tion to be rather poor. This intuition is demonstrated
by experiments we conducted for named entity (NE)
and parse annotation tasks2 (Figure 1). In this sce-
nario, extrinsic selection for the NE annotation task
means that examples where selected with respect
to the parsing task. Extrinsic selection performed
about the same as random selection for the NE task,
while for the parsing task extrinsic selection per-
formed markedly worse. This shows that examples
that were very informative for the NE learner were
not that informative for the parse learner.
</bodyText>
<subsectionHeader confidence="0.990088">
2.3 Protocols for Multi-Task AL
</subsectionHeader>
<bodyText confidence="0.999387083333333">
Obviously, we can expect one-sided selection to per-
form better for the reference learner (the one for
which an intrinsic selection took place) than multi-
task AL selection, because the latter would be a
compromise for all learners involved in the multi-
ple annotation scenario. However, the goal of multi-
task AL is to minimize the annotation effort over all
annotation tasks and not just the effort for a single
annotation task.
For a multi-task AL protocol to be valuable in a
specific multiple annotation scenario, the TQ for all
considered learners should be
</bodyText>
<footnote confidence="0.972751333333333">
1Of course, all selected examples would be annotated w.r.t.
all annotation tasks.
2See Section 4 for our experimental setup.
</footnote>
<listItem confidence="0.946628666666667">
1. better than the TQ of random selection,
2. and better than the TQ of any extrinsic selec-
tion.
</listItem>
<bodyText confidence="0.999198285714286">
In the following, we introduce two protocols for
multi-task AL. Multi-task AL protocols can be con-
sidered meta-protocols because they basically spec-
ify how task-specific, single-task AL approaches can
be combined into one selection decision. By this,
the protocols are independent of the underlying task-
specific AL approaches.
</bodyText>
<subsectionHeader confidence="0.84441">
2.3.1 Alternating Selection
</subsectionHeader>
<bodyText confidence="0.999989">
The alternating selection protocol alternates one-
sided AL selection. In sj consecutive AL iterations,
the selection is performed as one-sided selection
with respect to learning algorithm Xj. After that,
another learning algorithm is considered for selec-
tion for sk consecutive iterations and so on. Depend-
ing on the specific scenario, this enables to weight
the different annotation tasks by allowing them to
guide the selection in more or less AL iterations.
This protocol is a straight-forward compromise be-
tween the different single-task selection approaches.
In this paper we experiment with the special case
of si = 1, where in every AL iteration the selection
leadership is changed. More sophisticated calibra-
tion of the parameters si is beyond the scope of this
paper and will be dealt with in future work.
</bodyText>
<subsectionHeader confidence="0.522824">
2.3.2 Rank Combination
</subsectionHeader>
<bodyText confidence="0.999980923076923">
The rank combination protocol is more directly
based on the idea to combine single-task AL selec-
tion decisions. In each AL iteration, the usefulness
score sXj(e) of each unlabeled example e from the
pool of examples is calculated with respect to each
learner Xj and then translated into a rank rXj(e)
where higher usefulness means lower rank number
(examples with identical scores get the same rank
number). Then, for each example, we sum the rank
numbers of each annotation task to get the overall
rank r(e) = E&apos;jirXj(e). All examples are sorted
by this combined rank and b examples with lowest
rank numbers are selected for manual annotation.3
</bodyText>
<footnote confidence="0.644463666666667">
3As the number of ranks might differ between the single an-
notation tasks, we normalize them to the coarsest scale. Then
we can sum up the ranks as explained above.
</footnote>
<page confidence="0.99475">
863
</page>
<figure confidence="0.999131666666667">
f−score
f−score
tokens
constituents
random selection
extrinsic selection (PARSE−AL)
10000 20000 30000 40000 50000
0.65 0.70 0.75 0.80
10000 20000 30000 40000
random selection
extrinsic selection (NE−AL)
0.76 0.78 0.80 0.82 0.84
</figure>
<figureCaption confidence="0.992186">
Figure 1: Learning curves for random and extrinsic selection on both tasks: named entity annotation (left) and syntactic
parse annotation (right), using the WSJ corpus scenario
</figureCaption>
<bodyText confidence="0.99993725">
This protocol favors examples which are good for
all learning algorithms. Examples that are highly in-
formative for one task but rather uninformative for
another task will not be selected.
</bodyText>
<sectionHeader confidence="0.948302" genericHeader="method">
3 Evaluation of Multi-Task AL
</sectionHeader>
<bodyText confidence="0.999985863636364">
The notion of training quality (TQ) can be used to
quantify the effectiveness of a protocol, and by this,
annotation costs in a single-task AL scenario. To ac-
tually quantify the overall training quality in a multi-
ple annotation scenario one would have to sum over
all the single task’s TQs. Of course, depending on
the specific annotation task, one would not want to
quantify the number of examples being annotated
but different task-specific units of annotation. While
for entity annotations one does typically count the
number of tokens being annotated, in the parsing
scenario the number of constituents being annotated
is a generally accepted measure. As, however, the
actual time needed for the annotation of one exam-
ple usually differs for different annotation tasks, nor-
malizing exchange rates have to be specified which
can then be used as weighting factors. In this paper,
we do not define such weighting factors4, and leave
this challenging question to be discussed in the con-
text of psycholinguistic research.
We could quantify the overall efficiency score E
of a MTAL protocol P by
</bodyText>
<equation confidence="0.903820666666667">
n
E(P) = E αj · TQ(Xj, uj)
j=1
</equation>
<bodyText confidence="0.99559636">
where uj denotes the individual annotation task’s
4Such weighting factors not only depend on the annotation
level or task but also on the domain, and especially on the cog-
nitive load of the annotation task.
number of units being annotated (e.g., constituents
for parsing) and the task-specific weights are defined
by αj. Given weights are properly defined, such a
score can be applied to directly compare different
protocols and quantify their differences.
In practice, such task-specific weights might also
be considered in the MTAL protocols. In the alter-
nating selection protocol, the numbers of consecu-
tive iterations si each single task protocol can be
tuned according to the α parameters. As for the
rank combination protocol, the weights can be con-
sidered when calculating the overall rank: r(e) =
En j=1 0j · rXj(e) where the parameters 01 ... 0n re-
flect the values of α1 ... αn (though they need not
necessarily be the same).
In our experiments, we assumed the same weight
for all annotation schemata, thus simply setting si =
1, 0i = 1. This was done for the sake of a clear
framework presentation. Finding proper weights for
the single tasks and tuning the protocols accordingly
is a subject for further research.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.990973">
4.1 Scenario and Task-Specific Selection
Protocols
</subsectionHeader>
<bodyText confidence="0.9994955">
The tasks in our scenario comprise one semantic
task (annotation with named entities (NE)) and one
syntactic task (annotation with PCFG parse trees).
The tasks are highly dissimilar, thus increasing the
potential value of MTAL. Both tasks are subject to
intensive research by the NLP community.
The MTAL protocols proposed are meta-
protocols that combine the selection decisions of
the underlying, task-specific AL protocols. In
our scenario, the task-specific AL protocols are
</bodyText>
<page confidence="0.991969">
864
</page>
<bodyText confidence="0.997560066666667">
committee-based (Freund et al., 1997) selection
protocols. In committee-based AL, a committee
consists of k classifiers of the same type trained
on different subsets of the training data.5 Each
committee member then makes its predictions on
the unlabeled examples, and those examples on
which the committee members disagree most are
considered most informative for learning and are
thus selected for manual annotation. In our scenario
the example grain-size is the sentence level.
For the NE task, we apply the AL approach of
Tomanek et al. (2007). The committee consists of
k1 = 3 classifiers and the vote entropy (VE) (Engel-
son and Dagan, 1996) is employed as disagreement
metric. It is calculated on the token-level as
</bodyText>
<equation confidence="0.9985595">
V Etok(t) = − 1 c V (li, t) log V (li, t) (1)
log k E k k
</equation>
<bodyText confidence="0.999555818181818">
where V (l&amp;quot;t) kis the ratio of k classifiers where the
label li is assigned to a token t. The sentence level
vote entropy V Esent is then the average over all to-
kens tj of sentence s.
For the parsing task, the disagreement score is
based on a committee of k2 = 10 instances of Dan
Bikel’s reimplementation of Collins’ parser (Bickel,
2005; Collins, 1999). For each sentence in the un-
labeled pool, the agreement between the committee
members was calculated using the function reported
by Reichart and Rappoport (2007):
</bodyText>
<equation confidence="0.878282666666667">
1 � fscore(mi, ml) (2)
AF (s) =
N i,l∈[1...N],i=,4l
</equation>
<bodyText confidence="0.673146">
Where mi and ml are the committee members and
</bodyText>
<equation confidence="0.981578">
N = k2�(k2−1)
</equation>
<bodyText confidence="0.96855">
2 is the number of pairs of different
committee members. This function calculates the
agreement between the members of each pair by cal-
culating their relative f-score and then averages the
pairs’ scores. The disagreement of the committee on
a sentence is simply 1 − AF(s).
</bodyText>
<subsectionHeader confidence="0.949977">
4.2 Experimental settings
</subsectionHeader>
<bodyText confidence="0.780449333333333">
For the NE task we employed the classifier described
by Tomanek et al. (2007): The NE tagger is based on
Conditional Random Fields (Lafferty et al., 2001)
</bodyText>
<footnote confidence="0.715419">
5We randomly sampled L = 34 of the training data to create
each committee member.
</footnote>
<bodyText confidence="0.999453906976744">
and has a rich feature set including orthographical,
lexical, morphological, POS, and contextual fea-
tures. For parsing, Dan Bikel’s reimplementation of
Collins’ parser is employed, using gold POS tags.
In each AL iteration we select 100 sentences for
manual annotation.6 We start with a randomly cho-
sen seed set of 200 sentences. Within a corpus we
used the same seed set in all selection scenarios. We
compare the following five selection scenarios: Ran-
dom selection (RS), which serves as our baseline;
one-sided AL selection for both tasks (called NE-AL
and PARSE-AL); and multi-task AL selection with
the alternating selection protocol (alter-MTAL) and
the rank combination protocol (ranks-MTAL).
We performed our experiments on three dif-
ferent corpora, namely one from the newspaper
genre (WSJ), a mixed-genre corpus (Brown), and a
biomedical corpus (Bio). Our simulation corpora
contain both entity annotations and (constituent)
parse annotations. For each corpus we have a pool
set (from which we select the examples for annota-
tion) and an evaluation set (used for generating the
learning curves). The WSJ corpus is based on the
WSJ part of the PENN TREEBANK (Marcus et al.,
1993); we used the first 10,000 sentences of section
2-21 as the pool set, and section 00 as evaluation set
(1,921 sentences). The Brown corpus is also based
on the respective part of the PENN TREEBANK. We
created a sample consisting of 8 of any 10 consec-
utive sentences in the corpus. This was done as
Brown contains text from various English text gen-
res, and we did that to create a representative sample
of the corpus domains. We finally selected the first
10,000 sentences from this sample as pool set. Every
9th from every 10 consecutive sentences package
went into the evaluation set which consists of 2,424
sentences. For both WSJ and Brown only parse an-
notations though no entity annotations were avail-
able. Thus, we enriched both corpora with entity
annotations (three entities: person, location, and or-
ganization) by means of a tagger trained on the En-
glish data set of the CoNLL-2003 shared task (Tjong
Kim Sang and De Meulder, 2003).7 The Bio corpus
</bodyText>
<footnote confidence="0.9992854">
6Manual annotation is simulated by just unveiling the anno-
tations already contained in our corpora.
7We employed a tagger similar to the one presented by Set-
tles (2004). Our tagger has a performance of R� 84% f-score on
the CoNLL-2003 data; inspection of the predicted entities on
</footnote>
<page confidence="0.998648">
865
</page>
<bodyText confidence="0.999862875">
is based on the parsed section of the GENIA corpus
(Ohta et al., 2002). We performed the same divi-
sions as for Brown, resulting in 2,213 sentences in
our pool set and 276 sentences for the evaluation set.
This part of the GENIA corpus comes with entity an-
notations. We have collapsed the entity classes an-
notated in GENIA (cell line, cell type, DNA, RNA,
protein) into a single, biological entity class.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.998124024096386">
In this section we present and discuss our results
when applying the five selection strategies (RS, NE-
AL, PARSE-AL, alter-MTAL, and ranks-MTAL) to
our scenario on the three corpora. We refrain from
calculating the overall efficiency score (Section 3)
here due to the lack of generally accepted weights
for the considered annotation tasks. However, we
require from a good selection protocol to exceed the
performance of random selection and extrinsic se-
lection. In addition, recall from Section 3 that we
set the alternate selection and rank combination pa-
rameters to si = 1, 0i = 1, respectively to reflect a
tradeoff between the annotation efforts of both tasks.
Figures 2 and 3 depict the learning curves for
the NE tagger and the parser on WSJ and Brown,
respectively. Each figure shows the five selection
strategies. As expected, on both corpora and both
tasks intrinsic selection performs best, i.e., for the
NE tagger NE-AL and for the parser PARSE-AL.
Further, random selection and extrinsic selection
perform worst. Most importantly, both MTAL pro-
tocols clearly outperform extrinsic and random se-
lection in all our experiments. This is in contrast
to NE-AL which performs worse than random se-
lection for all corpora when used as extrinsic selec-
tion, and for PARSE-AL that outperforms the ran-
dom baseline only for Brown when used as extrin-
sic selection. That is, the MTAL protocols suggest a
tradeoff between the annotation efforts of the differ-
ent tasks, here.
On WSJ, both for the NE and the parse annotation
tasks, the performance of the MTAL protocols is
very similar, though ranks-MTAL performs slightly
better. For the parser task, up to 30,000 constituents
MTAL performs almost as good as does PARSE-
AL. This is different for the NE task where NE-AL
WSJ and Brown revealed a good tagging performance.
clearly outperforms MTAL. On Brown, in general
we see the same results, with some minor differ-
ences. On the NE task, extrinsic selection (PARSE-
AL) performs better than random selection, but it is
still much worse than intrinsic AL or MTAL. Here,
ranks-MTAL significantly outperforms alter-MTAL
and almost performs as good as intrinsic selection.
For the parser task, we see that extrinsic and ran-
dom selection are equally bad. Both MTAL proto-
cols perform equally well, again being quite similar
to the intrinsic selection. On the BIO corpus8 we ob-
served the same tendencies as in the other two cor-
pora, i.e., MTAL clearly outperforms extrinsic and
random selection and supplies a better tradeoff be-
tween annotation efforts of the task at hand than one-
sided selection.
Overall, we can say that in all scenarios MTAL
performs much better than random selection and ex-
trinsic selection, and in most cases the performance
of MTAL (especially but not exclusively, ranks-
MTAL) is even close to intrinsic selection. This is
promising evidence that MTAL selection can be a
better choice than one-sided selection in multiple an-
notation scenarios. Thus, considering all annotation
tasks in the selection process (even if the selection
protocol is as simple as the alternating selection pro-
tocol) is better than selecting only with respect to
one task. Further, it should be noted that overall the
more sophisticated rank combination protocol does
not perform much better than the simpler alternating
selection protocol in all scenarios.
Finally, Figure 4 shows the disagreement curves
for the two tasks on the WSJ corpus. As has already
been discussed by Tomanek and Hahn (2008), dis-
agreement curves can be used as a stopping crite-
rion and to monitor the progress of AL-driven an-
notation. This is especially valuable when no anno-
tated validation set is available (which is needed for
plotting learning curves). We can see that the dis-
agreement curves significantly flatten approximately
at the same time as the learning curves do. In the
context of MTAL, disagreement curves might not
only be interesting as a stopping criterion but rather
as a switching criterion, i.e., to identify when MTAL
could be turned into one-sided selection. This would
be the case if in an MTAL scenario, the disagree-
</bodyText>
<footnote confidence="0.997501">
8The plots for the Bio are omitted due to space restrictions.
</footnote>
<page confidence="0.997027">
866
</page>
<figure confidence="0.999375111111111">
f−score
f−score
tokens
tokens
10000 20000 30000 40000 50000
0.65 0.70 0.75 0.80 0.85
RS
NE−AL
PARSE−AL
alter−MTAL
ranks−MTAL
5000 10000 15000 20000 25000 30000
0.55 0.60 0.65 0.70 0.75 0.80
RS
NE−AL
PARSE−AL
alter−MTAL
ranks−MTAL
</figure>
<figureCaption confidence="0.9998785">
Figure 2: Learning curves for NE task on WSJ (left) and Brown (right)
Figure 3: Learning curves for parse task on WSJ (left) and Brown (right)
</figureCaption>
<figure confidence="0.999686470588235">
5000 10000 15000 20000 25000 30000 35000
RS
NE−AL
PARSE−AL
alter−MTAL
ranks−MTAL
constituents constituents
0.65 0.70 0.75 0.80
f−score
f−score
RS
NE−AL
PARSE−AL
alter−MTAL
ranks−MTAL
0.76 0.78 0.80 0.82 0.84
10000 20000 30000 40000
</figure>
<bodyText confidence="0.7733295">
ment curve of one task has a slope of (close to) zero.
Future work will focus on issues related to this.
</bodyText>
<sectionHeader confidence="0.999835" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999950269230769">
There is a large body of work on single-task AL ap-
proaches for many NLP tasks where the focus is
mainly on better, task-specific selection protocols
and methods to quantify the usefulness score in dif-
ferent scenarios. As to the tasks involved in our
scenario, several papers address AL for NER (Shen
et al., 2004; Hachey et al., 2005; Tomanek et al.,
2007) and syntactic parsing (Tang et al., 2001; Hwa,
2004; Baldridge and Osborne, 2004; Becker and Os-
borne, 2005). Further, there is some work on ques-
tions arising when AL is to be used in real-life anno-
tation scenarios, including impaired inter-annotator
agreement, stopping criteria for AL-driven annota-
tion, and issues of reusability (Baldridge and Os-
borne, 2004; Hachey et al., 2005; Zhu and Hovy,
2007; Tomanek et al., 2007).
Multi-task AL is methodologically related to ap-
proaches of decision combination, especially in the
context of classifier combination (Ho et al., 1994)
and ensemble methods (Breiman, 1996). Those ap-
proaches focus on the combination of classifiers in
order to improve the classification error rate for one
specific classification task. In contrast, the focus of
multi-task AL is on strategies to select training ma-
terial for multi classifier systems where all classifiers
cover different classification tasks.
</bodyText>
<sectionHeader confidence="0.999744" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999837666666667">
Our treatment of MTAL within the context of the
orthogonal two-task scenario leads to further inter-
esting research questions. First, future investiga-
tions will have to focus on the question whether
the positive results observed in our orthogonal (i.e.,
highly dissimilar) two-task scenario will also hold
for a more realistic (and maybe more complex) mul-
tiple annotation scenario where tasks are more sim-
ilar and more than two annotation tasks might be
involved. Furthermore, several forms of interde-
pendencies may arise between the single annotation
tasks. As a first example, consider the (functional)
interdependencies (i.e., task similarity) in higher-
level semantic NLP tasks of relation or event recog-
nition. In such a scenario, several tasks including
entity annotations and relation/event annotations, as
well as syntactic parse data, have to be incorporated
at the same time. Another type of (data flow) inter-
</bodyText>
<page confidence="0.977385">
867
</page>
<figure confidence="0.998902055555556">
tokens
constituents
10000 20000 30000 40000
disagreement
0.010 0.014 0.018
RS
NE−AL
PARSE−AL
alter−MTAL
ranks−MTAL
10000 20000 30000 40000
disagreement
5 10 15 20 25 30 35 40
RS
NE−AL
PARSE−AL
alter−MTAL
ranks−MTAL
</figure>
<figureCaption confidence="0.999966">
Figure 4: Disagreement curves for NE task (left) and parse task (right) on WSJ
</figureCaption>
<bodyText confidence="0.999977730769231">
dependency occurs in a second scenario where ma-
terial for several classifiers that are data-dependent
on each other – one takes the output of another clas-
sifier as input features – has to be efficiently anno-
tated. Whether the proposed protocols are beneficial
in the context of such highly interdependent tasks is
an open issue. Even more challenging is the idea
to provide methodologies helping to predict in an
arbitrary application scenario whether the choice of
MTAL is truly advantageous.
Another open question is how to measure and
quantify the overall annotation costs in multiple an-
notation scenarios. Exchange rates are inherently
tied to the specific task and domain. In practice, one
might just want to measure the time needed for the
annotations. However, in a simulation scenario, a
common metric is necessary to compare the perfor-
mance of different selection strategies with respect
to the overall annotation costs. This requires stud-
ies on how to quantify, with a comparable cost func-
tion, the efforts needed for the annotation of a textual
unit of choice (e.g., tokens, sentences) with respect
to different annotation tasks.
Finally, the question of reusability of the anno-
tated material is an important issue. Reusability in
the context of AL means to which degree corpora
assembled with the help of any AL technique can be
(re)used as a general resource, i.e., whether they are
well suited for the training of classifiers other than
the ones used during the selection process.This is
especially interesting as the details of the classifiers
that should be trained in a later stage are typically
not known at the resource building time. Thus, we
want to select samples valuable to a family of clas-
sifiers using the various annotation layers. This, of
course, is only possible if data annotated with the
help of AL is reusable by modified though similar
classifiers (e.g., with respect to the features being
used) – compared to the classifiers employed for the
selection procedure.
The issue of reusability has already been raised
but not yet conclusively answered in the context of
single-task AL (see Section 6). Evidence was found
that reusability up to a certain, though not well-
specified, level is possible. Of course, reusability
has to be analyzed separately in the context of var-
ious MTAL scenarios. We feel that these scenarios
might both be more challenging and more relevant
to the reusability issue than the single-task AL sce-
nario, since resources annotated with multiple lay-
ers can be used to the design of a larger number of a
(possibly more complex) learning algorithms.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999947818181818">
We proposed an extension to the single-task AL ap-
proach such that it can be used to select examples for
annotation with respect to several annotation tasks.
To the best of our knowledge this is the first paper on
this issue, with a focus on NLP tasks. We outlined
a problem definition and described a framework for
multi-task AL. We presented and tested two proto-
cols for multi-task AL. Our results are promising as
they give evidence that in a multiple annotation sce-
nario, multi-task AL outperforms naive one-sided
and random selection.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997478">
The work of the second author was funded by the
German Ministry of Education and Research within
the STEMNET project (01DS001A-C), while the
work of the third author was funded by the EC
within the BOOTSTREP project (FP6-028099).
</bodyText>
<page confidence="0.997132">
868
</page>
<sectionHeader confidence="0.998271" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99925916091954">
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
EMNLP’04, pages 9–16.
Markus Becker and Miles Osborne. 2005. A two-stage
method for active learning of statistical grammars. In
Proceedings ofIJCAI’05, pages 991–996.
Daniel M. Bickel. 2005. Code developed at the Univer-
sity of Pennsylvania, http://www.cis.upenn.
edu/˜dbikel/software.html.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123–140.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal ofArti�cial Intelligence Research, 4:129–145.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings ofACL’96, pages 319–326.
Yoav Freund, Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine Learning, 28(2-
3):133–168.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of CoNLL’05, pages
144–151.
Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier sys-
tems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 16(1):66–75.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253–276.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings ofICML’01, pages 282–289.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind K. Joshi. 2008. Sense annotation in the penn
discourse treebank. In Proceedings of CICLing’08,
pages 275–286.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of ACL’00,
pages 117–125.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA corpus: An annotated research abstract
corpus in molecular biology domain. In Proceedings
ofHLT’02, pages 82–86.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
106.
Roi Reichart and Ari Rappoport. 2007. An ensemble
method for selection of high quality parses. In Pro-
ceedings ofACL’07, pages 408–415, June.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings ofJNLPBA’04, pages 107–110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In Proceedings
ofACL’04, pages 589–596.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Ac-
tive learning for statistical natural language parsing. In
Proceedings ofACL’02, pages 120–127.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CONLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL’03, pages 142–147.
Katrin Tomanek and Udo Hahn. 2008. Approximating
learning curves for active-learning-driven annotation.
In Proceedings ofLREC’08.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains corpus reusabil-
ity of annotated data. In Proceedings of EMNLP-
CoNLL’07, pages 486–495.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of
EMNLP-CoNLL’07, pages 783–790.
</reference>
<page confidence="0.998919">
869
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.648546">
<title confidence="0.999954">Multi-Task Active Learning for Linguistic Annotations</title>
<author confidence="0.999212">Katrin Udo Ari</author>
<affiliation confidence="0.993653">of Computer Science Hebrew University of Jerusalem, Israel University Language &amp; Information Engineering Lab</affiliation>
<address confidence="0.663656">Friedrich-Schiller-Universit¨at Jena, Germany</address>
<abstract confidence="0.999596941176471">We extend the classical single-task active learning (AL) approach. In the multi-task active learning (MTAL) paradigm, we select examples for several annotation tasks rather than for a single one as usually done in the context of AL. We introduce two MTAL metaprotocols, alternating selection and rank combination, and propose a method to implement them in practice. We experiment with a twotask annotation scenario that includes named entity and syntactic parse tree annotations on three different corpora. MTAL outperforms random selection and a stronger baseline, onesided example selection, in which one task is pursued using AL and the selected examples are provided also to the other task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning and the total cost of annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="26117" citStr="Baldridge and Osborne, 2004" startWordPosition="4268" endWordPosition="4271">lter−MTAL ranks−MTAL 0.76 0.78 0.80 0.82 0.84 10000 20000 30000 40000 ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in order </context>
</contexts>
<marker>Baldridge, Osborne, 2004</marker>
<rawString>Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Proceedings of EMNLP’04, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Becker</author>
<author>Miles Osborne</author>
</authors>
<title>A two-stage method for active learning of statistical grammars.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCAI’05,</booktitle>
<pages>991--996</pages>
<contexts>
<context position="26144" citStr="Becker and Osborne, 2005" startWordPosition="4272" endWordPosition="4276">8 0.80 0.82 0.84 10000 20000 30000 40000 ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in order to improve the classificati</context>
</contexts>
<marker>Becker, Osborne, 2005</marker>
<rawString>Markus Becker and Miles Osborne. 2005. A two-stage method for active learning of statistical grammars. In Proceedings ofIJCAI’05, pages 991–996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bickel</author>
</authors>
<title>Code developed at the</title>
<date>2005</date>
<institution>University of Pennsylvania,</institution>
<note>http://www.cis.upenn. edu/˜dbikel/software.html.</note>
<contexts>
<context position="17122" citStr="Bickel, 2005" startWordPosition="2772" endWordPosition="2773"> the AL approach of Tomanek et al. (2007). The committee consists of k1 = 3 classifiers and the vote entropy (VE) (Engelson and Dagan, 1996) is employed as disagreement metric. It is calculated on the token-level as V Etok(t) = − 1 c V (li, t) log V (li, t) (1) log k E k k where V (l&amp;quot;t) kis the ratio of k classifiers where the label li is assigned to a token t. The sentence level vote entropy V Esent is then the average over all tokens tj of sentence s. For the parsing task, the disagreement score is based on a committee of k2 = 10 instances of Dan Bikel’s reimplementation of Collins’ parser (Bickel, 2005; Collins, 1999). For each sentence in the unlabeled pool, the agreement between the committee members was calculated using the function reported by Reichart and Rappoport (2007): 1 � fscore(mi, ml) (2) AF (s) = N i,l∈[1...N],i=,4l Where mi and ml are the committee members and N = k2�(k2−1) 2 is the number of pairs of different committee members. This function calculates the agreement between the members of each pair by calculating their relative f-score and then averages the pairs’ scores. The disagreement of the committee on a sentence is simply 1 − AF(s). 4.2 Experimental settings For the N</context>
</contexts>
<marker>Bickel, 2005</marker>
<rawString>Daniel M. Bickel. 2005. Code developed at the University of Pennsylvania, http://www.cis.upenn. edu/˜dbikel/software.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="26649" citStr="Breiman, 1996" startWordPosition="4353" endWordPosition="4354">) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in order to improve the classification error rate for one specific classification task. In contrast, the focus of multi-task AL is on strategies to select training material for multi classifier systems where all classifiers cover different classification tasks. 7 Discussion Our treatment of MTAL within the context of the orthogonal two-task scenario leads to further interesting research questions. First, future investigations will have to focus on the question whether the positive results observed in our orthogonal (i.e., highly dissim</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cohn</author>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Active learning with statistical models.</title>
<date>1996</date>
<journal>Journal ofArti�cial Intelligence Research,</journal>
<pages>4--129</pages>
<contexts>
<context position="2168" citStr="Cohn et al., 1996" startWordPosition="312" endWordPosition="315">ific biology journal articles), extensive retraining on newly supplied training material is often required, since different domains may use different syntactic structures as well as different semantic classes (entities and relations). * Both authors contributed equally to this work. Consequently, with an increasing coverage of a wide variety of domains in human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under </context>
<context position="7530" citStr="Cohn et al., 1996" startWordPosition="1180" endWordPosition="1183"> Framework for Multi-Task AL In this section we introduce a sample selection framework that aims at reducing the human annotation effort in a multiple annotation scenario. 2.1 Task Definition To measure the efficiency of selection methods, we define the training quality TQ of annotated material 5 as the performance p yielded with a reference learner X trained on that material: TQ(X, 5) = p. A selection method can be considered better than another one if a higher TQ is yielded with the same amount of examples being annotated. Our framework is an extension of the Active Learning (AL) framework (Cohn et al., 1996)). The original AL framework is based on querying in an iterative manner those examples to be manually annotated that are most useful for the learner at hand. The TQ of an annotated corpus selected by means of AL is much higher than random selection. This AL approach can be considered as single-task AL because it focuses on a single learner for which the examples are to be selected. In a multiple annotation scenario, however, there are several annotation tasks to be accomplished at once and for each task typically a separate statistical model will then be trained. Thus, the goal of multi-task </context>
</contexts>
<marker>Cohn, Ghahramani, Jordan, 1996</marker>
<rawString>David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. 1996. Active learning with statistical models. Journal ofArti�cial Intelligence Research, 4:129–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="17138" citStr="Collins, 1999" startWordPosition="2774" endWordPosition="2775">ch of Tomanek et al. (2007). The committee consists of k1 = 3 classifiers and the vote entropy (VE) (Engelson and Dagan, 1996) is employed as disagreement metric. It is calculated on the token-level as V Etok(t) = − 1 c V (li, t) log V (li, t) (1) log k E k k where V (l&amp;quot;t) kis the ratio of k classifiers where the label li is assigned to a token t. The sentence level vote entropy V Esent is then the average over all tokens tj of sentence s. For the parsing task, the disagreement score is based on a committee of k2 = 10 instances of Dan Bikel’s reimplementation of Collins’ parser (Bickel, 2005; Collins, 1999). For each sentence in the unlabeled pool, the agreement between the committee members was calculated using the function reported by Reichart and Rappoport (2007): 1 � fscore(mi, ml) (2) AF (s) = N i,l∈[1...N],i=,4l Where mi and ml are the committee members and N = k2�(k2−1) 2 is the number of pairs of different committee members. This function calculates the agreement between the members of each pair by calculating their relative f-score and then averages the pairs’ scores. The disagreement of the committee on a sentence is simply 1 − AF(s). 4.2 Experimental settings For the NE task we employ</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL’96,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="2494" citStr="Engelson and Dagan, 1996" startWordPosition="366" endWordPosition="369">verage of a wide variety of domains in human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a</context>
<context position="16650" citStr="Engelson and Dagan, 1996" startWordPosition="2674" endWordPosition="2678">nd et al., 1997) selection protocols. In committee-based AL, a committee consists of k classifiers of the same type trained on different subsets of the training data.5 Each committee member then makes its predictions on the unlabeled examples, and those examples on which the committee members disagree most are considered most informative for learning and are thus selected for manual annotation. In our scenario the example grain-size is the sentence level. For the NE task, we apply the AL approach of Tomanek et al. (2007). The committee consists of k1 = 3 classifiers and the vote entropy (VE) (Engelson and Dagan, 1996) is employed as disagreement metric. It is calculated on the token-level as V Etok(t) = − 1 c V (li, t) log V (li, t) (1) log k E k k where V (l&amp;quot;t) kis the ratio of k classifiers where the label li is assigned to a token t. The sentence level vote entropy V Esent is then the average over all tokens tj of sentence s. For the parsing task, the disagreement score is based on a committee of k2 = 10 instances of Dan Bikel’s reimplementation of Collins’ parser (Bickel, 2005; Collins, 1999). For each sentence in the unlabeled pool, the agreement between the committee members was calculated using the </context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In Proceedings ofACL’96, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="16041" citStr="Freund et al., 1997" startWordPosition="2576" endWordPosition="2579">y is a subject for further research. 4 Experiments 4.1 Scenario and Task-Specific Selection Protocols The tasks in our scenario comprise one semantic task (annotation with named entities (NE)) and one syntactic task (annotation with PCFG parse trees). The tasks are highly dissimilar, thus increasing the potential value of MTAL. Both tasks are subject to intensive research by the NLP community. The MTAL protocols proposed are metaprotocols that combine the selection decisions of the underlying, task-specific AL protocols. In our scenario, the task-specific AL protocols are 864 committee-based (Freund et al., 1997) selection protocols. In committee-based AL, a committee consists of k classifiers of the same type trained on different subsets of the training data.5 Each committee member then makes its predictions on the unlabeled examples, and those examples on which the committee members disagree most are considered most informative for learning and are thus selected for manual annotation. In our scenario the example grain-size is the sentence level. For the NE task, we apply the AL approach of Tomanek et al. (2007). The committee consists of k1 = 3 classifiers and the vote entropy (VE) (Engelson and Dag</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Beatrice Alex</author>
<author>Markus Becker</author>
</authors>
<title>Investigating the effects of selective sampling on the annotation task.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL’05,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="26013" citStr="Hachey et al., 2005" startWordPosition="4251" endWordPosition="4254">TAL ranks−MTAL constituents constituents 0.65 0.70 0.75 0.80 f−score f−score RS NE−AL PARSE−AL alter−MTAL ranks−MTAL 0.76 0.78 0.80 0.82 0.84 10000 20000 30000 40000 ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) </context>
</contexts>
<marker>Hachey, Alex, Becker, 2005</marker>
<rawString>Ben Hachey, Beatrice Alex, and Markus Becker. 2005. Investigating the effects of selective sampling on the annotation task. In Proceedings of CoNLL’05, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tin Kam Ho</author>
<author>Jonathan J Hull</author>
<author>Sargur N Srihari</author>
</authors>
<title>Decision combination in multiple classifier systems.</title>
<date>1994</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="26612" citStr="Ho et al., 1994" startWordPosition="4346" endWordPosition="4349">chey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in order to improve the classification error rate for one specific classification task. In contrast, the focus of multi-task AL is on strategies to select training material for multi classifier systems where all classifiers cover different classification tasks. 7 Discussion Our treatment of MTAL within the context of the orthogonal two-task scenario leads to further interesting research questions. First, future investigations will have to focus on the question whether the positive results observed i</context>
</contexts>
<marker>Ho, Hull, Srihari, 1994</marker>
<rawString>Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari. 1994. Decision combination in multiple classifier systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(1):66–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="2563" citStr="Hwa, 2004" startWordPosition="377" endWordPosition="378">an expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might use features extracted from parse t</context>
<context position="26088" citStr="Hwa, 2004" startWordPosition="4266" endWordPosition="4267"> PARSE−AL alter−MTAL ranks−MTAL 0.76 0.78 0.80 0.82 0.84 10000 20000 30000 40000 ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combina</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>Rebecca Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML’01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="17867" citStr="Lafferty et al., 2001" startWordPosition="2895" endWordPosition="2898"> the function reported by Reichart and Rappoport (2007): 1 � fscore(mi, ml) (2) AF (s) = N i,l∈[1...N],i=,4l Where mi and ml are the committee members and N = k2�(k2−1) 2 is the number of pairs of different committee members. This function calculates the agreement between the members of each pair by calculating their relative f-score and then averages the pairs’ scores. The disagreement of the committee on a sentence is simply 1 − AF(s). 4.2 Experimental settings For the NE task we employed the classifier described by Tomanek et al. (2007): The NE tagger is based on Conditional Random Fields (Lafferty et al., 2001) 5We randomly sampled L = 34 of the training data to create each committee member. and has a rich feature set including orthographical, lexical, morphological, POS, and contextual features. For parsing, Dan Bikel’s reimplementation of Collins’ parser is employed, using gold POS tags. In each AL iteration we select 100 sentences for manual annotation.6 We start with a randomly chosen seed set of 200 sentences. Within a corpus we used the same seed set in all selection scenarios. We compare the following five selection scenarios: Random selection (RS), which serves as our baseline; one-sided AL </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML’01, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5181" citStr="Marcus et al., 1993" startWordPosition="797" endWordPosition="800">ding classifier. As a consequence, training such a classifier which takes into account several annotation tasks will best be performed on a rich corpus annotated with respect to all inputrelevant tasks. Both kinds of annotation tasks, similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as</context>
<context position="19136" citStr="Marcus et al., 1993" startWordPosition="3097" endWordPosition="3100">AL); and multi-task AL selection with the alternating selection protocol (alter-MTAL) and the rank combination protocol (ranks-MTAL). We performed our experiments on three different corpora, namely one from the newspaper genre (WSJ), a mixed-genre corpus (Brown), and a biomedical corpus (Bio). Our simulation corpora contain both entity annotations and (constituent) parse annotations. For each corpus we have a pool set (from which we select the examples for annotation) and an evaluation set (used for generating the learning curves). The WSJ corpus is based on the WSJ part of the PENN TREEBANK (Marcus et al., 1993); we used the first 10,000 sentences of section 2-21 as the pool set, and section 00 as evaluation set (1,921 sentences). The Brown corpus is also based on the respective part of the PENN TREEBANK. We created a sample consisting of 8 of any 10 consecutive sentences in the corpus. This was done as Brown contains text from various English text genres, and we did that to create a representative sample of the corpus domains. We finally selected the first 10,000 sentences from this sample as pool set. Every 9th from every 10 consecutive sentences package went into the evaluation set which consists </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Alan Lee</author>
<author>Aravind K Joshi</author>
</authors>
<title>Sense annotation in the penn discourse treebank.</title>
<date>2008</date>
<booktitle>In Proceedings of CICLing’08,</booktitle>
<pages>275--286</pages>
<contexts>
<context position="5325" citStr="Miltsakaki et al., 2008" startWordPosition="820" endWordPosition="824">rich corpus annotated with respect to all inputrelevant tasks. Both kinds of annotation tasks, similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as possible for a set of classifiers instead of a single classifier only. This enables the creation of a single combined corpus annotated with res</context>
</contexts>
<marker>Miltsakaki, Robaldo, Lee, Joshi, 2008</marker>
<rawString>Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Aravind K. Joshi. 2008. Sense annotation in the penn discourse treebank. In Proceedings of CICLing’08, pages 275–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL’00,</booktitle>
<pages>117--125</pages>
<contexts>
<context position="2530" citStr="Ngai and Yarowsky, 2000" startWordPosition="371" endWordPosition="374">n human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might us</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of ACL’00, pages 117–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Jin-Dong Kim</author>
</authors>
<title>The GENIA corpus: An annotated research abstract corpus in molecular biology domain.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT’02,</booktitle>
<pages>82--86</pages>
<contexts>
<context position="5468" citStr="Ohta et al., 2002" startWordPosition="844" endWordPosition="847">at we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as possible for a set of classifiers instead of a single classifier only. This enables the creation of a single combined corpus annotated with respect to various annotation tasks, while preserving the advantages of AL with respect to the minimization of annotation efforts. In a proof-of-c</context>
<context position="20448" citStr="Ohta et al., 2002" startWordPosition="3325" endWordPosition="3328"> were available. Thus, we enriched both corpora with entity annotations (three entities: person, location, and organization) by means of a tagger trained on the English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003).7 The Bio corpus 6Manual annotation is simulated by just unveiling the annotations already contained in our corpora. 7We employed a tagger similar to the one presented by Settles (2004). Our tagger has a performance of R� 84% f-score on the CoNLL-2003 data; inspection of the predicted entities on 865 is based on the parsed section of the GENIA corpus (Ohta et al., 2002). We performed the same divisions as for Brown, resulting in 2,213 sentences in our pool set and 276 sentences for the evaluation set. This part of the GENIA corpus comes with entity annotations. We have collapsed the entity classes annotated in GENIA (cell line, cell type, DNA, RNA, protein) into a single, biological entity class. 5 Results In this section we present and discuss our results when applying the five selection strategies (RS, NEAL, PARSE-AL, alter-MTAL, and ranks-MTAL) to our scenario on the three corpora. We refrain from calculating the overall efficiency score (Section 3) here </context>
</contexts>
<marker>Ohta, Tateisi, Kim, 2002</marker>
<rawString>Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002. The GENIA corpus: An annotated research abstract corpus in molecular biology domain. In Proceedings ofHLT’02, pages 82–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Paul Kingsbury</author>
<author>Daniel Gildea</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="5416" citStr="Palmer et al., 2005" startWordPosition="835" endWordPosition="838">similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as possible for a set of classifiers instead of a single classifier only. This enables the creation of a single combined corpus annotated with respect to various annotation tasks, while preserving the advantages of AL with respect to the</context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>An ensemble method for selection of high quality parses.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL’07,</booktitle>
<pages>408--415</pages>
<contexts>
<context position="17300" citStr="Reichart and Rappoport (2007)" startWordPosition="2797" endWordPosition="2800">reement metric. It is calculated on the token-level as V Etok(t) = − 1 c V (li, t) log V (li, t) (1) log k E k k where V (l&amp;quot;t) kis the ratio of k classifiers where the label li is assigned to a token t. The sentence level vote entropy V Esent is then the average over all tokens tj of sentence s. For the parsing task, the disagreement score is based on a committee of k2 = 10 instances of Dan Bikel’s reimplementation of Collins’ parser (Bickel, 2005; Collins, 1999). For each sentence in the unlabeled pool, the agreement between the committee members was calculated using the function reported by Reichart and Rappoport (2007): 1 � fscore(mi, ml) (2) AF (s) = N i,l∈[1...N],i=,4l Where mi and ml are the committee members and N = k2�(k2−1) 2 is the number of pairs of different committee members. This function calculates the agreement between the members of each pair by calculating their relative f-score and then averages the pairs’ scores. The disagreement of the committee on a sentence is simply 1 − AF(s). 4.2 Experimental settings For the NE task we employed the classifier described by Tomanek et al. (2007): The NE tagger is based on Conditional Random Fields (Lafferty et al., 2001) 5We randomly sampled L = 34 of t</context>
</contexts>
<marker>Reichart, Rappoport, 2007</marker>
<rawString>Roi Reichart and Ari Rappoport. 2007. An ensemble method for selection of high quality parses. In Proceedings ofACL’07, pages 408–415, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Biomedical named entity recognition using conditional random fields and rich feature sets.</title>
<date>2004</date>
<booktitle>In Proceedings ofJNLPBA’04,</booktitle>
<pages>107--110</pages>
<contexts>
<context position="20261" citStr="Settles (2004)" startWordPosition="3292" endWordPosition="3294"> from every 10 consecutive sentences package went into the evaluation set which consists of 2,424 sentences. For both WSJ and Brown only parse annotations though no entity annotations were available. Thus, we enriched both corpora with entity annotations (three entities: person, location, and organization) by means of a tagger trained on the English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003).7 The Bio corpus 6Manual annotation is simulated by just unveiling the annotations already contained in our corpora. 7We employed a tagger similar to the one presented by Settles (2004). Our tagger has a performance of R� 84% f-score on the CoNLL-2003 data; inspection of the predicted entities on 865 is based on the parsed section of the GENIA corpus (Ohta et al., 2002). We performed the same divisions as for Brown, resulting in 2,213 sentences in our pool set and 276 sentences for the evaluation set. This part of the GENIA corpus comes with entity annotations. We have collapsed the entity classes annotated in GENIA (cell line, cell type, DNA, RNA, protein) into a single, biological entity class. 5 Results In this section we present and discuss our results when applying the </context>
</contexts>
<marker>Settles, 2004</marker>
<rawString>Burr Settles. 2004. Biomedical named entity recognition using conditional random fields and rich feature sets. In Proceedings ofJNLPBA’04, pages 107–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
<author>Chew Lim Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL’04,</booktitle>
<pages>589--596</pages>
<contexts>
<context position="25992" citStr="Shen et al., 2004" startWordPosition="4247" endWordPosition="4250">AL PARSE−AL alter−MTAL ranks−MTAL constituents constituents 0.65 0.70 0.75 0.80 f−score f−score RS NE−AL PARSE−AL alter−MTAL ranks−MTAL 0.76 0.78 0.80 0.82 0.84 10000 20000 30000 40000 ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combinati</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew Lim Tan. 2004. Multi-criteria-based active learning for named entity recognition. In Proceedings ofACL’04, pages 589–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Tang</author>
<author>Xiaoqiang Luo</author>
<author>Salim Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL’02,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="26077" citStr="Tang et al., 2001" startWordPosition="4262" endWordPosition="4265">re f−score RS NE−AL PARSE−AL alter−MTAL ranks−MTAL 0.76 0.78 0.80 0.82 0.84 10000 20000 30000 40000 ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on </context>
</contexts>
<marker>Tang, Luo, Roukos, 2001</marker>
<rawString>Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Active learning for statistical natural language parsing. In Proceedings ofACL’02, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL’03,</booktitle>
<pages>142--147</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CONLL-2003 shared task: Language-independent named entity recognition. In Proceedings of CoNLL’03, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
</authors>
<title>Approximating learning curves for active-learning-driven annotation.</title>
<date>2008</date>
<booktitle>In Proceedings ofLREC’08.</booktitle>
<contexts>
<context position="24272" citStr="Tomanek and Hahn (2008)" startWordPosition="3956" endWordPosition="3959"> selection can be a better choice than one-sided selection in multiple annotation scenarios. Thus, considering all annotation tasks in the selection process (even if the selection protocol is as simple as the alternating selection protocol) is better than selecting only with respect to one task. Further, it should be noted that overall the more sophisticated rank combination protocol does not perform much better than the simpler alternating selection protocol in all scenarios. Finally, Figure 4 shows the disagreement curves for the two tasks on the WSJ corpus. As has already been discussed by Tomanek and Hahn (2008), disagreement curves can be used as a stopping criterion and to monitor the progress of AL-driven annotation. This is especially valuable when no annotated validation set is available (which is needed for plotting learning curves). We can see that the disagreement curves significantly flatten approximately at the same time as the learning curves do. In the context of MTAL, disagreement curves might not only be interesting as a stopping criterion but rather as a switching criterion, i.e., to identify when MTAL could be turned into one-sided selection. This would be the case if in an MTAL scena</context>
</contexts>
<marker>Tomanek, Hahn, 2008</marker>
<rawString>Katrin Tomanek and Udo Hahn. 2008. Approximating learning curves for active-learning-driven annotation. In Proceedings ofLREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>An approach to text corpus construction which cuts annotation costs and maintains corpus reusability of annotated data.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL’07,</booktitle>
<pages>486--495</pages>
<contexts>
<context position="2616" citStr="Tomanek et al., 2007" startWordPosition="383" endWordPosition="386">tions to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might use features extracted from parse trees and named entity annotations. For such an applic</context>
<context position="16551" citStr="Tomanek et al. (2007)" startWordPosition="2657" endWordPosition="2660">fic AL protocols. In our scenario, the task-specific AL protocols are 864 committee-based (Freund et al., 1997) selection protocols. In committee-based AL, a committee consists of k classifiers of the same type trained on different subsets of the training data.5 Each committee member then makes its predictions on the unlabeled examples, and those examples on which the committee members disagree most are considered most informative for learning and are thus selected for manual annotation. In our scenario the example grain-size is the sentence level. For the NE task, we apply the AL approach of Tomanek et al. (2007). The committee consists of k1 = 3 classifiers and the vote entropy (VE) (Engelson and Dagan, 1996) is employed as disagreement metric. It is calculated on the token-level as V Etok(t) = − 1 c V (li, t) log V (li, t) (1) log k E k k where V (l&amp;quot;t) kis the ratio of k classifiers where the label li is assigned to a token t. The sentence level vote entropy V Esent is then the average over all tokens tj of sentence s. For the parsing task, the disagreement score is based on a committee of k2 = 10 instances of Dan Bikel’s reimplementation of Collins’ parser (Bickel, 2005; Collins, 1999). For each se</context>
<context position="17790" citStr="Tomanek et al. (2007)" startWordPosition="2882" endWordPosition="2885">beled pool, the agreement between the committee members was calculated using the function reported by Reichart and Rappoport (2007): 1 � fscore(mi, ml) (2) AF (s) = N i,l∈[1...N],i=,4l Where mi and ml are the committee members and N = k2�(k2−1) 2 is the number of pairs of different committee members. This function calculates the agreement between the members of each pair by calculating their relative f-score and then averages the pairs’ scores. The disagreement of the committee on a sentence is simply 1 − AF(s). 4.2 Experimental settings For the NE task we employed the classifier described by Tomanek et al. (2007): The NE tagger is based on Conditional Random Fields (Lafferty et al., 2001) 5We randomly sampled L = 34 of the training data to create each committee member. and has a rich feature set including orthographical, lexical, morphological, POS, and contextual features. For parsing, Dan Bikel’s reimplementation of Collins’ parser is employed, using gold POS tags. In each AL iteration we select 100 sentences for manual annotation.6 We start with a randomly chosen seed set of 200 sentences. Within a corpus we used the same seed set in all selection scenarios. We compare the following five selection </context>
<context position="26036" citStr="Tomanek et al., 2007" startWordPosition="4255" endWordPosition="4258">tuents constituents 0.65 0.70 0.75 0.80 f−score f−score RS NE−AL PARSE−AL alter−MTAL ranks−MTAL 0.76 0.78 0.80 0.82 0.84 10000 20000 30000 40000 ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (B</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. An approach to text corpus construction which cuts annotation costs and maintains corpus reusability of annotated data. In Proceedings of EMNLPCoNLL’07, pages 486–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Eduard Hovy</author>
</authors>
<title>Active learning for word sense disambiguation with methods for addressing the class imbalance problem.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL’07,</booktitle>
<pages>783--790</pages>
<contexts>
<context position="26437" citStr="Zhu and Hovy, 2007" startWordPosition="4320" endWordPosition="4323">protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in order to improve the classification error rate for one specific classification task. In contrast, the focus of multi-task AL is on strategies to select training material for multi classifier systems where all classifiers cover different classification tasks. 7 Discussion Our treatment of MTAL within the context of the orthog</context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>Jingbo Zhu and Eduard Hovy. 2007. Active learning for word sense disambiguation with methods for addressing the class imbalance problem. In Proceedings of EMNLP-CoNLL’07, pages 783–790.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>