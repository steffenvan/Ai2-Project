<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.994494">
Chunking with Support Vector Machines
</title>
<author confidence="0.992876">
Taku Kudo and Yuji Matsumoto
</author>
<affiliation confidence="0.996716">
Graduate School of Information Science,
Nara Institute of Science and Technology
</affiliation>
<email confidence="0.992524">
taku-ku,matsu@is.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.99374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998275">
We apply Support Vector Machines (SVMs) to
identify English base phrases (chunks). SVMs
are known to achieve high generalization perfor-
mance even with input data of high dimensional
feature spaces. Furthermore, by the Kernel princi-
ple, SVMs can carry out training with smaller com-
putational overhead independent of their dimen-
sionality. We apply weighted voting of 8 SVMs-
based systems trained with distinct chunk repre-
sentations. Experimental results show that our ap-
proach achieves higher accuracy than previous ap-
proaches.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965763636364">
Chunking is recognized as series of processes —
first identifying proper chunks from a sequence of
tokens (such as words), and second classifying these
chunks into some grammatical classes. Various
NLP tasks can be seen as a chunking task. Exam-
ples include English base noun phrase identification
(base NP chunking), English base phrase identifica-
tion (chunking), Japanese chunk (bunsetsu) identi-
fication and named entity extraction. Tokenization
and part-of-speech tagging can also be regarded as
a chunking task, if we assume each character as a
token.
Machine learning techniques are often applied to
chunking, since the task is formulated as estimating
an identifying function from the information (fea-
tures) available in the surrounding context. Various
machine learning approaches have been proposed
for chunking (Ramshaw and Marcus, 1995; Tjong
Kim Sang, 2000a; Tjong Kim Sang et al., 2000;
Tjong Kim Sang, 2000b; Sassano and Utsuro, 2000;
van Halteren, 2000).
Conventional machine learning techniques, such
as Hidden Markov Model (HMM) and Maximum
Entropy Model (ME), normally require a careful
feature selection in order to achieve high accuracy.
They do not provide a method for automatic selec-
tion of given feature sets. Usually, heuristics are
used for selecting effective features and their com-
binations.
New statistical learning techniques such as Sup-
port Vector Machines (SVMs) (Cortes and Vap-
nik, 1995; Vapnik, 1998) and Boosting(Freund and
Schapire, 1996) have been proposed. These tech-
niques take a strategy that maximizes the margin
between critical samples and the separating hyper-
plane. In particular, SVMs achieve high generaliza-
tion even with training data of a very high dimen-
sion. Furthermore, by introducing the Kernel func-
tion, SVMs handle non-linear feature spaces, and
carry out the training considering combinations of
more than one feature.
In the field of natural language processing, SVMs
are applied to text categorization and syntactic de-
pendency structure analysis, and are reported to
have achieved higher accuracy than previous ap-
proaches.(Joachims, 1998; Taira and Haruno, 1999;
Kudo and Matsumoto, 2000a).
In this paper, we apply Support Vector Machines
to the chunking task. In addition, in order to achieve
higher accuracy, we apply weighted voting of 8
SVM-based systems which are trained using dis-
tinct chunk representations. For the weighted vot-
ing systems, we introduce a new type of weighting
strategy which are derived from the theoretical basis
of the SVMs.
</bodyText>
<sectionHeader confidence="0.9617" genericHeader="method">
2 Support Vector Machines
</sectionHeader>
<subsectionHeader confidence="0.909436">
2.1 Optimal Hyperplane
</subsectionHeader>
<bodyText confidence="0.767876833333333">
Let us define the training samples each of which
belongs either to positive or negative class as:
is a feature vector of the-th sample repre-
sented by an dimensional vector. is the class
(positive( ) or negative( ) class) label of the-
th sample.is the number of the given training sam-
</bodyText>
<note confidence="0.708331">
Small Margin Large Margin
</note>
<figureCaption confidence="0.998103">
Figure 1: Two possible separating hyperplanes
</figureCaption>
<bodyText confidence="0.999415591836735">
ples. In the basic SVMs framework, we try to sep-
arate the positive and negative samples by a hyper-
plane expressed as:
. SVMs find an “optimal” hyperplane (i.e. an
optimal parameter set for ) which separates the
training data into two classes. What does “optimal”
mean? In order to define it, we need to consider
the margin between two classes. Figure 1 illus-
trates this idea. Solid lines show two possible hyper-
planes, each of which correctly separates the train-
ing data into two classes. Two dashed lines paral-
lel to the separating hyperplane indicate the bound-
aries in which one can move the separating hyper-
plane without any misclassification. We call the dis-
tance between those parallel dashed lines as mar-
gin. SVMs find the separating hyperplane which
maximizes its margin. Precisely, two dashed lines
and margin ( ) can be expressed as:
.
To maximize this margin, we should minimize
. In other words, this problem becomes equiva-
lent to solving the following optimization problem:
The training samples which lie on either of two
dashed lines are called support vectors. It is known
that only the support vectors in given training data
matter. This implies that we can obtain the same de-
cision function even if we remove all training sam-
ples except for the extracted support vectors.
In practice, even in the case where we cannot sep-
arate training data linearly because of some noise
in the training data, etc, we can build the sep-
arating linear hyperplane by allowing some mis-
classifications. Though we omit the details here, we
can build an optimal hyperplane by introducing a
soft margin parameter , which trades off between
the training error and the magnitude of the margin.
Furthermore, SVMs have a potential to carry out
the non-linear classification. Though we leave the
details to (Vapnik, 1998), the optimization problem
can be rewritten into a dual form, where all feature
vectors appear in their dot products. By simply sub-
stituting every dot product of and in dual form
with a certain Kernel function , SVMs can
handle non-linear hypotheses. Among many kinds
of Kernel functions available, we will focus on the
-th polynomial kernel: .
Use of-th polynomial kernel functions allows us to
build an optimal separating hyperplane which takes
into account all combinations of features up to.
</bodyText>
<subsectionHeader confidence="0.998566">
2.2 Generalization Ability of SVMs
</subsectionHeader>
<bodyText confidence="0.962078333333334">
Statistical Learning Theory(Vapnik, 1998) states
that training error (empirical risk) and test error
(risk) hold the following theorem.
Theorem 1 (Vapnik) If is the VC dimen-
sion ofthe class functions implemented by some ma-
chine learning algorithms, then for all functions of
that class, with a probability of at least , the
risk is bounded by
where is a non-negative integer called the Vapnik
Chervonenkis (VC) dimension, and is a measure of
the complexity of the given decision function. The
r.h.s. term of (1) is called VC bound. In order to
minimize the risk, we have to minimize the empir-
ical risk as well as VC dimension. It is known that
the following theorem holds for VC dimension
and margin (Vapnik, 1998).
Theorem 2 (Vapnik) Suppose as the dimension
of given training samples as the margin, and
as the smallest diameter which encloses all train-
ing sample, then VC dimension of the SVMs are
bounded by
In order to minimize the VC dimension, we have
to maximize the margin , which is exactly the
strategy that SVMs take.
Vapnik gives an alternative bound for the risk.
Theorem 3 (Vapnik) Suppose is an error rate
estimated by Leave-One-Out procedure, is
bounded as
Leave-One-Out procedure is a simple method to ex-
amine the risk of the decision function — first by
removing a single sample from the training data, we
construct the decision function on the basis of the
remaining training data, and then test the removed
sample. In this fashion, we test allsamples of the
training data usingdifferent decision functions. (3)
is a natural consequence bearing in mind that sup-
port vectors are the only factors contributing to the
final decision function. Namely, when the every re-
moved support vector becomes error in Leave-One-
Out procedure, becomes the r.h.s. term of (3). In
practice, it is known that this bound is less predic-
tive than the VC bound.
</bodyText>
<table confidence="0.714664083333333">
IOB1 IOB2 IOE1 IOE2 Start/End
In O O O O O
early I B I I B
trading I I I E E
in O O O O O
busy I B I I B
Hong I I I I I
Kong I I E E E
Monday B B I E S
, O O O O O
gold I B I E S
was O O O O O
</table>
<tableCaption confidence="0.993753">
Table 1: Example for each chunk representation
</tableCaption>
<sectionHeader confidence="0.997047" genericHeader="method">
3 Chunking
</sectionHeader>
<subsectionHeader confidence="0.999797">
3.1 Chunk representation
</subsectionHeader>
<bodyText confidence="0.999678333333333">
There are mainly two types of representations for
proper chunks. One is Inside/Outside representa-
tion, and the other is Start/End representation.
</bodyText>
<listItem confidence="0.592524">
1. Inside/Outside
</listItem>
<bodyText confidence="0.991074">
This representation was first introduced in
(Ramshaw and Marcus, 1995), and has been
applied for base NP chunking. This method
uses the following set of three tags for repre-
senting proper chunks.
I Current token is inside of a chunk.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk
which immediately follows another chunk.
Tjong Kim Sang calls this method as IOB1
representation, and introduces three alternative
versions — IOB2,IOE1 and IOE2 (Tjong Kim
Sang and Veenstra, 1999).
IOB2 A B tag is given for every token which
exists at the beginning of a chunk.
Other tokens are the same as IOB1.
</bodyText>
<listItem confidence="0.920050166666667">
IOE1 An E tag is used to mark the last to-
ken of a chunk immediately preceding
another chunk.
IOE2 An E tag is given for every token
which exists at the end of a chunk.
2. Start/End
</listItem>
<bodyText confidence="0.986756173913044">
This method has been used for the Japanese
named entity extraction task, and requires the
following five tags for representing proper
chunks(Uchimoto et al., 2000) 1.
1Originally, Uchimoto uses C/E/U/O/S representation.
However we rename them as B/I/O/E/S for our purpose, since
B Current token is the start of a chunk con-
sisting of more than one token.
E Current token is the end of a chunk consist-
ing of more than one token.
I Current token is a middle of a chunk con-
sisting of more than two tokens.
S Current token is a chunk consisting of only
one token.
O Current token is outside of any chunk.
Examples of these five representations are shown
in Table 1.
If we have to identify the grammatical class of
each chunk, we represent them by a pair of an
I/O/B/E/S label and a class label. For example, in
IOB2 representation, B-VP label is given to a to-
ken which represents the beginning of a verb base
phrase (VP).
</bodyText>
<subsectionHeader confidence="0.999988">
3.2 Chunking with SVMs
</subsectionHeader>
<bodyText confidence="0.985649520547945">
Basically, SVMs are binary classifiers, thus we must
extend SVMs to multi-class classifiers in order to
classify three (B,I,O) or more (B,I,O,E,S) classes.
There are two popular methods to extend a binary
classification task to that of classes. One is one
class vs. all others. The idea is to build classi-
fiers so as to separate one class from all others. The
other is pairwise classification. The idea is to build
classifiers considering all pairs of
classes, and final decision is given by their weighted
voting. There are a number of other methods to ex-
tend SVMs to multiclass classifiers. For example,
Dietterich and Bakiri(Dietterich and Bakiri, 1995)
and Allwein(Allwein et al., 2000) introduce a uni-
fying framework for solving the multiclass problem
we want to keep consistency with Inside/Start (B/I/O) represen-
tation.
by reducing them into binary models. However, we
employ the simple pairwise classifiers because of
the following reasons:
(1) In general, SVMs require
training cost (where is the size of training data).
Thus, if the size of training data for individual bi-
nary classifiers is small, we can significantly reduce
the training cost. Although pairwise classifiers tend
to build a larger number of binary classifiers, the
training cost required for pairwise method is much
more tractable compared to the one vs. all others.
(2) Some experiments (Kreßel, 1999) report that
a combination of pairwise classifiers performs bet-
ter than the one vs. all others.
For the feature sets for actual training and classi-
fication of SVMs, we use all the information avail-
able in the surrounding context, such as the words,
their part-of-speech tags as well as the chunk labels.
More precisely, we give the following features to
identify the chunk label for the-th word:
Here,is the word appearing at-th position, is
the POS tag of, and is the (extended) chunk
label for-th word. In addition, we can reverse the
parsing direction (from right to left) by using two
chunk tags which appear to the r.h.s. of the current
token ( ). In this paper, we call the method
which parses from left to right as forward parsing,
and the method which parses from right to left as
backward parsing.
Since the preceding chunk labels ( for
forward parsing , for backward parsing)
are not given in the test data, they are decided dy-
namically during the tagging of chunk labels. The
technique can be regarded as a sort of Dynamic Pro-
gramming (DP) matching, in which the best answer
is searched by maximizing the total certainty score
for the combination of tags. In using DP matching,
we limit a number of ambiguities by applying beam
search with width . In CoNLL 2000 shared task,
the number of votes for the class obtained through
the pairwise voting is used as the certain score for
beam search with width 5 (Kudo and Matsumoto,
2000a). In this paper, however, we apply determin-
istic method instead of applying beam search with
keeping some ambiguities. The reason we apply de-
terministic method is that our further experiments
and investigation for the selection of beam width
shows that larger beam width dose not always give a
significant improvement in the accuracy. Given our
experiments, we conclude that satisfying accuracies
can be obtained even with the deterministic parsing.
Another reason for selecting the simpler setting is
that the major purpose of this paper is to compare
weighted voting schemes and to show an effective
weighting method with the help of empirical risk
estimation frameworks.
</bodyText>
<subsectionHeader confidence="0.997132">
3.3 Weighted Voting
</subsectionHeader>
<bodyText confidence="0.999882342857143">
Tjong Kim Sang et al. report that they achieve
higher accuracy by applying weighted voting of sys-
tems which are trained using distinct chunk rep-
resentations and different machine learning algo-
rithms, such as MBL, ME and IGTree(Tjong Kim
Sang, 2000a; Tjong Kim Sang et al., 2000). It
is well-known that weighted voting scheme has a
potential to maximize the margin between critical
samples and the separating hyperplane, and pro-
duces a decision function with high generalization
performance(Schapire et al., 1997). The boosting
technique is a type of weighted voting scheme, and
has been applied to many NLP problems such as
parsing, part-of-speech tagging and text categoriza-
tion.
In our experiments, in order to obtain higher ac-
curacy, we also apply weighted voting of 8 SVM-
based systems which are trained using distinct
chunk representations. Before applying weighted
voting method, first we need to decide the weights
to be given to individual systems. We can obtain
the best weights if we could obtain the accuracy for
the “true” test data. However, it is impossible to
estimate them. In boosting technique, the voting
weights are given by the accuracy of the training
data during the iteration of changing the frequency
(distribution) of training data. However, we can-
not use the accuracy of the training data for vot-
ing weights, since SVMs do not depend on the fre-
quency (distribution) of training data, and can sepa-
rate the training data without any mis-classification
by selecting the appropriate kernel function and the
soft margin parameter. In this paper, we introduce
the following four weighting methods in our exper-
iments:
</bodyText>
<sectionHeader confidence="0.466247" genericHeader="method">
1. Uniform weights
</sectionHeader>
<bodyText confidence="0.988506333333333">
We give the same voting weight to all systems.
This method is taken as the baseline for other
weighting methods.
</bodyText>
<figure confidence="0.7995564">
Direction
Word:
POS:
Chunk:
2. Cross validation
</figure>
<bodyText confidence="0.936808545454545">
Dividing training data into portions, we em-
ploy the training by using portions, and
then evaluate the remaining portion. In this
fashion, we will have individual accuracy.
Final voting weights are given by the average
of these accuracies.
.
The value of , which represents the smallest
diameter enclosing all of the training data, is
approximated by the maximum distance from
the origin.
</bodyText>
<figure confidence="0.7090206">
4. Leave-One-Out bound
By using (3), we estimate the lower bound of
the accuracy of a system. The voting weight is
calculated as: .
3. VC-bound
</figure>
<figureCaption confidence="0.4921705">
By applying (1) and (2), we estimate the lower
bound of accuracy for each system, and use
the accuracy as a voting weight. The voting
weight is calculated as:
1. We convert the training data into 4 representa-
tions (IOB1/IOB2/IOE1/IOE2).
</figureCaption>
<bodyText confidence="0.963579">
2. We consider two parsing directions (For-
ward/Backward) for each representation, i.e.
systems for a single training data set.
Then, we employ SVMs training using these
independent chunk representations.
</bodyText>
<listItem confidence="0.636347">
3. After training, we examine the VC bound and
</listItem>
<bodyText confidence="0.99903075862069">
Leave-One-Out bound for each of 8 systems.
As for cross validation, we employ the steps 1
and 2 for each divided training data, and obtain
the weights.
4. We test these 8 systems with a separated test
data set. Before employing weighted voting,
we have to convert them into a uniform repre-
sentation, since the tag sets used in individual
8 systems are different. For this purpose, we
re-convert each of the estimated results into 4
representations (IOB1/IOB2/IOE2/IOE1).
5. We employ weighted voting of 8 systems with
respect to the converted 4 uniform representa-
tions and the 4 voting schemes respectively. Fi-
nally, we have (types of uniform representa-
tions) 4 (types of weights) results for
our experiments.
Although we can use models with IOBES-F or
IOBES-B representations for the committees for
the weighted voting, we do not use them in our
voting experiments. The reason is that the num-
ber of classes are different (3 vs. 5) and the esti-
mated VC and LOO bound cannot straightforwardly
be compared with other models that have three
classes (IOB1/IOB2/IOE1/IOE2) under the same
condition. We conduct experiments with IOBES-
F and IOBES-B representations only to investigate
how far the difference of various chunk representa-
tions would affect the actual chunking accuracies.
</bodyText>
<sectionHeader confidence="0.999912" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998438">
4.1 Experiment Setting
</subsectionHeader>
<bodyText confidence="0.986897754716981">
We use the following three annotated corpora for
our experiments.
Base NP standard data set (baseNP-S)
This data set was first introduced by (Ramshaw
and Marcus, 1995), and taken as the standard
data set for baseNP identification task2. This
data set consists of four sections (15-18) of
the Wall Street Journal (WSJ) part of the Penn
Treebank for the training data, and one section
(20) for the test data. The data has part-of-
speech (POS) tags annotated by the Brill tag-
ger(Brill, 1995).
Base NP large data set (baseNP-L)
This data set consists of 20 sections (02-21)
of the WSJ part of the Penn Treebank for the
training data, and one section (00) for the test
data. POS tags in this data sets are also anno-
tated by the Brill tagger. We omit the experi-
ments IOB1 and IOE1 representations for this
training data since the data size is too large for
our current SVMs learning program. In case
of IOB1 and IOE1, the size of training data for
one classifier which estimates the class I and
O becomes much larger compared with IOB2
and IOE2 models. In addition, we also omit to
estimate the voting weights using cross valida-
tion method due to a large amount of training
cost.
Chunking data set (chunking)
This data set was used for CoNLL-2000
shared task(Tjong Kim Sang and Buchholz,
2000). In this data set, the total of 10
base phrase classes (NP,VP,PP,ADJP,ADVP,CONJP,
2ftp://ftp.cis.upenn.edu/pub/chunker/
The procedure of our experiments is summarized
as follows:
INITJ,LST,PTR,SBAR) are annotated. This data
set consists of 4 sections (15-18) of the WSJ
part of the Penn Treebank for the training data,
and one section (20) for the test data 3.
All the experiments are carried out with our soft-
ware package TinySVM4, which is designed and op-
timized to handle large sparse feature vectors and
large number of training samples. This package can
estimate the VC bound and Leave-One-Out bound
automatically. For the kernel function, we use the
2-nd polynomial function and set the soft margin
parameter to be 1.
In the baseNP identification task, the perfor-
mance of the systems is usually measured with three
rates: precision, recall and
. In this paper, we re-
fer to as accuracy.
</bodyText>
<subsectionHeader confidence="0.925655">
4.2 Results of Experiments
</subsectionHeader>
<bodyText confidence="0.998233916666667">
Table 2 shows results of our SVMs based chunk-
ing with individual chunk representations. This ta-
ble also lists the voting weights estimated by differ-
ent approaches (B:Cross Validation, C:VC-bound,
D:Leave-one-out). We also show the results of
Start/End representation in Table 2.
Table 3 shows the results of the weighted vot-
ing of four different voting methods: A: Uniform,
B: Cross Validation ( ), C: VC bound, D:
Leave-One-Out Bound.
Table 4 shows the precision, recall and of
the best result for each data set.
</bodyText>
<subsectionHeader confidence="0.999837">
4.3 Accuracy vs Chunk Representation
</subsectionHeader>
<bodyText confidence="0.9998785625">
We obtain the best accuracy when we ap-
ply IOE2-B representation for baseNP-S and
chunking data set. In fact, we cannot find
a significant difference in the performance be-
tween Inside/Outside(IOB1/IOB2/IOE1/IOE2) and
Start/End(IOBES) representations.
Sassano and Utsuro evaluate how the difference
of the chunk representation would affect the perfor-
mance of the systems based on different machine
learning algorithms(Sassano and Utsuro, 2000).
They report that Decision List system performs
better with Start/End representation than with In-
side/Outside, since Decision List considers the spe-
cific combination of features. As for Maximum
Entropy, they report that it performs better with
Inside/Outside representation than with Start/End,
</bodyText>
<table confidence="0.989565407407407">
Training Condition Acc. Estimated Weights
data rep. B C D
baseNP-S IOB1-F 93.76 .9394 .4310 .9193
IOB1-B 93.93 .9422 .4351 .9184
IOB2-F 93.84 .9410 .4415 .9172
IOB2-B 93.70 .9407 .4300 .9166
IOE1-F 93.73 .9386 .4274 .9183
IOE1-B 93.98 .9425 .4400 .9217
IOE2-F 93.98 .9409 .4350 .9180
IOE2-B 94.11 .9426 .4510 .9193
baseNP-L IOB2-F 95.34 - .4500 .9497
IOB2-B 95.28 - .4362 .9487
IOE2-F 95.32 - .4467 .9496
IOE2-B 95.29 - .4556 .9503
chunking IOB1-F 93.48 .9342 .6585 .9605
IOB1-B 93.74 .9346 .6614 .9596
IOB2-F 93.46 .9341 .6809 .9586
IOB2-B 93.47 .9355 .6722 .9594
IOE1-F 93.45 .9335 .6533 .9589
IOE1-B 93.72 .9358 .6669 .9611
IOE2-F 93.45 .9341 .6740 .9606
IOE2-B 93.85 .9361 .6913 .9597
baseNP-S IOBES-F 93.96
IOBES-B 93.58
chunking IOBES-F 93.31
IOBES-B 93.41
B:Cross Validation, C:VC bound, D:LOO bound
</table>
<tableCaption confidence="0.975593">
Table 2: Accuracy of individual representations
</tableCaption>
<table confidence="0.999398142857143">
Training Condition A Accuracy D
data rep. B C
baseNP-S IOB1 94.14 94.20 94.20 94.16
IOB2 94.16 94.22 94.22 94.18
IOE1 94.14 94.19 94.19 94.16
IOE2 94.16 94.20 94.21 94.17
baseNP-L IOB2 95.77 - 95.66 95.66
IOE2 95.77 - 95.66 95.66
chunking IOB1 93.77 93.87 93.89 93.87
IOB2 93.72 93.87 93.90 93.88
IOE1 93.76 93.86 93.88 93.86
IOE2 93.77 93.89 93.91 93.85
A:Uniform Weights, B:Cross Validation
C:VC bound, D:LOO bound
</table>
<tableCaption confidence="0.999177">
Table 3: Results of weighted voting
</tableCaption>
<footnote confidence="0.8109998">
data set precision
baseNP-S 94.15%
baseNP-L 95.62%
chunking 93.89%
recall
94.29% 94.22
95.93% 95.77
93.92% 93.91
3http://lcg-www.uia.ac.be/conll2000/chunking/ Table 4: Best results for each data set
4http://cl.aist-nara.ac.jp/ taku-ku/software/TinySVM/
</footnote>
<bodyText confidence="0.999374714285714">
since Maximum Entropy model regards all features
as independent and tries to catch the more general
feature sets.
We believe that SVMs perform well regardless of
the chunk representation, since SVMs have a high
generalization performance and a potential to select
the optimal features for the given task.
</bodyText>
<subsectionHeader confidence="0.999721">
4.4 Effects of Weighted Voting
</subsectionHeader>
<bodyText confidence="0.999977413793104">
By applying weighted voting, we achieve higher ac-
curacy than any of single representation system re-
gardless of the voting weights. Furthermore, we
achieve higher accuracy by applying Cross valida-
tion and VC-bound and Leave-One-Out methods
than the baseline method.
By using VC bound for each weight, we achieve
nearly the same accuracy as that of Cross valida-
tion. This result suggests that the VC bound has a
potential to predict the error rate for the “true” test
data accurately. Focusing on the relationship be-
tween the accuracy of the test data and the estimated
weights, we find that VC bound can predict the ac-
curacy for the test data precisely. Even if we have
no room for applying the voting schemes because
of some real-world constraints (limited computation
and memory capacity), the use of VC bound may al-
low to obtain the best accuracy. On the other hand,
we find that the prediction ability of Leave-One-Out
is worse than that of VC bound.
Cross validation is the standard method to esti-
mate the voting weights for different systems. How-
ever, Cross validation requires a larger amount of
computational overhead as the training data is di-
vided and is repeatedly used to obtain the voting
weights. We believe that VC bound is more effec-
tive than Cross validation, since it can obtain the
comparable results to Cross validation without in-
creasing computational overhead.
</bodyText>
<subsectionHeader confidence="0.99598">
4.5 Comparison with Related Works
</subsectionHeader>
<bodyText confidence="0.998902772727273">
Tjong Kim Sang et al. report that they achieve accu-
racy of 93.86 for baseNP-S data set, and 94.90 for
baseNP-L data set. They apply weighted voting of
the systems which are trained using distinct chunk
representations and different machine learning al-
gorithms such as MBL, ME and IGTree(Tjong Kim
Sang, 2000a; Tjong Kim Sang et al., 2000).
Our experiments achieve the accuracy of 93.76 -
94.11 for baseNP-S, and 95.29 - 95.34 for baseNP-
L even with a single chunk representation. In addi-
tion, by applying the weighted voting framework,
we achieve accuracy of 94.22 for baseNP-S, and
95.77 for baseNP-L data set. As far as accuracies
are concerned, our model outperforms Tjong Kim
Sang’s model.
In the CoNLL-2000 shared task, we achieved
the accuracy of 93.48 using IOB2-F representation
(Kudo and Matsumoto, 2000b) 5. By combining
weighted voting schemes, we achieve accuracy of
93.91. In addition, our method also outperforms
other methods based on the weighted voting(van
Halteren, 2000; Tjong Kim Sang, 2000b).
</bodyText>
<subsectionHeader confidence="0.765389">
4.6 Future Work
</subsectionHeader>
<bodyText confidence="0.999905575757576">
Applying to other chunking tasks
Our chunking method can be equally appli-
cable to other chunking task, such as English
POS tagging, Japanese chunk(bunsetsu) iden-
tification and named entity extraction. For fu-
ture, we will apply our method to those chunk-
ing tasks and examine the performance of the
method.
Incorporating variable context length model
In our experiments, we simply use the so-
called fixed context length model. We believe
that we can achieve higher accuracy by select-
ing appropriate context length which is actu-
ally needed for identifying individual chunk
tags. Sassano and Utsuro(Sassano and Ut-
suro, 2000) introduce a variable context length
model for Japanese named entity identification
task and perform better results. We will incor-
porate the variable context length model into
our system.
Considering more predictable bound
In our experiments, we introduce new types
of voting methods which stem from the theo-
rems of SVMs — VC bound and Leave-One-
Out bound. On the other hand, Chapelle and
Vapnik introduce an alternative and more pre-
dictable bound for the risk and report their
proposed bound is quite useful for selecting
the kernel function and soft margin parame-
ter(Chapelle and Vapnik, 2000). We believe
that we can obtain higher accuracy using this
more predictable bound for the voting weights
in our experiments.
</bodyText>
<footnote confidence="0.8493565">
5In our experiments, the accuracy of 93.46 is obtained with
IOB2-F representation, which was the exactly the same repre-
sentation we applied for CoNLL 2000 shared task. This slight
difference of accuracy arises from the following two reason :
(1) The difference of beam width for parsing (N=1 vs. N=5),
(2) The difference of applied SVMs package (TinySVM vs.
</footnote>
<page confidence="0.266004">
.
</page>
<sectionHeader confidence="0.99374" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.99996475">
In this paper, we introduce a uniform framework for
chunking task based on Support Vector Machines
(SVMs). Experimental results on WSJ corpus show
that our method outperforms other conventional ma-
chine learning frameworks such MBL and Max-
imum Entropy Models. The results are due to
the good characteristics of generalization and non-
overfitting of SVMs even with a high dimensional
vector space. In addition, we achieve higher accu-
racy by applying weighted voting of 8-SVM based
systems which are trained using distinct chunk rep-
resentations.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892581395349">
Erin L. Allwein, Robert E. Schapire, and Yoram
Singer. 2000. Reducing multiclass to binary: A
unifying approach for margin classifiers. In In-
ternational Conf. on Machine Learning (ICML),
pages 9–16.
Eric Brill. 1995. Transformation-Based Error-
Driven Learning and Natural Language Process-
ing: A Case Study in Part-of-Speech Tagging.
Computational Linguistics, 21(4).
Oliver Chapelle and Vladimir Vapnik. 2000. Model
selection for support vector machines. In Ad-
vances in Neural Information Processing Systems
12. Cambridge, Mass: MIT Press.
C. Cortes and Vladimir N. Vapnik. 1995. Support
Vector Networks. Machine Learning, 20:273–
297.
T. G. Dietterich and G. Bakiri. 1995. Solving multi-
class learning problems via error-correcting out-
put codes. Journal of Artificial Intelligence Re-
search, 2:263–286.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Interna-
tional Conference on Machine Learning (ICML),
pages 148–146.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many
Relevant Features. In European Conference on
Machine Learning (ECML).
Ulrich H.-G Kreßel. 1999. Pairwise Classification
and Support Vector Machines. In Advances in
Kernel Mathods. MIT Press.
Taku Kudo and Yuji Matsumoto. 2000a. Japanese
Dependency Structure Analysis Based on Sup-
port Vector Machines. In Empirical Methods in
Natural Language Processing and Very Large
Corpora, pages 18–25.
Taku Kudo and Yuji Matsumoto. 2000b. Use of
Support Vector Learning for Chunk Identifica-
tion. In Proceedings of the 4th Conference on
CoNLL-2000 and LLL-2000, pages 142–144.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learn-
ing. In Proceedings of the 3rd Workshop on Very
Large Corpora, pages 88–94.
Manabu Sassano and Takehito Utsuro. 2000.
Named Entity Chunking Techniques in Su-
pervised Learning for Japanese Named Entity
Recognition. In Proceedings of COLING 2000,
pages 705–711.
Robert E. Schapire, Yoav Freund, Peter Bartlett,
and Wee Sun Lee. 1997. Boosting the margin:
a new explanation for the effectiveness of vot-
ing methods. In International Conference on Ma-
chine Learning (ICML), pages 322–330.
Hirotoshi Taira and Masahiko Haruno. 1999. Fea-
ture Selection in SVM Text Categorization. In
AAAI-99.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 Shared
Task: Chunking. In Proceedings of CoNLL-2000
and LLL-2000, pages 127–132.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999.
Representing text chunks. In Proceedings of
EACL’99, pages 173–179.
Erik F. Tjong Kim Sang, Walter Daelemans, Herv´e
D´ejean, Rob Koeling, Yuval Krymolowski, Vasin
Punyakanok, and Dan Roth. 2000. Applying
system combination to base noun phrase identi-
fication. In Proceedings of COLING 2000, pages
857–863.
Erik F. Tjong Kim Sang. 2000a. Noun phrase
recognition by system combination. In Proceed-
ings ofANLP-NAACL 2000, pages 50–55.
Erik F. Tjong Kim Sang. 2000b. Text Chunking by
System Combination. In Proceedings of CoNLL-
2000 and LLL-2000, pages 151–153.
Kiyotaka Uchimoto, Qing Ma, Masaki Murata, Hi-
romi Ozaku, and Hitoshi Isahara. 2000. Named
Entity Extraction Based on A Maximum Entropy
Model and Transformation Rules. In Processing
of the ACL 2000.
Hans van Halteren. 2000. Chunking with WPDV
Models. In Proceedings of CoNLL-2000 and
LLL-2000, pages 154–156.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. Wiley-Interscience.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588771">
<title confidence="0.999692">Chunking with Support Vector Machines</title>
<author confidence="0.992281">Kudo</author>
<affiliation confidence="0.9993095">Graduate School of Information Nara Institute of Science and</affiliation>
<email confidence="0.704452">taku-ku,matsu@is.aist-nara.ac.jp</email>
<abstract confidence="0.987461923076923">We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMsbased systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Erin L Allwein</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Reducing multiclass to binary: A unifying approach for margin classifiers.</title>
<date>2000</date>
<booktitle>In International Conf. on Machine Learning (ICML),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="10719" citStr="Allwein et al., 2000" startWordPosition="1802" endWordPosition="1805">ulti-class classifiers in order to classify three (B,I,O) or more (B,I,O,E,S) classes. There are two popular methods to extend a binary classification task to that of classes. One is one class vs. all others. The idea is to build classifiers so as to separate one class from all others. The other is pairwise classification. The idea is to build classifiers considering all pairs of classes, and final decision is given by their weighted voting. There are a number of other methods to extend SVMs to multiclass classifiers. For example, Dietterich and Bakiri(Dietterich and Bakiri, 1995) and Allwein(Allwein et al., 2000) introduce a unifying framework for solving the multiclass problem we want to keep consistency with Inside/Start (B/I/O) representation. by reducing them into binary models. However, we employ the simple pairwise classifiers because of the following reasons: (1) In general, SVMs require training cost (where is the size of training data). Thus, if the size of training data for individual binary classifiers is small, we can significantly reduce the training cost. Although pairwise classifiers tend to build a larger number of binary classifiers, the training cost required for pairwise method is m</context>
</contexts>
<marker>Allwein, Schapire, Singer, 2000</marker>
<rawString>Erin L. Allwein, Robert E. Schapire, and Yoram Singer. 2000. Reducing multiclass to binary: A unifying approach for margin classifiers. In International Conf. on Machine Learning (ICML), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-Based ErrorDriven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="18184" citStr="Brill, 1995" startWordPosition="3034" endWordPosition="3036">ifference of various chunk representations would affect the actual chunking accuracies. 4 Experiments 4.1 Experiment Setting We use the following three annotated corpora for our experiments. Base NP standard data set (baseNP-S) This data set was first introduced by (Ramshaw and Marcus, 1995), and taken as the standard data set for baseNP identification task2. This data set consists of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank for the training data, and one section (20) for the test data. The data has part-ofspeech (POS) tags annotated by the Brill tagger(Brill, 1995). Base NP large data set (baseNP-L) This data set consists of 20 sections (02-21) of the WSJ part of the Penn Treebank for the training data, and one section (00) for the test data. POS tags in this data sets are also annotated by the Brill tagger. We omit the experiments IOB1 and IOE1 representations for this training data since the data size is too large for our current SVMs learning program. In case of IOB1 and IOE1, the size of training data for one classifier which estimates the class I and O becomes much larger compared with IOB2 and IOE2 models. In addition, we also omit to estimate the</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-Based ErrorDriven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging. Computational Linguistics, 21(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Chapelle</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Model selection for support vector machines.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 12.</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass:</location>
<contexts>
<context position="26746" citStr="Chapelle and Vapnik, 2000" startWordPosition="4410" endWordPosition="4414">Utsuro(Sassano and Utsuro, 2000) introduce a variable context length model for Japanese named entity identification task and perform better results. We will incorporate the variable context length model into our system. Considering more predictable bound In our experiments, we introduce new types of voting methods which stem from the theorems of SVMs — VC bound and Leave-OneOut bound. On the other hand, Chapelle and Vapnik introduce an alternative and more predictable bound for the risk and report their proposed bound is quite useful for selecting the kernel function and soft margin parameter(Chapelle and Vapnik, 2000). We believe that we can obtain higher accuracy using this more predictable bound for the voting weights in our experiments. 5In our experiments, the accuracy of 93.46 is obtained with IOB2-F representation, which was the exactly the same representation we applied for CoNLL 2000 shared task. This slight difference of accuracy arises from the following two reason : (1) The difference of beam width for parsing (N=1 vs. N=5), (2) The difference of applied SVMs package (TinySVM vs. . 5 Summary In this paper, we introduce a uniform framework for chunking task based on Support Vector Machines (SVMs)</context>
</contexts>
<marker>Chapelle, Vapnik, 2000</marker>
<rawString>Oliver Chapelle and Vladimir Vapnik. 2000. Model selection for support vector machines. In Advances in Neural Information Processing Systems 12. Cambridge, Mass: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>Support Vector Networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="2147" citStr="Cortes and Vapnik, 1995" startWordPosition="316" endWordPosition="320">en proposed for chunking (Ramshaw and Marcus, 1995; Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000; Tjong Kim Sang, 2000b; Sassano and Utsuro, 2000; van Halteren, 2000). Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful feature selection in order to achieve high accuracy. They do not provide a method for automatic selection of given feature sets. Usually, heuristics are used for selecting effective features and their combinations. New statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) have been proposed. These techniques take a strategy that maximizes the margin between critical samples and the separating hyperplane. In particular, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out the training considering combinations of more than one feature. In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis, and are reporte</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and Vladimir N. Vapnik. 1995. Support Vector Networks. Machine Learning, 20:273– 297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
<author>G Bakiri</author>
</authors>
<title>Solving multiclass learning problems via error-correcting output codes.</title>
<date>1995</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>2--263</pages>
<contexts>
<context position="10685" citStr="Dietterich and Bakiri, 1995" startWordPosition="1797" endWordPosition="1800">lassifiers, thus we must extend SVMs to multi-class classifiers in order to classify three (B,I,O) or more (B,I,O,E,S) classes. There are two popular methods to extend a binary classification task to that of classes. One is one class vs. all others. The idea is to build classifiers so as to separate one class from all others. The other is pairwise classification. The idea is to build classifiers considering all pairs of classes, and final decision is given by their weighted voting. There are a number of other methods to extend SVMs to multiclass classifiers. For example, Dietterich and Bakiri(Dietterich and Bakiri, 1995) and Allwein(Allwein et al., 2000) introduce a unifying framework for solving the multiclass problem we want to keep consistency with Inside/Start (B/I/O) representation. by reducing them into binary models. However, we employ the simple pairwise classifiers because of the following reasons: (1) In general, SVMs require training cost (where is the size of training data). Thus, if the size of training data for individual binary classifiers is small, we can significantly reduce the training cost. Although pairwise classifiers tend to build a larger number of binary classifiers, the training cost</context>
</contexts>
<marker>Dietterich, Bakiri, 1995</marker>
<rawString>T. G. Dietterich and G. Bakiri. 1995. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>148--146</pages>
<contexts>
<context position="2202" citStr="Freund and Schapire, 1996" startWordPosition="324" endWordPosition="327">Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000; Tjong Kim Sang, 2000b; Sassano and Utsuro, 2000; van Halteren, 2000). Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful feature selection in order to achieve high accuracy. They do not provide a method for automatic selection of given feature sets. Usually, heuristics are used for selecting effective features and their combinations. New statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) have been proposed. These techniques take a strategy that maximizes the margin between critical samples and the separating hyperplane. In particular, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out the training considering combinations of more than one feature. In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis, and are reported to have achieved higher accuracy than previous approa</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a new boosting algorithm. In International Conference on Machine Learning (ICML), pages 148–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In European Conference on Machine Learning (ECML).</booktitle>
<contexts>
<context position="2822" citStr="Joachims, 1998" startWordPosition="419" endWordPosition="421">been proposed. These techniques take a strategy that maximizes the margin between critical samples and the separating hyperplane. In particular, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out the training considering combinations of more than one feature. In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis, and are reported to have achieved higher accuracy than previous approaches.(Joachims, 1998; Taira and Haruno, 1999; Kudo and Matsumoto, 2000a). In this paper, we apply Support Vector Machines to the chunking task. In addition, in order to achieve higher accuracy, we apply weighted voting of 8 SVM-based systems which are trained using distinct chunk representations. For the weighted voting systems, we introduce a new type of weighting strategy which are derived from the theoretical basis of the SVMs. 2 Support Vector Machines 2.1 Optimal Hyperplane Let us define the training samples each of which belongs either to positive or negative class as: is a feature vector of the-th sample r</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich H-G Kreßel</author>
</authors>
<title>Pairwise Classification and Support Vector Machines.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Mathods.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11409" citStr="Kreßel, 1999" startWordPosition="1912" endWordPosition="1913"> keep consistency with Inside/Start (B/I/O) representation. by reducing them into binary models. However, we employ the simple pairwise classifiers because of the following reasons: (1) In general, SVMs require training cost (where is the size of training data). Thus, if the size of training data for individual binary classifiers is small, we can significantly reduce the training cost. Although pairwise classifiers tend to build a larger number of binary classifiers, the training cost required for pairwise method is much more tractable compared to the one vs. all others. (2) Some experiments (Kreßel, 1999) report that a combination of pairwise classifiers performs better than the one vs. all others. For the feature sets for actual training and classification of SVMs, we use all the information available in the surrounding context, such as the words, their part-of-speech tags as well as the chunk labels. More precisely, we give the following features to identify the chunk label for the-th word: Here,is the word appearing at-th position, is the POS tag of, and is the (extended) chunk label for-th word. In addition, we can reverse the parsing direction (from right to left) by using two chunk tags </context>
</contexts>
<marker>Kreßel, 1999</marker>
<rawString>Ulrich H.-G Kreßel. 1999. Pairwise Classification and Support Vector Machines. In Advances in Kernel Mathods. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Structure Analysis Based on Support Vector Machines.</title>
<date>2000</date>
<booktitle>In Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="2872" citStr="Kudo and Matsumoto, 2000" startWordPosition="426" endWordPosition="429">trategy that maximizes the margin between critical samples and the separating hyperplane. In particular, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out the training considering combinations of more than one feature. In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis, and are reported to have achieved higher accuracy than previous approaches.(Joachims, 1998; Taira and Haruno, 1999; Kudo and Matsumoto, 2000a). In this paper, we apply Support Vector Machines to the chunking task. In addition, in order to achieve higher accuracy, we apply weighted voting of 8 SVM-based systems which are trained using distinct chunk representations. For the weighted voting systems, we introduce a new type of weighting strategy which are derived from the theoretical basis of the SVMs. 2 Support Vector Machines 2.1 Optimal Hyperplane Let us define the training samples each of which belongs either to positive or negative class as: is a feature vector of the-th sample represented by an dimensional vector. is the class </context>
<context position="12849" citStr="Kudo and Matsumoto, 2000" startWordPosition="2165" endWordPosition="2168"> preceding chunk labels ( for forward parsing , for backward parsing) are not given in the test data, they are decided dynamically during the tagging of chunk labels. The technique can be regarded as a sort of Dynamic Programming (DP) matching, in which the best answer is searched by maximizing the total certainty score for the combination of tags. In using DP matching, we limit a number of ambiguities by applying beam search with width . In CoNLL 2000 shared task, the number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto, 2000a). In this paper, however, we apply deterministic method instead of applying beam search with keeping some ambiguities. The reason we apply deterministic method is that our further experiments and investigation for the selection of beam width shows that larger beam width dose not always give a significant improvement in the accuracy. Given our experiments, we conclude that satisfying accuracies can be obtained even with the deterministic parsing. Another reason for selecting the simpler setting is that the major purpose of this paper is to compare weighted voting schemes and to show an effect</context>
<context position="25315" citStr="Kudo and Matsumoto, 2000" startWordPosition="4183" endWordPosition="4186">distinct chunk representations and different machine learning algorithms such as MBL, ME and IGTree(Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000). Our experiments achieve the accuracy of 93.76 - 94.11 for baseNP-S, and 95.29 - 95.34 for baseNPL even with a single chunk representation. In addition, by applying the weighted voting framework, we achieve accuracy of 94.22 for baseNP-S, and 95.77 for baseNP-L data set. As far as accuracies are concerned, our model outperforms Tjong Kim Sang’s model. In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b) 5. By combining weighted voting schemes, we achieve accuracy of 93.91. In addition, our method also outperforms other methods based on the weighted voting(van Halteren, 2000; Tjong Kim Sang, 2000b). 4.6 Future Work Applying to other chunking tasks Our chunking method can be equally applicable to other chunking task, such as English POS tagging, Japanese chunk(bunsetsu) identification and named entity extraction. For future, we will apply our method to those chunking tasks and examine the performance of the method. Incorporating variable context length model In our experiments, we simply use</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000a. Japanese Dependency Structure Analysis Based on Support Vector Machines. In Empirical Methods in Natural Language Processing and Very Large Corpora, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of Support Vector Learning for Chunk Identification.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Conference on CoNLL-2000 and LLL-2000,</booktitle>
<pages>142--144</pages>
<contexts>
<context position="2872" citStr="Kudo and Matsumoto, 2000" startWordPosition="426" endWordPosition="429">trategy that maximizes the margin between critical samples and the separating hyperplane. In particular, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out the training considering combinations of more than one feature. In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis, and are reported to have achieved higher accuracy than previous approaches.(Joachims, 1998; Taira and Haruno, 1999; Kudo and Matsumoto, 2000a). In this paper, we apply Support Vector Machines to the chunking task. In addition, in order to achieve higher accuracy, we apply weighted voting of 8 SVM-based systems which are trained using distinct chunk representations. For the weighted voting systems, we introduce a new type of weighting strategy which are derived from the theoretical basis of the SVMs. 2 Support Vector Machines 2.1 Optimal Hyperplane Let us define the training samples each of which belongs either to positive or negative class as: is a feature vector of the-th sample represented by an dimensional vector. is the class </context>
<context position="12849" citStr="Kudo and Matsumoto, 2000" startWordPosition="2165" endWordPosition="2168"> preceding chunk labels ( for forward parsing , for backward parsing) are not given in the test data, they are decided dynamically during the tagging of chunk labels. The technique can be regarded as a sort of Dynamic Programming (DP) matching, in which the best answer is searched by maximizing the total certainty score for the combination of tags. In using DP matching, we limit a number of ambiguities by applying beam search with width . In CoNLL 2000 shared task, the number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto, 2000a). In this paper, however, we apply deterministic method instead of applying beam search with keeping some ambiguities. The reason we apply deterministic method is that our further experiments and investigation for the selection of beam width shows that larger beam width dose not always give a significant improvement in the accuracy. Given our experiments, we conclude that satisfying accuracies can be obtained even with the deterministic parsing. Another reason for selecting the simpler setting is that the major purpose of this paper is to compare weighted voting schemes and to show an effect</context>
<context position="25315" citStr="Kudo and Matsumoto, 2000" startWordPosition="4183" endWordPosition="4186">distinct chunk representations and different machine learning algorithms such as MBL, ME and IGTree(Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000). Our experiments achieve the accuracy of 93.76 - 94.11 for baseNP-S, and 95.29 - 95.34 for baseNPL even with a single chunk representation. In addition, by applying the weighted voting framework, we achieve accuracy of 94.22 for baseNP-S, and 95.77 for baseNP-L data set. As far as accuracies are concerned, our model outperforms Tjong Kim Sang’s model. In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b) 5. By combining weighted voting schemes, we achieve accuracy of 93.91. In addition, our method also outperforms other methods based on the weighted voting(van Halteren, 2000; Tjong Kim Sang, 2000b). 4.6 Future Work Applying to other chunking tasks Our chunking method can be equally applicable to other chunking task, such as English POS tagging, Japanese chunk(bunsetsu) identification and named entity extraction. For future, we will apply our method to those chunking tasks and examine the performance of the method. Incorporating variable context length model In our experiments, we simply use</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000b. Use of Support Vector Learning for Chunk Identification. In Proceedings of the 4th Conference on CoNLL-2000 and LLL-2000, pages 142–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora,</booktitle>
<pages>88--94</pages>
<contexts>
<context position="1574" citStr="Ramshaw and Marcus, 1995" startWordPosition="227" endWordPosition="230">as a chunking task. Examples include English base noun phrase identification (base NP chunking), English base phrase identification (chunking), Japanese chunk (bunsetsu) identification and named entity extraction. Tokenization and part-of-speech tagging can also be regarded as a chunking task, if we assume each character as a token. Machine learning techniques are often applied to chunking, since the task is formulated as estimating an identifying function from the information (features) available in the surrounding context. Various machine learning approaches have been proposed for chunking (Ramshaw and Marcus, 1995; Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000; Tjong Kim Sang, 2000b; Sassano and Utsuro, 2000; van Halteren, 2000). Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful feature selection in order to achieve high accuracy. They do not provide a method for automatic selection of given feature sets. Usually, heuristics are used for selecting effective features and their combinations. New statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boostin</context>
<context position="8346" citStr="Ramshaw and Marcus, 1995" startWordPosition="1381" endWordPosition="1384">re, becomes the r.h.s. term of (3). In practice, it is known that this bound is less predictive than the VC bound. IOB1 IOB2 IOE1 IOE2 Start/End In O O O O O early I B I I B trading I I I E E in O O O O O busy I B I I B Hong I I I I I Kong I I E E E Monday B B I E S , O O O O O gold I B I E S was O O O O O Table 1: Example for each chunk representation 3 Chunking 3.1 Chunk representation There are mainly two types of representations for proper chunks. One is Inside/Outside representation, and the other is Start/End representation. 1. Inside/Outside This representation was first introduced in (Ramshaw and Marcus, 1995), and has been applied for base NP chunking. This method uses the following set of three tags for representing proper chunks. I Current token is inside of a chunk. O Current token is outside of any chunk. B Current token is the beginning of a chunk which immediately follows another chunk. Tjong Kim Sang calls this method as IOB1 representation, and introduces three alternative versions — IOB2,IOE1 and IOE2 (Tjong Kim Sang and Veenstra, 1999). IOB2 A B tag is given for every token which exists at the beginning of a chunk. Other tokens are the same as IOB1. IOE1 An E tag is used to mark the last</context>
<context position="17864" citStr="Ramshaw and Marcus, 1995" startWordPosition="2977" endWordPosition="2980">eriments. The reason is that the number of classes are different (3 vs. 5) and the estimated VC and LOO bound cannot straightforwardly be compared with other models that have three classes (IOB1/IOB2/IOE1/IOE2) under the same condition. We conduct experiments with IOBESF and IOBES-B representations only to investigate how far the difference of various chunk representations would affect the actual chunking accuracies. 4 Experiments 4.1 Experiment Setting We use the following three annotated corpora for our experiments. Base NP standard data set (baseNP-S) This data set was first introduced by (Ramshaw and Marcus, 1995), and taken as the standard data set for baseNP identification task2. This data set consists of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank for the training data, and one section (20) for the test data. The data has part-ofspeech (POS) tags annotated by the Brill tagger(Brill, 1995). Base NP large data set (baseNP-L) This data set consists of 20 sections (02-21) of the WSJ part of the Penn Treebank for the training data, and one section (00) for the test data. POS tags in this data sets are also annotated by the Brill tagger. We omit the experiments IOB1 an</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the 3rd Workshop on Very Large Corpora, pages 88–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
<author>Takehito Utsuro</author>
</authors>
<title>Named Entity Chunking Techniques in Supervised Learning for Japanese Named Entity Recognition.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>705--711</pages>
<contexts>
<context position="1675" citStr="Sassano and Utsuro, 2000" startWordPosition="245" endWordPosition="248">ish base phrase identification (chunking), Japanese chunk (bunsetsu) identification and named entity extraction. Tokenization and part-of-speech tagging can also be regarded as a chunking task, if we assume each character as a token. Machine learning techniques are often applied to chunking, since the task is formulated as estimating an identifying function from the information (features) available in the surrounding context. Various machine learning approaches have been proposed for chunking (Ramshaw and Marcus, 1995; Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000; Tjong Kim Sang, 2000b; Sassano and Utsuro, 2000; van Halteren, 2000). Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful feature selection in order to achieve high accuracy. They do not provide a method for automatic selection of given feature sets. Usually, heuristics are used for selecting effective features and their combinations. New statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) have been proposed. These techniques take a strategy that maximizes the </context>
<context position="20895" citStr="Sassano and Utsuro, 2000" startWordPosition="3478" endWordPosition="3481">iform, B: Cross Validation ( ), C: VC bound, D: Leave-One-Out Bound. Table 4 shows the precision, recall and of the best result for each data set. 4.3 Accuracy vs Chunk Representation We obtain the best accuracy when we apply IOE2-B representation for baseNP-S and chunking data set. In fact, we cannot find a significant difference in the performance between Inside/Outside(IOB1/IOB2/IOE1/IOE2) and Start/End(IOBES) representations. Sassano and Utsuro evaluate how the difference of the chunk representation would affect the performance of the systems based on different machine learning algorithms(Sassano and Utsuro, 2000). They report that Decision List system performs better with Start/End representation than with Inside/Outside, since Decision List considers the specific combination of features. As for Maximum Entropy, they report that it performs better with Inside/Outside representation than with Start/End, Training Condition Acc. Estimated Weights data rep. B C D baseNP-S IOB1-F 93.76 .9394 .4310 .9193 IOB1-B 93.93 .9422 .4351 .9184 IOB2-F 93.84 .9410 .4415 .9172 IOB2-B 93.70 .9407 .4300 .9166 IOE1-F 93.73 .9386 .4274 .9183 IOE1-B 93.98 .9425 .4400 .9217 IOE2-F 93.98 .9409 .4350 .9180 IOE2-B 94.11 .9426 .</context>
<context position="26152" citStr="Sassano and Utsuro, 2000" startWordPosition="4314" endWordPosition="4318"> Work Applying to other chunking tasks Our chunking method can be equally applicable to other chunking task, such as English POS tagging, Japanese chunk(bunsetsu) identification and named entity extraction. For future, we will apply our method to those chunking tasks and examine the performance of the method. Incorporating variable context length model In our experiments, we simply use the socalled fixed context length model. We believe that we can achieve higher accuracy by selecting appropriate context length which is actually needed for identifying individual chunk tags. Sassano and Utsuro(Sassano and Utsuro, 2000) introduce a variable context length model for Japanese named entity identification task and perform better results. We will incorporate the variable context length model into our system. Considering more predictable bound In our experiments, we introduce new types of voting methods which stem from the theorems of SVMs — VC bound and Leave-OneOut bound. On the other hand, Chapelle and Vapnik introduce an alternative and more predictable bound for the risk and report their proposed bound is quite useful for selecting the kernel function and soft margin parameter(Chapelle and Vapnik, 2000). We b</context>
</contexts>
<marker>Sassano, Utsuro, 2000</marker>
<rawString>Manabu Sassano and Takehito Utsuro. 2000. Named Entity Chunking Techniques in Supervised Learning for Japanese Named Entity Recognition. In Proceedings of COLING 2000, pages 705–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoav Freund</author>
<author>Peter Bartlett</author>
<author>Wee Sun Lee</author>
</authors>
<title>Boosting the margin: a new explanation for the effectiveness of voting methods.</title>
<date>1997</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>322--330</pages>
<contexts>
<context position="14057" citStr="Schapire et al., 1997" startWordPosition="2354" endWordPosition="2357">show an effective weighting method with the help of empirical risk estimation frameworks. 3.3 Weighted Voting Tjong Kim Sang et al. report that they achieve higher accuracy by applying weighted voting of systems which are trained using distinct chunk representations and different machine learning algorithms, such as MBL, ME and IGTree(Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000). It is well-known that weighted voting scheme has a potential to maximize the margin between critical samples and the separating hyperplane, and produces a decision function with high generalization performance(Schapire et al., 1997). The boosting technique is a type of weighted voting scheme, and has been applied to many NLP problems such as parsing, part-of-speech tagging and text categorization. In our experiments, in order to obtain higher accuracy, we also apply weighted voting of 8 SVMbased systems which are trained using distinct chunk representations. Before applying weighted voting method, first we need to decide the weights to be given to individual systems. We can obtain the best weights if we could obtain the accuracy for the “true” test data. However, it is impossible to estimate them. In boosting technique, </context>
</contexts>
<marker>Schapire, Freund, Bartlett, Lee, 1997</marker>
<rawString>Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. 1997. Boosting the margin: a new explanation for the effectiveness of voting methods. In International Conference on Machine Learning (ICML), pages 322–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirotoshi Taira</author>
<author>Masahiko Haruno</author>
</authors>
<title>Feature Selection in SVM Text Categorization.</title>
<date>1999</date>
<booktitle>In AAAI-99.</booktitle>
<contexts>
<context position="2846" citStr="Taira and Haruno, 1999" startWordPosition="422" endWordPosition="425">hese techniques take a strategy that maximizes the margin between critical samples and the separating hyperplane. In particular, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out the training considering combinations of more than one feature. In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis, and are reported to have achieved higher accuracy than previous approaches.(Joachims, 1998; Taira and Haruno, 1999; Kudo and Matsumoto, 2000a). In this paper, we apply Support Vector Machines to the chunking task. In addition, in order to achieve higher accuracy, we apply weighted voting of 8 SVM-based systems which are trained using distinct chunk representations. For the weighted voting systems, we introduce a new type of weighting strategy which are derived from the theoretical basis of the SVMs. 2 Support Vector Machines 2.1 Optimal Hyperplane Let us define the training samples each of which belongs either to positive or negative class as: is a feature vector of the-th sample represented by an dimensi</context>
</contexts>
<marker>Taira, Haruno, 1999</marker>
<rawString>Hirotoshi Taira and Masahiko Haruno. 1999. Feature Selection in SVM Text Categorization. In AAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 Shared Task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL-2000,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="18983" citStr="Sang and Buchholz, 2000" startWordPosition="3178" endWordPosition="3181"> data. POS tags in this data sets are also annotated by the Brill tagger. We omit the experiments IOB1 and IOE1 representations for this training data since the data size is too large for our current SVMs learning program. In case of IOB1 and IOE1, the size of training data for one classifier which estimates the class I and O becomes much larger compared with IOB2 and IOE2 models. In addition, we also omit to estimate the voting weights using cross validation method due to a large amount of training cost. Chunking data set (chunking) This data set was used for CoNLL-2000 shared task(Tjong Kim Sang and Buchholz, 2000). In this data set, the total of 10 base phrase classes (NP,VP,PP,ADJP,ADVP,CONJP, 2ftp://ftp.cis.upenn.edu/pub/chunker/ The procedure of our experiments is summarized as follows: INITJ,LST,PTR,SBAR) are annotated. This data set consists of 4 sections (15-18) of the WSJ part of the Penn Treebank for the training data, and one section (20) for the test data 3. All the experiments are carried out with our software package TinySVM4, which is designed and optimized to handle large sparse feature vectors and large number of training samples. This package can estimate the VC bound and Leave-One-Out </context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 Shared Task: Chunking. In Proceedings of CoNLL-2000 and LLL-2000, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Jorn Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL’99,</booktitle>
<pages>173--179</pages>
<contexts>
<context position="8791" citStr="Sang and Veenstra, 1999" startWordPosition="1457" endWordPosition="1460">r chunks. One is Inside/Outside representation, and the other is Start/End representation. 1. Inside/Outside This representation was first introduced in (Ramshaw and Marcus, 1995), and has been applied for base NP chunking. This method uses the following set of three tags for representing proper chunks. I Current token is inside of a chunk. O Current token is outside of any chunk. B Current token is the beginning of a chunk which immediately follows another chunk. Tjong Kim Sang calls this method as IOB1 representation, and introduces three alternative versions — IOB2,IOE1 and IOE2 (Tjong Kim Sang and Veenstra, 1999). IOB2 A B tag is given for every token which exists at the beginning of a chunk. Other tokens are the same as IOB1. IOE1 An E tag is used to mark the last token of a chunk immediately preceding another chunk. IOE2 An E tag is given for every token which exists at the end of a chunk. 2. Start/End This method has been used for the Japanese named entity extraction task, and requires the following five tags for representing proper chunks(Uchimoto et al., 2000) 1. 1Originally, Uchimoto uses C/E/U/O/S representation. However we rename them as B/I/O/E/S for our purpose, since B Current token is the </context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Representing text chunks. In Proceedings of EACL’99, pages 173–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Walter Daelemans</author>
<author>Herv´e D´ejean</author>
<author>Rob Koeling</author>
<author>Yuval Krymolowski</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
</authors>
<title>Applying system combination to base noun phrase identification.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>857--863</pages>
<marker>Sang, Daelemans, D´ejean, Koeling, Krymolowski, Punyakanok, Roth, 2000</marker>
<rawString>Erik F. Tjong Kim Sang, Walter Daelemans, Herv´e D´ejean, Rob Koeling, Yuval Krymolowski, Vasin Punyakanok, and Dan Roth. 2000. Applying system combination to base noun phrase identification. In Proceedings of COLING 2000, pages 857–863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang. 2000a. Noun phrase recognition by system combination.</title>
<date>2000</date>
<booktitle>In Proceedings ofANLP-NAACL</booktitle>
<pages>50--55</pages>
<marker>Erik, 2000</marker>
<rawString>Erik F. Tjong Kim Sang. 2000a. Noun phrase recognition by system combination. In Proceedings ofANLP-NAACL 2000, pages 50–55.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang. 2000b. Text Chunking by System Combination.</title>
<booktitle>In Proceedings of CoNLL2000 and LLL-2000,</booktitle>
<pages>151--153</pages>
<marker>Erik, </marker>
<rawString>Erik F. Tjong Kim Sang. 2000b. Text Chunking by System Combination. In Proceedings of CoNLL2000 and LLL-2000, pages 151–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Masaki Murata</author>
<author>Hiromi Ozaku</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Named Entity Extraction Based on A Maximum Entropy Model and Transformation Rules.</title>
<date>2000</date>
<booktitle>In Processing of the ACL</booktitle>
<contexts>
<context position="9252" citStr="Uchimoto et al., 2000" startWordPosition="1544" endWordPosition="1547">hunk. Tjong Kim Sang calls this method as IOB1 representation, and introduces three alternative versions — IOB2,IOE1 and IOE2 (Tjong Kim Sang and Veenstra, 1999). IOB2 A B tag is given for every token which exists at the beginning of a chunk. Other tokens are the same as IOB1. IOE1 An E tag is used to mark the last token of a chunk immediately preceding another chunk. IOE2 An E tag is given for every token which exists at the end of a chunk. 2. Start/End This method has been used for the Japanese named entity extraction task, and requires the following five tags for representing proper chunks(Uchimoto et al., 2000) 1. 1Originally, Uchimoto uses C/E/U/O/S representation. However we rename them as B/I/O/E/S for our purpose, since B Current token is the start of a chunk consisting of more than one token. E Current token is the end of a chunk consisting of more than one token. I Current token is a middle of a chunk consisting of more than two tokens. S Current token is a chunk consisting of only one token. O Current token is outside of any chunk. Examples of these five representations are shown in Table 1. If we have to identify the grammatical class of each chunk, we represent them by a pair of an I/O/B/E/</context>
</contexts>
<marker>Uchimoto, Ma, Murata, Ozaku, Isahara, 2000</marker>
<rawString>Kiyotaka Uchimoto, Qing Ma, Masaki Murata, Hiromi Ozaku, and Hitoshi Isahara. 2000. Named Entity Extraction Based on A Maximum Entropy Model and Transformation Rules. In Processing of the ACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
</authors>
<title>Chunking with WPDV Models.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL-2000,</booktitle>
<pages>154--156</pages>
<marker>van Halteren, 2000</marker>
<rawString>Hans van Halteren. 2000. Chunking with WPDV Models. In Proceedings of CoNLL-2000 and LLL-2000, pages 154–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="2162" citStr="Vapnik, 1998" startWordPosition="321" endWordPosition="322">(Ramshaw and Marcus, 1995; Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000; Tjong Kim Sang, 2000b; Sassano and Utsuro, 2000; van Halteren, 2000). Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful feature selection in order to achieve high accuracy. They do not provide a method for automatic selection of given feature sets. Usually, heuristics are used for selecting effective features and their combinations. New statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) have been proposed. These techniques take a strategy that maximizes the margin between critical samples and the separating hyperplane. In particular, SVMs achieve high generalization even with training data of a very high dimension. Furthermore, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out the training considering combinations of more than one feature. In the field of natural language processing, SVMs are applied to text categorization and syntactic dependency structure analysis, and are reported to have achie</context>
<context position="5448" citStr="Vapnik, 1998" startWordPosition="864" endWordPosition="865">on function even if we remove all training samples except for the extracted support vectors. In practice, even in the case where we cannot separate training data linearly because of some noise in the training data, etc, we can build the separating linear hyperplane by allowing some misclassifications. Though we omit the details here, we can build an optimal hyperplane by introducing a soft margin parameter , which trades off between the training error and the magnitude of the margin. Furthermore, SVMs have a potential to carry out the non-linear classification. Though we leave the details to (Vapnik, 1998), the optimization problem can be rewritten into a dual form, where all feature vectors appear in their dot products. By simply substituting every dot product of and in dual form with a certain Kernel function , SVMs can handle non-linear hypotheses. Among many kinds of Kernel functions available, we will focus on the -th polynomial kernel: . Use of-th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to. 2.2 Generalization Ability of SVMs Statistical Learning Theory(Vapnik, 1998) states that training error </context>
<context position="6693" citStr="Vapnik, 1998" startWordPosition="1072" endWordPosition="1073">(risk) hold the following theorem. Theorem 1 (Vapnik) If is the VC dimension ofthe class functions implemented by some machine learning algorithms, then for all functions of that class, with a probability of at least , the risk is bounded by where is a non-negative integer called the Vapnik Chervonenkis (VC) dimension, and is a measure of the complexity of the given decision function. The r.h.s. term of (1) is called VC bound. In order to minimize the risk, we have to minimize the empirical risk as well as VC dimension. It is known that the following theorem holds for VC dimension and margin (Vapnik, 1998). Theorem 2 (Vapnik) Suppose as the dimension of given training samples as the margin, and as the smallest diameter which encloses all training sample, then VC dimension of the SVMs are bounded by In order to minimize the VC dimension, we have to maximize the margin , which is exactly the strategy that SVMs take. Vapnik gives an alternative bound for the risk. Theorem 3 (Vapnik) Suppose is an error rate estimated by Leave-One-Out procedure, is bounded as Leave-One-Out procedure is a simple method to examine the risk of the decision function — first by removing a single sample from the training</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>