<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001455">
<title confidence="0.9945895">
The Contribution of Linguistic Features to Automatic Machine
Translation Evaluation
</title>
<author confidence="0.933598">
Enrique Amig´o1 Jes´us Gim´enez2 Julio Gonzalo 1 Felisa Verdejo1
</author>
<address confidence="0.45562">
1UNED, Madrid
</address>
<email confidence="0.821118">
{enrique,julio,felisa}@lsi.uned.es
</email>
<note confidence="0.598934">
2UPC, Barcelona
</note>
<email confidence="0.996223">
jgimenez@lsi.upc.edu
</email>
<sectionHeader confidence="0.994727" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999844380952381">
A number of approaches to Automatic
MT Evaluation based on deep linguistic
knowledge have been suggested. How-
ever, n-gram based metrics are still to-
day the dominant approach. The main
reason is that the advantages of employ-
ing deeper linguistic information have not
been clarified yet. In this work, we pro-
pose a novel approach for meta-evaluation
of MT evaluation metrics, since correla-
tion cofficient against human judges do
not reveal details about the advantages and
disadvantages of particular metrics. We
then use this approach to investigate the
benefits of introducing linguistic features
into evaluation metrics. Overall, our ex-
periments show that (i) both lexical and
linguistic metrics present complementary
advantages and (ii) combining both kinds
of metrics yields the most robust meta-
evaluation performance.
</bodyText>
<sectionHeader confidence="0.998688" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999420767857143">
Automatic evaluation methods based on similarity
to human references have substantially accelerated
the development cycle of many NLP tasks, such
as Machine Translation, Automatic Summariza-
tion, Sentence Compression and Language Gen-
eration. These automatic evaluation metrics allow
developers to optimize their systems without the
need for expensive human assessments for each
of their possible system configurations. However,
estimating the system output quality according to
its similarity to human references is not a trivial
task. The main problem is that many NLP tasks
are open/subjective; therefore, different humans
may generate different outputs, all of them equally
valid. Thus, language variability is an issue.
In order to tackle language variability in the
context of Machine Translation, a considerable ef-
fort has also been made to include deeper linguis-
tic information in automatic evaluation metrics,
both syntactic and semantic (see Section 2 for de-
tails). However, the most commonly used metrics
are still based on n-gram matching. The reason is
that the advantages of employing higher linguistic
processing levels have not been clarified yet.
The main goal of our work is to analyze to what
extent deep linguistic features can contribute to the
automatic evaluation of translation quality. For
that purpose, we compare – using four different
test beds – the performance of 16 n-gram based
metrics, 48 linguistic metrics and one combined
metric from the state of the art.
Analyzing the reliability of evaluation met-
rics requires meta-evaluation criteria. In this re-
spect, we identify important drawbacks of the
standard meta-evaluation methods based on cor-
relation with human judgements. In order to
overcome these drawbacks, we then introduce six
novel meta-evaluation criteria which represent dif-
ferent metric reliability dimensions. Our analysis
indicates that: (i) both lexical and linguistic met-
rics have complementary advantages and different
drawbacks; (ii) combining both kinds of metrics
is a more effective and robust evaluation method
across all meta-evaluation criteria.
In addition, we also perform a qualitative analy-
sis of one hundred sentences that were incorrectly
evaluated by state-of-the-art metrics. The analysis
confirms that deep linguistic techniques are neces-
sary to avoid the most common types of error.
Section 2 examines the state of the art Section 3
describes the test beds and metrics considered in
our experiments. In Section 4 the correlation be-
tween human assessors and metrics is computed,
with a discussion of its drawbacks. In Section 5
different quality aspects of metrics are analysed.
Conclusions are drawn in the last section.
</bodyText>
<page confidence="0.984151">
306
</page>
<note confidence="0.999628">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 306–314,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.8594535" genericHeader="introduction">
2 Previous Work on Machine
Translation Meta-Evaluation
</sectionHeader>
<bodyText confidence="0.999896761904762">
Insofar as automatic evaluation metrics for ma-
chine translation have been proposed, different
meta-evaluation frameworks have been gradually
introduced. For instance, Papineni et al. (2001)
introduced the BLEU metric and evaluated its re-
liability in terms of Pearson correlation with hu-
man assessments for adequacy and fluency judge-
ments. With the aim of overcoming some of the
deficiencies of BLEU, Doddington (2002) intro-
duced the NIST metric. Metric reliability was
also estimated in terms of correlation with human
assessments, but over different document sources
and for a varying number of references and seg-
ment sizes. Melamed et al. (2003) argued, at the
time of introducing the GTM metric, that Pearson
correlation coefficients can be affected by scale
properties, and suggested, in order to avoid this
effect, to use the non-parametric Spearman corre-
lation coefficients instead.
Lin and Och (2004) experimented, unlike pre-
vious works, with a wide set of metrics, including
NIST, WER (Nießen et al., 2000), PER (Tillmann
et al., 1997), and variants of ROUGE, BLEU and
GTM. They computed both Pearson and Spearman
correlation, obtaining similar results in both cases.
In a different work, Banerjee and Lavie (2005) ar-
gued that the measured reliability of metrics can
be due to averaging effects but might not be robust
across translations. In order to address this issue,
they computed the translation-by-translation cor-
relation with human judgements (i.e., correlation
at the segment level).
All that metrics were based on n-gram over-
lap. But there is also extensive research fo-
cused on including linguistic knowledge in met-
rics (Owczarzak et al., 2006; Reeder et al., 2001;
Liu and Gildea, 2005; Amig´o et al., 2006; Mehay
and Brew, 2007; Gim´enez and M`arquez, 2007;
Owczarzak et al., 2007; Popovic and Ney, 2007;
Gim´enez and M`arquez, 2008b) among others. In
all these cases, metrics were also evaluated by
means of correlation with human judgements.
In a different research line, several authors
have suggested approaching automatic evalua-
tion through the combination of individual metric
scores. Among the most relevant let us cite re-
search by Kulesza and Shieber (2004), Albrecht
and Hwa (2007). But finding optimal metric
combinations requires a meta-evaluation criterion.
Most approaches again rely on correlation with
human judgements. However, some of them mea-
sured the reliability of metric combinations in
terms of their ability to discriminate between hu-
man translations and automatic ones (human like-
ness) (Amig´o et al., 2005). .
In this work, we present a novel approach to
meta-evaluation which is distinguished by the use
of additional easily interpretable meta-evaluation
criteria oriented to measure different aspects of
metric reliability. We then apply this approach to
find out about the advantages and challenges of in-
cluding linguistic features in meta-evaluation cri-
teria.
</bodyText>
<sectionHeader confidence="0.979035" genericHeader="method">
3 Metrics and Test Beds
</sectionHeader>
<subsectionHeader confidence="0.998623">
3.1 Metric Set
</subsectionHeader>
<bodyText confidence="0.9999543125">
For our study, we have compiled a rich set of met-
ric variants at three linguistic levels: lexical, syn-
tactic, and semantic. In all cases, translation qual-
ity is measured by comparing automatic transla-
tions against a set of human references.
At the lexical level, we have included several
standard metrics, based on different similarity as-
sumptions: edit distance (WER, PER and TER),
lexical precision (BLEU and NIST), lexical recall
(ROUGE), and F-measure (GTM and METEOR). At
the syntactic level, we have used several families
of metrics based on dependency parsing (DP) and
constituency trees (CP). At the semantic level, we
have included three different families which op-
erate using named entities (NE), semantic roles
(SR), and discourse representations (DR). A de-
tailed description of these metrics can be found in
(Gim´enez and M`arquez, 2007).
Finally, we have also considered ULC, which
is a very simple approach to metric combina-
tion based on the unnormalized arithmetic mean
of metric scores, as described by Gim´enez and
M`arquez (2008a). ULC considers a subset of met-
rics which operate at several linguistic levels. This
approach has proven very effective in recent eval-
uation campaigns. Metric computation has been
carried out using the IQMT Framework for Auto-
matic MT Evaluation (Gim´enez, 2007)1. The sim-
plicity of this approach (with no training of the
metric weighting scheme) ensures that the poten-
tial advantages detected in our experiments are not
due to overfitting effects.
</bodyText>
<footnote confidence="0.983184">
1http://www.lsi.upc.edu/˜nlp/IQMT
</footnote>
<page confidence="0.994561">
307
</page>
<table confidence="0.9943342">
2004 2005
AE CE AE CE
#references 5 5 5 4
#systemsassessed 5 10 5+1 5
#casesassessed 347 447 266 272
</table>
<tableCaption confidence="0.9914885">
Table 1: NIST 2004/2005 MT Evaluation Cam-
paigns. Test bed description
</tableCaption>
<subsectionHeader confidence="0.999971">
3.2 Test Beds
</subsectionHeader>
<bodyText confidence="0.999934666666667">
We use the test beds from the 2004 and 2005
NIST MT Evaluation Campaigns (Le and Przy-
bocki, 2005)2. Both campaigns include two dif-
ferent translations exercises: Arabic-to-English
(‘AE’) and Chinese-to-English (‘CE’). Human as-
sessments of adequacy and fluency, on a 1-5 scale,
are available for a subset of sentences, each eval-
uated by two different human judges. A brief nu-
merical description of these test beds is available
in Table 1. The corpus AE05 includes, apart from
five automatic systems, one human-aided system
that is only used in our last experiment.
</bodyText>
<sectionHeader confidence="0.947425" genericHeader="method">
4 Correlation with Human Judgements
</sectionHeader>
<subsectionHeader confidence="0.99845">
4.1 Correlation at the Segment vs. System
Levels
</subsectionHeader>
<bodyText confidence="0.999930434782609">
Let us first analyze the correlation with human
judgements for linguistic vs. n-gram based met-
rics. Figure 1 shows the correlation obtained by
each automatic evaluation metric at system level
(horizontal axis) versus segment level (vertical
axis) in our test beds. Linguistic metrics are rep-
resented by grey plots, and black plots represent
metrics based on n-gram overlap.
The most remarkable aspect is that there exists
a certain trade-off between correlation at segment
versus system level. In fact, this graph produces
a negative Pearson correlation coefficient between
system and segment levels of 0.44. In other words,
depending on how the correlation is computed,
the relative predictive power of metrics can swap.
Therefore, we need additional meta-evaluation cri-
teria in order to clarify the behavior of linguistic
metrics as compared to n-gram based metrics.
However, there are some exceptions. Some
metrics achieve high correlation at both levels.
The first one is ULC (the circle in the plot), which
combines both kind of metrics in a heuristic way
(see Section 3.1). The metric nearest to ULC is
</bodyText>
<footnote confidence="0.920623">
2http://www.nist.gov/speech/tests/mt
</footnote>
<figureCaption confidence="0.9784675">
Figure 1: Averaged Pearson correlation at system
vs. segment level over all test beds.
</figureCaption>
<bodyText confidence="0.9993605">
DP-Or-?, which computes lexical overlapping but
on dependency relationships. These results are a
first evidence of the advantages of combining met-
rics at several linguistic processing levels.
</bodyText>
<subsectionHeader confidence="0.9985805">
4.2 Drawbacks of Correlation-based
Meta-evaluation
</subsectionHeader>
<bodyText confidence="0.999979230769231">
Although correlation with human judgements is
considered the standard meta-evaluation criterion,
it presents serious drawbacks. With respect to
correlation at system level, the main problem is
that the relative performance of different metrics
changes almost randomly between testbeds. One
of the reasons is that the number of assessed sys-
tems per testbed is usually low, and then correla-
tion has a small number of samples to be estimated
with. Usually, the correlation at system level is
computed over no more than a few systems.
For instance, Table 2 shows the best 10 met-
rics in CE05 according to their correlation with
human judges at the system level, and then the
ranking they obtain in the AE05 testbed. There
are substantial swaps between both rankings. In-
deed, the Pearson correlation of both ranks is only
0.26. This result supports the intuition in (Baner-
jee and Lavie, 2005) that correlation at segment
level is necessary to ensure the reliability of met-
rics in different situations.
However, the correlation values of metrics at
segment level have also drawbacks related to their
interpretability. Most metrics achieve a Pearson
coefficient lower than 0.5. Figure 2 shows two
possible relationships between human and metric
</bodyText>
<page confidence="0.998764">
308
</page>
<tableCaption confidence="0.836624">
Table 2: Metrics rankings according to correlation
with human judgements using CE05 vs. AE05
</tableCaption>
<figureCaption confidence="0.9528195">
Figure 2: Human judgements and scores of two
hypothetical metrics with Pearson correlation 0.5
</figureCaption>
<bodyText confidence="0.99956675">
produced scores. Both hypothetical metrics A and
B would achieve a 0.5 correlation. In the case
of Metric A, a high score implies a high human
assessed quality, but not the reverse. This is the
tendency hypothesized by Culy and Riehemann
(2003). In the case of Metric B, the high scored
translations can achieve both low or high quality
according to human judges but low scores ensure
low quality. Therefore, the same Pearson coeffi-
cient may hide very different behaviours. In this
work, we tackle these drawbacks by defining more
specific meta-evaluation criteria.
</bodyText>
<sectionHeader confidence="0.6892335" genericHeader="method">
5 Alternatives to Correlation-based
Meta-evaluation
</sectionHeader>
<bodyText confidence="0.9979848">
We have seen that correlation with human judge-
ments has serious limitations for metric evalua-
tion. Therefore, we have focused on other aspects
of metric reliability that have revealed differences
between n-gram and linguistic based metrics:
</bodyText>
<listItem confidence="0.99331775">
1. Is the metric able to accurately reveal im-
provements between two systems?
2. Can we trust the metric when it says that a
translation is very good or very bad?
</listItem>
<figureCaption confidence="0.879037">
Figure 3: SIP versus SIR
</figureCaption>
<listItem confidence="0.7912595">
3. Are metrics able to identify good translations
which are dissimilar from the models?
</listItem>
<bodyText confidence="0.99953">
We now discuss each of these aspects sepa-
rately.
</bodyText>
<subsectionHeader confidence="0.91685">
5.1 Ability of metrics to Reveal System
Improvements
</subsectionHeader>
<bodyText confidence="0.988531">
We now investigate to what extent a significant
system improvement according to the metric im-
plies a significant improvement according to hu-
man assessors, and viceversa. In other words: are
the metrics able to detect any quality improve-
ment? Is a metric score improvement a strong ev-
idence of quality increase? Knowing that a metric
has a 0.8 Pearson correlation at the system level or
0.5 at the segment level does not provide a direct
answer to this question.
In order to tackle this issue, we compare met-
rics versus human assessments in terms of pre-
cision and recall over statistically significant im-
provements within all system pairs in the test
beds. First, Table 3 shows the amount of signif-
icant improvements over human judgements ac-
cording to the Wilcoxon statistical significant test
(α &lt; 0.025). For instance, the testbed CE2004
consists of 10 systems, i.e. 45 system pairs; from
these, in 40 cases (rightmost column) one of the
systems significantly improves the other.
Now we would like to know, for every metric, if
the pairs which are significantly different accord-
ing to human judges are also the pairs which are
significantly different according to the metric.
Based on these data, we define two meta-
metrics: Significant Improvement Precision (SIP)
and Significant Improvement Recall (SIR). SIP
</bodyText>
<page confidence="0.989929">
309
</page>
<table confidence="0.997350666666667">
Systems System pairs Sig. imp.
CE2004 10 45 40
AE2004 5 10 8
CE2005 5 10 4
AE2005 5 10 6
Total 25 75 58
</table>
<tableCaption confidence="0.956259">
Table 3: System pairs with a significant difference
according to human judgements (Wilcoxon test)
</tableCaption>
<bodyText confidence="0.997059142857143">
(precision) represents the reliability of improve-
ments detected by metrics. SIR (recall) represents
to what extent the metric is able to cover the sig-
nificant improvements detected by humans. Let
Ih be the set of significant improvements detected
by human assessors and Im the set detected by the
metric m. Then:
</bodyText>
<equation confidence="0.574168">
SIP= |Ih n Im |SIR= |Ih n Im|
|Im ||Ih|
</equation>
<bodyText confidence="0.966598096774194">
Figure 3 shows the SIR and SIP values obtained
for each metric. Linguistic metrics achieve higher
precision values but at the cost of an important re-
call decrease. Given that linguistic metrics require
matching translation with references at additional
linguistic levels, the significant improvements de-
tected are more reliable (higher precision or SIP),
but at the cost of recall over real significant im-
provements (lower SIR).
This result supports the behaviour predicted in
(Gim´enez and M`arquez, 2009). Although linguis-
tic metrics were motivated by the idea of model-
ing linguistic variability, the practical effect is that
current linguistic metrics introduce additional re-
strictions (such as dependency tree overlap, for in-
stance) for accepting automatic translations. Then
they reward precision at the cost of recall in the
evaluation process, and this explains the high cor-
relation with human judgements at system level
with respect to segment level.
All n-gram based metrics achieve SIP and SIR
values between 0.8 and 0.9. This result suggests
that n-gram based metrics are reasonably reliable
for this purpose. Note that the combined met-
ric, ULC (the circle in the figure), achieves re-
sults comparable to n-gram based metrics with
this test3. That is, combining linguistic and n-
gram based metrics preserves the good behavior
of n-gram based metrics in this test.
3Notice that we just have 75 significant improvement
samples, so small differences in SIP or SIR have no relevance
</bodyText>
<subsectionHeader confidence="0.9603895">
5.2 Reliability of High and Low Metric
Scores
</subsectionHeader>
<bodyText confidence="0.999947347826087">
The issue tackled in this section is to what extent
a very low or high score according to the metric
is reliable for detecting extreme cases (very good
or very bad translations). In particular, note that
detecting wrong translations is crucial in order to
analyze the system drawbacks.
In order to define an accuracy measure for the
reliability of very low/high metric scores, it is nec-
essary to define quality thresholds for both the
human assessments and metric scales. Defining
thresholds for manual scores is immediate (e.g.,
lower than 4/10). However, each automatic evalu-
ation metric has its own scale properties. In order
to solve scaling problems we will focus on equiva-
lent rank positions: we associate the ith translation
according to the metric ranking with the quality
value manually assigned to the ith translation in
the manual ranking.
Being Qh(t) and Qm(t) the human and met-
ric assessed quality for the translation t, and being
rankh(t) and rankm(t) the rank of the translation
t according to humans and the metric, the normal-
ized metric assessed quality is:
</bodyText>
<equation confidence="0.998482">
QN.(t) = Qh(t�) |(rankh(t�) = rankm(t))
</equation>
<bodyText confidence="0.998810304347826">
In order to analyze the reliability of metrics
when identifying wrong or high quality transla-
tions, we look for contradictory results between
the metric and the assessments. In other words,
we look for metric errors in which the quality es-
timated by the metric is low (QN.(t) G 3) but the
quality assigned by assessors is high (Qh(t) &gt; 5)
or viceversa (QN.(t) &gt; 7 and Qh(t) G 4).
The vertical axis in Figure 4 represents the ra-
tio of errors in the set of low scored translations
according to a given metric. The horizontal axis
represents the ratio of errors over the set of high
scored translations. The first observation is that
all metrics are less reliable when they assign low
scores (which corresponds with the situation A de-
scribed in Section 4.2). For instance, the best met-
ric erroneously assigns a low score in more than
20% of the cases. In general, the linguistic met-
rics do not improve the ability to capture wrong
translations (horizontal axis in the figure). How-
ever, again, the combining metric ULC achieves
the same reliability as the best n-gram based met-
ric.
</bodyText>
<page confidence="0.99539">
310
</page>
<bodyText confidence="0.998560666666667">
In order to check the robustness of these results,
we computed the correlation of individual metric
failures between test beds, obtaining 0.67 Pearson
for the lowest correlated test bed pair (AE2004 and
CE2005) and 0.88 for the highest correlated pair
(AE2004 and CE2004).
</bodyText>
<figureCaption confidence="0.9552195">
Figure 4: Counter sample ratio for high vs low
metric scored translations
</figureCaption>
<subsectionHeader confidence="0.985133">
5.2.1 Analysis of Evaluation Samples
</subsectionHeader>
<bodyText confidence="0.999870684210526">
In order to shed some light on the reasons for the
automatic evaluation failures when assigning low
scores, we have manually analyzed cases in which
a metric score is low but the quality according to
humans is high (QN,,, &lt; 3 and Qh &gt; 7). We
have studied 100 sentence evaluation cases from
representatives of each metric family including: 1-
PER, BLEU, DP-Or-?, GTM (e = 2), METEOR
and ROUGEL. The evaluation cases have been ex-
tracted from the four test beds. We have identified
four main (non exclusive) failure causes:
Format issues, e.g. “US ” vs “United States”).
Elements such as abbreviations, acronyms or num-
bers which do not match the manual translation.
Pseudo-synonym terms, e.g. “US Scheduled the
Release” vs. “US set to Release”). ) In most of
these cases, synonymy can only be identified from
the discourse context. Therefore, terminological
resources (e.g., WordNet) are not enough to tackle
this problem.
Non relevant information omissions, e.g.
“Thank you” vs. “Thank you very much” or
“dollar” vs. “US dollar”)). The translation
system obviates some information which, in
context, is not considered crucial by the human
assessors. This effect is specially important in
short sentences.
Incorrect structures that change the meaning
while maintaining the same idea (e.g., “Bush
Praises NASA ’s Mars Mission” vs “ Bush praises
nasa of Mars mission” ).
Note that all of these kinds of failure - except
formatting issues - require deep linguistic process-
ing while n-gram overlap or even synonyms ex-
tracted from a standard ontology are not enough to
deal with them. This conclusion motivates the in-
corporation of linguistic processing into automatic
evaluation metrics.
</bodyText>
<subsectionHeader confidence="0.984072">
5.3 Ability to Deal with Translations that are
Dissimilar to References.
</subsectionHeader>
<bodyText confidence="0.999991333333333">
The results presented in Section 5.2 indicate that a
high score in metrics tends to be highly related to
truly good translations. This is due to the fact that
a high word overlapping with human references is
a reliable evidence of quality. However, in some
cases the translations to be evaluated are not so
similar to human references.
An example of this appears in the test bed
NIST05AE which includes a human-aided sys-
tem, LinearB (Callison-Burch, 2005). This system
produces correct translations whose words do not
necessarily overlap with references. On the other
hand, a statistics based system tends to produce
incorrect translations with a high level of lexical
overlapping with the set of human references. This
case was reported by Callison-Burch et al. (2006)
and later studied by Gim´enez and M`arquez (2007).
They found out that lexical metrics fail to pro-
duce reliable evaluation scores. They favor sys-
tems which share the expected reference sublan-
guage (e.g., statistical) and penalize those which
do not (e.g., LinearB).
We can find in our test bed many instances in
which the statistical systems obtain a metric score
similar to the assisted system while achieving a
lower mark according to human assessors. For in-
stance, for the following translations, ROUGEL
assigns a slightly higher score to the output of a
statistical system which contains a lot of grammat-
ical and syntactical failures.
</bodyText>
<footnote confidence="0.766992">
Human assisted system: The Chinese President made un-
precedented criticism of the leaders of Hong Kong after
political failings in the former British colony on Mon-
day. Human assessment=8.5.
Statistical system: Chinese President Hu Jintao today un-
precedented criticism to the leaders of Hong Kong
wake political and financial failure in the former
British colony. Human assessment=3.
</footnote>
<page confidence="0.996454">
311
</page>
<figureCaption confidence="0.991247">
Figure 5: Maximum translation quality decreasing
over similarly scored translation pairs.
</figureCaption>
<bodyText confidence="0.9999199">
In order to check the metric resistance to be
cheated by translations with high lexical over-
lapping, we estimate the quality decrease that
we could cause if we optimized the human-aided
translations according to the automatic metric. For
this, we consider in each translation case c, the
worse automatic translation t that equals or im-
proves the human-aided translation th according
to the automatic metric m. Formally the averaged
quality decrease is:
</bodyText>
<equation confidence="0.9586915">
Quality decrease(m) =
Avg,(maxt(Qh(th) − Qh(t)|Qm(th) :5 Qm(t)))
</equation>
<bodyText confidence="0.9994663">
Figure 5 illustrates the results obtained. All
metrics are suitable to be cheated, assigning sim-
ilar or higher scores to worse translations. How-
ever, linguistic metrics are more resistant. In addi-
tion, the combined metric ULC obtains the best re-
sults, better than both linguistic and n-gram based
metrics. Our conclusion is that including higher
linguistic levels in metrics is relevant to prevent
ungrammatical n-gram matching to achieve simi-
lar scores than grammatical constructions.
</bodyText>
<subsectionHeader confidence="0.999215">
5.4 The Oracle System Test
</subsectionHeader>
<bodyText confidence="0.9998791">
In order to obtain additional evidence about the
usefulness of combining evaluation metrics at dif-
ferent processing levels, let us consider the follow-
ing situation: given a set of reference translations
we want to train a combined system that takes
the most appropriate translation approach for each
text segment. We consider the set of translations
system presented in each competition as the trans-
lation approaches pool. Then, the upper bound on
the quality of the combined system is given by the
</bodyText>
<table confidence="0.999027555555556">
Metric OST
maxOST 6.72
ULC 5.79
ROUGEW 5.71
DP-OT-* 5.70
CP-Oc-* 5.70
NIST 5.70
randOST 5.20
minOST 3.67
</table>
<tableCaption confidence="0.891041">
Table 4: Metrics ranked according to the Oracle
System Test
</tableCaption>
<bodyText confidence="0.999080375">
predictive power of the employed automatic eval-
uation metric. This upper bound is obtained by se-
lecting the highest scored translation t according
to a specific metric m for each translation case c.
The Oracle System Test (OST) consists of com-
puting the averaged human assessed quality Qh
of the selected translations according to human as-
sessors across all cases. Formally:
</bodyText>
<equation confidence="0.919887">
OST(m) = Avgc(Qh(Argmaxt(Qr..(t))|t E c))
</equation>
<bodyText confidence="0.999965142857143">
We use the sum of adequacy and fluency, both
in a 1-5 scale, as a global quality measure. Thus,
OST scores are in a 2-10 range. In summary,
the OST represents the best combined system that
could be trained according to a specific automatic
evaluation metric.
Table 4 shows OST values obtained for the best
metrics. In the table we have also included a ran-
dom, a maximum (always pick the best transla-
tion according to humans) and a minimum (al-
ways pick the worse translation according to hu-
man) OST for all 4. The most remarkable result
in Table 4 is that metrics are closer to the random
baseline than to the upperbound (maximum OST).
This result confirms the idea that an improvement
on metric reliability could contribute considerably
to the systems optimization process. However, the
key point is that the combined metric, ULC, im-
proves all the others (5.79 vs. 5.71), indicating
the importance of combining n-gram and linguis-
tic features.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9983965">
Our experiments show that, on one hand, tradi-
tional n-gram based metrics are more or equally
</bodyText>
<footnote confidence="0.9798125">
4In all our experiments, the meta-metric values are com-
puted over each test bed independently before averaging in
order to assign equal relevance to the four possible contexts
(test beds)
</footnote>
<page confidence="0.996078">
312
</page>
<bodyText confidence="0.999971095238096">
reliable for estimating the translation quality at the
segment level, for predicting significant improve-
ment between systems and for detecting poor and
excellent translations.
On the other hand, linguistically motivated met-
rics improve n-gram metrics in two ways: (i) they
achieve higher correlation with human judgements
at system level and (ii) they are more resistant to
reward poor translations with high word overlap-
ping with references.
The underlying phenomenon is that, rather
than managing the linguistics variability, linguis-
tic based metrics introduce additional restrictions
for assigning high scores. This effect decreases
the recall over significant system improvements
achieved by n-gram based metrics and does not
solve the problem of detecting wrong translations.
Linguistic metrics, however, are more difficult to
cheat.
In general, the greatest pitfall of metrics is the
low reliability of low metric values. Our qualita-
tive analysis of evaluated sentences has shown that
deeper linguistic techniques are necessary to over-
come the important surface differences between
acceptable automatic translations and human ref-
erences.
But our key finding is that combining both kinds
of metrics gives top performance according to ev-
ery meta-evaluation criteria. In addition, our Com-
bined System Test shows that, when training a
combined translation system, using metrics at sev-
eral linguistic processing levels improves substan-
tially the use of individual metrics.
In summary, our results motivate: (i) work-
ing on new linguistic metrics for overcoming the
barrier of linguistic variability and (ii) perform-
ing new metric combining schemes based on lin-
ear regression over human judgements (Kulesza
and Shieber, 2004), training models over hu-
man/machine discrimination (Albrecht and Hwa,
2007) or non parametric methods based on refer-
ence to reference distances (Amig´o et al., 2005).
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999008">
This work has been partially supported by the
Spanish Government, project INES/Text-Mess.
We are indebted to the three ACL anonymous re-
viewers which provided detailed suggestions to
improve our work.
</bodyText>
<sectionHeader confidence="0.983195" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999759296296296">
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for Sentence-Level MT Evaluation with Pseudo Ref-
erences. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 296–303.
Enrique Amig´o, Julio Gonzalo, Anselmo Pe nas, and
Felisa Verdejo. 2005. QARLA: a Framework for
the Evaluation of Automatic Summarization. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
280–289.
Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and
Llu´ıs M`arquez. 2006. MT Evaluation: Human-
Like vs. Human Acceptable. In Proceedings of
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL), pages 17–24.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in
Machine Translation Research. In Proceedings of
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL).
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Christopher Culy and Susanne Z. Riehemann. 2003.
The Limits of N-gram Translation Evaluation Met-
rics. In Proceedings of MT-SUMMIT IX, pages 1–8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology, pages 138–145.
Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256–264.
Jes´us Gim´enez and Llu´ıs M`arquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319–326.
Jes´us Gim´enez and Llu´ıs M`arquez. 2008b. On the Ro-
bustness of Linguistic Features for Automatic MT
Evaluation. (Under submission).
</reference>
<page confidence="0.992377">
313
</page>
<reference confidence="0.999877168831169">
Jes´us Gim´enez and Llu´ıs M`arquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Jes´us Gim´enez. 2007. IQMT v 2.0. Technical Manual
(LSI-07-29-R). Technical report, TALP Research
Center. LSI Department. http://www.lsi.
upc.edu/˜nlp/IQMT/IQMT.v2.1.pdf.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI), pages 75–84.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Offi-
cial release of automatic evaluation scores for all
submissions, August.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
Evaluation of Machine Translation Quality Using
Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25–32.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evalu-
ation. In Proceedings of the 11th Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technology and the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL).
Sonja Nießen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation (LREC).
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), pages 148–155.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Labelled Dependencies in Machine
Translation Evaluation. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
104–111.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, RC22176. Technical
report, IBM T.J. Watson Research Center.
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Appli-
cations for Error Analysis. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 48–55, Prague, Czech Republic, June.
Association for Computational Linguistics.
Florence Reeder, Keith Miller, Jennifer Doyon, and
John White. 2001. The Naming of Things and
the Confusion of Tongues: an MT Metric. In Pro-
ceedings of the Workshop on MT Evaluation ”Who
did what to whom?” at Machine Translation Summit
VIII, pages 55–59.
Christoph Tillmann, Stefan Vogel, Hermann Ney,
A. Zubiaga, and H. Sawaf. 1997. Accelerated DP
based Search for Statistical Translation. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology.
</reference>
<page confidence="0.999134">
314
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.202604">
<title confidence="0.999682">The Contribution of Linguistic Features to Automatic Machine Translation Evaluation</title>
<author confidence="0.688445">Jes´us Julio Gonzalo Felisa Madrid</author>
<email confidence="0.750185">enrique@lsi.uned.es</email>
<email confidence="0.750185">julio@lsi.uned.es</email>
<email confidence="0.750185">felisa@lsi.uned.es</email>
<author confidence="0.598417">Barcelona</author>
<email confidence="0.999858">jgimenez@lsi.upc.edu</email>
<abstract confidence="0.996959318181818">A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested. Howbased metrics are still today the dominant approach. The main reason is that the advantages of employing deeper linguistic information have not been clarified yet. In this work, we propose a novel approach for meta-evaluation of MT evaluation metrics, since correlation cofficient against human judges do not reveal details about the advantages and disadvantages of particular metrics. We then use this approach to investigate the benefits of introducing linguistic features into evaluation metrics. Overall, our experiments show that (i) both lexical and linguistic metrics present complementary advantages and (ii) combining both kinds of metrics yields the most robust metaevaluation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>Regression for Sentence-Level MT Evaluation with Pseudo References.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>296--303</pages>
<contexts>
<context position="6191" citStr="Albrecht and Hwa (2007)" startWordPosition="937" endWordPosition="940">cused on including linguistic knowledge in metrics (Owczarzak et al., 2006; Reeder et al., 2001; Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Gim´enez and M`arquez, 2007; Owczarzak et al., 2007; Popovic and Ney, 2007; Gim´enez and M`arquez, 2008b) among others. In all these cases, metrics were also evaluated by means of correlation with human judgements. In a different research line, several authors have suggested approaching automatic evaluation through the combination of individual metric scores. Among the most relevant let us cite research by Kulesza and Shieber (2004), Albrecht and Hwa (2007). But finding optimal metric combinations requires a meta-evaluation criterion. Most approaches again rely on correlation with human judgements. However, some of them measured the reliability of metric combinations in terms of their ability to discriminate between human translations and automatic ones (human likeness) (Amig´o et al., 2005). . In this work, we present a novel approach to meta-evaluation which is distinguished by the use of additional easily interpretable meta-evaluation criteria oriented to measure different aspects of metric reliability. We then apply this approach to find out</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2007. Regression for Sentence-Level MT Evaluation with Pseudo References. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 296–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
</authors>
<title>Anselmo Pe nas, and Felisa Verdejo.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>280--289</pages>
<marker>Amig´o, Gonzalo, 2005</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Anselmo Pe nas, and Felisa Verdejo. 2005. QARLA: a Framework for the Evaluation of Automatic Summarization. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 280–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Jes´us Gim´enez</author>
<author>Julio Gonzalo</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>MT Evaluation: HumanLike vs. Human Acceptable.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL),</booktitle>
<pages>17--24</pages>
<marker>Amig´o, Gim´enez, Gonzalo, M`arquez, 2006</marker>
<rawString>Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Llu´ıs M`arquez. 2006. MT Evaluation: HumanLike vs. Human Acceptable. In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="5203" citStr="Banerjee and Lavie (2005)" startWordPosition="780" endWordPosition="783">eferences and segment sizes. Melamed et al. (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al., 2000), PER (Tillmann et al., 1997), and variants of ROUGE, BLEU and GTM. They computed both Pearson and Spearman correlation, obtaining similar results in both cases. In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations. In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level). All that metrics were based on n-gram overlap. But there is also extensive research focused on including linguistic knowledge in metrics (Owczarzak et al., 2006; Reeder et al., 2001; Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Gim´enez and M`arquez, 2007; Owczarzak et al., 2007; Popovic and Ney, 200</context>
<context position="11658" citStr="Banerjee and Lavie, 2005" startWordPosition="1795" endWordPosition="1799">ndomly between testbeds. One of the reasons is that the number of assessed systems per testbed is usually low, and then correlation has a small number of samples to be estimated with. Usually, the correlation at system level is computed over no more than a few systems. For instance, Table 2 shows the best 10 metrics in CE05 according to their correlation with human judges at the system level, and then the ranking they obtain in the AE05 testbed. There are substantial swaps between both rankings. Indeed, the Pearson correlation of both ranks is only 0.26. This result supports the intuition in (Banerjee and Lavie, 2005) that correlation at segment level is necessary to ensure the reliability of metrics in different situations. However, the correlation values of metrics at segment level have also drawbacks related to their interpretability. Most metrics achieve a Pearson coefficient lower than 0.5. Figure 2 shows two possible relationships between human and metric 308 Table 2: Metrics rankings according to correlation with human judgements using CE05 vs. AE05 Figure 2: Human judgements and scores of two hypothetical metrics with Pearson correlation 0.5 produced scores. Both hypothetical metrics A and B would </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the Role of BLEU in Machine Translation Research.</title>
<date>2006</date>
<booktitle>In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="21945" citStr="Callison-Burch et al. (2006)" startWordPosition="3480" endWordPosition="3483"> to the fact that a high word overlapping with human references is a reliable evidence of quality. However, in some cases the translations to be evaluated are not so similar to human references. An example of this appears in the test bed NIST05AE which includes a human-aided system, LinearB (Callison-Burch, 2005). This system produces correct translations whose words do not necessarily overlap with references. On the other hand, a statistics based system tends to produce incorrect translations with a high level of lexical overlapping with the set of human references. This case was reported by Callison-Burch et al. (2006) and later studied by Gim´enez and M`arquez (2007). They found out that lexical metrics fail to produce reliable evaluation scores. They favor systems which share the expected reference sublanguage (e.g., statistical) and penalize those which do not (e.g., LinearB). We can find in our test bed many instances in which the statistical systems obtain a metric score similar to the assisted system while achieving a lower mark according to human assessors. For instance, for the following translations, ROUGEL assigns a slightly higher score to the output of a statistical system which contains a lot o</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the Role of BLEU in Machine Translation Research. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Linear B system description for the 2005 NIST MT evaluation exercise.</title>
<date>2005</date>
<booktitle>In Proceedings of the NIST 2005 Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="21631" citStr="Callison-Burch, 2005" startWordPosition="3434" endWordPosition="3435">sion motivates the incorporation of linguistic processing into automatic evaluation metrics. 5.3 Ability to Deal with Translations that are Dissimilar to References. The results presented in Section 5.2 indicate that a high score in metrics tends to be highly related to truly good translations. This is due to the fact that a high word overlapping with human references is a reliable evidence of quality. However, in some cases the translations to be evaluated are not so similar to human references. An example of this appears in the test bed NIST05AE which includes a human-aided system, LinearB (Callison-Burch, 2005). This system produces correct translations whose words do not necessarily overlap with references. On the other hand, a statistics based system tends to produce incorrect translations with a high level of lexical overlapping with the set of human references. This case was reported by Callison-Burch et al. (2006) and later studied by Gim´enez and M`arquez (2007). They found out that lexical metrics fail to produce reliable evaluation scores. They favor systems which share the expected reference sublanguage (e.g., statistical) and penalize those which do not (e.g., LinearB). We can find in our </context>
</contexts>
<marker>Callison-Burch, 2005</marker>
<rawString>Chris Callison-Burch. 2005. Linear B system description for the 2005 NIST MT evaluation exercise. In Proceedings of the NIST 2005 Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Culy</author>
<author>Susanne Z Riehemann</author>
</authors>
<title>The Limits of N-gram Translation Evaluation Metrics.</title>
<date>2003</date>
<booktitle>In Proceedings of MT-SUMMIT IX,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="12445" citStr="Culy and Riehemann (2003)" startWordPosition="1919" endWordPosition="1922">t level have also drawbacks related to their interpretability. Most metrics achieve a Pearson coefficient lower than 0.5. Figure 2 shows two possible relationships between human and metric 308 Table 2: Metrics rankings according to correlation with human judgements using CE05 vs. AE05 Figure 2: Human judgements and scores of two hypothetical metrics with Pearson correlation 0.5 produced scores. Both hypothetical metrics A and B would achieve a 0.5 correlation. In the case of Metric A, a high score implies a high human assessed quality, but not the reverse. This is the tendency hypothesized by Culy and Riehemann (2003). In the case of Metric B, the high scored translations can achieve both low or high quality according to human judges but low scores ensure low quality. Therefore, the same Pearson coefficient may hide very different behaviours. In this work, we tackle these drawbacks by defining more specific meta-evaluation criteria. 5 Alternatives to Correlation-based Meta-evaluation We have seen that correlation with human judgements has serious limitations for metric evaluation. Therefore, we have focused on other aspects of metric reliability that have revealed differences between n-gram and linguistic </context>
</contexts>
<marker>Culy, Riehemann, 2003</marker>
<rawString>Christopher Culy and Susanne Z. Riehemann. 2003. The Limits of N-gram Translation Evaluation Metrics. In Proceedings of MT-SUMMIT IX, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="4398" citStr="Doddington (2002)" startWordPosition="654" endWordPosition="655">s of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 306–314, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Previous Work on Machine Translation Meta-Evaluation Insofar as automatic evaluation metrics for machine translation have been proposed, different meta-evaluation frameworks have been gradually introduced. For instance, Papineni et al. (2001) introduced the BLEU metric and evaluated its reliability in terms of Pearson correlation with human assessments for adequacy and fluency judgements. With the aim of overcoming some of the deficiencies of BLEU, Doddington (2002) introduced the NIST metric. Metric reliability was also estimated in terms of correlation with human assessments, but over different document sources and for a varying number of references and segment sizes. Melamed et al. (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al., 2000), P</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics. In Proceedings of the 2nd International Conference on Human Language Technology, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>256--264</pages>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 256–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Heterogeneous Automatic MT Evaluation Through NonParametric Metric Combinations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>319--326</pages>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2008a. Heterogeneous Automatic MT Evaluation Through NonParametric Metric Combinations. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP), pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>On the Robustness of Linguistic Features for Automatic MT Evaluation. (Under submission).</title>
<date>2008</date>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2008b. On the Robustness of Linguistic Features for Automatic MT Evaluation. (Under submission).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>On the Robustness of Syntactic and Semantic Features for Automatic MT Evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation (EACL</booktitle>
<marker>Gim´enez, M`arquez, 2009</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2009. On the Robustness of Syntactic and Semantic Features for Automatic MT Evaluation. In Proceedings of the 4th Workshop on Statistical Machine Translation (EACL 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
</authors>
<title>IQMT v 2.0.</title>
<date>2007</date>
<tech>Technical Manual (LSI-07-29-R). Technical report, TALP Research Center. LSI Department. http://www.lsi. upc.edu/˜nlp/IQMT/IQMT.v2.1.pdf.</tech>
<marker>Gim´enez, 2007</marker>
<rawString>Jes´us Gim´enez. 2007. IQMT v 2.0. Technical Manual (LSI-07-29-R). Technical report, TALP Research Center. LSI Department. http://www.lsi. upc.edu/˜nlp/IQMT/IQMT.v2.1.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<pages>75--84</pages>
<contexts>
<context position="6166" citStr="Kulesza and Shieber (2004)" startWordPosition="933" endWordPosition="936">s also extensive research focused on including linguistic knowledge in metrics (Owczarzak et al., 2006; Reeder et al., 2001; Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Gim´enez and M`arquez, 2007; Owczarzak et al., 2007; Popovic and Ney, 2007; Gim´enez and M`arquez, 2008b) among others. In all these cases, metrics were also evaluated by means of correlation with human judgements. In a different research line, several authors have suggested approaching automatic evaluation through the combination of individual metric scores. Among the most relevant let us cite research by Kulesza and Shieber (2004), Albrecht and Hwa (2007). But finding optimal metric combinations requires a meta-evaluation criterion. Most approaches again rely on correlation with human judgements. However, some of them measured the reliability of metric combinations in terms of their ability to discriminate between human translations and automatic ones (human likeness) (Amig´o et al., 2005). . In this work, we present a novel approach to meta-evaluation which is distinguished by the use of additional easily interpretable meta-evaluation criteria oriented to measure different aspects of metric reliability. We then apply </context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), pages 75–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Le and Mark Przybocki</author>
</authors>
<title>machine translation evaluation official results. In Official release of automatic evaluation scores for all submissions,</title>
<date>2005</date>
<contexts>
<context position="8745" citStr="Przybocki, 2005" startWordPosition="1343" endWordPosition="1345">Metric computation has been carried out using the IQMT Framework for Automatic MT Evaluation (Gim´enez, 2007)1. The simplicity of this approach (with no training of the metric weighting scheme) ensures that the potential advantages detected in our experiments are not due to overfitting effects. 1http://www.lsi.upc.edu/˜nlp/IQMT 307 2004 2005 AE CE AE CE #references 5 5 5 4 #systemsassessed 5 10 5+1 5 #casesassessed 347 447 266 272 Table 1: NIST 2004/2005 MT Evaluation Campaigns. Test bed description 3.2 Test Beds We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przybocki, 2005)2. Both campaigns include two different translations exercises: Arabic-to-English (‘AE’) and Chinese-to-English (‘CE’). Human assessments of adequacy and fluency, on a 1-5 scale, are available for a subset of sentences, each evaluated by two different human judges. A brief numerical description of these test beds is available in Table 1. The corpus AE05 includes, apart from five automatic systems, one human-aided system that is only used in our last experiment. 4 Correlation with Human Judgements 4.1 Correlation at the Segment vs. System Levels Let us first analyze the correlation with human j</context>
</contexts>
<marker>Przybocki, 2005</marker>
<rawString>Audrey Le and Mark Przybocki. 2005. NIST 2005 machine translation evaluation official results. In Official release of automatic evaluation scores for all submissions, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4888" citStr="Lin and Och (2004)" startWordPosition="729" endWordPosition="732"> assessments for adequacy and fluency judgements. With the aim of overcoming some of the deficiencies of BLEU, Doddington (2002) introduced the NIST metric. Metric reliability was also estimated in terms of correlation with human assessments, but over different document sources and for a varying number of references and segment sizes. Melamed et al. (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al., 2000), PER (Tillmann et al., 1997), and variants of ROUGE, BLEU and GTM. They computed both Pearson and Spearman correlation, obtaining similar results in both cases. In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations. In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level). All th</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="5685" citStr="Liu and Gildea, 2005" startWordPosition="858" endWordPosition="861">ey computed both Pearson and Spearman correlation, obtaining similar results in both cases. In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations. In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level). All that metrics were based on n-gram overlap. But there is also extensive research focused on including linguistic knowledge in metrics (Owczarzak et al., 2006; Reeder et al., 2001; Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Gim´enez and M`arquez, 2007; Owczarzak et al., 2007; Popovic and Ney, 2007; Gim´enez and M`arquez, 2008b) among others. In all these cases, metrics were also evaluated by means of correlation with human judgements. In a different research line, several authors have suggested approaching automatic evaluation through the combination of individual metric scores. Among the most relevant let us cite research by Kulesza and Shieber (2004), Albrecht and Hwa (2007). But finding optimal metric combinations requires a meta-evaluation criterion. Most approache</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Mehay</author>
<author>Chris Brew</author>
</authors>
<title>BLEUATRE: Flattening Syntactic Dependencies for MT Evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).</booktitle>
<contexts>
<context position="5728" citStr="Mehay and Brew, 2007" startWordPosition="866" endWordPosition="869">lation, obtaining similar results in both cases. In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations. In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level). All that metrics were based on n-gram overlap. But there is also extensive research focused on including linguistic knowledge in metrics (Owczarzak et al., 2006; Reeder et al., 2001; Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Gim´enez and M`arquez, 2007; Owczarzak et al., 2007; Popovic and Ney, 2007; Gim´enez and M`arquez, 2008b) among others. In all these cases, metrics were also evaluated by means of correlation with human judgements. In a different research line, several authors have suggested approaching automatic evaluation through the combination of individual metric scores. Among the most relevant let us cite research by Kulesza and Shieber (2004), Albrecht and Hwa (2007). But finding optimal metric combinations requires a meta-evaluation criterion. Most approaches again rely on correlation with human judg</context>
</contexts>
<marker>Mehay, Brew, 2007</marker>
<rawString>Dennis Mehay and Chris Brew. 2007. BLEUATRE: Flattening Syntactic Dependencies for MT Evaluation. In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="4628" citStr="Melamed et al. (2003)" startWordPosition="689" endWordPosition="692">ation metrics for machine translation have been proposed, different meta-evaluation frameworks have been gradually introduced. For instance, Papineni et al. (2001) introduced the BLEU metric and evaluated its reliability in terms of Pearson correlation with human assessments for adequacy and fluency judgements. With the aim of overcoming some of the deficiencies of BLEU, Doddington (2002) introduced the NIST metric. Metric reliability was also estimated in terms of correlation with human assessments, but over different document sources and for a varying number of references and segment sizes. Melamed et al. (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al., 2000), PER (Tillmann et al., 1997), and variants of ROUGE, BLEU and GTM. They computed both Pearson and Spearman correlation, obtaining similar results in both cases. In a different work, Banerjee and Lavie (2005) argued that the measured</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="4995" citStr="Nießen et al., 2000" startWordPosition="747" endWordPosition="750">BLEU, Doddington (2002) introduced the NIST metric. Metric reliability was also estimated in terms of correlation with human assessments, but over different document sources and for a varying number of references and segment sizes. Melamed et al. (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al., 2000), PER (Tillmann et al., 1997), and variants of ROUGE, BLEU and GTM. They computed both Pearson and Spearman correlation, obtaining similar results in both cases. In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations. In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level). All that metrics were based on n-gram overlap. But there is also extensive research focused on including linguist</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Declan Groves</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Contextual BitextDerived Paraphrases in Automatic MT Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>148--155</pages>
<marker>Owczarzak, Groves, Van Genabith, Way, 2006</marker>
<rawString>Karolina Owczarzak, Declan Groves, Josef Van Genabith, and Andy Way. 2006. Contextual BitextDerived Paraphrases in Automatic MT Evaluation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA), pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Labelled Dependencies in Machine Translation Evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>104--111</pages>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Labelled Dependencies in Machine Translation Evaluation. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation,</title>
<date>2001</date>
<tech>RC22176. Technical report, IBM</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="4170" citStr="Papineni et al. (2001)" startWordPosition="615" endWordPosition="618"> Section 4 the correlation between human assessors and metrics is computed, with a discussion of its drawbacks. In Section 5 different quality aspects of metrics are analysed. Conclusions are drawn in the last section. 306 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 306–314, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Previous Work on Machine Translation Meta-Evaluation Insofar as automatic evaluation metrics for machine translation have been proposed, different meta-evaluation frameworks have been gradually introduced. For instance, Papineni et al. (2001) introduced the BLEU metric and evaluated its reliability in terms of Pearson correlation with human assessments for adequacy and fluency judgements. With the aim of overcoming some of the deficiencies of BLEU, Doddington (2002) introduced the NIST metric. Metric reliability was also estimated in terms of correlation with human assessments, but over different document sources and for a varying number of references and segment sizes. Melamed et al. (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation, RC22176. Technical report, IBM T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovic</author>
<author>Hermann Ney</author>
</authors>
<title>Word Error Rates: Decomposition over POS classes and Applications for Error Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>48--55</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5804" citStr="Popovic and Ney, 2007" startWordPosition="878" endWordPosition="881">jee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations. In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level). All that metrics were based on n-gram overlap. But there is also extensive research focused on including linguistic knowledge in metrics (Owczarzak et al., 2006; Reeder et al., 2001; Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Gim´enez and M`arquez, 2007; Owczarzak et al., 2007; Popovic and Ney, 2007; Gim´enez and M`arquez, 2008b) among others. In all these cases, metrics were also evaluated by means of correlation with human judgements. In a different research line, several authors have suggested approaching automatic evaluation through the combination of individual metric scores. Among the most relevant let us cite research by Kulesza and Shieber (2004), Albrecht and Hwa (2007). But finding optimal metric combinations requires a meta-evaluation criterion. Most approaches again rely on correlation with human judgements. However, some of them measured the reliability of metric combination</context>
</contexts>
<marker>Popovic, Ney, 2007</marker>
<rawString>Maja Popovic and Hermann Ney. 2007. Word Error Rates: Decomposition over POS classes and Applications for Error Analysis. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florence Reeder</author>
<author>Keith Miller</author>
<author>Jennifer Doyon</author>
<author>John White</author>
</authors>
<title>The Naming of Things and the Confusion of Tongues: an MT Metric.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on MT Evaluation ”Who did what to whom?” at Machine Translation Summit VIII,</booktitle>
<pages>55--59</pages>
<contexts>
<context position="5663" citStr="Reeder et al., 2001" startWordPosition="854" endWordPosition="857">UGE, BLEU and GTM. They computed both Pearson and Spearman correlation, obtaining similar results in both cases. In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations. In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level). All that metrics were based on n-gram overlap. But there is also extensive research focused on including linguistic knowledge in metrics (Owczarzak et al., 2006; Reeder et al., 2001; Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Gim´enez and M`arquez, 2007; Owczarzak et al., 2007; Popovic and Ney, 2007; Gim´enez and M`arquez, 2008b) among others. In all these cases, metrics were also evaluated by means of correlation with human judgements. In a different research line, several authors have suggested approaching automatic evaluation through the combination of individual metric scores. Among the most relevant let us cite research by Kulesza and Shieber (2004), Albrecht and Hwa (2007). But finding optimal metric combinations requires a meta-evaluation cri</context>
</contexts>
<marker>Reeder, Miller, Doyon, White, 2001</marker>
<rawString>Florence Reeder, Keith Miller, Jennifer Doyon, and John White. 2001. The Naming of Things and the Confusion of Tongues: an MT Metric. In Proceedings of the Workshop on MT Evaluation ”Who did what to whom?” at Machine Translation Summit VIII, pages 55–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based Search for Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="5024" citStr="Tillmann et al., 1997" startWordPosition="752" endWordPosition="755">roduced the NIST metric. Metric reliability was also estimated in terms of correlation with human assessments, but over different document sources and for a varying number of references and segment sizes. Melamed et al. (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al., 2000), PER (Tillmann et al., 1997), and variants of ROUGE, BLEU and GTM. They computed both Pearson and Spearman correlation, obtaining similar results in both cases. In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations. In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level). All that metrics were based on n-gram overlap. But there is also extensive research focused on including linguistic knowledge in metrics (Owcz</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based Search for Statistical Translation. In Proceedings of European Conference on Speech Communication and Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>