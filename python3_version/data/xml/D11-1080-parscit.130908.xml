<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.99063">
Statistical Machine Translation with Local Language Models
</title>
<author confidence="0.99176">
Christof Monz
</author>
<affiliation confidence="0.988762">
Informatics Institute, University of Amsterdam
</affiliation>
<address confidence="0.741108">
P.O. Box 94323, 1090 GH Amsterdam, The Netherlands
</address>
<email confidence="0.944525">
c.monz@uva.nl
</email>
<sectionHeader confidence="0.978161" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999268">
Part-of-speech language modeling is com-
monly used as a component in statistical ma-
chine translation systems, but there is mixed
evidence that its usage leads to significant im-
provements. We argue that its limited effec-
tiveness is due to the lack of lexicalization.
We introduce a new approach that builds a
separate local language model for each word
and part-of-speech pair. The resulting mod-
els lead to more context-sensitive probabil-
ity distributions and we also exploit the fact
that different local models are used to esti-
mate the language model probability of each
word during decoding. Our approach is evalu-
ated for Arabic- and Chinese-to-English trans-
lation. We show that it leads to statistically
significant improvements for multiple test sets
and also across different genres, when com-
pared against a competitive baseline and a sys-
tem using a part-of-speech model.
</bodyText>
<sectionHeader confidence="0.995167" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955227272727">
Language models are an important component of
current statistical machine translation systems. They
affect the selection of phrase translation candidates
and reordering choices by estimating the probability
that an application of a phrase translation is a flu-
ent continuation of the current translation hypoth-
esis. The size and domain of the language model
can have a significant impact on translation quality.
Brants et al. (2007) have shown that each doubling
of the training data from the news domain (used to
build the language model), leads to improvements of
approximately 0.5 BLEU points. On the other hand,
each doubling using general web data leads to im-
provements of approximately 0.15 BLEU points.
While large n-gram language models do lead
to improved translation quality, they still lack any
generalization beyond the surface forms (Schwenk,
2007). Consider example (1), which is a short sen-
tence fragment from the MT09 Arabic-English test
set, with the corresponding machine translation out-
put (1.b), from a phrase-based statistical machine
translation system, and reference translation (1.c).
</bodyText>
<equation confidence="0.313901333333333">
(1) a. ÈYj. ÊË b�.�JÓ &lt;J��¯Am�• �HAm�
Qå”~~ �éJ��®Ê g ...
... éË ÑêÓAî~E@ð
</equation>
<bodyText confidence="0.960312714285714">
b. ... the background of press statements of
controversial and accused him ...
c. ... the background of controversial press
statements and accused him ...
Clearly, the adjective “controversial” should pre-
cede the nouns “press statement”, but since the AFP
and Xinhua portions of the Gigaword corpus, used
to build the language model for the translation sys-
tem, do not contain this surface n-gram, translations
with obviously ungrammatical constructions such as
(1.b) can result. For unseen n-grams, one would like
to model adjectives as being likely to precede nouns
in English, for example.
A straightforward approach to address this is to
exploit the part-of-speech (POS) tags of the tar-
get words during translation (Kirchhoff and Yang,
2005). Though models exploiting POS information
are not expressive enough to model long-distance
dependencies, they can account for locally ungram-
matical constructions such as (1.b). Several attempts
have been made to interpolate POS language models
</bodyText>
<page confidence="0.466733">
869
</page>
<note confidence="0.9933045">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 869–879,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999880666666667">
with surface models. Under constrained data condi-
tions, this can lead to improvements. But once larger
amounts of training data are used, the gains obtained
from adding POS language models decline substan-
tially. This raises the question of why POS language
models are not more effective. We argue that one of
the short-comings of previous approaches to using
POS language models is that these models are es-
timated globally, not lexically anchored, and hence
rather context insensitive.
In this paper, we introduce a novel approach that
builds and uses individual, local POS language mod-
els for each word in the vocabulary. Our experiments
show that it leads to statistically significant improve-
ments over a competitive baseline, using lexical-
ized reordering and a sizable 5-gram word language
model, as well as a standard 7-gram POS language
model approach.
</bodyText>
<sectionHeader confidence="0.888027" genericHeader="method">
2 Part-of-Speech Language Models
</sectionHeader>
<subsectionHeader confidence="0.85038">
2.1 Background
</subsectionHeader>
<bodyText confidence="0.999949142857143">
Typically, POS language models are used like word-
based language models. N-grams are extracted from
a POS-tagged corpus and an n-gram language model
is built from that. While word-based models esti-
mate the probability of a string of m words by Equa-
tion 2, POS-based models estimate the probability of
string of m POS tags by Equation 3.
</bodyText>
<equation confidence="0.99683325">
p(wi|wi−1
i−n+1) (2)
p(ti|ti−1
i−n+1) (3)
</equation>
<bodyText confidence="0.999954774193548">
where, n is the order of the language model, and wji
refers to the sub-sequence of words (or tags) from
positions i to j.
Word language models can be built directly from
large text corpora, such as LDC’s Gigaword corpus,
but POS models require texts that are annotated with
POS tags. Ideally, one would use manually anno-
tated corpora such as the Penn Treebank (Marcus et
al., 1993), but since those tend to be small, most ap-
proaches rely on larger corpora which have been au-
tomatically annotated by a POS tagger or a parser
(Koehn et al., 2008). Though automated annotation
inevitably contains errors, it is assumed that this is
ameliorated by the increased size of annotated data.
The event space of a language models is of size
|V |n, where V is the vocabulary, and n is the order
of the language model. The vocabulary of POS mod-
els, (typically ranging between 40 and 100 tags), is
much smaller than the vocabulary of a word model,
which can easily approach a million words. Nev-
ertheless, most POS language modeling approaches
apply some form of smoothing to account for unseen
events (Bonneau-Maynard et al., 2007).
To deploy POS language models in machine
translation, translation candidates need to be anno-
tated with POS tags. Each target phrase e¯ in a phrase
pair (¯f, ¯e) can be associated with a number of POS
tag sequences ¯t¯e. Heeman (1998) shows that using
the joint probability leads to improved perplexity for
POS models. For machine translation one can sum
over all possible tag sequences, as in Equation 4.
</bodyText>
<equation confidence="0.9989415">
�p(e|f) = arg maxe p(e,t|f) (4)
t
</equation>
<bodyText confidence="0.999876333333333">
Summing over all possible tag sequences has the dis-
advantage that it requires one to keep this informa-
tion during decoding. Below, we opt for an approxi-
mate solution, where each target phrase is annotated
with the most likely POS tag sequence given the
source and target phrase: ¯t¯e = arg max¯t p(¯t|¯e, f).
</bodyText>
<subsectionHeader confidence="0.999692">
2.2 Effectiveness of POS Language Models
</subsectionHeader>
<bodyText confidence="0.999977588235294">
Reported results on the effectiveness of POS lan-
guage models for machine translation are mixed, in
particular when translating into languages that are
not morphologically rich, such as English. While
they rarely seem to hurt translation quality, there
does not seem to be a clear consensus that they sig-
nificantly improve quality either.
Koehn and Hoang (2007) have reported an in-
crease of 0.86 BLEU points for German-to-English
translation for small training data. After relaxing
phrase-matching to include lemma and morpholog-
ical information on the source side, POS language
models lead to a decrease of -0.42 BLEU points. Su-
pertagging encapsulates more contextual informa-
tion than POS tags and Birch et al. (2007) report
improvements when comparing a supertag language
model to a baseline using a word language model
</bodyText>
<equation confidence="0.96729">
p(wm1 ) ∝ �m
i=1
p(tm1 ) ∝ �m
i=1
870
</equation>
<bodyText confidence="0.999966533333334">
only. Once the baseline incorporates lexicalized dis-
tortion (Tillmann, 2004; Koehn et al., 2005), these
improvements disappear. Factored language mod-
els have not resulted in significant improvements ei-
ther. Kirchhoff and Yang (2005) report slight im-
provements when re-ranking the n-best lists of their
decoder, which word tri-grams. But these improve-
ments are less than those gained by re-ranking the
n-best lists with a 4-gram word language model.
The impact of POS language models depends
among other things on the size of the parallel cor-
pus, the size and order of the word language model,
and whether lexicalized distortion models are used.
To gauge the potential effectiveness of POS lan-
guage models without taking into consideration all
these factors, we isolate the contribution of the lan-
guage model by simulating machine translation out-
put using English data only (Al-Onaizan and Pap-
ineni, 2006; Post and Gildea, 2008). Taking a set
of POS-tagged reference translations of the MT04
Arabic-to-English test set, each English sentence is
randomly chunked into n-grams of average length
three. The chunks of each sentence, with their cor-
responding POS tags, are randomly reordered. This
is repeated 500 times for each sentence in the test
set. The smoothed sentence BLEU score (ignor-
ing brevity penalty) is computed for each reordered
sentence with respect to all reference translations.
The higher the BLEU score, the more well-formed
the reordering is. As each reordered sentence only
contains words from at least one of the reference
translations, the uni-gram precision is always 1.0.
The language model probability is then computed
for each reordering. Table 1 shows the average cor-
relations between language model probabilities and
BLEU scores.
We can see that the surface language model corre-
lates moderately well with BLEU, explaining about
49% (r2 = 0.49) of the variation, whereas the POS
language model does not correlate with BLEU at
all.1 On the other hand, local language models alone
(as introduced in Section 3) correlate with BLEU
only slightly worse than surface models. The high-
est correlation is seen when they are interpolated
with word models. The BLEU scores in Table 1
</bodyText>
<footnote confidence="0.466584">
1Interpolating both models does not lead to further correla-
tion improvements.
</footnote>
<table confidence="0.9997115">
LM Kendall’s T Pearson r BLEU[%]
wordLM 0.53 0.71 80.20
POS 7gLM 0.01 0.01 48.44
locLM 0.45 0.62 76.03
AwordLM+(1−A)locLM 0.54 0.73 80.98
(A = 0.92)
</table>
<tableCaption confidence="0.9922575">
Table 1: Correlation between randomly permuted English
reference translations and BLEU.
</tableCaption>
<bodyText confidence="0.999842">
are computed using the 1-best sentences after re-
ranking. These system-agnostic correlation results
look promising for our local models and the end-
to-end translation results in Section 5 confirm these
initial findings.
</bodyText>
<sectionHeader confidence="0.976201" genericHeader="method">
3 Local Language Models
</sectionHeader>
<bodyText confidence="0.9999828125">
In this section, we introduce a novel approach to lan-
guage modeling that is more context-sensitive than
standard POS language models. Instead of using one
global POS language model that is built by using all
of a mono-lingual corpus in the target language, we
build individual models, or local models, for each
word-POS pair using the POS tags surrounding each
occurrence of that pair. This adds an aspect of lex-
icalization that is entirely absent in previous POS
language models. The effect is that the resulting n-
gram probability distributions of each local model
are more biased towards the contextual constraints
of each individual word-POS pair. This is similar to
the idea of cached language models (Kuhn, 1988),
but more fine-grained and with a tighter integration
of POS and lexical information.
</bodyText>
<subsectionHeader confidence="0.999773">
3.1 Definition of Local Language Models
</subsectionHeader>
<bodyText confidence="0.892950777777778">
Each conditional probability of order n in a local
model for the word-POS pair w:t is of the form:
pw:t(tn,pn|t1:p1, ... , tn−1:pn−1)
where ti refers to POS tags and pi to positions rel-
ative to an occurrence of the pair (w : t). For ex-
ample, consider the sentence fragment in Figure 1.
The conditional local n-gram probabilities (a–d) are
generated from the occurrence of the word told with
POS tag VBD. Probability (c) in Figure 1 estimates
that a word with POS tag NN occurs two positions
to the right of told, given the n-gram history that a
noun occurs to its left and a determiner to its right.
871
position ... 11 12 13 14 15 16 17 ...
relative -3 -2 -1 0 +1 +2 +3 . . .
position
word ... the new mayor told the reporter to ...
POS ... DT JJ NN VBD DT NN TO ...
</bodyText>
<figure confidence="0.99214">
(a) ptold:VBD(NN:-1|DT:-3 JJ:-2) (c) ptold:VBD(NN:+2|NN:-1 DT:+1)
(b) ptold:VBD(DT:+1|JJ:-2 NN:-1) (d) ptold:VBD(TO:+3|DT:+1 NN:+2)
</figure>
<figureCaption confidence="0.999928">
Figure 1: Sentence fragment with the tri- gram probabilities (a–d) linked to told.
</figureCaption>
<bodyText confidence="0.998355612903226">
For each local model we use a sliding window con-
sidering all n-grams of length n starting n words to
the left and ending n words to the right of an occur-
rence of the word-POS pair of the model at hand.
All local model probabilities are smoothed us-
ing Witten-Bell smoothing and interpolation.2 POS
tags are annotated with positional information to
distinguish between lower-order estimates such as
ptold:VBD(NN+2) and ptold:VBD(NN+3) both of
which can arise when backing off during smooth-
ing. Without positional information, ptold:VBD(NN)
only estimates the probability of the tag NN occur-
ring within the proximity of told.3
A local model of order n contains the conditional
probabilities for words occurring at relative posi-
tions -1, +1, ...+n. Therefore the probability of
a word occurrence is estimated by all local mod-
els covering this word’s position. Figure 2 shows
schematically how overlapping n-gram probabilities
interact. E.g., the probability of word wi+2 is based
on the probability of the local model for wi+1, wi,
wi−1, and wi−2 (the last two are not shown in Fig-
ure 2 for space reasons). Formally, the conditional
probability of a word-POS pair, given its word and
POS tag history is defined in Equation 5.
2The smaller event space of local models often leads to in-
complete counts-of-counts, preventing the use of Kneser-Ney
smoothing (Chen and Goodman, 1999).
3Despite the notational similarities, our approach should not
be confused with projected POS models, which use source side
POS tags to model reordering (Och et al., 2004).
</bodyText>
<figure confidence="0.56593">
w1 ... wi-3 wi-2 wi-1 wi wi+1wi+2wi+3 ... wm
</figure>
<figureCaption confidence="0.991943">
Figure 2: Schema of overlapping local language model
applications.
</figureCaption>
<bodyText confidence="0.9994476">
where Hi,n is an n x n matrix specifying the history
of the word at position i. Each row j of Hi,n rep-
resents the history of the conditional probability be-
longing to the local model associated with position
i−n+j. Each entry Hi,n[j, k] is defined as follows:
</bodyText>
<equation confidence="0.9415225">
�
Hi,n [j, k]
ti−n+k : k − j if j 7� k =
E otherwise
</equation>
<bodyText confidence="0.999940428571429">
where ti−n+k is the POS tag at position i − n + k
and k − j is the relative position with respect to the
diagonal of Hi,n, i.e., the position of the local lan-
guage model corresponding to row j. Hi,n[j, ·] is the
jth row vector from which the jth entry (the empty
element) has been removed. For instance, given the
example in Figure 1, H14,3 is
</bodyText>
<equation confidence="0.949846">
⎡
E JJ:+1 NN:+2
H14,3 = ⎣DT:-1 E NN:+1
DT:-2 JJ:-1 E
</equation>
<bodyText confidence="0.9294503">
For convenience we assume that the row and col-
umn indices are 0-based, i.e., the upper-left entry of
a matrix is referred to by Hi,n[0, 0]. In this example,
H14,3[1, ·] = (DT:-1, NN:+1).
. . .
. . .
n-gram history
predicted word
position of current
local model
</bodyText>
<equation confidence="0.998602684210526">
i−1 i−1
p(wi, ti  |wi−n+1, ti−n+1) =
pwi:ti(ti−1 :-1  |(ti−n:-n, ... ,ti−2:-2))
n−1�·
j=0
pwi−n+j:ti−n+j(ti :n−j  |Hi,n[j, ·]) (5)
872
p(cuba,
NNP|w00,t0 0) = pcuba:NNP(&lt;s&gt;:-1) · p&lt;s&gt;:&lt;s&gt;(NNP:+1)
p(frees, VBZ|w10,t10) = pfrees:VBZ(NNP:-1|&lt;s&gt;:-2) · p&lt;s&gt;:&lt;s&gt;(VBZ:+2|NNP:+1)
·pcuba:NNP(VBZ:+1|&lt;s&gt;:-1)
p(more, JJR|w20, t20) = pmore:JJR(VBZ:-1|NNP:-3 VBZ:-2) · p&lt;s&gt;:&lt;s&gt;(JJR:+3|NNP:+1 VBZ:+2)
·pcuba:NNP(JJR:+2|&lt;s&gt;:-1 VBZ:+1) · pfrees:VBZ(JJR:+1|&lt;s&gt;:-2 NNP:-1)
p(dissidents, NNS|wi, ti) = pdissidents:NNS(JJR:-1|NNP:-3 VBZ:-2) · pcuba:NNP(NNS:+3|VBZ:+1 JJR:+2)
·pfrees:VBZ(NNS:+2|NNP:-1 JJR:+1) · pmore:JJR(NNS:+1|NNP:-2 VBZ:-1)
p(. , .|w42, t42) = p.:.(NNS:-1|VBZ:-3 JJR:-2) · pfrees:VBZ(.:+3|JJR:+1 NNS:+2)
·pmore:JJR(.:+2|VBZ:-1 NNS:+1) · pdissidents:NNS(. :+1|VBZ:-2 JJR:-1)
p(&lt;/s&gt;, &lt;/s&gt;|w53,t53) = p&lt;/s&gt;:&lt;/s&gt;(.:-1|JJR:-3 NNS:-2) · pmore:JJR(&lt;/s&gt;:+3|NNS:+1 .:+2)
·pdissidents:NNS(&lt;/s&gt;:+2|JJR:-1 .:+1) · p.:.(&lt;/s&gt;:+1|JJR:-2 NNS:-1)
</equation>
<figureCaption confidence="0.9467815">
Figure 3: Language model probability computation for the sentence “Cuba frees more dissidents.” using our local
language modeling approach.
</figureCaption>
<bodyText confidence="0.999981">
The example in Figure 3 shows word-by-word
how tri-gram local language models are used to
compute the probability of a whole sentence.
Our local language model approach also bears
some resemblance to statistical approaches to mod-
eling subcategorization frames (Manning, 1993).
While our approach is more general by considering
all words and not just focusing on verbal subcatego-
rization frames, it is also more shallow in the sense
that only part-of-speech categories are considered
which does not model any contextual relationships
on the phrase level.
</bodyText>
<subsectionHeader confidence="0.999738">
3.2 Building Local Language Models
</subsectionHeader>
<bodyText confidence="0.999815076923077">
To build the local language models, we use the
SRILM toolkit (Stolcke, 2002), which is commonly
applied in speech recognition and statistical machine
translation. While SRILM collects n-gram statistics
from all n-grams occurring in a corpus to build a
single global language model, we build a language
model for each word-POS pair only using the n-
grams within the proximity of occurrences for that
word-POS pair in a POS-tagged corpus. This results
in separate n-gram count files, which are then pro-
cessed by SRILM to build the individual language
models.4 Charniak’s parser (Charniak, 2000) is used
to POS tag the corpus.
</bodyText>
<footnote confidence="0.5186105">
4The pre-processing scripts are available at http://www.
science.uva.nl/˜christof/locLM/.
</footnote>
<subsectionHeader confidence="0.922268">
3.3 Decoder Integration
</subsectionHeader>
<bodyText confidence="0.9944993">
position 0 1 2 3 4 5 6
token &lt;s&gt; cuba frees more dissidents . &lt;/s&gt;
POS tag &lt;s&gt; NNP VBZ JJR NNS . &lt;/s&gt;
Several approaches that integrate POS language
models have focused on n-best list re-ranking only
(Hasan et al., 2006; Wang et al., 2007). Often this
is due to the computational (and implementational)
complexities of integrating more complex language
models with the decoder, although it is expected that
a tighter integration with the decoder itself leads to
better improvements than n-best list re-ranking.
Integrating our local language modeling approach
with a decoder is straightforward. Our baseline
decoder already uses SRILM’s API for computing
word language model probabilities. Since SRILM
supports arbitrarily many language models, local
language models can be added using the same func-
tionalities of SRILM’s API. For the experiments dis-
cussed in Section 4, we add about 150,000 local
language models to the word model. All local lan-
guage model probabilities are coupled with the same
feature weight. Potentially, improvements could be
gained from using separate weights for individual
local models, but this would require an optimiza-
tion procedure such as MIRA (Chiang et al., 2009),
which can handle a larger number of features.
During decoding no POS tagging ambiguities are
resolved. Each target phrase is associated with its
most likely POS tag sequence, given the source and
target side of the phrase pair; see Section 2.1.
</bodyText>
<table confidence="0.650117777777778">
873
4 Experimental Setup of 11.4M source and 12.6M target tokens, and the
Three approaches are compared in our experiments: Chinese-English bitext of 10.6M source and 12.3M
the baseline system is a phrase-based statistical ma- target tokens. Word alignment was performed run-
chine translation system (Koehn et al., 2003), very ning GIZA++ in both directions and generating the
similar to Moses (Koehn et al., 2007), using a word- symmetric alignments using the ‘grow-diag-final-
based 5-gram language model. The second approach and’ heuristics.
extends the baseline by including a 7-gram POS- All three approaches, including the baseline, use
based language model. The third approach repre- lexicalized distortion, distinguishing between mono-
sents the work described in this paper, extending the tone, swap, and discontinuous reordering, all with
baseline by including 4-gram local language models. respect to the previous and next phrase (Koehn et
Translation quality is evaluated for two language al., 2005). The distortion limit is set to 5 for Arabic-
pairs: Arabic-to-English and Chinese-to-English. to-English, and 6 for Chinese-to-English. For each
NIST’s MT-Eval test sets are used for both pairs. source phrase the top 30 translations are considered.
Only resources allowed under NIST’s constrained For tuning and testing we use NIST’s official MT-
data conditions are used to train the language, trans- Eval test sets. MT04 was used as the development
lation, and lexicalized distortion models. set for both language pairs. Testing was carried out
To see whether our local language models result on MT05 to MT09 for Arabic-English and MT05
in improvements over a competitive baseline, we to MT08 for Chinese-English. NIST did not re-
designed the baseline to use a large 5-gram word lease a new Chinese-English test set for MT-Eval
language model and lexicalized distortion model- 2009. Parameter tuning of the decoder was done
ing, both of which are known to cancel-out improve- with minimum error rate training (MERT) (Och,
ments gained from POS language models (Birch et 2003), adapted to BLEU maximization.
al., 2007; Kirchhoff and Yang, 2005). The 5-gram As evaluation metrics we used NIST’s adapta-
word language model is trained on the Xinhua and tion of BLEU-4 (Papineni et al., 2001), version 13a,
AFP sections of the Gigaword corpus (3rd edition, where the brevity penalty is based on the reference
LDC2007T40) and the target side of the bitext. We translation with the closest length, and translation
removed from the training data all documents re- error rate (TER) version 0.7.25 (Snover et al., 2006).
leased during the periods that overlap with the pub- All results reported here are case-insensitive. TER
lication dates of the documents included in our de- scores are shown as 1-TER.
velopment or test data sets. In total, 630 million to- To see whether the differences between the ap-
kens were used to build the word language model. proaches we compared in our experiments are sta-
The language model was trained using SRILM with tistically significant, we apply approximate random-
modified Kneser-Ney smoothing and interpolation ization (Noreen, 1989); Riezler and Maxwell (2005)
(Chen and Goodman, 1999). It is common practice have shown that approximate randomization is less
not to include higher-order n-grams that occur fewer sensitive to Type-I errors, i.e., less likely to falsely
than a predefined number of times. Here, we applied reject the null hypothesis, than bootstrap resampling
rather conservative cut-offs, by ignoring 3-, 4-, and (Koehn, 2004) in the context of machine translation.
5-grams that occurred only once. The 7-gram POS 5 Results and Analysis
and 4-gram local language models were both trained The Arabic-to-English results are shown in Ta-
on the POS tagged English side of the bitext and ble 2, and the Chinese-to-English results in Ta-
10M sentences from Gigaword’s Xinhua and AFP ble 3. All results are subdivided by genre following
sections. NIST’s genre classification. Note that MT06 con-
The data for building the translation models
were primarily drawn from the parallel news re-
sources distributed by the Linguistic Data Consor-
tium (LDC).5 The Arabic-English bitext consists
LDC2004T17, LDC2004T18, LDC2005E46, LDC2005E83,
LDC2006E25, LDC2006E34, LDC2006E85, LDC2006E92,
and LDC2007T08. For Chinese-English: LDC2002E18,
LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06,
LDC2006E34, LDC2006E85, and LDC2006E92.
5LDC catalog numbers for Arabic-English: LDC2004E72,
874
systems and MT04 MT05 MT06 MT08 MT09 MT05–09
improvements tune
NW NW WB ALL NW WB ALL NW WB ALL NW WB ALL
BLEU[%]
1a wordLM 51.90 53.83 46.76 34.69 43.41 48.77 33.26 42.37 52.97 34.25 44.34 50.51 34.00 45.63
2a +posLM 51.92 54.29 47.02 34.44 43.51 48.81 33.30 42.31 53.52 34.04 44.36 50.89 33.87 45.70
3a &gt; wordLM +0.02 +0.46A +0.26 −0.25 +0.10 +0.04 +0.04 −0.06 +0.55A −0.21 +0.02 +0.38A −0.13 +0.07
4a +locLM 52.65 55.08 47.24 35.17 43.88 49.61 33.67 42.92 54.39 34.40 44.82 51.57 34.33 46.22
5a &gt; wordLM +0.75A +1.25A +0.48A +0.48° +0.47A +0.84A +0.41 +0.55A +1.42A +0.15 +0.48A +1.06A +0.33° +0.59A
6a &gt; +posLM +0.73A +0.79A +0.22 +0.73A +0.37° +0.80A +0.37 +0.61A +0.87A +0.36 +0.46A +0.68A +0.46A +0.52A
1-TER[%]
1b wordLM 58.32 59.04 54.27 45.62 51.68 55.59 44.41 50.69 59.90 46.43 53.03 56.94 45.49 53.13
2b +posLM 58.54 59.72 54.90 45.67 52.14 55.75 44.64 50.89 60.49 46.72 53.47 57.46 45.70 53.55
3b &gt; wordLM +0.22° +0.68A +0.63A +0.05 +0.46A +0.16 +0.23 +0.20° +0.59A +0.29° +0.44A +0.52A +0.21A +0.42A
4b +locLM 58.95 60.06 54.88 45.62 52.11 56.42 44.91 51.38 60.91 46.84 53.74 57.79 45.83 53.81
5b &gt; wordLM +0.63A +1.02A +0.61A +0.00 +0.43A +0.83A +0.50A +0.69A +1.01A +0.41° +0.71A +0.85A +0.34A +0.68A
6b &gt; +posLM +0.41A +0.34° −0.02 −0.05 −0.03 +0.67A +0.27 +0.49A +0.42° +0.12 +0.27° +0.33A +0.13 +0.26A
# segments 1,353 1,056 1,033 764 1,797 813 547 1,360 586 727 1,313 3,488 2,038 5,526
</table>
<tableCaption confidence="0.977696">
Table 2: Results for Arabic-to-English translation. Comparison of our approach (+locLM, rows 4a/b) to the baseline
</tableCaption>
<bodyText confidence="0.98354912962963">
using a word language model (wordLM, rows 1a/b) and a competing approach using a POS-based language model
(+posLM, rows 2a/b). Results are presented using BLEU[%] (rows 1a–6a) and 1-TER[%] (rows 1b–6b) and broken
down by genre: NW=newswire, WB=web, and ALL=NW∪WB. Rows 3a/b, 5a/b, and 6a/b show the relative improve-
ments over the system mentioned to the right of the &gt; sign. Statistically significant improvements/declines (using
approximate randomization) at the p &lt; .01 level are marked A/ • and °/ ° at the p &lt; .05 level.
tains the genres ‘broadcast news’ and ‘newsgroup’.
In both tables, the former has been classified under
‘newswire’ and the latter under ‘web’.
The first approach is the baseline system
‘wordLM’ (rows 1a/b in Tables 2 and 3), which uses
a 5-gram word-based language model. The next ap-
proach ‘+posLM’ extends the baseline by adding a
7-gram POS language model (rows 2a/b in both ta-
bles). Rows 3a/b show the relative improvements
over the baseline. The third approach ‘+locLM’
(rows 4a/b) uses local language models in addition
to the baseline’s word-based model. Note that +lo-
cLM does not use the 7-gram POS language model
as well. Rows 5a/b show the relative improvements
of the local modeling approach over the baseline and
rows 6a/b the improvements over the approach using
a POS language model.
Let us first take a closer look at the Arabic-to-
English results in Table 2. The approach using a
POS language model results in statistically signifi-
cant improvements for only one test set (MT05) and
the newswire documents of MT09. The average im-
provements across all sets and genres are negligible
(+0.07 BLEU). Our local language modeling ap-
proach achieves the highest BLEU scores for all test
sets and across all genres. In particular, the improve-
ments of +1.06 BLEU for newswire documents are
substantial. With the exception of MT08-WB and
MT09-WB all BLEU improvements over the base-
line are statistically significant.
When evaluating with 1-TER, local language
modeling also achieves the best results, with the ex-
ception of MT06, where the POS language model
approach performs slightly better.
Turning to the Chinese-English results in Table 3,
we see similar improvements in BLEU. The im-
provements of using a POS language model are neg-
ligible (+0.04 BLEU). Here as well, local language
modeling leads to the best results, with substantial
improvements of +0.88 BLEU for web documents.
The major difference between Arabic-English and
Chinese-English is the discrepancy between BLEU
score improvements and decreases in 1-TER. While
we cannot explain this discrepancy, it is worth not-
ing that similar discrepancies between BLEU and
TER and Arabic-to-English and Chinese-to-English
translation can be found in the literature. The results
described in Shen et al. (2009) show a strong cor-
relation between BLEU and 1-TER improvements6
</bodyText>
<note confidence="0.715395">
6Shen et al. (2009) report TER rather than 1-TER scores.
</note>
<page confidence="0.519082">
875
</page>
<table confidence="0.999681777777778">
systems and MT04 MT05 MT06 MT08 MT05–08
improvements tune
NW NW WB ALL NW WB ALL NW WB ALL
BLEU[%]
1a wordLM 37.32 32.55 33.33 23.40 31.16 28.67 17.57 24.03 31.93 19.82 29.30
2a +posLM 37.32 32.47 33.13 23.67 31.06 28.63 18.46 24.35 31.82 20.46 29.34
3a &gt; wordLM +0.00 −0.08 −0.20 +0.27 −0.10 −0.04 +0.89N +0.32 −0.11 +0.64N +0.04
4a +locLM 38.15 33.05 33.33 24.62 31.42 29.52 18.24 24.79 32.36 20.70 29.82
5a &gt; wordLM +0.83N +0.50M +0.00 +1.22N +0.26 +0.85N +0.67M +0.76N +0.43N +0.88N +0.52N
6a &gt; +posLM +0.83N +0.58N +0.20 +0.95N +0.36M +0.89N −0.22 +0.44M +0.54N +0.24 +0.48N
1-TER[%]
1b wordLM 42.81 40.73 42.99 39.42 42.15 40.42 36.77 38.78 41.53 37.77 40.63
2b +posLM 42.50 40.60 42.75 38.87 41.84 39.76 36.75 38.41 41.23 37.55 40.34
3b &gt; wordLM −0.31O −0.13 −0.24 −0.55 −0.31O −0.66H −0.02 −0.37O −0.30H −0.22 −0.29H
4b +locLM 42.77 40.49 42.62 39.40 41.86 40.00 36.11 38.26 41.20 37.35 40.27
5b &gt; wordLM −0.04 −0.24 −0.37 −0.02 −0.29 −0.42 −0.66H −0.52H −0.33O −0.42O −0.36H
6b &gt; posLM +0.27 −0.11 −0.13 +0.53 +0.02 +0.24 −0.64H −0.15 −0.03 −0.20 −0.07
# segments 1,788 1,082 1,181 483 1,664 691 666 1,357 2,954 1,149 4,103
</table>
<tableCaption confidence="0.999976">
Table 3: Comparison of our system for Chinese-to-English translation. See Table 2 for details on notation.
</tableCaption>
<bodyText confidence="0.999352769230769">
for Arabic-to-English on the MT06 and MT08 sets,
but for Chinese-to-English the correlation seems to
be much weaker and BLEU improvements of +0.75
can correspond to decreases of up to -0.80 in 1-TER.
One of the motivations of using POS language
models in general, and local language models in our
case, is to improve the fluency of translations, which
should be reflected in increased precision for higher-
order n-grams. Table 4 shows that this is the case
when comparing local modeling to both word and
POS language models for Arabic-to-English trans-
lation. The same trend, but to a somewhat weaker
degree can be observed for Chinese-to-English.
</bodyText>
<table confidence="0.999820444444445">
Prec-1 Prec-2 Prec-3 Prec-4 BP
Arabic-English (MT05–09)
wordLM 81.38 54.51 38.10 26.99 0.987
+posLM 81.81 54.82 38.34 27.17 0.983
+locLM 81.90 55.35 39.01 27.86 0.981
Chinese-English (MT05–08)
wordLM 75.03 40.56 22.55 12.93 0.955
+posLM 74.81 40.30 22.41 12.83 0.962
+locLM 74.24 40.70 22.83 13.19 0.966
</table>
<tableCaption confidence="0.981767">
Table 4: BLEU n-gram precision (1&lt;n&lt;4) and Brevity
Penalty (BP) scores over all test sets.
</tableCaption>
<bodyText confidence="0.999970171428571">
The effectiveness of a POS language model of-
ten diminishes with improved translation quality of
the base system to which it is added. Naturally,
we are interested in the extent that this diminish-
ing effect also holds for our local language mod-
els. A full experimental setup, varying all relevant
factors, such as language, translation, and distor-
tion model size, and the various meta-parameters,
is beyond the scope of this paper. Nevertheless,
we can gauge this by taking a closer look at the
distribution of improvements within our experi-
ments. Figure 4 shows performance improvements
in document-level BLEU for both language pairs.
The document-level BLEU score for the baseline
system is plotted on the x-axis and improvements are
plotted on the y-axis. The dotted line is the linear
fit (using least square regression). If the effective-
ness of either added model (POS or local) dimin-
ishes with increasing translation quality, we would
expect a declining regression line. This is not the
case for Arabic-to-English translation. Relative im-
provements for both added models increase as the
translation quality of the baseline increases. The
slope of both regression fits is almost identical, but
the y-intercept is larger for our local modeling ap-
proach. Note that the small slope is also due to dif-
ference in scale between full BLEU scores and rel-
ative improvements. We can observe the opposite
for Chinese-to-English translation, where the slope
is negative. Both models seem to help more for
documents with lower baseline translation quality.
For the POS model, the regression line intersects
with the neutral line (±0 improvement) at around
31 BLEU, which is close to the average BLEU score
and in line with its negligible improvements (see Ta-
</bodyText>
<figure confidence="0.991198523076923">
876
10
8
6
4
2
0
-2
-4
document BLEU improvements
-6
-8
10
8
6
4
2
0
-2
-4
document BLEU improvements
-6
-8
10 20 30 40 50 60 70
baseline document BLEU score
10 20 30 40 50 60 70
baseline document BLEU score
document
least square fit
neutral
document
least square fit
neutral
Arabic-English: +posLM &gt; wordLM Arabic-English: +locLM &gt; wordLM
document BLEU improvements
10
-2
-4
-6
-8
8
6
4
2
0
document
least square fit
neutral
document BLEU improvements
10
-2
-4
-6
-8
8
6
4
2
0
document
least square fit
neutral
10 20 30 40 50 10 20 30 40 50
baseline document BLEU score baseline document BLEU score
Chinese-English: +posLM &gt; wordLM Chinese-English: +locLM &gt; wordLM
</figure>
<figureCaption confidence="0.9984065">
Figure 4: Correlation between baseline BLEU scores for individual documents and the relative, absolute improvements
achieved by +posLM (left) and +locLM (right). BLEU scores (and improvements) are computed at the document level.
</figureCaption>
<bodyText confidence="0.9999872">
ble 3). For the local language model, the regres-
sion line intersects with the neutral line at about
40 BLEU, suggesting that until translation quality
improves substantially, local language models could
still have a positive impact.
</bodyText>
<sectionHeader confidence="0.999699" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999977178571428">
The main goal of this paper is to show that by tying
POS language models to lexical items, we get more
accurate distributions for specific words. The work
on factored language models (Bilmes and Kirchhoff,
2003) is related to our work to the extent that it also
mixes POS tags with lexical information, albeit in
a very different manner. Factored language models
use more general representations, such as POS tags
or stems, only during back-off. Kirchhoff and Yang
(2005) applied factored language models to machine
translation but the improvements were negligible.
Collins et al. (2005) proposed a discriminative
language modeling approach that uses mixtures of
POS and surface information and showed that it
leads to a reduction in speech recognition word er-
ror rates. On the other hand, their approach seems
more suited for n-best list re-ranking and it is not
clear whether those improvements carry over to ma-
chine translation. Li and Khudanpur (2008) adapted
this discriminative approach to machine translation
re-ranking but used surface forms only.
Wang et al. (2007) and Zheng et al. (2008)
use elaborately enriched representations, called su-
per abstract role values (Wang and Harper, 2002),
which capture contextual dependencies using lexi-
cal categories, role labels, and dependency grammar
structures. So far their approach has been limited to
re-ranking n-best lists only.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999979428571429">
Though POS language models do not lead to signif-
icant improvements over a competitive baseline, we
have shown that a competitive phrase-based baseline
system can benefit from using POS information by
building lexically anchored local models. Our local
model approach does not only lead to more context-
specific probability distributions, but also takes ad-
</bodyText>
<page confidence="0.624356">
877
</page>
<bodyText confidence="0.9998881">
vantage of the language model probability of each
word being based on all surrounding local models.
The evaluations for Arabic- and Chinese-to-English
show that local models lead to statistically signifi-
cant improvements across different test sets and gen-
res. Correlating the translation quality of the base-
line with the improvements that result from adding
local models, further suggests that these improve-
ments are sustainable and should carry over to im-
proved baseline systems.
</bodyText>
<sectionHeader confidence="0.995494" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998672666666667">
This research was funded in part by the European
Commission through the CoSyne project FP7-ICT-
4-248531, the European Commission’s ICT Pol-
icy Support Program as part of the Competitive-
ness and Innovation Framework Program, CIP ICT-
PSP under grant agreement nr. 250430, and the
PROMISE Network of Excellence co-funded by the
7th Framework Programme of the European Com-
mission, grant agreement no. 258191.
</bodyText>
<sectionHeader confidence="0.988171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997426762500001">
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
529–536.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proceedings of the the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology, pages 4–6.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9–16.
H´el`ene Bonneau-Maynard, Alexandre Allauzen, Daniel
D´echelotte, and Holger Schwenk. 2007. Combining
morphosyntactic enriched representation with n-best
reranking in statistical translation. In Proceedings of
the NAACL-HLT Workshop on Syntax and Structure in
Statistical Translation, pages 65–71.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858–867.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132–139.
Stanley F. Chen and Joshua Goodman. 1999. An empiri-
cal study of smoothing techniques for language model-
ing. Computer Speech and Language, 13(4):359–393.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of the North American Chapter of
the Association for Computational Linguistics, pages
218–226.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 507–514.
Saˇsa Hasan, Oliver Bender, and Hermann Ney. 2006.
Reranking translation hypotheses using structural
properties. In Proceedings of the EACL Workshop on
Learning Structured Information in Natural Language
Applications, pages 41–48.
Peter Heeman. 1998. POS tagging versus classes in lan-
guage modeling. In Proceedings of the Sixth Work-
shop on Very Large Corpora, pages 179–187.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation. In
Proceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, pages 125–128.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868–876.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48–
54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
</reference>
<page confidence="0.587135">
878
</page>
<reference confidence="0.999733010526316">
active Poster and Demonstration Sessions, pages 177–
180.
Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008.
Towards better machine translation quality for the
german–english language pairs. In Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 139–142.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 388–395.
Roland Kuhn. 1988. Speech recognition and the fre-
quency of recently used words: a modified Markov
model for natural language. In Proceedings of the 12th
conference on Computational Linguistics, pages 348–
350.
Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statisti- cal
machine translation. In Proceedings of AMTA, pages
133–142.
Christopher D. Manning. 1993. Automatic acquisition of
a large subcategorization dictionary from corpora. In
Proceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 235–242.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19:313–330.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley-
Interscience.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, ALex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the 2004 Meeting of the
North American chapter of the Association for Com-
putational Linguistics, pages 161–168.
Franz-Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics (ACL 2001), pages 311–318.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 172–181.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57–
64.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492–
518.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of Empirical Methods in
Natural Language Processing, pages 72–80.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation forMachine Translation
in the Americas, pages 223–231.
Andreas Stolcke. 2002. SRILM—an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901–904.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
the Human Language Technology and North American
Association for Computational Linguistics Conference
(HLT/NAACL-04), pages 101–104.
Wen Wang and Mary P. Harper. 2002. The Super-
ARV language model: investigating the effectiveness
of tightly integrating multiple knowledge sources. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing, pages 238–247.
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with struc-
tured and web-based language models. In IEEE Work-
shop on Automatic Speech Recognition &amp; Understand-
ing, pages 159–164.
Jing Zheng, Necip Fazil Ayan, Wen Wang, Dimitra Ver-
gyri, Nicolas Scheffer, and Andreas Stolcke. 2008.
SRI systems in the NIST MT08 Evaluation. In Pro-
ceedings of the NIST 2008 Open MT Evaluation Work-
shop.
</reference>
<page confidence="0.948279">
879
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416446">
<title confidence="0.993961">Statistical Machine Translation with Local Language Models</title>
<author confidence="0.451713">Christof</author>
<affiliation confidence="0.623948">Informatics Institute, University of</affiliation>
<address confidence="0.657271">P.O. Box 94323, 1090 GH Amsterdam, The</address>
<email confidence="0.978557">c.monz@uva.nl</email>
<abstract confidence="0.999040476190476">Part-of-speech language modeling is commonly used as a component in statistical machine translation systems, but there is mixed evidence that its usage leads to significant improvements. We argue that its limited effectiveness is due to the lack of lexicalization. We introduce a new approach that builds a separate local language model for each word and part-of-speech pair. The resulting models lead to more context-sensitive probability distributions and we also exploit the fact that different local models are used to estimate the language model probability of each word during decoding. Our approach is evaluated for Arabicand Chinese-to-English translation. We show that it leads to statistically significant improvements for multiple test sets and also across different genres, when compared against a competitive baseline and a system using a part-of-speech model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>529--536</pages>
<contexts>
<context position="8409" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="1342" endWordPosition="1346">g the n-best lists of their decoder, which word tri-grams. But these improvements are less than those gained by re-ranking the n-best lists with a 4-gram word language model. The impact of POS language models depends among other things on the size of the parallel corpus, the size and order of the word language model, and whether lexicalized distortion models are used. To gauge the potential effectiveness of POS language models without taking into consideration all these factors, we isolate the contribution of the language model by simulating machine translation output using English data only (Al-Onaizan and Papineni, 2006; Post and Gildea, 2008). Taking a set of POS-tagged reference translations of the MT04 Arabic-to-English test set, each English sentence is randomly chunked into n-grams of average length three. The chunks of each sentence, with their corresponding POS tags, are randomly reordered. This is repeated 500 times for each sentence in the test set. The smoothed sentence BLEU score (ignoring brevity penalty) is computed for each reordered sentence with respect to all reference translations. The higher the BLEU score, the more well-formed the reordering is. As each reordered sentence only contains wo</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 529–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>4--6</pages>
<contexts>
<context position="32885" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="5270" endWordPosition="5273">dual documents and the relative, absolute improvements achieved by +posLM (left) and +locLM (right). BLEU scores (and improvements) are computed at the document level. ble 3). For the local language model, the regression line intersects with the neutral line at about 40 BLEU, suggesting that until translation quality improves substantially, local language models could still have a positive impact. 6 Related Work The main goal of this paper is to show that by tying POS language models to lexical items, we get more accurate distributions for specific words. The work on factored language models (Bilmes and Kirchhoff, 2003) is related to our work to the extent that it also mixes POS tags with lexical information, albeit in a very different manner. Factored language models use more general representations, such as POS tags or stems, only during back-off. Kirchhoff and Yang (2005) applied factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word error rates. On the other hand, their approach seems </context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proceedings of the the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 4–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="7364" citStr="Birch et al. (2007)" startWordPosition="1173" endWordPosition="1176">n particular when translating into languages that are not morphologically rich, such as English. While they rarely seem to hurt translation quality, there does not seem to be a clear consensus that they significantly improve quality either. Koehn and Hoang (2007) have reported an increase of 0.86 BLEU points for German-to-English translation for small training data. After relaxing phrase-matching to include lemma and morphological information on the source side, POS language models lead to a decrease of -0.42 BLEU points. Supertagging encapsulates more contextual information than POS tags and Birch et al. (2007) report improvements when comparing a supertag language model to a baseline using a word language model p(wm1 ) ∝ �m i=1 p(tm1 ) ∝ �m i=1 870 only. Once the baseline incorporates lexicalized distortion (Tillmann, 2004; Koehn et al., 2005), these improvements disappear. Factored language models have not resulted in significant improvements either. Kirchhoff and Yang (2005) report slight improvements when re-ranking the n-best lists of their decoder, which word tri-grams. But these improvements are less than those gained by re-ranking the n-best lists with a 4-gram word language model. The impac</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. CCG supertags in factored statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H´el`ene Bonneau-Maynard</author>
<author>Alexandre Allauzen</author>
<author>Daniel D´echelotte</author>
<author>Holger Schwenk</author>
</authors>
<title>Combining morphosyntactic enriched representation with n-best reranking in statistical translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>65--71</pages>
<marker>Bonneau-Maynard, Allauzen, D´echelotte, Schwenk, 2007</marker>
<rawString>H´el`ene Bonneau-Maynard, Alexandre Allauzen, Daniel D´echelotte, and Holger Schwenk. 2007. Combining morphosyntactic enriched representation with n-best reranking in statistical translation. In Proceedings of the NAACL-HLT Workshop on Syntax and Structure in Statistical Translation, pages 65–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>858--867</pages>
<contexts>
<context position="1516" citStr="Brants et al. (2007)" startWordPosition="227" endWordPosition="230">lly significant improvements for multiple test sets and also across different genres, when compared against a competitive baseline and a system using a part-of-speech model. 1 Introduction Language models are an important component of current statistical machine translation systems. They affect the selection of phrase translation candidates and reordering choices by estimating the probability that an application of a phrase translation is a fluent continuation of the current translation hypothesis. The size and domain of the language model can have a significant impact on translation quality. Brants et al. (2007) have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points. On the other hand, each doubling using general web data leads to improvements of approximately 0.15 BLEU points. While large n-gram language models do lead to improved translation quality, they still lack any generalization beyond the surface forms (Schwenk, 2007). Consider example (1), which is a short sentence fragment from the MT09 Arabic-English test set, with the corresponding machine translation output (1.b), from a phrase-ba</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="16964" citStr="Charniak, 2000" startWordPosition="2711" endWordPosition="2712">g Local Language Models To build the local language models, we use the SRILM toolkit (Stolcke, 2002), which is commonly applied in speech recognition and statistical machine translation. While SRILM collects n-gram statistics from all n-grams occurring in a corpus to build a single global language model, we build a language model for each word-POS pair only using the ngrams within the proximity of occurrences for that word-POS pair in a POS-tagged corpus. This results in separate n-gram count files, which are then processed by SRILM to build the individual language models.4 Charniak’s parser (Charniak, 2000) is used to POS tag the corpus. 4The pre-processing scripts are available at http://www. science.uva.nl/˜christof/locLM/. 3.3 Decoder Integration position 0 1 2 3 4 5 6 token &lt;s&gt; cuba frees more dissidents . &lt;/s&gt; POS tag &lt;s&gt; NNP VBZ JJR NNS . &lt;/s&gt; Several approaches that integrate POS language models have focused on n-best list re-ranking only (Hasan et al., 2006; Wang et al., 2007). Often this is due to the computational (and implementational) complexities of integrating more complex language models with the decoder, although it is expected that a tighter integration with the decoder itself l</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="13462" citStr="Chen and Goodman, 1999" startWordPosition="2176" endWordPosition="2179">ore the probability of a word occurrence is estimated by all local models covering this word’s position. Figure 2 shows schematically how overlapping n-gram probabilities interact. E.g., the probability of word wi+2 is based on the probability of the local model for wi+1, wi, wi−1, and wi−2 (the last two are not shown in Figure 2 for space reasons). Formally, the conditional probability of a word-POS pair, given its word and POS tag history is defined in Equation 5. 2The smaller event space of local models often leads to incomplete counts-of-counts, preventing the use of Kneser-Ney smoothing (Chen and Goodman, 1999). 3Despite the notational similarities, our approach should not be confused with projected POS models, which use source side POS tags to model reordering (Och et al., 2004). w1 ... wi-3 wi-2 wi-1 wi wi+1wi+2wi+3 ... wm Figure 2: Schema of overlapping local language model applications. where Hi,n is an n x n matrix specifying the history of the word at position i. Each row j of Hi,n represents the history of the conditional probability belonging to the local model associated with position i−n+j. Each entry Hi,n[j, k] is defined as follows: � Hi,n [j, k] ti−n+k : k − j if j 7� k = E otherwise wh</context>
<context position="21770" citStr="Chen and Goodman, 1999" startWordPosition="3462" endWordPosition="3465">nover et al., 2006). leased during the periods that overlap with the pub- All results reported here are case-insensitive. TER lication dates of the documents included in our de- scores are shown as 1-TER. velopment or test data sets. In total, 630 million to- To see whether the differences between the apkens were used to build the word language model. proaches we compared in our experiments are staThe language model was trained using SRILM with tistically significant, we apply approximate randommodified Kneser-Ney smoothing and interpolation ization (Noreen, 1989); Riezler and Maxwell (2005) (Chen and Goodman, 1999). It is common practice have shown that approximate randomization is less not to include higher-order n-grams that occur fewer sensitive to Type-I errors, i.e., less likely to falsely than a predefined number of times. Here, we applied reject the null hypothesis, than bootstrap resampling rather conservative cut-offs, by ignoring 3-, 4-, and (Koehn, 2004) in the context of machine translation. 5-grams that occurred only once. The 7-gram POS 5 Results and Analysis and 4-gram local language models were both trained The Arabic-to-English results are shown in Taon the POS tagged English side of th</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13(4):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="18306" citStr="Chiang et al., 2009" startWordPosition="2918" endWordPosition="2921">s straightforward. Our baseline decoder already uses SRILM’s API for computing word language model probabilities. Since SRILM supports arbitrarily many language models, local language models can be added using the same functionalities of SRILM’s API. For the experiments discussed in Section 4, we add about 150,000 local language models to the word model. All local language model probabilities are coupled with the same feature weight. Potentially, improvements could be gained from using separate weights for individual local models, but this would require an optimization procedure such as MIRA (Chiang et al., 2009), which can handle a larger number of features. During decoding no POS tagging ambiguities are resolved. Each target phrase is associated with its most likely POS tag sequence, given the source and target side of the phrase pair; see Section 2.1. 873 4 Experimental Setup of 11.4M source and 12.6M target tokens, and the Three approaches are compared in our experiments: Chinese-English bitext of 10.6M source and 12.3M the baseline system is a phrase-based statistical ma- target tokens. Word alignment was performed runchine translation system (Koehn et al., 2003), very ning GIZA++ in both directi</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
</authors>
<title>Discriminative syntactic language modeling for speech recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>507--514</pages>
<contexts>
<context position="33261" citStr="Collins et al. (2005)" startWordPosition="5329" endWordPosition="5332">a positive impact. 6 Related Work The main goal of this paper is to show that by tying POS language models to lexical items, we get more accurate distributions for specific words. The work on factored language models (Bilmes and Kirchhoff, 2003) is related to our work to the extent that it also mixes POS tags with lexical information, albeit in a very different manner. Factored language models use more general representations, such as POS tags or stems, only during back-off. Kirchhoff and Yang (2005) applied factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word error rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (2007) and Zheng et al. (2008) use elaborately enriched representations, called super abstract role values (Wang and Ha</context>
</contexts>
<marker>Collins, Roark, Saraclar, 2005</marker>
<rawString>Michael Collins, Brian Roark, and Murat Saraclar. 2005. Discriminative syntactic language modeling for speech recognition. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 507–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Hasan</author>
<author>Oliver Bender</author>
<author>Hermann Ney</author>
</authors>
<title>Reranking translation hypotheses using structural properties.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL Workshop on Learning Structured Information in Natural Language Applications,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="17329" citStr="Hasan et al., 2006" startWordPosition="2770" endWordPosition="2773">ing the ngrams within the proximity of occurrences for that word-POS pair in a POS-tagged corpus. This results in separate n-gram count files, which are then processed by SRILM to build the individual language models.4 Charniak’s parser (Charniak, 2000) is used to POS tag the corpus. 4The pre-processing scripts are available at http://www. science.uva.nl/˜christof/locLM/. 3.3 Decoder Integration position 0 1 2 3 4 5 6 token &lt;s&gt; cuba frees more dissidents . &lt;/s&gt; POS tag &lt;s&gt; NNP VBZ JJR NNS . &lt;/s&gt; Several approaches that integrate POS language models have focused on n-best list re-ranking only (Hasan et al., 2006; Wang et al., 2007). Often this is due to the computational (and implementational) complexities of integrating more complex language models with the decoder, although it is expected that a tighter integration with the decoder itself leads to better improvements than n-best list re-ranking. Integrating our local language modeling approach with a decoder is straightforward. Our baseline decoder already uses SRILM’s API for computing word language model probabilities. Since SRILM supports arbitrarily many language models, local language models can be added using the same functionalities of SRILM</context>
</contexts>
<marker>Hasan, Bender, Ney, 2006</marker>
<rawString>Saˇsa Hasan, Oliver Bender, and Hermann Ney. 2006. Reranking translation hypotheses using structural properties. In Proceedings of the EACL Workshop on Learning Structured Information in Natural Language Applications, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
</authors>
<title>POS tagging versus classes in language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>179--187</pages>
<contexts>
<context position="6092" citStr="Heeman (1998)" startWordPosition="967" endWordPosition="968">ere V is the vocabulary, and n is the order of the language model. The vocabulary of POS models, (typically ranging between 40 and 100 tags), is much smaller than the vocabulary of a word model, which can easily approach a million words. Nevertheless, most POS language modeling approaches apply some form of smoothing to account for unseen events (Bonneau-Maynard et al., 2007). To deploy POS language models in machine translation, translation candidates need to be annotated with POS tags. Each target phrase e¯ in a phrase pair (¯f, ¯e) can be associated with a number of POS tag sequences ¯t¯e. Heeman (1998) shows that using the joint probability leads to improved perplexity for POS models. For machine translation one can sum over all possible tag sequences, as in Equation 4. �p(e|f) = arg maxe p(e,t|f) (4) t Summing over all possible tag sequences has the disadvantage that it requires one to keep this information during decoding. Below, we opt for an approximate solution, where each target phrase is annotated with the most likely POS tag sequence given the source and target phrase: ¯t¯e = arg max¯t p(¯t|¯e, f). 2.2 Effectiveness of POS Language Models Reported results on the effectiveness of POS</context>
</contexts>
<marker>Heeman, 1998</marker>
<rawString>Peter Heeman. 1998. POS tagging versus classes in language modeling. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 179–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Mei Yang</author>
</authors>
<title>Improved language modeling for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>125--128</pages>
<contexts>
<context position="3012" citStr="Kirchhoff and Yang, 2005" startWordPosition="461" endWordPosition="464">ress statements and accused him ... Clearly, the adjective “controversial” should precede the nouns “press statement”, but since the AFP and Xinhua portions of the Gigaword corpus, used to build the language model for the translation system, do not contain this surface n-gram, translations with obviously ungrammatical constructions such as (1.b) can result. For unseen n-grams, one would like to model adjectives as being likely to precede nouns in English, for example. A straightforward approach to address this is to exploit the part-of-speech (POS) tags of the target words during translation (Kirchhoff and Yang, 2005). Though models exploiting POS information are not expressive enough to model long-distance dependencies, they can account for locally ungrammatical constructions such as (1.b). Several attempts have been made to interpolate POS language models 869 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 869–879, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics with surface models. Under constrained data conditions, this can lead to improvements. But once larger amounts of training data are used, the gains obtained</context>
<context position="7738" citStr="Kirchhoff and Yang (2005)" startWordPosition="1233" endWordPosition="1236"> After relaxing phrase-matching to include lemma and morphological information on the source side, POS language models lead to a decrease of -0.42 BLEU points. Supertagging encapsulates more contextual information than POS tags and Birch et al. (2007) report improvements when comparing a supertag language model to a baseline using a word language model p(wm1 ) ∝ �m i=1 p(tm1 ) ∝ �m i=1 870 only. Once the baseline incorporates lexicalized distortion (Tillmann, 2004; Koehn et al., 2005), these improvements disappear. Factored language models have not resulted in significant improvements either. Kirchhoff and Yang (2005) report slight improvements when re-ranking the n-best lists of their decoder, which word tri-grams. But these improvements are less than those gained by re-ranking the n-best lists with a 4-gram word language model. The impact of POS language models depends among other things on the size of the parallel corpus, the size and order of the word language model, and whether lexicalized distortion models are used. To gauge the potential effectiveness of POS language models without taking into consideration all these factors, we isolate the contribution of the language model by simulating machine tr</context>
<context position="20701" citStr="Kirchhoff and Yang, 2005" startWordPosition="3290" endWordPosition="3293">rs. Testing was carried out To see whether our local language models result on MT05 to MT09 for Arabic-English and MT05 in improvements over a competitive baseline, we to MT08 for Chinese-English. NIST did not redesigned the baseline to use a large 5-gram word lease a new Chinese-English test set for MT-Eval language model and lexicalized distortion model- 2009. Parameter tuning of the decoder was done ing, both of which are known to cancel-out improve- with minimum error rate training (MERT) (Och, ments gained from POS language models (Birch et 2003), adapted to BLEU maximization. al., 2007; Kirchhoff and Yang, 2005). The 5-gram As evaluation metrics we used NIST’s adaptaword language model is trained on the Xinhua and tion of BLEU-4 (Papineni et al., 2001), version 13a, AFP sections of the Gigaword corpus (3rd edition, where the brevity penalty is based on the reference LDC2007T40) and the target side of the bitext. We translation with the closest length, and translation removed from the training data all documents re- error rate (TER) version 0.7.25 (Snover et al., 2006). leased during the periods that overlap with the pub- All results reported here are case-insensitive. TER lication dates of the docume</context>
<context position="33145" citStr="Kirchhoff and Yang (2005)" startWordPosition="5313" endWordPosition="5316">about 40 BLEU, suggesting that until translation quality improves substantially, local language models could still have a positive impact. 6 Related Work The main goal of this paper is to show that by tying POS language models to lexical items, we get more accurate distributions for specific words. The work on factored language models (Bilmes and Kirchhoff, 2003) is related to our work to the extent that it also mixes POS tags with lexical information, albeit in a very different manner. Factored language models use more general representations, such as POS tags or stems, only during back-off. Kirchhoff and Yang (2005) applied factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word error rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (20</context>
</contexts>
<marker>Kirchhoff, Yang, 2005</marker>
<rawString>Katrin Kirchhoff and Mei Yang. 2005. Improved language modeling for statistical machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 125–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<contexts>
<context position="7008" citStr="Koehn and Hoang (2007)" startWordPosition="1117" endWordPosition="1120">p this information during decoding. Below, we opt for an approximate solution, where each target phrase is annotated with the most likely POS tag sequence given the source and target phrase: ¯t¯e = arg max¯t p(¯t|¯e, f). 2.2 Effectiveness of POS Language Models Reported results on the effectiveness of POS language models for machine translation are mixed, in particular when translating into languages that are not morphologically rich, such as English. While they rarely seem to hurt translation quality, there does not seem to be a clear consensus that they significantly improve quality either. Koehn and Hoang (2007) have reported an increase of 0.86 BLEU points for German-to-English translation for small training data. After relaxing phrase-matching to include lemma and morphological information on the source side, POS language models lead to a decrease of -0.42 BLEU points. Supertagging encapsulates more contextual information than POS tags and Birch et al. (2007) report improvements when comparing a supertag language model to a baseline using a word language model p(wm1 ) ∝ �m i=1 p(tm1 ) ∝ �m i=1 870 only. Once the baseline incorporates lexicalized distortion (Tillmann, 2004; Koehn et al., 2005), thes</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="18872" citStr="Koehn et al., 2003" startWordPosition="3008" endWordPosition="3011">mization procedure such as MIRA (Chiang et al., 2009), which can handle a larger number of features. During decoding no POS tagging ambiguities are resolved. Each target phrase is associated with its most likely POS tag sequence, given the source and target side of the phrase pair; see Section 2.1. 873 4 Experimental Setup of 11.4M source and 12.6M target tokens, and the Three approaches are compared in our experiments: Chinese-English bitext of 10.6M source and 12.3M the baseline system is a phrase-based statistical ma- target tokens. Word alignment was performed runchine translation system (Koehn et al., 2003), very ning GIZA++ in both directions and generating the similar to Moses (Koehn et al., 2007), using a word- symmetric alignments using the ‘grow-diag-finalbased 5-gram language model. The second approach and’ heuristics. extends the baseline by including a 7-gram POS- All three approaches, including the baseline, use based language model. The third approach repre- lexicalized distortion, distinguishing between monosents the work described in this paper, extending the tone, swap, and discontinuous reordering, all with baseline by including 4-gram local language models. respect to the previous</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48– 54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="7602" citStr="Koehn et al., 2005" startWordPosition="1214" endWordPosition="1217">r. Koehn and Hoang (2007) have reported an increase of 0.86 BLEU points for German-to-English translation for small training data. After relaxing phrase-matching to include lemma and morphological information on the source side, POS language models lead to a decrease of -0.42 BLEU points. Supertagging encapsulates more contextual information than POS tags and Birch et al. (2007) report improvements when comparing a supertag language model to a baseline using a word language model p(wm1 ) ∝ �m i=1 p(tm1 ) ∝ �m i=1 870 only. Once the baseline incorporates lexicalized distortion (Tillmann, 2004; Koehn et al., 2005), these improvements disappear. Factored language models have not resulted in significant improvements either. Kirchhoff and Yang (2005) report slight improvements when re-ranking the n-best lists of their decoder, which word tri-grams. But these improvements are less than those gained by re-ranking the n-best lists with a 4-gram word language model. The impact of POS language models depends among other things on the size of the parallel corpus, the size and order of the word language model, and whether lexicalized distortion models are used. To gauge the potential effectiveness of POS languag</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondˇrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="18966" citStr="Koehn et al., 2007" startWordPosition="3024" endWordPosition="3027">tures. During decoding no POS tagging ambiguities are resolved. Each target phrase is associated with its most likely POS tag sequence, given the source and target side of the phrase pair; see Section 2.1. 873 4 Experimental Setup of 11.4M source and 12.6M target tokens, and the Three approaches are compared in our experiments: Chinese-English bitext of 10.6M source and 12.3M the baseline system is a phrase-based statistical ma- target tokens. Word alignment was performed runchine translation system (Koehn et al., 2003), very ning GIZA++ in both directions and generating the similar to Moses (Koehn et al., 2007), using a word- symmetric alignments using the ‘grow-diag-finalbased 5-gram language model. The second approach and’ heuristics. extends the baseline by including a 7-gram POS- All three approaches, including the baseline, use based language model. The third approach repre- lexicalized distortion, distinguishing between monosents the work described in this paper, extending the tone, swap, and discontinuous reordering, all with baseline by including 4-gram local language models. respect to the previous and next phrase (Koehn et Translation quality is evaluated for two language al., 2005). The d</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Abhishek Arun</author>
<author>Hieu Hoang</author>
</authors>
<title>Towards better machine translation quality for the german–english language pairs.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>139--142</pages>
<contexts>
<context position="5284" citStr="Koehn et al., 2008" startWordPosition="827" endWordPosition="830">ing of m POS tags by Equation 3. p(wi|wi−1 i−n+1) (2) p(ti|ti−1 i−n+1) (3) where, n is the order of the language model, and wji refers to the sub-sequence of words (or tags) from positions i to j. Word language models can be built directly from large text corpora, such as LDC’s Gigaword corpus, but POS models require texts that are annotated with POS tags. Ideally, one would use manually annotated corpora such as the Penn Treebank (Marcus et al., 1993), but since those tend to be small, most approaches rely on larger corpora which have been automatically annotated by a POS tagger or a parser (Koehn et al., 2008). Though automated annotation inevitably contains errors, it is assumed that this is ameliorated by the increased size of annotated data. The event space of a language models is of size |V |n, where V is the vocabulary, and n is the order of the language model. The vocabulary of POS models, (typically ranging between 40 and 100 tags), is much smaller than the vocabulary of a word model, which can easily approach a million words. Nevertheless, most POS language modeling approaches apply some form of smoothing to account for unseen events (Bonneau-Maynard et al., 2007). To deploy POS language mo</context>
</contexts>
<marker>Koehn, Arun, Hoang, 2008</marker>
<rawString>Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008. Towards better machine translation quality for the german–english language pairs. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 139–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="22127" citStr="Koehn, 2004" startWordPosition="3518" endWordPosition="3519">mpared in our experiments are staThe language model was trained using SRILM with tistically significant, we apply approximate randommodified Kneser-Ney smoothing and interpolation ization (Noreen, 1989); Riezler and Maxwell (2005) (Chen and Goodman, 1999). It is common practice have shown that approximate randomization is less not to include higher-order n-grams that occur fewer sensitive to Type-I errors, i.e., less likely to falsely than a predefined number of times. Here, we applied reject the null hypothesis, than bootstrap resampling rather conservative cut-offs, by ignoring 3-, 4-, and (Koehn, 2004) in the context of machine translation. 5-grams that occurred only once. The 7-gram POS 5 Results and Analysis and 4-gram local language models were both trained The Arabic-to-English results are shown in Taon the POS tagged English side of the bitext and ble 2, and the Chinese-to-English results in Ta10M sentences from Gigaword’s Xinhua and AFP ble 3. All results are subdivided by genre following sections. NIST’s genre classification. Note that MT06 conThe data for building the translation models were primarily drawn from the parallel news resources distributed by the Linguistic Data Consorti</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
</authors>
<title>Speech recognition and the frequency of recently used words: a modified Markov model for natural language.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th conference on Computational Linguistics,</booktitle>
<pages>348--350</pages>
<contexts>
<context position="10978" citStr="Kuhn, 1988" startWordPosition="1755" endWordPosition="1756">andard POS language models. Instead of using one global POS language model that is built by using all of a mono-lingual corpus in the target language, we build individual models, or local models, for each word-POS pair using the POS tags surrounding each occurrence of that pair. This adds an aspect of lexicalization that is entirely absent in previous POS language models. The effect is that the resulting ngram probability distributions of each local model are more biased towards the contextual constraints of each individual word-POS pair. This is similar to the idea of cached language models (Kuhn, 1988), but more fine-grained and with a tighter integration of POS and lexical information. 3.1 Definition of Local Language Models Each conditional probability of order n in a local model for the word-POS pair w:t is of the form: pw:t(tn,pn|t1:p1, ... , tn−1:pn−1) where ti refers to POS tags and pi to positions relative to an occurrence of the pair (w : t). For example, consider the sentence fragment in Figure 1. The conditional local n-gram probabilities (a–d) are generated from the occurrence of the word told with POS tag VBD. Probability (c) in Figure 1 estimates that a word with POS tag NN occ</context>
</contexts>
<marker>Kuhn, 1988</marker>
<rawString>Roland Kuhn. 1988. Speech recognition and the frequency of recently used words: a modified Markov model for natural language. In Proceedings of the 12th conference on Computational Linguistics, pages 348– 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Large-scale discriminative n-gram language models for statisti- cal machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="33629" citStr="Li and Khudanpur (2008)" startWordPosition="5389" endWordPosition="5392">nner. Factored language models use more general representations, such as POS tags or stems, only during back-off. Kirchhoff and Yang (2005) applied factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word error rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (2007) and Zheng et al. (2008) use elaborately enriched representations, called super abstract role values (Wang and Harper, 2002), which capture contextual dependencies using lexical categories, role labels, and dependency grammar structures. So far their approach has been limited to re-ranking n-best lists only. 7 Conclusion Though POS language models do not lead to significant improvements over a competitive baseline, we have shown that a competitive phrase-based baseline system </context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale discriminative n-gram language models for statisti- cal machine translation. In Proceedings of AMTA, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<contexts>
<context position="16059" citStr="Manning, 1993" startWordPosition="2568" endWordPosition="2569">+1) · pdissidents:NNS(. :+1|VBZ:-2 JJR:-1) p(&lt;/s&gt;, &lt;/s&gt;|w53,t53) = p&lt;/s&gt;:&lt;/s&gt;(.:-1|JJR:-3 NNS:-2) · pmore:JJR(&lt;/s&gt;:+3|NNS:+1 .:+2) ·pdissidents:NNS(&lt;/s&gt;:+2|JJR:-1 .:+1) · p.:.(&lt;/s&gt;:+1|JJR:-2 NNS:-1) Figure 3: Language model probability computation for the sentence “Cuba frees more dissidents.” using our local language modeling approach. The example in Figure 3 shows word-by-word how tri-gram local language models are used to compute the probability of a whole sentence. Our local language model approach also bears some resemblance to statistical approaches to modeling subcategorization frames (Manning, 1993). While our approach is more general by considering all words and not just focusing on verbal subcategorization frames, it is also more shallow in the sense that only part-of-speech categories are considered which does not model any contextual relationships on the phrase level. 3.2 Building Local Language Models To build the local language models, we use the SRILM toolkit (Stolcke, 2002), which is commonly applied in speech recognition and statistical machine translation. While SRILM collects n-gram statistics from all n-grams occurring in a corpus to build a single global language model, we b</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Christopher D. Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--313</pages>
<contexts>
<context position="5121" citStr="Marcus et al., 1993" startWordPosition="796" endWordPosition="799">ge model is built from that. While word-based models estimate the probability of a string of m words by Equation 2, POS-based models estimate the probability of string of m POS tags by Equation 3. p(wi|wi−1 i−n+1) (2) p(ti|ti−1 i−n+1) (3) where, n is the order of the language model, and wji refers to the sub-sequence of words (or tags) from positions i to j. Word language models can be built directly from large text corpora, such as LDC’s Gigaword corpus, but POS models require texts that are annotated with POS tags. Ideally, one would use manually annotated corpora such as the Penn Treebank (Marcus et al., 1993), but since those tend to be small, most approaches rely on larger corpora which have been automatically annotated by a POS tagger or a parser (Koehn et al., 2008). Though automated annotation inevitably contains errors, it is assumed that this is ameliorated by the increased size of annotated data. The event space of a language models is of size |V |n, where V is the vocabulary, and n is the order of the language model. The vocabulary of POS models, (typically ranging between 40 and 100 tags), is much smaller than the vocabulary of a word model, which can easily approach a million words. Neve</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>WileyInterscience.</publisher>
<contexts>
<context position="21717" citStr="Noreen, 1989" startWordPosition="3456" endWordPosition="3457">ents re- error rate (TER) version 0.7.25 (Snover et al., 2006). leased during the periods that overlap with the pub- All results reported here are case-insensitive. TER lication dates of the documents included in our de- scores are shown as 1-TER. velopment or test data sets. In total, 630 million to- To see whether the differences between the apkens were used to build the word language model. proaches we compared in our experiments are staThe language model was trained using SRILM with tistically significant, we apply approximate randommodified Kneser-Ney smoothing and interpolation ization (Noreen, 1989); Riezler and Maxwell (2005) (Chen and Goodman, 1999). It is common practice have shown that approximate randomization is less not to include higher-order n-grams that occur fewer sensitive to Type-I errors, i.e., less likely to falsely than a predefined number of times. Here, we applied reject the null hypothesis, than bootstrap resampling rather conservative cut-offs, by ignoring 3-, 4-, and (Koehn, 2004) in the context of machine translation. 5-grams that occurred only once. The 7-gram POS 5 Results and Analysis and 4-gram local language models were both trained The Arabic-to-English result</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, ALex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Meeting of the North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>161--168</pages>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, ALex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of the 2004 Meeting of the North American chapter of the Association for Computational Linguistics, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz-Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<marker>Och, 2003</marker>
<rawString>Franz-Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="20844" citStr="Papineni et al., 2001" startWordPosition="3315" endWordPosition="3318">petitive baseline, we to MT08 for Chinese-English. NIST did not redesigned the baseline to use a large 5-gram word lease a new Chinese-English test set for MT-Eval language model and lexicalized distortion model- 2009. Parameter tuning of the decoder was done ing, both of which are known to cancel-out improve- with minimum error rate training (MERT) (Och, ments gained from POS language models (Birch et 2003), adapted to BLEU maximization. al., 2007; Kirchhoff and Yang, 2005). The 5-gram As evaluation metrics we used NIST’s adaptaword language model is trained on the Xinhua and tion of BLEU-4 (Papineni et al., 2001), version 13a, AFP sections of the Gigaword corpus (3rd edition, where the brevity penalty is based on the reference LDC2007T40) and the target side of the bitext. We translation with the closest length, and translation removed from the training data all documents re- error rate (TER) version 0.7.25 (Snover et al., 2006). leased during the periods that overlap with the pub- All results reported here are case-insensitive. TER lication dates of the documents included in our de- scores are shown as 1-TER. velopment or test data sets. In total, 630 million to- To see whether the differences betwee</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL 2001), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Parsers as language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>172--181</pages>
<contexts>
<context position="8433" citStr="Post and Gildea, 2008" startWordPosition="1347" endWordPosition="1350">oder, which word tri-grams. But these improvements are less than those gained by re-ranking the n-best lists with a 4-gram word language model. The impact of POS language models depends among other things on the size of the parallel corpus, the size and order of the word language model, and whether lexicalized distortion models are used. To gauge the potential effectiveness of POS language models without taking into consideration all these factors, we isolate the contribution of the language model by simulating machine translation output using English data only (Al-Onaizan and Papineni, 2006; Post and Gildea, 2008). Taking a set of POS-tagged reference translations of the MT04 Arabic-to-English test set, each English sentence is randomly chunked into n-grams of average length three. The chunks of each sentence, with their corresponding POS tags, are randomly reordered. This is repeated 500 times for each sentence in the test set. The smoothed sentence BLEU score (ignoring brevity penalty) is computed for each reordered sentence with respect to all reference translations. The higher the BLEU score, the more well-formed the reordering is. As each reordered sentence only contains words from at least one of</context>
</contexts>
<marker>Post, Gildea, 2008</marker>
<rawString>Matt Post and Daniel Gildea. 2008. Parsers as language models for statistical machine translation. In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas, pages 172–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="21745" citStr="Riezler and Maxwell (2005)" startWordPosition="3458" endWordPosition="3461">rate (TER) version 0.7.25 (Snover et al., 2006). leased during the periods that overlap with the pub- All results reported here are case-insensitive. TER lication dates of the documents included in our de- scores are shown as 1-TER. velopment or test data sets. In total, 630 million to- To see whether the differences between the apkens were used to build the word language model. proaches we compared in our experiments are staThe language model was trained using SRILM with tistically significant, we apply approximate randommodified Kneser-Ney smoothing and interpolation ization (Noreen, 1989); Riezler and Maxwell (2005) (Chen and Goodman, 1999). It is common practice have shown that approximate randomization is less not to include higher-order n-grams that occur fewer sensitive to Type-I errors, i.e., less likely to falsely than a predefined number of times. Here, we applied reject the null hypothesis, than bootstrap resampling rather conservative cut-offs, by ignoring 3-, 4-, and (Koehn, 2004) in the context of machine translation. 5-grams that occurred only once. The 7-gram POS 5 Results and Analysis and 4-gram local language models were both trained The Arabic-to-English results are shown in Taon the POS </context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57– 64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<pages>518</pages>
<contexts>
<context position="1945" citStr="Schwenk, 2007" startWordPosition="297" endWordPosition="298">tion is a fluent continuation of the current translation hypothesis. The size and domain of the language model can have a significant impact on translation quality. Brants et al. (2007) have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points. On the other hand, each doubling using general web data leads to improvements of approximately 0.15 BLEU points. While large n-gram language models do lead to improved translation quality, they still lack any generalization beyond the surface forms (Schwenk, 2007). Consider example (1), which is a short sentence fragment from the MT09 Arabic-English test set, with the corresponding machine translation output (1.b), from a phrase-based statistical machine translation system, and reference translation (1.c). (1) a. ÈYj. ÊË b�.�JÓ &lt;J��¯Am�• �HAm� Qå”~~ �éJ��®Ê g ... ... éË ÑêÓAî~E@ð b. ... the background of press statements of controversial and accused him ... c. ... the background of controversial press statements and accused him ... Clearly, the adjective “controversial” should precede the nouns “press statement”, but since the AFP and Xinhua portions o</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21:492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="27376" citStr="Shen et al. (2009)" startWordPosition="4352" endWordPosition="4355">see similar improvements in BLEU. The improvements of using a POS language model are negligible (+0.04 BLEU). Here as well, local language modeling leads to the best results, with substantial improvements of +0.88 BLEU for web documents. The major difference between Arabic-English and Chinese-English is the discrepancy between BLEU score improvements and decreases in 1-TER. While we cannot explain this discrepancy, it is worth noting that similar discrepancies between BLEU and TER and Arabic-to-English and Chinese-to-English translation can be found in the literature. The results described in Shen et al. (2009) show a strong correlation between BLEU and 1-TER improvements6 6Shen et al. (2009) report TER rather than 1-TER scores. 875 systems and MT04 MT05 MT06 MT08 MT05–08 improvements tune NW NW WB ALL NW WB ALL NW WB ALL BLEU[%] 1a wordLM 37.32 32.55 33.33 23.40 31.16 28.67 17.57 24.03 31.93 19.82 29.30 2a +posLM 37.32 32.47 33.13 23.67 31.06 28.63 18.46 24.35 31.82 20.46 29.34 3a &gt; wordLM +0.00 −0.08 −0.20 +0.27 −0.10 −0.04 +0.89N +0.32 −0.11 +0.64N +0.04 4a +locLM 38.15 33.05 33.33 24.62 31.42 29.52 18.24 24.79 32.36 20.70 29.82 5a &gt; wordLM +0.83N +0.50M +0.00 +1.22N +0.26 +0.85N +0.67M +0.76N +0</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In Proceedings of Empirical Methods in Natural Language Processing, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation forMachine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="21166" citStr="Snover et al., 2006" startWordPosition="3367" endWordPosition="3370"> error rate training (MERT) (Och, ments gained from POS language models (Birch et 2003), adapted to BLEU maximization. al., 2007; Kirchhoff and Yang, 2005). The 5-gram As evaluation metrics we used NIST’s adaptaword language model is trained on the Xinhua and tion of BLEU-4 (Papineni et al., 2001), version 13a, AFP sections of the Gigaword corpus (3rd edition, where the brevity penalty is based on the reference LDC2007T40) and the target side of the bitext. We translation with the closest length, and translation removed from the training data all documents re- error rate (TER) version 0.7.25 (Snover et al., 2006). leased during the periods that overlap with the pub- All results reported here are case-insensitive. TER lication dates of the documents included in our de- scores are shown as 1-TER. velopment or test data sets. In total, 630 million to- To see whether the differences between the apkens were used to build the word language model. proaches we compared in our experiments are staThe language model was trained using SRILM with tistically significant, we apply approximate randommodified Kneser-Ney smoothing and interpolation ization (Noreen, 1989); Riezler and Maxwell (2005) (Chen and Goodman, 1</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation forMachine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="16449" citStr="Stolcke, 2002" startWordPosition="2630" endWordPosition="2631">gram local language models are used to compute the probability of a whole sentence. Our local language model approach also bears some resemblance to statistical approaches to modeling subcategorization frames (Manning, 1993). While our approach is more general by considering all words and not just focusing on verbal subcategorization frames, it is also more shallow in the sense that only part-of-speech categories are considered which does not model any contextual relationships on the phrase level. 3.2 Building Local Language Models To build the local language models, we use the SRILM toolkit (Stolcke, 2002), which is commonly applied in speech recognition and statistical machine translation. While SRILM collects n-gram statistics from all n-grams occurring in a corpus to build a single global language model, we build a language model for each word-POS pair only using the ngrams within the proximity of occurrences for that word-POS pair in a POS-tagged corpus. This results in separate n-gram count files, which are then processed by SRILM to build the individual language models.4 Charniak’s parser (Charniak, 2000) is used to POS tag the corpus. 4The pre-processing scripts are available at http://w</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM—an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL-04),</booktitle>
<pages>101--104</pages>
<contexts>
<context position="7581" citStr="Tillmann, 2004" startWordPosition="1212" endWordPosition="1213">ve quality either. Koehn and Hoang (2007) have reported an increase of 0.86 BLEU points for German-to-English translation for small training data. After relaxing phrase-matching to include lemma and morphological information on the source side, POS language models lead to a decrease of -0.42 BLEU points. Supertagging encapsulates more contextual information than POS tags and Birch et al. (2007) report improvements when comparing a supertag language model to a baseline using a word language model p(wm1 ) ∝ �m i=1 p(tm1 ) ∝ �m i=1 870 only. Once the baseline incorporates lexicalized distortion (Tillmann, 2004; Koehn et al., 2005), these improvements disappear. Factored language models have not resulted in significant improvements either. Kirchhoff and Yang (2005) report slight improvements when re-ranking the n-best lists of their decoder, which word tri-grams. But these improvements are less than those gained by re-ranking the n-best lists with a 4-gram word language model. The impact of POS language models depends among other things on the size of the parallel corpus, the size and order of the word language model, and whether lexicalized distortion models are used. To gauge the potential effecti</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL-04), pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
</authors>
<title>The SuperARV language model: investigating the effectiveness of tightly integrating multiple knowledge sources.</title>
<date>2002</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="33872" citStr="Wang and Harper, 2002" startWordPosition="5425" endWordPosition="5428"> al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word error rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (2007) and Zheng et al. (2008) use elaborately enriched representations, called super abstract role values (Wang and Harper, 2002), which capture contextual dependencies using lexical categories, role labels, and dependency grammar structures. So far their approach has been limited to re-ranking n-best lists only. 7 Conclusion Though POS language models do not lead to significant improvements over a competitive baseline, we have shown that a competitive phrase-based baseline system can benefit from using POS information by building lexically anchored local models. Our local model approach does not only lead to more contextspecific probability distributions, but also takes ad877 vantage of the language model probability o</context>
</contexts>
<marker>Wang, Harper, 2002</marker>
<rawString>Wen Wang and Mary P. Harper. 2002. The SuperARV language model: investigating the effectiveness of tightly integrating multiple knowledge sources. In Proceedings of Empirical Methods in Natural Language Processing, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
</authors>
<title>Reranking machine translation hypotheses with structured and web-based language models.</title>
<date>2007</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition &amp; Understanding,</booktitle>
<pages>159--164</pages>
<contexts>
<context position="17349" citStr="Wang et al., 2007" startWordPosition="2774" endWordPosition="2777">n the proximity of occurrences for that word-POS pair in a POS-tagged corpus. This results in separate n-gram count files, which are then processed by SRILM to build the individual language models.4 Charniak’s parser (Charniak, 2000) is used to POS tag the corpus. 4The pre-processing scripts are available at http://www. science.uva.nl/˜christof/locLM/. 3.3 Decoder Integration position 0 1 2 3 4 5 6 token &lt;s&gt; cuba frees more dissidents . &lt;/s&gt; POS tag &lt;s&gt; NNP VBZ JJR NNS . &lt;/s&gt; Several approaches that integrate POS language models have focused on n-best list re-ranking only (Hasan et al., 2006; Wang et al., 2007). Often this is due to the computational (and implementational) complexities of integrating more complex language models with the decoder, although it is expected that a tighter integration with the decoder itself leads to better improvements than n-best list re-ranking. Integrating our local language modeling approach with a decoder is straightforward. Our baseline decoder already uses SRILM’s API for computing word language model probabilities. Since SRILM supports arbitrarily many language models, local language models can be added using the same functionalities of SRILM’s API. For the expe</context>
<context position="33748" citStr="Wang et al. (2007)" startWordPosition="5406" endWordPosition="5409">and Yang (2005) applied factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word error rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (2007) and Zheng et al. (2008) use elaborately enriched representations, called super abstract role values (Wang and Harper, 2002), which capture contextual dependencies using lexical categories, role labels, and dependency grammar structures. So far their approach has been limited to re-ranking n-best lists only. 7 Conclusion Though POS language models do not lead to significant improvements over a competitive baseline, we have shown that a competitive phrase-based baseline system can benefit from using POS information by building lexically anchored local models. Our local model approach does not o</context>
</contexts>
<marker>Wang, Stolcke, Zheng, 2007</marker>
<rawString>Wen Wang, Andreas Stolcke, and Jing Zheng. 2007. Reranking machine translation hypotheses with structured and web-based language models. In IEEE Workshop on Automatic Speech Recognition &amp; Understanding, pages 159–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Zheng</author>
<author>Necip Fazil Ayan</author>
<author>Wen Wang</author>
<author>Dimitra Vergyri</author>
<author>Nicolas Scheffer</author>
<author>Andreas Stolcke</author>
</authors>
<date>2008</date>
<booktitle>SRI systems in the NIST MT08 Evaluation. In Proceedings of the NIST 2008 Open MT Evaluation Workshop.</booktitle>
<contexts>
<context position="33772" citStr="Zheng et al. (2008)" startWordPosition="5411" endWordPosition="5414"> factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word error rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (2007) and Zheng et al. (2008) use elaborately enriched representations, called super abstract role values (Wang and Harper, 2002), which capture contextual dependencies using lexical categories, role labels, and dependency grammar structures. So far their approach has been limited to re-ranking n-best lists only. 7 Conclusion Though POS language models do not lead to significant improvements over a competitive baseline, we have shown that a competitive phrase-based baseline system can benefit from using POS information by building lexically anchored local models. Our local model approach does not only lead to more context</context>
</contexts>
<marker>Zheng, Ayan, Wang, Vergyri, Scheffer, Stolcke, 2008</marker>
<rawString>Jing Zheng, Necip Fazil Ayan, Wen Wang, Dimitra Vergyri, Nicolas Scheffer, and Andreas Stolcke. 2008. SRI systems in the NIST MT08 Evaluation. In Proceedings of the NIST 2008 Open MT Evaluation Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>