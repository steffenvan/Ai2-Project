<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005044">
<title confidence="0.997228">
A Salience-Based Approach to Gesture-Speech Alignment
</title>
<author confidence="0.858623">
Jacob Eisenstein and C. Mario Christoudias
</author>
<affiliation confidence="0.59557">
MIT Computer Science and Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.9521695">
32 Vassar Street
Cambridge, MA 02139
</address>
<email confidence="0.999573">
{jacobe+cmch}@csail.mit.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999285625">
One of the first steps towards understanding
natural multimodal language is aligning ges-
ture and speech, so that the appropriate ges-
tures ground referential pronouns in the speech.
This paper presents a novel technique for
gesture-speech alignment, inspired by salience-
based approaches to anaphoric pronoun reso-
lution. We use a hybrid between data-driven
and knowledge-based mtehods: the basic struc-
ture is derived from a set of rules about gesture
salience, but the salience weights themselves
are learned from a corpus. Our system achieves
95% recall and precision on a corpus of tran-
scriptions of unconstrained multimodal mono-
logues, significantly outperforming a competi-
tive baseline.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951">
In face to face communication, speakers frequently use
gesture to supplement speech (Chovil, 1992), using the
additional modality to provide unique, non-redundant in-
formation (McNeill, 1992). In the context of pen/speech
user interfaces, Oviatt finds that “multimodal ... language
is briefer, syntactically simpler, and less disfluent than
users’ unimodal speech.” (Oviatt, 1999)
One of the simplest and most direct ways in which ges-
ture can supplement verbal communication is by ground-
ing references, usually through deixis. For example, it is
impossible to extract the semantic content of the verbal
utterance “I’ll take this one” without an accompanying
pointing gesture indicating the thing that is desired. The
problem of gesture-speech alignment involves choosing
the appropriate gesture to ground each verbal utterance.
This paper describes a novel technique for this problem.
We evaluate our system on a corpus of multimodal mono-
logues with no fixed grammar or vocabulary.
</bodyText>
<subsectionHeader confidence="0.88284">
1.1 Example
</subsectionHeader>
<bodyText confidence="0.873163">
[This]_1 thing goes over [here]_2 so
that it goes back ...
</bodyText>
<listItem confidence="0.983882">
1. Deictic: Hand rests on latch mechanism
2. Iconic: Hand draws trajectory from
right to left
</listItem>
<bodyText confidence="0.999920363636364">
In this example, there are three verbal references. The
word “this” refers to the latch mechanism, which is indi-
cated by the rest position of the hand. “Here” refers to the
endpoint of the trajectory indicated by the iconic gesture.
“It” is an anaphoric reference to a noun phrase defined
earlier in the sentence; there is no accompanying gesture.
The word “that” does not act as a reference, although it
could in other cases. Not every pronoun keyword (e.g.,
this, here, it, that, etc.) will act as a reference in all cases.
In addition, there will be many gestures that do not re-
solve any keyword.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999929357142857">
This research draws mainly from two streams of re-
lated work. Researchers in human-computer interaction
have worked towards developing multimodal user inter-
faces, which allow spoken and gestural input. These sys-
tems often feature powerful algorithms for fusing modal-
ities; however, they also restrict communication to short
grammatically-constrained commands over a very lim-
ited vocabulary. Since our goal is to handle more com-
plex linguistic phenomena, these systems were of little
help in the design of our algorithm. Conversely, we found
that the problem of anaphora resolution faces a very sim-
ilar set of challenges as gesture-speech alignment. We
were able to apply techniques from anaphora resolution
to gesture-speech alignment.
</bodyText>
<subsectionHeader confidence="0.984613">
2.1 Multimodal User Interfaces
</subsectionHeader>
<bodyText confidence="0.9999715625">
Discussion of multimodal user interfaces begins with the
seminal “Put-That-There” system (Bolt, 1980), which al-
lowed users to issue natural language commands and use
deictic hand gestures to resolve references from speech.
Commands were subject to a strict grammar and align-
ment was straightforward: keywords created holes in the
semantic frame, and temporally-aligned gestures filled
the holes.
More recent systems have extended this approach
somewhat. Johnston and Bangalore describe a multi-
modal parsing algorithm that is built using a 3-tape, finite
state transducer (FST) (Johnston and Bangalore, 2000).
The speech and gestures of each multimodal utterance
are provided as input to an FST whose output is a se-
mantic representation conveying the combined meaning.
A similar system, based on a graph-matching algorithm,
is described in (Chai et al., 2004). These systems per-
form mutual disambiguation, where each modality helps
to correct errors in the others. However, both approaches
restrict users to a predefined grammar and lexicon, and
rely heavily on having a complete, formal ontology of
the domain.
In (Kettebekov et al., 2002), a co-occurrence model
relates the salient prosodic features of the speech (pitch
variation and pause) to characteristic features of gestic-
ulation (velocity and acceleration). The goal was to im-
prove performance of gesture recognition, rather than to
address the problem of alignment directly. Their ap-
proach also differs from ours in that they operate at the
level of speech signals, rather than recognized words.
Potentially, the two approaches could compliment each
other in a unified system.
</bodyText>
<subsectionHeader confidence="0.999408">
2.2 Anaphora Resolution
</subsectionHeader>
<bodyText confidence="0.9996856875">
Anaphora resolution involves linking an anaphor to its
corresponding antecedent in the same or previous sen-
tence. In many cases, speech/gesture multimodal fusion
works in a very similar way, with gestures grounding
some of the same anaphoric pronouns (e.g., “this”, “that”,
“here”).
One approach to anaphora resolution is to assign a
salience value to each noun phrase that is a candidate
for acting as a grounding referent, and then to choose
the noun phrase with the greatest salience (Lappin and
Leass, 1994). Mitkov showed that a salience-based ap-
proach can be applied across genres and without com-
plex syntactic, semantic, and discourse analysis (Mitkov,
1998). Salience values are typically computed by apply-
ing linguistic knowledge; e.g., recent noun phrases are
more salient, gender and number should agree, etc. This
knowledge is applied to derive a salience value through
the application of a set of predefined salience weights
on each feature. Salience weights may be defined by
hand, as in (Lappin and Leass, 1994), or learned from
data (Mitkov et al., 2002).
Anaphora resolution and gesture-speech alignment are
very similar problems. Both involve resolving ambigu-
ous words which reference other parts of the utterance.
In the case of anaphora resolution, pronomial references
resolve to previously uttered noun phrases; in gesture-
speech alignment, keywords are resolved by gestures,
which usually precede the keyword. The salience-based
approach works for anaphora resolution because the fac-
tors that contribute to noun-phrase salience are well un-
derstood. We define a parallel set of factors for evaluating
the salience of gestures.
</bodyText>
<sectionHeader confidence="0.97358" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999918">
The most important goal of our system is the ability to
handle natural, human-to-human language usage. This
includes disfluencies and grammatically incorrect utter-
ances, which become even more problematic when con-
sidering that the output of speech recognizers is far from
perfect. Any approach that requires significant parsing
or other grammatical analysis may be ill-suited to meet
these goals.
Instead, we identify keywords that are likely to require
gestural referents for resolution. Our goal is to produce
an alignment – a set of bindings – that match at least some
of the identified keywords with one or more gestures.
There are several things that are known to contribute to
the salience of candidate gesture-speech bindings:
</bodyText>
<listItem confidence="0.997346055555556">
• The relevant gesture is usually close in time to the
keyword (Oviatt et al., 1997; Cohen et al., 2002)
• The gesture usually precedes the keyword (Oviatt et
al., 1997).
• A one-to-one mapping is preferred. Multiple key-
words rarely align with a single gesture, and mul-
tiple gestures almost never align with a single key-
word (Eisenstein and Davis, 2003).
• Some types of gestures, such as deictic pointing ges-
tures, are more likely to take part in keyword bind-
ings. Other gestures (i.e., beats) do not carry this
type of semantic content, and instead act to moder-
ate turn taking or indicate emphasis. These gestures
are unlikely to take part in keyword bindings (Cas-
sell, 1998).
• Some keyword/gesture combinations may be partic-
ularly likely; for example, the keyword “this” and a
deictic pointing gesture.
</listItem>
<bodyText confidence="0.999841">
These rules mirror the salience weighting features em-
ployed by the anaphora resolution methods described in
the previous section. We define a parameterizable penalty
function that prefers alignments that adhere to as many of
these rules as possible. Given a set of verbal utterances
and gestures, we then try to find the set of bindings with
the minimal penalty. This is essentially an optimization
approach, and we use the simplest possible optimization
technique: greedy hill-climbing. Of course, given a set
of penalties and the appropriate representation, any op-
timization technique could be applied. In the evaluation
section, we discuss whether and how much our system
would benefit from using a more sophisticated optimiza-
tion technique. Later in this section, we formalize the
problem and our proposed solution.
</bodyText>
<subsectionHeader confidence="0.999952">
3.1 Leveraging Empirical Data
</subsectionHeader>
<bodyText confidence="0.999946066666667">
One of the advantages of the salience-based approach is
that it enables the creation of a hybrid system that ben-
efits both from our intuitions about multimodal commu-
nication and from a corpus of annotated data. The form
of the salience metric, and the choice of features that fac-
tor into it, is governed by our knowledge about the way
speech and gesture work. However, the penalty func-
tion also requires parameters that weigh the importance
of each factor. These parameters can be crafted by hand
if no corpus is available, but they can also be learned from
data. By using knowledge about multimodal language to
derive the form and features of the salience metric, and
using a corpus to fine-tune the parameters of this metric,
we can leverage the strengths of both knowledge-based
and data-driven approaches.
</bodyText>
<sectionHeader confidence="0.998555" genericHeader="method">
4 Formalization
</sectionHeader>
<bodyText confidence="0.999944">
We define a multimodal transcript M to consist of a set
of spoken utterances S and gestures G. S contains a set
of references R that must be ground by a gestural ref-
erent. We define a binding, b E B, as a tuple relating
a gesture, g E G, to a corresponding speech reference,
r E R. Provided G and R, the set B enumerates all
possible bindings between them. Formally, each gesture,
reference, and binding are defined as
</bodyText>
<equation confidence="0.877255">
g = (tgs, tge, Gtype)
</equation>
<bodyText confidence="0.999502857142857">
speech and gesture. We begin by presenting the analytical
form for the binding penalty function, ψb.
It is most often the case that verbal references closely
follow the gestures that they refer to; the verbal reference
rarely precedes the gesture. To reflect this knowledge,
we parameterize ψb using a time gap penalty, αtg, and a
wrong order penalty, αwo as follows,
</bodyText>
<equation confidence="0.9425986">
ψb(b) = αtgwtg(b) + αwowwo(b) (3)
where,
ww(b) — r0 trs &gt; tgs
o
l 1 trs &lt; tgs
</equation>
<bodyText confidence="0.992845">
and wtg = Itrs − tgs�
In addition to temporal agreement, specific words or
parts-of-speech have varying affinities for different types
of gestures. We incorporate these penalties into ψb by in-
troducing a binding agreement penalty, α(b), as follows:
</bodyText>
<equation confidence="0.99952">
ψb(b) = αtgwwo(b) + α(b) (4)
</equation>
<bodyText confidence="0.999974833333333">
The remaining penalty functions model binding fertil-
ity. Specifically, we assign a penalty for each unassigned
gesture and reference, ψg(g) and ψr(r) respectively, that
reflect our desire for the algorithm to produce bindings.
Certain gesture types (e.g., deictics) are much more likely
to participate in bindings than others (e.g., beats). An
unassigned gesture penalty is associated with each ges-
ture type, given by ψg(g). Similarly, we expect refer-
ences to have a likelihood of being bound that is condi-
tioned on their word or part-of-speech tag. However, we
currently handle all keywords in the same way, with a
constant penalty ψr(r) for unassigned keywords.
</bodyText>
<subsectionHeader confidence="0.997181">
4.2 Minimization Algorithm
</subsectionHeader>
<bodyText confidence="0.9999615">
Given G and R we wish to find a B* C_ B that minimizes
the penalty function ψ(B, G, R):
</bodyText>
<equation confidence="0.9916205">
r = (trs, tre, w) (1) B* = arg min ψ( ˆB, G, R) (5)
b = (g, r) Bˆ
</equation>
<bodyText confidence="0.9999625">
where ts, te describe the start and ending time of a gesture
or reference, w E S is the word corresponding to r, and
Gtype is the type of gesture (e.g. deictic or trajectory).
An alternative, useful description of the set B is as the
function b(g) which returns for each gesture a set of cor-
responding references. This function is defined as
</bodyText>
<equation confidence="0.983022">
b(g) = {rl(g, r) E B1 (2)
</equation>
<subsectionHeader confidence="0.932198">
4.1 Rules
</subsectionHeader>
<bodyText confidence="0.999486333333333">
In this section we provide the analytical form for the
penalty functions of Section 3. We have designed these
functions to penalize bindings that violate the preferences
that model our intuitions about the relationship between
Using the penalty functions of Section 4.1 ψ( ˆB, G, R) is
defined as,
</bodyText>
<equation confidence="0.9760355">
ψ( ˆB, G, R) = � ψb(b) + ψg(Ga) + ψr(Ra) (6)
bE Bˆ
</equation>
<bodyText confidence="0.847057">
where
</bodyText>
<equation confidence="0.9950395">
Ga = {gIb(g) = 01
Ra = {rIb(r) = 01 .
</equation>
<bodyText confidence="0.9997766">
Although there are numerous optimization techniques
that may be applied to minimize Equation 5, we have
chosen to implement a naive gradient decent algorithm
presented below as Algorithm 1. Observing the prob-
lem, note we could have initialized B* = B; in other
</bodyText>
<equation confidence="0.904642894736842">
Algorithm 1 Gradient Descent
Initialize B* = 0 and B&apos; = B
repeat
Let b0 be the first element in B&apos;
6max = O(B*, G, R) − O({B*, b0}, G, R)
bmax = b0
for all b E B&apos;, b =� b0 do
6 = O(B*, G, R) − O({B*, b}, G, R)
if 6 &gt; 6max then
bmax = b
6max = 6
end if
end for
if 6max &gt; 0 then
B* = {B*,bmax}
B&apos; = B&apos; − bmax
end if
Convergence test: is 6max &lt; limit?
until convergence
</equation>
<bodyText confidence="0.999912714285714">
words, start off with all possible bindings, and gradu-
ally prune away the bad ones. But it seems likely that
|B* |&lt; min(|R|, |G|); thus, starting from the empty set
will converge faster. The time complexity of this algo-
rithm is given by O(|B*||B|). Since |B |= |G||R|, and
assuming |B* |a |G |a |R|, this simplifies to O(|B*|3),
cubic in the number of bindings returned.
</bodyText>
<subsectionHeader confidence="0.999185">
4.3 Learning Parameters
</subsectionHeader>
<bodyText confidence="0.999916277777778">
We explored a number of different techniques for find-
ing the parameters of the penalty function: setting them
by hand, gradient descent, simulated annealing, and a ge-
netic algorithm. A detailed comparison of the results with
each approach is beyond the scope of this paper, but the
genetic algorithm outperformed the other approaches in
both accuracy and rate of convergence.
The genome representation consisted of a thirteen bit
string for each penalty parameter; three bits were used
for the exponent, and the remaining ten were used for
the base. Parameters were allowed to vary from 10−4
to 103. Since there were eleven parameters, the overall
string length was 143. A population size of 200 was used,
and training proceeded for 50 generations. Single-point
crossover was applied at a rate of 90%, and the mutation
rate was set to 3% per bit. Tournament selection was used
rather than straightforward fitness-based selection (Gold-
berg, 1989).
</bodyText>
<sectionHeader confidence="0.999545" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.995587666666667">
We evaluated our system by testing its performance
on a set of 26 transcriptions of unconstrained human-
to-human communication, from nine different speak-
</bodyText>
<table confidence="0.9997646">
Baseline Training Test
Recall 84.2% 94.6% 95.1%
Q n/a 1.2% 5.1%
Precision 82.8% 94.5% 94.5%
Q n/a 1.2% 5.0%
</table>
<tableCaption confidence="0.999927">
Table 1: Performance of our system versus a baseline
</tableCaption>
<bodyText confidence="0.999866807692308">
ers (Eisenstein and Davis, 2003). Of the four women and
five men who participated, eight were right-handed, and
one was a non-native English speaker. The participants
ranged in age from 22 to 28. All had extensive computer
experience, but none had any experience in the task do-
main, which required explaining the behavior of simple
mechanical devices.
The participants were presented with three conditions,
each of which involved describing the operation of a me-
chanical device based on a computer simulation. The
conditions were shown in order of increasing complexity,
as measured by the number of moving parts: a latchbox, a
piston, and a pinball machine. Monologues ranged in du-
ration from 15 to 90 seconds; the number of gestures used
ranged from six to 58. In total, 574 gesture phrases were
transcribed, of which 239 participated in gesture-speech
bindings.
In explaining the devices, the participants were al-
lowed – but not instructed – to refer to a predrawn di-
agram that corresponded to the simulation. Vocabulary,
grammar, and gesture were not constrained in any way.
The monologues were videotaped, transcribed, and an-
notated by hand. No gesture or speech recognition was
performed. The decision to use transcriptions rather than
speech and gesture recognizers will be discussed in detail
below.
</bodyText>
<subsectionHeader confidence="0.987872">
5.1 Empirical Results
</subsectionHeader>
<bodyText confidence="0.999968914893617">
We averaged results over ten experiments, in which 20%
of the data was selected randomly and held out as a test
set. Entire transcripts were held out, rather than parts of
each transcript. This was necessary because the system
considers the entire transcript holistically when choosing
an alignment.
For a baseline, we evaluated the performance of choos-
ing the temporally closest gesture to each keyword.
While simplistic, this approach is used in several imple-
mented multimodal user interfaces (Bolt, 1980; Koons
et al., 1993). Kettebekov and Sharma even reported that
93.7% of gesture phrases were “temporally aligned” with
the semantically associated keyword in their corpus (Ket-
tebekov and Sharma, 2001). Our results with this base-
line were somewhat lower, for reasons discussed below.
Table 1 shows the results of our system and the base-
line on our corpus. Our system significantly outperforms
the baseline on both recall and precision on this corpus
(p &lt; 0.05, two-tailed). Precision and recall differ slightly
because there are keywords that do not bind to any ges-
ture. Our system does not assume a one-to-one mapping
between keywords and gestures, and will refuse to bind
some keywords if there is no gesture with a high enough
salience. One benefit of our penalty-based approach is
that it allows us to easily trade off between recall and
precision. Reducing the penalties for unassigned ges-
tures and keywords will cause the system to create fewer
alignments, increasing precision and decreasing recall.
This could be useful in a system where mistaken ges-
ture/speech alignments are particularly undesirable. By
increasing these same penalties, the opposite effect can
also be achieved.
Both systems perform worse on longer monologues.
On the top quartile of monologues by length (measured
in number of keywords), the recall of the baseline system
falls to 75%, and the recall of our system falls to 90%.
For the baseline system, we found a correlation of -0.55
(df = 23, p &lt; 0.01) between F-measure and monologue
length.
This may help to explain why Kettebekov and Sharma
found such success with the baseline algorithm. The mul-
timodal utterances in their corpus consisted of relatively
short commands. The longer monologues in our corpus
tended to be more grammatically complex and included
more disfluency. Consequently, alignment was more dif-
ficult, and a relatively naive strategy, such as the baseline
algorithm, was less effective.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999921875">
To our knowledge, very few multimodal understanding
systems have been evaluated using natural, unconstrained
speech and gesture. One exception is (Quek et al., 2002),
which describes a system that extracts discourse struc-
ture from gesture on a corpus of unconstrained human-
to-human communication; however, no quantitative anal-
ysis is provided. Of the systems that are more relevant
to the specific problem of gesture-speech alignment (Co-
hen et al., 1997; Johnston and Bangalore, 2000; Kette-
bekov and Sharma, 2001), evaluation is always conducted
from an HCI perspective, in which participants act as
users of a computer system and communicate in short,
grammatically-constrained multimodal commands. As
shown in Section 5.1, such commands are significantly
easier to align than the natural multimodal communica-
tion found in our corpus.
</bodyText>
<subsectionHeader confidence="0.993386">
6.1 The Corpus
</subsectionHeader>
<bodyText confidence="0.999973571428572">
A number of considerations went into gathering this cor-
pus.&apos; One of our goals was to minimize the use of
discourse-related “beat” gestures, so as to better focus on
the deictic and iconic gestures that are more closely re-
lated to the content of the speech; that is why we focused
on monologues rather than dialogues. We also wanted the
corpus to be relevant to the HCI community. That is why
we provided a diagram to gesture at, which we believe
serves a similar function to a computer display, providing
reference points for deictic gestures. We used a predrawn
diagram – rather than letting participants draw the dia-
gram themselves – because interleaved speech, gesture,
and sketching is a much more complicated problem, to
be addressed only after bimodal speech-gesture commu-
nication is better understood.
For a number of reasons, we decided to focus on tran-
scriptions of speech and gesture, rather than using speech
and gesture recognition systems. Foremost is that we
wanted the language in our corpus to be as natural as pos-
sible; in particular, we wanted to avoid restricting speak-
ers to a finite list of gestures. Building a recognizer that
could handle such unconstrained gesture would be a sub-
stantial undertaking and an important research contribu-
tion in its own right. However, we are sensitive to the con-
cern that our system should scale to handle possibly erro-
neous recognition data. There are three relevant classes of
errors that our system may need to handle: speech recog-
nition, gesture recognition, and gesture segmentation.
</bodyText>
<listItem confidence="0.907273">
• Speech Recognition Errors
</listItem>
<bodyText confidence="0.99878175">
The speech recognizer could fail to recognize a key-
word; in this case, a binding would simply not be
created. If the speech recognizer misrecognized
a non-keyword as a keyword, a spurious binding
might be created. However, since our system does
not require that all keywords have bindings, we feel
that our approach is likely to degrade gracefully in
the face of this type of error.
</bodyText>
<listItem confidence="0.86407">
• Gesture Recognition Errors
</listItem>
<bodyText confidence="0.999959666666667">
This type of error would imply a gestural misclas-
sification, e.g., classifying a deictic pointing gesture
as an iconic. Again, we feel that a salience-based
system will degrade gracefully with this type of er-
ror, since there are no hard requirements on the type
of gesture for forming a binding. In contrast, a sys-
tem that required, say, a deictic gesture to accom-
pany a certain type of command would be very sen-
sitive to a gesture misclassification.
</bodyText>
<footnote confidence="0.580337">
&apos;We also considered using the recently released FORM2
corpus from the Linguistic Data Consortium. However, this
corpus is presently more focused on the kinematics of hand and
upper body movement, rather than on higher-level linguistic in-
formation relating to gestures and speech.
</footnote>
<listItem confidence="0.947189">
• Gesture Segmentation Errors
</listItem>
<bodyText confidence="0.999942">
Gesture segmentation errors are probably the most
dangerous, since this could involve incorrectly
grouping two separate gestures into a single gesture,
or vice versa. It seems that this type of error would
be problematic for any approach, and we have no
reason to believe that our salience-based approach
would fare differently from any other approach.
</bodyText>
<subsectionHeader confidence="0.99978">
6.2 Success Cases
</subsectionHeader>
<bodyText confidence="0.999986652173913">
Our system outperformed the baseline by more than 10%.
There were several types of phenomena that the base-
line failed to handle. In this corpus, each gesture pre-
cedes the semantically-associated keyword 85% of the
time. Guided by this fact, we first created a baseline sys-
tem that selected the nearest preceding gesture for each
keyword; clearly, the maximum performance for such a
baseline is 85%. Slightly better results were achieved by
simply choosing the nearest gesture regardless of whether
it precedes the keyword; this is the baseline shown in Ta-
ble 1. However, this baseline incorrectly bound several
cataphoric gestures. The best strategy is to accept just a
few cataphoric gestures in unusual circumstances, but a
naive baseline approach is unable to do this.
Most of the other baseline errors came about when
the mapping from gesture to speech was not one-to-one.
For example, in the utterance “this piece here,” the two
keywords actually refer to a single deictic gesture. In
the salience-based approach, the two keywords were cor-
rectly bound to a single gesture, but the baseline insisted
on finding two gestures. The baseline similarly mishan-
dled situations where a keyword was used without refer-
ring to any gesture.
</bodyText>
<subsectionHeader confidence="0.999108">
6.3 Failure Cases
</subsectionHeader>
<bodyText confidence="0.999968">
Although the recall and precision of our system neared
95%, investigating the causes of error could suggest
potential improvements. We were particularly interested
in errors on the training set, where overfitting could not
be blamed. This section describes two sources of error,
and suggests some potential improvements.
</bodyText>
<subsectionHeader confidence="0.68643">
6.3.1 Disfluencies
</subsectionHeader>
<bodyText confidence="0.999643857142857">
We adopted a keyword-based approach so that our sys-
tem would be more robust to disfluency than alternative
approaches that depended on parsing. While we were
able to handle many instances of disfluent speech, we
found that disfluencies occasionally disturbed the usual
relationship between gesture and speech. For example,
consider the following utterance:
It has this... this spinning thing...
Our system attempted to bind gestures to each occur-
rence of “this”, and ended up binding each reference to a
different gesture. Moreover, both references were bound
incorrectly. The relevant gesture in this case occurs after
both references. This is an uncommon phenomenon, and
as such, was penalized highly. However, anecdotally
it appears that the presence of a disfluency makes this
phenomenon more likely. A disfluency is frequently
accompanied by an abortive gesture, followed by the
full gesture occurring somewhat later than the spoken
reference. It is possible that a system that could detect
disfluency in the speech transcript could account for this
phenomenon.
</bodyText>
<subsectionHeader confidence="0.653007">
6.3.2 Greedy Search
</subsectionHeader>
<bodyText confidence="0.999482">
Our system applies a greedy hill-climbing opti-
mization to minimize the penalty. While this greedy
optimization performs surprisingly well, we were able
to identify a few cases of errors that were caused by the
greedy nature of our optimization, e.g.
...once it hits this, this thing is blocked.
In this example, the two references are right next to
each other. The relevant gestures are also very near each
other. The ideal bindings are shown in Figure 1a. The
earlier “this” is considered first, but from the system’s
perspective, the best possible binding is the second ges-
ture, since it overlaps almost completely with the spoken
utterance (Figure 1b). However, once the second gesture
is bound to the first reference, it is removed from the list
of unassigned gestures. Thus, if the second gesture were
also bound to the second utterance, the penalty would
still be relatively high. Even though the earlier gesture
is farther away from the second reference, it is still on the
list of unassigned gestures, and the system can reduce the
overall penalty considerably by binding it. The system
ends up crisscrossing, and binding the earlier gesture to
the later reference, and vice versa (Figure 1c).
</bodyText>
<sectionHeader confidence="0.998564" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.9999646">
The errors discussed in the previous section suggest some
potential improvements to our system. In this section, we
describe four possible avenues of future work: dynamic
programming, deeper syntactic analysis, other anaphora
resolution techniques, and user adaptation.
</bodyText>
<subsectionHeader confidence="0.980033">
7.1 Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.9832276">
Algorithm 1 provides only an approximate solution to
Equation 5. As demonstrated in Section 6.3.2, the greedy
choice is not always optimal. Using dynamic program-
ming, an exhaustive search of the space of bindings can
be performed within polynomial time.
</bodyText>
<figure confidence="0.990152">
(a) (b) (c)
</figure>
<figureCaption confidence="0.999996">
Figure 1: The greedy binding problem. (a) The correct binding, (b) the greedy binding, (c) the result.
</figureCaption>
<bodyText confidence="0.999967642857143">
We define m[i, j] to be the penalty of the optimal sub-
set B* C {bi, ..., bj} E B, i &lt; j. m[i, j] is implemented
as a k x k lookup table, where k = |B |= |G||R|. Each
entry of this table is recursively defined by preceding ta-
ble entries. Specifically, m[i, j] is computed by perform-
ing exhaustive search on its subsets of bindings. Using
this lookup table, an optimal solution to Equation 5 is
therefore found as ψ(B*, G, R) = m[1, k]. Again as-
suming |B* |a |G |a |R|, the size of the lookup table is
given by O(|B*|4). Thus, it is possible to find the glob-
ally optimal set of bindings, by moving from an O(n3)
algorithm to O(n4). The precise definition of a recur-
rence relation for m[i, j] and a proof of correctness will
be described in a future publication.
</bodyText>
<subsectionHeader confidence="0.996135">
7.2 Syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.999886">
One obvious possibility for improvement would be to in-
clude more sophisticated syntactic information beyond
keyword spotting. However, we require that our system
remain robust to disfluency and recognition errors. Part
of speech tagging is a robust method of syntactic anal-
ysis which could allow us to refine the penalty function
depending on the usage case. Consider that there at least
three relevant uses of the keyword “this.”
</bodyText>
<listItem confidence="0.977045666666667">
1. This movie is better than A.I.
2. This is the bicycle ridden by E.T.
3. The wheel moves like this.
</listItem>
<bodyText confidence="0.99985925">
When “this” is followed by a noun (case 1), a deic-
tic gesture is likely, although not strictly necessary. But
when “this” is followed by a verb (case 2), a deictic
gesture is usually crucial for understanding the sentence.
Thus, the penalty for not assigning this keyword should
be very high. Finally, in the third case, when the keyword
follows a preposition, a trajectory gesture is more likely,
and the penalty for any such binding should be lowered.
</bodyText>
<subsectionHeader confidence="0.949999">
7.3 Other Anaphora Resolution Techniques
</subsectionHeader>
<bodyText confidence="0.999944272727273">
We have based this research on salience values, which
is just one of several possible alternative approaches to
anaphora resolution. One such alternative is the use of
constraints: rules that eliminate candidates from the list
of possible antecedents (Rich and Luperfoy, 1988). An
example of a constraint in anaphora resolution is a rule
requiring the elimination of all candidates that disagree
in gender or number with the referential pronoun. Con-
straints may be used in combination with a salience met-
ric, to prune away unlikely choices before searching.
The advantage is that enforcing constraints could be sub-
stantially less computationally expensive than searching
through the space of all possible bindings for the one with
the highest salience. One possible future project would be
to develop a set of constraints for speech-gesture align-
ment, and investigate the effect of these constraints on
both accuracy and speed.
Ge, Hale, and Charniak propose a data-driven ap-
proach to anaphora resolution (Ge et al., 1998). For a
given pronoun, their system can compute a probability
for each candidate antecedent. Their approach of seek-
ing to maximize this probability is similar to the salience-
maximizing approach that we have described. However,
instead of using a parametric salience function, they learn
a set of conditional probability distributions directly from
the data. If this approach could be applied to gesture-
speech alignment, it would be advantageous because the
binding probabilities could be combined with the output
of probabilistic recognizers to produce a pipeline archi-
tecture, similar to that proposed in (Wu et al., 1999). Such
an architecture would provide multimodal disambigua-
tion, where the errors of each component are corrected
by other components.
</bodyText>
<subsectionHeader confidence="0.974531">
7.4 Multimodal Adaptation
</subsectionHeader>
<bodyText confidence="0.999799545454546">
Speakers have remarkably entrenched multimodal com-
munication patterns, with some users overlapping ges-
ture and speech, and others using each modality sequen-
tially (Oviatt et al., 1997). Moreover, these multimodal
integration patterns do not seem to be malleable, sug-
gesting that multimodal user interfaces should adapt to
the user’s tendencies. We have already shown how the
weights of the salience metric can adapt for optimal per-
formance against a corpus of user data; this approach
could also be extended to adapt over time to an individual
user.
</bodyText>
<sectionHeader confidence="0.998807" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999985192307693">
This work represents one of the first efforts at aligning
gesture and speech on a corpus of natural multimodal
communication. Using greedy optimization and only a
minimum of linguistic processing, we significantly out-
perform a competitive baseline, which has actually been
implemented in existing multimodal user interfaces. Our
approach is shown to be robust to spoken English, even
with a high level of disfluency. By blending some of the
benefits of empirical and knowledge-based approaches,
our system can learn from a large corpus of data, but de-
grades gracefully when limited data is available.
Obviously, alignment is only one small component of
a comprehensive system for recognizing and understand-
ing multimodal communication. Putting aside the issue
of gesture recognition, there is still the problem of de-
riving semantic information from aligned speech-gesture
units. The solutions to this problem will likely have to be
specially tailored to the application domain. While our
evaluation indicates that our approach achieves what ap-
pears to be a high level of accuracy, the true test will be
whether our system can actually support semantic infor-
mation extraction from multimodal data. Only the con-
struction of such a comprehensive end-to-end system will
reveal whether the algorithm and features that we have
chosen are sufficient, or whether a more sophisticated ap-
proach is required.
</bodyText>
<sectionHeader confidence="0.994798" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.93205525">
We thank Robert Berwick, Michael Collins, Trevor Darrell,
Randall Davis, Tracy Hammond, Sanshzar Kettebekov, ¨Ozlem
Uzuner, and the anonymous reviewers for their helpful com-
ments on this paper.
</bodyText>
<sectionHeader confidence="0.998254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999967794520548">
Richard A. Bolt. 1980. Put-That-There: Voice and gesture at
the graphics interface. Computer Graphics, 14(3):262–270.
Justine Cassell. 1998. A framework for gesture generation and
interpretation. In Computer Vision in Human-Machine Inter-
action, pages 191–215. Cambridge University Press.
Joyce Y. Chai, Pengyu Hong, , and Michelle X. Zhou. 2004. A
probabilistic approach to reference resolution in multimodal
user interfaces. In Proceedings of 2004 International Con-
ference on Intelligent User Intefaces (IUI’04), pages 70–77.
Nicole Chovil. 1992. Discourse-oriented facial displays in con-
versation. Research on Language and Social Interaction,
25:163–194.
Philip R. Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman,
I. Smith, L. Chen, and J. Clow. 1997. Quickset: Multimodal
interaction for distributed applications. In ACM Multime-
dia’97, pages 31–40. ACM Press.
Philip R. Cohen, Rachel Coulston, and Kelly Krout. 2002.
Multimodal interaction during multiparty dialogues: Initial
results. In IEEE Conference on Multimodal Interfaces.
Jacob Eisenstein and Randall Davis. 2003. Natural gesture in
descriptive monologues. In UIST’03 Supplemental Proceed-
ings, pages 69–70.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical
approach to anaphora resolution. In Proceedings of the Sixth
Workshop on Very Large Corpora, pages 161–171.
David E. Goldberg. 1989. Genetic Algorithms in Search, Opti-
mization, and Machine Learning. Addison-Wesley.
Michael Johnston and Srinivas Bangalore. 2000. Finite-state
multimodal parsing and understanding. In Proceedings of
COLING-2000. ICCL.
Sanshzar Kettebekov and Rajeev Sharma. 2001. Toward natu-
ral gesture/speech control of a large display. In Engineering
for Human-Computer Interaction (EHCI’01). Lecture Notes
in Computer Science. Springer Verlag.
Sanshzar Kettebekov, Mohammed Yeasin, and Rajeev Sharma.
2002. Prosody based co-analysis for continuous recognition
of coverbal gestures. In International Conference on Mul-
timodal Interfaces (ICMI’02), pages 161–166, Pittsburgh,
USA.
David B. Koons, Carlton J. Sparrell, and Kristinn R. Thorisson.
1993. Integrating simultaneous input from speech, gaze, and
hand gestures. In Intelligent Multimedia Interfaces, pages
257–276. AAAI Press.
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Linguistics,
20(4):535–561.
David McNeill. 1992. Hand and Mind. The University of
Chicago Press.
Ruslan Mitkov, Richard Evans, and Constantin Or˘asan. 2002.
A new, fully automatic version of mitkov’s knowledge-poor
pronoun resolution method. In Intelligent Text Processing
and Computational Linguistics (CICLing’02), Mexico City,
Mexico, February, 17 – 23.
Ruslan Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In COLING-ACL, pages 869–875.
Sharon L. Oviatt, Antonella DeAngeli, and Karen Kuhn. 1997.
Integration and synchronization of input modes during mul-
timodal human-computer interaction. In Human Factors in
Computing Systems (CHI’97), pages 415–422. ACM Press.
Sharon L. Oviatt. 1999. Ten myths of multimodal interaction.
Communications oftheACM, 42(11):74–81.
Francis Quek, David McNeill, Robert Bryll, Susan Duncan,
Xin-Feng Ma, Cemil Kirbas, Karl E. McCullough, and
Rashid Ansari. 2002. Multimodal human discourse: gesture
and speech. Transactions on Computer-Human Interaction,
9(3):171–193.
Elaine Rich and Susann Luperfoy. 1988. An architecture for
anaphora resolution. In Proceedings of the Second Confer-
ence on Applied Natural Language Processing (ANLP-2),
pages 18–24, Texas, USA.
Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen. 1999.
Multimodal integration - a statistical view. IEEE Transac-
tions on Multimedia, 1(4):334–341.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920931">
<title confidence="0.999865">A Salience-Based Approach to Gesture-Speech Alignment</title>
<author confidence="0.999978">Jacob Eisenstein</author>
<author confidence="0.999978">C Mario</author>
<affiliation confidence="0.999634">MIT Computer Science and Artificial Intelligence</affiliation>
<address confidence="0.9888535">32 Vassar Cambridge, MA</address>
<abstract confidence="0.996509411764706">One of the first steps towards understanding natural multimodal language is aligning gesture and speech, so that the appropriate gestures ground referential pronouns in the speech. This paper presents a novel technique for gesture-speech alignment, inspired by saliencebased approaches to anaphoric pronoun resolution. We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus. Our system achieves 95% recall and precision on a corpus of transcriptions of unconstrained multimodal monologues, significantly outperforming a competitive baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard A Bolt</author>
</authors>
<title>Put-That-There: Voice and gesture at the graphics interface.</title>
<date>1980</date>
<journal>Computer Graphics,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="3563" citStr="Bolt, 1980" startWordPosition="542" endWordPosition="543">ing modalities; however, they also restrict communication to short grammatically-constrained commands over a very limited vocabulary. Since our goal is to handle more complex linguistic phenomena, these systems were of little help in the design of our algorithm. Conversely, we found that the problem of anaphora resolution faces a very similar set of challenges as gesture-speech alignment. We were able to apply techniques from anaphora resolution to gesture-speech alignment. 2.1 Multimodal User Interfaces Discussion of multimodal user interfaces begins with the seminal “Put-That-There” system (Bolt, 1980), which allowed users to issue natural language commands and use deictic hand gestures to resolve references from speech. Commands were subject to a strict grammar and alignment was straightforward: keywords created holes in the semantic frame, and temporally-aligned gestures filled the holes. More recent systems have extended this approach somewhat. Johnston and Bangalore describe a multimodal parsing algorithm that is built using a 3-tape, finite state transducer (FST) (Johnston and Bangalore, 2000). The speech and gestures of each multimodal utterance are provided as input to an FST whose o</context>
<context position="16915" citStr="Bolt, 1980" startWordPosition="2767" endWordPosition="2768">ons rather than speech and gesture recognizers will be discussed in detail below. 5.1 Empirical Results We averaged results over ten experiments, in which 20% of the data was selected randomly and held out as a test set. Entire transcripts were held out, rather than parts of each transcript. This was necessary because the system considers the entire transcript holistically when choosing an alignment. For a baseline, we evaluated the performance of choosing the temporally closest gesture to each keyword. While simplistic, this approach is used in several implemented multimodal user interfaces (Bolt, 1980; Koons et al., 1993). Kettebekov and Sharma even reported that 93.7% of gesture phrases were “temporally aligned” with the semantically associated keyword in their corpus (Kettebekov and Sharma, 2001). Our results with this baseline were somewhat lower, for reasons discussed below. Table 1 shows the results of our system and the baseline on our corpus. Our system significantly outperforms the baseline on both recall and precision on this corpus (p &lt; 0.05, two-tailed). Precision and recall differ slightly because there are keywords that do not bind to any gesture. Our system does not assume a </context>
</contexts>
<marker>Bolt, 1980</marker>
<rawString>Richard A. Bolt. 1980. Put-That-There: Voice and gesture at the graphics interface. Computer Graphics, 14(3):262–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
</authors>
<title>A framework for gesture generation and interpretation.</title>
<date>1998</date>
<booktitle>In Computer Vision in Human-Machine Interaction,</booktitle>
<pages>191--215</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8196" citStr="Cassell, 1998" startWordPosition="1271" endWordPosition="1273">(Oviatt et al., 1997; Cohen et al., 2002) • The gesture usually precedes the keyword (Oviatt et al., 1997). • A one-to-one mapping is preferred. Multiple keywords rarely align with a single gesture, and multiple gestures almost never align with a single keyword (Eisenstein and Davis, 2003). • Some types of gestures, such as deictic pointing gestures, are more likely to take part in keyword bindings. Other gestures (i.e., beats) do not carry this type of semantic content, and instead act to moderate turn taking or indicate emphasis. These gestures are unlikely to take part in keyword bindings (Cassell, 1998). • Some keyword/gesture combinations may be particularly likely; for example, the keyword “this” and a deictic pointing gesture. These rules mirror the salience weighting features employed by the anaphora resolution methods described in the previous section. We define a parameterizable penalty function that prefers alignments that adhere to as many of these rules as possible. Given a set of verbal utterances and gestures, we then try to find the set of bindings with the minimal penalty. This is essentially an optimization approach, and we use the simplest possible optimization technique: gree</context>
</contexts>
<marker>Cassell, 1998</marker>
<rawString>Justine Cassell. 1998. A framework for gesture generation and interpretation. In Computer Vision in Human-Machine Interaction, pages 191–215. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joyce Y Chai</author>
<author>Pengyu Hong</author>
</authors>
<title>A probabilistic approach to reference resolution in multimodal user interfaces.</title>
<date>2004</date>
<booktitle>In Proceedings of 2004 International Conference on Intelligent User Intefaces (IUI’04),</booktitle>
<pages>70--77</pages>
<marker>Chai, Hong, 2004</marker>
<rawString>Joyce Y. Chai, Pengyu Hong, , and Michelle X. Zhou. 2004. A probabilistic approach to reference resolution in multimodal user interfaces. In Proceedings of 2004 International Conference on Intelligent User Intefaces (IUI’04), pages 70–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicole Chovil</author>
</authors>
<title>Discourse-oriented facial displays in conversation.</title>
<date>1992</date>
<booktitle>Research on Language and Social Interaction,</booktitle>
<pages>25--163</pages>
<contexts>
<context position="1033" citStr="Chovil, 1992" startWordPosition="145" endWordPosition="146">er presents a novel technique for gesture-speech alignment, inspired by saliencebased approaches to anaphoric pronoun resolution. We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus. Our system achieves 95% recall and precision on a corpus of transcriptions of unconstrained multimodal monologues, significantly outperforming a competitive baseline. 1 Introduction In face to face communication, speakers frequently use gesture to supplement speech (Chovil, 1992), using the additional modality to provide unique, non-redundant information (McNeill, 1992). In the context of pen/speech user interfaces, Oviatt finds that “multimodal ... language is briefer, syntactically simpler, and less disfluent than users’ unimodal speech.” (Oviatt, 1999) One of the simplest and most direct ways in which gesture can supplement verbal communication is by grounding references, usually through deixis. For example, it is impossible to extract the semantic content of the verbal utterance “I’ll take this one” without an accompanying pointing gesture indicating the thing tha</context>
</contexts>
<marker>Chovil, 1992</marker>
<rawString>Nicole Chovil. 1992. Discourse-oriented facial displays in conversation. Research on Language and Social Interaction, 25:163–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>M Johnston</author>
<author>D McGee</author>
<author>S Oviatt</author>
<author>J Pittman</author>
<author>I Smith</author>
<author>L Chen</author>
<author>J Clow</author>
</authors>
<title>Quickset: Multimodal interaction for distributed applications.</title>
<date>1997</date>
<booktitle>In ACM Multimedia’97,</booktitle>
<pages>31--40</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="19321" citStr="Cohen et al., 1997" startWordPosition="3147" endWordPosition="3151">luded more disfluency. Consequently, alignment was more difficult, and a relatively naive strategy, such as the baseline algorithm, was less effective. 6 Discussion To our knowledge, very few multimodal understanding systems have been evaluated using natural, unconstrained speech and gesture. One exception is (Quek et al., 2002), which describes a system that extracts discourse structure from gesture on a corpus of unconstrained humanto-human communication; however, no quantitative analysis is provided. Of the systems that are more relevant to the specific problem of gesture-speech alignment (Cohen et al., 1997; Johnston and Bangalore, 2000; Kettebekov and Sharma, 2001), evaluation is always conducted from an HCI perspective, in which participants act as users of a computer system and communicate in short, grammatically-constrained multimodal commands. As shown in Section 5.1, such commands are significantly easier to align than the natural multimodal communication found in our corpus. 6.1 The Corpus A number of considerations went into gathering this corpus.&apos; One of our goals was to minimize the use of discourse-related “beat” gestures, so as to better focus on the deictic and iconic gestures that </context>
</contexts>
<marker>Cohen, Johnston, McGee, Oviatt, Pittman, Smith, Chen, Clow, 1997</marker>
<rawString>Philip R. Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman, I. Smith, L. Chen, and J. Clow. 1997. Quickset: Multimodal interaction for distributed applications. In ACM Multimedia’97, pages 31–40. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>Rachel Coulston</author>
<author>Kelly Krout</author>
</authors>
<title>Multimodal interaction during multiparty dialogues: Initial results.</title>
<date>2002</date>
<booktitle>In IEEE Conference on Multimodal Interfaces.</booktitle>
<contexts>
<context position="7623" citStr="Cohen et al., 2002" startWordPosition="1171" endWordPosition="1174">ng that the output of speech recognizers is far from perfect. Any approach that requires significant parsing or other grammatical analysis may be ill-suited to meet these goals. Instead, we identify keywords that are likely to require gestural referents for resolution. Our goal is to produce an alignment – a set of bindings – that match at least some of the identified keywords with one or more gestures. There are several things that are known to contribute to the salience of candidate gesture-speech bindings: • The relevant gesture is usually close in time to the keyword (Oviatt et al., 1997; Cohen et al., 2002) • The gesture usually precedes the keyword (Oviatt et al., 1997). • A one-to-one mapping is preferred. Multiple keywords rarely align with a single gesture, and multiple gestures almost never align with a single keyword (Eisenstein and Davis, 2003). • Some types of gestures, such as deictic pointing gestures, are more likely to take part in keyword bindings. Other gestures (i.e., beats) do not carry this type of semantic content, and instead act to moderate turn taking or indicate emphasis. These gestures are unlikely to take part in keyword bindings (Cassell, 1998). • Some keyword/gesture co</context>
</contexts>
<marker>Cohen, Coulston, Krout, 2002</marker>
<rawString>Philip R. Cohen, Rachel Coulston, and Kelly Krout. 2002. Multimodal interaction during multiparty dialogues: Initial results. In IEEE Conference on Multimodal Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Randall Davis</author>
</authors>
<title>Natural gesture in descriptive monologues.</title>
<date>2003</date>
<booktitle>In UIST’03 Supplemental Proceedings,</booktitle>
<pages>69--70</pages>
<contexts>
<context position="7872" citStr="Eisenstein and Davis, 2003" startWordPosition="1213" endWordPosition="1216">al referents for resolution. Our goal is to produce an alignment – a set of bindings – that match at least some of the identified keywords with one or more gestures. There are several things that are known to contribute to the salience of candidate gesture-speech bindings: • The relevant gesture is usually close in time to the keyword (Oviatt et al., 1997; Cohen et al., 2002) • The gesture usually precedes the keyword (Oviatt et al., 1997). • A one-to-one mapping is preferred. Multiple keywords rarely align with a single gesture, and multiple gestures almost never align with a single keyword (Eisenstein and Davis, 2003). • Some types of gestures, such as deictic pointing gestures, are more likely to take part in keyword bindings. Other gestures (i.e., beats) do not carry this type of semantic content, and instead act to moderate turn taking or indicate emphasis. These gestures are unlikely to take part in keyword bindings (Cassell, 1998). • Some keyword/gesture combinations may be particularly likely; for example, the keyword “this” and a deictic pointing gesture. These rules mirror the salience weighting features employed by the anaphora resolution methods described in the previous section. We define a para</context>
<context position="15111" citStr="Eisenstein and Davis, 2003" startWordPosition="2477" endWordPosition="2480">ation size of 200 was used, and training proceeded for 50 generations. Single-point crossover was applied at a rate of 90%, and the mutation rate was set to 3% per bit. Tournament selection was used rather than straightforward fitness-based selection (Goldberg, 1989). 5 Evaluation We evaluated our system by testing its performance on a set of 26 transcriptions of unconstrained humanto-human communication, from nine different speakBaseline Training Test Recall 84.2% 94.6% 95.1% Q n/a 1.2% 5.1% Precision 82.8% 94.5% 94.5% Q n/a 1.2% 5.0% Table 1: Performance of our system versus a baseline ers (Eisenstein and Davis, 2003). Of the four women and five men who participated, eight were right-handed, and one was a non-native English speaker. The participants ranged in age from 22 to 28. All had extensive computer experience, but none had any experience in the task domain, which required explaining the behavior of simple mechanical devices. The participants were presented with three conditions, each of which involved describing the operation of a mechanical device based on a computer simulation. The conditions were shown in order of increasing complexity, as measured by the number of moving parts: a latchbox, a pist</context>
</contexts>
<marker>Eisenstein, Davis, 2003</marker>
<rawString>Jacob Eisenstein and Randall Davis. 2003. Natural gesture in descriptive monologues. In UIST’03 Supplemental Proceedings, pages 69–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>161--171</pages>
<contexts>
<context position="30225" citStr="Ge et al., 1998" startWordPosition="4928" endWordPosition="4931">or number with the referential pronoun. Constraints may be used in combination with a salience metric, to prune away unlikely choices before searching. The advantage is that enforcing constraints could be substantially less computationally expensive than searching through the space of all possible bindings for the one with the highest salience. One possible future project would be to develop a set of constraints for speech-gesture alignment, and investigate the effect of these constraints on both accuracy and speed. Ge, Hale, and Charniak propose a data-driven approach to anaphora resolution (Ge et al., 1998). For a given pronoun, their system can compute a probability for each candidate antecedent. Their approach of seeking to maximize this probability is similar to the saliencemaximizing approach that we have described. However, instead of using a parametric salience function, they learn a set of conditional probability distributions directly from the data. If this approach could be applied to gesturespeech alignment, it would be advantageous because the binding probabilities could be combined with the output of probabilistic recognizers to produce a pipeline architecture, similar to that propos</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 161–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Goldberg</author>
</authors>
<date>1989</date>
<booktitle>Genetic Algorithms in Search, Optimization, and Machine Learning.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="14751" citStr="Goldberg, 1989" startWordPosition="2420" endWordPosition="2422">hes in both accuracy and rate of convergence. The genome representation consisted of a thirteen bit string for each penalty parameter; three bits were used for the exponent, and the remaining ten were used for the base. Parameters were allowed to vary from 10−4 to 103. Since there were eleven parameters, the overall string length was 143. A population size of 200 was used, and training proceeded for 50 generations. Single-point crossover was applied at a rate of 90%, and the mutation rate was set to 3% per bit. Tournament selection was used rather than straightforward fitness-based selection (Goldberg, 1989). 5 Evaluation We evaluated our system by testing its performance on a set of 26 transcriptions of unconstrained humanto-human communication, from nine different speakBaseline Training Test Recall 84.2% 94.6% 95.1% Q n/a 1.2% 5.1% Precision 82.8% 94.5% 94.5% Q n/a 1.2% 5.0% Table 1: Performance of our system versus a baseline ers (Eisenstein and Davis, 2003). Of the four women and five men who participated, eight were right-handed, and one was a non-native English speaker. The participants ranged in age from 22 to 28. All had extensive computer experience, but none had any experience in the ta</context>
</contexts>
<marker>Goldberg, 1989</marker>
<rawString>David E. Goldberg. 1989. Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Finite-state multimodal parsing and understanding.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000. ICCL.</booktitle>
<contexts>
<context position="4069" citStr="Johnston and Bangalore, 2000" startWordPosition="615" endWordPosition="618">modal User Interfaces Discussion of multimodal user interfaces begins with the seminal “Put-That-There” system (Bolt, 1980), which allowed users to issue natural language commands and use deictic hand gestures to resolve references from speech. Commands were subject to a strict grammar and alignment was straightforward: keywords created holes in the semantic frame, and temporally-aligned gestures filled the holes. More recent systems have extended this approach somewhat. Johnston and Bangalore describe a multimodal parsing algorithm that is built using a 3-tape, finite state transducer (FST) (Johnston and Bangalore, 2000). The speech and gestures of each multimodal utterance are provided as input to an FST whose output is a semantic representation conveying the combined meaning. A similar system, based on a graph-matching algorithm, is described in (Chai et al., 2004). These systems perform mutual disambiguation, where each modality helps to correct errors in the others. However, both approaches restrict users to a predefined grammar and lexicon, and rely heavily on having a complete, formal ontology of the domain. In (Kettebekov et al., 2002), a co-occurrence model relates the salient prosodic features of the</context>
<context position="19351" citStr="Johnston and Bangalore, 2000" startWordPosition="3152" endWordPosition="3155">y. Consequently, alignment was more difficult, and a relatively naive strategy, such as the baseline algorithm, was less effective. 6 Discussion To our knowledge, very few multimodal understanding systems have been evaluated using natural, unconstrained speech and gesture. One exception is (Quek et al., 2002), which describes a system that extracts discourse structure from gesture on a corpus of unconstrained humanto-human communication; however, no quantitative analysis is provided. Of the systems that are more relevant to the specific problem of gesture-speech alignment (Cohen et al., 1997; Johnston and Bangalore, 2000; Kettebekov and Sharma, 2001), evaluation is always conducted from an HCI perspective, in which participants act as users of a computer system and communicate in short, grammatically-constrained multimodal commands. As shown in Section 5.1, such commands are significantly easier to align than the natural multimodal communication found in our corpus. 6.1 The Corpus A number of considerations went into gathering this corpus.&apos; One of our goals was to minimize the use of discourse-related “beat” gestures, so as to better focus on the deictic and iconic gestures that are more closely related to th</context>
</contexts>
<marker>Johnston, Bangalore, 2000</marker>
<rawString>Michael Johnston and Srinivas Bangalore. 2000. Finite-state multimodal parsing and understanding. In Proceedings of COLING-2000. ICCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanshzar Kettebekov</author>
<author>Rajeev Sharma</author>
</authors>
<title>Toward natural gesture/speech control of a large display.</title>
<date>2001</date>
<booktitle>In Engineering for Human-Computer Interaction (EHCI’01). Lecture Notes in Computer Science.</booktitle>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="17116" citStr="Kettebekov and Sharma, 2001" startWordPosition="2794" endWordPosition="2798"> randomly and held out as a test set. Entire transcripts were held out, rather than parts of each transcript. This was necessary because the system considers the entire transcript holistically when choosing an alignment. For a baseline, we evaluated the performance of choosing the temporally closest gesture to each keyword. While simplistic, this approach is used in several implemented multimodal user interfaces (Bolt, 1980; Koons et al., 1993). Kettebekov and Sharma even reported that 93.7% of gesture phrases were “temporally aligned” with the semantically associated keyword in their corpus (Kettebekov and Sharma, 2001). Our results with this baseline were somewhat lower, for reasons discussed below. Table 1 shows the results of our system and the baseline on our corpus. Our system significantly outperforms the baseline on both recall and precision on this corpus (p &lt; 0.05, two-tailed). Precision and recall differ slightly because there are keywords that do not bind to any gesture. Our system does not assume a one-to-one mapping between keywords and gestures, and will refuse to bind some keywords if there is no gesture with a high enough salience. One benefit of our penalty-based approach is that it allows u</context>
<context position="19381" citStr="Kettebekov and Sharma, 2001" startWordPosition="3156" endWordPosition="3160"> more difficult, and a relatively naive strategy, such as the baseline algorithm, was less effective. 6 Discussion To our knowledge, very few multimodal understanding systems have been evaluated using natural, unconstrained speech and gesture. One exception is (Quek et al., 2002), which describes a system that extracts discourse structure from gesture on a corpus of unconstrained humanto-human communication; however, no quantitative analysis is provided. Of the systems that are more relevant to the specific problem of gesture-speech alignment (Cohen et al., 1997; Johnston and Bangalore, 2000; Kettebekov and Sharma, 2001), evaluation is always conducted from an HCI perspective, in which participants act as users of a computer system and communicate in short, grammatically-constrained multimodal commands. As shown in Section 5.1, such commands are significantly easier to align than the natural multimodal communication found in our corpus. 6.1 The Corpus A number of considerations went into gathering this corpus.&apos; One of our goals was to minimize the use of discourse-related “beat” gestures, so as to better focus on the deictic and iconic gestures that are more closely related to the content of the speech; that </context>
</contexts>
<marker>Kettebekov, Sharma, 2001</marker>
<rawString>Sanshzar Kettebekov and Rajeev Sharma. 2001. Toward natural gesture/speech control of a large display. In Engineering for Human-Computer Interaction (EHCI’01). Lecture Notes in Computer Science. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanshzar Kettebekov</author>
<author>Mohammed Yeasin</author>
<author>Rajeev Sharma</author>
</authors>
<title>Prosody based co-analysis for continuous recognition of coverbal gestures.</title>
<date>2002</date>
<booktitle>In International Conference on Multimodal Interfaces (ICMI’02),</booktitle>
<pages>161--166</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="4601" citStr="Kettebekov et al., 2002" startWordPosition="700" endWordPosition="703">m that is built using a 3-tape, finite state transducer (FST) (Johnston and Bangalore, 2000). The speech and gestures of each multimodal utterance are provided as input to an FST whose output is a semantic representation conveying the combined meaning. A similar system, based on a graph-matching algorithm, is described in (Chai et al., 2004). These systems perform mutual disambiguation, where each modality helps to correct errors in the others. However, both approaches restrict users to a predefined grammar and lexicon, and rely heavily on having a complete, formal ontology of the domain. In (Kettebekov et al., 2002), a co-occurrence model relates the salient prosodic features of the speech (pitch variation and pause) to characteristic features of gesticulation (velocity and acceleration). The goal was to improve performance of gesture recognition, rather than to address the problem of alignment directly. Their approach also differs from ours in that they operate at the level of speech signals, rather than recognized words. Potentially, the two approaches could compliment each other in a unified system. 2.2 Anaphora Resolution Anaphora resolution involves linking an anaphor to its corresponding antecedent</context>
</contexts>
<marker>Kettebekov, Yeasin, Sharma, 2002</marker>
<rawString>Sanshzar Kettebekov, Mohammed Yeasin, and Rajeev Sharma. 2002. Prosody based co-analysis for continuous recognition of coverbal gestures. In International Conference on Multimodal Interfaces (ICMI’02), pages 161–166, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David B Koons</author>
<author>Carlton J Sparrell</author>
<author>Kristinn R Thorisson</author>
</authors>
<title>Integrating simultaneous input from speech, gaze, and hand gestures.</title>
<date>1993</date>
<booktitle>In Intelligent Multimedia Interfaces,</booktitle>
<pages>257--276</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="16936" citStr="Koons et al., 1993" startWordPosition="2769" endWordPosition="2772">han speech and gesture recognizers will be discussed in detail below. 5.1 Empirical Results We averaged results over ten experiments, in which 20% of the data was selected randomly and held out as a test set. Entire transcripts were held out, rather than parts of each transcript. This was necessary because the system considers the entire transcript holistically when choosing an alignment. For a baseline, we evaluated the performance of choosing the temporally closest gesture to each keyword. While simplistic, this approach is used in several implemented multimodal user interfaces (Bolt, 1980; Koons et al., 1993). Kettebekov and Sharma even reported that 93.7% of gesture phrases were “temporally aligned” with the semantically associated keyword in their corpus (Kettebekov and Sharma, 2001). Our results with this baseline were somewhat lower, for reasons discussed below. Table 1 shows the results of our system and the baseline on our corpus. Our system significantly outperforms the baseline on both recall and precision on this corpus (p &lt; 0.05, two-tailed). Precision and recall differ slightly because there are keywords that do not bind to any gesture. Our system does not assume a one-to-one mapping be</context>
</contexts>
<marker>Koons, Sparrell, Thorisson, 1993</marker>
<rawString>David B. Koons, Carlton J. Sparrell, and Kristinn R. Thorisson. 1993. Integrating simultaneous input from speech, gaze, and hand gestures. In Intelligent Multimedia Interfaces, pages 257–276. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="5633" citStr="Lappin and Leass, 1994" startWordPosition="861" endWordPosition="864">words. Potentially, the two approaches could compliment each other in a unified system. 2.2 Anaphora Resolution Anaphora resolution involves linking an anaphor to its corresponding antecedent in the same or previous sentence. In many cases, speech/gesture multimodal fusion works in a very similar way, with gestures grounding some of the same anaphoric pronouns (e.g., “this”, “that”, “here”). One approach to anaphora resolution is to assign a salience value to each noun phrase that is a candidate for acting as a grounding referent, and then to choose the noun phrase with the greatest salience (Lappin and Leass, 1994). Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998). Salience values are typically computed by applying linguistic knowledge; e.g., recent noun phrases are more salient, gender and number should agree, etc. This knowledge is applied to derive a salience value through the application of a set of predefined salience weights on each feature. Salience weights may be defined by hand, as in (Lappin and Leass, 1994), or learned from data (Mitkov et al., 2002). Anaphora resolution and gesture-speech </context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McNeill</author>
</authors>
<title>Hand and Mind.</title>
<date>1992</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1125" citStr="McNeill, 1992" startWordPosition="157" endWordPosition="158">aches to anaphoric pronoun resolution. We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus. Our system achieves 95% recall and precision on a corpus of transcriptions of unconstrained multimodal monologues, significantly outperforming a competitive baseline. 1 Introduction In face to face communication, speakers frequently use gesture to supplement speech (Chovil, 1992), using the additional modality to provide unique, non-redundant information (McNeill, 1992). In the context of pen/speech user interfaces, Oviatt finds that “multimodal ... language is briefer, syntactically simpler, and less disfluent than users’ unimodal speech.” (Oviatt, 1999) One of the simplest and most direct ways in which gesture can supplement verbal communication is by grounding references, usually through deixis. For example, it is impossible to extract the semantic content of the verbal utterance “I’ll take this one” without an accompanying pointing gesture indicating the thing that is desired. The problem of gesture-speech alignment involves choosing the appropriate gest</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>David McNeill. 1992. Hand and Mind. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Richard Evans</author>
<author>Constantin Or˘asan</author>
</authors>
<title>A new, fully automatic version of mitkov’s knowledge-poor pronoun resolution method.</title>
<date>2002</date>
<booktitle>In Intelligent Text Processing and Computational Linguistics (CICLing’02),</booktitle>
<pages>23</pages>
<location>Mexico City, Mexico,</location>
<marker>Mitkov, Evans, Or˘asan, 2002</marker>
<rawString>Ruslan Mitkov, Richard Evans, and Constantin Or˘asan. 2002. A new, fully automatic version of mitkov’s knowledge-poor pronoun resolution method. In Intelligent Text Processing and Computational Linguistics (CICLing’02), Mexico City, Mexico, February, 17 – 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Robust pronoun resolution with limited knowledge.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>869--875</pages>
<contexts>
<context position="5787" citStr="Mitkov, 1998" startWordPosition="886" endWordPosition="887">ts corresponding antecedent in the same or previous sentence. In many cases, speech/gesture multimodal fusion works in a very similar way, with gestures grounding some of the same anaphoric pronouns (e.g., “this”, “that”, “here”). One approach to anaphora resolution is to assign a salience value to each noun phrase that is a candidate for acting as a grounding referent, and then to choose the noun phrase with the greatest salience (Lappin and Leass, 1994). Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998). Salience values are typically computed by applying linguistic knowledge; e.g., recent noun phrases are more salient, gender and number should agree, etc. This knowledge is applied to derive a salience value through the application of a set of predefined salience weights on each feature. Salience weights may be defined by hand, as in (Lappin and Leass, 1994), or learned from data (Mitkov et al., 2002). Anaphora resolution and gesture-speech alignment are very similar problems. Both involve resolving ambiguous words which reference other parts of the utterance. In the case of anaphora resoluti</context>
</contexts>
<marker>Mitkov, 1998</marker>
<rawString>Ruslan Mitkov. 1998. Robust pronoun resolution with limited knowledge. In COLING-ACL, pages 869–875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon L Oviatt</author>
<author>Antonella DeAngeli</author>
<author>Karen Kuhn</author>
</authors>
<title>Integration and synchronization of input modes during multimodal human-computer interaction.</title>
<date>1997</date>
<booktitle>In Human Factors in Computing Systems (CHI’97),</booktitle>
<pages>415--422</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="7602" citStr="Oviatt et al., 1997" startWordPosition="1167" endWordPosition="1170">ematic when considering that the output of speech recognizers is far from perfect. Any approach that requires significant parsing or other grammatical analysis may be ill-suited to meet these goals. Instead, we identify keywords that are likely to require gestural referents for resolution. Our goal is to produce an alignment – a set of bindings – that match at least some of the identified keywords with one or more gestures. There are several things that are known to contribute to the salience of candidate gesture-speech bindings: • The relevant gesture is usually close in time to the keyword (Oviatt et al., 1997; Cohen et al., 2002) • The gesture usually precedes the keyword (Oviatt et al., 1997). • A one-to-one mapping is preferred. Multiple keywords rarely align with a single gesture, and multiple gestures almost never align with a single keyword (Eisenstein and Davis, 2003). • Some types of gestures, such as deictic pointing gestures, are more likely to take part in keyword bindings. Other gestures (i.e., beats) do not carry this type of semantic content, and instead act to moderate turn taking or indicate emphasis. These gestures are unlikely to take part in keyword bindings (Cassell, 1998). • So</context>
<context position="31192" citStr="Oviatt et al., 1997" startWordPosition="5073" endWordPosition="5076"> the data. If this approach could be applied to gesturespeech alignment, it would be advantageous because the binding probabilities could be combined with the output of probabilistic recognizers to produce a pipeline architecture, similar to that proposed in (Wu et al., 1999). Such an architecture would provide multimodal disambiguation, where the errors of each component are corrected by other components. 7.4 Multimodal Adaptation Speakers have remarkably entrenched multimodal communication patterns, with some users overlapping gesture and speech, and others using each modality sequentially (Oviatt et al., 1997). Moreover, these multimodal integration patterns do not seem to be malleable, suggesting that multimodal user interfaces should adapt to the user’s tendencies. We have already shown how the weights of the salience metric can adapt for optimal performance against a corpus of user data; this approach could also be extended to adapt over time to an individual user. 8 Conclusions This work represents one of the first efforts at aligning gesture and speech on a corpus of natural multimodal communication. Using greedy optimization and only a minimum of linguistic processing, we significantly outper</context>
</contexts>
<marker>Oviatt, DeAngeli, Kuhn, 1997</marker>
<rawString>Sharon L. Oviatt, Antonella DeAngeli, and Karen Kuhn. 1997. Integration and synchronization of input modes during multimodal human-computer interaction. In Human Factors in Computing Systems (CHI’97), pages 415–422. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon L Oviatt</author>
</authors>
<title>Ten myths of multimodal interaction.</title>
<date>1999</date>
<journal>Communications oftheACM,</journal>
<volume>42</volume>
<issue>11</issue>
<contexts>
<context position="1314" citStr="Oviatt, 1999" startWordPosition="183" endWordPosition="184">ience weights themselves are learned from a corpus. Our system achieves 95% recall and precision on a corpus of transcriptions of unconstrained multimodal monologues, significantly outperforming a competitive baseline. 1 Introduction In face to face communication, speakers frequently use gesture to supplement speech (Chovil, 1992), using the additional modality to provide unique, non-redundant information (McNeill, 1992). In the context of pen/speech user interfaces, Oviatt finds that “multimodal ... language is briefer, syntactically simpler, and less disfluent than users’ unimodal speech.” (Oviatt, 1999) One of the simplest and most direct ways in which gesture can supplement verbal communication is by grounding references, usually through deixis. For example, it is impossible to extract the semantic content of the verbal utterance “I’ll take this one” without an accompanying pointing gesture indicating the thing that is desired. The problem of gesture-speech alignment involves choosing the appropriate gesture to ground each verbal utterance. This paper describes a novel technique for this problem. We evaluate our system on a corpus of multimodal monologues with no fixed grammar or vocabulary</context>
</contexts>
<marker>Oviatt, 1999</marker>
<rawString>Sharon L. Oviatt. 1999. Ten myths of multimodal interaction. Communications oftheACM, 42(11):74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Quek</author>
<author>David McNeill</author>
<author>Robert Bryll</author>
<author>Susan Duncan</author>
<author>Xin-Feng Ma</author>
<author>Cemil Kirbas</author>
<author>Karl E McCullough</author>
<author>Rashid Ansari</author>
</authors>
<title>Multimodal human discourse: gesture and speech.</title>
<date>2002</date>
<journal>Transactions on Computer-Human Interaction,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="19033" citStr="Quek et al., 2002" startWordPosition="3103" endWordPosition="3106">e and monologue length. This may help to explain why Kettebekov and Sharma found such success with the baseline algorithm. The multimodal utterances in their corpus consisted of relatively short commands. The longer monologues in our corpus tended to be more grammatically complex and included more disfluency. Consequently, alignment was more difficult, and a relatively naive strategy, such as the baseline algorithm, was less effective. 6 Discussion To our knowledge, very few multimodal understanding systems have been evaluated using natural, unconstrained speech and gesture. One exception is (Quek et al., 2002), which describes a system that extracts discourse structure from gesture on a corpus of unconstrained humanto-human communication; however, no quantitative analysis is provided. Of the systems that are more relevant to the specific problem of gesture-speech alignment (Cohen et al., 1997; Johnston and Bangalore, 2000; Kettebekov and Sharma, 2001), evaluation is always conducted from an HCI perspective, in which participants act as users of a computer system and communicate in short, grammatically-constrained multimodal commands. As shown in Section 5.1, such commands are significantly easier t</context>
</contexts>
<marker>Quek, McNeill, Bryll, Duncan, Ma, Kirbas, McCullough, Ansari, 2002</marker>
<rawString>Francis Quek, David McNeill, Robert Bryll, Susan Duncan, Xin-Feng Ma, Cemil Kirbas, Karl E. McCullough, and Rashid Ansari. 2002. Multimodal human discourse: gesture and speech. Transactions on Computer-Human Interaction, 9(3):171–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elaine Rich</author>
<author>Susann Luperfoy</author>
</authors>
<title>An architecture for anaphora resolution.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-2),</booktitle>
<pages>18--24</pages>
<location>Texas, USA.</location>
<contexts>
<context position="29479" citStr="Rich and Luperfoy, 1988" startWordPosition="4809" endWordPosition="4812">se 2), a deictic gesture is usually crucial for understanding the sentence. Thus, the penalty for not assigning this keyword should be very high. Finally, in the third case, when the keyword follows a preposition, a trajectory gesture is more likely, and the penalty for any such binding should be lowered. 7.3 Other Anaphora Resolution Techniques We have based this research on salience values, which is just one of several possible alternative approaches to anaphora resolution. One such alternative is the use of constraints: rules that eliminate candidates from the list of possible antecedents (Rich and Luperfoy, 1988). An example of a constraint in anaphora resolution is a rule requiring the elimination of all candidates that disagree in gender or number with the referential pronoun. Constraints may be used in combination with a salience metric, to prune away unlikely choices before searching. The advantage is that enforcing constraints could be substantially less computationally expensive than searching through the space of all possible bindings for the one with the highest salience. One possible future project would be to develop a set of constraints for speech-gesture alignment, and investigate the effe</context>
</contexts>
<marker>Rich, Luperfoy, 1988</marker>
<rawString>Elaine Rich and Susann Luperfoy. 1988. An architecture for anaphora resolution. In Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-2), pages 18–24, Texas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lizhong Wu</author>
<author>Sharon L Oviatt</author>
<author>Philip R Cohen</author>
</authors>
<title>Multimodal integration - a statistical view.</title>
<date>1999</date>
<journal>IEEE Transactions on Multimedia,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="30848" citStr="Wu et al., 1999" startWordPosition="5024" endWordPosition="5027"> given pronoun, their system can compute a probability for each candidate antecedent. Their approach of seeking to maximize this probability is similar to the saliencemaximizing approach that we have described. However, instead of using a parametric salience function, they learn a set of conditional probability distributions directly from the data. If this approach could be applied to gesturespeech alignment, it would be advantageous because the binding probabilities could be combined with the output of probabilistic recognizers to produce a pipeline architecture, similar to that proposed in (Wu et al., 1999). Such an architecture would provide multimodal disambiguation, where the errors of each component are corrected by other components. 7.4 Multimodal Adaptation Speakers have remarkably entrenched multimodal communication patterns, with some users overlapping gesture and speech, and others using each modality sequentially (Oviatt et al., 1997). Moreover, these multimodal integration patterns do not seem to be malleable, suggesting that multimodal user interfaces should adapt to the user’s tendencies. We have already shown how the weights of the salience metric can adapt for optimal performance </context>
</contexts>
<marker>Wu, Oviatt, Cohen, 1999</marker>
<rawString>Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen. 1999. Multimodal integration - a statistical view. IEEE Transactions on Multimedia, 1(4):334–341.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>