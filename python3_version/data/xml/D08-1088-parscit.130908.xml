<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992689">
Complexity of finding the BLEU-optimal hypothesis in a confusion network
</title>
<author confidence="0.976202">
Gregor Leusch and Evgeny Matusov and Hermann Ney
</author>
<affiliation confidence="0.973689">
RWTH Aachen University, Germany
</affiliation>
<email confidence="0.994864">
{leusch,matusov,ney}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.99469" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838714285714">
Confusion networks are a simple representa-
tion of multiple speech recognition or transla-
tion hypotheses in a machine translation sys-
tem. A typical operation on a confusion net-
work is to find the path which minimizes or
maximizes a certain evaluation metric. In this
article, we show that this problem is gener-
ally NP-hard for the popular BLEU metric,
as well as for smaller variants of BLEU. This
also holds for more complex representations
like generic word graphs. In addition, we give
an efficient polynomial-time algorithm to cal-
culate unigram BLEU on confusion networks,
but show that even small generalizations of
this data structure render the problem to be
NP-hard again.
Since finding the optimal solution is thus not
always feasible, we introduce an approximat-
ing algorithm based on a multi-stack decoder,
which finds a (not necessarily optimal) solu-
tion for n-gram BLEU in polynomial time.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999806333333334">
In machine translation (MT), confusion networks
(CNs) are commonly used to represent alternative
versions of sentences. Typical applications include
translation of different speech recognition hypothe-
ses (Bertoldi et al., 2007) or system combination
(Fiscus, 1997; Matusov et al., 2006).
A typical operation on a given CN is to find the
path which minimizes or maximizes a certain eval-
uation metric. This operation can be used in ap-
plications like Minimum Error Rate Training (Och,
2003), or optimizing system combination as de-
scribed by Hillard et al. (2007). Whereas this is
easily achievable for simple metrics like the Word
Error Rate (WER) as described by Mohri and Riley
(2002), current research in MT uses more sophisti-
cated measures, like the BLEU score (Papineni et
al., 2001). Zens and Ney (2005) first described this
task on general word graphs, and sketched a com-
plete algorithm for calculating the maximum BLEU
score in a word graph. While they do not give an
estimate on the complexity of their algorithm, they
note that already a simpler algorithm for calculating
the Position independent Error Rate (PER) has an
exponential worst-case complexity. The same can
be expected for their BLEU algorithm. Dreyer et
al (2007) examined a special class of word graphs,
namely those that denote constrained reorderings of
single sentences. These word graphs have some
properties which simplify the calculation; for exam-
ple, no edge is labeled with the empty word, and
all paths have the same length and end in the same
node. Even then, their decoder does not optimize
the true BLEU score, but an approximate version
which uses a language-model-like unmodified pre-
cision. We give a very short introduction to CNs and
the BLEU score in Section 2.
In Section 3 we show that finding the best BLEU
score is an NP-hard problem, even for a simplified
variant of BLEU which only scores unigrams and
bigrams. The main reason for this problem to be-
come NP-hard is that by looking at bigrams, we al-
low for one decision to also influence the following
decision, which itself can influence the decisions af-
ter that. We also show that this also holds for uni-
gram BLEU and the position independent error rate
(PER) on a slightly augmented variant of CNs which
allows for edges to carry multiple symbols. The con-
catenation of symbols corresponds to the interde-
pendency of decisions in the case of bigram matches
above.
NP-hard problems are quite common in machine
</bodyText>
<page confidence="0.981645">
839
</page>
<note confidence="0.962106">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 839–847,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999802766666666">
translation; for example, Knight (1999) has shown
that even for a simple form of statistical MT mod-
els, the decoding problem is NP-complete. More
recently, DeNero and Klein (2008) have proven the
NP-completeness of the phrase alignment problem.
But even a simple, common procedure as BLEU
scoring, which can be performed in linear time on
single sentences, becomes a potentially intractable
problem as soon as it has to be performed on a
slightly more powerful representation, such as con-
fusion networks. This rather surprising result is the
motivation of this paper.
The problem of finding the best unigram BLEU
score in an unaugmented variant of CNs is not NP-
complete, as we show in Section 4. We present an
algorithm that finds such a unigram BLEU-best path
in polynomial time.
An important corollary of this work is that calcu-
lating the BLEU-best path on general word graphs
is also NP-complete, as CNs are a true subclass
of word graphs. It is still desirable to calculate a
“good” path in terms of the BLEU score in a CN,
even if calculating the best path is infeasible. In Sec-
tion 5, we present an algorithm which can calculate
“good” solutions for CNs in polynomial time. This
algorithm can easily be extended to handle arbitrary
word graphs. We assess the algorithm experimen-
tally on real-world MT data in Section 6, and draw
some conclusions from the results in this article in
Section 7.
</bodyText>
<sectionHeader confidence="0.942602" genericHeader="introduction">
2 Confusion networks
</sectionHeader>
<bodyText confidence="0.999874">
A confusion network (CN) is a word graph where
each edge is labeled with exactly zero or one sym-
bol, and each path from the start node to the end
node visits each node of the graph in canonical or-
der. Usually, we represent unlabeled edges by label-
ing them with the empty word E.
Within this paper, we represent a CN by a list of
lists of words {wz,�}, where each wz,� corresponds
to a symbol on an edge between nodes i and i + 1.
A path in this CN can be written as a string of inte-
gers, an 1 = a1, ... , an, such that the path is labeled
w1,a1w2,a2 . . . wn,an. Note that there can be a differ-
ent number of possible words, j, for different posi-
tions i.
</bodyText>
<subsectionHeader confidence="0.961828">
2.1 BLEU and variants
</subsectionHeader>
<bodyText confidence="0.99959419047619">
The BLEU score, as defined by Papineni et al.
(2001), is the modified n-gram precision of a hy-
pothesis, with 1 &lt; n &lt; N, given a set of reference
translations R. “Modified precision” here means
that for each n-gram, its maximum number of oc-
currences within the reference sentences is counted,
and only up to that many occurrences in the hypothe-
sis are considered to be correct. The geometric mean
over the precisions for all n is calculated, and mul-
tiplied by a brevity penalty bp. This brevity penalty
is 1.0 if the hypothesis sentence is at least as long as
the reference sentence (special cases occur if multi-
ple reference sentences with different length exists),
and less than 1.0 otherwise. The exact formulation
can be found in the cited paper; for the proofs in
our paper it is enough to note that the BLEU score
is 1.0 exactly if all n-grams in the hypothesis oc-
cur at least that many times in a reference sentence,
and if there is a reference sentence which is as long
as or shorter than the hypothesis. Assuming that
we can always provide a dummy reference sentence
shorter than this length, we do not need to regard
the brevity penalty in these proofs. Within the fol-
lowing proofs of NP-hardness, we will only require
confusion networks (and word graphs) which do not
contain empty words, and where all paths from the
start node to the end node have the same length.
Usually, in the definition of the BLEU score, N is
set to 4; within this article we denote this metric as
4BLEU. We can also restrict the calculations to un-
igrams only, which would be 1BLEU, or to bigrams
and unigrams, which we denote as 2BLEU.
Similar to the 1BLEU metric is the Position in-
dependent Error Rate PER (Tillmann et al., 1997),
which counts the number of substitutions, insertions,
and deletions that have to be performed on the uni-
gram counts to have the hypothesis counts match the
reference counts. Unlike 1BLEU, for PER to be op-
timal (here, 0.0), the reference counts must match
the candidate counts exactly.
Given a CN {wz,�} and a set of reference sen-
tences R, we define the optimization problem
</bodyText>
<construct confidence="0.474458285714286">
Definition 1(CN-2BLEU-OPTIMIZE) Among
all paths ai through the CN, what is the path with
the highest 2BLEU score?
Related to this is the decision problem
Definition 2 (CN-2BLEU-DECIDE) Among all
paths ai through the CN, is there a path with a
2BLEU score of 1.0?
</construct>
<bodyText confidence="0.5971135">
Similarly we define CN-4BLEU-DECIDE, CN-
PER-DECIDE, etc.
</bodyText>
<page confidence="0.993873">
840
</page>
<sectionHeader confidence="0.884833" genericHeader="method">
3 CN-2BLEU-DECIDE is NP-complete
</sectionHeader>
<bodyText confidence="0.999985888888889">
We now show that CN-2BLEU-DECIDE is NP-
complete. It is obvious that the problem is in NP:
Given a path aI1, which is polynomial in size to the
problem, we can decide in polynomial time whether
aI1 is a solution to the problem – namely by calcu-
lating the BLEU score. We now show that there is
a problem known to be NP-complete which can be
polynomially reduced to CN-2BLEU-DECIDE. For
our proof, we choose 3SAT.
</bodyText>
<subsectionHeader confidence="0.998559">
3.1 3SAT
</subsectionHeader>
<bodyText confidence="0.954252214285714">
Consider the following problem:
Definition 3 (3SAT) Let X = {x1, ... , xn}
be a set of Boolean variables, let F =
^k i=1 (Li,1∨Li,2∨Li,3) be a Boolean formula,
where each literal Li,j is either a variable x or its
negate x. Is there a assignment Q : X → {0, 1}
such that Q |= F? In other words, if we replace
each x in F by Q(x), and each x by 1 − Q(x), does
F become true?
It has been shown by Karp (1972) that 3SAT is
NP-complete. Consequently, if for another problem
in NP there is polynomial-size and -time reduction
of an arbitrary instance of 3SAT to an instance of this
new problem, this new problem is also NP-complete.
</bodyText>
<subsectionHeader confidence="0.973309">
3.2 Reduction of 3SAT to
CN-2BLEU-DECIDE
</subsectionHeader>
<bodyText confidence="0.9976704375">
Let F be a Boolean formula in 3CNF, and let k be
its size, as in Definition 3. We will now reduce it to a
corresponding CN-2BLEU-DECIDE problem. This
means that we create an alphabet E, a confusion net-
work C, and a set of reference sentences R, such that
there is a path through C with a BLEU score of 1.0
exactly if F is solvable:
Create an alphabet E based on F as E :=
{x1, ... , xn} ∪ {x1, ... , xn} ∪ {o}. Here, the xi
and xi symbols will correspond to the variable with
the same name or their negate, respectively, whereas
O will serve as an “isolator symbol”, to avoid un-
wanted bigram matches or mismatches between sep-
arate parts of the constructed CN or sentences.
Consider the CN C from Figure 1.
Consider the following set of reference sentences:
</bodyText>
<equation confidence="0.966598">
R := { O (x1O)k(x2O)k ... (xnO)k
O (x1O)k(x2O)k ... (xnO)k,
(x1)k O (x1)k O ... (xn)k O (xn)k O (0)k+n }
</equation>
<bodyText confidence="0.98209025">
where (x)k denotes k subsequent occurrences of x.
Clearly, both C and R are of polynomial size in n
and k, and can be constructed in polynomial time.
Then,
There is an assignment Q such that Q |= F
⇔
There is a path aI1 through C such that
BLEU(aI1, R) = 1.0.
</bodyText>
<subsectionHeader confidence="0.306458">
Proof: “⇒”
</subsectionHeader>
<bodyText confidence="0.9934706">
Let Q be an assignment under which F becomes
true. Create a path aI1 as follows: Within A, for
each set of edges Li,1, Li,2, Li,3, choose the path
through an x where Q(x) = 1, or through an x
where Q(x) = 0. Note that there must be such an
x, because otherwise the clause Li,1 ∨ Li,2 ∨ Li,3
would not be true under Q. Within B, select the path
always through xi if Q(xi) = 0, and through xi if
Q(xi) = 1.
Then, aI1 consists of, for each i,
</bodyText>
<listItem confidence="0.999616">
• At most k occurrences of both xi and xi
• At most k occurrences of each of the bigrams
xio, oxi, xio, oxi, xixi, and xixi
• No other bigram than those listed above.
</listItem>
<bodyText confidence="0.998127611111111">
For all of these unigram and bigram counts, there is
a reference sentence in R which contains at least as
many of those unigrams/bigrams as the path. Thus,
the unigram and bigram precision of aI1 is 1.0. In ad-
dition, there is always a reference sentence whose
length is shorter than that of aI1, such that the brevity
penalty is also 1.0. As a result, BLEU(aI1, R) =
1.0.
“⇐”
Let aI1 be a path through C such that
BLEU(aI1, R) = 1.0. Because there is no bi-
gram xixi or xixi in R, we can assume that for each
xi, either only xi edges, or only xi edges appear
in the B part of aI1, each at most k times. As no
unigram xi and xi appears more than k times in R,
we can assume that, if the xi edges are passed in
B, then only the xi edges are passed in A, and vice
versa. Now, create an assignment Q as follows:
</bodyText>
<equation confidence="0.9502245">
�
0 if xi edges are passed in B
Q :=
1 otherwise
</equation>
<bodyText confidence="0.9942395">
Then, Q |= F. Proof: Assume that Fβ = 0. Then
there must be a clause i such that Li,1 ∨Li,2 ∨Li,3 =
</bodyText>
<page confidence="0.966745">
841
</page>
<figure confidence="0.9161095">
C := A•0 B
�
</figure>
<figureCaption confidence="0.9904385">
Figure 1: CN constructed from a 3SAT formula F. C is the concatenation of the left part A, and the right path B,
separated by an isolating o.
</figureCaption>
<equation confidence="0.784003206896551">
L2,1
L1,1
O
� �
.O ···.O
� � �
� �
� �
� �
.o � �
� �
L1,2
� �
� �
� �
L2,2
Lk,1
Lk,2
A := •
L1,3
L2,3
Lk,3
x1
x1
x1
O
O
B:= •
··· .
� �
� �
� �
� �
� �
� �
� �
� �
� �
� �
· · ·
· · ·
� � � �
O · · ·
� � � �
� �
x1
x2
xn
 |{z }
k times
x2
 |{z }
k
 |{z }
k
xn
x1
x1
</equation>
<bodyText confidence="0.8896131">
0. At least one of the edges Li,j associated with the
literals of this clause must have been passed by aK1
in A. This literal, though, can not have been passed
in B. As a consequence, β(Li,j) = 1. But this
means that Li,1 ∨ Li,2 ∨ Li,3 = 1 . &gt; contra-
diction.
Because CN-2BLEU-DECIDE is in NP, and we
can reduce an NP-complete problem (3SAT) in poly-
nomial time to a CN-2BLEU-DECIDE problem, this
means that CN-2BLEU-DECIDE is NP-complete.
</bodyText>
<subsectionHeader confidence="0.998712">
3.3 CN-4BLEU-DECIDE
</subsectionHeader>
<bodyText confidence="0.9998725">
It is straightforward to modify the construction
above to create an equivalent CN-4BLEU-DECIDE
problem instead: Replace each occurrence of the
isolating symbol O in A, B, C, R by three consecu-
tive isolating symbols 000. Then, everything said
about unigrams still holds, and bi-, tri- and four-
grams are handled equivalently: Previous unigram
matches on � correspond to uni-, bi-, and trigram
matches on o, oo, 000. Bigram matches on x0 corre-
spond to bi-, tri-, and fourgram matches on xo, xo0,
x~~~, and similar holds for bigram matches xo, ox,
ox. Unigram matches x, x, and bigram matches
xx etc. stay the same. Consequently, CN-4BLEU-
DECIDE is also an NP-complete problem.
</bodyText>
<subsectionHeader confidence="0.989844">
3.4 CN*-1BLEU-DECIDE
</subsectionHeader>
<bodyText confidence="0.995903166666667">
Is it possible to get rid of the necessity for bi-
gram counts in this proof? One possibility might be
to look at slightly more powerful graph structures,
CN*. In these graphs, each edge can be labeled
by arbitrarily many symbols (instead of just zero or
one). Then, consider a CN* graph C&apos; := A• 0 B&apos;,
</bodyText>
<figureCaption confidence="0.988683">
Figure 2: Right part of a CN* constructed from a 3SAT
formula F.
</figureCaption>
<bodyText confidence="0.960863">
with B&apos; as in Figure 2.
With
</bodyText>
<equation confidence="0.994424">
R&apos; := {(x1)k(x1)k ... (xn)k(xn)k(0)k}
</equation>
<bodyText confidence="0.999155071428571">
we can again assume that either xi or xi ap-
pears k times in the B&apos;-part of a path aK1 with
1BLEU(aK1 , R&apos;) = 1.0, and that for every solution
β to F there is a corresponding path aK1 through C&apos;
and vice versa. In this construction, we also have
exact matches of the counts, so we can also use PER
in the decision problem.
While CN* are generally not word graphs by
themselves due to the multiple symbols on edges,
it is straightforward to create an equivalent word
graph from a given CN*, as demonstrated in Fig-
ure 3. Consequently, deciding unigram BLEU and
unigram PER are NP-complete problems for general
word graphs as well.
</bodyText>
<sectionHeader confidence="0.887922" genericHeader="method">
4 Solving CN-1BLEU-DECIDE in
polynomial time
</sectionHeader>
<bodyText confidence="0.9996315">
It is not a coincidence that we had to resort to
bigrams or to edges with multiple symbols for
NP-completeness: It turns out that CN-1BLEU-
DECIDE, where the order of the words does not
</bodyText>
<figure confidence="0.5472011875">
x1
x1
� �O · · ·
� �
� �
� �
� �
� �
O
� � � �
(xn)k
� �
� �
· · · � �
(x1)k
B&apos; := -
�
 |{z }
ε
k times
(x1)k
(xn)k
842
the
that
on
� �
� �
C :=
E time
R := { on the same day,
at the time and the day}
</figure>
<figureCaption confidence="0.998481">
Figure 4: Example for CN-1BLEU-DECIDE.
</figureCaption>
<figure confidence="0.913760571428571">
� �
� �
� �
� �
� �
� �
at
very day
x1 x1 x1
• v
k times
� �
� �
� �
</figure>
<equation confidence="0.96206416">
� � � �
�
WG: · · · �
· · ·
� �
� �
� �
� � � · · ·
� �
� �
� � � �
� �
� � � �
� �
(x1)k
(x1)k
� �
� �
�
CN*: · · · �
� � � · · ·
� �
� �
�
x1 x1 x1
</equation>
<figureCaption confidence="0.9444655">
Figure 3: Construction of a word graph from a CN* as in
B1.
</figureCaption>
<bodyText confidence="0.962174333333333">
matter at all, can be decided in polynomial time
using the following algorithm, which disregards a
brevity penalty for the sake of simplicity:
Given a vocabulary X, a CN {wi,j}, and a set of
reference sentences R together with their unigram
BLEU counts c(x) : X → N and C := Excx c(x),
</bodyText>
<listItem confidence="0.88840037037037">
1. Remove all parts from w where there is an edge
labeled with the empty word E. This step will
always increase unigram precision, and can not
hurt any higher n-gram precision here, because
n = 1. In the example in Figure 4, the edges
labeled very and E respectively are affected in
this step.
2. Create nodes A0 := {1, ... , n}, one for each
node with edges in the CN. In the example in
Figure 5, the three leftmost column heads cor-
respond to these nodes.
3. Create nodes B := {x.j  |x ∈ X,1≤j≤c(x)}.
In other words, create a unique node for each
“running” word in R – e.g. if the first and
second reference sentence contain x once each,
and the third reference contains x twice, create
exactly x.1 and x.2. In Figure 5, those are the
row heads to the right.
4. Fill A with empty nodes to match the total
length: A := A0 ∪ {E.j |1 ≤ j ≤ C − n}.
If n &gt; C, the BLEU precision can not be 1.0.
The five rightmost columns in Figure 5 corre-
spond to those.
5. Create edges
E:={(i,wi,j.k) |1≤i≤n, allj,1≤c(wi,j)}
∪ {(i, E.j) |1 ≤ i ≤ n, all j}. These edges are
denoted as ◦ or • in Figure 5.
</listItem>
<figure confidence="0.735441333333333">
1 2 3 E.1 E.2 E.3 E.� E.5
• ◦ ◦ ◦ ◦ ◦ on
• ◦ ◦ ◦ ◦ ◦ the.1
◦ • ◦ ◦ ◦ ◦ the.2
◦ • ◦ ◦ ◦ same
• ◦ ◦ ◦ ◦ ◦ day
◦ ◦ ◦ • ◦ ◦ at
◦ ◦ ◦ ◦ • ◦ time
◦ ◦ ◦ ◦ • and
</figure>
<figureCaption confidence="0.929497666666667">
Figure 5: Bipartite graph constructed to find the optimal
1BLEU path in Figure 4. One possible maximum bipar-
tite matching is marked with •.
</figureCaption>
<listItem confidence="0.991071285714286">
6. Find the maximum bipartite matching M be-
tween A and B given E. Figure 5 shows such
a matching with •.
7. If all nodes in A and B are covered by M,
then 1BLEU({wi,j}, R) = 1.0. The words that
are matched to A0 then form the solution path
through {wi,j}.
</listItem>
<bodyText confidence="0.894172125">
Figure 4 gives an example of a CN and a set of ref-
erences R, for which the best 1BLEU path can be
constructed by the algorithm above. The bipartite
graph constructed in Step 1 to Step 4 for this exam-
ple, given in matrix form, can be found in Figure 5.
Such a solution to Step 6, if found, corresponds
exactly to a path through the confusion network with
1BLEU=1.0, and vice versa: for each position 1 ≤
i ≤ n, the matched word corresponds to the word
that is selected for the position of the path; “surplus”
counts are matched with Es.
Step 6 can be performed in polynomial time
(Hopcroft and Karp, 1973) O((C + n)5/2); all other
steps in linear time O(C + n). Consequently, CN-
1BLEU can be decided in polynomial time O((C +
n)5/2). Similarly, an actual optimum 1BLEU score
</bodyText>
<page confidence="0.997917">
843
</page>
<bodyText confidence="0.990754777777778">
can be calculated in O((C + n)5/2).
It should be noted that the only alterations in the
hypothesis length, and as a result the only alterations
in the brevity penalty, will come from Step 1. Con-
sequently, the brevity penalty can be taken into ac-
count as follows: Consider that there are M nodes
with an empty edge in {wjjI. Instead of remov-
ing them in Step 1, keep them in, but for each
1 &lt; m &lt; M, run through steps 2 to 6, but add
m nodes e.1, ... , e.m to B in Step 3, and add corre-
sponding edges to these nodes to E in Step 5. After
each iteration (which leads to a constant hypothesis
length), calculate precision and brevity penalty. Se-
lect the best product of precision and brevity penalty
in the end. The overall time complexity now is in
M · O((C + n)5/2).
A PER score can be calculated in a similar fash-
ion.
</bodyText>
<sectionHeader confidence="0.8496605" genericHeader="method">
5 Finding approximating solutions for
CN-4BLEU in polynomial time
</sectionHeader>
<bodyText confidence="0.999952025316456">
Knowing that the problem of finding the BLEU-best
path is an NP-complete problem is an unsatisfactory
answer in practice – in many cases, having a good,
but not necessarily optimum path is preferable to
having no good path at all.
A simple approach would be to walk the CN from
the start node to the end node, keeping track of n-
grams visited so far, and choosing the word next
which maximizes the n-gram precision up to this
word. Track is kept by keeping n-gram count vec-
tors for the hypothesis path and the reference sen-
tences, and update those in each step.
The main problem with this approach is that of-
ten the local optimum is suboptimal on the global
scale, for example if a word occurs on a later posi-
tion again.
Zens and Ney (2005) on the other hand propose
to keep all n-gram count vectors instead, and only
recombine path hypotheses with identical count vec-
tors. As they suspect, the search space can become
exponentially large.
In this paper, we suggest a compromise between
these two extremes, namely keeping active a suffi-
ciently large number of “path hypotheses” in terms
of n-gram precision, instead of only the first best,
or of all. But even then, edges with empty words
pose a problem, as stepping along an empty edge
will never decrease the precision of the local path.
In certain cases, steps along empty edges may affect
the n-gram precision for higher n-grams. But this
will only take effect after the next non-empty step, it
does not influence the local decision in a node. Step-
ping along a non-empty edge will often decrease the
local precision, though. As a consequence, a simple
algorithm will prefer paths with shorter hypotheses,
which leads to a suboptimal total BLEU score, be-
cause of the brevity penalty. One can counter this
problem for example by using a brevity penalty al-
ready during the search. But this is problematic as
well, because it is difficult to define a proper partial
reference length in this case.
The approach we propose is to compare only par-
tial path hypotheses with the same number of empty
edges, and ending in the same position in the confu-
sion network. This idea is illustrated in Figure 6: We
compare only the partial precision of path hypothe-
ses ending in the same node. Due to the simple na-
ture of this search graph, it can easily be traversed in
a left-to-right, top-to-bottom manner. With regard to
a node currently being expanded, only the next node
in the same row, and the corresponding columns in
the next row need to be kept active. When imple-
menting this algorithm, Hypotheses should be com-
pared on the modified BLEUS precision by Lin and
Och (2004) because the original BLEU precision
equals zero as long as there are no higher n-gram
matches in the partial hypotheses, which renders
meaningful comparison hard or impossible.
In the rightmost column, all path hypotheses
within a node have the same hypothesis length. Con-
sequently, we can select the hypothesis with the best
(brevity-penalized) BLEU score by multiplying the
appropriate brevity penalty to the precision of the
best path ending in each of these nodes. If we al-
ways expand all possible path hypotheses within the
nodes, and basically run a full search, we will al-
ways find the BLEU-best path this way. From the
proof above, it follows that the number of path hy-
pothesis we would have to keep can become expo-
nentially large. Fortunately, if a “good” solution is
good enough, we do not have to keep all possible
path hypotheses, but only the 5 best ones for a given
constant 5, or those with a precision not worse than
c times the precision of the best hypothesis within
the node. Assuming that adding and removing an
element to/from a size-limited stack of size 5 takes
time O(log 5), that we allow at most E empty edges
in a solution, and that there are j edges in each of the
n positions, this algorithm has a time complexity of
</bodyText>
<page confidence="0.99799">
844
</page>
<figureCaption confidence="0.9988206">
Figure 6: Principle of the multi-stack decoder used to find
a path with a good BLEU score. The first row shows
the original confusion network, the following rows show
the search graph. Duplicate edges were removed, but no
word was considered “unknown”.
</figureCaption>
<equation confidence="0.3595">
O(E · n · j · Slog S).
</equation>
<bodyText confidence="0.9989284">
To reduce redundant duplicated path hypotheses,
and by this to speed up the algorithm and reduce the
risk that good path hypotheses are pruned, the confu-
sion network should be simplified before the search,
as shown in Figure 6:
</bodyText>
<listItem confidence="0.8653243">
1. Remove all words in the CN which do not ap-
pear in any reference sentence, if there at least
one “known” non-empty word at the same po-
sition. If there is no such “known” word, re-
place them all by a single token denoting the
“unknown word”.
2. Remove all duplicate edges in a position, that
is, if there are two or more edges carrying the
same label in one position, remove all but one
for them.
</listItem>
<bodyText confidence="0.999771">
These two steps will keep at least one of the BLEU-
best paths intact. But they can remove the average
branching factor (j) of the CN significantly, which
leads to a significantly lower number of duplicate
path hypotheses during the search.
</bodyText>
<tableCaption confidence="0.9963">
Table 1: Statistics of the (Chinese–)English MT corpora
used for the experiments
</tableCaption>
<table confidence="0.999547454545455">
NIST NIST
2003 2006
number of systems 4 4
number of ref. 4 4 per sent.
sentences 919 249
system length 28.4 33.2 words*
ref. length 27.5 34.2 words*
best path 24.4 33.9 words*
CN length 40.7 39.5 nodes*
best single system 29.3 52.5 BLEU
30.5 51.6 BLEUS*
</table>
<tableCaption confidence="0.376868">
*average per sentence
</tableCaption>
<bodyText confidence="0.99987224">
Our algorithm can easily be extended to handle ar-
bitrary word graphs instead of confusion networks.
In this case, each “row” in Figure 6 will reflect the
structure of the word graph instead of the “linear”
structure of the CN.
While this algorithm searches for the best path
for a single sentence only, a common task is to
find the best BLEU score over a whole test set –
which can mean suboptimal BLEU scores for in-
dividual sentences. This adds an additional com-
binatorial problem over the sentences to the actual
decoding process. Both Zens and Ney (2005) and
Dreyer et al (2007) use a greedy approach here; the
latter estimated the impact of this to be insignifi-
cant in random sampling experiments. In our exper-
iments, we used the per-sentence BLEUS score as
(greedy) decision criterion, as this is also the prun-
ing criterion. One possibility to adapt this approach
to Zens’s/Dreyer’s greedy approach for system-level
BLEU scores might be to initialize n-gram counts
and hypothesis length not to zero at the beginning
of each sentence, but to those of the corpus so far.
But as this diverts from our goal to optimize the
sentence-level scores, we have not implemented it
so far.
</bodyText>
<sectionHeader confidence="0.993027" genericHeader="method">
6 Experimental assessment of the
algorithm
</sectionHeader>
<bodyText confidence="0.99991975">
The question arises how many path hypotheses we
need to retain in each step to obtain optimal paths.
To examine this, we created confusion networks out
of the translations of the four best MT systems of
</bodyText>
<page confidence="0.99234">
845
</page>
<figure confidence="0.991536">
5 10 15 20
# path hyps
</figure>
<figureCaption confidence="0.99952775">
Figure 7: Average of the sentence-wise BLEU and
BLEUS score and the system-wide BLEU score versus
the number of path hypotheses kept per node during the
search. NIST MT03 corpus.
</figureCaption>
<figure confidence="0.9935535">
5 10 15 20 25
# path hyps
</figure>
<figureCaption confidence="0.99947625">
Figure 8: Average of the sentence-wise BLEU and
BLEUS score and the system-wide BLEU score versus
the number of path hypotheses kept per node during the
search. NIST MT06 corpus.
</figureCaption>
<figure confidence="0.9825185625">
●
●
●
sys−BLEU
avg. seg−BLEUS
avg. seg−BLEU
●
●
●
sys−BLEU
avg. seg−BLEUS
avg. seg−BLEU
BLEU, BLEUS
0.32 0.34 0.36 0.38 0.40
0.65 0.67 0.69 0.71
BLEU, BLEUS
</figure>
<bodyText confidence="0.992592954545455">
the NIST 2003 and 2006 Chinese–English evalu-
ation campaigns, as available from the Linguistic
Data Consortium (LDC). The hypotheses of the best
single system served as skeleton, those of the three
remaining systems were reordered and aligned to the
skeleton hypothesis. This corpus is described in Ta-
ble 1. Figures 7 and 8 show the measured BLEU
scores in three different definitions, versus the max-
imum number of path hypotheses that are kept in
each node of the search graph. Shown are the av-
erage sentence-wise BLEUS score, which is what
the algorithm actually optimizes, for comparison the
average sentence-wise BLEU score, and the total
document-wise BLEU score.
All scores increase with increasing number of re-
tained hypotheses, but stabilize around a total of 15
hypotheses per node. The difference over a greedy
approach, which corresponds to a maximum of one
hypothesis per node if we leave out the separation by
path length, is quite significant. No further improve-
ments can be expected for a higher number of hy-
potheses, as experiments up to 100 hypotheses show.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999970517241379">
In this paper, we showed that deciding whether a
given CN contains a path with a BLEU score of 1.0
is an NP-complete problem for n-gram lengths &gt; 2.
The problem is also NP-complete if we only look
at unigram BLEU, but allow for CNs where edges
may contain multiple symbols, or for arbitrary word
graphs. As a corollary, any proposed algorithm to
find the path with an optimal BLEU score in a CN,
even more in an arbitrary word graph, which runs
in worst case polynomial time can only deliver an
approximation1.
We gave an efficient polynomial time algorithm
for the simplest variant, namely deciding on a uni-
gram BLEU score for a CN. This algorithm can eas-
ily be modified to decide on the PER score as well,
or to calculate an actual unigram BLEU score for the
hypothesis CN.
Comparing these results, we conclude that the
ability to take bi- or higher n-grams into account,
be it in the scoring (as in 2BLEU), or in the graph
structure (as in CN*), is the key to render the prob-
lem NP-hard. Doing so creates long-range depen-
dencies, which oppose local decisions.
We also gave an efficient approximating algo-
rithm for higher-order BLEU scores. This algorithm
is based on a multi-stack decoder, taking into ac-
count the empty arcs within a path. Experimental
results on real-world data show that our method is
indeed able to find paths with a significantly better
</bodyText>
<footnote confidence="0.88166">
1provided that P # NP, of course.
</footnote>
<page confidence="0.997631">
846
</page>
<bodyText confidence="0.9999106">
BLEU score than that of a greedy search. The re-
sulting BLEUS score stabilizes already on a quite
restricted search space, showing that despite the
proven NP-hardness of the exact problem, our al-
gorithm can give useful approximations in reason-
able time. It is yet an open problem in how far
the problems of finding the best paths regarding a
sentence-level BLEU score, and regarding a system-
level BLEU score correlate. Our experiments here
suggest a good correspondence.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993895">
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
The proofs and algorithms in this paper emerged
while the first author was visiting researcher at
the Interactive Language Technologies Group of
the National Research Council (NRC) of Canada,
Gatineau. The author wishes to thank NRC and
Aachen University for the opportunity to jointly
work on this project.
</bodyText>
<sectionHeader confidence="0.998739" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918039473684">
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 1297–1300,
Honululu, HI, USA, April.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In Human Language
Technologies 2008: The Conference of the Association
for Computational Linguistics, Short Papers, pages
25–28, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing Reordering Constraints for SMT
Using Efficient BLEU Oracle Computation. In AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation (SSST) at the Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT), pages 103–110,
Rochester, NY, USA, April.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recogniser output vot-
ing error reduction (ROVER). In Proceedings 1997
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347–352, Santa Barbara, CA.
Dustin Hillard, Bj¨orn Hoffmeister, Mari Ostendorf, Ralf
Schl¨uter, and Hermann Ney. 2007. iROVER: Improv-
ing system combination with classification. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 65–68, Rochester, New York, April.
John E. Hopcroft and Richard M. Karp. 1973. An n5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on Computing, 2(4):225–231.
Richard M. Karp. 1972. Reducibility among combina-
torial problems. In R. E. Miller and J. W. Thatcher,
editors, Complexity of Computer Computations, pages
85–103. Plenum Press.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607–615, December.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluation automatic evaluation metrics for
machine translation. In Proc. COLING 2004, pages
501–507, Geneva, Switzerland, August.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 33–40, Trento, Italy, April.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proc. of
the 7th Int. Conf. on Spoken Language Processing (IC-
SLP’02), pages 1313–1316, Denver, CO, September.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the 41th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 160–167, Sapporo, Japan,
July.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for auto-
matic evaluation of machine translation. Technical Re-
port RC22176 (W0109-022), IBM Research Division,
Thomas J. Watson Research Center, September.
Christoph Tillmann, Stephan Vogel, Hermann Ney, Alex
Zubiaga, and Hassan Sawaf. 1997. Accelerated
DP based search for statistical translation. In Euro-
pean Conf. on Speech Communication and Technol-
ogy, pages 2667–2670, Rhodes, Greece, September.
Richard Zens and Hermann Ney. 2005. Word graphs
for statistical machine translation. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond, pages
191–198, Ann Arbor, MI, June.
</reference>
<page confidence="0.998225">
847
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741813">
<title confidence="0.978889">Complexity of finding the BLEU-optimal hypothesis in a confusion network</title>
<author confidence="0.914841">Leusch Matusov</author>
<affiliation confidence="0.978804">RWTH Aachen University,</affiliation>
<abstract confidence="0.991904681818182">Confusion networks are a simple representation of multiple speech recognition or translation hypotheses in a machine translation system. A typical operation on a confusion network is to find the path which minimizes or maximizes a certain evaluation metric. In this article, we show that this problem is generally NP-hard for the popular BLEU metric, as well as for smaller variants of BLEU. This also holds for more complex representations like generic word graphs. In addition, we give an efficient polynomial-time algorithm to calculate unigram BLEU on confusion networks, but show that even small generalizations of this data structure render the problem to be NP-hard again. Since finding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solufor BLEU in polynomial time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Richard Zens</author>
<author>Marcello Federico</author>
</authors>
<title>Speech translation by confusion network decoding.</title>
<date>2007</date>
<booktitle>In IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>1297--1300</pages>
<location>Honululu, HI, USA,</location>
<contexts>
<context position="1345" citStr="Bertoldi et al., 2007" startWordPosition="198" endWordPosition="201">rithm to calculate unigram BLEU on confusion networks, but show that even small generalizations of this data structure render the problem to be NP-hard again. Since finding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solution for n-gram BLEU in polynomial time. 1 Introduction In machine translation (MT), confusion networks (CNs) are commonly used to represent alternative versions of sentences. Typical applications include translation of different speech recognition hypotheses (Bertoldi et al., 2007) or system combination (Fiscus, 1997; Matusov et al., 2006). A typical operation on a given CN is to find the path which minimizes or maximizes a certain evaluation metric. This operation can be used in applications like Minimum Error Rate Training (Och, 2003), or optimizing system combination as described by Hillard et al. (2007). Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by Mohri and Riley (2002), current research in MT uses more sophisticated measures, like the BLEU score (Papineni et al., 2001). Zens and Ney (2005) first described this</context>
</contexts>
<marker>Bertoldi, Zens, Federico, 2007</marker>
<rawString>Nicola Bertoldi, Richard Zens, and Marcello Federico. 2007. Speech translation by confusion network decoding. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 1297–1300, Honululu, HI, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Human Language Technologies 2008: The Conference of the Association for Computational Linguistics, Short Papers,</booktitle>
<pages>25--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3929" citStr="DeNero and Klein (2008)" startWordPosition="629" endWordPosition="632">error rate (PER) on a slightly augmented variant of CNs which allows for edges to carry multiple symbols. The concatenation of symbols corresponds to the interdependency of decisions in the case of bigram matches above. NP-hard problems are quite common in machine 839 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 839–847, Honolulu, October 2008.c�2008 Association for Computational Linguistics translation; for example, Knight (1999) has shown that even for a simple form of statistical MT models, the decoding problem is NP-complete. More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. But even a simple, common procedure as BLEU scoring, which can be performed in linear time on single sentences, becomes a potentially intractable problem as soon as it has to be performed on a slightly more powerful representation, such as confusion networks. This rather surprising result is the motivation of this paper. The problem of finding the best unigram BLEU score in an unaugmented variant of CNs is not NPcomplete, as we show in Section 4. We present an algorithm that finds such a unigram BLEU-best path in polynomial time</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In Human Language Technologies 2008: The Conference of the Association for Computational Linguistics, Short Papers, pages 25–28, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Keith Hall</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation.</title>
<date>2007</date>
<booktitle>In AMTA Workshop on Syntax and Structure in Statistical Translation (SSST) at the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT),</booktitle>
<pages>103--110</pages>
<location>Rochester, NY, USA,</location>
<contexts>
<context position="2351" citStr="Dreyer et al (2007)" startWordPosition="367" endWordPosition="370">like the Word Error Rate (WER) as described by Mohri and Riley (2002), current research in MT uses more sophisticated measures, like the BLEU score (Papineni et al., 2001). Zens and Ney (2005) first described this task on general word graphs, and sketched a complete algorithm for calculating the maximum BLEU score in a word graph. While they do not give an estimate on the complexity of their algorithm, they note that already a simpler algorithm for calculating the Position independent Error Rate (PER) has an exponential worst-case complexity. The same can be expected for their BLEU algorithm. Dreyer et al (2007) examined a special class of word graphs, namely those that denote constrained reorderings of single sentences. These word graphs have some properties which simplify the calculation; for example, no edge is labeled with the empty word, and all paths have the same length and end in the same node. Even then, their decoder does not optimize the true BLEU score, but an approximate version which uses a language-model-like unmodified precision. We give a very short introduction to CNs and the BLEU score in Section 2. In Section 3 we show that finding the best BLEU score is an NP-hard problem, even f</context>
<context position="24985" citStr="Dreyer et al (2007)" startWordPosition="4722" endWordPosition="4725">0.5 51.6 BLEUS* *average per sentence Our algorithm can easily be extended to handle arbitrary word graphs instead of confusion networks. In this case, each “row” in Figure 6 will reflect the structure of the word graph instead of the “linear” structure of the CN. While this algorithm searches for the best path for a single sentence only, a common task is to find the best BLEU score over a whole test set – which can mean suboptimal BLEU scores for individual sentences. This adds an additional combinatorial problem over the sentences to the actual decoding process. Both Zens and Ney (2005) and Dreyer et al (2007) use a greedy approach here; the latter estimated the impact of this to be insignificant in random sampling experiments. In our experiments, we used the per-sentence BLEUS score as (greedy) decision criterion, as this is also the pruning criterion. One possibility to adapt this approach to Zens’s/Dreyer’s greedy approach for system-level BLEU scores might be to initialize n-gram counts and hypothesis length not to zero at the beginning of each sentence, but to those of the corpus so far. But as this diverts from our goal to optimize the sentence-level scores, we have not implemented it so far.</context>
</contexts>
<marker>Dreyer, Hall, Khudanpur, 2007</marker>
<rawString>Markus Dreyer, Keith Hall, and Sanjeev Khudanpur. 2007. Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation. In AMTA Workshop on Syntax and Structure in Statistical Translation (SSST) at the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 103–110, Rochester, NY, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recogniser output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In Proceedings</booktitle>
<pages>347--352</pages>
<location>Santa Barbara, CA.</location>
<contexts>
<context position="1381" citStr="Fiscus, 1997" startWordPosition="205" endWordPosition="206">etworks, but show that even small generalizations of this data structure render the problem to be NP-hard again. Since finding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solution for n-gram BLEU in polynomial time. 1 Introduction In machine translation (MT), confusion networks (CNs) are commonly used to represent alternative versions of sentences. Typical applications include translation of different speech recognition hypotheses (Bertoldi et al., 2007) or system combination (Fiscus, 1997; Matusov et al., 2006). A typical operation on a given CN is to find the path which minimizes or maximizes a certain evaluation metric. This operation can be used in applications like Minimum Error Rate Training (Och, 2003), or optimizing system combination as described by Hillard et al. (2007). Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by Mohri and Riley (2002), current research in MT uses more sophisticated measures, like the BLEU score (Papineni et al., 2001). Zens and Ney (2005) first described this task on general word graphs, and sk</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recogniser output voting error reduction (ROVER). In Proceedings 1997 IEEE Workshop on Automatic Speech Recognition and Understanding, pages 347–352, Santa Barbara, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Hillard</author>
<author>Bj¨orn Hoffmeister</author>
<author>Mari Ostendorf</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>iROVER: Improving system combination with classification.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers,</booktitle>
<pages>65--68</pages>
<location>Rochester, New York,</location>
<marker>Hillard, Hoffmeister, Ostendorf, Schl¨uter, Ney, 2007</marker>
<rawString>Dustin Hillard, Bj¨orn Hoffmeister, Mari Ostendorf, Ralf Schl¨uter, and Hermann Ney. 2007. iROVER: Improving system combination with classification. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 65–68, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hopcroft</author>
<author>Richard M Karp</author>
</authors>
<title>An n5/2 algorithm for maximum matchings in bipartite graphs.</title>
<date>1973</date>
<journal>SIAM Journal on Computing,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="17985" citStr="Hopcroft and Karp, 1973" startWordPosition="3459" endWordPosition="3462">ath through {wi,j}. Figure 4 gives an example of a CN and a set of references R, for which the best 1BLEU path can be constructed by the algorithm above. The bipartite graph constructed in Step 1 to Step 4 for this example, given in matrix form, can be found in Figure 5. Such a solution to Step 6, if found, corresponds exactly to a path through the confusion network with 1BLEU=1.0, and vice versa: for each position 1 ≤ i ≤ n, the matched word corresponds to the word that is selected for the position of the path; “surplus” counts are matched with Es. Step 6 can be performed in polynomial time (Hopcroft and Karp, 1973) O((C + n)5/2); all other steps in linear time O(C + n). Consequently, CN1BLEU can be decided in polynomial time O((C + n)5/2). Similarly, an actual optimum 1BLEU score 843 can be calculated in O((C + n)5/2). It should be noted that the only alterations in the hypothesis length, and as a result the only alterations in the brevity penalty, will come from Step 1. Consequently, the brevity penalty can be taken into account as follows: Consider that there are M nodes with an empty edge in {wjjI. Instead of removing them in Step 1, keep them in, but for each 1 &lt; m &lt; M, run through steps 2 to 6, but</context>
</contexts>
<marker>Hopcroft, Karp, 1973</marker>
<rawString>John E. Hopcroft and Richard M. Karp. 1973. An n5/2 algorithm for maximum matchings in bipartite graphs. SIAM Journal on Computing, 2(4):225–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard M Karp</author>
</authors>
<title>Reducibility among combinatorial problems.</title>
<date>1972</date>
<journal>Complexity of Computer Computations,</journal>
<pages>85--103</pages>
<editor>In R. E. Miller and J. W. Thatcher, editors,</editor>
<publisher>Plenum Press.</publisher>
<contexts>
<context position="9119" citStr="Karp (1972)" startWordPosition="1584" endWordPosition="1585">he problem – namely by calculating the BLEU score. We now show that there is a problem known to be NP-complete which can be polynomially reduced to CN-2BLEU-DECIDE. For our proof, we choose 3SAT. 3.1 3SAT Consider the following problem: Definition 3 (3SAT) Let X = {x1, ... , xn} be a set of Boolean variables, let F = ^k i=1 (Li,1∨Li,2∨Li,3) be a Boolean formula, where each literal Li,j is either a variable x or its negate x. Is there a assignment Q : X → {0, 1} such that Q |= F? In other words, if we replace each x in F by Q(x), and each x by 1 − Q(x), does F become true? It has been shown by Karp (1972) that 3SAT is NP-complete. Consequently, if for another problem in NP there is polynomial-size and -time reduction of an arbitrary instance of 3SAT to an instance of this new problem, this new problem is also NP-complete. 3.2 Reduction of 3SAT to CN-2BLEU-DECIDE Let F be a Boolean formula in 3CNF, and let k be its size, as in Definition 3. We will now reduce it to a corresponding CN-2BLEU-DECIDE problem. This means that we create an alphabet E, a confusion network C, and a set of reference sentences R, such that there is a path through C with a BLEU score of 1.0 exactly if F is solvable: Creat</context>
</contexts>
<marker>Karp, 1972</marker>
<rawString>Richard M. Karp. 1972. Reducibility among combinatorial problems. In R. E. Miller and J. W. Thatcher, editors, Complexity of Computer Computations, pages 85–103. Plenum Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="3789" citStr="Knight (1999)" startWordPosition="607" endWordPosition="608">ch itself can influence the decisions after that. We also show that this also holds for unigram BLEU and the position independent error rate (PER) on a slightly augmented variant of CNs which allows for edges to carry multiple symbols. The concatenation of symbols corresponds to the interdependency of decisions in the case of bigram matches above. NP-hard problems are quite common in machine 839 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 839–847, Honolulu, October 2008.c�2008 Association for Computational Linguistics translation; for example, Knight (1999) has shown that even for a simple form of statistical MT models, the decoding problem is NP-complete. More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. But even a simple, common procedure as BLEU scoring, which can be performed in linear time on single sentences, becomes a potentially intractable problem as soon as it has to be performed on a slightly more powerful representation, such as confusion networks. This rather surprising result is the motivation of this paper. The problem of finding the best unigram BLEU score in an unaugmented va</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluation automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proc. COLING 2004,</booktitle>
<pages>501--507</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="21656" citStr="Lin and Och (2004)" startWordPosition="4125" endWordPosition="4128">path hypotheses with the same number of empty edges, and ending in the same position in the confusion network. This idea is illustrated in Figure 6: We compare only the partial precision of path hypotheses ending in the same node. Due to the simple nature of this search graph, it can easily be traversed in a left-to-right, top-to-bottom manner. With regard to a node currently being expanded, only the next node in the same row, and the corresponding columns in the next row need to be kept active. When implementing this algorithm, Hypotheses should be compared on the modified BLEUS precision by Lin and Och (2004) because the original BLEU precision equals zero as long as there are no higher n-gram matches in the partial hypotheses, which renders meaningful comparison hard or impossible. In the rightmost column, all path hypotheses within a node have the same hypothesis length. Consequently, we can select the hypothesis with the best (brevity-penalized) BLEU score by multiplying the appropriate brevity penalty to the precision of the best path ending in each of these nodes. If we always expand all possible path hypotheses within the nodes, and basically run a full search, we will always find the BLEU-b</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluation automatic evaluation metrics for machine translation. In Proc. COLING 2004, pages 501–507, Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="1404" citStr="Matusov et al., 2006" startWordPosition="207" endWordPosition="210">how that even small generalizations of this data structure render the problem to be NP-hard again. Since finding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solution for n-gram BLEU in polynomial time. 1 Introduction In machine translation (MT), confusion networks (CNs) are commonly used to represent alternative versions of sentences. Typical applications include translation of different speech recognition hypotheses (Bertoldi et al., 2007) or system combination (Fiscus, 1997; Matusov et al., 2006). A typical operation on a given CN is to find the path which minimizes or maximizes a certain evaluation metric. This operation can be used in applications like Minimum Error Rate Training (Och, 2003), or optimizing system combination as described by Hillard et al. (2007). Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by Mohri and Riley (2002), current research in MT uses more sophisticated measures, like the BLEU score (Papineni et al., 2001). Zens and Ney (2005) first described this task on general word graphs, and sketched a complete algor</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Conference of the European Chapter of the Association for Computational Linguistics, pages 33–40, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<title>An efficient algorithm for the n-best-strings problem.</title>
<date>2002</date>
<booktitle>In Proc. of the 7th Int. Conf. on Spoken Language Processing (ICSLP’02),</booktitle>
<pages>1313--1316</pages>
<location>Denver, CO,</location>
<contexts>
<context position="1801" citStr="Mohri and Riley (2002)" startWordPosition="276" endWordPosition="279">monly used to represent alternative versions of sentences. Typical applications include translation of different speech recognition hypotheses (Bertoldi et al., 2007) or system combination (Fiscus, 1997; Matusov et al., 2006). A typical operation on a given CN is to find the path which minimizes or maximizes a certain evaluation metric. This operation can be used in applications like Minimum Error Rate Training (Och, 2003), or optimizing system combination as described by Hillard et al. (2007). Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by Mohri and Riley (2002), current research in MT uses more sophisticated measures, like the BLEU score (Papineni et al., 2001). Zens and Ney (2005) first described this task on general word graphs, and sketched a complete algorithm for calculating the maximum BLEU score in a word graph. While they do not give an estimate on the complexity of their algorithm, they note that already a simpler algorithm for calculating the Position independent Error Rate (PER) has an exponential worst-case complexity. The same can be expected for their BLEU algorithm. Dreyer et al (2007) examined a special class of word graphs, namely t</context>
</contexts>
<marker>Mohri, Riley, 2002</marker>
<rawString>Mehryar Mohri and Michael Riley. 2002. An efficient algorithm for the n-best-strings problem. In Proc. of the 7th Int. Conf. on Spoken Language Processing (ICSLP’02), pages 1313–1316, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1605" citStr="Och, 2003" startWordPosition="245" endWordPosition="246">a multi-stack decoder, which finds a (not necessarily optimal) solution for n-gram BLEU in polynomial time. 1 Introduction In machine translation (MT), confusion networks (CNs) are commonly used to represent alternative versions of sentences. Typical applications include translation of different speech recognition hypotheses (Bertoldi et al., 2007) or system combination (Fiscus, 1997; Matusov et al., 2006). A typical operation on a given CN is to find the path which minimizes or maximizes a certain evaluation metric. This operation can be used in applications like Minimum Error Rate Training (Och, 2003), or optimizing system combination as described by Hillard et al. (2007). Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by Mohri and Riley (2002), current research in MT uses more sophisticated measures, like the BLEU score (Papineni et al., 2001). Zens and Ney (2005) first described this task on general word graphs, and sketched a complete algorithm for calculating the maximum BLEU score in a word graph. While they do not give an estimate on the complexity of their algorithm, they note that already a simpler algorithm for calculating the Posi</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center,</location>
<contexts>
<context position="1903" citStr="Papineni et al., 2001" startWordPosition="293" endWordPosition="296"> different speech recognition hypotheses (Bertoldi et al., 2007) or system combination (Fiscus, 1997; Matusov et al., 2006). A typical operation on a given CN is to find the path which minimizes or maximizes a certain evaluation metric. This operation can be used in applications like Minimum Error Rate Training (Och, 2003), or optimizing system combination as described by Hillard et al. (2007). Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by Mohri and Riley (2002), current research in MT uses more sophisticated measures, like the BLEU score (Papineni et al., 2001). Zens and Ney (2005) first described this task on general word graphs, and sketched a complete algorithm for calculating the maximum BLEU score in a word graph. While they do not give an estimate on the complexity of their algorithm, they note that already a simpler algorithm for calculating the Position independent Error Rate (PER) has an exponential worst-case complexity. The same can be expected for their BLEU algorithm. Dreyer et al (2007) examined a special class of word graphs, namely those that denote constrained reorderings of single sentences. These word graphs have some properties w</context>
<context position="5898" citStr="Papineni et al. (2001)" startWordPosition="994" endWordPosition="997">ach path from the start node to the end node visits each node of the graph in canonical order. Usually, we represent unlabeled edges by labeling them with the empty word E. Within this paper, we represent a CN by a list of lists of words {wz,�}, where each wz,� corresponds to a symbol on an edge between nodes i and i + 1. A path in this CN can be written as a string of integers, an 1 = a1, ... , an, such that the path is labeled w1,a1w2,a2 . . . wn,an. Note that there can be a different number of possible words, j, for different positions i. 2.1 BLEU and variants The BLEU score, as defined by Papineni et al. (2001), is the modified n-gram precision of a hypothesis, with 1 &lt; n &lt; N, given a set of reference translations R. “Modified precision” here means that for each n-gram, its maximum number of occurrences within the reference sentences is counted, and only up to that many occurrences in the hypothesis are considered to be correct. The geometric mean over the precisions for all n is calculated, and multiplied by a brevity penalty bp. This brevity penalty is 1.0 if the hypothesis sentence is at least as long as the reference sentence (special cases occur if multiple reference sentences with different le</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Alex Zubiaga</author>
<author>Hassan Sawaf</author>
</authors>
<title>Accelerated DP based search for statistical translation.</title>
<date>1997</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>2667--2670</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="7562" citStr="Tillmann et al., 1997" startWordPosition="1295" endWordPosition="1298">ength, we do not need to regard the brevity penalty in these proofs. Within the following proofs of NP-hardness, we will only require confusion networks (and word graphs) which do not contain empty words, and where all paths from the start node to the end node have the same length. Usually, in the definition of the BLEU score, N is set to 4; within this article we denote this metric as 4BLEU. We can also restrict the calculations to unigrams only, which would be 1BLEU, or to bigrams and unigrams, which we denote as 2BLEU. Similar to the 1BLEU metric is the Position independent Error Rate PER (Tillmann et al., 1997), which counts the number of substitutions, insertions, and deletions that have to be performed on the unigram counts to have the hypothesis counts match the reference counts. Unlike 1BLEU, for PER to be optimal (here, 0.0), the reference counts must match the candidate counts exactly. Given a CN {wz,�} and a set of reference sentences R, we define the optimization problem Definition 1(CN-2BLEU-OPTIMIZE) Among all paths ai through the CN, what is the path with the highest 2BLEU score? Related to this is the decision problem Definition 2 (CN-2BLEU-DECIDE) Among all paths ai through the CN, is t</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, Alex Zubiaga, and Hassan Sawaf. 1997. Accelerated DP based search for statistical translation. In European Conf. on Speech Communication and Technology, pages 2667–2670, Rhodes, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Word graphs for statistical machine translation.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond,</booktitle>
<pages>191--198</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="1924" citStr="Zens and Ney (2005)" startWordPosition="297" endWordPosition="300">ition hypotheses (Bertoldi et al., 2007) or system combination (Fiscus, 1997; Matusov et al., 2006). A typical operation on a given CN is to find the path which minimizes or maximizes a certain evaluation metric. This operation can be used in applications like Minimum Error Rate Training (Och, 2003), or optimizing system combination as described by Hillard et al. (2007). Whereas this is easily achievable for simple metrics like the Word Error Rate (WER) as described by Mohri and Riley (2002), current research in MT uses more sophisticated measures, like the BLEU score (Papineni et al., 2001). Zens and Ney (2005) first described this task on general word graphs, and sketched a complete algorithm for calculating the maximum BLEU score in a word graph. While they do not give an estimate on the complexity of their algorithm, they note that already a simpler algorithm for calculating the Position independent Error Rate (PER) has an exponential worst-case complexity. The same can be expected for their BLEU algorithm. Dreyer et al (2007) examined a special class of word graphs, namely those that denote constrained reorderings of single sentences. These word graphs have some properties which simplify the cal</context>
<context position="19780" citStr="Zens and Ney (2005)" startWordPosition="3799" endWordPosition="3802"> – in many cases, having a good, but not necessarily optimum path is preferable to having no good path at all. A simple approach would be to walk the CN from the start node to the end node, keeping track of ngrams visited so far, and choosing the word next which maximizes the n-gram precision up to this word. Track is kept by keeping n-gram count vectors for the hypothesis path and the reference sentences, and update those in each step. The main problem with this approach is that often the local optimum is suboptimal on the global scale, for example if a word occurs on a later position again. Zens and Ney (2005) on the other hand propose to keep all n-gram count vectors instead, and only recombine path hypotheses with identical count vectors. As they suspect, the search space can become exponentially large. In this paper, we suggest a compromise between these two extremes, namely keeping active a sufficiently large number of “path hypotheses” in terms of n-gram precision, instead of only the first best, or of all. But even then, edges with empty words pose a problem, as stepping along an empty edge will never decrease the precision of the local path. In certain cases, steps along empty edges may affe</context>
<context position="24961" citStr="Zens and Ney (2005)" startWordPosition="4717" endWordPosition="4720"> system 29.3 52.5 BLEU 30.5 51.6 BLEUS* *average per sentence Our algorithm can easily be extended to handle arbitrary word graphs instead of confusion networks. In this case, each “row” in Figure 6 will reflect the structure of the word graph instead of the “linear” structure of the CN. While this algorithm searches for the best path for a single sentence only, a common task is to find the best BLEU score over a whole test set – which can mean suboptimal BLEU scores for individual sentences. This adds an additional combinatorial problem over the sentences to the actual decoding process. Both Zens and Ney (2005) and Dreyer et al (2007) use a greedy approach here; the latter estimated the impact of this to be insignificant in random sampling experiments. In our experiments, we used the per-sentence BLEUS score as (greedy) decision criterion, as this is also the pruning criterion. One possibility to adapt this approach to Zens’s/Dreyer’s greedy approach for system-level BLEU scores might be to initialize n-gram counts and hypothesis length not to zero at the beginning of each sentence, but to those of the corpus so far. But as this diverts from our goal to optimize the sentence-level scores, we have no</context>
</contexts>
<marker>Zens, Ney, 2005</marker>
<rawString>Richard Zens and Hermann Ney. 2005. Word graphs for statistical machine translation. In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond, pages 191–198, Ann Arbor, MI, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>