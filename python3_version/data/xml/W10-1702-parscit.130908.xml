<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000785">
<title confidence="0.982498">
Fast Consensus Hypothesis Regeneration for Machine Translation
</title>
<author confidence="0.989477">
Boxing Chen, George Foster and Roland Kuhn
</author>
<affiliation confidence="0.989293">
National Research Council Canada
</affiliation>
<address confidence="0.978603">
283 Alexandre-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7
</address>
<email confidence="0.986654">
{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca
</email>
<sectionHeader confidence="0.997198" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999825">
This paper presents a fast consensus hy-
pothesis regeneration approach for ma-
chine translation. It combines the advan-
tages of feature-based fast consensus de-
coding and hypothesis regeneration. Our
approach is more efficient than previous
work on hypothesis regeneration, and it
explores a wider search space than con-
sensus decoding, resulting in improved
performance. Experimental results show
consistent improvements across language
pairs, and an improvement of up to 0.72
BLEU is obtained over a competitive
single-pass baseline on the Chinese-to-
English NIST task.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980384615385">
State-of-the-art statistical machine translation
(SMT) systems are often described as a two-pass
process. In the first pass, decoding algorithms are
applied to generate either a translation N-best list
or a translation forest. Then in the second pass,
various re-ranking algorithms are adopted to
compute the final translation. The re-ranking al-
gorithms include rescoring (Och et al., 2004) and
Minimum Bayes-Risk (MBR) decoding (Kumar
and Byrne, 2004; Zhang and Gildea, 2008;
Tromble et al., 2008). Rescoring uses more so-
phisticated additional feature functions to score
the hypotheses. MBR decoding directly incorpo-
rates the evaluation metrics (i.e., loss function),
into the decision criterion, so it is effective in
tuning the MT performance for a specific loss
function. In particular, sentence-level BLEU loss
function gives gains on BLEU (Kumar and
Byrne, 2004).
The naïve MBR algorithm computes the loss
function between every pair of k hypotheses,
needing O(k2) comparisons. Therefore, only
small number k is applicable. Very recently, De-
Nero et al. (2009) proposed a fast consensus de-
coding (FCD) algorithm in which the similarity
scores are computed based on the feature expec-
tations over the translation N-best list or transla-
tion forest. It is equivalent to MBR decoding
when using a linear similarity function, such as
unigram precision.
Re-ranking approaches improve performance
on an N-best list whose contents are fixed. A
complementary strategy is to augment the con-
tents of an N-best list in order to broaden the
search space. Chen et al (2008) have proposed a
three-pass SMT process, in which a hypothesis
regeneration pass is added between the decoding
and rescoring passes. New hypotheses are gener-
ated based on the original N-best hypotheses
through n-gram expansion, confusion-network
decoding or re-decoding. All three hypothesis
regeneration methods obtained decent and com-
parable improvements in conjunction with the
same rescoring model. However, since the final
translation candidates in this approach are pro-
duced from different methods, local feature func-
tions (such as translation models and reordering
models) of each hypothesis are not directly com-
parable and rescoring must exploit rich global
feature functions to compensate for the loss of
local feature functions. Thus this approach is de-
pendent on the use of computationally expensive
features for rescoring, which makes it inefficient.
In this paper, we propose a fast consensus hy-
pothesis regeneration method that combines the
advantages of feature-based fast consensus de-
coding and hypothesis regeneration. That is, we
integrate the feature-based similarity/loss func-
tion based on evaluation metrics such as BLEU
score into the hypothesis regeneration procedure
to score the partial hypotheses in the beam search
and compute the final translations. Thus, our ap-
proach is more efficient than the original three-
pass hypothesis regeneration. Moreover, our ap-
proach explores more search space than consen-
</bodyText>
<page confidence="0.990454">
11
</page>
<note confidence="0.4521405">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 11–16,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998584375">
sus decoding, giving it an advantage over the
latter.
In particular, we extend linear corpus BLEU
(Tromble et al., 2008) to n-gram expectation-
based linear BLEU, then further extend the n-
gram expectation computed on full-length hypo-
theses to n-gram expectation computed on fixed-
length partial hypotheses. Finally, we extend the
hypothesis regeneration with forward n-gram
expansion to bidirectional n-gram expansion in-
cluding both the forward and backward n-gram
expansion. Experimental results show consistent
improvements over the baseline across language
pairs, and up to 0.72 BLEU points are obtained
from a competitive baseline on the Chinese-to-
English NIST task.
</bodyText>
<sectionHeader confidence="0.996223" genericHeader="method">
2 Fast Consensus Hypothesis Regenera-
tion
</sectionHeader>
<bodyText confidence="0.999736">
Since the three hypothesis regeneration methods
with n-gram expansion, confusion network de-
coding and re-decoding produce very similar per-
formance (Chen et al., 2008), we consider only
n-gram expansion method in this paper. N-gram
expansion can (almost) fully exploit the search
space of target strings which can be generated by
an n-gram language model trained on the N-best
hypotheses (Chen et al., 2007).
</bodyText>
<subsectionHeader confidence="0.984974">
2.1 Hypothesis regeneration with bidirec-
tional n-gram expansion
</subsectionHeader>
<bodyText confidence="0.980468344827586">
N-gram expansion (Chen et al., 2007) works as
follows: firstly, train an n-gram language model
based on the translation N-best list or translation
forest; secondly, expand each partial hypothesis
by appending a word via overlapped (n-1)-grams
until the partial hypothesis reaches the sentence
ending symbol. In each expanding step, the par-
tial hypotheses are pruned through a beam-search
algorithm with scoring functions.
Duchateau et al. (2001) shows that the back-
ward language model contains information com-
plementary to the information in the forward
language model. Hence, on top of the forward n-
gram expansion used in (Chen et al., 2008), we
further introduce backward n-gram expansion to
the hypothesis regeneration procedure. Backward
n-gram expansion involves letting the partial hy-
potheses start from the last words that appeared
in the translation N-best list and having the ex-
pansion go from right to left.
Figure 1 gives an example of backward n-
gram expansion. The second row shows bi-grams
which are extracted from the original hypotheses
in the first row. The third row shows how a par-
tial hypothesis is expanded via backward n-gram
expansion method. The fourth row lists some
new hypotheses generated by backward n-gram
expansion which do not exist in the original hy-
pothesis list.
</bodyText>
<table confidence="0.884830823529412">
original about weeks&apos; work .
hypotheses one week&apos;s work
about one week&apos;s
about a week work
about one week work
bi-grams about weeks&apos;, weeks&apos; work, ...,
about one, ..., week work.
backward partial hyp. week&apos;s work
n-gram
expansion
n-gram one week&apos;s
new partial hyp. one week&apos;s work
new about one week&apos;s work
hypotheses about week&apos;s work
one weeks&apos; work .
one week&apos;s work .
one week&apos;s work .
</table>
<figureCaption confidence="0.9196226">
Figure 1: Example of original hypotheses; bi-grams
collected from them; backward expanding a partial
hypothesis via an overlapped n-1-gram; and new hy-
potheses generated through backward n-gram expan-
sion.
</figureCaption>
<subsectionHeader confidence="0.99755">
2.2 Feature-based scoring functions
</subsectionHeader>
<bodyText confidence="0.994331090909091">
To speed up the search, the partial hypotheses
are pruned via beam-search in each expanding
step. Therefore, the scoring functions applied
with the beam-search algorithm are very impor-
tant. In (Chen et al., 2008), more than 10 addi-
tional global features are computed to rank the
partial hypothesis list, and this is not an efficient
way. In this paper, we propose to directly incor-
porate the evaluation metrics such as BLEU
score to rank the candidates. The scoring func-
tions of this work are derived from the method of
lattice Minimum Bayes-risk (MBR) decoding
(Tromble et al., 2008) and fast consensus decod-
ing (DeNero et al., 2009), which were originally
inspired from N-best MBR decoding (Kumar and
Byrne, 2004).
From a set of translation candidates E, MBR
decoding chooses the translation that has the
least expected loss with respect to other candi-
dates. Given a hypothesis set E, under the proba-
bility model P(e  |f ) , MBR computes the trans-
lation e~ as follows:
</bodyText>
<page confidence="0.989466">
12
</page>
<bodyText confidence="0.999951321428571">
where f is the source sentence, L(e, e′) is the loss
function of two translations e and e′ .
Suppose that we are interested in maximizing
the BLEU score (Papineni et al., 2002) to optim-
ize the translation performance. The loss func-
tion is defined as L(e, e′) =1− BLEU(e, e′) ,
then the MBR objective can be re-written as
E represents the space of the translations. For
N-best MBR decoding, this space is the N-best
list produced by a baseline decoder (Kumar and
Byrne, 2004). For lattice MBR decoding, this
space is the set of candidates encoded in the lat-
tice (Tromble et al., 2008). Here, with hypothesis
regeneration, this space includes: 1) the transla-
tions produced by the baseline decoder either in
an N-best list or encoded in a translation lattice,
and 2) the translations created by hypothesis re-
generation.
However, BLEU score is not linear with the
length of the hypothesis, which makes the scor-
ing process for each expanding step of hypothe-
sis regeneration very slow. To further speed up
the beam search procedure, we use an extension
of a linear function of a Taylor approximation to
the logarithm of corpus BLEU which was devel-
oped by (Tromble et al., 2008). The original
BLEU score of two hypotheses e and e’ are
computed as follows.
</bodyText>
<equation confidence="0.989602125">
4
1
BLEU e e γ e e
( , ) ( , ) exp(
′ = ′ × log( ( , ))
′ (3)
4 ∑= P n e e
n 1
</equation>
<bodyText confidence="0.999814666666667">
where Pn (e, e′) is the precision of n-grams in the
hypothesis e given e’ and γ(e, e′) is a brevity
penalty. Let |e |denote the length of e. The corpus
log-BLEU gain is defined as follows:
where cn ( e , e ′ ) are the counts of the matched n-
grams and θn ( 0 ≤ n ≤ 4 ) are constant weights
estimated with held-out data.
Suppose we have computed the expected n-
gram counts from the N-best list or translation
forest. Then we may extend linear corpus BLEU
in (5) to n-gram expectation-based linear corpus
BLEU to score the partial hypotheses h. That is
</bodyText>
<equation confidence="0.967167666666667">
G h e
( , &apos; )  ||
0 4
</equation>
<bodyText confidence="0.99982924137931">
where δn (h, t) are n-gram indicator functions that
equal 1 if n-gram t appears in h and 0 other-
wise; E[cn (e&apos; , t)] (1 ≤ n ≤ 4) are the real-valued
n-gram expectations. Different from lattice MBR
decoding, n-gram expectations in this work are
computed over the original translation N-best list
or translation forest; Tn (1≤ n≤ 4) are the sets of
n-grams collected from translation N-best list or
translation forest. Then we make a further exten-
sion: the expectations of the n-gram counts for
each expanding step are computed over the par-
tial translations. The lengths of all partial hypo-
theses are the same in each n-gram expanding
step. For instance, in the 5th n-gram expanding
step, the lengths of all the partial hypotheses are
5 words. Therefore, we use n-gram count expec-
tations computed over partial original transla-
tions that only contain the first 5 words. The rea-
son is that this solution contains more informa-
tion about word orderings, since some n-grams
appear more than others at the beginning of the
translations while they may appear with the same
or even lower frequencies than others in the full
translations.
Once the expanding process of hypothesis re-
generation is finished, we use a more precise
BLEU metric to score all the translation candi-
dates. We extend BLEU score in (3) to n-gram
expectation-based BLEU. That is:
</bodyText>
<equation confidence="0.917703052631579">
Scoreh BLEU
( ) = (
e~ = arg min ∑ L(e, e′) ⋅ P(e  |f
e′∈E
e E
∈
) (1)
e~ =argmax∑BLEU(e,e)⋅P(eI If
e E
∈
) (2)
e′∈E
4(6)
= θ h + ∑ ∑
θ ⋅ E c e t
[ ( &apos; ⋅ δ h t
1 , )] ( , )
n n n
n = 1 t T
</equation>
<figure confidence="0.948112046511628">
∈ n
&apos;
h e
,
)



 
=
exp
min
4
)] )
∑
min( c h t E c e t
n ( , ), [ ( &apos; ,
n
4
 ||
e 1 (4)
log(BLEU e e
( , )) min(0, 1
′ = − ) + log( ( , ))
  ||4 ∑= Pn e e
e ′
′ n 1
 − E e
[ |&apos; |] 
0,1
 
 ||
h 
t T
∈ n
+ ∑ log
4 n=1 ∑
Tn



 
1
</figure>
<equation confidence="0.688258333333333">
t ∈
c h t
n ( , )
</equation>
<bodyText confidence="0.998646">
Therefore, the first-order Taylor approxima-
tion to the logarithm of corpus BLEU is shown
in Equation (5).
</bodyText>
<page confidence="0.754019">
4
</page>
<equation confidence="0.99726125">
1 (5)
G e e θ e
( , )  ||
′ = + ∑= θn cn e e
⋅ ( , )
′
0 4 n 1
(7)
</equation>
<bodyText confidence="0.97984225">
where cn (h, t) is the count of n-gram t in the
hypothesis h. The step of choosing the final
translation is the same as fast consensus decod-
ing (DeNero et al., 2009): first we compute n-
</bodyText>
<page confidence="0.997972">
13
</page>
<bodyText confidence="0.999951222222222">
gram feature expectations, and then we choose
the translation that is most similar to the others
via expected similarity according to feature-
based BLEU score as shown in (7). The differ-
ence is the space of translations: the space of fast
consensus decoding is the same as MBR decod-
ing, while the space of hypothesis regeneration is
enlarged by the new translations produced via n-
gram expansion.
</bodyText>
<subsectionHeader confidence="0.999434">
2.3 Fast consensus hypothesis regeneration
</subsectionHeader>
<bodyText confidence="0.999972916666667">
We first generate two new hypothesis lists via
forward and backward n-gram expansion using
the scoring function in Equation (6). Then we
choose a final translation using the scoring func-
tion in Equation (7) from the union of the origi-
nal hypotheses and newly generated hypotheses.
The original hypotheses are from the N-best list
or extracted from the translation forest. The new
hypotheses are generated by forward or back-
ward n-gram expansion or are the union of both
two new hypothesis lists (this is called “bi-
directional n-gram expansion”).
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="evaluation">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999994666666667">
We carried out experiments based on translation
N-best lists generated by a state-of-the-art
phrase-based statistical machine translation sys-
tem, similar to (Koehn et al., 2007). In detail, the
phrase table is derived from merged counts of
symmetrized IBM2 and HMM alignments; the
system has both lexicalized and distance-based
distortion components (there is a 7-word distor-
tion limit) and employs cube pruning (Huang and
Chiang, 2007). The baseline is a log-linear fea-
ture combination that includes language models,
the distortion components, translation model,
phrase and word penalties. Weights on feature
functions are found by lattice MERT (Macherey
et al., 2008).
</bodyText>
<subsectionHeader confidence="0.996955">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.974859636363636">
We evaluated with different language pairs: Chi-
nese-to-English, and German-to-English. Chi-
nese-to-English tasks are based on training data
for the NIST 1 2009 evaluation Chinese-to-
English track. All the allowed bilingual corpora
have been used for estimating the translation
model. We trained two language models: the first
one is a 5-gram LM which is estimated on the
target side of the parallel data. The second is a 5-
gram LM trained on the so-called English Giga-
word corpus.
</bodyText>
<table confidence="0.999051125">
Chi Eng
Parallel Large |S |10.1M
Train Data
|W |270.0M 279.1M
Dev |S |1,506 1,506×4
Test NIST06 |S |1,664 1,664×4
NIST08 |S |1,357 1,357×4
Gigaword |S |- 11.7M
</table>
<tableCaption confidence="0.996211">
Table 1: Statistics of training, dev, and test sets for
Chinese-to-English task.
</tableCaption>
<bodyText confidence="0.999952052631579">
We carried out experiments for translating
Chinese to English. We first created a develop-
ment set which used mainly data from the NIST
2005 test set, and also some balanced-genre web-
text from the NIST training material. Evaluation
was performed on the NIST 2006 and 2008 test
sets. Table 1 gives figures for training, develop-
ment and test corpora; |S |is the number of the
sentences, and |W |is the size of running words.
Four references are provided for all dev and test
sets.
For German-to-English tasks, we used WMT
20062 data sets. The parallel training data con-
tains about 1 million sentence pairs and includes
21 million target words; both the dev set and test
set contain 2000 sentences; one reference is pro-
vided for each source input sentence. Only the
target-language half of the parallel training data
are used to train the language model in this task.
</bodyText>
<subsectionHeader confidence="0.970833">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999679470588235">
Our evaluation metric is IBM BLEU (Papineni et
al., 2002), which performs case-insensitive
matching of n-grams up to n = 4.
Our first experiment was carried out over
1000-best lists on Chinese-to-English task. For
comparison, we also conducted experiments with
rescoring (two-pass) and three-pass hypothesis
regeneration with only forward n-gram expan-
sion as proposed in (Chen et al., 2008). In the
“rescoring” and “three-pass” systems, we used
the same rescoring model. There are 21 rescoring
features in total, mainly translation lexicon
scores from IBM and HMM models, posterior
probabilities for words, n-grams, and sentence
length, and language models, etc. For a complete
description, please refer to (Ueffing et al., 2007).
The results in BLEU-4 are reported in Table 2.
</bodyText>
<table confidence="0.9102432">
1 http://www.nist.gov/speech/tests/mt 2 http://www.statmt.org/wmt06/
14
testset NIST’06 NIST’08
baseline 35.70 28.60
rescoring 36.01 28.97
three-pass 35.98 28.99
FCD 36.00 29.10
Fwd. 36.13 29.19
Bwd. 36.11 29.20
Bid. 36.20 29.28
</table>
<tableCaption confidence="0.439417333333333">
Table 2: Translation performances in BLEU-4(%)
over 1000-best lists for Chinese-to-English task: “res-
coring” represents the results of rescoring; “three-
pass”, three-pass hypothesis regeneration with for-
ward n-gram expansion; “FCD”, fast consensus de-
coding; “Fwd”, the results of hypothesis regeneration
with forward n-gram expansion; “Bwd”, backward n-
gram expansion; and “Bid”, bi-directional n-gram
expansion.
</tableCaption>
<bodyText confidence="0.969570954545454">
Firstly, rescoring improved performance over
the baseline by 0.3-0.4 BLEU point. Three-pass
hypothesis regeneration with only forward n-
gram expansion (“three-pass” in Table 2) ob-
tained almost the same improvements as rescor-
ing. Three-pass hypothesis regeneration exploits
more hypotheses than rescoring, while rescoring
involves more scoring feature functions than the
former. They reached a balance in this experi-
ment. Then, fast consensus decoding (“FCD” in
Table 2) obtains 0.3-0.5 BLEU point improve-
ments over the baseline. Both forward and back-
ward n-gram expansion (“Fwd.” and “Bwd.” in
Table 2) improved about 0.1 BLEU point over
the results of consensus decoding. Fast consen-
sus hypothesis regeneration (Fwd. and Bwd. in
Table 2) got better improvements than three-pass
hypothesis regeneration (“three-pass” in Table 2)
by 0.1-0.2 BLEU point. Finally, combining hy-
pothesis lists from forward and backward n-gram
expansion (“Bid.” in Table 2), further slight
gains were obtained.
</bodyText>
<table confidence="0.994666">
testset Average time
three-pass 3h 54m
Fwd. 25m
Bwd. 28m
Bid. 40m
</table>
<tableCaption confidence="0.966658">
Table 3: Average processing time of NIST’06 and
NIST’08 test sets used in different systems. Times
include n-best list regeneration and re-ranking.
</tableCaption>
<bodyText confidence="0.99933215">
Moreover, fast consensus hypothesis regenera-
tion is much faster than the three-pass one, be-
cause the former only needs to compute one fea-
ture, while the latter needs to compute more than
20 additional features. In this experiment, the
former is about 10 times faster than the latter in
terms of processing time, as shown in Table 3.
In our second experiment, we set the size of
N-best list N equal to 10,000 for both Chinese-to-
English and German-to-English tasks. The re-
sults are reported in Table 4. The same trend as
in the first experiment can also be observed in
this experiment. It is worth noticing that enlarg-
ing the size of the N-best list from 1000 to
10,000 did not change the performance signifi-
cantly. Bi-directional n-gram expansion obtained
improvements of 0.24 BLEU-score for WMT
2006 de-en test set; 0.55 for NIST 2006 test set;
and 0.72 for NIST 2008 test set over the base-
line.
</bodyText>
<table confidence="0.994258857142857">
Lang. ch-en de-en
testset NIST’06 NIST’08 Test2006
baseline 35.70 28.60 26.92
FCD 36.03 29.08 27.03
Fwd. 36.16 29.25 27.11
Bwd. 36.17 29.22 27.12
Bid. 36.25 29.32 27.16
</table>
<tableCaption confidence="0.9876315">
Table 4: Translation performances in BLEU-4 (%)
over 10K-best lists.
</tableCaption>
<bodyText confidence="0.99969875">
We then tested the effect of the extension ac-
cording to which the expectations over n-gram
counts are computed on partial hypotheses rather
than whole candidate translations as described in
Section 2.2. As shown in Table 5, we got tiny
improvements on both test sets by computing the
expectations over n-gram counts on partial hypo-
theses.
</bodyText>
<table confidence="0.984777666666667">
testset NIST’06 NIST’08
full 36.11 29.14
partial 36.13 29.19
</table>
<tableCaption confidence="0.992531833333333">
Table 5: Translation performances in BLEU-4 (%)
over 1000-best lists for Chinese-to-English task:
“full” represents expectations over n-gram counts that
are computed on whole hypotheses; “partial”
represents expectations over n-gram counts that are
computed on partial hypotheses.
</tableCaption>
<subsectionHeader confidence="0.962765">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9997646">
To speed up the search, the partial hypotheses in
each expanding step are pruned. When pruning is
applied, forward and backward n-gram expan-
sion would generate different new hypothesis
lists. Let us look back at the example in Figure 1.
</bodyText>
<page confidence="0.992445">
15
</page>
<bodyText confidence="0.9999368">
Given 5 original hypotheses in Figure 1, if we set
the beam size equal to 5 (the size of the original
hypotheses), the forward and backward n-gram
expansion generated different new hypothesis
lists, as shown in Figure 2.
</bodyText>
<figureCaption confidence="0.998614">
Figure 2: Different new hypothesis lists generated by
forward and backward n-gram expansion.
</figureCaption>
<bodyText confidence="0.9998975">
For bi-directional n-gram expansion, the cho-
sen translation for a source sentence comes from
the decoder 94% of the time for WMT 2006 test
set, 90% for NIST test sets; it comes from for-
ward n-gram expansion 2% of the time for WMT
2006 test set, 4% for NIST test sets; it comes
from backward n-gram expansion 4% of the time
for WMT 2006 test set, 6% for NIST test sets.
This proves bidirectional n-gram expansion is a
good way of enlarging the search space.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999987277777778">
We have proposed a fast consensus hypothesis
regeneration approach for machine translation. It
combines the advantages of feature-based con-
sensus decoding and hypothesis regeneration.
This approach is more efficient than previous
work on hypothesis regeneration, and it explores
a wider search space than consensus decoding,
resulting in improved performance. Experiments
showed consistent improvements across lan-
guage pairs.
Instead of N-best lists, translation lattices or
forests have been shown to be effective for MBR
decoding (Zhang and Gildea, 2008; Tromble et
al., 2008), and DeNero et al. (2009) showed how
to compute expectations of n-grams from a trans-
lation forest. Therefore, our future work may
involve hypothesis regeneration using an n-gram
language model trained on the translation forest.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999771272727273">
B. Chen, M. Federico and M. Cettolo. 2007. Better N-
best Translations through Generative n-gram Lan-
guage Models. In: Proceedings of MT Summit XI.
Copenhagen, Denmark. September.
B. Chen, M. Zhang, A. Aw, and H. Li. 2008. Regene-
rating Hypotheses for Statistical Machine Transla-
tion. In: Proceedings of COLING. pp105-112.
Manchester, UK, August.
J. DeNero, D. Chiang and K. Knight. 2009. Fast Con-
sensus Decoding over Translation Forests. In: Pro-
ceedings of ACL. Singapore, August.
J. Duchateau, K. Demuynck, and P. Wambacq. 2001.
Confidence scoring based on backward language
models. In: Proceedings of ICASSP 2001. Salt
Lake City, Utah, USA, May.
L. Huang and D. Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models.
In: Proceedings of ACL. pp. 144-151, Prague,
Czech Republic, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and
E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In: Proceedings of
ACL. pp. 177-180, Prague, Czech Republic.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In:
Proceedings of NAACL. Boston, MA, May.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based Minimum Error Rate Training
for Statistical Machine Translation. In: Proceed-
ings of EMNLP. pp. 725-734, Honolulu, USA,
October.
F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL.
Sapporo, Japan. July.
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K.
Eng, V. Jain, Z. Jin, and D. Radev. 2004. A Smor-
gasbord of Features for Statistical Machine Trans-
lation. In: Proceedings of NAACL. Boston.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In: Proceedings of the ACL 2002.
R. Tromble, S. Kumar, F. J. Och, and W. Macherey.
2008. Lattice minimum Bayes-risk decoding for
statistical machine translation. In: Proceedings of
EMNLP. Hawaii, US. October.
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.
2007. NRC’s Portage system for WMT 2007. In:
Proceedings of ACL Workshop on SMT. Prague,
Czech Republic, June.
H. Zhang and D. Gildea. 2008. Efficient multipass
decoding for synchronous context free grammars.
In: Proceedings of ACL. Columbus, US. June.
</reference>
<bodyText confidence="0.7230945">
forward
backward
one week&apos;s work .
about week&apos;s work
one week&apos;s work .
about one week&apos;s work
</bodyText>
<page confidence="0.981336">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.210900">
<title confidence="0.999971">Fast Consensus Hypothesis Regeneration for Machine Translation</title>
<author confidence="0.995153">George Foster Chen</author>
<affiliation confidence="0.997746">National Research Council Canada</affiliation>
<address confidence="0.977906">283 Alexandre-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7</address>
<email confidence="0.741819">Boxing.Chen@nrc.ca</email>
<email confidence="0.741819">George.Foster@nrc.ca</email>
<email confidence="0.741819">Roland.Kuhn@nrc.ca</email>
<abstract confidence="0.95526675">This paper presents a fast consensus hypothesis regeneration approach for machine translation. It combines the advantages of feature-based fast consensus decoding and hypothesis regeneration. Our approach is more efficient than previous work on hypothesis regeneration, and it explores a wider search space than consensus decoding, resulting in improved performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-to- English NIST task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>M Federico</author>
<author>M Cettolo</author>
</authors>
<title>Better Nbest Translations through Generative n-gram Language Models. In:</title>
<date>2007</date>
<booktitle>Proceedings of MT Summit XI.</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="5139" citStr="Chen et al., 2007" startWordPosition="757" endWordPosition="760">w consistent improvements over the baseline across language pairs, and up to 0.72 BLEU points are obtained from a competitive baseline on the Chinese-toEnglish NIST task. 2 Fast Consensus Hypothesis Regeneration Since the three hypothesis regeneration methods with n-gram expansion, confusion network decoding and re-decoding produce very similar performance (Chen et al., 2008), we consider only n-gram expansion method in this paper. N-gram expansion can (almost) fully exploit the search space of target strings which can be generated by an n-gram language model trained on the N-best hypotheses (Chen et al., 2007). 2.1 Hypothesis regeneration with bidirectional n-gram expansion N-gram expansion (Chen et al., 2007) works as follows: firstly, train an n-gram language model based on the translation N-best list or translation forest; secondly, expand each partial hypothesis by appending a word via overlapped (n-1)-grams until the partial hypothesis reaches the sentence ending symbol. In each expanding step, the partial hypotheses are pruned through a beam-search algorithm with scoring functions. Duchateau et al. (2001) shows that the backward language model contains information complementary to the informa</context>
</contexts>
<marker>Chen, Federico, Cettolo, 2007</marker>
<rawString>B. Chen, M. Federico and M. Cettolo. 2007. Better Nbest Translations through Generative n-gram Language Models. In: Proceedings of MT Summit XI. Copenhagen, Denmark. September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>M Zhang</author>
<author>A Aw</author>
<author>H Li</author>
</authors>
<title>Regenerating Hypotheses for Statistical Machine Translation. In:</title>
<date>2008</date>
<booktitle>Proceedings of COLING.</booktitle>
<pages>105--112</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2406" citStr="Chen et al (2008)" startWordPosition="353" endWordPosition="356">ypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, DeNero et al. (2009) proposed a fast consensus decoding (FCD) algorithm in which the similarity scores are computed based on the feature expectations over the translation N-best list or translation forest. It is equivalent to MBR decoding when using a linear similarity function, such as unigram precision. Re-ranking approaches improve performance on an N-best list whose contents are fixed. A complementary strategy is to augment the contents of an N-best list in order to broaden the search space. Chen et al (2008) have proposed a three-pass SMT process, in which a hypothesis regeneration pass is added between the decoding and rescoring passes. New hypotheses are generated based on the original N-best hypotheses through n-gram expansion, confusion-network decoding or re-decoding. All three hypothesis regeneration methods obtained decent and comparable improvements in conjunction with the same rescoring model. However, since the final translation candidates in this approach are produced from different methods, local feature functions (such as translation models and reordering models) of each hypothesis a</context>
<context position="4899" citStr="Chen et al., 2008" startWordPosition="718" endWordPosition="721">on computed on fixedlength partial hypotheses. Finally, we extend the hypothesis regeneration with forward n-gram expansion to bidirectional n-gram expansion including both the forward and backward n-gram expansion. Experimental results show consistent improvements over the baseline across language pairs, and up to 0.72 BLEU points are obtained from a competitive baseline on the Chinese-toEnglish NIST task. 2 Fast Consensus Hypothesis Regeneration Since the three hypothesis regeneration methods with n-gram expansion, confusion network decoding and re-decoding produce very similar performance (Chen et al., 2008), we consider only n-gram expansion method in this paper. N-gram expansion can (almost) fully exploit the search space of target strings which can be generated by an n-gram language model trained on the N-best hypotheses (Chen et al., 2007). 2.1 Hypothesis regeneration with bidirectional n-gram expansion N-gram expansion (Chen et al., 2007) works as follows: firstly, train an n-gram language model based on the translation N-best list or translation forest; secondly, expand each partial hypothesis by appending a word via overlapped (n-1)-grams until the partial hypothesis reaches the sentence e</context>
<context position="7344" citStr="Chen et al., 2008" startWordPosition="1106" endWordPosition="1109">nsion n-gram one week&apos;s new partial hyp. one week&apos;s work new about one week&apos;s work hypotheses about week&apos;s work one weeks&apos; work . one week&apos;s work . one week&apos;s work . Figure 1: Example of original hypotheses; bi-grams collected from them; backward expanding a partial hypothesis via an overlapped n-1-gram; and new hypotheses generated through backward n-gram expansion. 2.2 Feature-based scoring functions To speed up the search, the partial hypotheses are pruned via beam-search in each expanding step. Therefore, the scoring functions applied with the beam-search algorithm are very important. In (Chen et al., 2008), more than 10 additional global features are computed to rank the partial hypothesis list, and this is not an efficient way. In this paper, we propose to directly incorporate the evaluation metrics such as BLEU score to rank the candidates. The scoring functions of this work are derived from the method of lattice Minimum Bayes-risk (MBR) decoding (Tromble et al., 2008) and fast consensus decoding (DeNero et al., 2009), which were originally inspired from N-best MBR decoding (Kumar and Byrne, 2004). From a set of translation candidates E, MBR decoding chooses the translation that has the least</context>
<context position="15829" citStr="Chen et al., 2008" startWordPosition="2690" endWordPosition="2693">e dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 3.2 Results Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Our first experiment was carried out over 1000-best lists on Chinese-to-English task. For comparison, we also conducted experiments with rescoring (two-pass) and three-pass hypothesis regeneration with only forward n-gram expansion as proposed in (Chen et al., 2008). In the “rescoring” and “three-pass” systems, we used the same rescoring model. There are 21 rescoring features in total, mainly translation lexicon scores from IBM and HMM models, posterior probabilities for words, n-grams, and sentence length, and language models, etc. For a complete description, please refer to (Ueffing et al., 2007). The results in BLEU-4 are reported in Table 2. 1 http://www.nist.gov/speech/tests/mt 2 http://www.statmt.org/wmt06/ 14 testset NIST’06 NIST’08 baseline 35.70 28.60 rescoring 36.01 28.97 three-pass 35.98 28.99 FCD 36.00 29.10 Fwd. 36.13 29.19 Bwd. 36.11 29.20 </context>
</contexts>
<marker>Chen, Zhang, Aw, Li, 2008</marker>
<rawString>B. Chen, M. Zhang, A. Aw, and H. Li. 2008. Regenerating Hypotheses for Statistical Machine Translation. In: Proceedings of COLING. pp105-112. Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Chiang</author>
<author>K Knight</author>
</authors>
<title>Fast Consensus Decoding over Translation Forests. In:</title>
<date>2009</date>
<booktitle>Proceedings of ACL. Singapore,</booktitle>
<contexts>
<context position="1908" citStr="DeNero et al. (2009)" startWordPosition="270" endWordPosition="274">, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, DeNero et al. (2009) proposed a fast consensus decoding (FCD) algorithm in which the similarity scores are computed based on the feature expectations over the translation N-best list or translation forest. It is equivalent to MBR decoding when using a linear similarity function, such as unigram precision. Re-ranking approaches improve performance on an N-best list whose contents are fixed. A complementary strategy is to augment the contents of an N-best list in order to broaden the search space. Chen et al (2008) have proposed a three-pass SMT process, in which a hypothesis regeneration pass is added between the </context>
<context position="7766" citStr="DeNero et al., 2009" startWordPosition="1179" endWordPosition="1182">e search, the partial hypotheses are pruned via beam-search in each expanding step. Therefore, the scoring functions applied with the beam-search algorithm are very important. In (Chen et al., 2008), more than 10 additional global features are computed to rank the partial hypothesis list, and this is not an efficient way. In this paper, we propose to directly incorporate the evaluation metrics such as BLEU score to rank the candidates. The scoring functions of this work are derived from the method of lattice Minimum Bayes-risk (MBR) decoding (Tromble et al., 2008) and fast consensus decoding (DeNero et al., 2009), which were originally inspired from N-best MBR decoding (Kumar and Byrne, 2004). From a set of translation candidates E, MBR decoding chooses the translation that has the least expected loss with respect to other candidates. Given a hypothesis set E, under the probability model P(e |f ) , MBR computes the translation e~ as follows: 12 where f is the source sentence, L(e, e′) is the loss function of two translations e and e′ . Suppose that we are interested in maximizing the BLEU score (Papineni et al., 2002) to optimize the translation performance. The loss function is defined as L(e, e′) =1</context>
<context position="12132" citStr="DeNero et al., 2009" startWordPosition="2092" endWordPosition="2095">, ) n n n n = 1 t T ∈ n &apos; h e , )      = exp min 4 )] ) ∑ min( c h t E c e t n ( , ), [ ( &apos; , n 4 || e 1 (4) log(BLEU e e ( , )) min(0, 1 ′ = − ) + log( ( , )) ||4 ∑= Pn e e e ′ ′ n 1  − E e [ |&apos; |]  0,1    || h  t T ∈ n + ∑ log 4 n=1 ∑ Tn      1 t ∈ c h t n ( , ) Therefore, the first-order Taylor approximation to the logarithm of corpus BLEU is shown in Equation (5). 4 1 (5) G e e θ e ( , ) || ′ = + ∑= θn cn e e ⋅ ( , ) ′ 0 4 n 1 (7) where cn (h, t) is the count of n-gram t in the hypothesis h. The step of choosing the final translation is the same as fast consensus decoding (DeNero et al., 2009): first we compute n13 gram feature expectations, and then we choose the translation that is most similar to the others via expected similarity according to featurebased BLEU score as shown in (7). The difference is the space of translations: the space of fast consensus decoding is the same as MBR decoding, while the space of hypothesis regeneration is enlarged by the new translations produced via ngram expansion. 2.3 Fast consensus hypothesis regeneration We first generate two new hypothesis lists via forward and backward n-gram expansion using the scoring function in Equation (6). Then we ch</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>J. DeNero, D. Chiang and K. Knight. 2009. Fast Consensus Decoding over Translation Forests. In: Proceedings of ACL. Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchateau</author>
<author>K Demuynck</author>
<author>P Wambacq</author>
</authors>
<title>Confidence scoring based on backward language models. In:</title>
<date>2001</date>
<booktitle>Proceedings of ICASSP</booktitle>
<location>Salt Lake City, Utah, USA,</location>
<contexts>
<context position="5650" citStr="Duchateau et al. (2001)" startWordPosition="831" endWordPosition="834"> strings which can be generated by an n-gram language model trained on the N-best hypotheses (Chen et al., 2007). 2.1 Hypothesis regeneration with bidirectional n-gram expansion N-gram expansion (Chen et al., 2007) works as follows: firstly, train an n-gram language model based on the translation N-best list or translation forest; secondly, expand each partial hypothesis by appending a word via overlapped (n-1)-grams until the partial hypothesis reaches the sentence ending symbol. In each expanding step, the partial hypotheses are pruned through a beam-search algorithm with scoring functions. Duchateau et al. (2001) shows that the backward language model contains information complementary to the information in the forward language model. Hence, on top of the forward ngram expansion used in (Chen et al., 2008), we further introduce backward n-gram expansion to the hypothesis regeneration procedure. Backward n-gram expansion involves letting the partial hypotheses start from the last words that appeared in the translation N-best list and having the expansion go from right to left. Figure 1 gives an example of backward ngram expansion. The second row shows bi-grams which are extracted from the original hypo</context>
</contexts>
<marker>Duchateau, Demuynck, Wambacq, 2001</marker>
<rawString>J. Duchateau, K. Demuynck, and P. Wambacq. 2001. Confidence scoring based on backward language models. In: Proceedings of ICASSP 2001. Salt Lake City, Utah, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models. In:</title>
<date>2007</date>
<booktitle>Proceedings of ACL.</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13598" citStr="Huang and Chiang, 2007" startWordPosition="2326" endWordPosition="2329">potheses are generated by forward or backward n-gram expansion or are the union of both two new hypothesis lists (this is called “bidirectional n-gram expansion”). 3 Experimental Results We carried out experiments based on translation N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 Data We evaluated with different language pairs: Chinese-to-English, and German-to-English. Chinese-to-English tasks are based on training data for the NIST 1 2009 evaluation Chinese-toEnglish track. All the allowed bilingual corpora have been used for estimating the translation model. We trained two language models: the first one is a 5-gram LM which is esti</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In: Proceedings of ACL. pp. 144-151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. In:</title>
<date>2007</date>
<booktitle>Proceedings of ACL.</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="13339" citStr="Koehn et al., 2007" startWordPosition="2286" endWordPosition="2289">. Then we choose a final translation using the scoring function in Equation (7) from the union of the original hypotheses and newly generated hypotheses. The original hypotheses are from the N-best list or extracted from the translation forest. The new hypotheses are generated by forward or backward n-gram expansion or are the union of both two new hypothesis lists (this is called “bidirectional n-gram expansion”). 3 Experimental Results We carried out experiments based on translation N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 Data We evaluated with different language pairs: Chinese-to-English, and German-to-English. Chinese-to</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In: Proceedings of ACL. pp. 177-180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation. In:</title>
<date>2004</date>
<booktitle>Proceedings of NAACL.</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="1294" citStr="Kumar and Byrne, 2004" startWordPosition="177" endWordPosition="180">ments across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, DeNero</context>
<context position="7847" citStr="Kumar and Byrne, 2004" startWordPosition="1191" endWordPosition="1194">tep. Therefore, the scoring functions applied with the beam-search algorithm are very important. In (Chen et al., 2008), more than 10 additional global features are computed to rank the partial hypothesis list, and this is not an efficient way. In this paper, we propose to directly incorporate the evaluation metrics such as BLEU score to rank the candidates. The scoring functions of this work are derived from the method of lattice Minimum Bayes-risk (MBR) decoding (Tromble et al., 2008) and fast consensus decoding (DeNero et al., 2009), which were originally inspired from N-best MBR decoding (Kumar and Byrne, 2004). From a set of translation candidates E, MBR decoding chooses the translation that has the least expected loss with respect to other candidates. Given a hypothesis set E, under the probability model P(e |f ) , MBR computes the translation e~ as follows: 12 where f is the source sentence, L(e, e′) is the loss function of two translations e and e′ . Suppose that we are interested in maximizing the BLEU score (Papineni et al., 2002) to optimize the translation performance. The loss function is defined as L(e, e′) =1− BLEU(e, e′) , then the MBR objective can be re-written as E represents the spac</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In: Proceedings of NAACL. Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F Och</author>
<author>I Thayer</author>
<author>J Uszkoreit</author>
</authors>
<title>Lattice-based Minimum Error Rate Training for Statistical Machine Translation. In:</title>
<date>2008</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<pages>725--734</pages>
<location>Honolulu, USA,</location>
<contexts>
<context position="13831" citStr="Macherey et al., 2008" startWordPosition="2360" endWordPosition="2363"> N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 Data We evaluated with different language pairs: Chinese-to-English, and German-to-English. Chinese-to-English tasks are based on training data for the NIST 1 2009 evaluation Chinese-toEnglish track. All the allowed bilingual corpora have been used for estimating the translation model. We trained two language models: the first one is a 5-gram LM which is estimated on the target side of the parallel data. The second is a 5- gram LM trained on the so-called English Gigaword corpus. Chi Eng Parallel Large |S |10.1M Train Data |W |270.0M 279.1M Dev |S |1,506 1,506×4 Test NIST06 |S |1,664 1,6</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-based Minimum Error Rate Training for Statistical Machine Translation. In: Proceedings of EMNLP. pp. 725-734, Honolulu, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation. In:</title>
<date>2003</date>
<booktitle>Proceedings of ACL.</booktitle>
<location>Sapporo, Japan.</location>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In: Proceedings of ACL. Sapporo, Japan. July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A Smorgasbord of Features for Statistical Machine Translation. In:</title>
<date>2004</date>
<booktitle>Proceedings of NAACL.</booktitle>
<location>Boston.</location>
<contexts>
<context position="1233" citStr="Och et al., 2004" startWordPosition="168" endWordPosition="171">performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Theref</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A Smorgasbord of Features for Statistical Machine Translation. In: Proceedings of NAACL. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation. In:</title>
<date>2002</date>
<booktitle>Proceedings of the ACL</booktitle>
<contexts>
<context position="8281" citStr="Papineni et al., 2002" startWordPosition="1271" endWordPosition="1274"> Minimum Bayes-risk (MBR) decoding (Tromble et al., 2008) and fast consensus decoding (DeNero et al., 2009), which were originally inspired from N-best MBR decoding (Kumar and Byrne, 2004). From a set of translation candidates E, MBR decoding chooses the translation that has the least expected loss with respect to other candidates. Given a hypothesis set E, under the probability model P(e |f ) , MBR computes the translation e~ as follows: 12 where f is the source sentence, L(e, e′) is the loss function of two translations e and e′ . Suppose that we are interested in maximizing the BLEU score (Papineni et al., 2002) to optimize the translation performance. The loss function is defined as L(e, e′) =1− BLEU(e, e′) , then the MBR objective can be re-written as E represents the space of the translations. For N-best MBR decoding, this space is the N-best list produced by a baseline decoder (Kumar and Byrne, 2004). For lattice MBR decoding, this space is the set of candidates encoded in the lattice (Tromble et al., 2008). Here, with hypothesis regeneration, this space includes: 1) the translations produced by the baseline decoder either in an N-best list or encoded in a translation lattice, and 2) the translat</context>
<context position="15496" citStr="Papineni et al., 2002" startWordPosition="2641" endWordPosition="2644">ining, development and test corpora; |S |is the number of the sentences, and |W |is the size of running words. Four references are provided for all dev and test sets. For German-to-English tasks, we used WMT 20062 data sets. The parallel training data contains about 1 million sentence pairs and includes 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 3.2 Results Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Our first experiment was carried out over 1000-best lists on Chinese-to-English task. For comparison, we also conducted experiments with rescoring (two-pass) and three-pass hypothesis regeneration with only forward n-gram expansion as proposed in (Chen et al., 2008). In the “rescoring” and “three-pass” systems, we used the same rescoring model. There are 21 rescoring features in total, mainly translation lexicon scores from IBM and HMM models, posterior probabilities for words, n-grams, and sentence length, and language models,</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In: Proceedings of the ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tromble</author>
<author>S Kumar</author>
<author>F J Och</author>
<author>W Macherey</author>
</authors>
<title>Lattice minimum Bayes-risk decoding for statistical machine translation. In:</title>
<date>2008</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<location>Hawaii, US.</location>
<contexts>
<context position="1341" citStr="Tromble et al., 2008" startWordPosition="185" endWordPosition="188"> of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, DeNero et al. (2009) proposed a fast consensus decodi</context>
<context position="4144" citStr="Tromble et al., 2008" startWordPosition="609" endWordPosition="612">ion metrics such as BLEU score into the hypothesis regeneration procedure to score the partial hypotheses in the beam search and compute the final translations. Thus, our approach is more efficient than the original threepass hypothesis regeneration. Moreover, our approach explores more search space than consen11 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 11–16, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics sus decoding, giving it an advantage over the latter. In particular, we extend linear corpus BLEU (Tromble et al., 2008) to n-gram expectationbased linear BLEU, then further extend the ngram expectation computed on full-length hypotheses to n-gram expectation computed on fixedlength partial hypotheses. Finally, we extend the hypothesis regeneration with forward n-gram expansion to bidirectional n-gram expansion including both the forward and backward n-gram expansion. Experimental results show consistent improvements over the baseline across language pairs, and up to 0.72 BLEU points are obtained from a competitive baseline on the Chinese-toEnglish NIST task. 2 Fast Consensus Hypothesis Regeneration Since the t</context>
<context position="7716" citStr="Tromble et al., 2008" startWordPosition="1170" endWordPosition="1173"> 2.2 Feature-based scoring functions To speed up the search, the partial hypotheses are pruned via beam-search in each expanding step. Therefore, the scoring functions applied with the beam-search algorithm are very important. In (Chen et al., 2008), more than 10 additional global features are computed to rank the partial hypothesis list, and this is not an efficient way. In this paper, we propose to directly incorporate the evaluation metrics such as BLEU score to rank the candidates. The scoring functions of this work are derived from the method of lattice Minimum Bayes-risk (MBR) decoding (Tromble et al., 2008) and fast consensus decoding (DeNero et al., 2009), which were originally inspired from N-best MBR decoding (Kumar and Byrne, 2004). From a set of translation candidates E, MBR decoding chooses the translation that has the least expected loss with respect to other candidates. Given a hypothesis set E, under the probability model P(e |f ) , MBR computes the translation e~ as follows: 12 where f is the source sentence, L(e, e′) is the loss function of two translations e and e′ . Suppose that we are interested in maximizing the BLEU score (Papineni et al., 2002) to optimize the translation perfor</context>
<context position="9276" citStr="Tromble et al., 2008" startWordPosition="1443" endWordPosition="1446">he lattice (Tromble et al., 2008). Here, with hypothesis regeneration, this space includes: 1) the translations produced by the baseline decoder either in an N-best list or encoded in a translation lattice, and 2) the translations created by hypothesis regeneration. However, BLEU score is not linear with the length of the hypothesis, which makes the scoring process for each expanding step of hypothesis regeneration very slow. To further speed up the beam search procedure, we use an extension of a linear function of a Taylor approximation to the logarithm of corpus BLEU which was developed by (Tromble et al., 2008). The original BLEU score of two hypotheses e and e’ are computed as follows. 4 1 BLEU e e γ e e ( , ) ( , ) exp( ′ = ′ × log( ( , )) ′ (3) 4 ∑= P n e e n 1 where Pn (e, e′) is the precision of n-grams in the hypothesis e given e’ and γ(e, e′) is a brevity penalty. Let |e |denote the length of e. The corpus log-BLEU gain is defined as follows: where cn ( e , e ′ ) are the counts of the matched ngrams and θn ( 0 ≤ n ≤ 4 ) are constant weights estimated with held-out data. Suppose we have computed the expected ngram counts from the N-best list or translation forest. Then we may extend linear cor</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008. Lattice minimum Bayes-risk decoding for statistical machine translation. In: Proceedings of EMNLP. Hawaii, US. October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>M Simard</author>
<author>S Larkin</author>
<author>J H Johnson</author>
</authors>
<title>NRC’s Portage system for WMT</title>
<date>2007</date>
<booktitle>Proceedings of ACL Workshop on SMT.</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="16168" citStr="Ueffing et al., 2007" startWordPosition="2741" endWordPosition="2744">ms up to n = 4. Our first experiment was carried out over 1000-best lists on Chinese-to-English task. For comparison, we also conducted experiments with rescoring (two-pass) and three-pass hypothesis regeneration with only forward n-gram expansion as proposed in (Chen et al., 2008). In the “rescoring” and “three-pass” systems, we used the same rescoring model. There are 21 rescoring features in total, mainly translation lexicon scores from IBM and HMM models, posterior probabilities for words, n-grams, and sentence length, and language models, etc. For a complete description, please refer to (Ueffing et al., 2007). The results in BLEU-4 are reported in Table 2. 1 http://www.nist.gov/speech/tests/mt 2 http://www.statmt.org/wmt06/ 14 testset NIST’06 NIST’08 baseline 35.70 28.60 rescoring 36.01 28.97 three-pass 35.98 28.99 FCD 36.00 29.10 Fwd. 36.13 29.19 Bwd. 36.11 29.20 Bid. 36.20 29.28 Table 2: Translation performances in BLEU-4(%) over 1000-best lists for Chinese-to-English task: “rescoring” represents the results of rescoring; “threepass”, three-pass hypothesis regeneration with forward n-gram expansion; “FCD”, fast consensus decoding; “Fwd”, the results of hypothesis regeneration with forward n-gram</context>
</contexts>
<marker>Ueffing, Simard, Larkin, Johnson, 2007</marker>
<rawString>N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson. 2007. NRC’s Portage system for WMT 2007. In: Proceedings of ACL Workshop on SMT. Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Efficient multipass decoding for synchronous context free grammars. In:</title>
<date>2008</date>
<booktitle>Proceedings of ACL.</booktitle>
<location>Columbus, US.</location>
<contexts>
<context position="1318" citStr="Zhang and Gildea, 2008" startWordPosition="181" endWordPosition="184">airs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, DeNero et al. (2009) proposed </context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>H. Zhang and D. Gildea. 2008. Efficient multipass decoding for synchronous context free grammars. In: Proceedings of ACL. Columbus, US. June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>