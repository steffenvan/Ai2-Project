<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022312">
<title confidence="0.9949155">
“I Thou Thee, Thou Traitor”:
Predicting Formal vs. Informal Address in English Literature
</title>
<author confidence="0.987304">
Manaal Faruqui
</author>
<affiliation confidence="0.936558">
Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
</affiliation>
<email confidence="0.994707">
manaalfar@gmail.com
</email>
<author confidence="0.975035">
Sebastian Padó
</author>
<affiliation confidence="0.9620795">
Computational Linguistics
Heidelberg University
</affiliation>
<address confidence="0.677997">
Heidelberg, Germany
</address>
<email confidence="0.99586">
pado@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999477">
In contrast to many languages (like Russian or
French), modern English does not distinguish
formal and informal (“T/V”) address overtly,
for example by pronoun choice. We describe
an ongoing study which investigates to what
degree the T/V distinction is recoverable in
English text, and with what textual features it
correlates. Our findings are: (a) human raters
can label English utterances as T or V fairly
well, given sufficient context; (b), lexical cues
can predict T/V almost at human level.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829166666667">
In many Indo-European languages, such as French,
German, or Hindi, there are two pronouns corre-
sponding to the English you. This distinction is
generally referred to as the T/V dichotomy, from
the Latin pronouns tu (informal, T) and vos (formal,
V) (Brown and Gilman, 1960). The V form can
express neutrality or polite distance and is used to
address socially superiors. The T form is employed
for friends or addressees of lower social standing,
and implies solidarity or lack of formality. Some
examples for V pronouns in different languages are
Sie (German), Vous (French), and ❛❆♣ [Aap] (Hindi).
The corresponding T pronouns are du, tu, and �� ♠
[tum].
English used to have a T/V distinction until the
18th century, using you as V and thou as T pronoun.
However, in contemporary English, you has taken
over both uses, and the T/V distinction is not marked
morphosyntactically any more. This makes gener-
ation in English and translation into English easy.
Conversely, the extraction of social information from
texts, and translation from English into languages
with a T/V distinction is very difficult.
In this paper, we investigate the possibility to re-
cover the T/V distinction based on monolingual En-
glish text. We first demonstrate that annotators can
assign T/V labels to English utterances fairly well
(but not perfectly). To identify features that indicate
T and V, we create a parallel English–German corpus
of literary texts and preliminarily identify features
that correlate with formal address (like titles, and
formulaic language) as well as informal address. Our
results could be useful, for example, for MT from
English into languages that distinguish T and V, al-
though we did not test this prediction with the limits
of a short paper.
From a Natural Language Processing point of view,
the recovery of T/V information is an instance of a
more general issue in cross-lingual NLP and ma-
chine translation where for almost every language
pair, there are distinctions that are not expressed
overtly in the source language, but are in the target
language, and must therefore be recovered in some
way. Other examples from the literature include
morphology (Fraser, 2009) and tense (Schiehlen,
1998). The particular problem of T/V address has
been considered in the context of translation into
Japanese (Hobbs and Kameyama, 1990; Kanayama,
2003) and generation (Bateman, 1988), but only
on the context of knowledge-rich methods. As for
data-driven studies, we are only aware of Li and
Yarowsky’s (2008) work, who learn pairs of formal
and informal constructions in Chinese where T/V is
expressed mainly in construction choice.
</bodyText>
<page confidence="0.9903">
467
</page>
<note confidence="0.5841895">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999872333333333">
Naturally, there is a large body of work on T/V
in (socio-)linguistics and translation science, cover-
ing in particular the conditions governing T/V use
in different languages (Kretzenbacher et al., 2006;
Schüpbach et al., 2006) and on the difficulties in
translating them (Ardila, 2003; Künzli, 2010). How-
ever, these studies are generally not computational in
nature, and most of their observations and predictions
are difficult to operationalize.
</bodyText>
<sectionHeader confidence="0.941686" genericHeader="method">
2 A Parallel Corpus of Literary Texts
</sectionHeader>
<subsectionHeader confidence="0.990244">
2.1 Data Selection
</subsectionHeader>
<bodyText confidence="0.9998844">
We chose literary texts to build a parallel corpus for
the investigation of the T/V distinction. The main
reason is that commonly used non-literary collections
like EUROPARL (Koehn, 2005) consist almost ex-
clusively of formal interactions and are therefore of
no use to us. Fortunately, many 18th and 19th century
texts are freely available in several languages.
We identified 115 novels among the texts pro-
vided by Project Gutenberg (English) and Project
Gutenberg-DE (German) that were available in both
languages, with a total of 0.5M sentences per lan-
guage.1 Examples include Dickens’ David Copper-
field or Tolstoy’s Anna Karenina. We decided to
exclude plays and poems as they often include partial
sentences and structures that are difficult to align.
</bodyText>
<subsectionHeader confidence="0.997811">
2.2 Data Preparation
</subsectionHeader>
<bodyText confidence="0.99989">
As the German and English novels come from two
different websites, they were not coherent in their
structure. They were first manually cleaned by delet-
ing the index, prologue, epilogue and Gutenberg li-
cense from the beginning and end of the files. To
some extent the chapter numbers and titles occurring
at the beginning of each chapter were cleared as well.
The files were then formatted to contain one sentence
per line and a blank line was inserted to preserve the
segmentation information.
The sentence splitter and tokenizer provided with
EUROPARL (Koehn, 2005) were used. We ob-
tained a comparable corpus of English and German
novels using the above pre-processing. The files
in the corpus were sentence-aligned using Gargan-
tuan (Braune and Fraser, 2010), an aligner that sup-
ports one-to-many alignments. After obtaining the
</bodyText>
<footnote confidence="0.996595">
1http://www.gutenberg.org and http://gutenberg.spiegel.de/
</footnote>
<table confidence="0.5421554">
ID Position Lemma Cap Category
any du any T
non-initial sie yes V
non-initial ihr no T
non-initial ihr yes V
</table>
<tableCaption confidence="0.9805225">
Table 1: Rules for T/V determination for German personal
pronouns. (Cap: Capitalized)
</tableCaption>
<bodyText confidence="0.999758833333333">
sentence aligned corpus we computed word align-
ments in both English to German and German to En-
glish directions using Giza++ (Och and Ney, 2003).
The corpus was lemmatized and POS-tagged using
TreeTagger (Schmid, 1994). We did not apply a full
parser to keep processing as efficient as possible.
</bodyText>
<subsectionHeader confidence="0.995701">
2.3 T/V Gold Labels for English Utterances
</subsectionHeader>
<bodyText confidence="0.99975775862069">
The goal of creating our corpus is to enable the in-
vestigation of contextual correlates of T/V in English.
In order to do this, we need to decide for as many
English utterances in our corpus as possible whether
they instantiate formal or informal address. Given
that we have a parallel corpus where the German side
overtly realizes T and V, this is a classical case of
annotation projection (Yarowsky and Ngai, 2001):
We transfer the German T/V information onto the
English side to create an annotated English corpus.
This allows us to train and evaluate a monolingual
English classifier for this phenomenon. However,
two problems arise on the way:
Identification of T/V in German pronouns. Ger-
man has three relevant personal pronouns: du, sie,
and ihr. These pronouns indicate T and V, but due to
their ambiguity, it is impossible to simply interpret
their presence or absense as T or V. We developed
four simple disambiguation rules based on position
on the sentence and capitalization, shown in Table 1.
The only unambiguous pronoun is du, which ex-
presses (singular) T (Rule 1). The V pronoun for
singular, sie, doubles as the pronoun for third person
(singular and plural), which is neutral with respect
to T/V. Since TreeTagger does not provide person
information, the only indicator that is available is
capitalization: Sie is 2nd person V. However, since
all words are capitalized in utterance-initial positions,
we only assign the label V in non-initial positions
</bodyText>
<page confidence="0.996244">
468
</page>
<bodyText confidence="0.997742058823529">
(Rule 2).2
Finally, ihr is also ambiguous: non-capitalized, it
is used as T plural (Rule 3); capitalized, it is used as
an archaic alternative to Sie for V plural (Rule 4).
These rules leave a substantial number of instances
of German second person pronouns unlabeled; we
cover somewhat more than half of all pronouns. In
absolute numbers, from 0.5M German sentences we
obtained about 15% labeled sentences (45K for V
and 30K for T). However, this is not a fundamental
problem, since we subsequently used the English
data to train a classifier that is able to process any
English sentence.
Choice of English units to label. On the German
side, we assign the T/V labels to pronouns, and the
most straightforward way of setting up annotation
projection would be to label their word-aligned En-
glish pronouns as T/V. However, pronouns are not
necessarily translated into pronouns; additionally, we
found word alignment accuracy for pronouns, as a
function of word class, to be far from perfect. For
these reasons, we decided to treat complete sentences
as either T or V. This means that sentence alignment
is sufficient for projection, but English sentences can
receive conflicting labels, if a German sentence con-
tains both a T and a V label. However, this occurs
very rarely: of the 76K German sentences with T or
V pronouns, only 515, or less than 1%, contain both.
Our projection on the English side results in 53K V
and 35K T sentences, of which 731 are labeled as
both T and V.3
Finally, from the English labeled sentences we ex-
tracted a training set with 72 novels (63K sentences)
and a test set with 21 novels (15K sentences).4
</bodyText>
<sectionHeader confidence="0.992549" genericHeader="method">
3 Experiment 1: Human Annotation
</sectionHeader>
<bodyText confidence="0.9999406">
The purpose of our first experiment is to investigate
how well the T/V distinction can be made in English
by human raters, and on the basis of what information.
We extracted 100 random sentences from the training
set. Two annotators with advanced knowledge of
</bodyText>
<footnote confidence="0.981288833333333">
2An initial position is defined as a position after a sentence
boundary (POS “$.”) or after a bracket (POS “$(”).
3Our sentence aligner supports one-to-many alignments and
often aligns single German to multiple English sentences.
4The corpus can be downloaded for research purposes from
http://www.nlpado.de/~sebastian/data.shtml.
</footnote>
<table confidence="0.999776">
Acc (Ann1) Acc (Ann2) IAA
No context 63 65 68
In context 70 69 81
</table>
<tableCaption confidence="0.9888525">
Table 2: Manual annotation for T/V on a 100-sentence
sample (Acc: Accuracy, IAA: Inter-annotator agreement)
</tableCaption>
<bodyText confidence="0.99871208">
English were asked to label these sentences as T or V.
In a first round, the sentences were presented in isola-
tion. In a second round, the sentences were presented
with three sentences pre-context and three sentences
post-context. The results in Table 2 show that it is
fairly difficult to annotate the T/V distinction on indi-
vidual sentences since it is not expressed systemati-
cally. At the level of small discourses, the distinction
can be made much more confidently: In context, av-
erage agreement with the gold standard rises from
64% to 70%, and raw inter-annotator agreement goes
up from 68% to 81%.
Concerning the interpretation of these findings, we
note that the two taggers were both native speakers
of languages which make an overt T/V distinction.
Thus, our present findings cannot be construed as
firm evidence that English speakers make a distinc-
tion, even if implicitly. However, they demonstrate
at least that native speakers of such languages can
recover the distinction based solely on the clues in
English text.
An analysis of the annotation errors showed that
many individual sentences can be uttered in both T
and V situations, making it impossible to label them
in isolation:
</bodyText>
<listItem confidence="0.986719">
(1) “And perhaps sometime you may see her.”
</listItem>
<bodyText confidence="0.920412083333333">
This case (gold label: V) is however disambiguated
by looking at the previous sentence, which indicates
the social relation between speaker and addressee:
(2) “And she is a sort of relation of your lord-
ship’s,” said Dawson.
Still, a three-sentence window is often not sufficient,
since the surrounding sentences may be just as unin-
formative. In these cases, global information about
the situation would be necessary.
A second problem is the age of the texts. They are
often difficult to label because they talk about social
situations that are unfamiliar to modern speakers (as
</bodyText>
<page confidence="0.999283">
469
</page>
<bodyText confidence="0.991906">
between aristocratic friends) or where the usage has
changed (as in married couples).
</bodyText>
<sectionHeader confidence="0.984215" genericHeader="method">
4 Experiment 2: Statistical Modeling
</sectionHeader>
<bodyText confidence="0.999924166666667">
Task Setup. In this pilot modeling experiment, we
explore a (limited) set of cues which can be used to
predict the V vs. T dichotomy for English sentences.
Specifically, we use local words (i.e. information
present within the current sentence – similar to the
information available to the human annotators in the
“No context” condition of Experiment 1). We ap-
proach the task by supervised classification, applying
a model acquired from the training set on the test
set. Note, however, that the labeled training data are
acquired automatically through the parallel corpus,
without the need for human annotation.
Statistical Model. We train a Naive Bayes classi-
fier, a simple but effective model for text categoriza-
tion (Domingos and Pazzani, 1997). It predicts the
class c for a sentence s by maximising the product
of the probabilities for the features f given the class,
multiplied by the class probability:
</bodyText>
<equation confidence="0.901099">
cˆ = argmax P(cls) = argmax P(c)P(slc) (3)
c c
= argmax � P(f|c) (4)
c P(c)
fEs
</equation>
<bodyText confidence="0.999991615384616">
We experiment with three sets of features. The first
set consists of words, following the intuition that
some words should be correlated with formal ad-
dress (like titles), while others should indicate infor-
mal address (like first names). The second set con-
sists of part of speech bigrams, to explore whether
this more coarse-grained, but at the same time less
sparse, information can support the T/V decision.
The third set consists of one feature that represents a
semantic class, namely a set of 25 archaic verbs and
pronouns (like hadst or thyself), which we expect
to correlate with old-fashioned T use. All features
are computed by MLE with add-one smoothing as
</bodyText>
<equation confidence="0.9939">
P (f|c) = freq(f,c)+1
freq(c)+1 .
</equation>
<bodyText confidence="0.9709068">
Results. Accuracies are shown in Table 3. A ran-
dom baseline is at 50%, and the majority class (V)
corresponds to 60%. The Naive Bayes models signif-
icantly outperform the frequency baselines at up to
67.0%; however, only the difference between the best
</bodyText>
<table confidence="0.99895325">
Model Accuracy
Random BL 50.0
Frequency BL 60.1
Words 66.1
Words + POS 65.0
Words + Archaic 67.0
Human (no context) 64
Human (in context) 70
</table>
<tableCaption confidence="0.999822">
Table 3: NB classifier results for the T/V distinction
</tableCaption>
<bodyText confidence="0.9998362">
(Words+Archaic) and the worst (Words+POS) model
is significant according to a X2 test. Thus, POS fea-
tures tend to hurt, and the archaic feature helps, even
though it technically overcounts evidence.5
The Naive Bayes model notably performs at a
roughly human level, better than human annotators
on the same setup (no context sentences), but worse
than humans that have more context at their disposal.
Overall, however, the T/V distinction appears to be a
fairly difficult one. An important part of the problem
is the absence of strong indicators in many sentences,
in particular short ones (cf. Example 1). In contrast
to most text categorization tasks, there is no topi-
cal difference between the two categories: T and V
can both co-occur with words from practically any
domain.
Table 4, which lists the top ten words for T and
V (ranked by the ratio of probabilities for the two
classes), shows that among these indicators, many
are furthermore names of persons from particular
novels which are systematically addressed formally
(like Phileas Fogg from Jules Vernes’ In eighty days
around the world) or informally (like Mowgli, Baloo,
and Bagheera from Rudyard Kipling’s Jungle Book).
Nevertheless, some features point towards more
general patterns. In particular, we observe ti-
tles among the V-indicators (gentlemen, madam,
ma+’am) as well as formulaic language (Permit
(me)). Indicators for T seem to be much more general,
with the expected exception of archaic thou forms.
</bodyText>
<sectionHeader confidence="0.992813" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9975655">
In this paper, we have reported on an ongoing study
of the formal/informal (T/V) address distinction in
</bodyText>
<footnote confidence="0.995279333333333">
5We experimented with logistic regression models, but were
unable to obtain better performance, probably because we intro-
duced a frequency threshold to limit the feature set size.
</footnote>
<page confidence="0.986368">
470
</page>
<table confidence="0.999917230769231">
Top 10 words for V Top 10 words for T
Word w P(w|V ) Word w P(w1T)
P(w1T) P(wIV)
Fogg 49.7 Thee 67.2
Oswald 32.5 Trot 46.8
Ma 31.8 Bagheera 37.7
Gentlemen 25.2 Khan 34.7
Madam 24.2 Mowgli 33.2
Parfenovitch 23.2 Baloo 30.2
Monsieur 22.6 Sahib 30.2
Fix 22.5 Clare 29.7
Permit 22.5 didst 27.7
’am 22.4 Reinhard 27.2
</table>
<tableCaption confidence="0.99991">
Table 4: Words that are indicative for T or V
</tableCaption>
<bodyText confidence="0.999940394736842">
modern English, where it is not determined through
pronoun choice or other overt means. We see this task
as an instance of the general problem of recovering
“hidden” information that is not expressed overtly.
We have created a parallel German-English cor-
pus and have used the information provided by the
German pronouns to induce T/V labels for English
sentences. In a manual annotation study for English,
annotators find the form of address very difficult to
determine for individual sentences, but can draw this
information from broader English discourse context.
Since our annotators are not native speakers of En-
glish, but of languages that make the T/V distinction,
we can conclude that English provides lexical cues
that can be interpreted as to the form of address, but
cannot speak to the question whether English speak-
ers in fact have a concept of this distinction.
In a first statistical analysis, we found that lexical
cues from the sentence can be used to predict the
form of address automatically, although not yet on a
very satisfactory level.
Our analyses suggest a number of directions for
future research. On the technical level, we would like
to apply a sequence model to account for the depen-
decies among sentences, and obtain more meaningful
features for formal and informal address. In order
to remove idiosyncratic features like names, we will
only consider features that occur in several novels;
furthermore, we will group words using distributional
clustering methods (Clark, 2003) and predict T/V
based on cluster probabilities.
The conceptually most promising direction, how-
ever, is the induction of social networks in such nov-
els (Elson et al., 2010): Information on the social re-
lationship between a speaker and an addressee should
provide global constraints on all instances of com-
munications between them, and predict the form of
address much more reliably than word features can.
</bodyText>
<sectionHeader confidence="0.998559" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<copyright confidence="0.527007">
Manaal Faruqui has been partially supported by a
Microsoft Research India Travel Grant.
</copyright>
<sectionHeader confidence="0.99699" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999837394736842">
John Ardila. 2003. (Non-Deictic, Socio-Expressive) T-
/V-Pronoun Distinction in Spanish/English Formal Lo-
cutionary Acts. Forum for Modern Language Studies,
39(1):74–86.
John A. Bateman. 1988. Aspects of clause politeness in
japanese: An extended inquiry semantics treatment. In
Proceedings of the 26th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 147–154,
Buffalo, New York.
Fabienne Braune and Alexander Fraser. 2010. Improved
unsupervised sentence alignment for symmetrical and
asymmetrical parallel corpora. In Coling 2010: Posters,
pages 81–89, Beijing, China.
Roger Brown and Albert Gilman. 1960. The pronouns
of power and solidarity. In Thomas A. Sebeok, editor,
Style in Language, pages 253–277. MIT Press, Cam-
bridge, MA.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 59–66, Budapest, Hungary.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple Bayesian classifier under zero-
one loss. Machine Learning, 29(2–3):103–130.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 138–147,
Uppsala, Sweden.
Alexander Fraser. 2009. Experiments in morphosyntactic
processing for translating to and from German. In Pro-
ceedings of the Fourth Workshop on Statistical Machine
Translation, pages 115–119, Athens, Greece.
Jerry Hobbs and Megumi Kameyama. 1990. Translation
by abduction. In Proceedings of the 13th International
Conference on Computational Linguistics, Helsinki,
Finland.
</reference>
<page confidence="0.981827">
471
</page>
<reference confidence="0.999782066666667">
Hiroshi Kanayama. 2003. Paraphrasing rules for auto-
matic evaluation of translation into japanese. In Pro-
ceedings of the Second International Workshop on Para-
phrasing, pages 88–93, Sapporo, Japan.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Proceedings of the 10th
Machine Translation Summit, pages 79–86, Phuket,
Thailand.
Heinz L. Kretzenbacher, Michael Clyne, and Doris Schüp-
bach. 2006. Pronominal Address in German: Rules,
Anarchy and Embarrassment Potential. Australian Re-
view of Applied Linguistics, 39(2):17.1–17.18.
Alexander Künzli. 2010. Address pronouns as a problem
in French-Swedish translation and translation revision.
Babel, 55(4):364–380.
Zhifei Li and David Yarowsky. 2008. Mining and mod-
eling relations between formal and informal Chinese
phrases from web corpora. In Proceedings of the 2008
Conference on Empirical Methods in Natural Language
Processing, pages 1031–1040, Honolulu, Hawaii.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
Michael Schiehlen. 1998. Learning tense translation
from bilingual corpora. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Com-
putational Linguistics, pages 1183–1187, Montreal,
Canada.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44–49.
Doris Schüpbach, John Hajek, Jane Warren, Michael
Clyne, Heinz Kretzenbacher, and Catrin Norrby. 2006.
A cross-linguistic comparison of address pronoun use in
four European languages: Intralingual and interlingual
dimensions. In Proceedings of the Annual Meeting of
the Australian Linguistic Society, Brisbane, Australia.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the 2nd Meeting of the North American Chapter of
the Association of Computational Linguistics, pages
200–207, Pittsburgh, PA.
</reference>
<page confidence="0.998499">
472
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.258851">
<title confidence="0.977583">I Thou Thee, Thou Traitor”: Predicting Formal vs. Informal Address in English Literature</title>
<author confidence="0.775545">Manaal</author>
<affiliation confidence="0.960428666666667">Computer Science and Indian Institute of Kharagpur,</affiliation>
<email confidence="0.999574">manaalfar@gmail.com</email>
<author confidence="0.892047">Sebastian</author>
<affiliation confidence="0.876118">Computational</affiliation>
<address confidence="0.702533">Heidelberg Heidelberg,</address>
<email confidence="0.998821">pado@cl.uni-heidelberg.de</email>
<abstract confidence="0.999702416666667">In contrast to many languages (like Russian or French), modern English does not distinguish formal and informal (“T/V”) address overtly, for example by pronoun choice. We describe an ongoing study which investigates to what degree the T/V distinction is recoverable in English text, and with what textual features it correlates. Our findings are: (a) human raters can label English utterances as T or V fairly well, given sufficient context; (b), lexical cues can predict T/V almost at human level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Ardila</author>
</authors>
<title>(Non-Deictic, Socio-Expressive) T/V-Pronoun Distinction in Spanish/English Formal Locutionary Acts. Forum for Modern Language Studies,</title>
<date>2003</date>
<contexts>
<context position="3962" citStr="Ardila, 2003" startWordPosition="609" endWordPosition="610">rk, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in particular the conditions governing T/V use in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and on the difficulties in translating them (Ardila, 2003; Künzli, 2010). However, these studies are generally not computational in nature, and most of their observations and predictions are difficult to operationalize. 2 A Parallel Corpus of Literary Texts 2.1 Data Selection We chose literary texts to build a parallel corpus for the investigation of the T/V distinction. The main reason is that commonly used non-literary collections like EUROPARL (Koehn, 2005) consist almost exclusively of formal interactions and are therefore of no use to us. Fortunately, many 18th and 19th century texts are freely available in several languages. We identified 115 </context>
</contexts>
<marker>Ardila, 2003</marker>
<rawString>John Ardila. 2003. (Non-Deictic, Socio-Expressive) T/V-Pronoun Distinction in Spanish/English Formal Locutionary Acts. Forum for Modern Language Studies, 39(1):74–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bateman</author>
</authors>
<title>Aspects of clause politeness in japanese: An extended inquiry semantics treatment.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>147--154</pages>
<location>Buffalo, New York.</location>
<contexts>
<context position="3221" citStr="Bateman, 1988" startWordPosition="500" endWordPosition="501">guage Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in particular the conditions governing </context>
</contexts>
<marker>Bateman, 1988</marker>
<rawString>John A. Bateman. 1988. Aspects of clause politeness in japanese: An extended inquiry semantics treatment. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 147–154, Buffalo, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Braune</author>
<author>Alexander Fraser</author>
</authors>
<title>Improved unsupervised sentence alignment for symmetrical and asymmetrical parallel corpora.</title>
<date>2010</date>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>81--89</pages>
<location>Beijing, China.</location>
<contexts>
<context position="5718" citStr="Braune and Fraser, 2010" startWordPosition="886" endWordPosition="889">cleaned by deleting the index, prologue, epilogue and Gutenberg license from the beginning and end of the files. To some extent the chapter numbers and titles occurring at the beginning of each chapter were cleared as well. The files were then formatted to contain one sentence per line and a blank line was inserted to preserve the segmentation information. The sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005) were used. We obtained a comparable corpus of English and German novels using the above pre-processing. The files in the corpus were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments. After obtaining the 1http://www.gutenberg.org and http://gutenberg.spiegel.de/ ID Position Lemma Cap Category any du any T non-initial sie yes V non-initial ihr no T non-initial ihr yes V Table 1: Rules for T/V determination for German personal pronouns. (Cap: Capitalized) sentence aligned corpus we computed word alignments in both English to German and German to English directions using Giza++ (Och and Ney, 2003). The corpus was lemmatized and POS-tagged using TreeTagger (Schmid, 1994). We did not apply a full parser to keep processing as eff</context>
</contexts>
<marker>Braune, Fraser, 2010</marker>
<rawString>Fabienne Braune and Alexander Fraser. 2010. Improved unsupervised sentence alignment for symmetrical and asymmetrical parallel corpora. In Coling 2010: Posters, pages 81–89, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
<author>Albert Gilman</author>
</authors>
<title>The pronouns of power and solidarity.</title>
<date>1960</date>
<booktitle>Style in Language,</booktitle>
<pages>253--277</pages>
<editor>In Thomas A. Sebeok, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1111" citStr="Brown and Gilman, 1960" startWordPosition="158" endWordPosition="161">ce. We describe an ongoing study which investigates to what degree the T/V distinction is recoverable in English text, and with what textual features it correlates. Our findings are: (a) human raters can label English utterances as T or V fairly well, given sufficient context; (b), lexical cues can predict T/V almost at human level. 1 Introduction In many Indo-European languages, such as French, German, or Hindi, there are two pronouns corresponding to the English you. This distinction is generally referred to as the T/V dichotomy, from the Latin pronouns tu (informal, T) and vos (formal, V) (Brown and Gilman, 1960). The V form can express neutrality or polite distance and is used to address socially superiors. The T form is employed for friends or addressees of lower social standing, and implies solidarity or lack of formality. Some examples for V pronouns in different languages are Sie (German), Vous (French), and ❛❆♣ [Aap] (Hindi). The corresponding T pronouns are du, tu, and �� ♠ [tum]. English used to have a T/V distinction until the 18th century, using you as V and thou as T pronoun. However, in contemporary English, you has taken over both uses, and the T/V distinction is not marked morphosyntacti</context>
</contexts>
<marker>Brown, Gilman, 1960</marker>
<rawString>Roger Brown and Albert Gilman. 1960. The pronouns of power and solidarity. In Thomas A. Sebeok, editor, Style in Language, pages 253–277. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>59--66</pages>
<location>Budapest, Hungary.</location>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, pages 59–66, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Michael J Pazzani</author>
</authors>
<title>On the optimality of the simple Bayesian classifier under zeroone loss.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="12979" citStr="Domingos and Pazzani, 1997" startWordPosition="2077" endWordPosition="2080">. T dichotomy for English sentences. Specifically, we use local words (i.e. information present within the current sentence – similar to the information available to the human annotators in the “No context” condition of Experiment 1). We approach the task by supervised classification, applying a model acquired from the training set on the test set. Note, however, that the labeled training data are acquired automatically through the parallel corpus, without the need for human annotation. Statistical Model. We train a Naive Bayes classifier, a simple but effective model for text categorization (Domingos and Pazzani, 1997). It predicts the class c for a sentence s by maximising the product of the probabilities for the features f given the class, multiplied by the class probability: cˆ = argmax P(cls) = argmax P(c)P(slc) (3) c c = argmax � P(f|c) (4) c P(c) fEs We experiment with three sets of features. The first set consists of words, following the intuition that some words should be correlated with formal address (like titles), while others should indicate informal address (like first names). The second set consists of part of speech bigrams, to explore whether this more coarse-grained, but at the same time le</context>
</contexts>
<marker>Domingos, Pazzani, 1997</marker>
<rawString>Pedro Domingos and Michael J. Pazzani. 1997. On the optimality of the simple Bayesian classifier under zeroone loss. Machine Learning, 29(2–3):103–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elson</author>
<author>Nicholas Dames</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting social networks from literary fiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>138--147</pages>
<location>Uppsala,</location>
<marker>Elson, Dames, McKeown, 2010</marker>
<rawString>David Elson, Nicholas Dames, and Kathleen McKeown. 2010. Extracting social networks from literary fiction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 138–147, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
</authors>
<title>Experiments in morphosyntactic processing for translating to and from German.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>115--119</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="3016" citStr="Fraser, 2009" startWordPosition="470" endWordPosition="471">ddress. Our results could be useful, for example, for MT from English into languages that distinguish T and V, although we did not test this prediction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 1</context>
</contexts>
<marker>Fraser, 2009</marker>
<rawString>Alexander Fraser. 2009. Experiments in morphosyntactic processing for translating to and from German. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115–119, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
<author>Megumi Kameyama</author>
</authors>
<title>Translation by abduction.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="3173" citStr="Hobbs and Kameyama, 1990" startWordPosition="492" endWordPosition="495">ction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, </context>
</contexts>
<marker>Hobbs, Kameyama, 1990</marker>
<rawString>Jerry Hobbs and Megumi Kameyama. 1990. Translation by abduction. In Proceedings of the 13th International Conference on Computational Linguistics, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
</authors>
<title>Paraphrasing rules for automatic evaluation of translation into japanese.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Workshop on Paraphrasing,</booktitle>
<pages>88--93</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3190" citStr="Kanayama, 2003" startWordPosition="496" endWordPosition="497"> short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in parti</context>
</contexts>
<marker>Kanayama, 2003</marker>
<rawString>Hiroshi Kanayama. 2003. Paraphrasing rules for automatic evaluation of translation into japanese. In Proceedings of the Second International Workshop on Paraphrasing, pages 88–93, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="4369" citStr="Koehn, 2005" startWordPosition="671" endWordPosition="672">lation science, covering in particular the conditions governing T/V use in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and on the difficulties in translating them (Ardila, 2003; Künzli, 2010). However, these studies are generally not computational in nature, and most of their observations and predictions are difficult to operationalize. 2 A Parallel Corpus of Literary Texts 2.1 Data Selection We chose literary texts to build a parallel corpus for the investigation of the T/V distinction. The main reason is that commonly used non-literary collections like EUROPARL (Koehn, 2005) consist almost exclusively of formal interactions and are therefore of no use to us. Fortunately, many 18th and 19th century texts are freely available in several languages. We identified 115 novels among the texts provided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per language.1 Examples include Dickens’ David Copperfield or Tolstoy’s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align. 2.2 Data Preparation As the Ger</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the 10th Machine Translation Summit, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinz L Kretzenbacher</author>
<author>Michael Clyne</author>
<author>Doris Schüpbach</author>
</authors>
<date>2006</date>
<booktitle>Pronominal Address in German: Rules, Anarchy and Embarrassment Potential. Australian Review of Applied Linguistics,</booktitle>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="3879" citStr="Kretzenbacher et al., 2006" startWordPosition="594" endWordPosition="597">wledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in particular the conditions governing T/V use in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and on the difficulties in translating them (Ardila, 2003; Künzli, 2010). However, these studies are generally not computational in nature, and most of their observations and predictions are difficult to operationalize. 2 A Parallel Corpus of Literary Texts 2.1 Data Selection We chose literary texts to build a parallel corpus for the investigation of the T/V distinction. The main reason is that commonly used non-literary collections like EUROPARL (Koehn, 2005) consist almost exclusively of formal interactions and are therefore of no use to us. Fortunately, many 18th a</context>
</contexts>
<marker>Kretzenbacher, Clyne, Schüpbach, 2006</marker>
<rawString>Heinz L. Kretzenbacher, Michael Clyne, and Doris Schüpbach. 2006. Pronominal Address in German: Rules, Anarchy and Embarrassment Potential. Australian Review of Applied Linguistics, 39(2):17.1–17.18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Künzli</author>
</authors>
<title>Address pronouns as a problem in French-Swedish translation and translation revision.</title>
<date>2010</date>
<journal>Babel,</journal>
<volume>55</volume>
<issue>4</issue>
<contexts>
<context position="3977" citStr="Künzli, 2010" startWordPosition="611" endWordPosition="612">pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in particular the conditions governing T/V use in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and on the difficulties in translating them (Ardila, 2003; Künzli, 2010). However, these studies are generally not computational in nature, and most of their observations and predictions are difficult to operationalize. 2 A Parallel Corpus of Literary Texts 2.1 Data Selection We chose literary texts to build a parallel corpus for the investigation of the T/V distinction. The main reason is that commonly used non-literary collections like EUROPARL (Koehn, 2005) consist almost exclusively of formal interactions and are therefore of no use to us. Fortunately, many 18th and 19th century texts are freely available in several languages. We identified 115 novels among th</context>
</contexts>
<marker>Künzli, 2010</marker>
<rawString>Alexander Künzli. 2010. Address pronouns as a problem in French-Swedish translation and translation revision. Babel, 55(4):364–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>David Yarowsky</author>
</authors>
<title>Mining and modeling relations between formal and informal Chinese phrases from web corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1031--1040</pages>
<location>Honolulu, Hawaii.</location>
<marker>Li, Yarowsky, 2008</marker>
<rawString>Zhifei Li and David Yarowsky. 2008. Mining and modeling relations between formal and informal Chinese phrases from web corpora. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1031–1040, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6186" citStr="Och and Ney, 2003" startWordPosition="957" endWordPosition="960"> of English and German novels using the above pre-processing. The files in the corpus were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments. After obtaining the 1http://www.gutenberg.org and http://gutenberg.spiegel.de/ ID Position Lemma Cap Category any du any T non-initial sie yes V non-initial ihr no T non-initial ihr yes V Table 1: Rules for T/V determination for German personal pronouns. (Cap: Capitalized) sentence aligned corpus we computed word alignments in both English to German and German to English directions using Giza++ (Och and Ney, 2003). The corpus was lemmatized and POS-tagged using TreeTagger (Schmid, 1994). We did not apply a full parser to keep processing as efficient as possible. 2.3 T/V Gold Labels for English Utterances The goal of creating our corpus is to enable the investigation of contextual correlates of T/V in English. In order to do this, we need to decide for as many English utterances in our corpus as possible whether they instantiate formal or informal address. Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schiehlen</author>
</authors>
<title>Learning tense translation from bilingual corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>1183--1187</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3044" citStr="Schiehlen, 1998" startWordPosition="474" endWordPosition="475"> be useful, for example, for MT from English into languages that distinguish T and V, although we did not test this prediction with the limits of a short paper. From a Natural Language Processing point of view, the recovery of T/V information is an instance of a more general issue in cross-lingual NLP and machine translation where for almost every language pair, there are distinctions that are not expressed overtly in the source language, but are in the target language, and must therefore be recovered in some way. Other examples from the literature include morphology (Fraser, 2009) and tense (Schiehlen, 1998). The particular problem of T/V address has been considered in the context of translation into Japanese (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation (Bateman, 1988), but only on the context of knowledge-rich methods. As for data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 19-24, 2011. c�2011 Associati</context>
</contexts>
<marker>Schiehlen, 1998</marker>
<rawString>Michael Schiehlen. 1998. Learning tense translation from bilingual corpora. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 1183–1187, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="6260" citStr="Schmid, 1994" startWordPosition="969" endWordPosition="970">corpus were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments. After obtaining the 1http://www.gutenberg.org and http://gutenberg.spiegel.de/ ID Position Lemma Cap Category any du any T non-initial sie yes V non-initial ihr no T non-initial ihr yes V Table 1: Rules for T/V determination for German personal pronouns. (Cap: Capitalized) sentence aligned corpus we computed word alignments in both English to German and German to English directions using Giza++ (Och and Ney, 2003). The corpus was lemmatized and POS-tagged using TreeTagger (Schmid, 1994). We did not apply a full parser to keep processing as efficient as possible. 2.3 T/V Gold Labels for English Utterances The goal of creating our corpus is to enable the investigation of contextual correlates of T/V in English. In order to do this, we need to decide for as many English utterances in our corpus as possible whether they instantiate formal or informal address. Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and Ngai, 2001): We transfer the German T/V information onto the English side </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doris Schüpbach</author>
<author>John Hajek</author>
<author>Jane Warren</author>
<author>Michael Clyne</author>
<author>Heinz Kretzenbacher</author>
<author>Catrin Norrby</author>
</authors>
<date>2006</date>
<contexts>
<context position="3904" citStr="Schüpbach et al., 2006" startWordPosition="598" endWordPosition="601">data-driven studies, we are only aware of Li and Yarowsky’s (2008) work, who learn pairs of formal and informal constructions in Chinese where T/V is expressed mainly in construction choice. 467 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 467–472, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Naturally, there is a large body of work on T/V in (socio-)linguistics and translation science, covering in particular the conditions governing T/V use in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and on the difficulties in translating them (Ardila, 2003; Künzli, 2010). However, these studies are generally not computational in nature, and most of their observations and predictions are difficult to operationalize. 2 A Parallel Corpus of Literary Texts 2.1 Data Selection We chose literary texts to build a parallel corpus for the investigation of the T/V distinction. The main reason is that commonly used non-literary collections like EUROPARL (Koehn, 2005) consist almost exclusively of formal interactions and are therefore of no use to us. Fortunately, many 18th and 19th century texts are</context>
</contexts>
<marker>Schüpbach, Hajek, Warren, Clyne, Kretzenbacher, Norrby, 2006</marker>
<rawString>Doris Schüpbach, John Hajek, Jane Warren, Michael Clyne, Heinz Kretzenbacher, and Catrin Norrby. 2006.</rawString>
</citation>
<citation valid="false">
<title>A cross-linguistic comparison of address pronoun use in four European languages: Intralingual and interlingual dimensions.</title>
<booktitle>In Proceedings of the Annual Meeting of the Australian Linguistic Society,</booktitle>
<location>Brisbane, Australia.</location>
<marker></marker>
<rawString>A cross-linguistic comparison of address pronoun use in four European languages: Intralingual and interlingual dimensions. In Proceedings of the Annual Meeting of the Australian Linguistic Society, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>200--207</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="6797" citStr="Yarowsky and Ngai, 2001" startWordPosition="1061" endWordPosition="1064">nd Ney, 2003). The corpus was lemmatized and POS-tagged using TreeTagger (Schmid, 1994). We did not apply a full parser to keep processing as efficient as possible. 2.3 T/V Gold Labels for English Utterances The goal of creating our corpus is to enable the investigation of contextual correlates of T/V in English. In order to do this, we need to decide for as many English utterances in our corpus as possible whether they instantiate formal or informal address. Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and Ngai, 2001): We transfer the German T/V information onto the English side to create an annotated English corpus. This allows us to train and evaluate a monolingual English classifier for this phenomenon. However, two problems arise on the way: Identification of T/V in German pronouns. German has three relevant personal pronouns: du, sie, and ihr. These pronouns indicate T and V, but due to their ambiguity, it is impossible to simply interpret their presence or absense as T or V. We developed four simple disambiguation rules based on position on the sentence and capitalization, shown in Table 1. The only </context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proceedings of the 2nd Meeting of the North American Chapter of the Association of Computational Linguistics, pages 200–207, Pittsburgh, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>