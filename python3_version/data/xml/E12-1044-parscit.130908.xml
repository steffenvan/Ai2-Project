<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989722">
Coordination Structure Analysis using Dual Decomposition
</title>
<author confidence="0.90604">
Atsushi Hanamoto 1 Takuya Matsuzaki 1 Jun’ichi Tsujii 2
</author>
<affiliation confidence="0.689605">
1. Department of Computer Science, University of Tokyo, Japan
</affiliation>
<address confidence="0.216582">
2. Web Search &amp; Mining Group, Microsoft Research Asia, China
</address>
<email confidence="0.963065">
{hanamoto, matuzaki}@is.s.u-tokyo.ac.jp
jtsujii@microsoft.com
</email>
<sectionHeader confidence="0.996573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998125">
Coordination disambiguation remains a dif-
ficult sub-problem in parsing despite the
frequency and importance of coordination
structures. We propose a method for disam-
biguating coordination structures. In this
method, dual decomposition is used as a
framework to take advantage of both HPSG
parsing and coordinate structure analysis
with alignment-based local features. We
evaluate the performance of the proposed
method on the Genia corpus and the Wall
Street Journal portion of the Penn Tree-
bank. Results show it increases the per-
centage of sentences in which coordination
structures are detected correctly, compared
with each of the two algorithms alone.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999861509090909">
Coordination structures often give syntactic ambi-
guity in natural language. Although a wrong anal-
ysis of a coordination structure often leads to a
totally garbled parsing result, coordination disam-
biguation remains a difficult sub-problem in pars-
ing, even for state-of-the-art parsers.
One approach to solve this problem is a gram-
matical approach. This approach, however, of-
ten fails in noun and adjective coordinations be-
cause there are many possible structures in these
coordinations that are grammatically correct. For
example, a noun sequence of the form “n0 n1
and n2 n3” has as many as five possible struc-
tures (Resnik, 1999). Therefore, a grammatical
approach is not sufficient to disambiguate coor-
dination structures. In fact, the Stanford parser
(Klein and Manning, 2003) and Enju (Miyao and
Tsujii, 2004) fail to disambiguate a sentence I am
a freshman advertising and marketing major. Ta-
ble 1 shows the output from them and the correct
coordination structure.
The coordination structure above is obvious to
humans because there is a symmetry of conjuncts
(-ing) in the sentence. Coordination structures of-
ten have such structural and semantic symmetry
of conjuncts. One approach is to capture local
symmetry of conjuncts. However, this approach
fails in VP and sentential coordinations, which
can easily be detected by a grammatical approach.
This is because conjuncts in these coordinations
do not necessarily have local symmetry.
It is therefore natural to think that consider-
ing both the syntax and local symmetry of con-
juncts would lead to a more accurate analysis.
However, it is difficult to consider both of them
in a dynamic programming algorithm, which has
been often used for each of them, because it ex-
plodes the computational and implementational
complexity. Thus, previous studies on coordina-
tion disambiguation often dealt only with a re-
stricted form of coordination (e.g. noun phrases)
or used a heuristic approach for simplicity.
In this paper, we present a statistical analysis
model for coordination disambiguation that uses
the dual decomposition as a framework. We con-
sider both of the syntax, and structural and se-
mantic symmetry of conjuncts so that it outper-
forms existing methods that consider only either
of them. Moreover, it is still simple and requires
only O(n4) time per iteration, where n is the num-
ber of words in a sentence. This is equal to that
of coordination structure analysis with alignment-
based local features. The overall system still has a
quite simple structure because we need just slight
modifications of existing models in this approach,
</bodyText>
<page confidence="0.974417">
430
</page>
<note confidence="0.65679775">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 430–438,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
Stanford parser/Enju
I am a ( freshman advertising ) and (
marketing major)
Correct coordination structure
I am a freshman ( ( advertising and mar-
keting ) major)
</note>
<tableCaption confidence="0.96731">
Table 1: Output from the Stanford parser, Enju and the
correct coordination structure
</tableCaption>
<bodyText confidence="0.999866230769231">
so we can easily add other modules or features for
future.
The structure of this paper is as follows. First,
we describe three basic methods required in the
technique we propose: 1) coordination structure
analysis with alignment-based local features, 2)
HPSG parsing, and 3) dual decomposition. Fi-
nally, we show experimental results that demon-
strate the effectiveness of our approach. We com-
pare three methods: coordination structure anal-
ysis with alignment-based local features, HPSG
parsing, and the dual-decomposition-based ap-
proach that combines both.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999283">
Many previous studies for coordination disam-
biguation have focused on a particular type of NP
coordination (Hogan, 2007). Resnik (1999) dis-
ambiguated coordination structures by using se-
mantic similarity of the conjuncts in a taxonomy.
He dealt with two kinds of patterns, [n0 n1 and
n2 n3] and [n1 and n2 n3], where ni are all nouns.
He detected coordination structures based on sim-
ilarity of form, meaning and conceptual associa-
tion between n1 and n2 and between n1 and n3.
Nakov and Hearst (2005) used the Web as a train-
ing set and applied it to a task that is similar to
Resnik’s.
In terms of integrating coordination disam-
biguation with an existing parsing model, our ap-
proach resembles the approach by Hogan (2007).
She detected noun phrase coordinations by find-
ing symmetry in conjunct structure and the depen-
dency between the lexical heads of the conjuncts.
They are used to rerank the n-best outputs of the
Bikel parser (2004), whereas two models interact
with each other in our method.
Shimbo and Hara (2007) proposed an
alignment-based method for detecting and dis-
ambiguating non-nested coordination structures.
They disambiguated coordination structures
based on the edit distance between two conjuncts.
Hara et al. (2009) extended the method, dealing
with nested coordinations as well. We used their
method as one of the two sub-models.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="method">
3 Background
</sectionHeader>
<subsectionHeader confidence="0.998871">
3.1 Coordination structure analysis with
alignment-based local features
</subsectionHeader>
<bodyText confidence="0.919845125">
Coordination structure analysis with alignment-
based local features (Hara et al., 2009) is a hy-
brid approach to coordination disambiguation that
combines a simple grammar to ensure consistent
global structure of coordinations in a sentence,
and features based on sequence alignment to cap-
ture local symmetry of conjuncts. In this section,
we describe the method briefly.
A sentence is denoted by x = x1...xk, where xi
is the i-th word of x. A coordination boundaries
set is denoted by y = y1...yk, where
yi = { (bl, el, br, er) (if xi is a coordinating
conjunction having left
conjunct xb,...xe, and
right conjunct xb,,...xe,.)
null (otherwise)
In other words, yi has a non-null value
only when it is a coordinating conjunction.
For example, a sentence I bought books and
stationary has a coordination boundaries set
(null, null, null, (3, 3, 5, 5), null).
The score of a coordination boundaries set is
defined as the sum of score of all coordinating
conjunctions in the sentence.
</bodyText>
<equation confidence="0.998854166666666">
k
score(x,y) = score(x, ym)
m=1
k
= w · f(x, ym) (1)
m=1
</equation>
<bodyText confidence="0.999924888888889">
where f(x, ym) is a real-valued feature vector of
the coordination conjunct xm. We used almost the
same feature set as Hara et al. (2009): namely, the
surface word, part-of-speech, suffix and prefix of
the words, and their combinations. We used the
averaged perceptron to tune the weight vector w.
Hara et al. (2009) proposed to use a context-
free grammar to find a properly nested coordina-
tion structure. That is, the scoring function Eq (1)
</bodyText>
<page confidence="0.997684">
431
</page>
<figure confidence="0.7220976">
COORD Coordination.
CJT Conjunct.
N Non-coordination.
CC Coordinating conjunction like “and”.
W Any word.
</figure>
<tableCaption confidence="0.934154">
Table 2: Non-terminals
</tableCaption>
<subsectionHeader confidence="0.857573">
Rules for coordinations:
</subsectionHeader>
<bodyText confidence="0.618018">
COORDZ,m → CJTZ,jCCj+1,k_1CJTk,m
</bodyText>
<subsectionHeader confidence="0.818859">
Rules for conjuncts:
</subsectionHeader>
<bodyText confidence="0.59194">
CJTZ,j → (COORD|N)Z,j
</bodyText>
<subsectionHeader confidence="0.584487">
Rules for non-coordinations:
</subsectionHeader>
<equation confidence="0.996065125">
NZ,k → COORDZ,jNj+1,k
NZ,j → WZ,Z(COORD|N)Z+1,j
NZ,Z → WZ,Z
Rules for pre-terminals:
CCZ,Z → (and|or|but|,|; |+|+/−)Z
CCZ,Z+1 → (, |; )Z(and|or|but)Z+1
CCZ,Z+2 → (as)Z(well)Z+1(as)Z+2
WZ,Z → ∗Z
</equation>
<tableCaption confidence="0.964945">
Table 3: Production rules
</tableCaption>
<bodyText confidence="0.999959391304348">
is only defined on the coordination structures that
are licensed by the grammar. We only slightly ex-
tended their grammar for convering more variety
of coordinating conjunctions.
Table 2 and Table 3 show the non-terminals and
production rules used in the model. The only ob-
jective of the grammar is to ensure the consistency
of two or more coordinations in a sentence, which
means for any two coordinations they must be ei-
ther non-overlapping or nested coordinations. We
use a bottom-up chart parsing algorithm to out-
put the coordination boundaries with the highest
score. Note that these production rules don’t need
to be isomorphic to those of HPSG parsing and
actually they aren’t. This is because the two meth-
ods interact only through dual decomposition and
the search spaces defined by the methods are con-
sidered separately.
This method requires O(n4) time, where n is
the number of words. This is because there are
O(n2) possible coordination structures in a sen-
tence, and the method requires O(n2) time to get
a feature vector of each coordination structure.
</bodyText>
<subsectionHeader confidence="0.996408">
3.2 HPSG parsing
</subsectionHeader>
<bodyText confidence="0.809097">
HPSG (Pollard and Sag, 1994) is one of the
linguistic theories based on lexicalized grammar
</bodyText>
<figureCaption confidence="0.954732">
Figure 1: subject-head schema (left) and head-
</figureCaption>
<figure confidence="0.50982075">
FgbjetHdh(left)He
complement schema (right); taken from Miyao et al.
Complement Schema (right)
(2004).
</figure>
<figureCaption confidence="0.200729">
formalism. In a lexicalized grammar, quite a
</figureCaption>
<bodyText confidence="0.91744435">
nts emic o a n, th
ud xps pragntstr.
small numbers of schemata are used to explain
Figu 2 pets h SbjHd hema
general grammatical constraints, compared with
ad the HedComlet Shea1 defi in
other theories. On the other hand, rich word-
(Pld d S 1994) I d t e
specific characteristics are embedded in lexical
eal tait hta l id hig f
entries. Both of schemata an lexical entries
feature alues and no instantated values
are represented by typed feature structures, and
Figure 3 ha a example of HPSG parsng
constraints in parsing are checked by unification
of the sentence “Spring has come” First each
among them. Figur 1 shows exmples of HPSG
of the lexical entrie for “has” and “come” are
schema.
unified
</bodyText>
<subsectionHeader confidence="0.541246">
Head-Complement Schema Unification provids
</subsectionHeader>
<bodyText confidence="0.995955416666667">
Figure 2 shows an HPSG parse tree of the sen-
tence “Spring has come.” Fist, te lexical en-
the phrasal sign o the mther. The ign of he
tries of “has” and “come” are joine by head-
larger constituent is obtained by epeatedly apply-
complemen schema. Unification gives the HPSG
ing schemata to lexical/phrasal signs. Finally, the
sign of mother. Aftr applying schemaa to HPSG
phrasal sign of the entire sentence is output on the
signs repeatdly, the HPSG sign of the whole sen-
top of the derivation tree.
tence is output.
</bodyText>
<sectionHeader confidence="0.903556" genericHeader="method">
3 Airi H o th e
</sectionHeader>
<bodyText confidence="0.966972333333333">
We use Enju for an English HPSG parser
Tbak
(Miyao et al., 2004). Figure 3 shows how a co-
ordination tructur is built in the Enju grammar.
As discussed in Section 1, our grammar devel-
First, a coordinating cojution and the right
opmn requires each sentence to be annoated
conjunct are jined by coord right schema. Af-
with i) a history of rule applcations, and ii) d-
terwards, the parent and the left conjunct are
ditional nnoations o mke the gammar rues
joined by coord left schema.
be pseudo-injecive. In HPSG
ppcations s epesee y a ee otae
The Enju parser is equipped with a disam-
biguation model trained by the maximum entropy
w s ams Addti nn
method (Miyao and Tsujii, 2008). Since we do
</bodyText>
<subsectionHeader confidence="0.595162">
The value of caaegoy has ben presend for simpliciy,
</subsectionHeader>
<bodyText confidence="0.998962416666667">
e h pos of t gn e b
not need the probability of each parse tree, we
treat the model just as a linear model that defines
the score of a parse tree as the sum of feature
weights. The features of the model are defined
on local subtrees of a parse tree.
The Enju parser takes O(n3) time since it uses
the CKY algorithm, and each cell in the CKY
parse table has at most a constant number of edges
because we use beam search algorithm. Thus, we
can regard the parser as a decoder for a weighted
CFG.
</bodyText>
<subsectionHeader confidence="0.994267">
3.3 Dual decomposition
</subsectionHeader>
<bodyText confidence="0.9958585">
Dual decomposition is a classical method to solve
complex optimization problems that can be de-
</bodyText>
<figure confidence="0.99900075">
1
HEAD
SUBJ &lt; &gt;
COMPS &lt; &gt;
2
SUBJ &lt; &gt;
COMPS &lt; &gt;
SUBJ &lt; &gt;
2
COMPS &lt; &gt;
HEAD
1
HEAD
SUBJ 2
COMPS &lt;  |&gt;
3 4
1
3 COMPS &lt; &gt;
1
2
4
HEAD
SUBJ
COMPS
</figure>
<page confidence="0.877818">
432
</page>
<figureCaption confidence="0.9999925">
Figure 2: HPSG parsing; taken from Miyao et al.
Figure 3: HSG prsing
</figureCaption>
<figure confidence="0.962752">
(2004).
Coordina(on
i th
,
← coord_left_schema
Par(al
to th
es are requ
Coordina(on
← coord_right_schema
l, 2003a). Finally,
Coordina(ng
Compleme
Conjunc(on
schema m
</figure>
<figureCaption confidence="0.979644">
ted wih at least these features, the lexical e
Figure 3: Construction of coordination in Enju
composed into efficiently solvable sub-problems.
</figureCaption>
<bodyText confidence="0.896229944444444">
speciÞed derivation trees as tree structures anno
It is becoming popular in the NLP community
tated with schema name and HPSG gns includ
and has been show to work effectively on sev-
ing the specifications of the above feature
eral NLP tasks (Rush et al., 2010).
We describe the process of gram
ment in terms of the four phases: spec
We consider an optimization problem
arg max (f(x) + g(x)) (2)
x
which is difficult to solve (e.g. NP-hard), while
Gl gramtil taint defind in
arg maxx f(x) and arg maxx g(x) are effectively
thi phase and i HPSG they a ptd
solvable. In dual decomposition, we solve
thh th dsi f the si nd shat
min 1 showsthe dfiition fr the yped
</bodyText>
<equation confidence="0.794809333333333">
max(f(x) + g(y) + u(x − y))
Uu of
x�y d i t S
</equation>
<bodyText confidence="0.9929842">
instead of the original problem.
er are fine fo ch y
To find the minimum value, we can use a sub-
gradient method (Rush et al., 2010). The subgra-
dient method is given in Table 4. As the algorithm
</bodyText>
<equation confidence="0.8770628">
u(1) , 0
fork= 1toKdo
x(k) , arg maxx(f(x) + u(k)x)
y(k) , argmaxy(g(y) − u(k)y)
if x = y then
return u(k)
end if
u(k+1) , uk − ak(x(k) − y(k))
end for
return u(K)
</equation>
<tableCaption confidence="0.997006">
Table 4: The subgradient method
</tableCaption>
<bodyText confidence="0.9996565">
shows, you can use existing algorithms and don’t
need to have an exact algorithm for the optimiza-
tion problem, which are features of dual decom-
position.
If x(k) = y(k) occurs during the algorithm, then
we simply take x(k) as the primal solution, which
is the exact answer. If not, we simply take x(K),
the answer of coordination structure analysis with
alignment-based features, as an approximate an-
swer to the primal solution. The answer does not
always solve the original problem Eq (2), but pre-
vious works (e.g., (Rush et al., 2010)) has shown
that it is effective in practice. We use it in this
paper.
</bodyText>
<sectionHeader confidence="0.995372" genericHeader="method">
4 Proposed method
</sectionHeader>
<bodyText confidence="0.9999185">
In this section, we describe how we apply dual
decomposition to the two models.
</bodyText>
<subsectionHeader confidence="0.892817">
4.1 Notation
</subsectionHeader>
<bodyText confidence="0.968985235294118">
We define some notations here. First we describe
weighted CFG parsing, which is used for both
coordination structure analysis with alignment-
based features and HPSG parsing. We follows the
formulation by Rush et al., (2010). We assume a
context-free grammar in Chomsky normal form,
with a set of non-terminals N. All rules of the
grammar are either the form A —* BC or A —* w
where A, B, C E N and w E V . For rules of the
form A —* w we refer to A as the pre-terminal for
w.
Given a sentence with n words, w1w2...wn, a
parse tree is a set of rule productions of the form
(A —* BC, i, k, j) where A, B, C E N, and
1 G i G k G j G n. Each rule production rep-
resents the use of CFG rule A —* BC where non-
terminal A spans words wi...wj, non-terminal B
</bodyText>
<figure confidence="0.999592794117647">
come
has
Spring
Head-complement
schema
HEAD
SUBJ
COMPS
2
1
4
Unify
HEAD
SUBJ
2
1
Unify
3 COMPS &lt; &gt;
COMPS &lt;  |&gt;
3 4
HEAD noun
SUBJ &lt; &gt;
COMPS &lt; &gt;
HEAD verb
SUBJ &lt; &gt;
5
COMPS &lt;
HEAD verb
SUBJ &lt; &gt;
5
COMPS &lt; &gt;
&gt;
SUBJ &lt;
COMPS &lt; &gt;
HEAD verb
HEAD noun
SUBJ &lt; &gt;
COMPS &lt; &gt;
&gt;
Lexical entries
HEAD verb
SUBJ &lt; &gt;
COMPS &lt; &gt;
Spring has come
subject-head
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt; &gt;
head-comp
HEAD noun
SUBJ &lt; &gt;
COMPS &lt; &gt; COMPS &lt; 2
HEAD verb
2 SUBJ &lt; &gt;
1
COMPS &lt; &gt;
HEAD verb
SUBJ &lt; &gt;
1
&gt;
1
Le3LASH/REL
Conjunct
ma mu
Right
Conjunct
ategori
</figure>
<page confidence="0.99935">
433
</page>
<bodyText confidence="0.994051">
spans word wi...wk, and non-terminal C spans
word wk+1...wj if k &lt; j, and the use of CFG
rule A → wi if i = k = j.
We now define the index set for the coordina-
tion structure analysis as
</bodyText>
<equation confidence="0.9868805">
Icsa = {⟨A → BC, i, k, j⟩ : A,B,C ∈ N,
1 ≤ i ≤ k ≤ j ≤ n}
</equation>
<bodyText confidence="0.983474704545455">
Each parse tree is a vector y = {yr : r ∈ Icsa},
with yr = 1 if rule r is in the parse tree, and yr =
0 otherwise. Therefore, each parse tree is repre-
sented as a vector in {0,1}m, where m = |Icsa|.
We use Y to denote the set of all valid parse-tree
vectors. The set Y is a subset of {0,1}m.
In addition, we assume a vector Bcsa =
r ∈ Icsa} that specifies a score for each rule pro-
duction. Each �csa
r can take any real value. The
optimal parse tree is y* = arg maxyEY y · Bcsa
where y · Bcsa = Er yr · �csa
r is the inner product
between y and Bcsa.
We use similar notation for HPSG parsing. We
define Ihpsg, Z and Bhpsg as the index set for
HPSG parsing, the set of all valid parse-tree vec-
tors and the weight vector for HPSG parsing re-
spectively.
We extend the index sets for both the coor-
dination structure analysis with alignment-based
features and HPSG parsing to make a constraint
between the two sub-problems. For the coor-
dination structure analysis with alignment-based
features we define the extended index set to be
I′csa = Icsa UIuni where
Iuni = {(a, b, c) : a, b, c ∈ {1...n}}
Here each triple (a, b, c) represents that word
wc is recognized as the last word of the right
conjunct and the scope of the left conjunct or
the coordinating conjunction is wa...wb1. Thus
each parse-tree vector y will have additional com-
ponents ya,b,c. Note that this representation is
over-complete, since a parse tree is enough to
determine unique coordination structures for a
sentence: more explicitly, the value of ya,b,c is
&apos;This definition is derived from the structure of a co-
ordination in Enju (Figure 3). The triples show where
the coordinating conjunction and right conjunct are in
coord right schema, and the left conjunct and partial coor-
dination are in coord left schema. Thus they alone enable
not only the coordination structure analysis with alignment-
based features but Enju to uniquely determine the structure
of a coordination.
</bodyText>
<equation confidence="0.696874666666667">
1 if rule COORDa,c → CJTa,bCC , CJT ,c or
COORD ,c → CJT ,CCa,bCJT ,c is in the parse
tree; otherwise it is 0.
</equation>
<bodyText confidence="0.99999">
We apply the same extension to the HPSG in-
dex set, also giving an over-complete representa-
tion. We define za,b,c analogously to ya,b,c.
</bodyText>
<subsectionHeader confidence="0.989926">
4.2 Proposed method
</subsectionHeader>
<bodyText confidence="0.999951">
We now describe the dual decomposition ap-
proach for coordination disambiguation. First, we
define the set Q as follows:
</bodyText>
<equation confidence="0.991922">
Q = {(y, z) : y ∈ Y, z ∈ Z, ya,b,c = za,b,c
</equation>
<bodyText confidence="0.946575">
for all (a, b, c) ∈ Iuni}
Therefore, Q is the set of all (y, z) pairs that
agree on their coordination structures. The coor-
dination structure analysis with alignment-based
features and HPSG parsing problem is then to
solve
</bodyText>
<equation confidence="0.995095">
ym x (y · 0csa + 1&apos;z · Bhpsg) (3)
(
</equation>
<bodyText confidence="0.9986905">
where -y &gt; 0 is a parameter dictating the relative
weight of the two models and is chosen to opti-
mize performance on the development test set.
This problem is equivalent to
</bodyText>
<equation confidence="0.9975995">
max(g(z) • 0&amp;quot;&apos; +,yz - BhPs9) (4)
Z
</equation>
<bodyText confidence="0.998772913043478">
where g : Z → Y is a function that maps a
HPSG tree z to its set of coordination structures
z = g(y).
We solve this optimization problem by using
dual decomposition. Figure 4 shows the result-
ing algorithm. The algorithm tries to optimize
the combined objective by separately solving the
sub-problems again and again. After each itera-
tion, the algorithm updates the weights u(a, b, c).
These updates modify the objective functions for
the two sub-problems, encouraging them to agree
on the same coordination structures. If y(k) =
z(k) occurs during the iterations, then the algo-
rithm simply returns y(k) as the exact answer. If
not, the algorithm returns the answer of coordina-
tion analysis with alignment features as a heuristic
answer.
It is needed to modify original sub-problems
for calculating (1) and (2) in Table 4. We modified
the sub-problems to regard the score of u(a, b, c)
as a bonus/penalty of the coordination. The mod-
ified coordination structure analysis with align-
ment features adds u(k)(i, j, m) and u(k)(j+1, l−
</bodyText>
<equation confidence="0.931353416666667">
{Bcsa
r :
434
u(1)(a, b, c) ← 0 for all (a, b, c) ∈ Zuni
fork= 1toKdo
y(k) ← arg maxyEY(y · θcsa − F-(a,b,c)EIuni u(k)(a, b, c)ya,b,c) ... (1)
z(k) ← arg maxzEB(z · θhpsg + E(a,b,c)EIuni u(k)(a, b, c)za,b,c) ... (2)
if y(k)(a, b, c) = z(k)(a, b, c) for all (a, b, c) ∈ Zuni then
return y(k)
end if
for all (a, b, c) ∈ Zuni do
u(k+1)(a, b, c) ← u(k)(a, b, c) − ak(y(k)(a, b, c) − z(k)(a, b, c))
</equation>
<listItem confidence="0.638041">
end for
end for
return y(K)
</listItem>
<figureCaption confidence="0.996346">
Figure 4: Proposed algorithm
</figureCaption>
<equation confidence="0.7152385">
(, ( ))
1, m), as well as adding w · �(x, (i, j,l, m)) to
</equation>
<bodyText confidence="0.930906666666667">
te h th ule producti CO
the score of the subtree, when the rule produc-
tion COORDi,n —* CJTijCCj+1,i_1CJTi,n is j
</bodyText>
<equation confidence="0.558852">
applied.
s ad
The modfied Enju adds u(k)(a, b, c) when
i gnid cordinti
</equation>
<bodyText confidence="0.9444739">
coord right schema is appled, where word
d lft id of it i
wa...wb is recognized as a coordinating conjunc-
ordrightschemaipldh
tion and the last word of the right conjunct is
is egnid ooditin j
wc, or coord left schema is applied, where word
iht id f it op i w
wa...wb is recognized as the left conjunct and the
last word of the right conjunc is wc.
</bodyText>
<sectionHeader confidence="0.993887" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<tableCaption confidence="0.887738">
Table 6: The percentage of each conjunct type (%) of
each test set
</tableCaption>
<figure confidence="0.96707235">
each test set
COORD
NP
VP
ADJP
S
PP
Others
WSJ Genia
NP 6
63.7 66.3
VP 1
13.8 11.4
ADJ
S 6.8 9.6 1
11.4 6.0
PP
2.4 5.1
h
1.9 1.5
</figure>
<bodyText confidence="0.936009272727273">
We trai
analysis m
et al., 2003
of the Penn
evaluated the
the Genia
nal portion
we used HP
Treebank
training/test
sis with ali
tation in the
of
ne
used in the
The Wall
Treebank in
WSJ articles,
in the sentence
test set has
stracts, and
sentences.
</bodyText>
<figure confidence="0.9896448">
5 Experiments
5.1 T
r
T
aining data
</figure>
<figureCaption confidence="0.9639415">
d thehalignment-based coordination
odel on both)thenGenia corpush(Kim
and thehWallnStreet Journal portion
Treebanke(Marcus et al., 1993), and
</figureCaption>
<bodyText confidence="0.950704042553191">
performanceyof ourumethod on (i)e5
corpusdand (ii)hthe Wall Street Jour-
uthe PenntTreebank. More precisely, We used
SG treebank convertedlfrom the Pennnthe imple
andaGenia, and further extractedhthera wide
data?for coordination structure analy-eand an
gnment-based features usinglthe anno- implemente
Treebank. Table 5 shows the corpus slight
experiments.
StreethJournalaportion of the Penn 5.2.1
the test set has 2317 sentences from WeOus
andnthere are 1356 coordinationsrrithmt(Fi
s, while the Geniahcorpusyin theuhisachosen
1764 sentences-from MEDLINE ab-osopmentrs
theregaref1848hcoordinations inathe where qk
Coordinationseare furtheresubcatego- L(u(k′—1
rized into phrase types such as a NP coordination
COOD tags whie the Genia corpu
or PP coordination. Table 6 shows the percentage
NPCOOD tags and ADJPCOOD t
of each phrase type in all coordianitons. It indi-
cates the Wall2Street Journaltportionfof the Penn
Treebank has more VP coordinations and S co-
WeedEju(?)frhiplem
ordianitons, while the Genia corpus has more NP
coordianitons and ADJP coordiations.
.2 Implementation of sub-problems
,
Step
efficient parsingralgorithm, whilenwe re-
dsHaraset al., (2009)’s algorithmnwith
modifications.
-c
Enju (Miyao and Tsujii, 2004) for
mentation ofeHPSG parsing, which has
-coverageeprobabilisticlHPSG grammar
ed the following step size in our algo-
53 Evaluation metric
gure 4). First, we initialized a0, which
tooptimize performance on th devel-
We evaluated the perfomance of the
et. Then we defined ak = a0 · �−ηk
ds by the accuracy of coordnaio
is the number of times that L(u(k′)) &gt;
eting (?); i.e., we count each of the
)) for k′ � k.
scopes as o
</bodyText>
<figure confidence="0.5009275">
L(u
size
5.1 Test/
,
</figure>
<page confidence="0.942899">
435
</page>
<table confidence="0.99960725">
Task (i) Task (ii)
Training WSJ (sec. 2–21) + Genia (No. 1–1600) WSJ (sec. 2–21)
Development Genia (No. 1601–1800) WSJ (sec. 22)
Test Genia (No. 1801–1999) WSJ (sec. 23)
</table>
<tableCaption confidence="0.998739">
Table 5: The corpus used in the experiments
</tableCaption>
<table confidence="0.999784">
Proposed Enju CSA
Precision 72.4 66.3 65.3
Recall 67.8 65.5 60.5
F1 70.0 65.9 62.8
</table>
<tableCaption confidence="0.95035375">
Table 7: Results of Task (i) on the test set. The preci-
sion, recall, and F1 (%) for the proposed method, Enju,
and Coordination structure analysis with alignment-
based features (CSA)
</tableCaption>
<subsectionHeader confidence="0.961586">
5.3 Evaluation metric
</subsectionHeader>
<bodyText confidence="0.999983625">
We evaluated the performance of the tested meth-
ods by the accuracy of coordination-level bracket-
ing (Shimbo and Hara, 2007); i.e., we count each
of the coordination scopes as one output of the
system, and the system output is regarded as cor-
rect if both of the beginning of the first output
conjunct and the end of the last conjunct match
annotations in the Treebank (Hara et al., 2009).
</bodyText>
<subsectionHeader confidence="0.950691">
5.4 Experimental results of Task (i)
</subsectionHeader>
<bodyText confidence="0.9999846">
We ran the dual decomposition algorithm with a
limit of K = 50 iterations. We found the two
sub-problems return the same answer during the
algorithm in over 95% of sentences.
We compare the accuracy of the dual decompo-
sition approach to two baselines: Enju and coor-
dination structure analysis with alignment-based
features. Table 7 shows all three results. The dual
decomposition method gives a statistically signif-
icant gain in precision and recall over the two
methods2.
Table 8 shows the recall of coordinations of
each type. It indicates our re-implementation of
CSA and Hara et al. (2009) have a roughly simi-
lar performance, although their experimental set-
tings are different. It also shows the proposed
method took advantage of Enju and CSA in NP
coordination, while it is likely just to take the an-
swer of Enju in VP and sentential coordinations.
This means we might well use dual decomposi-
</bodyText>
<figure confidence="0.9352145">
2p &lt; 0.01 (by chi-square test)
100%
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
accuracy certificates
</figure>
<figureCaption confidence="0.9995214">
Figure 5: Performance of the approach as a function of
K of Task (i) on the development set. accuracy (%):
the percentage of sentences that are correctly parsed.
certificates (%): the percentage of sentences for which
a certificate of optimality is obtained.
</figureCaption>
<bodyText confidence="0.999752636363636">
tion only on NP coordinations to have a better re-
sult.
Figure 5 shows performance of the approach as
a function of K, the maximum number of iter-
ations of dual decomposition. The graphs show
that values of K much less than 50 produce al-
most identical performance to K = 50 (with
K = 50, the accuracy of the method is 73.4%,
with K = 20 it is 72.6%, and with K = 1 it
is 69.3%). This means you can use smaller K in
practical use for speed.
</bodyText>
<subsectionHeader confidence="0.949579">
5.5 Experimental results of Task (ii)
</subsectionHeader>
<bodyText confidence="0.999725833333333">
We also ran the dual decomposition algorithm
with a limit of K = 50 iterations on Task (ii).
Table 9 and 10 show the results of task (ii). They
show the proposed method outperformed the two
methods statistically in precision and recall3.
Figure 6 shows performance of the approach as
a function of K, the maximum number of iter-
ations of dual decomposition. The convergence
speed for WSJ was faster than that for Genia. This
is because a sentence of WSJ often have a simpler
coordination structure, compared with that of Ge-
nia.
</bodyText>
<figure confidence="0.980205333333333">
3p &lt; 0.01 (by chi-square test)
95%
90%
85%
80%
75%
70%
65%
60%
</figure>
<page confidence="0.997022">
436
</page>
<table confidence="0.99877525">
COORD # Proposed Enju CSA # Hara et al. (2009)
Overall 1848 67.7 63.3 61.9 3598 61.5
NP 1213 67.5 61.4 64.1 2317 64.2
VP 208 79.8 78.8 66.3 456 54.2
ADJP 193 58.5 59.1 54.4 312 80.4
S 111 51.4 52.3 34.2 188 22.9
PP 110 64.5 59.1 57.3 167 59.9
Others 13 78.3 73.9 65.2 140 49.3
</table>
<tableCaption confidence="0.96982025">
Table 8: The number of coordinations of each type (#), and the recall (%) for the proposed method, Enju,
coordination structure analysis with alignment-based features (CSA) , and Hara et al. (2009) of Task (i) on the
development set. Note that Hara et al. (2009) uses a different test set and different annotation rules, although its
test data is also taken from the Genia corpus. Thus we cannot compare them directly.
</tableCaption>
<table confidence="0.99978925">
Proposed Enju CSA
Precision 76.3 70.7 66.0
Recall 70.6 69.0 60.1
F1 73.3 69.9 62.9
</table>
<tableCaption confidence="0.9910965">
Table 9: Results of Task (ii) on the test set. The preci-
sion, recall, and F1 (%) for the proposed method, Enju,
and Coordination structure analysis with alignment-
based features (CSA)
</tableCaption>
<table confidence="0.999751125">
COORD # Proposed Enju CSA
Overall 1017 71.6 68.1 60.7
NP 573 76.1 71.0 67.7
VP 187 62.0 62.6 47.6
ADJP 73 82.2 75.3 79.5
S 141 64.5 62.4 42.6
PP 19 52.6 47.4 47.4
Others 24 62.5 70.8 54.2
</table>
<tableCaption confidence="0.918134">
Table 10: The number of coordinations of each type
</tableCaption>
<bodyText confidence="0.9458185">
(#), and the recall (%) for the proposed method, Enju,
and coordination structure analysis with alignment-
based features (CSA) of Task (ii) on the development
set.
</bodyText>
<sectionHeader confidence="0.997642" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999748">
In this paper, we presented an efficient method for
detecting and disambiguating coordinate struc-
tures. Our basic idea was to consider both gram-
mar and symmetries of conjuncts by using dual
decomposition. Experiments on the Genia corpus
and the Wall Street Journal portion of the Penn
Treebank showed that we could obtain statisti-
cally significant improvement in accuracy when
using dual decomposition.
We would need a further study in the follow-
ing points of view: First, we should evaluate our
</bodyText>
<figureCaption confidence="0.9996772">
Figure 6: Performance of the approach as a function of
K of Task (ii) on the development set. accuracy (%):
the percentage of sentences that are correctly parsed.
certificates (%): the percentage of sentences for which
a certificate of optimality is provided.
</figureCaption>
<bodyText confidence="0.999468272727273">
method with corpus in different domains. Be-
cause characteristics of coordination structures
differs from corpus to corpus, experiments on
other corpus would lead to a different result. Sec-
ond, we would want to add some features to coor-
dination structure analysis with alignment-based
local features such as ontology. Finally, we can
add other methods (e.g. dependency parsing) as
sub-problems to our method by using the exten-
sion of dual decomposition, which can deal with
more than two sub-problems.
</bodyText>
<sectionHeader confidence="0.998857" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.95566">
The second author is partially supported by KAK-
ENHI Grant-in-Aid for Scientific Research C
21500131 and Microsoft CORE project 7.
</bodyText>
<figure confidence="0.998953727272727">
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49
accuracy certificates
100%
95%
90%
85%
80%
75%
70%
65%
60%
</figure>
<page confidence="0.994146">
437
</page>
<sectionHeader confidence="0.993222" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999795576271186">
Kazuo Hara, Masashi Shimbo, Hideharu Okuma, and
Yuji Matsumoto. 2009. Coordinate structure analy-
sis with global structural constraints and alignment-
based local features. In Proceedings of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP, pages 967–975, Aug.
Deirdre Hogan. 2007. Coordinate noun phrase dis-
ambiguation in a generative parsing model. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL 2007),
pages 680–687.
Jun-Dong Kim, Tomoko Ohta, and Jun’ich Tsujii.
2003. Genia corpus - a semantically annotated cor-
pus for bio-textmining. Bioinformatics, 19.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems, 15:3–10.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19:313–330.
Yusuke Miyao and Jun’ich Tsujii. 2004. Deep lin-
guistic analysis for the accurate identification of
predicate-argument relations. In Proceeding of
COLING 2004, pages 1392–1397.
Yusuke Miyao and Jun’ich Tsujii. 2008. Feature
forest models for probabilistic hpsg parsing. MIT
Press, 1(34):35–80.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP 2004).
Preslav Nakov and Marti Hearst. 2005. Using the web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language (HLT-
EMNLP 2005), pages 835–842.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.
Philip Resnik. 1999. Semantic similarity in a takon-
omy. Journal of Artificial Intelligence Research,
11:95–130.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natu-
ral language processing. In Proceeding of the con-
ference on Empirical Methods in Natural Language
Processing.
Masashi Shimbo and Kazuo Hara. 2007. A discrimi-
native learning model for coordinate conjunctions.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 610–619, Jun.
</reference>
<page confidence="0.998276">
438
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.590361">
<title confidence="0.999918">Coordination Structure Analysis using Dual Decomposition</title>
<author confidence="0.989104">Hanamoto Matsuzaki Tsujii</author>
<affiliation confidence="0.651809">1. Department of Computer Science, University of Tokyo, Japan</affiliation>
<address confidence="0.979285">2. Web Search &amp; Mining Group, Microsoft Research Asia, China</address>
<email confidence="0.999922">jtsujii@microsoft.com</email>
<abstract confidence="0.994578117647059">Coordination disambiguation remains a difficult sub-problem in parsing despite the frequency and importance of coordination structures. We propose a method for disambiguating coordination structures. In this method, dual decomposition is used as a framework to take advantage of both HPSG parsing and coordinate structure analysis with alignment-based local features. We evaluate the performance of the proposed method on the Genia corpus and the Wall Street Journal portion of the Penn Treebank. Results show it increases the percentage of sentences in which coordination structures are detected correctly, compared with each of the two algorithms alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kazuo Hara</author>
<author>Masashi Shimbo</author>
<author>Hideharu Okuma</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Coordinate structure analysis with global structural constraints and alignmentbased local features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>967--975</pages>
<contexts>
<context position="5839" citStr="Hara et al. (2009)" startWordPosition="908" endWordPosition="911">ation disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007). She detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts. They are used to rerank the n-best outputs of the Bikel parser (2004), whereas two models interact with each other in our method. Shimbo and Hara (2007) proposed an alignment-based method for detecting and disambiguating non-nested coordination structures. They disambiguated coordination structures based on the edit distance between two conjuncts. Hara et al. (2009) extended the method, dealing with nested coordinations as well. We used their method as one of the two sub-models. 3 Background 3.1 Coordination structure analysis with alignment-based local features Coordination structure analysis with alignmentbased local features (Hara et al., 2009) is a hybrid approach to coordination disambiguation that combines a simple grammar to ensure consistent global structure of coordinations in a sentence, and features based on sequence alignment to capture local symmetry of conjuncts. In this section, we describe the method briefly. A sentence is denoted by x = </context>
<context position="7214" citStr="Hara et al. (2009)" startWordPosition="1140" endWordPosition="1143">unction having left conjunct xb,...xe, and right conjunct xb,,...xe,.) null (otherwise) In other words, yi has a non-null value only when it is a coordinating conjunction. For example, a sentence I bought books and stationary has a coordination boundaries set (null, null, null, (3, 3, 5, 5), null). The score of a coordination boundaries set is defined as the sum of score of all coordinating conjunctions in the sentence. k score(x,y) = score(x, ym) m=1 k = w · f(x, ym) (1) m=1 where f(x, ym) is a real-valued feature vector of the coordination conjunct xm. We used almost the same feature set as Hara et al. (2009): namely, the surface word, part-of-speech, suffix and prefix of the words, and their combinations. We used the averaged perceptron to tune the weight vector w. Hara et al. (2009) proposed to use a contextfree grammar to find a properly nested coordination structure. That is, the scoring function Eq (1) 431 COORD Coordination. CJT Conjunct. N Non-coordination. CC Coordinating conjunction like “and”. W Any word. Table 2: Non-terminals Rules for coordinations: COORDZ,m → CJTZ,jCCj+1,k_1CJTk,m Rules for conjuncts: CJTZ,j → (COORD|N)Z,j Rules for non-coordinations: NZ,k → COORDZ,jNj+1,k NZ,j → WZ,</context>
<context position="23942" citStr="Hara et al., 2009" startWordPosition="4189" endWordPosition="4192">65.5 60.5 F1 70.0 65.9 62.8 Table 7: Results of Task (i) on the test set. The precision, recall, and F1 (%) for the proposed method, Enju, and Coordination structure analysis with alignmentbased features (CSA) 5.3 Evaluation metric We evaluated the performance of the tested methods by the accuracy of coordination-level bracketing (Shimbo and Hara, 2007); i.e., we count each of the coordination scopes as one output of the system, and the system output is regarded as correct if both of the beginning of the first output conjunct and the end of the last conjunct match annotations in the Treebank (Hara et al., 2009). 5.4 Experimental results of Task (i) We ran the dual decomposition algorithm with a limit of K = 50 iterations. We found the two sub-problems return the same answer during the algorithm in over 95% of sentences. We compare the accuracy of the dual decomposition approach to two baselines: Enju and coordination structure analysis with alignment-based features. Table 7 shows all three results. The dual decomposition method gives a statistically significant gain in precision and recall over the two methods2. Table 8 shows the recall of coordinations of each type. It indicates our re-implementati</context>
<context position="26381" citStr="Hara et al. (2009)" startWordPosition="4636" endWordPosition="4639">ecomposition algorithm with a limit of K = 50 iterations on Task (ii). Table 9 and 10 show the results of task (ii). They show the proposed method outperformed the two methods statistically in precision and recall3. Figure 6 shows performance of the approach as a function of K, the maximum number of iterations of dual decomposition. The convergence speed for WSJ was faster than that for Genia. This is because a sentence of WSJ often have a simpler coordination structure, compared with that of Genia. 3p &lt; 0.01 (by chi-square test) 95% 90% 85% 80% 75% 70% 65% 60% 436 COORD # Proposed Enju CSA # Hara et al. (2009) Overall 1848 67.7 63.3 61.9 3598 61.5 NP 1213 67.5 61.4 64.1 2317 64.2 VP 208 79.8 78.8 66.3 456 54.2 ADJP 193 58.5 59.1 54.4 312 80.4 S 111 51.4 52.3 34.2 188 22.9 PP 110 64.5 59.1 57.3 167 59.9 Others 13 78.3 73.9 65.2 140 49.3 Table 8: The number of coordinations of each type (#), and the recall (%) for the proposed method, Enju, coordination structure analysis with alignment-based features (CSA) , and Hara et al. (2009) of Task (i) on the development set. Note that Hara et al. (2009) uses a different test set and different annotation rules, although its test data is also taken from the Ge</context>
</contexts>
<marker>Hara, Shimbo, Okuma, Matsumoto, 2009</marker>
<rawString>Kazuo Hara, Masashi Shimbo, Hideharu Okuma, and Yuji Matsumoto. 2009. Coordinate structure analysis with global structural constraints and alignmentbased local features. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 967–975, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deirdre Hogan</author>
</authors>
<title>Coordinate noun phrase disambiguation in a generative parsing model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>680--687</pages>
<contexts>
<context position="4726" citStr="Hogan, 2007" startWordPosition="725" endWordPosition="726">s paper is as follows. First, we describe three basic methods required in the technique we propose: 1) coordination structure analysis with alignment-based local features, 2) HPSG parsing, and 3) dual decomposition. Finally, we show experimental results that demonstrate the effectiveness of our approach. We compare three methods: coordination structure analysis with alignment-based local features, HPSG parsing, and the dual-decomposition-based approach that combines both. 2 Related Work Many previous studies for coordination disambiguation have focused on a particular type of NP coordination (Hogan, 2007). Resnik (1999) disambiguated coordination structures by using semantic similarity of the conjuncts in a taxonomy. He dealt with two kinds of patterns, [n0 n1 and n2 n3] and [n1 and n2 n3], where ni are all nouns. He detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) used the Web as a training set and applied it to a task that is similar to Resnik’s. In terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007).</context>
</contexts>
<marker>Hogan, 2007</marker>
<rawString>Deirdre Hogan. 2007. Coordinate noun phrase disambiguation in a generative parsing model. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2007), pages 680–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Jun’ich Tsujii</author>
</authors>
<title>Genia corpus - a semantically annotated corpus for bio-textmining.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<volume>19</volume>
<marker>Kim, Ohta, Tsujii, 2003</marker>
<rawString>Jun-Dong Kim, Tomoko Ohta, and Jun’ich Tsujii. 2003. Genia corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>15--3</pages>
<contexts>
<context position="1758" citStr="Klein and Manning, 2003" startWordPosition="255" endWordPosition="258">arbled parsing result, coordination disambiguation remains a difficult sub-problem in parsing, even for state-of-the-art parsers. One approach to solve this problem is a grammatical approach. This approach, however, often fails in noun and adjective coordinations because there are many possible structures in these coordinations that are grammatically correct. For example, a noun sequence of the form “n0 n1 and n2 n3” has as many as five possible structures (Resnik, 1999). Therefore, a grammatical approach is not sufficient to disambiguate coordination structures. In fact, the Stanford parser (Klein and Manning, 2003) and Enju (Miyao and Tsujii, 2004) fail to disambiguate a sentence I am a freshman advertising and marketing major. Table 1 shows the output from them and the correct coordination structure. The coordination structure above is obvious to humans because there is a symmetry of conjuncts (-ing) in the sentence. Coordination structures often have such structural and semantic symmetry of conjuncts. One approach is to capture local symmetry of conjuncts. However, this approach fails in VP and sentential coordinations, which can easily be detected by a grammatical approach. This is because conjuncts </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. Advances in Neural Information Processing Systems, 15:3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="21376" citStr="Marcus et al., 1993" startWordPosition="3789" endWordPosition="3792">ents Table 6: The percentage of each conjunct type (%) of each test set each test set COORD NP VP ADJP S PP Others WSJ Genia NP 6 63.7 66.3 VP 1 13.8 11.4 ADJ S 6.8 9.6 1 11.4 6.0 PP 2.4 5.1 h 1.9 1.5 We trai analysis m et al., 2003 of the Penn evaluated the the Genia nal portion we used HP Treebank training/test sis with ali tation in the of ne used in the The Wall Treebank in WSJ articles, in the sentence test set has stracts, and sentences. 5 Experiments 5.1 T r T aining data d thehalignment-based coordination odel on both)thenGenia corpush(Kim and thehWallnStreet Journal portion Treebanke(Marcus et al., 1993), and performanceyof ourumethod on (i)e5 corpusdand (ii)hthe Wall Street Jouruthe PenntTreebank. More precisely, We used SG treebank convertedlfrom the Pennnthe imple andaGenia, and further extractedhthera wide data?for coordination structure analy-eand an gnment-based features usinglthe anno- implemente Treebank. Table 5 shows the corpus slight experiments. StreethJournalaportion of the Penn 5.2.1 the test set has 2317 sentences from WeOus andnthere are 1356 coordinationsrrithmt(Fi s, while the Geniahcorpusyin theuhisachosen 1764 sentences-from MEDLINE ab-osopmentrs theregaref1848hcoordinatio</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ich Tsujii</author>
</authors>
<title>Deep linguistic analysis for the accurate identification of predicate-argument relations.</title>
<date>2004</date>
<booktitle>In Proceeding of COLING</booktitle>
<pages>1392--1397</pages>
<contexts>
<context position="1792" citStr="Miyao and Tsujii, 2004" startWordPosition="261" endWordPosition="264"> disambiguation remains a difficult sub-problem in parsing, even for state-of-the-art parsers. One approach to solve this problem is a grammatical approach. This approach, however, often fails in noun and adjective coordinations because there are many possible structures in these coordinations that are grammatically correct. For example, a noun sequence of the form “n0 n1 and n2 n3” has as many as five possible structures (Resnik, 1999). Therefore, a grammatical approach is not sufficient to disambiguate coordination structures. In fact, the Stanford parser (Klein and Manning, 2003) and Enju (Miyao and Tsujii, 2004) fail to disambiguate a sentence I am a freshman advertising and marketing major. Table 1 shows the output from them and the correct coordination structure. The coordination structure above is obvious to humans because there is a symmetry of conjuncts (-ing) in the sentence. Coordination structures often have such structural and semantic symmetry of conjuncts. One approach is to capture local symmetry of conjuncts. However, this approach fails in VP and sentential coordinations, which can easily be detected by a grammatical approach. This is because conjuncts in these coordinations do not nece</context>
<context position="22611" citStr="Miyao and Tsujii, 2004" startWordPosition="3952" endWordPosition="3955">ere qk Coordinationseare furtheresubcatego- L(u(k′—1 rized into phrase types such as a NP coordination COOD tags whie the Genia corpu or PP coordination. Table 6 shows the percentage NPCOOD tags and ADJPCOOD t of each phrase type in all coordianitons. It indicates the Wall2Street Journaltportionfof the Penn Treebank has more VP coordinations and S coWeedEju(?)frhiplem ordianitons, while the Genia corpus has more NP coordianitons and ADJP coordiations. .2 Implementation of sub-problems , Step efficient parsingralgorithm, whilenwe redsHaraset al., (2009)’s algorithmnwith modifications. -c Enju (Miyao and Tsujii, 2004) for mentation ofeHPSG parsing, which has -coverageeprobabilisticlHPSG grammar ed the following step size in our algo53 Evaluation metric gure 4). First, we initialized a0, which tooptimize performance on th develWe evaluated the perfomance of the et. Then we defined ak = a0 · �−ηk ds by the accuracy of coordnaio is the number of times that L(u(k′)) &gt; eting (?); i.e., we count each of the )) for k′ � k. scopes as o L(u size 5.1 Test/ , 435 Task (i) Task (ii) Training WSJ (sec. 2–21) + Genia (No. 1–1600) WSJ (sec. 2–21) Development Genia (No. 1601–1800) WSJ (sec. 22) Test Genia (No. 1801–1999) </context>
</contexts>
<marker>Miyao, Tsujii, 2004</marker>
<rawString>Yusuke Miyao and Jun’ich Tsujii. 2004. Deep linguistic analysis for the accurate identification of predicate-argument relations. In Proceeding of COLING 2004, pages 1392–1397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ich Tsujii</author>
</authors>
<title>Feature forest models for probabilistic hpsg parsing.</title>
<date>2008</date>
<pages>1--34</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="11310" citStr="Miyao and Tsujii, 2008" startWordPosition="1827" endWordPosition="1830"> et al., 2004). Figure 3 shows how a coordination tructur is built in the Enju grammar. As discussed in Section 1, our grammar develFirst, a coordinating cojution and the right opmn requires each sentence to be annoated conjunct are jined by coord right schema. Afwith i) a history of rule applcations, and ii) dterwards, the parent and the left conjunct are ditional nnoations o mke the gammar rues joined by coord left schema. be pseudo-injecive. In HPSG ppcations s epesee y a ee otae The Enju parser is equipped with a disambiguation model trained by the maximum entropy w s ams Addti nn method (Miyao and Tsujii, 2008). Since we do The value of caaegoy has ben presend for simpliciy, e h pos of t gn e b not need the probability of each parse tree, we treat the model just as a linear model that defines the score of a parse tree as the sum of feature weights. The features of the model are defined on local subtrees of a parse tree. The Enju parser takes O(n3) time since it uses the CKY algorithm, and each cell in the CKY parse table has at most a constant number of edges because we use beam search algorithm. Thus, we can regard the parser as a decoder for a weighted CFG. 3.3 Dual decomposition Dual decompositio</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ich Tsujii. 2008. Feature forest models for probabilistic hpsg parsing. MIT Press, 1(34):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<contexts>
<context position="10701" citStr="Miyao et al., 2004" startWordPosition="1717" endWordPosition="1720">Schema Unification provids Figure 2 shows an HPSG parse tree of the sentence “Spring has come.” Fist, te lexical enthe phrasal sign o the mther. The ign of he tries of “has” and “come” are joine by headlarger constituent is obtained by epeatedly applycomplemen schema. Unification gives the HPSG ing schemata to lexical/phrasal signs. Finally, the sign of mother. Aftr applying schemaa to HPSG phrasal sign of the entire sentence is output on the signs repeatdly, the HPSG sign of the whole sentop of the derivation tree. tence is output. 3 Airi H o th e We use Enju for an English HPSG parser Tbak (Miyao et al., 2004). Figure 3 shows how a coordination tructur is built in the Enju grammar. As discussed in Section 1, our grammar develFirst, a coordinating cojution and the right opmn requires each sentence to be annoated conjunct are jined by coord right schema. Afwith i) a history of rule applcations, and ii) dterwards, the parent and the left conjunct are ditional nnoations o mke the gammar rues joined by coord left schema. be pseudo-injecive. In HPSG ppcations s epesee y a ee otae The Enju parser is equipped with a disambiguation model trained by the maximum entropy w s ams Addti nn method (Miyao and Tsuj</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank. In Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the web as an implicit training set: Application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language (HLTEMNLP</booktitle>
<pages>835--842</pages>
<contexts>
<context position="5104" citStr="Nakov and Hearst (2005)" startWordPosition="790" endWordPosition="793">h alignment-based local features, HPSG parsing, and the dual-decomposition-based approach that combines both. 2 Related Work Many previous studies for coordination disambiguation have focused on a particular type of NP coordination (Hogan, 2007). Resnik (1999) disambiguated coordination structures by using semantic similarity of the conjuncts in a taxonomy. He dealt with two kinds of patterns, [n0 n1 and n2 n3] and [n1 and n2 n3], where ni are all nouns. He detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) used the Web as a training set and applied it to a task that is similar to Resnik’s. In terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007). She detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts. They are used to rerank the n-best outputs of the Bikel parser (2004), whereas two models interact with each other in our method. Shimbo and Hara (2007) proposed an alignment-based method for detecting and disambiguating non-nested c</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Using the web as an implicit training set: Application to structural ambiguity resolution. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language (HLTEMNLP 2005), pages 835–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-driven phrase structure grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="9112" citStr="Pollard and Sag, 1994" startWordPosition="1440" endWordPosition="1443">art parsing algorithm to output the coordination boundaries with the highest score. Note that these production rules don’t need to be isomorphic to those of HPSG parsing and actually they aren’t. This is because the two methods interact only through dual decomposition and the search spaces defined by the methods are considered separately. This method requires O(n4) time, where n is the number of words. This is because there are O(n2) possible coordination structures in a sentence, and the method requires O(n2) time to get a feature vector of each coordination structure. 3.2 HPSG parsing HPSG (Pollard and Sag, 1994) is one of the linguistic theories based on lexicalized grammar Figure 1: subject-head schema (left) and headFgbjetHdh(left)He complement schema (right); taken from Miyao et al. Complement Schema (right) (2004). formalism. In a lexicalized grammar, quite a nts emic o a n, th ud xps pragntstr. small numbers of schemata are used to explain Figu 2 pets h SbjHd hema general grammatical constraints, compared with ad the HedComlet Shea1 defi in other theories. On the other hand, rich word(Pld d S 1994) I d t e specific characteristics are embedded in lexical eal tait hta l id hig f entries. Both of </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-driven phrase structure grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a takonomy.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--95</pages>
<contexts>
<context position="1609" citStr="Resnik, 1999" startWordPosition="236" endWordPosition="237">tures often give syntactic ambiguity in natural language. Although a wrong analysis of a coordination structure often leads to a totally garbled parsing result, coordination disambiguation remains a difficult sub-problem in parsing, even for state-of-the-art parsers. One approach to solve this problem is a grammatical approach. This approach, however, often fails in noun and adjective coordinations because there are many possible structures in these coordinations that are grammatically correct. For example, a noun sequence of the form “n0 n1 and n2 n3” has as many as five possible structures (Resnik, 1999). Therefore, a grammatical approach is not sufficient to disambiguate coordination structures. In fact, the Stanford parser (Klein and Manning, 2003) and Enju (Miyao and Tsujii, 2004) fail to disambiguate a sentence I am a freshman advertising and marketing major. Table 1 shows the output from them and the correct coordination structure. The coordination structure above is obvious to humans because there is a symmetry of conjuncts (-ing) in the sentence. Coordination structures often have such structural and semantic symmetry of conjuncts. One approach is to capture local symmetry of conjuncts</context>
<context position="4741" citStr="Resnik (1999)" startWordPosition="727" endWordPosition="728">follows. First, we describe three basic methods required in the technique we propose: 1) coordination structure analysis with alignment-based local features, 2) HPSG parsing, and 3) dual decomposition. Finally, we show experimental results that demonstrate the effectiveness of our approach. We compare three methods: coordination structure analysis with alignment-based local features, HPSG parsing, and the dual-decomposition-based approach that combines both. 2 Related Work Many previous studies for coordination disambiguation have focused on a particular type of NP coordination (Hogan, 2007). Resnik (1999) disambiguated coordination structures by using semantic similarity of the conjuncts in a taxonomy. He dealt with two kinds of patterns, [n0 n1 and n2 n3] and [n1 and n2 n3], where ni are all nouns. He detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) used the Web as a training set and applied it to a task that is similar to Resnik’s. In terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007). She detected n</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic similarity in a takonomy. Journal of Artificial Intelligence Research, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceeding of the conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="12766" citStr="Rush et al., 2010" startWordPosition="2105" endWordPosition="2108">ing; taken from Miyao et al. Figure 3: HSG prsing (2004). Coordina(on i th , ← coord_left_schema Par(al to th es are requ Coordina(on ← coord_right_schema l, 2003a). Finally, Coordina(ng Compleme Conjunc(on schema m ted wih at least these features, the lexical e Figure 3: Construction of coordination in Enju composed into efficiently solvable sub-problems. speciÞed derivation trees as tree structures anno It is becoming popular in the NLP community tated with schema name and HPSG gns includ and has been show to work effectively on seving the specifications of the above feature eral NLP tasks (Rush et al., 2010). We describe the process of gram ment in terms of the four phases: spec We consider an optimization problem arg max (f(x) + g(x)) (2) x which is difficult to solve (e.g. NP-hard), while Gl gramtil taint defind in arg maxx f(x) and arg maxx g(x) are effectively thi phase and i HPSG they a ptd solvable. In dual decomposition, we solve thh th dsi f the si nd shat min 1 showsthe dfiition fr the yped max(f(x) + g(y) + u(x − y)) Uu of x�y d i t S instead of the original problem. er are fine fo ch y To find the minimum value, we can use a subgradient method (Rush et al., 2010). The subgradient metho</context>
<context position="14137" citStr="Rush et al., 2010" startWordPosition="2368" endWordPosition="2371"> u(k+1) , uk − ak(x(k) − y(k)) end for return u(K) Table 4: The subgradient method shows, you can use existing algorithms and don’t need to have an exact algorithm for the optimization problem, which are features of dual decomposition. If x(k) = y(k) occurs during the algorithm, then we simply take x(k) as the primal solution, which is the exact answer. If not, we simply take x(K), the answer of coordination structure analysis with alignment-based features, as an approximate answer to the primal solution. The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al., 2010)) has shown that it is effective in practice. We use it in this paper. 4 Proposed method In this section, we describe how we apply dual decomposition to the two models. 4.1 Notation We define some notations here. First we describe weighted CFG parsing, which is used for both coordination structure analysis with alignmentbased features and HPSG parsing. We follows the formulation by Rush et al., (2010). We assume a context-free grammar in Chomsky normal form, with a set of non-terminals N. All rules of the grammar are either the form A —* BC or A —* w where A, B, C E N and w E V . For rules of </context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceeding of the conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masashi Shimbo</author>
<author>Kazuo Hara</author>
</authors>
<title>A discriminative learning model for coordinate conjunctions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>610--619</pages>
<contexts>
<context position="5623" citStr="Shimbo and Hara (2007)" startWordPosition="880" endWordPosition="883">rm, meaning and conceptual association between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) used the Web as a training set and applied it to a task that is similar to Resnik’s. In terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007). She detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts. They are used to rerank the n-best outputs of the Bikel parser (2004), whereas two models interact with each other in our method. Shimbo and Hara (2007) proposed an alignment-based method for detecting and disambiguating non-nested coordination structures. They disambiguated coordination structures based on the edit distance between two conjuncts. Hara et al. (2009) extended the method, dealing with nested coordinations as well. We used their method as one of the two sub-models. 3 Background 3.1 Coordination structure analysis with alignment-based local features Coordination structure analysis with alignmentbased local features (Hara et al., 2009) is a hybrid approach to coordination disambiguation that combines a simple grammar to ensure con</context>
<context position="23679" citStr="Shimbo and Hara, 2007" startWordPosition="4140" endWordPosition="4143">sk (i) Task (ii) Training WSJ (sec. 2–21) + Genia (No. 1–1600) WSJ (sec. 2–21) Development Genia (No. 1601–1800) WSJ (sec. 22) Test Genia (No. 1801–1999) WSJ (sec. 23) Table 5: The corpus used in the experiments Proposed Enju CSA Precision 72.4 66.3 65.3 Recall 67.8 65.5 60.5 F1 70.0 65.9 62.8 Table 7: Results of Task (i) on the test set. The precision, recall, and F1 (%) for the proposed method, Enju, and Coordination structure analysis with alignmentbased features (CSA) 5.3 Evaluation metric We evaluated the performance of the tested methods by the accuracy of coordination-level bracketing (Shimbo and Hara, 2007); i.e., we count each of the coordination scopes as one output of the system, and the system output is regarded as correct if both of the beginning of the first output conjunct and the end of the last conjunct match annotations in the Treebank (Hara et al., 2009). 5.4 Experimental results of Task (i) We ran the dual decomposition algorithm with a limit of K = 50 iterations. We found the two sub-problems return the same answer during the algorithm in over 95% of sentences. We compare the accuracy of the dual decomposition approach to two baselines: Enju and coordination structure analysis with </context>
</contexts>
<marker>Shimbo, Hara, 2007</marker>
<rawString>Masashi Shimbo and Kazuo Hara. 2007. A discriminative learning model for coordinate conjunctions. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 610–619, Jun.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>