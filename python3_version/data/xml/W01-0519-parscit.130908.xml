<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001011">
<title confidence="0.9969745">
Comparing Data-Driven Learning Algorithms for PoS Tagging
of Swedish
</title>
<author confidence="0.996869">
Beata Megyesi
</author>
<affiliation confidence="0.999266">
Centre for Speech Technology
Department of Speech, Music and Hearing
Royal Institute of Technology
</affiliation>
<address confidence="0.973185">
SE-100 44, Stockholm, Sweden
</address>
<email confidence="0.866706">
beaOspeech.kth.se
</email>
<sectionHeader confidence="0.991568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851133333333">
The aim of this study is a systematic evaluation
and comparison of four state-of-the-art data-
driven learning algorithms applied to part of
speech tagging of Swedish. The algorithms in-
cluded in this study are Hidden Markov Model,
Maximum Entropy, Memory-Based Learning,
and Transformation-Based Learning. The sys-
tems are evaluated from several aspects. Both
the effects of tag set and the effects of the size
of training data are examined. The accuracy is
calculated as well as the error rate for known
and unknown tokens. The results show differ-
ences between the approaches due to the dif-
ferent linguistic information built into the sys-
tems.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870588235294">
In the last decade several machine learning algo-
rithms have been developed and applied to var-
ious natural language processing (NLP) tasks.
One of the most popular approaches to test the
data-driven methods has been morphosyntac-
tic disambiguation of running texts, also called
Part-of-Speech (PoS) tagging. One of the rea-
sons is the need of PoS annotated texts in nat-
ural language processing systems and applica-
tions. Another reason is that benchmark data,
i.e. correctly annotated texts, are available for
several languages, making the training and test
procedure easily feasible.
The data-driven PoS taggers used in this
study are claimed to be both language- and
tagset-independent and easily applicable to new
languages, given a set of correctly annotated
training corpora. According to the literature,
each tagger has been tested for English with
an average accuracy of between 95% and 97%.
However, the manner in which the taggers
are evaluated by the researchers differs greatly,
which makes it difficult to compare the perfor-
mance of the systems.
Several recent studies report comparisons of
data-driven PoS taggers. However, the purpose
of these studies has been primarily to attain
higher tagging performance by means of differ-
ent system combinations.
Brill Si Wu (1998) trained statistical un-
igram and trigram, Maximum Entropy and
Transformation-Based learning on the English
Penn Treebank Wall Street Journal Corpus con-
sisting of 1.1 million words. They showed that
the Maximum Entropy framework as it was im-
plemented by Ratnaparkhi (1996) achieved the
highest accuracy in total and in the annotation
of ambiguous and unknown words.
Van Halteren et al. (1998) included Maxi-
mum Entropy, Memory-Based, Statistical tri-
gram, and Transformation-Based approaches in
the ensemble of classifiers and trained on 80%
of the LOB corpus consisting of 931062 to-
kens. Similarly to the results given by Brill Si
Wu (1998), the Maximum Entropy framework
achieved highest accuracy.
In contrast to the previous studies, Zavrel
and Daelemans (2000) report that a statisti-
cal trigram approach, TNT (Brants, 2000) gave
the best result over the Maximum Entropy,
Memory-Based, and Transformation-Based ap-
proaches when trained on 90% and tested on
10% of the Spoken Dutch Corpus consisting of
5,000, 10,000 and 20,000 tokens respectively.
However, the goal of these studies was not
a systematic evaluation and comparison of the
classifiers.
De Pauw and Daelemans (2000) describe a
systematic comparison between the Maximum
Entropy framework and Memory-Based Learn-
ing performed on the LOB-corpus. They report
that the overall tagging accuracy of the methods
are similar, although Maximum Entropy suc-
ceeds better in tagging unknown words. Fur-
thermore, they pointed out that the &amp;quot;differences
in accuracy can be attributed largely to differ-
ences in information sources used, rather than
to algorithm bias&amp;quot;.
</bodyText>
<sectionHeader confidence="0.425482" genericHeader="method">
2 Comparison of four PoS taggers
</sectionHeader>
<bodyText confidence="0.9999815">
The purpose of the study is to evaluate four
widespread learning algorithms in a systematic
way. The aim is to find the advantages and
drawbacks of the methods and to describe the
type of errors they make, the effects of the
tag set size, and the effect of the size of train-
ing material. Since English is a widely studied
language and has received significant attention
from computational linguists, it seems appro-
priate to evaluate the taggers on a different lan-
guage; especially, as the taggers are said to be
language-independent.
The experiments described in this paper
are based on well-known algorithms that
have implementations for the PoS tagging ap-
proach. Common to these taggers is that
they are claimed to be language- and tagset-
independent, easy to apply to new domains, lan-
guages and tag sets and available to the public.
Each approach will be briefly described below.
</bodyText>
<subsectionHeader confidence="0.948832">
2.1 Taggers
</subsectionHeader>
<bodyText confidence="0.997965647058824">
MEMORY-BASED LEARNING (MB), described
by Daelemans et al. (1996), is a case-based
approach where new items are classified on
the basis of similarities to the earlier exam-
ples stored in memory during learning. In this
study, decision tree induction, called IG-TREE,
was re-implemented for Swedish by Berthelsen,
based on the description given in Zavrel, et
al. (1999)1. Here, an instance is represented
by a vector where the elements are the dif-
ferent features of the instance. Information
gain is used to determine at each node in the
tree which feature should be used to create
new branches. The implementation of the sys-
tem used in this study contains information
about the focus word, the preceding and fol-
lowing word forms, the two preceding (and al-
</bodyText>
<footnote confidence="0.964463">
1-The reimplementation of the Swedish tagger was
necessary because it was not available on the ILK
web page (http://ilk.kub.nl/software.html). The results
given by the re-implemented tagger are comparable to
the results reported by (Daelemans et al., 1996)
</footnote>
<bodyText confidence="0.999763734375">
ready disambiguated) tags and the one follow-
ing (still ambiguous) tag for known words. For
unknown words, information about capitaliza-
tion, the presence of a hyphen or a numeral
feature, the preceding tag, the focus word, the
ambiguous right tag and the last three letters
occurring in the word is used.
The MAXIMUM ENTROPY (ME) framework,
called MXPOST, is described by Ratnaparkhi
(1996). It is a probabilistic classification-based
approach based on a Maximum Entropy model
where contextual information is represented as
binary features that are used simultaneously in
order to predict the PoS tag. The default binary
features include the current word, the follow-
ing and preceding two words and the preceding
two tags. For rare and unknown words the first
and last four characters are included in the fea-
tures, as well as information about whether the
word contains uppercase characters, hyphens or
numbers. The tagger uses a beam search in or-
der to find the most probable sequence of tags.
For known words it generates the possible tags,
and for unknown words it generates all tags in
the tag set. The tag sequence with the highest
probability is chosen.
TRANSFORMATION-BASED LEARNING (TBL),
developed by Brill (1995), is a rule-based ap-
proach that learns by detecting errors. It be-
gins with an unannotated text that is labeled
by an initial-state annotator in a heuristic fash-
ion. Known words (according to a lexicon) are
annotated with their most frequent tag while
unknown words receive an initial tag (e.g. the
most frequently occurring tag in the corpus).
Then, an ordered list of rules learned during
training is applied deterministically to change
the tags of the words according to their con-
texts. TBL uses a context of three preceding
and following words and/or tags of the focus
word. Unknown words are first assumed to be
nouns and handled by prefix and suffix analy-
sis by looking at the first/last one to four let-
ters, capitalization feature and adjacent word
co-occurrence.
TRIGRAMS&apos;N&apos;TAGS (TNT) is a statistical ap-
proach, developed by Brants (2000). The tag-
ger is a trigram Hidden Markov Model and uses
the Viterbi algorithm with beam search for fast
processing. The states represent tags and the
transition probabilities depend on pairs of tags.
The system uses maximum likelihood probabil-
ities derived from the relative frequencies. The
main smoothing technique implemented by de-
fault is linear interpolation. Unknown words are
handled by suffix analysis, i.e. up to the last ten
letters of the word. Additionally, information
about capitalization is included as default.
Since the main goal is to evaluate the sys-
tems as they are available, all systems are used
with the default settings according to their doc-
umentation. The taggers were retrained on the
Swedish training data that will be described
next.
</bodyText>
<subsectionHeader confidence="0.988498">
2.2 Data
</subsectionHeader>
<bodyText confidence="0.985755047619048">
Swedish belongs to the Scandinavian, North
Germanic family of the Germanic branch of
Indo-European languages. It is morphologi-
cally richer than for example English. Nouns in
general have two gender distinction. The gen-
ders are marked mainly by articles, adjectives,
anaphoric pronouns and in plural endings. As
in English, nouns can appear with or without
articles. There are, however, definite and in-
definite articles that agree with the head noun
in gender, number and definiteness. Further-
more, adjectives have gender, definiteness and
plurality markers. Thus, in a noun phrase both
articles and adjectives agree in number, gen-
der and definiteness with the head noun. Also,
compound nouns are frequent and productive.
Verbs lack markers for person or number of the
subject but retain tense including complex tense
forms. From a syntactic point of view, Swedish
has subject-verb-object (svo) order in indepen-
dent declarative sentences, as well as in subor-
dinate clauses, similar to English. However, in
subordinate clauses the sentence adverbs nor-
mally precede the finite verb and the perfect
auxiliary can be omitted.
All experiments presented in this paper were
run on the second version of Stockholm-Umeh
Corpus (suc) (Ejerhed et al., 1992). The cor-
pus is balanced, consisting of over one million
PoS tagged words taken from different text gen-
res in Swedish. The corpus used in this study
is annotated with a Swedish version of PAROLE
tags2. The tag set consists of totally 139 tags
2Thanks to Britt Hartmann at the Department of
Linguistics, Stockholm University for making the second
version of suc with PAROLE tags available.
and encodes part-of-speech as well as morpho-
logical features.
The corpus was randomly divided into ten ap-
proximately equal parts, sentence by sentence,
in order to get subsets containing different gen-
res.
</bodyText>
<subsectionHeader confidence="0.992686">
2.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.974223575">
There are many ways in which evaluation can
proceed. The type and the size of the tag set,
the training and test data are all factors that
have an effect on the results. Therefore, the
evaluation of the learning methods is carried out
from three different aspects.
First, the accuracy of each classifier is deter-
mined using the entire tag set (of 139 different
tags), and one part (10%) of the suc corpus
as training data. The reason for the small size
of the training data is that two of the learning
algorithms are very time-consuming.
Secondly, the effect of training on different
sizes of tag sets is examined. This is done be-
cause different NLP applications require anno-
tation of various explicitness, e.g. full morpho-
logical analysis is not always needed.
Thirdly, the size of the training corpus is var-
ied from one thousand up to one million tokens
for each algorithm in order to find out how the
size of the learning data influences the error rate
for each approach.
For a fair comparison of the methods, each al-
gorithm was trained in each experiment on the
same part of the suc corpus to build four clas-
sifiers. Then, in all cases, each classifier was
evaluated on the same test set. In all the exper-
iments, the training and the test set were dis-
joint and the test sets included unknown words.
When tagging, the classifiers are allowed to as-
sign exactly one tag to each token in the test.
The systems are evaluated from several per-
spectives. First, the overall tagging accuracy is
computed by calculating the percentage of cor-
rectly assigned tags (given by the output of each
classifier) in the test set compared to the cor-
rectly annotated benchmark.
Number of correctly tagged tokens
Accuracy =
Total number of tokens
</bodyText>
<equation confidence="0.724722">
(1)
</equation>
<bodyText confidence="0.999747470588235">
Additionally, since unknown tokens are more
difficult to process than known tokens for which
the possible tags are available from the lexicon
during learning, separate accuracy is given for
known and unknown words. Furthermore, for
some of the experiments, the types of errors will
be described but due to the sparse space, the
recall and precision rates cannot be given for
each category.
At the starting point of this study, the aim
was to use 10-fold cross validation as the evalu-
ation method for the experiments, but unfor-
tunately that proved to be impracticable be-
cause of the long learning time for two of the
approaches (see Section 3.4). Therefore, most
of the experiments were accomplished on 100k
tokens for learning and 100k for test.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="method">
3 Results
</sectionHeader>
<subsectionHeader confidence="0.999364">
3.1 System performance
</subsectionHeader>
<bodyText confidence="0.978157333333333">
In order to find out the average accuracy of the
systems, each algorithm was trained on 7388
sentences, including 115862 tokens and 24572
types. The corpus used for testing consists of
totally 7464 sentences, 117685 tokens, 24492
types, of which 85.23% are known and 14.77%
are unknown words.
The baseline performance is 77.37% and is
obtained on the test data by selecting the PoS
tag that is most frequently associated with the
current word.
The results, given in Table 1, show that all
systems outperformed the baseline, but the per-
formance of the taggers is significantly lower
than is reported for English.
The statistical trigram approach, TNT, has
the highest overall accuracy and also succeeds
best in the annotations of known and un-
known words. The MXPOST tagger (ME) shows
high performance because of the high pre-
cision of the annotation of unknown words.
The Transformation-Based learner (TBL) man-
ages to disambiguate known words but suc-
ceeds poorly on unknown words compared to
the other systems. The memory-based method
(MB) is slightly better than TBL because of its
better success in the annotation of unknown
words.
The analysis of the errors made by each clas-
sifier on individual tags shows that all systems
failed most frequently on words belonging to the
open classes (nouns, verbs, and adjectives), as
could be predicted, since these categories are
morphologically more complex and 95% of the
Table 1: The tagging accuracy for all the words,
and the accuracy of known and unknown words
are given for each classifier. Training and test
set are disjoint, consisting of 100k tokens, re-
spectively. Tag Set includes 139 tags.
</bodyText>
<table confidence="0.997295">
ACCURACY MB ME TBL TNT
TOTAL % 89.28 91.20 89.06 93.55
KNOWN % 92.85 93.34 94.35 95.50
UNKNOWN % 68.65 78.85 58.52 82.29
</table>
<tableCaption confidence="0.7354006">
Table 2: The tagging accuracy for all the words,
for known and unknown words are given for
each classifier when tagging and testing on the
original tag set but not considering the correct-
ness of the subtags in the evaluation.
</tableCaption>
<table confidence="0.9968255">
ACCURACY MB ME TBL TNT
TOTAL % 92.28 93.49 92.39 95.31
KNOWN % 94.69 94.72 95.63 96.53
UNKNOWN % 78.37 86.39 73.70 88.24
</table>
<bodyText confidence="0.99929228">
unknown words belong to these classes. Among
the closed categories, verbal particles, preposi-
tions and adverbs are often confused, as well
as determiners and pronouns. These belong to
well-known ambiguity classes in Swedish.
The difference in the type of errors among
the classifiers lies in the frequency of the type of
confusions. For example, by looking at the most
frequent types of fault each system makes by
counting precision and recall for each category,
we found that TBL and MB more often make
mistakes in the morphological analysis of cate-
gories (e.g. plural is often annotated as singu-
lar) while ME and TNT more frequently confuse
PoS categories among ambiguity classes. This
fact can be utilized for improvement of accuracy
by using the large tag set marking inflectional
properties of a word in training and tagging but
not considering the correctness of the morpho-
logical tags in the evaluation. The results are
shown in Table 2.
By comparing the results from Table 1 and
2, it is clear that accuracy is improved by the
removal of the morphological tags in the eval-
uation. This can be useful for applications in
</bodyText>
<figure confidence="0.874498833333333">
139 tags
048 tags
044 tags
039 tags
026 tags
MB ME TBL TnT
</figure>
<figureCaption confidence="0.9790635">
Figure 1: The error rate for each classifier when
training on 139, 48, 44, 39 and 26 tags.
</figureCaption>
<bodyText confidence="0.950176">
which high performance is required but mor-
phological analysis is not needed.
</bodyText>
<subsectionHeader confidence="0.999723">
3.2 Training on different size of tag sets
</subsectionHeader>
<bodyText confidence="0.999980340425532">
In some NLP applications, a tag set with com-
plete morphological tags is not needed. One
example is parsing where the PoS tag, and in
some cases a few morphological tags for han-
dling agreement, is enough. Therefore, the orig-
inal tag set was mapped into smaller tag sets.
The goal was to construct tag sets that can
be useful for different applications. The most
reduced tag set consists of the 26 PoS tags
used in the second version of suc. The PoS
tags include some subcategorization informa-
tion. There are, for example, distinctions be-
tween common and proper nouns, ordinal and
cardinal numbers, possessive, possessive wh-,
and personal/indefinite pronouns. The other
tag sets, consisting of 39, 44 and 48 tags re-
spectively are all subsets of the original PAROLE
version. The tags were mapped together in cat-
egories designed for parsing. The difference be-
tween the smaller subsets is in the types of mor-
phological tags that are included in order to be
able to handle agreement in NPs.
As is shown in Figure 1, the number of errors
made by each algorithm is higher when using a
large tag set. This is not surprising since the
classification is more difficult when the system
has to choose from many categories. However,
the amount with which the error rate decreases
with a smaller tag set differs from system to sys-
tem. By decreasing the size of the tag set from
139 to 26 tags, the error rate decreases by 38%
for TBL, 29% for ME, and 23% for MB and TNT.
Thus, TBL and ME seem to be more sensitive to
the size of tag set than MB and TNT. Further-
more, when considering the results from train-
ing on between 39 and 48 tags (i.e. the distance
between the amount of tags is small), the sys-
tem performances show rather similar results.
In the case of TNT, the error rate when train-
ing on 39 tags even increases with 3% compared
to when training is done on 44 tags. Here, the
type of information the tags bear seems to be
important. Thus, the size of the tag set as well
as the type of information are crucial factors for
system performance.
Next, the results from training on different
sizes of training corpora will be described.
</bodyText>
<subsectionHeader confidence="0.999155">
3.3 Training on different sizes of data
</subsectionHeader>
<bodyText confidence="0.999995121212121">
In order to examine how the size of training
data influences the performance of the classi-
fiers, each algorithm was trained ten times on
the same data set of various sizes from one thou-
sand to one million tokens: 1k, 2k, 5k, 10k, 20k,
50k, 100k, 200k, 500k and 1000k tokens, respec-
tively. Then, the same test set was annotated
by each classifier.
Unfortunately, the results for ME and TBL
when training on one million words cannot be
reported in this paper. The learning algorithms
will still be occupied after the deadline for the
final version of this manuscript. The results
will be published on the author&apos;s web page
http://www.speech.kth.serbea/research.html
as soon as the learners have finished their
struggle.
The total error rate, i.e. the percentage of er-
roneous tags, is shown in Figure 2 for each clas-
sifier. It is not surprising that as the size of the
training data increases, the error rate decreases.
This is partly due to the fact that the number of
unknown words is significantly smaller in large
training data as shown in the first and second
columns of Table 3.
There are some remarkable differences be-
tween the systems in their sensitivity to the size
of the training data. ME shows most sensitiv-
ity, i.e. when increasing the training data from
one thousand to five hundred thousand tokens,
the error rate decreases by 88%. TBL, on the
other hand, shows less sensitivity, the error rate
is decreasing by 50%. This is due to the fact
</bodyText>
<figureCaption confidence="0.95585075">
Figure 2: Error rates when training on 1000
to 1 million tokens, totally ten training corpora
of various sizes, seen as ten columns for each
classifier.
</figureCaption>
<tableCaption confidence="0.885131">
Table 3: The size of the training data, the per-
</tableCaption>
<bodyText confidence="0.556051">
centage of unknown tokens in the test set, and
the error rate, i.e. the percentage of wrongly
annotated unknown words for each system.
</bodyText>
<table confidence="0.705555318181818">
TOKEN UNKNOWN MB ME TBL TNT
(K) (7) large small
1 50.2 63.9 60.9 25.5 66.5 54.8
2 44.2 56.4 54.5 28.1 69.7 46.3
5 37.2 49.2 42.1 30.8 63.9 37.3
10 32.8 45.3 36.7 29.5 58.6 31.6
20 27.8 40.3 31.7 29.0 50.7 26.6
50 20.8 35.2 25.5 32.8 44.5 21.9
100 15.6 32.4 21.5 32.5 34.2 18.4
200 11.9 30.2 19.6 26.6 27.1 15.9
500 8.1 28.2 18.8 32.0 32.5 14.1
1000 5.9 28.5 * * 12.7
▪ 11(
▪ 2k
[5k
10k
E 20k
E 50k
▪ 100k
S 200k
▪ 500k
▪ 1000k
</table>
<bodyText confidence="0.998037">
that there is a possibility to use a large lexi-
con listing all possible tags for a word that the
tagger can choose from during tagging. Such a
possibility is not available in the current public
implementations of the other systems. The us-
age of the large lexicon decreases the error rate
when training is performed on a small corpus
only. Thus, TBL is useful when the user does
not have access to a large correctly annotated
corpus but only to a large lexicon. TBL can be
put to use in the development of large corpora
by applying a boot-strapping procedure.
Since PoS tagging has two main purposes,
namely the annotation of unknown words and
the disambiguation of the known words accord-
ing to their context, the error rates are pre-
sented separately for unknown and known to-
kens as well. Additionally, because a large lex-
icon may not be available in some cases or for
some languages, results for TBL are also given
when tagging is performed on the basis of a
small lexicon derived from the training data
only.
The error rates for the annotation of unknown
words is shown in columns 3-7 in Table 3. For
MB, ME, TBL with a small lexicon and TNT,
larger training data improves the overall accu-
racy. The error rate decreases by 75% for TNT,
70% for ME, 56% for MB and 51% for TBL with a
small lexicon when increasing the training data
from one thousand tokens to five hundred thou-
sand tokens.
TNT conquers TBL as well as the other systems
in the correct annotation of unknown words
when the size of the training corpus is 20k to-
kens or more. The success of TNT can be ex-
plained by the way the system handles unknown
words; The tag probabilities are set on the basis
of the word endings, i.e. suffix analysis up to ten
final characters of a word. The other systems do
not look at as many characters, which has an ef-
fect on the results because inflectional and ag-
glutinative languages with a complex morpho-
logical structure have suffix combinations longer
than four characters.
The annotation of known words, i.e. the mor-
phological disambiguation, seems to be an easier
task for all the systems included in this study
as is shown in Table 4. Here, TNT shows the
lowest error rates in all the experiments. The
ME approach succeeds poorly when training on
a small corpus compared to the other three sys-
tems. In the case of large training corpora (100k
and above), the differences between the systems
converge due to the similar linguistic informa-
tion implemented in the systems. For example,
all systems use a lexicon listing possible tags
for a token derived from the training data and
contextual information in order to disambiguate
the target word. The differences between the
systems can be explained by the various win-
dow sizes implemented in the systems.
Table 4: The size of the training data, the per-
centage of known tokens in the test set, and the
error rate, i.e. the percentage of wrongly anno-
tated known words for each system.
</bodyText>
<table confidence="0.994850333333333">
TOKEN KNOWN MB ME TBL TNT
(K) (%) large small
1 49.8 10.1 32.2 9.4 11.0 9.1
2 55.8 10.2 25.2 8.6 10.2 8.3
5 62.8 9.2 18.5 8.1 9.2 7.0
10 67.2 8.7 14.7 8.9 9.2 6.7
20 72.2 8.3 11.4 7.0 7.8 5.9
50 79.2 7.9 8.4 6.6 6.9 5.2
100 84.4 7.2 7.1 5.7 5.7 4.6
200 88.1 6.7 5.8 5.8 5.0 4.1
500 92.9 5.9 4.7 6.7 4.3 3.7
1000 94.1 4.6 * * * 3.5
</table>
<subsectionHeader confidence="0.714714">
3.4 Time for learning and test
</subsectionHeader>
<bodyText confidence="0.9999801">
Another approach that has to be mentioned
when comparing algorithms is the time it takes
to learn from the training data and to test new
texts. There are significant differences in the
learning time between the systems3. TNT is
able to learn from 100k tokens within one sec-
ond and manages to tag a text containing the
same amount of data in three seconds. MB is
also fast in both training and tagging, i.e. the
learning and annotation are carried out within
a minute. ME and TBL, on the other hand, are
time-consuming. Training on 100k words takes
approximately one day for both systems.
When large training corpora are used (200k
tokens or above) the training time can be ex-
pressed in a few seconds for TNT, in a minute for
MB and in weeks or months for ME and TBL de-
pending on the size of the corpus. TBL is slower
in training than ME, but as fast as MB in tag-
ging.
</bodyText>
<sectionHeader confidence="0.983409" genericHeader="evaluation">
4 Discussion and future work
</sectionHeader>
<bodyText confidence="0.968472">
This paper has given a comparison of four state-
of-the-art data-driven PoS taggers applied to
Swedish. Although the performance of the tag-
gers is lower than has been reported for English,
the accuracy and the effect of training size are
</bodyText>
<footnote confidence="0.69556">
3In this study, the experiments were run on a Pentium
III, 800 MHz computer running Linux.
</footnote>
<bodyText confidence="0.999914410714286">
in some cases comparable to the results for En-
glish.
In the case of TNT, tagging accuracy is high
for known tokens even with a small amount of
training data just as has been reported for En-
glish by Brants (2000).
Also, the performance of MB, ME and TBL de-
scribed in this study are comparable to the re-
sults given by van Halteren, et al. (1998) since
both studies were performed on the same size
of training data consisting of 100k tokens. In
both cases, highest performance is achieved by
ME, followed by MB and TBL.
The result that ME is generally better in the
annotation of unknown words than MB is also
supported by the study on the systematic eval-
uation of MB and ME (De Pauw Si Daelemans,
2000).
In Zavrel Si Daelemans, (2000) the same al-
gorithms were used as in this paper but the al-
gorithms were trained on Dutch training data of
small size: 5k, 10k, and 20k tokens. In their ex-
periments, TNT achieved the highest accuracy
while in our experiment TBL showed highest
performance on small training data. A possible
explanation could be that a small lexicon, in-
cluding the words from the training data only,
was used when tagging with TBL without using
any additional lexicon available for Dutch.
Future work includes the optimization of the
taggers to better fit Swedish. For example, in
TNT, several smoothing techniques are available
that were not tested in this study. In the case
of TBL and ME, the maximum length of the
first/last characters should be increased in order
to improve the system performance on unknown
words since Swedish has a more complex mor-
phological structure - suffixes are often longer
than four characters.
Additionally, in order to further improve tag-
ging accuracy, one could determine the best
combinations of the approaches by constructing
ensembles of classifiers.
It would be very interesting to evaluate the
taggers in a systematic way on other languages
belonging to different language types; in par-
ticular agglutinative and inflective languages
with complex morphological structure and/or
free word order. The linguistic information in-
cluded in the systems seems in many cases to
be optimized for English and other Germanic
or perhaps Romance languages, but not for, for
example, Uralic or Turkic languages. The task
of data-driven PoS taggers is not completed un-
til we have systems with high performance for
the various language types.
</bodyText>
<sectionHeader confidence="0.994501" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.989955545454545">
In this study, four state-of-the-art data-driven
algorithms have been compared based on PoS
tagging of Swedish texts. The effects of the
size of the tag set and the size of the training
data have been examined by a systematic
evaluation of the systems. The results show
interesting differences between the classifiers.
The TRIGRAMS&apos;N&apos;TAGS (TNT) approach has the
highest overall accuracy, succeeds best in the
annotation of known as well as unknown words,
and is also fastest in both training and tagging.
TRANSFORMATION-BASED LEARNING (TBL)
has high performance on small training data,
hence can be used as an aid when building large
corpora by using a boot-strapping procedure.
MXPOST (ME) has a high error rate in the
annotation of known tokens when training on
small corpora, but succeeds similarly to the
other approaches when training is performed on
large training data. MEMORY-BASED LEARNING
(MB) is fast in both training and test and
succeeds well in morphological disambiguation.
Acknowledgements
Many thanks go to my supervisor Rolf Carl-
son for his endless support and encouragement,
and to the anonymous reviewers for their help-
ful suggestions and comments. Thanks also to
Sheri Hunnicutt and Jens Edlund for reading
and comments. Last, but not least, I would like
to thank all the researchers who created the tag-
gers used in this study for the excellent systems
they have developed and making them available
to the public.
</bodyText>
<sectionHeader confidence="0.998665" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999749418181818">
T. Brants. 2000. TnT - A Statistical Part-
of-Speech Tagger. In Proceedings of the 6th
Applied Natural Language Processing Confer-
ence. Seattle, Washington, USA.
E. Brill. 1994. Some Advances in Rule-Based
Part of Speech Tagging. In Proceedings of the
12th National Conference on Artificial Intel-
ligence (AAAI-94). Seattle, Washington.
E. Brill and J. Wu. 1998. Classifier Combina-
tion for Improved Lexical Combination. In
Proceedings of the 17th International Con-
ference on Computational Linguistics (ACL-
98). Montreal, Canada.
W. Daelemans, J. Zavrel, P. Berck, and
S.E. Gillis. 1996. MBT: a Memory-Based
Part of Speech Tagger-Generator. In Proceed-
ings of Fourth Workshop on Very Large Cor-
pora (VLC-96). pp. 14-27. Copenhagen, Den-
mark.
G. De Pauw, W. Daelemans. 2000. The Role
of Algorithm Bias vs Information Source
in Learning Algorithms for Morphosyntactic
Disambiguation. In Proceedings of Computa-
tional Natural Language Learning pp. 19-24.
Lisbon, Portugal.
E. Ejerhed, G. Kallgren, 0. Wennstedt, and
M. Astrom. 1992. The Linguistic Annotation
System of the Stockholm-UmeciProject. De-
partment of General Linguistics, University
of Umeh.
A. Ratnaparkhi. 1996. A Maximum En-
tropy Model for Part-of-Speech Tagging. In
Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP-96). Philadelphia, PA, USA.
H. van Halteren, J. Zavrel, and W. Daelemans.
1998. Improving Data-Driven Wordclass Tag-
ging by System Combination. In Proceedings
of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-
98). Montreal, Canada.
H. van Halteren. 1999. Syntactic Wordclass
Tagging. Kluwer Academic Publishers. Dor-
drecht, The Netherlands.
J. Zavrel, and W. Daelemans. 1999. Recent Ad-
vances in Memory-Based Part-of-Speech Tag-
ging. In Proceedings of the VI Simposio In-
ternacional de Comunicacion Social. pp. 590-
597. Santiago de Cuba.
J. Zavrel, and W. Daelemans. 2000. Bootsrap-
ping a Tagged Corpus trough Combination of
Existing Heterogeneous Taggers. In Proceed-
ings of the 2nd International Conference on
Language Resources and Evaluation (LREC
2000). pp. 17-20. Athens, Greece.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.522626">
<title confidence="0.962926">Comparing Data-Driven Learning Algorithms for PoS Tagging of Swedish</title>
<author confidence="0.697498">Beata</author>
<affiliation confidence="0.972662333333333">Centre for Speech Department of Speech, Music and Royal Institute of</affiliation>
<address confidence="0.974683">SE-100 44, Stockholm,</address>
<email confidence="0.996905">beaOspeech.kth.se</email>
<abstract confidence="0.9830629375">The aim of this study is a systematic evaluation and comparison of four state-of-the-art datadriven learning algorithms applied to part of speech tagging of Swedish. The algorithms included in this study are Hidden Markov Model, Maximum Entropy, Memory-Based Learning, and Transformation-Based Learning. The systems are evaluated from several aspects. Both the effects of tag set and the effects of the size of training data are examined. The accuracy is calculated as well as the error rate for known and unknown tokens. The results show differences between the approaches due to the different linguistic information built into the systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT - A Statistical Partof-Speech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference.</booktitle>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="2998" citStr="Brants, 2000" startWordPosition="462" endWordPosition="463">ntropy framework as it was implemented by Ratnaparkhi (1996) achieved the highest accuracy in total and in the annotation of ambiguous and unknown words. Van Halteren et al. (1998) included Maximum Entropy, Memory-Based, Statistical trigram, and Transformation-Based approaches in the ensemble of classifiers and trained on 80% of the LOB corpus consisting of 931062 tokens. Similarly to the results given by Brill Si Wu (1998), the Maximum Entropy framework achieved highest accuracy. In contrast to the previous studies, Zavrel and Daelemans (2000) report that a statistical trigram approach, TNT (Brants, 2000) gave the best result over the Maximum Entropy, Memory-Based, and Transformation-Based approaches when trained on 90% and tested on 10% of the Spoken Dutch Corpus consisting of 5,000, 10,000 and 20,000 tokens respectively. However, the goal of these studies was not a systematic evaluation and comparison of the classifiers. De Pauw and Daelemans (2000) describe a systematic comparison between the Maximum Entropy framework and Memory-Based Learning performed on the LOB-corpus. They report that the overall tagging accuracy of the methods are similar, although Maximum Entropy succeeds better in ta</context>
<context position="7787" citStr="Brants (2000)" startWordPosition="1240" endWordPosition="1241">frequent tag while unknown words receive an initial tag (e.g. the most frequently occurring tag in the corpus). Then, an ordered list of rules learned during training is applied deterministically to change the tags of the words according to their contexts. TBL uses a context of three preceding and following words and/or tags of the focus word. Unknown words are first assumed to be nouns and handled by prefix and suffix analysis by looking at the first/last one to four letters, capitalization feature and adjacent word co-occurrence. TRIGRAMS&apos;N&apos;TAGS (TNT) is a statistical approach, developed by Brants (2000). The tagger is a trigram Hidden Markov Model and uses the Viterbi algorithm with beam search for fast processing. The states represent tags and the transition probabilities depend on pairs of tags. The system uses maximum likelihood probabilities derived from the relative frequencies. The main smoothing technique implemented by default is linear interpolation. Unknown words are handled by suffix analysis, i.e. up to the last ten letters of the word. Additionally, information about capitalization is included as default. Since the main goal is to evaluate the systems as they are available, all </context>
<context position="25540" citStr="Brants (2000)" startWordPosition="4370" endWordPosition="4371">aining than ME, but as fast as MB in tagging. 4 Discussion and future work This paper has given a comparison of four stateof-the-art data-driven PoS taggers applied to Swedish. Although the performance of the taggers is lower than has been reported for English, the accuracy and the effect of training size are 3In this study, the experiments were run on a Pentium III, 800 MHz computer running Linux. in some cases comparable to the results for English. In the case of TNT, tagging accuracy is high for known tokens even with a small amount of training data just as has been reported for English by Brants (2000). Also, the performance of MB, ME and TBL described in this study are comparable to the results given by van Halteren, et al. (1998) since both studies were performed on the same size of training data consisting of 100k tokens. In both cases, highest performance is achieved by ME, followed by MB and TBL. The result that ME is generally better in the annotation of unknown words than MB is also supported by the study on the systematic evaluation of MB and ME (De Pauw Si Daelemans, 2000). In Zavrel Si Daelemans, (2000) the same algorithms were used as in this paper but the algorithms were trained</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT - A Statistical Partof-Speech Tagger. In Proceedings of the 6th Applied Natural Language Processing Conference. Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some Advances in Rule-Based Part of Speech Tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94).</booktitle>
<location>Seattle, Washington.</location>
<marker>Brill, 1994</marker>
<rawString>E. Brill. 1994. Some Advances in Rule-Based Part of Speech Tagging. In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94). Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier Combination for Improved Lexical Combination.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics (ACL98).</booktitle>
<location>Montreal, Canada.</location>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier Combination for Improved Lexical Combination. In Proceedings of the 17th International Conference on Computational Linguistics (ACL98). Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S E Gillis</author>
</authors>
<title>MBT: a Memory-Based Part of Speech Tagger-Generator.</title>
<date>1996</date>
<booktitle>In Proceedings of Fourth Workshop on Very Large Corpora (VLC-96).</booktitle>
<pages>14--27</pages>
<location>Copenhagen,</location>
<contexts>
<context position="4776" citStr="Daelemans et al. (1996)" startWordPosition="746" endWordPosition="749">has received significant attention from computational linguists, it seems appropriate to evaluate the taggers on a different language; especially, as the taggers are said to be language-independent. The experiments described in this paper are based on well-known algorithms that have implementations for the PoS tagging approach. Common to these taggers is that they are claimed to be language- and tagsetindependent, easy to apply to new domains, languages and tag sets and available to the public. Each approach will be briefly described below. 2.1 Taggers MEMORY-BASED LEARNING (MB), described by Daelemans et al. (1996), is a case-based approach where new items are classified on the basis of similarities to the earlier examples stored in memory during learning. In this study, decision tree induction, called IG-TREE, was re-implemented for Swedish by Berthelsen, based on the description given in Zavrel, et al. (1999)1. Here, an instance is represented by a vector where the elements are the different features of the instance. Information gain is used to determine at each node in the tree which feature should be used to create new branches. The implementation of the system used in this study contains informatio</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S.E. Gillis. 1996. MBT: a Memory-Based Part of Speech Tagger-Generator. In Proceedings of Fourth Workshop on Very Large Corpora (VLC-96). pp. 14-27. Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G De Pauw</author>
<author>W Daelemans</author>
</authors>
<title>The Role of Algorithm Bias vs Information Source in Learning Algorithms for Morphosyntactic Disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of Computational Natural Language Learning</booktitle>
<pages>pp.</pages>
<location>Lisbon,</location>
<marker>De Pauw, Daelemans, 2000</marker>
<rawString>G. De Pauw, W. Daelemans. 2000. The Role of Algorithm Bias vs Information Source in Learning Algorithms for Morphosyntactic Disambiguation. In Proceedings of Computational Natural Language Learning pp. 19-24. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ejerhed</author>
<author>G Kallgren</author>
</authors>
<date>1992</date>
<booktitle>The Linguistic Annotation System of the Stockholm-UmeciProject.</booktitle>
<institution>Department of General Linguistics, University of Umeh.</institution>
<marker>Ejerhed, Kallgren, 1992</marker>
<rawString>E. Ejerhed, G. Kallgren, 0. Wennstedt, and M. Astrom. 1992. The Linguistic Annotation System of the Stockholm-UmeciProject. Department of General Linguistics, University of Umeh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-96).</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="2445" citStr="Ratnaparkhi (1996)" startWordPosition="376" endWordPosition="377">ers are evaluated by the researchers differs greatly, which makes it difficult to compare the performance of the systems. Several recent studies report comparisons of data-driven PoS taggers. However, the purpose of these studies has been primarily to attain higher tagging performance by means of different system combinations. Brill Si Wu (1998) trained statistical unigram and trigram, Maximum Entropy and Transformation-Based learning on the English Penn Treebank Wall Street Journal Corpus consisting of 1.1 million words. They showed that the Maximum Entropy framework as it was implemented by Ratnaparkhi (1996) achieved the highest accuracy in total and in the annotation of ambiguous and unknown words. Van Halteren et al. (1998) included Maximum Entropy, Memory-Based, Statistical trigram, and Transformation-Based approaches in the ensemble of classifiers and trained on 80% of the LOB corpus consisting of 931062 tokens. Similarly to the results given by Brill Si Wu (1998), the Maximum Entropy framework achieved highest accuracy. In contrast to the previous studies, Zavrel and Daelemans (2000) report that a statistical trigram approach, TNT (Brants, 2000) gave the best result over the Maximum Entropy,</context>
<context position="6111" citStr="Ratnaparkhi (1996)" startWordPosition="964" endWordPosition="965">e Swedish tagger was necessary because it was not available on the ILK web page (http://ilk.kub.nl/software.html). The results given by the re-implemented tagger are comparable to the results reported by (Daelemans et al., 1996) ready disambiguated) tags and the one following (still ambiguous) tag for known words. For unknown words, information about capitalization, the presence of a hyphen or a numeral feature, the preceding tag, the focus word, the ambiguous right tag and the last three letters occurring in the word is used. The MAXIMUM ENTROPY (ME) framework, called MXPOST, is described by Ratnaparkhi (1996). It is a probabilistic classification-based approach based on a Maximum Entropy model where contextual information is represented as binary features that are used simultaneously in order to predict the PoS tag. The default binary features include the current word, the following and preceding two words and the preceding two tags. For rare and unknown words the first and last four characters are included in the features, as well as information about whether the word contains uppercase characters, hyphens or numbers. The tagger uses a beam search in order to find the most probable sequence of ta</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-96). Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving Data-Driven Wordclass Tagging by System Combination.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL98).</booktitle>
<location>Montreal, Canada.</location>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improving Data-Driven Wordclass Tagging by System Combination. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL98). Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
</authors>
<title>Syntactic Wordclass Tagging.</title>
<date>1999</date>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Dordrecht, The Netherlands.</location>
<marker>van Halteren, 1999</marker>
<rawString>H. van Halteren. 1999. Syntactic Wordclass Tagging. Kluwer Academic Publishers. Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Recent Advances in Memory-Based Part-of-Speech Tagging.</title>
<date>1999</date>
<booktitle>In Proceedings of the VI Simposio Internacional de Comunicacion Social.</booktitle>
<pages>590--597</pages>
<location>Santiago de</location>
<marker>Zavrel, Daelemans, 1999</marker>
<rawString>J. Zavrel, and W. Daelemans. 1999. Recent Advances in Memory-Based Part-of-Speech Tagging. In Proceedings of the VI Simposio Internacional de Comunicacion Social. pp. 590-597. Santiago de Cuba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Bootsrapping a Tagged Corpus trough Combination of Existing Heterogeneous Taggers.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>17--20</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2935" citStr="Zavrel and Daelemans (2000)" startWordPosition="450" endWordPosition="453">ournal Corpus consisting of 1.1 million words. They showed that the Maximum Entropy framework as it was implemented by Ratnaparkhi (1996) achieved the highest accuracy in total and in the annotation of ambiguous and unknown words. Van Halteren et al. (1998) included Maximum Entropy, Memory-Based, Statistical trigram, and Transformation-Based approaches in the ensemble of classifiers and trained on 80% of the LOB corpus consisting of 931062 tokens. Similarly to the results given by Brill Si Wu (1998), the Maximum Entropy framework achieved highest accuracy. In contrast to the previous studies, Zavrel and Daelemans (2000) report that a statistical trigram approach, TNT (Brants, 2000) gave the best result over the Maximum Entropy, Memory-Based, and Transformation-Based approaches when trained on 90% and tested on 10% of the Spoken Dutch Corpus consisting of 5,000, 10,000 and 20,000 tokens respectively. However, the goal of these studies was not a systematic evaluation and comparison of the classifiers. De Pauw and Daelemans (2000) describe a systematic comparison between the Maximum Entropy framework and Memory-Based Learning performed on the LOB-corpus. They report that the overall tagging accuracy of the meth</context>
</contexts>
<marker>Zavrel, Daelemans, 2000</marker>
<rawString>J. Zavrel, and W. Daelemans. 2000. Bootsrapping a Tagged Corpus trough Combination of Existing Heterogeneous Taggers. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC 2000). pp. 17-20. Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>