<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000328">
<title confidence="0.969125">
Non-Parametric Bayesian Areal Linguistics
</title>
<author confidence="0.995371">
Hal Daum´e III
</author>
<affiliation confidence="0.865484333333333">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.99538">
me@hal3.name
</email>
<sectionHeader confidence="0.998571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998628777777778">
We describe a statistical model over linguis-
tic areas and phylogeny. Our model recov-
ers known areas and identifies a plausible hi-
erarchy of areal features. The use of areas
improves genetic reconstruction of languages
both qualitatively and quantitatively according
to a variety of metrics. We model linguistic
areas by a Pitman-Yor process and linguistic
phylogeny by Kingman’s coalescent.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883764705883">
Why are some languages more alike than others?
This question is one of the most central issues in his-
torical linguistics. Typically, one of three answers
is given (Aikhenvald and Dixon, 2001; Campbell,
2006). First, the languages may be related “genet-
ically.” That is, they may have all derived from a
common ancestor language. Second, the similarities
may be due to chance. Some language properties
are simply more common than others, which is of-
ten attributed to be mostly due to linguistic univer-
sals (Greenberg, 1963). Third, the languages may
be related areally. Languages that occupy the same
geographic area often exhibit similar characteristics,
not due to genetic relatedness, but due to sharing.
Regions (and the languages contained within them)
that exhibit sharing are called linguistic areas and
the features that are shared are called areal features.
Much is not understood or agreed upon in the field
of areal linguistics. Different linguists favor differ-
ent defintions of what it means to be a linguistic area
(are two languages sufficient to describe an area or
do you need three (Thomason, 2001; Katz, 1975)?),
what areal features are (is there a linear ordering of
“borrowability” (Katz, 1975; Curnow, 2001) or is
that too prescriptive?), and what causes sharing to
take place (does social status or number of speakers
play a role (Thomason, 2001)?).
In this paper, we attempt to provide a statistical
answer to some of these questions. In particular,
we develop a Bayesian model of typology that al-
lows for, but does not force, the existence of linguis-
tic areas. Our model also allows for, but does not
force, preference for some feature to be shared are-
ally. When applied to a large typological database
of linguistic features (Haspelmath et al., 2005), we
find that it discovers linguistic areas that are well
documented in the literature (see Campbell (2005)
for an overview), and a small preference for cer-
tain features to be shared areally. This latter agrees,
to a lesser degree, with some of the published hi-
erarchies of borrowability (Curnow, 2001). Finally,
we show that reconstructing language family trees is
significantly aided by knowledge of areal features.
We note that Warnow et al. (2005) have indepen-
dently proposed a model for phonological change in
Indo-European (based on the Dyen dataset (Dyen et
al., 1992)) that includes notions of borrowing. Our
model is different in that we (a) base our model on
typological features rather than just lexical patterns
and (b) we explicitly represent language areas, not
just one-time borrowing phenomena.
</bodyText>
<sectionHeader confidence="0.996992" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9979925">
We describe (in Section 3) a non-parametric, hier-
archical Bayesian model for finding linguistic areas
and areal features. In this section, we provide nec-
essary background—both linguistic and statistical—
</bodyText>
<page confidence="0.985214">
593
</page>
<note confidence="0.8922415">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 593–601,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.938837">
for understanding our model.
</bodyText>
<subsectionHeader confidence="0.986513">
2.1 Areal Linguistics
</subsectionHeader>
<bodyText confidence="0.999977285714286">
Areal effects on linguistic typology have been stud-
ied since, at least, the late 1920s by Trubetzkoy,
though the idea of tracing family trees for languages
goes back to the mid 1800s and the comparative
study of historical linguistics dates back, perhaps to
Giraldus Cambrenis in 1194 (Campbell, In press).
A recent article provides a short introduction to both
the issues that surround areal linguistics, as well as
an enumeration of many of the known language ar-
eas (Campbell, 2005). A fairly wide, modern treat-
ment of the issues surrounding areal diffusion is also
given by essays in a recent book edited by Aikhen-
vald and Dixon (2001). The essays in this book pro-
vide a good introduction to the issues in the field.
Campbell (2006) provides a critical survey of these
and other hypotheses relating to areal linguistics.
There are several issues which are basic to the
study of areal linguistics (these are copied almost
directly from Campbell (2006)). Must a linguistic
area comprise more than two languages? Must it
comprise more than one language family? Is a sin-
gle trait sufficient to define an area? How “nearby”
must languages in an area be to one another? Are
some feature more easily borrowed that others?
Despite these formal definitional issues of what
constitutes a language area and areal features, most
historical linguists seem to believe that areal effects
play some role in the change of languages.
</bodyText>
<subsectionHeader confidence="0.732335">
2.1.1 Established Linguistic Areas
</subsectionHeader>
<bodyText confidence="0.8281626">
Below, we list some of the well-known linguistic
areas; Campbell (2005) provides are more complete
listing together with example areal features for these
areas. For each area, we list associated languages:
The Balkans: Albanian, Bulgarian, Greek, Mace-
donian, Rumanian and Serbo-Croatian. (Sometimes:
Romani and Turkish)
South Asian: Languages belonging to the Dravid-
ian, Indo-Aryan, Munda, Tibeto-Burman families.
Meso-America: Cuitlatec, Huave, Mayan, Mixe-
Zoquean, Nahua, Otomanguean, Tarascan, Tequist-
latecan, Totonacan and Xincan.
North-west America: Alsea, Chimakuan, Coosan,
Eyak, Haida, Kalapuyan, Lower Chinook, Salishan,
Takelman, Tlingit, Tsimshian and Wakashan.
The Baltic: Baltic languages, Baltic German, and
Finnic languages (especially Estonian and Livo-
nian). (Sometimes many more are included, such as:
Belorussian, Lavian, Lithuanian, Norwegian, Old
Prussian, Polish, Romani, Russian, Ukranian.)
Ethiopia: Afar, Amharic, Anyuak, Awngi, Beja,
Ge’ez, Gumuz, Janjero, Kefa, Sidamo, Somali, Ti-
gre, Tigrinya and Wellamo.
Needless to say, the exact definition and extent of
the actual areas is up to significant debate. More-
over, claims have been made in favor of many lin-
guistic areas not defined above. For instance, Dixon
(2001) presents arguments for several Australian lin-
guistic areas and Matisoff (2001) defines a South-
East Asian language area. Finally, although “folk
lore” is in favor of identifying a linguistic area in-
cluding English, French and certain Norse languages
(Norwegian, Swedish, Low Dutch, High German,
etc.), there are counter-arguments to this position
(Thomason, 2001) (see especially Case Study 9.8).
</bodyText>
<subsectionHeader confidence="0.811372">
2.1.2 Linguistic Features
</subsectionHeader>
<bodyText confidence="0.99918862962963">
Identifying which linguistic features are most eas-
ily shared “areally” is a long standing problem in
contact linguistics. Here we briefly review some of
the major claims. Much of this overview is adoped
from the summary given by Curnow (2001).
Haugen (1950) considers only borrowability as
far as the lexicon is concerned. He provided evi-
dence that nouns are the easiest, followed by verbs,
adjectives, adverbs, prepositions, etc. Ross (1988)
corroborates Haugen’s analysis and deepens it to
cover morphology, syntax and phonology. He pro-
poses the following hierarchy of borrowability (eas-
iest items coming first): nouns &gt; verbs &gt; adjectives
&gt; syntax &gt; non-bound function words &gt; bound
morphemes &gt; phonemes. Coming from a “con-
straints” perspective, Moravcsik (1978) suggests
that: lexical items must be borrowed before lexi-
cal properties; inflected words before bound mor-
phemes; verbal items can never be borrowed; etc.
Curnow (2001) argues that coming up with a rea-
sonable hierarchy of borrowability is that “we may
never be able to develop such constraints.” Never-
theless, he divides the space of borrowable features
into 15 categories and discusses the evidence sup-
porting each of these categories, including: phonet-
ics (rare), phonology (common), lexical (very com-
mon), interjections and discourse markers (com-
</bodyText>
<page confidence="0.997318">
594
</page>
<bodyText confidence="0.9999225">
mon), free grammatical forms (occasional), bound
grammatical forms (rare), position of morphology
(rare), syntactic frames (rare), clause-internal syntax
(common), between-clause syntax (occasional).
</bodyText>
<subsectionHeader confidence="0.99931">
2.2 Non-parametric Bayesian Models
</subsectionHeader>
<bodyText confidence="0.999952">
We treat the problem of understanding areal linguis-
tics as a statistical question, based on a database of
typological information. Due to the issues raised in
the previous section, we do not want to commit to
the existence of a particular number of linguistic ar-
eas, or particular sizes thereof. (Indeed, we do not
even want to commit to the existence of any linguis-
tic areas.) However, we will need to “unify” the
languages that fall into a linguistic area (if such a
thing exists) by means of some statistical param-
eter. Such problems have been studied under the
name non-parametric models. The idea behind non-
parametric models is that one does not commit a pri-
ori to a particularly number of parameters. Instead,
we allow the data to dictate how many parameters
there are. In Bayesian modeling, non-parametric
distributions are typically used as priors; see Jor-
dan (2005) or Ghahramani (2005) for overviews. In
our model, we use two different non-parametric pri-
ors: the Pitman-Yor process (for modeling linguistic
areas) and Kingman’s coalescent (for modeling lin-
guistic phylogeny), both described below.
</bodyText>
<subsectionHeader confidence="0.755568">
2.2.1 The Pitman-Yor Process
</subsectionHeader>
<bodyText confidence="0.999962222222222">
One particular example of a non-parametric prior
is the Pitman-Yor process (Pitman and Yor, 1997),
which can be seen as an extension to the better-
known Dirichlet process (Ferguson, 1974). The
Pitman-Yor process can be understood as a particu-
lar example of a Chinese Restaurant process (CRP)
(Pitman, 2002). The idea in all CRPs is that there
exists a restaurant with an infinite number of ta-
bles. Customers come into the restaurant and have
to choose a table at which to sit.
The Pitman-Yor process is described by three pa-
rameters: a base rate α, a discount parameter d and
a mean distribution Go. These combine to describe
a process denoted by PY(α, d, Go). The parameters
α and d must satisfy: 0 &lt; d &lt; 1 and α &gt; −d. In
the CRP analogy, the model works as follows. The
first customer comes in and sits at any table. After
N customers have come in and seated themselves
(at a total of K tables), the Nth customer arrives. In
the Pitman-Yor process, the Nth customer sits at a
new table with probability proportional to α + Kd
and sits at a previously occupied table k with proba-
bility proportional to #k − d, where #k is the num-
ber of customers already seated at table k. Finally,
with each table k we associate a parameter Bk, with
each Bk drawn independently from Go. An impor-
tant property of the Pitman-Yor process is that draws
from it are exchangable: perhaps counterintuitively,
the distribution does not care about customer order.
The Pitman-Yor process induces a power-law dis-
tribution on the number of singleton tables (i.e., the
number of tables that have only one customer). This
can be seen by noticing two things. In general,
the number of singleton tables grows as O(αNd).
When d = 0, we obtain a Dirichlet process with the
number of singleton tables growing as O(α log N).
</bodyText>
<subsubsectionHeader confidence="0.768603">
2.2.2 Kingman’s Coalescent
</subsubsectionHeader>
<bodyText confidence="0.999927620689655">
Kingman’s coalescent is a standard model in pop-
ulation genetics describing the common genealogy
(ancestral tree) of a set of individuals (Kingman,
1982b; Kingman, 1982a). In its full form it is a dis-
tribution over the genealogy of a countable set.
Consider the genealogy of n individuals alive at
the present time t = 0. We can trace their ances-
try backwards in time to the distant past t = −oc.
Assume each individual has one parent (in genet-
ics, haploid organisms), and therefore genealogies
of [n] = 11, ... , n} form a directed forest. King-
man’s n-coalescent is simply a distribution over ge-
nealogies of n individuals. To describe the Markov
process in its entirety, it is sufficient to describe
the jump process (i.e. the embedded, discrete-time,
Markov chain over partitions) and the distribution
over coalescent times. In the n-coalescent, every
pair of lineages merges independently with rate 1,
with parents chosen uniformly at random from the
set of possible parents at the previous time step.
The n-coalescent has some interesting statistical
properties (Kingman, 1982b; Kingman, 1982a). The
marginal distribution over tree topologies is uni-
form and independent of the coalescent times. Sec-
ondly, it is infinitely exchangeable: given a geneal-
ogy drawn from an n-coalescent, the genealogy of
any m contemporary individuals alive at time t &lt; 0
embedded within the genealogy is a draw from the
m-coalescent. Thus, taking n—*oc, there is a distri-
</bodyText>
<page confidence="0.992594">
595
</page>
<bodyText confidence="0.9999861875">
bution over genealogies of a countably infinite pop-
ulation for which the marginal distribution of the ge-
nealogy of any n individuals gives the n-coalescent.
Kingman called this the coalescent.
Teh et al. (2007) recently described efficient in-
ference algorithms for Kingman’s coalescent. They
applied the coalescent to the problem of recovering
linguistic phylogenies. The application was largely
successful—at least in comparison to alternative al-
gorithms that use the same data-. Unfortunately,
even in the results they present, one can see signif-
icant areal effects. For instance, in their Figure(3a),
Romanian is very near Albanian and Bulgarian. This
is likely an areal effect: specifically, an effect due to
the Balkan langauge area. We will revisit this issue
in our own experiments.
</bodyText>
<sectionHeader confidence="0.997464" genericHeader="method">
3 A Bayesian Model for Areal Linguistics
</sectionHeader>
<bodyText confidence="0.999578292682927">
We will consider a data set consisting of N lan-
guages and F typological features. We denote the
value of feature f in language n as Xn,f. For sim-
plicity of exposition, we will assume two things: (1)
there is no unobserved data and (2) all features are
binary. In practice, for the data we use (described in
Section 4), neither of these is true. However, both
extensions are straightforward.
When we construct our model, we attempt to be
as neutral to the “areal linguistics” questions defined
in Section 2.1 as possible. We allow areas with only
two languages (though for brevity we do not present
them in the results). We allow areas with only one
family (though, again, do not present them). We are
generous with our notion of locality, allowing a ra-
dius of 1000 kilometers (though see Section 5.4 for
an analysis of the effect of radius).1 And we allow,
but do not enforce trait weights. All of this is ac-
complished through the construction of the model
and the choice of the model hyperparameters.
At a high-level, our model works as follows. Val-
ues Xn,f appear for one of two reasons: they are ei-
ther areally derived or genetically derived. A latent
variable Zn,f determines this. If it is derived areally,
then the value Xn,f is drawn from a latent variable
1An reader might worry about exchangeability: Our method
of making language centers and locations part of the Pitman-Yor
distribution ensures this is not an issue. An alternative would
be to use a location-sensitive process such as the kernel stick-
breaking process (Dunson and Park, 2007), though we do not
explore that here.
corresponding to the value preferences in the lan-
gauge area to which language n belongs. If it is de-
rived genetically, then Xn,f is drawn from a variable
corresponding to value preferences for the genetic
substrate to which language n belongs. The set of
areas, and the area to which a language belongs are
given by yet more latent variables. It is this aspect of
the model for which we use the Pitman-Yor process:
languages are customers, areas are tables and area
value preferences are the parameters of the tables.
</bodyText>
<subsectionHeader confidence="0.99653">
3.1 The formal model
</subsectionHeader>
<bodyText confidence="0.999983939393939">
We assume that the value a feature takes for a par-
ticular language (i.e., the value of Xn,f) can be ex-
plained either genetically or areally.2 We denote this
by a binary indicator variable Zn,f, where a value 1
means “areal” and a value 0 means “genetic.” We as-
sume that each Zn,f is drawn from a feature-specific
binomial parameter 7rf. By having the parameter
feature-specific, we express the fact that some fea-
tures may be more or less likely to be shared than
others. In other words, a high value of 7rf would
mean that feature f is easily shared areally, while a
low value would mean that feature f is hard to share.
Each language n has a known latitude/longitude En.
We further assume that there are K linguistic ar-
eas, where K is treated non-parametrically by means
of the Pitman-Yor process. Note that in our context,
a linguistic area may contain only one language,
which would technically not be allowed according
to the linguistic definition. When a language belongs
to a singleton area, we interpret this to mean that it
does not belong to any language area.
Each language area k (including the singleton ar-
eas) has a set of F associated parameters Ok,f, where
Ok,f is the probability that feature f is “on” in area k.
It also has a “central location” given by a longitude
and latitude denoted ck. We only allow languages
to belong to areas that fall within a given radius R
of them (distances computed according to geodesic
distance). This accounts for the “geographical” con-
straints on language areas. We denote the area to
which language n belongs as an.
We assume that each language belongs to a “fam-
ily tree.” We denote the parent of language n in the
</bodyText>
<footnote confidence="0.712741666666667">
2As mentioned in the introduction, (at least) one more option
is possible: chance. We treat “chance” as noise and model it in
the data generation process, not as an alternative “source.”
</footnote>
<page confidence="0.989476">
596
</page>
<construct confidence="0.424068">
if
Xn, f —{ Sin (0.., f) . f 01 feature values are derived genetically or areally
(0 n f) if Z n,f =
</construct>
<equation confidence="0.99681">
Zn,f — Sin(7rf) feature source is a biased coin, parameterized per feature
in — Sall(can, R) language position is uniform within a ball around area center, radius R
7rf — Set(1,1) bias for a feature being genetic/areal is uniform
(p, B) — Coalescent(7ro, mo) language hierarchy and genetic traits are drawn from a Coalescent
(a, (φ, c)) — PY(α0, do, Set(1,1) x Llni) area features are drawn Beta and centers Uniformly across the globe
</equation>
<figureCaption confidence="0.998358">
Figure 1: Full hierarchical Areal model; see Section 3.1 for a complete description.
</figureCaption>
<bodyText confidence="0.999958727272727">
family tree by pn. We associate with each node i in
the family tree and each feature f a parameter BZ,f.
As in the areal case, BZ,f is the probability that fea-
ture f is on for languages that descend from node i
in the family tree. We model genetic trees by King-
man’s coalescent with binomial mutation.
Finally, we put non-informative priors on all the
hyperparameters. Written hierarchically, our model
has the following shown in Figure 1. There, by
(p, B) — Coalescent(7ro, mo), we mean that the tree
and parameters are given by a coalescent.
</bodyText>
<subsectionHeader confidence="0.904297">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.999563882352941">
Inference in our model is mostly by Gibbs sam-
pling. Most of the distributions used are conju-
gate, so Gibbs sampling can be implemented effi-
ciently. The only exceptions are: (1) the coales-
cent for which we use the GreedyRate1 algorithm
described by Teh et al. (2007); (2) the area centers c,
for which we using a Metropolis-Hastings step. Our
proposal distribution is a Gaussian centered at the
previous center, with standard deviation of 5. Ex-
perimentally, this resulted in an acceptance rate of
about 50%.
In our implementation, we analytically integrate
out 7r and φ and sample only over Z, the coalescent
tree, and the area assignments. In some of our ex-
periments, we treat the family tree as given. In this
case, we also analytically integrate out the B param-
eters and sample only over Z and area assignments.
</bodyText>
<sectionHeader confidence="0.996875" genericHeader="method">
4 Typological Data
</sectionHeader>
<bodyText confidence="0.999639814814815">
The database on which we perform our analysis is
the World Atlas of Language Structures (henceforth,
WALS) (Haspelmath et al., 2005). The database
contains information about 2150 languages (sam-
pled from across the world). There are 139 typologi-
cal features in this database. The database is sparse:
only 16% of the possible language/feature pairs are
known. We use the version extracted and prepro-
cessed by Daum´e III and Campbell (2007).
In WALS, languages a grouped into 38 language
families (including Indo-European, Afro-Asiatic,
Austronesian, Niger-Congo, etc.). Each of these lan-
guage families is grouped into a number of language
geni. The Indo-European family includes ten geni,
including: Germanic, Romance, Indic and Slavic.
The Austronesian family includes seventeen geni,
including: Borneo, Oceanic, Palauan and Sundic.
Overall, there are 275 geni represented in WALS.
We further preprocess the data as follows. For
the Indo-European subset (hence-forth, “IE”), we re-
move all languages with &lt; 10 known features and
then remove all features that appear in at most 1/4
of the languages. This leads to 73 languages and
87 features. For the whole-world subset, we remove
languages with &lt; 25 known features and then fea-
tures that appear in at most 1/10 of the languages.
This leads to 349 languages and 129 features.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999097">
5.1 Identifying Language Areas
</subsectionHeader>
<bodyText confidence="0.999996777777778">
Our first experiment is aimed at discovering lan-
guage areas. We first focus on the IE family, and
then extend the analysis to all languages. In both
cases, we use a known family tree (for the IE ex-
periment, we use a tree given by the language genus
structure; for the whole-world experiment, we use a
tree given by the language family structure). We run
each experiment with five random restarts and 2000
iterations. We select the MAP configuration from
the combination of these runs.
In the IE experiment, the model identified the
areas shown in Figure 5.1. The best area identi-
fied by our model is the second one listed, which
clearly correlates highly with the Balkans. There
are two areas identified by our model (the first and
last) that include only Indic and Iranian languages.
While we are not aware of previous studies of these
as linguistic areas, they are not implausible given
</bodyText>
<page confidence="0.988468">
597
</page>
<table confidence="0.991972307692308">
(Indic) Bhojpuri, Darai, Gujarati, Hindi, Kalami, Kashmiri,
Kumauni, Nepali, Panjabi, Shekhawati, Sindhi (Iranian) Or-
muri, Pashto
(Albanian) Albanian (Greek) Greek (Modern) (Indic) Romani
(Kalderash) (Romance) Romanian, Romansch (Scharans), Ro-
mansch (Sursilvan), Sardinian (Slavic) Bulgarian, Macedonian,
Serbian-Croatian, Slovak, Slovene, Sorbian
(Baltic) Latvian, Lithuanian (Germanic) Danish, Swedish
(Slavic) Polish, Russian
(Celtic) Irish (Germanic) English, German, Norwegian (Ro-
mance) French
(Indic) Prasuni, Urdu (Iranian) Persian, Tajik
Plus 46 non-areal languages
</table>
<figureCaption confidence="0.596726">
Figure 2: IE areas identified. Areas that consist of just
one genus are not listed, nor are areas with two languages.
</figureCaption>
<table confidence="0.895252285714286">
(Mayan) Huastec, Jakaltek, Mam, Tzutujil (Mixe-Zoque)
Zoque (Copainal´a) (Oto-Manguean) Mixtec (Chalcatongo),
Otomi(Mezquital) (Uto-Aztecan) Nahualtl (Tetelcingo), Pipil
(Baltic) Latvian, Lithuanian (Finnic) Estonian, Finnish
(Slavic) Polish, Russian, Ukranian
(Austro-Asiatic) Khasi (Dravidian) Telugu (IE) Bengali
(Sino-Tibetan) Bawm, Garo, Newari (Kathmandu)
</table>
<figureCaption confidence="0.996414">
Figure 3: A small subset of the world areas identified.
</figureCaption>
<bodyText confidence="0.99994372">
the history of the region. The fourth area identi-
fied by our model corresponds roughly to the de-
bated “English” area. Our area includes the req-
uisite French/English/German/Norwegian group, as
well as the somewhat surprising Irish. However, in
addition to being intuitively plausible, it is not hard
to find evidence in the literature for the contact re-
lationship between English and Irish (Sommerfelt,
1960).
In the whole-world experiment, the model identi-
fied too many linguistic areas to fit (39 in total that
contained at least two languages, and contained at
least two language families). In Figure 5.1, we de-
pict the areas found by our model that best corre-
spond to the areas described in Section 2.1.1. We
acknowledge that this gives a warped sense of the
quality of our model. Nevertheless, our model is
able to identify large parts of the the Meso-American
area, the Baltic area and the South Asian area. (It
also finds the Balkans, but since these languages
are all IE, we do not consider it a linguistic area in
this evaluation.) While our model does find areas
that match Meso-American and North-west Ameri-
can areas, neither is represented in its entirety (ac-
cording to the definition of these areas given in Sec-
</bodyText>
<table confidence="0.99803325">
Model Rand F-Sc Edit NVI
K-means 0.9149 0.0735 0.1856 0.5889
Pitman-Yor 0.9637 0.1871 0.6364 0.7998
Areal model 0.9825 0.2637 0.8295 0.9090
</table>
<tableCaption confidence="0.999234">
Table 1: Area identification scores for two baseline algo-
</tableCaption>
<bodyText confidence="0.96030372">
rithms (K-means and Pitman-Yor clustering) that do not
use hierarchical structure, and for the Areal model we
have presented. Higher is better and all differences are
statistically significant at the 95% level.
tion 2.1.1).
Despite the difficulty humans have in assigning
linguistic areas, In Table 1, we explicitly compare
the quality of the areal clusters found on the IE sub-
set. We compare against the most inclusive areal
lists from Section 2.1.1 for IE: the Balkans and the
Baltic. When there is overlap (eg., Romani appears
in both lists), we assigned it to the Balkans.
We compare our model with a flat Pitman-Yor
model that does not use the hierarchy. We also
compare to a baseline K-means algorithm. For K-
means, we ran with K E 15,10,15, ... , 80, 85}
and chose the value of K for each metric that did
best (giving an unfair advantage). Clustering per-
formance is measured on the Indo-European task
according to the Rand Index, F-score, Normalized
Edit Score (Pantel, 2003) and Normalized Variation
of Information (Meila, 2003). In these results, we
see that the Pitman-Yor process model dominates the
K-means model and the Areal model dominates the
Pitman-Yor model.
</bodyText>
<subsectionHeader confidence="0.99815">
5.2 Identifying Areal Features
</subsectionHeader>
<bodyText confidence="0.999967625">
Our second experiment is an analysis of the features
that tend to be shared areally (as opposed to genet-
ically). For this experiment, we make use of the
whole-world version of the data, again with known
language family structure. We initialize a Gibbs
sampler from the MAP configuration found in Sec-
tion 5.1. We run the sampler for 1000 iterations and
take samples every ten steps.
From one particular sample, we can estimate a
posterior distribution over each 7rf. Due to con-
jugacy, we obtain a posterior distribution of 7rf —
Bet(1 + &amp; Zn,f,1 + &amp;[1 — Zn,f]). The 1s come
from the prior. From this Beta distribution, we can
ask the question: what is the probability that a value
of 7rf drawn from this distribution will have value
&lt; 0.5? If this value is high, then the feature is likely
</bodyText>
<page confidence="0.995687">
598
</page>
<table confidence="0.99986305">
p(gen) #f Feature Category
.00 1 Tea
.73 19 Phonology
.73 9 Lexicon
.74 4 Nominal Categories / Numerals
.79 5 Simple Clauses / Predication
.80 5 Verbal Categories / Tense and Aspect
.87 8 Nominal Syntax
.87 8 Simple Clauses / Simple Clauses
.91 12 Nominal Categories / Articles and Pronouns
.94 17 Word Order
.99 10 Morphology
.99 6 Simple Clauses / Valence and Voice
.99 7 Complex Sentences
.99 7 Nominal Categories / Gender and Number
.99 5 Simple Clauses / Negation and Questions
1.0 1 Other / Clicks
1.0 2 Verbal Categories / Suppletion
1.0 9 Verbal Categories / Modality
1.0 4 Nominal Categories / Case
</table>
<tableCaption confidence="0.9864565">
Table 2: Average probability of genetic for each feature
category and the number of features in that category.
</tableCaption>
<bodyText confidence="0.999927678571429">
to be a “genetic feature”; if it is low, then the feature
is likely to be an “areal feature.” We average these
probabilities across all 100 samples.
The features that are most likely to be areal ac-
cording to our model are summaries in Table 2. In
this table, we list the categories to which each fea-
ture belongs, together with the number of features in
that category, and the average probability that a fea-
ture in that category is genetically transmitted. Ap-
parently, the vast majority of features are not areal.
We can treat the results presented in Table 2 as a
hierarchy of borrowability. In doing so, we see that
our hierarchy agrees to a large degree with the hier-
archies summarized in Section 2.1.2. Indeed, (aside
from “Tea”, which we will ignore) the two most
easily shared categories according to our model are
phonology and the lexicon; this is in total agreement
with the agreed state of affairs in linguistics.
Lower in our list, we see that noun-related cat-
egories tend to precede their verb-related counter-
parts (nominal categories before verbal categores,
nominal syntax before complex sentences). Accord-
ing to Curnow (2001), the most difficult features to
borrow are phonetics (for which we have no data),
bound grammatical forms (which appear low on our
list), morphology (which is 99% genetic, according
to our model) and syntactic frames (which would
roughly correspond to “complex sentences”, another
</bodyText>
<table confidence="0.950897">
Indo-European
Model Accuracy Log Prob
Baseline 0.635 (±0.007) −0.583 (±0.008)
Areal model 0.689 (±0.010) −0.526 (±0.027)
World
Model Accuracy Log Prob
Baseline 0.628 (±0.001) −0.654 (±0.003)
Areal model 0.635 (±0.002) −0.565 (±0.011)
</table>
<tableCaption confidence="0.966649">
Table 3: Prediction accuracies and log probabilities for
IE (top) and the world (bottom).
</tableCaption>
<bodyText confidence="0.964889">
item which is 99% genetic in our model).
</bodyText>
<subsectionHeader confidence="0.993159">
5.3 Genetic Reconstruction
</subsectionHeader>
<bodyText confidence="0.999624972222222">
In this section, we investigate whether the use of
areal knowledge can improve the automatic recon-
struction of language family trees. We use King-
man’s coalescent (see Section 2.2.2) as a probabilis-
tic model of trees, endowed with a binomial muta-
tion process on the language features.
Our baseline model is to run the vanilla coalescent
on the WALS data, effective reproducing the results
presented by Teh et al. (2007). This method was al-
ready shown to outperform competing hierarchical
clustering algorithms such as average-link agglom-
erative clustering (see, eg., Duda and Hart (1973))
and the Bayesian Hierarchical Clustering algorithm
(Heller and Ghahramani, 2005).
We run the same experiment both on the IE sub-
set of data and on the whole-world subset. We eval-
uate the results qualitatively, by observing the trees
found (on the IE subset) and quantitatively (below).
For the qualitative analysis, we show the subset of
IE that does not contain Indic languages or Iranian
languages (just to keep the figures small). The tree
derived from the original data is on the left in Fig-
ure 4, below:
The tree based on areal information is on the right in
Figure 4, below. As we can see, the use of areal in-
formation qualitatively improves the structure of the
tree. Where the original tree had a number of errors
with respect to Romance and Germanic languages,
these are sorted out in the areally-aware tree. More-
over, Greek now appears in a more appropriate part
of the tree and English appears on a branch that is
further out from the Norse languages.
We perform two varieties of quantitative analysis.
In the first, we attempt to predict unknown feature
values. In particular, we hide an addition 10% of
the feature values in the WALS data and fit a model
</bodyText>
<page confidence="0.992487">
599
</page>
<figure confidence="0.999950695121951">
] Sardinian
) ] Romansch (Scharans)
Danish
Germanic]
[Germanic] Danish
[Romance ] Sardinian
Romance
Romai
] Portuguese
] Italian
] Catalan
] Spanish
] Romansch (Sursilvan)
Slavic ] Polish
[Slavic ] Polish
[Romance ] Romansch (Sursilva)
Romance
[Romance ] Spanish
Romance
Baltic ] Latvian
Slavic ] Slovene
[Slavic ] Slovene
] French
[Baltic ] Latvian
Baltr
[Romance ] Italian
Romance
Rom&lt;
[Romance ] French
Romance
Roma
[Romance ] Catalan
Romance
Roma
[Romance ] Portuguese
Romance
Roman
Celtic ] Irish (Donegal)
[Celtic ] Irish (Donel)
Celtc ]
Slavic ] Czech
[Slavic ] Czech
Greek ] Greek (Modern)
[Greek ] Greek (Mo)
Slavic ] Macedonian
[Slavic ] Macedonia
[Armenian] Armenian (Western)
[Armenian] Armenian (Western)
[Armenian] Armenian (Eastern)
[Armenian] Armenian (Eastern)
Celtic ] Irish
[Celtic ] Irish
Celt
[Celtic ] Breton
Celt,
[Celtic ] Gaelic (Sc)
Cel§c
Celtic ] Welsh
Celtic ] Gaelic (Scots)
Celtic ] Breton
[Celtic ] Welsh
Celt
[Albanian] Albanian
[Albanian] Albanian
Celtic ] Cornish
[Celtic ] Cornish
Celt(
Germanic] Afrikaans
[Germanic] Afrikaans
Romance ] French
[Albanian] Albanian
Slavic ] Macedonian
Greek ] Greek (Modern)
] Sorbian
Slavic
] Slovak
Slavic
Slavic
Slavic
] Bulgarian
] Polish
] Russian
Slavic
Celtic ] Irish (Donegal)
] Czech
Slavic
Germanic] Faroese
Slavic ] Belorussian
[Armenian] Armenian (Western)
[Armenian] Armenian (Eastern)
Celtic ] Irish
Romance ] Romansch (Scharans)
Celtic ] Gaelic (Scots)
Celtic ] Breton
Frisian
English
Germanic]
[Germanic] English
Germanic]
[Germanic] Norwegian
Norwegian
Swedish
Germanic]
[Germanic] Swedish
Slavic ] Slovak
[Slavic ] Slovak
Germanic] English
Romance ] Romanian
Germanic] Dutch
Germanic] Danish
Germanic] Norwegian
Germanic] Frisian
Germanic] Swedish
Romance ] Sardinian
Baltic ] Latvian
Celtic ] Welsh
Germanic] German
[Germanic] German
[Romance ] Romansch (Schara
Romance
Romance
Slavic ] Serbian−Croatian
[Slavic ] Serbian−Crtia
[Slavic ] Ukrainian
Slavic ] Ukrainian
[Slavic ] Russian
Slavic ] Russian
Slavic ] Bulgarian
[Slavic ] Bulgarian
[Slavic ] Sorbian
Slavic ] Sorbian
[Germanic] Faroese
Germanic] Faroese
Germanic] Icelandic
Romance ] Spanish
Romance ] Romansch (Sursilvan)
Baltic ] Lithuanian
] Slovene
] Serbian−Croatian
Slavic Slavic
Celtic ] Cornish
Germanic] Afrikaans
Dutch
Germanic]
[Germanic] Dutch
] Romanian
Icelandic
Germanic]
[Germanic] Icelandic
[Romance ] Romanian
Romance
Romer
Germanic]
[Germanic] Frisian
Baltic ] Lithuanian
[Baltic ] Lithuanian
Baltic
Slavic ] Belorussian
[Slavic ] Belorussia
Germanic] German
Romance ] Italian
Romance ] Catalan
Romance ] Portuguese
Slavic ] Ukrainian
</figure>
<figureCaption confidence="0.999591">
Figure 4: Genetic trees of IE languages. (Left) with no areal knowledge; (Right) with areal model.
</figureCaption>
<table confidence="0.976476">
Indo-European versus Genus
Model Purity Subtree LOO Acc
Baseline 0.6078 0.5065 0.3218
Areal model 0.6494 0.5455 0.2528
World versus Genus
Model Purity Subtree LOO Acc
Baseline 0.3599 0.2253 0.7747
Areal model 0.4001 0.2450 0.7982
World versus Family
Model Purity Subtree LOO Acc
Baseline 0.4163 0.3280 0.4842
Areal model 0.5143 0.3318 0.5198
</table>
<tableCaption confidence="0.964881">
Table 4: Scores for IE as compared against genus (top);
for world against genus (mid) and against family (low).
</tableCaption>
<bodyText confidence="0.999971041666667">
to the remaining 90%. We then use that model to
predict the hidden 10%. The baseline model is to
make predictions according to the family tree. The
augmented model is to make predictions according
to the family tree for those features identified as ge-
netic and according to the linguistic area for those
features identified as areal. For both settings, we
compute both the absolute accuracy as well as the
log probability of the hidden data under the model
(the latter is less noisy). We repeat this experiment
10 times with a different random 10% hidden. The
results are shown in Table 3, below. The differences
are not large, but are outside one standard deviation.
For the second quantitative analysis, we use
present purity scores (Heller and Ghahramani,
2005), subtree scores (the number of interior nodes
with pure leaf labels, normalized) and leave-one-out
log accuracies (all scores are between 0 and 1, and
higher scores are better). These scores are computed
against both language family and language genus as
the “classes.” The results are in Table 4, below. As
we can see, the results are generally in favor of the
Areal model (LOO Acc on IE versus Genus non-
withstanding), depending on the evaluation metric.
</bodyText>
<table confidence="0.997334428571429">
Radius Purity Subtree LOO Acc
125 0.6237 0.4855 0.2013
250 0.6457 0.5325 0.2299
500 0.6483 0.5455 0.2413
1000 0.6494 0.5455 0.2528
2000 0.6464 0.4935 0.3218
4000 0.6342 0.4156 0.4138
</table>
<tableCaption confidence="0.985529">
Table 5: Scores for IE vs genus at varying radii.
</tableCaption>
<subsectionHeader confidence="0.993976">
5.4 Effect of Radius
</subsectionHeader>
<bodyText confidence="0.999951777777778">
Finally, we evaluate the effect of the radius hyper-
parameter on performance. Table 5 shows perfor-
mance for models built with varying radii. As can
be seen by purity and subtree scores, there is a
“sweet spot” around 500 to 1000 kilometers where
the model seems optimal. LOO (strangely) seems
to continue to improve as we allow areas to grow
arbitrarily large. This is perhaps overfitting. Never-
theless, performance is robust for a range of radii.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999994461538461">
We presented a model that is able to recover well-
known linguistic areas. Using this areas, we have
shown improvement in the ability to recover phylo-
genetic trees of languages. It is important to note
that despite our successes, there is much at our
model does not account for: borrowing is known to
be assymetric; contact is temporal; borrowing must
obey univeral implications. Despite the failure of
our model to account for these issues, however, it
appears largely successful. Moreover, like any “data
mining” expedition, our model suggests new lin-
guistic areas (particularly in the “whole world” ex-
periments) that deserve consideration.
</bodyText>
<sectionHeader confidence="0.998666" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993645">
Deep thanks to Lyle Campbell, Yee Whye Teh and
Eric Xing for discussions; comments from the three
anonymous reviewers were very helpful. This work
was partially supported by NSF grant IIS0712764.
</bodyText>
<page confidence="0.995448">
600
</page>
<sectionHeader confidence="0.998334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999686747474748">
Alexandra Aikhenvald and R.M.W. Dixon, editors. 2001.
Areal diffusion and genetic inheritance: problems in
comparative linguistics. Oxford University Press.
Lyle Campbell. 2005. Areal linguistics. In Keith Brown,
editor, Encyclopedia of Language and Linguistics. El-
sevier, 2 edition.
Lyle Campbell. 2006. Areal linguistics: the problem
to the answer. In April McMahon, Nigel Vincent, and
Yaron Matras, editors, Language contact and areal lin-
guistics.
Lyle Campbell. In press. Why Sir William Jones
got it all wrong, or Jones’ role in how to estab-
lish language families. In Joseba Lakarra, editor,
Festschrift/Memorial volume for Larry Trask.
Timothy Curnow. 2001. What language features can be
”borrowed”? In Aikhenvald and Dixon, editors, Areal
diffusion and genetic inheritance: problems in com-
parative linguistics, pages 412–436. Oxford Univer-
sity Press.
Hal Daum´e III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
R.M.W. Dixon. 2001. The Australian linguistic area.
In Aikhenvald and Dixon, editors, Areal diffusion and
genetic inheritance: problems in comparative linguis-
tics, pages 64–104. Oxford University Press.
R. O. Duda and P. E. Hart. 1973. Pattern Classification
And Scene Analysis. Wiley and Sons, New York.
David Dunson and Ju-Hyun Park. 2007. Kernel stick
breaking processes. Biometrika, 95:307–323.
Isidore Dyen, Joseph Kurskal, and Paul Black. 1992. An
Indoeuropean classification: A lexicostatistical experi-
ment. Transactions of the American Philosophical So-
ciety, 82(5). American Philosophical Society.
Thomas S. Ferguson. 1974. Prior distributions on spaces
of probability measures. The Annals of Statistics,
2(4):615–629, July.
Zoubin Ghahramani. 2005. Nonparametric Bayesian
methods. Tutorial presented at UAI conference.
Joseph Greenberg, editor. 1963. Universals of Lan-
guages. MIT Press.
Martin Haspelmath, Matthew Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
E. Haugen. 1950. The analysis of linguistic borrowing.
Language, 26:210–231.
Katherine Heller and Zoubin Ghahramani. 2005.
Bayesian hierarchical clustering. In Proceedings of
the International Conference on Machine Learning
(ICML), volume 22.
Michael I. Jordan. 2005. Dirichlet processes, Chinese
restaurant processes and all that. Tutorial presented at
NIPS conference.
Harmut Katz. 1975. Generative Phonologie und phonol-
ogische Sprachb¨unde des Ostjakischen un Samojedis-
chen. Wilhelm Fink.
J. F. C. Kingman. 1982a. The coalescent. Stochastic
Processes and their Applications, 13:235–248.
J. F. C. Kingman. 1982b. On the genealogy of large
populations. Journal of Applied Probability, 19:27–
43. Essays in Statistical Science.
James Matisoff. 2001. Genetic versus contact relation-
ship: prosodic diffusibility in South-East Asian lan-
guages. In Aikhenvald and Dixon, editors, Areal diffu-
sion and genetic inheritance: problems in comparative
linguistics, pages 291–327. Oxford University Press.
Marina Meila. 2003. Comparing clusterings. In Pro-
ceedings of the Conference on Computational Learn-
ing Theory (COLT).
E. Moravcsik. 1978. Language contact. In J.H. Green-
berg, C. Ferguson, and E. Moravcsik, editors, Univer-
sals of Human Language, volume 1; Method and The-
ory, pages 3–123. Stanford University Press.
Patrick Pantel. 2003. Clustering by Committee. Ph.D.
thesis, University of Alberta.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855–900.
Jim Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, University of California at
Berkeley. Lecture notes for St. Flour Summer School.
M.D. Ross. 1988. Proto Oceanic and the Austronesian
languages of western melanesia. Canberra: Pacific
Linguitics, Australian National University.
Alf Sommerfelt. 1960. External versus internal factors
in the development of language. Norsk Tidsskrift for
Sprogvidenskap, 19:296–315.
Yee Whye Teh, Hal Daum´e III, and Daniel Roy. 2007.
Bayesian agglomerative clustering with coalescents.
In Advances in Neural Information Processing Sys-
tems (NIPS).
Sarah Thomason. 2001. Language contact: an introduc-
tion. Edinburgh University Press.
T. Warnow, S.N. Evans, D. Ringe, and L. Nakhleh. 2005.
A stochastic model of language evolution that incor-
porates homoplasy and borrowing. In Phylogenetic
Methods and the Prehistory of Language. Cambridge
University Press. Invited paper.
</reference>
<page confidence="0.998234">
601
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.413491">
<title confidence="0.998188">Non-Parametric Bayesian Areal Linguistics</title>
<author confidence="0.999206">Hal Daum´e</author>
<affiliation confidence="0.886399">School of University of Salt Lake City, UT</affiliation>
<email confidence="0.709939">me@hal3.name</email>
<abstract confidence="0.9878769">We describe a statistical model over linguistic areas and phylogeny. Our model recovers known areas and identifies a plausible hierarchy of areal features. The use of areas improves genetic reconstruction of languages both qualitatively and quantitatively according to a variety of metrics. We model linguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman’s coalescent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Aikhenvald</author>
<author>R M W Dixon</author>
<author>editors</author>
</authors>
<title>Areal diffusion and genetic inheritance: problems in comparative linguistics.</title>
<date>2001</date>
<publisher>Oxford University Press.</publisher>
<marker>Aikhenvald, Dixon, editors, 2001</marker>
<rawString>Alexandra Aikhenvald and R.M.W. Dixon, editors. 2001. Areal diffusion and genetic inheritance: problems in comparative linguistics. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyle Campbell</author>
</authors>
<title>Areal linguistics.</title>
<date>2005</date>
<booktitle>Encyclopedia of Language and Linguistics. Elsevier,</booktitle>
<volume>2</volume>
<pages>edition.</pages>
<editor>In Keith Brown, editor,</editor>
<contexts>
<context position="2430" citStr="Campbell (2005)" startWordPosition="387" endWordPosition="388"> causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?). In this paper, we attempt to provide a statistical answer to some of these questions. In particular, we develop a Bayesian model of typology that allows for, but does not force, the existence of linguistic areas. Our model also allows for, but does not force, preference for some feature to be shared areally. When applied to a large typological database of linguistic features (Haspelmath et al., 2005), we find that it discovers linguistic areas that are well documented in the literature (see Campbell (2005) for an overview), and a small preference for certain features to be shared areally. This latter agrees, to a lesser degree, with some of the published hierarchies of borrowability (Curnow, 2001). Finally, we show that reconstructing language family trees is significantly aided by knowledge of areal features. We note that Warnow et al. (2005) have independently proposed a model for phonological change in Indo-European (based on the Dyen dataset (Dyen et al., 1992)) that includes notions of borrowing. Our model is different in that we (a) base our model on typological features rather than just </context>
<context position="4082" citStr="Campbell, 2005" startWordPosition="644" endWordPosition="645"> Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics for understanding our model. 2.1 Areal Linguistics Areal effects on linguistic typology have been studied since, at least, the late 1920s by Trubetzkoy, though the idea of tracing family trees for languages goes back to the mid 1800s and the comparative study of historical linguistics dates back, perhaps to Giraldus Cambrenis in 1194 (Campbell, In press). A recent article provides a short introduction to both the issues that surround areal linguistics, as well as an enumeration of many of the known language areas (Campbell, 2005). A fairly wide, modern treatment of the issues surrounding areal diffusion is also given by essays in a recent book edited by Aikhenvald and Dixon (2001). The essays in this book provide a good introduction to the issues in the field. Campbell (2006) provides a critical survey of these and other hypotheses relating to areal linguistics. There are several issues which are basic to the study of areal linguistics (these are copied almost directly from Campbell (2006)). Must a linguistic area comprise more than two languages? Must it comprise more than one language family? Is a single trait suffi</context>
</contexts>
<marker>Campbell, 2005</marker>
<rawString>Lyle Campbell. 2005. Areal linguistics. In Keith Brown, editor, Encyclopedia of Language and Linguistics. Elsevier, 2 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyle Campbell</author>
</authors>
<title>Areal linguistics: the problem to the answer.</title>
<date>2006</date>
<editor>In April McMahon, Nigel Vincent, and Yaron Matras, editors,</editor>
<contexts>
<context position="756" citStr="Campbell, 2006" startWordPosition="113" endWordPosition="114">t We describe a statistical model over linguistic areas and phylogeny. Our model recovers known areas and identifies a plausible hierarchy of areal features. The use of areas improves genetic reconstruction of languages both qualitatively and quantitatively according to a variety of metrics. We model linguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman’s coalescent. 1 Introduction Why are some languages more alike than others? This question is one of the most central issues in historical linguistics. Typically, one of three answers is given (Aikhenvald and Dixon, 2001; Campbell, 2006). First, the languages may be related “genetically.” That is, they may have all derived from a common ancestor language. Second, the similarities may be due to chance. Some language properties are simply more common than others, which is often attributed to be mostly due to linguistic universals (Greenberg, 1963). Third, the languages may be related areally. Languages that occupy the same geographic area often exhibit similar characteristics, not due to genetic relatedness, but due to sharing. Regions (and the languages contained within them) that exhibit sharing are called linguistic areas an</context>
<context position="4333" citStr="Campbell (2006)" startWordPosition="690" endWordPosition="691"> tracing family trees for languages goes back to the mid 1800s and the comparative study of historical linguistics dates back, perhaps to Giraldus Cambrenis in 1194 (Campbell, In press). A recent article provides a short introduction to both the issues that surround areal linguistics, as well as an enumeration of many of the known language areas (Campbell, 2005). A fairly wide, modern treatment of the issues surrounding areal diffusion is also given by essays in a recent book edited by Aikhenvald and Dixon (2001). The essays in this book provide a good introduction to the issues in the field. Campbell (2006) provides a critical survey of these and other hypotheses relating to areal linguistics. There are several issues which are basic to the study of areal linguistics (these are copied almost directly from Campbell (2006)). Must a linguistic area comprise more than two languages? Must it comprise more than one language family? Is a single trait sufficient to define an area? How “nearby” must languages in an area be to one another? Are some feature more easily borrowed that others? Despite these formal definitional issues of what constitutes a language area and areal features, most historical ling</context>
</contexts>
<marker>Campbell, 2006</marker>
<rawString>Lyle Campbell. 2006. Areal linguistics: the problem to the answer. In April McMahon, Nigel Vincent, and Yaron Matras, editors, Language contact and areal linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lyle Campbell</author>
</authors>
<title>In press. Why Sir William Jones got it all wrong, or Jones’ role in how to establish language families.</title>
<booktitle>In Joseba Lakarra, editor, Festschrift/Memorial</booktitle>
<volume>volume</volume>
<institution>for Larry Trask.</institution>
<marker>Campbell, </marker>
<rawString>Lyle Campbell. In press. Why Sir William Jones got it all wrong, or Jones’ role in how to establish language families. In Joseba Lakarra, editor, Festschrift/Memorial volume for Larry Trask.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Curnow</author>
</authors>
<title>What language features can be ”borrowed”? In Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics,</title>
<date>2001</date>
<pages>412--436</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1775" citStr="Curnow, 2001" startWordPosition="276" endWordPosition="277">a often exhibit similar characteristics, not due to genetic relatedness, but due to sharing. Regions (and the languages contained within them) that exhibit sharing are called linguistic areas and the features that are shared are called areal features. Much is not understood or agreed upon in the field of areal linguistics. Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of “borrowability” (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?). In this paper, we attempt to provide a statistical answer to some of these questions. In particular, we develop a Bayesian model of typology that allows for, but does not force, the existence of linguistic areas. Our model also allows for, but does not force, preference for some feature to be shared areally. When applied to a large typological database of linguistic features (Haspelmath et al., 2005), we find that it discovers linguistic areas that are</context>
<context position="6958" citStr="Curnow (2001)" startWordPosition="1078" endWordPosition="1079">c areas and Matisoff (2001) defines a SouthEast Asian language area. Finally, although “folk lore” is in favor of identifying a linguistic area including English, French and certain Norse languages (Norwegian, Swedish, Low Dutch, High German, etc.), there are counter-arguments to this position (Thomason, 2001) (see especially Case Study 9.8). 2.1.2 Linguistic Features Identifying which linguistic features are most easily shared “areally” is a long standing problem in contact linguistics. Here we briefly review some of the major claims. Much of this overview is adoped from the summary given by Curnow (2001). Haugen (1950) considers only borrowability as far as the lexicon is concerned. He provided evidence that nouns are the easiest, followed by verbs, adjectives, adverbs, prepositions, etc. Ross (1988) corroborates Haugen’s analysis and deepens it to cover morphology, syntax and phonology. He proposes the following hierarchy of borrowability (easiest items coming first): nouns &gt; verbs &gt; adjectives &gt; syntax &gt; non-bound function words &gt; bound morphemes &gt; phonemes. Coming from a “constraints” perspective, Moravcsik (1978) suggests that: lexical items must be borrowed before lexical properties; inf</context>
<context position="28160" citStr="Curnow (2001)" startWordPosition="4610" endWordPosition="4611">the results presented in Table 2 as a hierarchy of borrowability. In doing so, we see that our hierarchy agrees to a large degree with the hierarchies summarized in Section 2.1.2. Indeed, (aside from “Tea”, which we will ignore) the two most easily shared categories according to our model are phonology and the lexicon; this is in total agreement with the agreed state of affairs in linguistics. Lower in our list, we see that noun-related categories tend to precede their verb-related counterparts (nominal categories before verbal categores, nominal syntax before complex sentences). According to Curnow (2001), the most difficult features to borrow are phonetics (for which we have no data), bound grammatical forms (which appear low on our list), morphology (which is 99% genetic, according to our model) and syntactic frames (which would roughly correspond to “complex sentences”, another Indo-European Model Accuracy Log Prob Baseline 0.635 (±0.007) −0.583 (±0.008) Areal model 0.689 (±0.010) −0.526 (±0.027) World Model Accuracy Log Prob Baseline 0.628 (±0.001) −0.654 (±0.003) Areal model 0.635 (±0.002) −0.565 (±0.011) Table 3: Prediction accuracies and log probabilities for IE (top) and the world (bot</context>
</contexts>
<marker>Curnow, 2001</marker>
<rawString>Timothy Curnow. 2001. What language features can be ”borrowed”? In Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics, pages 412–436. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Lyle Campbell</author>
</authors>
<title>A Bayesian model for discovering typological implications.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, Campbell, 2007</marker>
<rawString>Hal Daum´e III and Lyle Campbell. 2007. A Bayesian model for discovering typological implications. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M W Dixon</author>
</authors>
<title>The Australian linguistic area. In Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics,</title>
<date>2001</date>
<pages>64--104</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="739" citStr="Dixon, 2001" startWordPosition="111" endWordPosition="112">.name Abstract We describe a statistical model over linguistic areas and phylogeny. Our model recovers known areas and identifies a plausible hierarchy of areal features. The use of areas improves genetic reconstruction of languages both qualitatively and quantitatively according to a variety of metrics. We model linguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman’s coalescent. 1 Introduction Why are some languages more alike than others? This question is one of the most central issues in historical linguistics. Typically, one of three answers is given (Aikhenvald and Dixon, 2001; Campbell, 2006). First, the languages may be related “genetically.” That is, they may have all derived from a common ancestor language. Second, the similarities may be due to chance. Some language properties are simply more common than others, which is often attributed to be mostly due to linguistic universals (Greenberg, 1963). Third, the languages may be related areally. Languages that occupy the same geographic area often exhibit similar characteristics, not due to genetic relatedness, but due to sharing. Regions (and the languages contained within them) that exhibit sharing are called li</context>
<context position="4236" citStr="Dixon (2001)" startWordPosition="672" endWordPosition="673">c typology have been studied since, at least, the late 1920s by Trubetzkoy, though the idea of tracing family trees for languages goes back to the mid 1800s and the comparative study of historical linguistics dates back, perhaps to Giraldus Cambrenis in 1194 (Campbell, In press). A recent article provides a short introduction to both the issues that surround areal linguistics, as well as an enumeration of many of the known language areas (Campbell, 2005). A fairly wide, modern treatment of the issues surrounding areal diffusion is also given by essays in a recent book edited by Aikhenvald and Dixon (2001). The essays in this book provide a good introduction to the issues in the field. Campbell (2006) provides a critical survey of these and other hypotheses relating to areal linguistics. There are several issues which are basic to the study of areal linguistics (these are copied almost directly from Campbell (2006)). Must a linguistic area comprise more than two languages? Must it comprise more than one language family? Is a single trait sufficient to define an area? How “nearby” must languages in an area be to one another? Are some feature more easily borrowed that others? Despite these formal</context>
<context position="6293" citStr="Dixon (2001)" startWordPosition="977" endWordPosition="978">Takelman, Tlingit, Tsimshian and Wakashan. The Baltic: Baltic languages, Baltic German, and Finnic languages (especially Estonian and Livonian). (Sometimes many more are included, such as: Belorussian, Lavian, Lithuanian, Norwegian, Old Prussian, Polish, Romani, Russian, Ukranian.) Ethiopia: Afar, Amharic, Anyuak, Awngi, Beja, Ge’ez, Gumuz, Janjero, Kefa, Sidamo, Somali, Tigre, Tigrinya and Wellamo. Needless to say, the exact definition and extent of the actual areas is up to significant debate. Moreover, claims have been made in favor of many linguistic areas not defined above. For instance, Dixon (2001) presents arguments for several Australian linguistic areas and Matisoff (2001) defines a SouthEast Asian language area. Finally, although “folk lore” is in favor of identifying a linguistic area including English, French and certain Norse languages (Norwegian, Swedish, Low Dutch, High German, etc.), there are counter-arguments to this position (Thomason, 2001) (see especially Case Study 9.8). 2.1.2 Linguistic Features Identifying which linguistic features are most easily shared “areally” is a long standing problem in contact linguistics. Here we briefly review some of the major claims. Much o</context>
</contexts>
<marker>Dixon, 2001</marker>
<rawString>R.M.W. Dixon. 2001. The Australian linguistic area. In Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics, pages 64–104. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
</authors>
<title>Pattern Classification And Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley and Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="29420" citStr="Duda and Hart (1973)" startWordPosition="4802" endWordPosition="4805">odel). 5.3 Genetic Reconstruction In this section, we investigate whether the use of areal knowledge can improve the automatic reconstruction of language family trees. We use Kingman’s coalescent (see Section 2.2.2) as a probabilistic model of trees, endowed with a binomial mutation process on the language features. Our baseline model is to run the vanilla coalescent on the WALS data, effective reproducing the results presented by Teh et al. (2007). This method was already shown to outperform competing hierarchical clustering algorithms such as average-link agglomerative clustering (see, eg., Duda and Hart (1973)) and the Bayesian Hierarchical Clustering algorithm (Heller and Ghahramani, 2005). We run the same experiment both on the IE subset of data and on the whole-world subset. We evaluate the results qualitatively, by observing the trees found (on the IE subset) and quantitatively (below). For the qualitative analysis, we show the subset of IE that does not contain Indic languages or Iranian languages (just to keep the figures small). The tree derived from the original data is on the left in Figure 4, below: The tree based on areal information is on the right in Figure 4, below. As we can see, the</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. O. Duda and P. E. Hart. 1973. Pattern Classification And Scene Analysis. Wiley and Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dunson</author>
<author>Ju-Hyun Park</author>
</authors>
<title>Kernel stick breaking processes.</title>
<date>2007</date>
<journal>Biometrika,</journal>
<pages>95--307</pages>
<contexts>
<context position="15062" citStr="Dunson and Park, 2007" startWordPosition="2414" endWordPosition="2417"> construction of the model and the choice of the model hyperparameters. At a high-level, our model works as follows. Values Xn,f appear for one of two reasons: they are either areally derived or genetically derived. A latent variable Zn,f determines this. If it is derived areally, then the value Xn,f is drawn from a latent variable 1An reader might worry about exchangeability: Our method of making language centers and locations part of the Pitman-Yor distribution ensures this is not an issue. An alternative would be to use a location-sensitive process such as the kernel stickbreaking process (Dunson and Park, 2007), though we do not explore that here. corresponding to the value preferences in the langauge area to which language n belongs. If it is derived genetically, then Xn,f is drawn from a variable corresponding to value preferences for the genetic substrate to which language n belongs. The set of areas, and the area to which a language belongs are given by yet more latent variables. It is this aspect of the model for which we use the Pitman-Yor process: languages are customers, areas are tables and area value preferences are the parameters of the tables. 3.1 The formal model We assume that the valu</context>
</contexts>
<marker>Dunson, Park, 2007</marker>
<rawString>David Dunson and Ju-Hyun Park. 2007. Kernel stick breaking processes. Biometrika, 95:307–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isidore Dyen</author>
<author>Joseph Kurskal</author>
<author>Paul Black</author>
</authors>
<title>An Indoeuropean classification: A lexicostatistical experiment.</title>
<date>1992</date>
<journal>Transactions of the American Philosophical Society,</journal>
<volume>82</volume>
<issue>5</issue>
<publisher>American Philosophical Society.</publisher>
<contexts>
<context position="2898" citStr="Dyen et al., 1992" startWordPosition="461" endWordPosition="464">guistic features (Haspelmath et al., 2005), we find that it discovers linguistic areas that are well documented in the literature (see Campbell (2005) for an overview), and a small preference for certain features to be shared areally. This latter agrees, to a lesser degree, with some of the published hierarchies of borrowability (Curnow, 2001). Finally, we show that reconstructing language family trees is significantly aided by knowledge of areal features. We note that Warnow et al. (2005) have independently proposed a model for phonological change in Indo-European (based on the Dyen dataset (Dyen et al., 1992)) that includes notions of borrowing. Our model is different in that we (a) base our model on typological features rather than just lexical patterns and (b) we explicitly represent language areas, not just one-time borrowing phenomena. 2 Background We describe (in Section 3) a non-parametric, hierarchical Bayesian model for finding linguistic areas and areal features. In this section, we provide necessary background—both linguistic and statistical— 593 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 593–601, Boulder, Colorado, June 2009. </context>
</contexts>
<marker>Dyen, Kurskal, Black, 1992</marker>
<rawString>Isidore Dyen, Joseph Kurskal, and Paul Black. 1992. An Indoeuropean classification: A lexicostatistical experiment. Transactions of the American Philosophical Society, 82(5). American Philosophical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>Prior distributions on spaces of probability measures.</title>
<date>1974</date>
<journal>The Annals of Statistics,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="9592" citStr="Ferguson, 1974" startWordPosition="1485" endWordPosition="1486">nstead, we allow the data to dictate how many parameters there are. In Bayesian modeling, non-parametric distributions are typically used as priors; see Jordan (2005) or Ghahramani (2005) for overviews. In our model, we use two different non-parametric priors: the Pitman-Yor process (for modeling linguistic areas) and Kingman’s coalescent (for modeling linguistic phylogeny), both described below. 2.2.1 The Pitman-Yor Process One particular example of a non-parametric prior is the Pitman-Yor process (Pitman and Yor, 1997), which can be seen as an extension to the betterknown Dirichlet process (Ferguson, 1974). The Pitman-Yor process can be understood as a particular example of a Chinese Restaurant process (CRP) (Pitman, 2002). The idea in all CRPs is that there exists a restaurant with an infinite number of tables. Customers come into the restaurant and have to choose a table at which to sit. The Pitman-Yor process is described by three parameters: a base rate α, a discount parameter d and a mean distribution Go. These combine to describe a process denoted by PY(α, d, Go). The parameters α and d must satisfy: 0 &lt; d &lt; 1 and α &gt; −d. In the CRP analogy, the model works as follows. The first customer </context>
</contexts>
<marker>Ferguson, 1974</marker>
<rawString>Thomas S. Ferguson. 1974. Prior distributions on spaces of probability measures. The Annals of Statistics, 2(4):615–629, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
</authors>
<title>Nonparametric Bayesian methods.</title>
<date>2005</date>
<note>Tutorial presented at UAI conference.</note>
<contexts>
<context position="9164" citStr="Ghahramani (2005)" startWordPosition="1421" endWordPosition="1422">ar sizes thereof. (Indeed, we do not even want to commit to the existence of any linguistic areas.) However, we will need to “unify” the languages that fall into a linguistic area (if such a thing exists) by means of some statistical parameter. Such problems have been studied under the name non-parametric models. The idea behind nonparametric models is that one does not commit a priori to a particularly number of parameters. Instead, we allow the data to dictate how many parameters there are. In Bayesian modeling, non-parametric distributions are typically used as priors; see Jordan (2005) or Ghahramani (2005) for overviews. In our model, we use two different non-parametric priors: the Pitman-Yor process (for modeling linguistic areas) and Kingman’s coalescent (for modeling linguistic phylogeny), both described below. 2.2.1 The Pitman-Yor Process One particular example of a non-parametric prior is the Pitman-Yor process (Pitman and Yor, 1997), which can be seen as an extension to the betterknown Dirichlet process (Ferguson, 1974). The Pitman-Yor process can be understood as a particular example of a Chinese Restaurant process (CRP) (Pitman, 2002). The idea in all CRPs is that there exists a restaur</context>
<context position="29502" citStr="Ghahramani, 2005" startWordPosition="4814" endWordPosition="4815">areal knowledge can improve the automatic reconstruction of language family trees. We use Kingman’s coalescent (see Section 2.2.2) as a probabilistic model of trees, endowed with a binomial mutation process on the language features. Our baseline model is to run the vanilla coalescent on the WALS data, effective reproducing the results presented by Teh et al. (2007). This method was already shown to outperform competing hierarchical clustering algorithms such as average-link agglomerative clustering (see, eg., Duda and Hart (1973)) and the Bayesian Hierarchical Clustering algorithm (Heller and Ghahramani, 2005). We run the same experiment both on the IE subset of data and on the whole-world subset. We evaluate the results qualitatively, by observing the trees found (on the IE subset) and quantitatively (below). For the qualitative analysis, we show the subset of IE that does not contain Indic languages or Iranian languages (just to keep the figures small). The tree derived from the original data is on the left in Figure 4, below: The tree based on areal information is on the right in Figure 4, below. As we can see, the use of areal information qualitatively improves the structure of the tree. Where </context>
<context position="34510" citStr="Ghahramani, 2005" startWordPosition="5602" endWordPosition="5603"> augmented model is to make predictions according to the family tree for those features identified as genetic and according to the linguistic area for those features identified as areal. For both settings, we compute both the absolute accuracy as well as the log probability of the hidden data under the model (the latter is less noisy). We repeat this experiment 10 times with a different random 10% hidden. The results are shown in Table 3, below. The differences are not large, but are outside one standard deviation. For the second quantitative analysis, we use present purity scores (Heller and Ghahramani, 2005), subtree scores (the number of interior nodes with pure leaf labels, normalized) and leave-one-out log accuracies (all scores are between 0 and 1, and higher scores are better). These scores are computed against both language family and language genus as the “classes.” The results are in Table 4, below. As we can see, the results are generally in favor of the Areal model (LOO Acc on IE versus Genus nonwithstanding), depending on the evaluation metric. Radius Purity Subtree LOO Acc 125 0.6237 0.4855 0.2013 250 0.6457 0.5325 0.2299 500 0.6483 0.5455 0.2413 1000 0.6494 0.5455 0.2528 2000 0.6464 </context>
</contexts>
<marker>Ghahramani, 2005</marker>
<rawString>Zoubin Ghahramani. 2005. Nonparametric Bayesian methods. Tutorial presented at UAI conference.</rawString>
</citation>
<citation valid="true">
<title>Universals of Languages.</title>
<date>1963</date>
<editor>Joseph Greenberg, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1963</marker>
<rawString>Joseph Greenberg, editor. 1963. Universals of Languages. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Haspelmath</author>
<author>Matthew Dryer</author>
<author>David Gil</author>
<author>Bernard Comrie</author>
<author>editors</author>
</authors>
<title>The World Atlas of Language Structures.</title>
<date>2005</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="2322" citStr="Haspelmath et al., 2005" startWordPosition="368" endWordPosition="371">are (is there a linear ordering of “borrowability” (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?). In this paper, we attempt to provide a statistical answer to some of these questions. In particular, we develop a Bayesian model of typology that allows for, but does not force, the existence of linguistic areas. Our model also allows for, but does not force, preference for some feature to be shared areally. When applied to a large typological database of linguistic features (Haspelmath et al., 2005), we find that it discovers linguistic areas that are well documented in the literature (see Campbell (2005) for an overview), and a small preference for certain features to be shared areally. This latter agrees, to a lesser degree, with some of the published hierarchies of borrowability (Curnow, 2001). Finally, we show that reconstructing language family trees is significantly aided by knowledge of areal features. We note that Warnow et al. (2005) have independently proposed a model for phonological change in Indo-European (based on the Dyen dataset (Dyen et al., 1992)) that includes notions </context>
<context position="19642" citStr="Haspelmath et al., 2005" startWordPosition="3220" endWordPosition="3223"> proposal distribution is a Gaussian centered at the previous center, with standard deviation of 5. Experimentally, this resulted in an acceptance rate of about 50%. In our implementation, we analytically integrate out 7r and φ and sample only over Z, the coalescent tree, and the area assignments. In some of our experiments, we treat the family tree as given. In this case, we also analytically integrate out the B parameters and sample only over Z and area assignments. 4 Typological Data The database on which we perform our analysis is the World Atlas of Language Structures (henceforth, WALS) (Haspelmath et al., 2005). The database contains information about 2150 languages (sampled from across the world). There are 139 typological features in this database. The database is sparse: only 16% of the possible language/feature pairs are known. We use the version extracted and preprocessed by Daum´e III and Campbell (2007). In WALS, languages a grouped into 38 language families (including Indo-European, Afro-Asiatic, Austronesian, Niger-Congo, etc.). Each of these language families is grouped into a number of language geni. The Indo-European family includes ten geni, including: Germanic, Romance, Indic and Slavi</context>
</contexts>
<marker>Haspelmath, Dryer, Gil, Comrie, editors, 2005</marker>
<rawString>Martin Haspelmath, Matthew Dryer, David Gil, and Bernard Comrie, editors. 2005. The World Atlas of Language Structures. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Haugen</author>
</authors>
<title>The analysis of linguistic borrowing.</title>
<date>1950</date>
<journal>Language,</journal>
<pages>26--210</pages>
<contexts>
<context position="6973" citStr="Haugen (1950)" startWordPosition="1080" endWordPosition="1081">isoff (2001) defines a SouthEast Asian language area. Finally, although “folk lore” is in favor of identifying a linguistic area including English, French and certain Norse languages (Norwegian, Swedish, Low Dutch, High German, etc.), there are counter-arguments to this position (Thomason, 2001) (see especially Case Study 9.8). 2.1.2 Linguistic Features Identifying which linguistic features are most easily shared “areally” is a long standing problem in contact linguistics. Here we briefly review some of the major claims. Much of this overview is adoped from the summary given by Curnow (2001). Haugen (1950) considers only borrowability as far as the lexicon is concerned. He provided evidence that nouns are the easiest, followed by verbs, adjectives, adverbs, prepositions, etc. Ross (1988) corroborates Haugen’s analysis and deepens it to cover morphology, syntax and phonology. He proposes the following hierarchy of borrowability (easiest items coming first): nouns &gt; verbs &gt; adjectives &gt; syntax &gt; non-bound function words &gt; bound morphemes &gt; phonemes. Coming from a “constraints” perspective, Moravcsik (1978) suggests that: lexical items must be borrowed before lexical properties; inflected words be</context>
</contexts>
<marker>Haugen, 1950</marker>
<rawString>E. Haugen. 1950. The analysis of linguistic borrowing. Language, 26:210–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine Heller</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Bayesian hierarchical clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<volume>22</volume>
<contexts>
<context position="29502" citStr="Heller and Ghahramani, 2005" startWordPosition="4812" endWordPosition="4815">the use of areal knowledge can improve the automatic reconstruction of language family trees. We use Kingman’s coalescent (see Section 2.2.2) as a probabilistic model of trees, endowed with a binomial mutation process on the language features. Our baseline model is to run the vanilla coalescent on the WALS data, effective reproducing the results presented by Teh et al. (2007). This method was already shown to outperform competing hierarchical clustering algorithms such as average-link agglomerative clustering (see, eg., Duda and Hart (1973)) and the Bayesian Hierarchical Clustering algorithm (Heller and Ghahramani, 2005). We run the same experiment both on the IE subset of data and on the whole-world subset. We evaluate the results qualitatively, by observing the trees found (on the IE subset) and quantitatively (below). For the qualitative analysis, we show the subset of IE that does not contain Indic languages or Iranian languages (just to keep the figures small). The tree derived from the original data is on the left in Figure 4, below: The tree based on areal information is on the right in Figure 4, below. As we can see, the use of areal information qualitatively improves the structure of the tree. Where </context>
<context position="34510" citStr="Heller and Ghahramani, 2005" startWordPosition="5600" endWordPosition="5603">y tree. The augmented model is to make predictions according to the family tree for those features identified as genetic and according to the linguistic area for those features identified as areal. For both settings, we compute both the absolute accuracy as well as the log probability of the hidden data under the model (the latter is less noisy). We repeat this experiment 10 times with a different random 10% hidden. The results are shown in Table 3, below. The differences are not large, but are outside one standard deviation. For the second quantitative analysis, we use present purity scores (Heller and Ghahramani, 2005), subtree scores (the number of interior nodes with pure leaf labels, normalized) and leave-one-out log accuracies (all scores are between 0 and 1, and higher scores are better). These scores are computed against both language family and language genus as the “classes.” The results are in Table 4, below. As we can see, the results are generally in favor of the Areal model (LOO Acc on IE versus Genus nonwithstanding), depending on the evaluation metric. Radius Purity Subtree LOO Acc 125 0.6237 0.4855 0.2013 250 0.6457 0.5325 0.2299 500 0.6483 0.5455 0.2413 1000 0.6494 0.5455 0.2528 2000 0.6464 </context>
</contexts>
<marker>Heller, Ghahramani, 2005</marker>
<rawString>Katherine Heller and Zoubin Ghahramani. 2005. Bayesian hierarchical clustering. In Proceedings of the International Conference on Machine Learning (ICML), volume 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Jordan</author>
</authors>
<title>Dirichlet processes, Chinese restaurant processes and all that. Tutorial presented at NIPS conference.</title>
<date>2005</date>
<contexts>
<context position="9143" citStr="Jordan (2005)" startWordPosition="1417" endWordPosition="1419">reas, or particular sizes thereof. (Indeed, we do not even want to commit to the existence of any linguistic areas.) However, we will need to “unify” the languages that fall into a linguistic area (if such a thing exists) by means of some statistical parameter. Such problems have been studied under the name non-parametric models. The idea behind nonparametric models is that one does not commit a priori to a particularly number of parameters. Instead, we allow the data to dictate how many parameters there are. In Bayesian modeling, non-parametric distributions are typically used as priors; see Jordan (2005) or Ghahramani (2005) for overviews. In our model, we use two different non-parametric priors: the Pitman-Yor process (for modeling linguistic areas) and Kingman’s coalescent (for modeling linguistic phylogeny), both described below. 2.2.1 The Pitman-Yor Process One particular example of a non-parametric prior is the Pitman-Yor process (Pitman and Yor, 1997), which can be seen as an extension to the betterknown Dirichlet process (Ferguson, 1974). The Pitman-Yor process can be understood as a particular example of a Chinese Restaurant process (CRP) (Pitman, 2002). The idea in all CRPs is that t</context>
</contexts>
<marker>Jordan, 2005</marker>
<rawString>Michael I. Jordan. 2005. Dirichlet processes, Chinese restaurant processes and all that. Tutorial presented at NIPS conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harmut Katz</author>
</authors>
<title>Generative Phonologie und phonologische Sprachb¨unde des Ostjakischen un Samojedischen. Wilhelm Fink.</title>
<date>1975</date>
<contexts>
<context position="1674" citStr="Katz, 1975" startWordPosition="261" endWordPosition="262">, 1963). Third, the languages may be related areally. Languages that occupy the same geographic area often exhibit similar characteristics, not due to genetic relatedness, but due to sharing. Regions (and the languages contained within them) that exhibit sharing are called linguistic areas and the features that are shared are called areal features. Much is not understood or agreed upon in the field of areal linguistics. Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of “borrowability” (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?). In this paper, we attempt to provide a statistical answer to some of these questions. In particular, we develop a Bayesian model of typology that allows for, but does not force, the existence of linguistic areas. Our model also allows for, but does not force, preference for some feature to be shared areally. When applied to a large typological database </context>
</contexts>
<marker>Katz, 1975</marker>
<rawString>Harmut Katz. 1975. Generative Phonologie und phonologische Sprachb¨unde des Ostjakischen un Samojedischen. Wilhelm Fink.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J F C Kingman</author>
</authors>
<booktitle>1982a. The coalescent. Stochastic Processes and their Applications,</booktitle>
<pages>13--235</pages>
<marker>Kingman, </marker>
<rawString>J. F. C. Kingman. 1982a. The coalescent. Stochastic Processes and their Applications, 13:235–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F C Kingman</author>
</authors>
<title>On the genealogy of large populations.</title>
<date>1982</date>
<journal>Journal of Applied Probability,</journal>
<contexts>
<context position="11373" citStr="Kingman, 1982" startWordPosition="1802" endWordPosition="1803">le: perhaps counterintuitively, the distribution does not care about customer order. The Pitman-Yor process induces a power-law distribution on the number of singleton tables (i.e., the number of tables that have only one customer). This can be seen by noticing two things. In general, the number of singleton tables grows as O(αNd). When d = 0, we obtain a Dirichlet process with the number of singleton tables growing as O(α log N). 2.2.2 Kingman’s Coalescent Kingman’s coalescent is a standard model in population genetics describing the common genealogy (ancestral tree) of a set of individuals (Kingman, 1982b; Kingman, 1982a). In its full form it is a distribution over the genealogy of a countable set. Consider the genealogy of n individuals alive at the present time t = 0. We can trace their ancestry backwards in time to the distant past t = −oc. Assume each individual has one parent (in genetics, haploid organisms), and therefore genealogies of [n] = 11, ... , n} form a directed forest. Kingman’s n-coalescent is simply a distribution over genealogies of n individuals. To describe the Markov process in its entirety, it is sufficient to describe the jump process (i.e. the embedded, discrete-time,</context>
</contexts>
<marker>Kingman, 1982</marker>
<rawString>J. F. C. Kingman. 1982b. On the genealogy of large populations. Journal of Applied Probability, 19:27– 43. Essays in Statistical Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Matisoff</author>
</authors>
<title>Genetic versus contact relationship: prosodic diffusibility in South-East Asian languages. In Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics,</title>
<date>2001</date>
<pages>291--327</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="6372" citStr="Matisoff (2001)" startWordPosition="988" endWordPosition="989">tic German, and Finnic languages (especially Estonian and Livonian). (Sometimes many more are included, such as: Belorussian, Lavian, Lithuanian, Norwegian, Old Prussian, Polish, Romani, Russian, Ukranian.) Ethiopia: Afar, Amharic, Anyuak, Awngi, Beja, Ge’ez, Gumuz, Janjero, Kefa, Sidamo, Somali, Tigre, Tigrinya and Wellamo. Needless to say, the exact definition and extent of the actual areas is up to significant debate. Moreover, claims have been made in favor of many linguistic areas not defined above. For instance, Dixon (2001) presents arguments for several Australian linguistic areas and Matisoff (2001) defines a SouthEast Asian language area. Finally, although “folk lore” is in favor of identifying a linguistic area including English, French and certain Norse languages (Norwegian, Swedish, Low Dutch, High German, etc.), there are counter-arguments to this position (Thomason, 2001) (see especially Case Study 9.8). 2.1.2 Linguistic Features Identifying which linguistic features are most easily shared “areally” is a long standing problem in contact linguistics. Here we briefly review some of the major claims. Much of this overview is adoped from the summary given by Curnow (2001). Haugen (1950</context>
</contexts>
<marker>Matisoff, 2001</marker>
<rawString>James Matisoff. 2001. Genetic versus contact relationship: prosodic diffusibility in South-East Asian languages. In Aikhenvald and Dixon, editors, Areal diffusion and genetic inheritance: problems in comparative linguistics, pages 291–327. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
</authors>
<title>Comparing clusterings.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Computational Learning Theory (COLT).</booktitle>
<contexts>
<context position="25337" citStr="Meila, 2003" startWordPosition="4118" endWordPosition="4119">al lists from Section 2.1.1 for IE: the Balkans and the Baltic. When there is overlap (eg., Romani appears in both lists), we assigned it to the Balkans. We compare our model with a flat Pitman-Yor model that does not use the hierarchy. We also compare to a baseline K-means algorithm. For Kmeans, we ran with K E 15,10,15, ... , 80, 85} and chose the value of K for each metric that did best (giving an unfair advantage). Clustering performance is measured on the Indo-European task according to the Rand Index, F-score, Normalized Edit Score (Pantel, 2003) and Normalized Variation of Information (Meila, 2003). In these results, we see that the Pitman-Yor process model dominates the K-means model and the Areal model dominates the Pitman-Yor model. 5.2 Identifying Areal Features Our second experiment is an analysis of the features that tend to be shared areally (as opposed to genetically). For this experiment, we make use of the whole-world version of the data, again with known language family structure. We initialize a Gibbs sampler from the MAP configuration found in Section 5.1. We run the sampler for 1000 iterations and take samples every ten steps. From one particular sample, we can estimate a </context>
</contexts>
<marker>Meila, 2003</marker>
<rawString>Marina Meila. 2003. Comparing clusterings. In Proceedings of the Conference on Computational Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Moravcsik</author>
</authors>
<title>Language contact.</title>
<date>1978</date>
<journal>Universals of Human Language,</journal>
<booktitle>Method and Theory,</booktitle>
<volume>1</volume>
<pages>3--123</pages>
<editor>In J.H. Greenberg, C. Ferguson, and E. Moravcsik, editors,</editor>
<publisher>Stanford University Press.</publisher>
<contexts>
<context position="7481" citStr="Moravcsik (1978)" startWordPosition="1157" endWordPosition="1158"> of the major claims. Much of this overview is adoped from the summary given by Curnow (2001). Haugen (1950) considers only borrowability as far as the lexicon is concerned. He provided evidence that nouns are the easiest, followed by verbs, adjectives, adverbs, prepositions, etc. Ross (1988) corroborates Haugen’s analysis and deepens it to cover morphology, syntax and phonology. He proposes the following hierarchy of borrowability (easiest items coming first): nouns &gt; verbs &gt; adjectives &gt; syntax &gt; non-bound function words &gt; bound morphemes &gt; phonemes. Coming from a “constraints” perspective, Moravcsik (1978) suggests that: lexical items must be borrowed before lexical properties; inflected words before bound morphemes; verbal items can never be borrowed; etc. Curnow (2001) argues that coming up with a reasonable hierarchy of borrowability is that “we may never be able to develop such constraints.” Nevertheless, he divides the space of borrowable features into 15 categories and discusses the evidence supporting each of these categories, including: phonetics (rare), phonology (common), lexical (very common), interjections and discourse markers (com594 mon), free grammatical forms (occasional), boun</context>
</contexts>
<marker>Moravcsik, 1978</marker>
<rawString>E. Moravcsik. 1978. Language contact. In J.H. Greenberg, C. Ferguson, and E. Moravcsik, editors, Universals of Human Language, volume 1; Method and Theory, pages 3–123. Stanford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Clustering by Committee.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Alberta.</institution>
<contexts>
<context position="25283" citStr="Pantel, 2003" startWordPosition="4111" endWordPosition="4112">he IE subset. We compare against the most inclusive areal lists from Section 2.1.1 for IE: the Balkans and the Baltic. When there is overlap (eg., Romani appears in both lists), we assigned it to the Balkans. We compare our model with a flat Pitman-Yor model that does not use the hierarchy. We also compare to a baseline K-means algorithm. For Kmeans, we ran with K E 15,10,15, ... , 80, 85} and chose the value of K for each metric that did best (giving an unfair advantage). Clustering performance is measured on the Indo-European task according to the Rand Index, F-score, Normalized Edit Score (Pantel, 2003) and Normalized Variation of Information (Meila, 2003). In these results, we see that the Pitman-Yor process model dominates the K-means model and the Areal model dominates the Pitman-Yor model. 5.2 Identifying Areal Features Our second experiment is an analysis of the features that tend to be shared areally (as opposed to genetically). For this experiment, we make use of the whole-world version of the data, again with known language family structure. We initialize a Gibbs sampler from the MAP configuration found in Section 5.1. We run the sampler for 1000 iterations and take samples every ten</context>
</contexts>
<marker>Pantel, 2003</marker>
<rawString>Patrick Pantel. 2003. Clustering by Committee. Ph.D. thesis, University of Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability,</title>
<date>1997</date>
<pages>25--855</pages>
<contexts>
<context position="9503" citStr="Pitman and Yor, 1997" startWordPosition="1468" endWordPosition="1471">arametric models is that one does not commit a priori to a particularly number of parameters. Instead, we allow the data to dictate how many parameters there are. In Bayesian modeling, non-parametric distributions are typically used as priors; see Jordan (2005) or Ghahramani (2005) for overviews. In our model, we use two different non-parametric priors: the Pitman-Yor process (for modeling linguistic areas) and Kingman’s coalescent (for modeling linguistic phylogeny), both described below. 2.2.1 The Pitman-Yor Process One particular example of a non-parametric prior is the Pitman-Yor process (Pitman and Yor, 1997), which can be seen as an extension to the betterknown Dirichlet process (Ferguson, 1974). The Pitman-Yor process can be understood as a particular example of a Chinese Restaurant process (CRP) (Pitman, 2002). The idea in all CRPs is that there exists a restaurant with an infinite number of tables. Customers come into the restaurant and have to choose a table at which to sit. The Pitman-Yor process is described by three parameters: a base rate α, a discount parameter d and a mean distribution Go. These combine to describe a process denoted by PY(α, d, Go). The parameters α and d must satisfy: </context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability, 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
</authors>
<title>Combinatorial stochastic processes.</title>
<date>2002</date>
<tech>Technical Report 621,</tech>
<institution>University of California at Berkeley. Lecture notes for St. Flour Summer School.</institution>
<contexts>
<context position="9711" citStr="Pitman, 2002" startWordPosition="1504" endWordPosition="1505">e typically used as priors; see Jordan (2005) or Ghahramani (2005) for overviews. In our model, we use two different non-parametric priors: the Pitman-Yor process (for modeling linguistic areas) and Kingman’s coalescent (for modeling linguistic phylogeny), both described below. 2.2.1 The Pitman-Yor Process One particular example of a non-parametric prior is the Pitman-Yor process (Pitman and Yor, 1997), which can be seen as an extension to the betterknown Dirichlet process (Ferguson, 1974). The Pitman-Yor process can be understood as a particular example of a Chinese Restaurant process (CRP) (Pitman, 2002). The idea in all CRPs is that there exists a restaurant with an infinite number of tables. Customers come into the restaurant and have to choose a table at which to sit. The Pitman-Yor process is described by three parameters: a base rate α, a discount parameter d and a mean distribution Go. These combine to describe a process denoted by PY(α, d, Go). The parameters α and d must satisfy: 0 &lt; d &lt; 1 and α &gt; −d. In the CRP analogy, the model works as follows. The first customer comes in and sits at any table. After N customers have come in and seated themselves (at a total of K tables), the Nth </context>
</contexts>
<marker>Pitman, 2002</marker>
<rawString>Jim Pitman. 2002. Combinatorial stochastic processes. Technical Report 621, University of California at Berkeley. Lecture notes for St. Flour Summer School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Ross</author>
</authors>
<title>Proto Oceanic and the Austronesian languages of western melanesia.</title>
<date>1988</date>
<institution>Linguitics, Australian National University.</institution>
<location>Canberra: Pacific</location>
<contexts>
<context position="7158" citStr="Ross (1988)" startWordPosition="1108" endWordPosition="1109">wegian, Swedish, Low Dutch, High German, etc.), there are counter-arguments to this position (Thomason, 2001) (see especially Case Study 9.8). 2.1.2 Linguistic Features Identifying which linguistic features are most easily shared “areally” is a long standing problem in contact linguistics. Here we briefly review some of the major claims. Much of this overview is adoped from the summary given by Curnow (2001). Haugen (1950) considers only borrowability as far as the lexicon is concerned. He provided evidence that nouns are the easiest, followed by verbs, adjectives, adverbs, prepositions, etc. Ross (1988) corroborates Haugen’s analysis and deepens it to cover morphology, syntax and phonology. He proposes the following hierarchy of borrowability (easiest items coming first): nouns &gt; verbs &gt; adjectives &gt; syntax &gt; non-bound function words &gt; bound morphemes &gt; phonemes. Coming from a “constraints” perspective, Moravcsik (1978) suggests that: lexical items must be borrowed before lexical properties; inflected words before bound morphemes; verbal items can never be borrowed; etc. Curnow (2001) argues that coming up with a reasonable hierarchy of borrowability is that “we may never be able to develop </context>
</contexts>
<marker>Ross, 1988</marker>
<rawString>M.D. Ross. 1988. Proto Oceanic and the Austronesian languages of western melanesia. Canberra: Pacific Linguitics, Australian National University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alf Sommerfelt</author>
</authors>
<title>External versus internal factors in the development of language. Norsk Tidsskrift for Sprogvidenskap,</title>
<date>1960</date>
<contexts>
<context position="23289" citStr="Sommerfelt, 1960" startWordPosition="3771" endWordPosition="3772">Estonian, Finnish (Slavic) Polish, Russian, Ukranian (Austro-Asiatic) Khasi (Dravidian) Telugu (IE) Bengali (Sino-Tibetan) Bawm, Garo, Newari (Kathmandu) Figure 3: A small subset of the world areas identified. the history of the region. The fourth area identified by our model corresponds roughly to the debated “English” area. Our area includes the requisite French/English/German/Norwegian group, as well as the somewhat surprising Irish. However, in addition to being intuitively plausible, it is not hard to find evidence in the literature for the contact relationship between English and Irish (Sommerfelt, 1960). In the whole-world experiment, the model identified too many linguistic areas to fit (39 in total that contained at least two languages, and contained at least two language families). In Figure 5.1, we depict the areas found by our model that best correspond to the areas described in Section 2.1.1. We acknowledge that this gives a warped sense of the quality of our model. Nevertheless, our model is able to identify large parts of the the Meso-American area, the Baltic area and the South Asian area. (It also finds the Balkans, but since these languages are all IE, we do not consider it a ling</context>
</contexts>
<marker>Sommerfelt, 1960</marker>
<rawString>Alf Sommerfelt. 1960. External versus internal factors in the development of language. Norsk Tidsskrift for Sprogvidenskap, 19:296–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Hal Daum´e</author>
<author>Daniel Roy</author>
</authors>
<title>Bayesian agglomerative clustering with coalescents.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<marker>Teh, Daum´e, Roy, 2007</marker>
<rawString>Yee Whye Teh, Hal Daum´e III, and Daniel Roy. 2007. Bayesian agglomerative clustering with coalescents. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Thomason</author>
</authors>
<title>Language contact: an introduction.</title>
<date>2001</date>
<publisher>Edinburgh University Press.</publisher>
<contexts>
<context position="1661" citStr="Thomason, 2001" startWordPosition="259" endWordPosition="260">rsals (Greenberg, 1963). Third, the languages may be related areally. Languages that occupy the same geographic area often exhibit similar characteristics, not due to genetic relatedness, but due to sharing. Regions (and the languages contained within them) that exhibit sharing are called linguistic areas and the features that are shared are called areal features. Much is not understood or agreed upon in the field of areal linguistics. Different linguists favor different defintions of what it means to be a linguistic area (are two languages sufficient to describe an area or do you need three (Thomason, 2001; Katz, 1975)?), what areal features are (is there a linear ordering of “borrowability” (Katz, 1975; Curnow, 2001) or is that too prescriptive?), and what causes sharing to take place (does social status or number of speakers play a role (Thomason, 2001)?). In this paper, we attempt to provide a statistical answer to some of these questions. In particular, we develop a Bayesian model of typology that allows for, but does not force, the existence of linguistic areas. Our model also allows for, but does not force, preference for some feature to be shared areally. When applied to a large typologi</context>
<context position="6656" citStr="Thomason, 2001" startWordPosition="1030" endWordPosition="1031">damo, Somali, Tigre, Tigrinya and Wellamo. Needless to say, the exact definition and extent of the actual areas is up to significant debate. Moreover, claims have been made in favor of many linguistic areas not defined above. For instance, Dixon (2001) presents arguments for several Australian linguistic areas and Matisoff (2001) defines a SouthEast Asian language area. Finally, although “folk lore” is in favor of identifying a linguistic area including English, French and certain Norse languages (Norwegian, Swedish, Low Dutch, High German, etc.), there are counter-arguments to this position (Thomason, 2001) (see especially Case Study 9.8). 2.1.2 Linguistic Features Identifying which linguistic features are most easily shared “areally” is a long standing problem in contact linguistics. Here we briefly review some of the major claims. Much of this overview is adoped from the summary given by Curnow (2001). Haugen (1950) considers only borrowability as far as the lexicon is concerned. He provided evidence that nouns are the easiest, followed by verbs, adjectives, adverbs, prepositions, etc. Ross (1988) corroborates Haugen’s analysis and deepens it to cover morphology, syntax and phonology. He propo</context>
</contexts>
<marker>Thomason, 2001</marker>
<rawString>Sarah Thomason. 2001. Language contact: an introduction. Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Warnow</author>
<author>S N Evans</author>
<author>D Ringe</author>
<author>L Nakhleh</author>
</authors>
<title>A stochastic model of language evolution that incorporates homoplasy and borrowing.</title>
<date>2005</date>
<booktitle>In Phylogenetic Methods and the Prehistory of Language.</booktitle>
<publisher>Cambridge University Press.</publisher>
<note>Invited paper.</note>
<contexts>
<context position="2774" citStr="Warnow et al. (2005)" startWordPosition="441" endWordPosition="444">for, but does not force, preference for some feature to be shared areally. When applied to a large typological database of linguistic features (Haspelmath et al., 2005), we find that it discovers linguistic areas that are well documented in the literature (see Campbell (2005) for an overview), and a small preference for certain features to be shared areally. This latter agrees, to a lesser degree, with some of the published hierarchies of borrowability (Curnow, 2001). Finally, we show that reconstructing language family trees is significantly aided by knowledge of areal features. We note that Warnow et al. (2005) have independently proposed a model for phonological change in Indo-European (based on the Dyen dataset (Dyen et al., 1992)) that includes notions of borrowing. Our model is different in that we (a) base our model on typological features rather than just lexical patterns and (b) we explicitly represent language areas, not just one-time borrowing phenomena. 2 Background We describe (in Section 3) a non-parametric, hierarchical Bayesian model for finding linguistic areas and areal features. In this section, we provide necessary background—both linguistic and statistical— 593 Human Language Tech</context>
</contexts>
<marker>Warnow, Evans, Ringe, Nakhleh, 2005</marker>
<rawString>T. Warnow, S.N. Evans, D. Ringe, and L. Nakhleh. 2005. A stochastic model of language evolution that incorporates homoplasy and borrowing. In Phylogenetic Methods and the Prehistory of Language. Cambridge University Press. Invited paper.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>