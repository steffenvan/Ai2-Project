<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009194">
<title confidence="0.992686">
Modeling of term-distance and term-occurrence information for im-
proving n-gram language model performance
</title>
<author confidence="0.980226">
Tze Yuang Chong1,2, Rafael E. Banchs3, Eng Siong Chng1,2, Haizhou Li1,2,3
</author>
<affiliation confidence="0.950637">
1Temasek Laboratory, Nanyang Technological University, Singapore 639798
2School of Computer Engineering, Nanyang Technological University, Singapore 639798
3Institute for Infocomm Research, Singapore 138632
</affiliation>
<email confidence="0.9843485">
tychong@ntu.edu.sg, rembanchs@i2r.a-star.edu.sg,
aseschng@ntu.edu.sg, hli@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985295307692308">
In this paper, we explore the use of distance
and co-occurrence information of word-pairs
for language modeling. We attempt to extract
this information from history-contexts of up to
ten words in size, and found it complements
well the n-gram model, which inherently suf-
fers from data scarcity in learning long histo-
ry-contexts. Evaluated on the WSJ corpus, bi-
gram and trigram model perplexity were re-
duced up to 23.5% and 14.0%, respectively.
Compared to the distant bigram, we show that
word-pairs can be more effectively modeled in
terms of both distance and occurrence.
</bodyText>
<sectionHeader confidence="0.998907" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999821818181818">
Language models have been extensively studied
in natural language processing. The role of a lan-
guage model is to measure how probably a (tar-
get) word would occur based on some given evi-
dence extracted from the history-context. The
commonly used n-gram model (Bahl et al. 1983)
takes the immediately preceding history-word
sequence, of length n — 1, as the evidence for
prediction. Although n-gram models are simple
and effective, modeling long history-contexts
lead to severe data scarcity problems. Hence, the
context length is commonly limited to as short as
three, i.e. the trigram model, and any useful in-
formation beyond this window is neglected.
In this work, we explore the possibility of
modeling the presence of a history-word in terms
of: (1) the distance and (2) the co-occurrence,
with a target-word. These two attributes will be
exploited and modeled independently from each
other, i.e. the distance is described regardless the
actual frequency of the history-word, while the
co-occurrence is described regardless the actual
position of the history-word. We refer to these
two attributes as the term-distance (TD) and the
term-occurrence (TO) components, respectively.
The rest of this paper is structured as follows.
The following section presents the most relevant
related works. Section 3 introduces and moti-
vates our proposed approach. Section 4 presents
in detail the derivation of both TD and TO model
components. Section 5 presents some perplexity
evaluation results. Finally, section 6 presents our
conclusions and proposed future work.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99994975">
The distant bigram model (Huang et.al 1993,
Simon et al. 1997, Brun et al. 2007) disassembles
the n-gram into (n−1) word-pairs, such that each
pair is modeled by a distance-k bigram model,
where 1 &lt;— k &lt;— n — 1 . Each distance-k bigram
model predicts the target-word based on the oc-
currence of a history-word located k positions
behind.
Zhou &amp; Lua (1998) enhanced the effective-
ness of the model by filtering out those word-
pairs exhibiting low correlation, so that only the
well associated distant bigrams are retained. This
approach is referred to as the distance-dependent
trigger model, and is similar to the earlier pro-
posed trigger model (Lau et al. 1993, Rosenfeld
1996) that relies on the bigrams of arbitrary dis-
tance, i.e. distance-independent.
Latent-semantic language model approaches
(Bellegarda 1998, Coccaro 2005) weight word
counts with TFIDF to highlight their semantic
importance towards the prediction. In this type of
approach, count statistics are accumulated from
long contexts, typically beyond ten to twenty
words. In order to confine the complexity intro-
duced by such long contexts, word ordering is
ignored (i.e. bag-of-words paradigm).
Other approaches such as the class-based lan-
guage model (Brown 1992, Kneser &amp; Ney 1993)
</bodyText>
<page confidence="0.980037">
233
</page>
<bodyText confidence="0.8780085">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
use POS or POS-like classes of the history-words
for prediction. The structured language model
(Chelba &amp; Jelinek 2000) determines the “heads”
in the history-context by using a parsing tree.
There are also works on skipping irrelevant his-
tory-words in order to reveal more informative n-
grams (Siu &amp; Ostendorf 2000, Guthrie et al.
2006). Cache language models exploit temporal
word frequencies in the history (Kuhn &amp; Mori
1990, Clarkson &amp; Robinson 1997).
</bodyText>
<sectionHeader confidence="0.54885" genericHeader="method">
3 Motivation of the Proposed Approach
</sectionHeader>
<bodyText confidence="0.999523677419355">
The attributes of distance and co-occurrence are
exploited and modeled differently in each lan-
guage modeling approach. In the n-gram model,
for example, these two attributes are jointly taken
into account in the ordered word-sequence. Con-
sequently, the n-gram model can only be effec-
tively implemented within a short history-context
(e.g. of size of three or four).
Both, the conventional trigger model and the
latent-semantic model capture the co-occurrence
information while ignoring the distance informa-
tion. It is reasonable to assume that distance in-
formation at far contexts is less likely to be in-
formative and, hence, can be discarded. Howev-
er, intermediate distances beyond the n-gram
model limits can be very useful and should not
be discarded.
On the other hand, distant-bigram models and
distance-dependent trigger models make use of
both, distance and co-occurrence, information up
to window sizes of ten to twenty. They achieve
this by compromising inter-dependencies among
history-words (i.e. the context is represented as
separated word-pairs). However, similarly to n-
gram models, distance and co-occurrence infor-
mation are implicitly tied within the word-pairs.
In our proposed approach, we attempt to ex-
ploit the TD and TO attributes, separately, to in-
corporate distant context information into the n-
gram, as a remedy to the data scarcity problem
when learning the far context.
</bodyText>
<sectionHeader confidence="0.924907" genericHeader="method">
4 Language Modeling with TD and TO
</sectionHeader>
<bodyText confidence="0.983164733333333">
A language model estimates word probabilities
given their history, i.e. P(t = wiIh = ����
������ ,
where t denotes the target word and h denotes its
corresponding history. Let the word located at ith
position, wi , be the target-word and its preceding
word-sequence ����
����� = (wi_��� ••• wi_ w_i) of
length n — 1, be its history-context. Also, in or-
der to alleviate the data scarcity problem, we as-
sume the occurrences of the history-words to be
independent from each other, conditioned to the
occurrence of the target-word wi , i.e. wi-k 1
wi-1Iwi , where wi_k, wi-1 E h, and k # 1. The
probability can then be approximated as:
</bodyText>
<equation confidence="0.9980925">
P(t = wilh = ����
������
p(t = wi) n-~
~!1 P(hk = wi-k It = wi)
�
&amp;quot;(h)
</equation>
<bodyText confidence="0.996402">
where &amp;quot;(h) is a normalizing term, and hk = wi-k
indicates that wi_k is the word at position kth.
</bodyText>
<subsectionHeader confidence="0.983563">
4.1 Derivation of the TD-TO Model
</subsectionHeader>
<bodyText confidence="0.969262">
In order to define the TD and TO components for
language modeling, we express the observation
of an arbitrary history-word, wi_k at the kth posi-
tion behind the target-word, as the joint of two
events: i) the word wi_k occurs within the histo-
ry-context: wi_k E h, and ii) it occurs at distance
k from the target-word: #(wi_k) = k, (#= k for
brevity); i.e. (hk = wi-k) $(wi-k E h) % (#= k).
Thus, the probability in Eq.1 can be written as:
</bodyText>
<equation confidence="0.998064857142857">
P(t = ���� � ����
������
��� � ��� ������ � �� #� ��� � ���
���
�!�
�
&amp;quot;(h)
</equation>
<bodyText confidence="0.999981">
where the likelihood P(wi_k E h,#= klt = wi)
measures how likely the joint event (wi-k E h,
#= k) would be observed given the target-word
wi. This can be rewritten in terms of the likelih-
ood function of the distance event (i.e. #= k)
and the occurrence event (i.e. wi_k E h), where
both of them can be modeled and exploited sepa-
rately, as follows:
</bodyText>
<equation confidence="0.9987024">
P(t = wilh = ����
������
��� � ���
���
&amp; &apos;
�!� ��#� ������ � �� � � ���
���
�!� P(wi-k E hIt = wi)
�
&amp;quot;(h)
</equation>
<bodyText confidence="0.999983">
The formulation above yields three terms, re-
ferred to as the prior, the TD likelihood, and the
TO likelihood, respectively.
In Eq.3, we have decoupled the observation of
a word-pair into the events of distance and co-
occurrence. This allows for independently mod-
eling and exploiting them. In order to control
their contributions towards the final prediction of
the target-word, we weight these components:
</bodyText>
<equation confidence="0.9939584">
P(t = with = ����
������
p(t = W,)()
� ��#� ������ � �� � � ���
���
&amp; �(* &apos;
�!�
( k!1 P(wi-k E hI t = wi)���)(+
AZ
&amp;quot;(h)
</equation>
<page confidence="0.976907">
234
</page>
<bodyText confidence="0.99964725">
where ,, ,- , and ,. are the weights for the
prior, TD and TO models, respectively.
Notice that the model depicted in Eq.4 is the
log-linear interpolation (Klakow 1998) of these
models. The prior, which is usually implemented
as a unigram model, can be also replaced with a
higher order n-gram model as, for instance, the
bigram model:
</bodyText>
<equation confidence="0.9986379">
P(t = with = ����
������
P(t = with = wi-i)()
� ��#� ������ � �� � � ���
���
&amp; �(* &apos;
�!�
( k!1 P(wi-k E hl t = wi)���)(+
AZ
&amp;quot;(h)
</equation>
<bodyText confidence="0.99974525">
Replacing the unigram model with a higher
order n-gram model is important to compensate
the damage incurred by the conditional indepen-
dence assumption made earlier.
</bodyText>
<sectionHeader confidence="0.724504" genericHeader="method">
4.2 Term-Distance Model Component
</sectionHeader>
<bodyText confidence="0.999292571428571">
Basically, the TD likelihood measures how likely
a given word-pair would be separated by a given
distance. So, word-pairs possessing consistent
separation distances will favor this likelihood.
The TD likelihood for a distance k given the co-
occurrence of the word-pair (wi-k, wi) can be
estimated from counts as follows:
</bodyText>
<equation confidence="0.993998666666667">
P(#= klwi-k E h,t = wi)
� /(wi-k E h, t = wi, #= k) (6)
/(wi-k E h, t = wi)
</equation>
<bodyText confidence="0.999911">
The above formulation of the TD likelihood
requires smoothing for resolving two problems:
i) a word-pair at a particular distance has a zero
count, i.e. /(wi-k E h, t = wi, #= k) = 0 , which
results in a zero probability, and ii) a word-pair is
not seen at any distance within the observation
window, i.e. zero co-occurrence /(wi-k E h, t =
wi) = 0, which results in a division by zero.
For the first problem, we have attempted to
redistribute the counts among the word-pairs at
different distances (as observed within the win-
dow). We assumed that the counts of word-pairs
are smooth in the distance domain and that the
influence of a word decays as the distance in-
creases. Accordingly, we used a weighted mov-
ing-average filter for performing the smoothing.
Similar approaches have also been used in other
works (Coccaro 2005, Lv &amp; Zhai 2009). Notice,
however, that this strategy is different from other
conventional smoothing techniques (Chen &amp;
Goodman 1996), which rely mainly on the count-
of-count statistics for re-estimating and smooth-
ing the original counts.
For the second problem, when a word-pair
was not seen at any distance (within the win-
dow), we arbitrarily assigned a small probability
value, P(#= kIwi-k E h, t = wi) = 0101 , to pro-
vide a slight chance for such a word-pair
(wi-k, wi) to occur at close distances.
</bodyText>
<subsectionHeader confidence="0.971036">
4.3 Term-Occurrence Model Component
</subsectionHeader>
<bodyText confidence="0.999984666666667">
During the decoupling operation (from Eq.2 to
Eq.3), the TD model held only the distance in-
formation while the count information has been
ignored. Notice the normalization of word-pair
counts in Eq.6.
As a complement to the TD model, the TO
model focuses on co-occurrence, and holds only
count information. As the distance information is
captured by the TD model, the co-occurrence
count captured by the TO model is independent
from the given word-pair distance.
In fact, the TO model is closely related to the
trigger language model (Rosenfeld 1996), as the
prediction of the target-word (the triggered word)
is based on the presence of a history-word (the
trigger). However, differently from the trigger
model, the TO model considers all the word-
pairs without filtering out the weak associated
ones. Additionally, the TO model takes into ac-
count multiple co-occurrences of the same histo-
ry-word within the window, while the trigger
model would count them only once (i.e. consid-
ers binary counts).
The word-pairs that frequently co-occur at ar-
bitrary distances (within an observation window)
would favor the TO likelihood. It can be esti-
mated from counts as:
</bodyText>
<equation confidence="0.945846">
P(wi-k E hl t = wi) = /(Wi-k E h, t = wi)/(t _ wi) (7)
</equation>
<bodyText confidence="0.9997004">
When a word-pair did not co-occur (within the
observation window), we assigned a small prob-
ability value, P(wi-k E hIt = wi) = 0101, to pro-
vide a slight chance for the history word to occur
within the history-context of the target word.
</bodyText>
<sectionHeader confidence="0.992125" genericHeader="method">
5 Perplexity Evaluation
</sectionHeader>
<bodyText confidence="0.999880666666667">
A perplexity test was run on the BLLIP WSJ
corpus (Charniak 2000) with the standard 5K
vocabulary. The entire WSJ ’87 data (740K sen-
tences 18M words) was used as train-set to train
the n-gram, TD, and TO models. The dev-set and
the test-set, each comprising 500 sentences and
about 12K terms, were selected randomly from
WSJ ’88 data. We used them for parameter fine-
tuning and performance evaluation.
</bodyText>
<figure confidence="0.522542">
(5)
</figure>
<page confidence="0.991808">
235
</page>
<subsectionHeader confidence="0.998419">
5.1 Capturing Distant Information
</subsectionHeader>
<bodyText confidence="0.999117458333333">
In this experiment, we assessed the effectiveness
of the TD and TO components in reducing the n-
gram’s perplexity. Following Eq.5, we interpo-
lated n-gram models (of orders from two to six)
with the TD, TO, and the both of them (referred
to as TD-TO model).
By using the dev-set, optimal interpolation
weights (i.e. fl, fld, and fl.) for the three combi-
nations (n-gram with TD, TO, and TD-TO) were
computed. The resulting interpolation weights
were as follows: n-gram with TD = (0.85, 0.15),
n-gram with TO = (0.85, 0.15), and n-gram with
TD-TO = (0.80, 0.07, 0.13).
The history-context window sizes were opti-
mized too. Optimal sizes resulted to be 7, 5 and 8
for TD, TO, and TD-TO models, respectively. In
fact, we observed that the performance is quite
robust with respect to the window’s length. De-
viating about two words from the optimum
length only worsens the perplexity less than 1%.
Baseline models, in each case, are standard n-
gram models with modified Kneser-Ney interpo-
lation (Chen 1996). The test-set results are de-
picted in Table 1.
</bodyText>
<table confidence="0.996480571428571">
N NG NG- Red. NG- Red. NG- Red.
TD (%) TO (%) TDTO (%)
2 151.7 134.5 11.3 119.9 21.0 116.0 23.5
3 99.2 92.9 6.3 86.7 12.6 85.3 14.0
4 91.8 86.1 6.2 81.4 11.3 80.1 12.7
5 90.1 84.7 6.0 80.2 11.0 79.0 12.3
6 89.7 84.4 5.9 79.9 10.9 78.7 12.2
</table>
<tableCaption confidence="0.979851">
Table 1. Perplexities of the n-gram model (NG)
</tableCaption>
<bodyText confidence="0.983770266666667">
of order (N) two to six and their combinations
with the TD, TO, and TD-TO models.
As seen from the table, for lower order n-gram
models, the complementary information captured
by the TD and TO components reduced the per-
plexity up to 23.5% and 14.0%, for bigram and
trigram models, respectively. Higher order n-
gram models, e.g. hexagram, observe history-
contexts of similar lengths as the ones observed
by the TD, TO, and TD-TO models. Due to the
incapability of n-grams to model long history-
contexts, the TD and TO components are still
effective in helping to enhance the prediction.
Similar results were obtained by using the stan-
dard back-off model (Katz 1987) as baseline.
</bodyText>
<subsectionHeader confidence="0.999883">
5.2 Benefit of Decoupling Distant-Bigram
</subsectionHeader>
<bodyText confidence="0.998516285714286">
In this second experiment, we examined whether
the proposed decoupling procedure leads to bet-
ter modeling of word-pairs compared to the dis-
tant bigram model. Here we compare the per-
plexity of both, the distance-k bigram model and
distance-k TD model (for values of k ranging
from two to ten), when combined with a standard
bigram model.
In order to make a fair comparison, without
taking into account smoothing effects, we trained
both models with raw counts and evaluated their
perplexities over the train-set (so that no zero-
probability will be encountered). The results are
depicted in Table 2.
</bodyText>
<table confidence="0.993754">
k 2 4 6 8 10
DBG 105.7 112.5 114.4 115.9 116.8
TD 98.5 106.6 109.1 111.0 112.2
</table>
<tableCaption confidence="0.892585333333333">
Table 2. Perplexities of the distant bigram (DBG)
and TD models when interpolated with a stan-
dard bigram model.
</tableCaption>
<bodyText confidence="0.999754333333333">
The results from Table 2 show that the TD
component complements the bigram model bet-
ter than the distant bigram itself. Firstly, these
results suggest that the distance information (as
modeled by the TD) offers better cue than the
count information (as modeled by the distant bi-
gram) to complement the n-gram model.
The normalization of distant bigram counts, as
indicated in Eq.6, aims at highlighting the infor-
mation provided by the relative positions of
words in the history-context. This has been
shown to be an effective manner to exploit the
far context. By also considering the results in
Table 1, we can deduce that better performance
can be obtained when the TO attribute is also
involved. Overall, decoupling the word history-
context into the TD and TO components offers a
good approach to enhance language modeling.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999175">
We have proposed a new approach to compute
the n-gram probabilities, based on the TD and
TO model components. Evaluated on the WSJ
corpus, the proposed TD and TO models reduced
the bigram’s and trigram’s perplexities up to
23.5% and 14.0%, respectively. We have shown
the advantages of modeling word-pairs with TD
and TO, as compared to the distant bigram.
As future work, we plan to explore the useful-
ness of the proposed model components in actual
natural language processing applications such as
machine translation and speech recognition. Ad-
ditionally, we also plan to develop a more prin-
cipled framework for dealing with TD smoothing.
</bodyText>
<page confidence="0.996971">
236
</page>
<sectionHeader confidence="0.986267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999378333333334">
Bahl, L., Jelinek, F. &amp; Mercer, R. 1983. A statistical
approach to continuous speech recognition. IEEE
Trans. Pattern Analysis and Machine Intelligence,
5:179-190.
Bellegarda, J. R. 1998. A multispan language model-
ing framework for larfge vocabulary speech recog-
nition. IEEE Trans. on Speech and Audio
Processing, 6(5): 456-467.
Brown, P.F. 1992 Class-based n-gram models of natu-
ral language. Computational Linguistics, 18: 467-
479.
Brun, A., Langlois, D. &amp; Smaili, K. 2007. Improving
language models by using distant information. In
Proc. ISSPA 2007, pp.1-4.
Cavnar, W.B. &amp; Trenkle, J.M. 1994. N-gram-based
text categorization. Proc. SDAIR-94, pp.161-175.
Charniak, E., et al. 2000. BLLIP 1987-89 WSJ Cor-
pus Release 1. Linguistic Data Consortium, Phila-
delphia.
Chen, S.F. &amp; Goodman, J. 1996. An empirical study
of smoothing techniques for language modeling.
In. Proc. ACL ’96, pp. 310-318.
Chelba, C. &amp; Jelinek, F. 2000. Structured language
modeling. Computer Speech &amp; Language, 14: 283-
332.
Clarkson, P.R. &amp; Robinson, A.J. 1997. Language
model adaptation using mixtures and an exponen-
tially decaying cache. In Proc. ICASSP-97, pp.799-
802.
Coccaro, N. 2005. Latent semantic analysis as a tool
to improve automatic speech recognition perfor-
mance. Doctoral Dissertation, University of Colo-
rado, Boulder, CO, USA.
Guthrie, D., Allison, B., Liu, W., Guthrie, L., &amp;
Wilks, Y. 2006. A closer look at skip-gram model-
ling. In Proc. LREC-2006, pp.1222-1225.
Huang, X. et al. 1993. The SPHINX-II speech recog-
nition system: an overview. Computer Speech and
Language, 2: 137-148.
Katz, S.M. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Trans. on Acoustics,
Speech, &amp; Signal Processing, 35:400-401.
Klakow, D. 1998. Log-linear interpolation of lan-
guage model. In Proc. ICSLP 1998, pp.1-4.
Kneser, R. &amp; Ney, H. 1993. Improving clustering
techniques for class-based statistical language
modeling. In Proc. EUROSPEECH ’93, pp.973-
976.
Kuhn, R. &amp; Mori, R.D. 1990. A cache-based natural
language model for speech recognition. IEEE
Trans. Pattern Analysis and Machine Intelligence,
12(6): 570-583.
Lau, R. et al. 1993. Trigger-based language models: a
maximum-entropy approach. In Proc. ICASSP-94,
pp.45-48.
Lv Y. &amp; Zhai C. 2009. Positional language models for
information retrieval. In Proc. SIGIR’09, pp.299-
306.
Rosenfeld, R. 1996. A maximum entropy approach to
adaptive statistical language modelling. Computer
Speech and Language, 10: 187-228.
Simons, M., Ney, H. &amp; Martin S.C. 1997. Distant
bigram language modelling using maximum entro-
py. In Proc. ICASSP-97, pp.787-790.
Siu, M. &amp; Ostendorf, M. 2000. Variable n-grams and
extensions for conversational speech language
modeling. IEEE Trans. on Speech and Audio
Processing, 8(1): 63-75.
Zhou G. &amp; Lua K.T. 1998. Word association and MI-
trigger-based language modeling. In Proc. COL-
ING-ACL, 1465-1471.
</reference>
<page confidence="0.997409">
237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372812">
<title confidence="0.932497">of term-distance and term-occurrence information for proving n-gram language model performance</title>
<author confidence="0.981482">Yuang Rafael E Eng Siong Haizhou</author>
<affiliation confidence="0.7811085">Laboratory, Nanyang Technological University, Singapore of Computer Engineering, Nanyang Technological University, Singapore</affiliation>
<address confidence="0.763471">for Infocomm Research, Singapore</address>
<email confidence="0.8876675">tychong@ntu.edu.sg,aseschng@ntu.edu.sg,hli@i2r.a-star.edu.sg</email>
<abstract confidence="0.9979375">In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements the model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>A statistical approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Trans. Pattern Analysis and Machine Intelligence,</journal>
<pages>5--179</pages>
<contexts>
<context position="1353" citStr="Bahl et al. 1983" startWordPosition="189" endWordPosition="192">, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1 Introduction Language models have been extensively studied in natural language processing. The role of a language model is to measure how probably a (target) word would occur based on some given evidence extracted from the history-context. The commonly used n-gram model (Bahl et al. 1983) takes the immediately preceding history-word sequence, of length n — 1, as the evidence for prediction. Although n-gram models are simple and effective, modeling long history-contexts lead to severe data scarcity problems. Hence, the context length is commonly limited to as short as three, i.e. the trigram model, and any useful information beyond this window is neglected. In this work, we explore the possibility of modeling the presence of a history-word in terms of: (1) the distance and (2) the co-occurrence, with a target-word. These two attributes will be exploited and modeled independentl</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, L., Jelinek, F. &amp; Mercer, R. 1983. A statistical approach to continuous speech recognition. IEEE Trans. Pattern Analysis and Machine Intelligence, 5:179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<title>A multispan language modeling framework for larfge vocabulary speech recognition.</title>
<date>1998</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>6</volume>
<issue>5</issue>
<pages>456--467</pages>
<contexts>
<context position="3461" citStr="Bellegarda 1998" startWordPosition="522" endWordPosition="523"> &lt;— k &lt;— n — 1 . Each distance-k bigram model predicts the target-word based on the occurrence of a history-word located k positions behind. Zhou &amp; Lua (1998) enhanced the effectiveness of the model by filtering out those wordpairs exhibiting low correlation, so that only the well associated distant bigrams are retained. This approach is referred to as the distance-dependent trigger model, and is similar to the earlier proposed trigger model (Lau et al. 1993, Rosenfeld 1996) that relies on the bigrams of arbitrary distance, i.e. distance-independent. Latent-semantic language model approaches (Bellegarda 1998, Coccaro 2005) weight word counts with TFIDF to highlight their semantic importance towards the prediction. In this type of approach, count statistics are accumulated from long contexts, typically beyond ten to twenty words. In order to confine the complexity introduced by such long contexts, word ordering is ignored (i.e. bag-of-words paradigm). Other approaches such as the class-based language model (Brown 1992, Kneser &amp; Ney 1993) 233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association f</context>
</contexts>
<marker>Bellegarda, 1998</marker>
<rawString>Bellegarda, J. R. 1998. A multispan language modeling framework for larfge vocabulary speech recognition. IEEE Trans. on Speech and Audio Processing, 6(5): 456-467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>467--479</pages>
<contexts>
<context position="3878" citStr="Brown 1992" startWordPosition="585" endWordPosition="586"> proposed trigger model (Lau et al. 1993, Rosenfeld 1996) that relies on the bigrams of arbitrary distance, i.e. distance-independent. Latent-semantic language model approaches (Bellegarda 1998, Coccaro 2005) weight word counts with TFIDF to highlight their semantic importance towards the prediction. In this type of approach, count statistics are accumulated from long contexts, typically beyond ten to twenty words. In order to confine the complexity introduced by such long contexts, word ordering is ignored (i.e. bag-of-words paradigm). Other approaches such as the class-based language model (Brown 1992, Kneser &amp; Ney 1993) 233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics use POS or POS-like classes of the history-words for prediction. The structured language model (Chelba &amp; Jelinek 2000) determines the “heads” in the history-context by using a parsing tree. There are also works on skipping irrelevant history-words in order to reveal more informative ngrams (Siu &amp; Ostendorf 2000, Guthrie et al. 2006). Cache language models exploit temporal word frequenc</context>
</contexts>
<marker>Brown, 1992</marker>
<rawString>Brown, P.F. 1992 Class-based n-gram models of natural language. Computational Linguistics, 18: 467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Brun</author>
<author>D Langlois</author>
<author>K Smaili</author>
</authors>
<title>Improving language models by using distant information.</title>
<date>2007</date>
<booktitle>In Proc. ISSPA</booktitle>
<pages>1--4</pages>
<contexts>
<context position="2730" citStr="Brun et al. 2007" startWordPosition="402" endWordPosition="405">l position of the history-word. We refer to these two attributes as the term-distance (TD) and the term-occurrence (TO) components, respectively. The rest of this paper is structured as follows. The following section presents the most relevant related works. Section 3 introduces and motivates our proposed approach. Section 4 presents in detail the derivation of both TD and TO model components. Section 5 presents some perplexity evaluation results. Finally, section 6 presents our conclusions and proposed future work. 2 Related Work The distant bigram model (Huang et.al 1993, Simon et al. 1997, Brun et al. 2007) disassembles the n-gram into (n−1) word-pairs, such that each pair is modeled by a distance-k bigram model, where 1 &lt;— k &lt;— n — 1 . Each distance-k bigram model predicts the target-word based on the occurrence of a history-word located k positions behind. Zhou &amp; Lua (1998) enhanced the effectiveness of the model by filtering out those wordpairs exhibiting low correlation, so that only the well associated distant bigrams are retained. This approach is referred to as the distance-dependent trigger model, and is similar to the earlier proposed trigger model (Lau et al. 1993, Rosenfeld 1996) that</context>
</contexts>
<marker>Brun, Langlois, Smaili, 2007</marker>
<rawString>Brun, A., Langlois, D. &amp; Smaili, K. 2007. Improving language models by using distant information. In Proc. ISSPA 2007, pp.1-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Cavnar</author>
<author>J M Trenkle</author>
</authors>
<title>N-gram-based text categorization.</title>
<date>1994</date>
<booktitle>Proc. SDAIR-94,</booktitle>
<pages>161--175</pages>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>Cavnar, W.B. &amp; Trenkle, J.M. 1994. N-gram-based text categorization. Proc. SDAIR-94, pp.161-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<date>2000</date>
<booktitle>BLLIP 1987-89 WSJ Corpus Release 1. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="12307" citStr="Charniak 2000" startWordPosition="2060" endWordPosition="2061">rigger model would count them only once (i.e. considers binary counts). The word-pairs that frequently co-occur at arbitrary distances (within an observation window) would favor the TO likelihood. It can be estimated from counts as: P(wi-k E hl t = wi) = /(Wi-k E h, t = wi)/(t _ wi) (7) When a word-pair did not co-occur (within the observation window), we assigned a small probability value, P(wi-k E hIt = wi) = 0101, to provide a slight chance for the history word to occur within the history-context of the target word. 5 Perplexity Evaluation A perplexity test was run on the BLLIP WSJ corpus (Charniak 2000) with the standard 5K vocabulary. The entire WSJ ’87 data (740K sentences 18M words) was used as train-set to train the n-gram, TD, and TO models. The dev-set and the test-set, each comprising 500 sentences and about 12K terms, were selected randomly from WSJ ’88 data. We used them for parameter finetuning and performance evaluation. (5) 235 5.1 Capturing Distant Information In this experiment, we assessed the effectiveness of the TD and TO components in reducing the ngram’s perplexity. Following Eq.5, we interpolated n-gram models (of orders from two to six) with the TD, TO, and the both of t</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E., et al. 2000. BLLIP 1987-89 WSJ Corpus Release 1. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In. Proc. ACL ’96,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="10361" citStr="Chen &amp; Goodman 1996" startWordPosition="1725" endWordPosition="1728">= wi) = 0, which results in a division by zero. For the first problem, we have attempted to redistribute the counts among the word-pairs at different distances (as observed within the window). We assumed that the counts of word-pairs are smooth in the distance domain and that the influence of a word decays as the distance increases. Accordingly, we used a weighted moving-average filter for performing the smoothing. Similar approaches have also been used in other works (Coccaro 2005, Lv &amp; Zhai 2009). Notice, however, that this strategy is different from other conventional smoothing techniques (Chen &amp; Goodman 1996), which rely mainly on the countof-count statistics for re-estimating and smoothing the original counts. For the second problem, when a word-pair was not seen at any distance (within the window), we arbitrarily assigned a small probability value, P(#= kIwi-k E h, t = wi) = 0101 , to provide a slight chance for such a word-pair (wi-k, wi) to occur at close distances. 4.3 Term-Occurrence Model Component During the decoupling operation (from Eq.2 to Eq.3), the TD model held only the distance information while the count information has been ignored. Notice the normalization of word-pair counts in </context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Chen, S.F. &amp; Goodman, J. 1996. An empirical study of smoothing techniques for language modeling. In. Proc. ACL ’96, pp. 310-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>14</volume>
<pages>283--332</pages>
<contexts>
<context position="4208" citStr="Chelba &amp; Jelinek 2000" startWordPosition="630" endWordPosition="633">approach, count statistics are accumulated from long contexts, typically beyond ten to twenty words. In order to confine the complexity introduced by such long contexts, word ordering is ignored (i.e. bag-of-words paradigm). Other approaches such as the class-based language model (Brown 1992, Kneser &amp; Ney 1993) 233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics use POS or POS-like classes of the history-words for prediction. The structured language model (Chelba &amp; Jelinek 2000) determines the “heads” in the history-context by using a parsing tree. There are also works on skipping irrelevant history-words in order to reveal more informative ngrams (Siu &amp; Ostendorf 2000, Guthrie et al. 2006). Cache language models exploit temporal word frequencies in the history (Kuhn &amp; Mori 1990, Clarkson &amp; Robinson 1997). 3 Motivation of the Proposed Approach The attributes of distance and co-occurrence are exploited and modeled differently in each language modeling approach. In the n-gram model, for example, these two attributes are jointly taken into account in the ordered word-se</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Chelba, C. &amp; Jelinek, F. 2000. Structured language modeling. Computer Speech &amp; Language, 14: 283-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Clarkson</author>
<author>A J Robinson</author>
</authors>
<title>Language model adaptation using mixtures and an exponentially decaying cache.</title>
<date>1997</date>
<booktitle>In Proc. ICASSP-97,</booktitle>
<pages>799--802</pages>
<contexts>
<context position="4541" citStr="Clarkson &amp; Robinson 1997" startWordPosition="684" endWordPosition="687">f the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics use POS or POS-like classes of the history-words for prediction. The structured language model (Chelba &amp; Jelinek 2000) determines the “heads” in the history-context by using a parsing tree. There are also works on skipping irrelevant history-words in order to reveal more informative ngrams (Siu &amp; Ostendorf 2000, Guthrie et al. 2006). Cache language models exploit temporal word frequencies in the history (Kuhn &amp; Mori 1990, Clarkson &amp; Robinson 1997). 3 Motivation of the Proposed Approach The attributes of distance and co-occurrence are exploited and modeled differently in each language modeling approach. In the n-gram model, for example, these two attributes are jointly taken into account in the ordered word-sequence. Consequently, the n-gram model can only be effectively implemented within a short history-context (e.g. of size of three or four). Both, the conventional trigger model and the latent-semantic model capture the co-occurrence information while ignoring the distance information. It is reasonable to assume that distance informa</context>
</contexts>
<marker>Clarkson, Robinson, 1997</marker>
<rawString>Clarkson, P.R. &amp; Robinson, A.J. 1997. Language model adaptation using mixtures and an exponentially decaying cache. In Proc. ICASSP-97, pp.799-802.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Coccaro</author>
</authors>
<title>Latent semantic analysis as a tool to improve automatic speech recognition performance. Doctoral Dissertation,</title>
<date>2005</date>
<institution>University of Colorado,</institution>
<location>Boulder, CO, USA.</location>
<contexts>
<context position="3476" citStr="Coccaro 2005" startWordPosition="524" endWordPosition="525">Each distance-k bigram model predicts the target-word based on the occurrence of a history-word located k positions behind. Zhou &amp; Lua (1998) enhanced the effectiveness of the model by filtering out those wordpairs exhibiting low correlation, so that only the well associated distant bigrams are retained. This approach is referred to as the distance-dependent trigger model, and is similar to the earlier proposed trigger model (Lau et al. 1993, Rosenfeld 1996) that relies on the bigrams of arbitrary distance, i.e. distance-independent. Latent-semantic language model approaches (Bellegarda 1998, Coccaro 2005) weight word counts with TFIDF to highlight their semantic importance towards the prediction. In this type of approach, count statistics are accumulated from long contexts, typically beyond ten to twenty words. In order to confine the complexity introduced by such long contexts, word ordering is ignored (i.e. bag-of-words paradigm). Other approaches such as the class-based language model (Brown 1992, Kneser &amp; Ney 1993) 233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computationa</context>
<context position="10227" citStr="Coccaro 2005" startWordPosition="1707" endWordPosition="1708">ability, and ii) a word-pair is not seen at any distance within the observation window, i.e. zero co-occurrence /(wi-k E h, t = wi) = 0, which results in a division by zero. For the first problem, we have attempted to redistribute the counts among the word-pairs at different distances (as observed within the window). We assumed that the counts of word-pairs are smooth in the distance domain and that the influence of a word decays as the distance increases. Accordingly, we used a weighted moving-average filter for performing the smoothing. Similar approaches have also been used in other works (Coccaro 2005, Lv &amp; Zhai 2009). Notice, however, that this strategy is different from other conventional smoothing techniques (Chen &amp; Goodman 1996), which rely mainly on the countof-count statistics for re-estimating and smoothing the original counts. For the second problem, when a word-pair was not seen at any distance (within the window), we arbitrarily assigned a small probability value, P(#= kIwi-k E h, t = wi) = 0101 , to provide a slight chance for such a word-pair (wi-k, wi) to occur at close distances. 4.3 Term-Occurrence Model Component During the decoupling operation (from Eq.2 to Eq.3), the TD m</context>
</contexts>
<marker>Coccaro, 2005</marker>
<rawString>Coccaro, N. 2005. Latent semantic analysis as a tool to improve automatic speech recognition performance. Doctoral Dissertation, University of Colorado, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Guthrie</author>
<author>B Allison</author>
<author>W Liu</author>
<author>L Guthrie</author>
<author>Y Wilks</author>
</authors>
<title>A closer look at skip-gram modelling.</title>
<date>2006</date>
<booktitle>In Proc. LREC-2006,</booktitle>
<pages>1222--1225</pages>
<contexts>
<context position="4424" citStr="Guthrie et al. 2006" startWordPosition="666" endWordPosition="669">digm). Other approaches such as the class-based language model (Brown 1992, Kneser &amp; Ney 1993) 233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics use POS or POS-like classes of the history-words for prediction. The structured language model (Chelba &amp; Jelinek 2000) determines the “heads” in the history-context by using a parsing tree. There are also works on skipping irrelevant history-words in order to reveal more informative ngrams (Siu &amp; Ostendorf 2000, Guthrie et al. 2006). Cache language models exploit temporal word frequencies in the history (Kuhn &amp; Mori 1990, Clarkson &amp; Robinson 1997). 3 Motivation of the Proposed Approach The attributes of distance and co-occurrence are exploited and modeled differently in each language modeling approach. In the n-gram model, for example, these two attributes are jointly taken into account in the ordered word-sequence. Consequently, the n-gram model can only be effectively implemented within a short history-context (e.g. of size of three or four). Both, the conventional trigger model and the latent-semantic model capture th</context>
</contexts>
<marker>Guthrie, Allison, Liu, Guthrie, Wilks, 2006</marker>
<rawString>Guthrie, D., Allison, B., Liu, W., Guthrie, L., &amp; Wilks, Y. 2006. A closer look at skip-gram modelling. In Proc. LREC-2006, pp.1222-1225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
</authors>
<title>The SPHINX-II speech recognition system: an overview.</title>
<date>1993</date>
<journal>Computer Speech and Language,</journal>
<volume>2</volume>
<pages>137--148</pages>
<marker>Huang, 1993</marker>
<rawString>Huang, X. et al. 1993. The SPHINX-II speech recognition system: an overview. Computer Speech and Language, 2: 137-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Trans. on Acoustics, Speech, &amp; Signal Processing,</journal>
<pages>35--400</pages>
<contexts>
<context position="14677" citStr="Katz 1987" startWordPosition="2478" endWordPosition="2479"> the TD, TO, and TD-TO models. As seen from the table, for lower order n-gram models, the complementary information captured by the TD and TO components reduced the perplexity up to 23.5% and 14.0%, for bigram and trigram models, respectively. Higher order ngram models, e.g. hexagram, observe historycontexts of similar lengths as the ones observed by the TD, TO, and TD-TO models. Due to the incapability of n-grams to model long historycontexts, the TD and TO components are still effective in helping to enhance the prediction. Similar results were obtained by using the standard back-off model (Katz 1987) as baseline. 5.2 Benefit of Decoupling Distant-Bigram In this second experiment, we examined whether the proposed decoupling procedure leads to better modeling of word-pairs compared to the distant bigram model. Here we compare the perplexity of both, the distance-k bigram model and distance-k TD model (for values of k ranging from two to ten), when combined with a standard bigram model. In order to make a fair comparison, without taking into account smoothing effects, we trained both models with raw counts and evaluated their perplexities over the train-set (so that no zeroprobability will b</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S.M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Trans. on Acoustics, Speech, &amp; Signal Processing, 35:400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klakow</author>
</authors>
<title>Log-linear interpolation of language model.</title>
<date>1998</date>
<booktitle>In Proc. ICSLP</booktitle>
<pages>1--4</pages>
<contexts>
<context position="8510" citStr="Klakow 1998" startWordPosition="1397" endWordPosition="1398">, and the TO likelihood, respectively. In Eq.3, we have decoupled the observation of a word-pair into the events of distance and cooccurrence. This allows for independently modeling and exploiting them. In order to control their contributions towards the final prediction of the target-word, we weight these components: P(t = with = ���� ������ p(t = W,)() � ��#� ������ � �� � � ��� ��� &amp; �(* &apos; �!� ( k!1 P(wi-k E hI t = wi)���)(+ AZ &amp;quot;(h) 234 where ,, ,- , and ,. are the weights for the prior, TD and TO models, respectively. Notice that the model depicted in Eq.4 is the log-linear interpolation (Klakow 1998) of these models. The prior, which is usually implemented as a unigram model, can be also replaced with a higher order n-gram model as, for instance, the bigram model: P(t = with = ���� ������ P(t = with = wi-i)() � ��#� ������ � �� � � ��� ��� &amp; �(* &apos; �!� ( k!1 P(wi-k E hl t = wi)���)(+ AZ &amp;quot;(h) Replacing the unigram model with a higher order n-gram model is important to compensate the damage incurred by the conditional independence assumption made earlier. 4.2 Term-Distance Model Component Basically, the TD likelihood measures how likely a given word-pair would be separated by a given distanc</context>
</contexts>
<marker>Klakow, 1998</marker>
<rawString>Klakow, D. 1998. Log-linear interpolation of language model. In Proc. ICSLP 1998, pp.1-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improving clustering techniques for class-based statistical language modeling.</title>
<date>1993</date>
<booktitle>In Proc. EUROSPEECH ’93,</booktitle>
<pages>973--976</pages>
<contexts>
<context position="3898" citStr="Kneser &amp; Ney 1993" startWordPosition="587" endWordPosition="590">igger model (Lau et al. 1993, Rosenfeld 1996) that relies on the bigrams of arbitrary distance, i.e. distance-independent. Latent-semantic language model approaches (Bellegarda 1998, Coccaro 2005) weight word counts with TFIDF to highlight their semantic importance towards the prediction. In this type of approach, count statistics are accumulated from long contexts, typically beyond ten to twenty words. In order to confine the complexity introduced by such long contexts, word ordering is ignored (i.e. bag-of-words paradigm). Other approaches such as the class-based language model (Brown 1992, Kneser &amp; Ney 1993) 233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics use POS or POS-like classes of the history-words for prediction. The structured language model (Chelba &amp; Jelinek 2000) determines the “heads” in the history-context by using a parsing tree. There are also works on skipping irrelevant history-words in order to reveal more informative ngrams (Siu &amp; Ostendorf 2000, Guthrie et al. 2006). Cache language models exploit temporal word frequencies in the history (</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>Kneser, R. &amp; Ney, H. 1993. Improving clustering techniques for class-based statistical language modeling. In Proc. EUROSPEECH ’93, pp.973-976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R D Mori</author>
</authors>
<title>A cache-based natural language model for speech recognition.</title>
<date>1990</date>
<journal>IEEE Trans. Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<issue>6</issue>
<pages>570--583</pages>
<contexts>
<context position="4514" citStr="Kuhn &amp; Mori 1990" startWordPosition="680" endWordPosition="683"> 233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics use POS or POS-like classes of the history-words for prediction. The structured language model (Chelba &amp; Jelinek 2000) determines the “heads” in the history-context by using a parsing tree. There are also works on skipping irrelevant history-words in order to reveal more informative ngrams (Siu &amp; Ostendorf 2000, Guthrie et al. 2006). Cache language models exploit temporal word frequencies in the history (Kuhn &amp; Mori 1990, Clarkson &amp; Robinson 1997). 3 Motivation of the Proposed Approach The attributes of distance and co-occurrence are exploited and modeled differently in each language modeling approach. In the n-gram model, for example, these two attributes are jointly taken into account in the ordered word-sequence. Consequently, the n-gram model can only be effectively implemented within a short history-context (e.g. of size of three or four). Both, the conventional trigger model and the latent-semantic model capture the co-occurrence information while ignoring the distance information. It is reasonable to a</context>
</contexts>
<marker>Kuhn, Mori, 1990</marker>
<rawString>Kuhn, R. &amp; Mori, R.D. 1990. A cache-based natural language model for speech recognition. IEEE Trans. Pattern Analysis and Machine Intelligence, 12(6): 570-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
</authors>
<title>Trigger-based language models: a maximum-entropy approach.</title>
<date>1993</date>
<booktitle>In Proc. ICASSP-94,</booktitle>
<pages>45--48</pages>
<marker>Lau, 1993</marker>
<rawString>Lau, R. et al. 1993. Trigger-based language models: a maximum-entropy approach. In Proc. ICASSP-94, pp.45-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lv</author>
<author>C Zhai</author>
</authors>
<title>Positional language models for information retrieval.</title>
<date>2009</date>
<booktitle>In Proc. SIGIR’09,</booktitle>
<pages>299--306</pages>
<contexts>
<context position="10244" citStr="Lv &amp; Zhai 2009" startWordPosition="1709" endWordPosition="1712">i) a word-pair is not seen at any distance within the observation window, i.e. zero co-occurrence /(wi-k E h, t = wi) = 0, which results in a division by zero. For the first problem, we have attempted to redistribute the counts among the word-pairs at different distances (as observed within the window). We assumed that the counts of word-pairs are smooth in the distance domain and that the influence of a word decays as the distance increases. Accordingly, we used a weighted moving-average filter for performing the smoothing. Similar approaches have also been used in other works (Coccaro 2005, Lv &amp; Zhai 2009). Notice, however, that this strategy is different from other conventional smoothing techniques (Chen &amp; Goodman 1996), which rely mainly on the countof-count statistics for re-estimating and smoothing the original counts. For the second problem, when a word-pair was not seen at any distance (within the window), we arbitrarily assigned a small probability value, P(#= kIwi-k E h, t = wi) = 0101 , to provide a slight chance for such a word-pair (wi-k, wi) to occur at close distances. 4.3 Term-Occurrence Model Component During the decoupling operation (from Eq.2 to Eq.3), the TD model held only th</context>
</contexts>
<marker>Lv, Zhai, 2009</marker>
<rawString>Lv Y. &amp; Zhai C. 2009. Positional language models for information retrieval. In Proc. SIGIR’09, pp.299-306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modelling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<volume>10</volume>
<pages>187--228</pages>
<contexts>
<context position="3325" citStr="Rosenfeld 1996" startWordPosition="505" endWordPosition="506">7, Brun et al. 2007) disassembles the n-gram into (n−1) word-pairs, such that each pair is modeled by a distance-k bigram model, where 1 &lt;— k &lt;— n — 1 . Each distance-k bigram model predicts the target-word based on the occurrence of a history-word located k positions behind. Zhou &amp; Lua (1998) enhanced the effectiveness of the model by filtering out those wordpairs exhibiting low correlation, so that only the well associated distant bigrams are retained. This approach is referred to as the distance-dependent trigger model, and is similar to the earlier proposed trigger model (Lau et al. 1993, Rosenfeld 1996) that relies on the bigrams of arbitrary distance, i.e. distance-independent. Latent-semantic language model approaches (Bellegarda 1998, Coccaro 2005) weight word counts with TFIDF to highlight their semantic importance towards the prediction. In this type of approach, count statistics are accumulated from long contexts, typically beyond ten to twenty words. In order to confine the complexity introduced by such long contexts, word ordering is ignored (i.e. bag-of-words paradigm). Other approaches such as the class-based language model (Brown 1992, Kneser &amp; Ney 1993) 233 Proceedings of the 51s</context>
<context position="11316" citStr="Rosenfeld 1996" startWordPosition="1888" endWordPosition="1889">to occur at close distances. 4.3 Term-Occurrence Model Component During the decoupling operation (from Eq.2 to Eq.3), the TD model held only the distance information while the count information has been ignored. Notice the normalization of word-pair counts in Eq.6. As a complement to the TD model, the TO model focuses on co-occurrence, and holds only count information. As the distance information is captured by the TD model, the co-occurrence count captured by the TO model is independent from the given word-pair distance. In fact, the TO model is closely related to the trigger language model (Rosenfeld 1996), as the prediction of the target-word (the triggered word) is based on the presence of a history-word (the trigger). However, differently from the trigger model, the TO model considers all the wordpairs without filtering out the weak associated ones. Additionally, the TO model takes into account multiple co-occurrences of the same history-word within the window, while the trigger model would count them only once (i.e. considers binary counts). The word-pairs that frequently co-occur at arbitrary distances (within an observation window) would favor the TO likelihood. It can be estimated from c</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Rosenfeld, R. 1996. A maximum entropy approach to adaptive statistical language modelling. Computer Speech and Language, 10: 187-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simons</author>
<author>H Ney</author>
<author>S C Martin</author>
</authors>
<title>Distant bigram language modelling using maximum entropy.</title>
<date>1997</date>
<booktitle>In Proc. ICASSP-97,</booktitle>
<pages>787--790</pages>
<marker>Simons, Ney, Martin, 1997</marker>
<rawString>Simons, M., Ney, H. &amp; Martin S.C. 1997. Distant bigram language modelling using maximum entropy. In Proc. ICASSP-97, pp.787-790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Siu</author>
<author>M Ostendorf</author>
</authors>
<title>Variable n-grams and extensions for conversational speech language modeling.</title>
<date>2000</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>63--75</pages>
<contexts>
<context position="4402" citStr="Siu &amp; Ostendorf 2000" startWordPosition="662" endWordPosition="665">i.e. bag-of-words paradigm). Other approaches such as the class-based language model (Brown 1992, Kneser &amp; Ney 1993) 233 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics use POS or POS-like classes of the history-words for prediction. The structured language model (Chelba &amp; Jelinek 2000) determines the “heads” in the history-context by using a parsing tree. There are also works on skipping irrelevant history-words in order to reveal more informative ngrams (Siu &amp; Ostendorf 2000, Guthrie et al. 2006). Cache language models exploit temporal word frequencies in the history (Kuhn &amp; Mori 1990, Clarkson &amp; Robinson 1997). 3 Motivation of the Proposed Approach The attributes of distance and co-occurrence are exploited and modeled differently in each language modeling approach. In the n-gram model, for example, these two attributes are jointly taken into account in the ordered word-sequence. Consequently, the n-gram model can only be effectively implemented within a short history-context (e.g. of size of three or four). Both, the conventional trigger model and the latent-sem</context>
</contexts>
<marker>Siu, Ostendorf, 2000</marker>
<rawString>Siu, M. &amp; Ostendorf, M. 2000. Variable n-grams and extensions for conversational speech language modeling. IEEE Trans. on Speech and Audio Processing, 8(1): 63-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>K T Lua</author>
</authors>
<title>Word association and MItrigger-based language modeling.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>1465--1471</pages>
<contexts>
<context position="3004" citStr="Zhou &amp; Lua (1998)" startWordPosition="451" endWordPosition="454">duces and motivates our proposed approach. Section 4 presents in detail the derivation of both TD and TO model components. Section 5 presents some perplexity evaluation results. Finally, section 6 presents our conclusions and proposed future work. 2 Related Work The distant bigram model (Huang et.al 1993, Simon et al. 1997, Brun et al. 2007) disassembles the n-gram into (n−1) word-pairs, such that each pair is modeled by a distance-k bigram model, where 1 &lt;— k &lt;— n — 1 . Each distance-k bigram model predicts the target-word based on the occurrence of a history-word located k positions behind. Zhou &amp; Lua (1998) enhanced the effectiveness of the model by filtering out those wordpairs exhibiting low correlation, so that only the well associated distant bigrams are retained. This approach is referred to as the distance-dependent trigger model, and is similar to the earlier proposed trigger model (Lau et al. 1993, Rosenfeld 1996) that relies on the bigrams of arbitrary distance, i.e. distance-independent. Latent-semantic language model approaches (Bellegarda 1998, Coccaro 2005) weight word counts with TFIDF to highlight their semantic importance towards the prediction. In this type of approach, count st</context>
</contexts>
<marker>Zhou, Lua, 1998</marker>
<rawString>Zhou G. &amp; Lua K.T. 1998. Word association and MItrigger-based language modeling. In Proc. COLING-ACL, 1465-1471.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>