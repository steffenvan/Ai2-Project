<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001423">
<title confidence="0.999251">
User Requirements Analysis for Meeting Information Retrieval
Based on Query Elicitation
</title>
<author confidence="0.998614">
Vincenzo Pallotta
</author>
<affiliation confidence="0.890657">
Department of Computer Science
University of Fribourg
Switzerland
</affiliation>
<email confidence="0.986199">
Vincenzo.Pallotta@unifr.ch
</email>
<author confidence="0.988713">
Violeta Seretan
</author>
<affiliation confidence="0.890339333333333">
Language Technology Laboratory
University of Geneva
Switzerland
</affiliation>
<email confidence="0.992942">
seretan@lettres.unige.ch
</email>
<author confidence="0.983011">
Marita Ailomaa
</author>
<affiliation confidence="0.961577333333333">
Artificial Intelligence Laboratory
Ecole Polytechnique Fédérale
de Lausanne (EPFL), Switzerland
</affiliation>
<email confidence="0.987705">
Marita.Ailomaa@epfl.ch
</email>
<sectionHeader confidence="0.995436" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998306">
We present a user requirements study for
Question Answering on meeting records
that assesses the difficulty of users ques-
tions in terms of what type of knowledge is
required in order to provide the correct an-
swer. We grounded our work on the em-
pirical analysis of elicited user queries. We
found that the majority of elicited queries
(around 60%) pertain to argumentative
processes and outcomes. Our analysis also
suggests that standard keyword-based In-
formation Retrieval can only deal success-
fully with less than 20% of the queries, and
that it must be complemented with other
types of metadata and inference.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955037037037">
Meeting records constitute a particularly important
and rich source of information. Meetings are a
frequent and sustained activity, in which multi-
party dialogues take place that are goal-oriented
and where participants perform a series of actions,
usually aimed at reaching a common goal: they
exchange information, raise issues, express
opinions, make suggestions, propose solutions,
provide arguments (pro or con), negotiate
alternatives, and make decisions. As outcomes of
the meeting, agreements on future action items are
reached, tasks are assigned, conflicts are solved,
etc. Meeting outcomes have a direct impact on the
efficiency of organization and team performance,
and the stored and indexed meeting records serve
as reference for further processing (Post et al.,
2004). They can also be used in future meetings in
order to facilitate the decision-making process by
accessing relevant information from previous
meetings (Cremers et al., 2005), or in order to
make the discussion more focused (Conklin, 2006).
Meetings constitute a substantial and important
source of information that improves corporate or-
ganization and performance (Corrall, 1998; Ro-
mano and Nunamaker, 2001). Novel multimedia
techniques have been dedicated to meeting record-
ing, structuring and content analysis according to
the metadata schema, and finally, to accessing the
analyzed content via browsing, querying or filter-
ing (Cremers et al., 2005; Tucker and Whittaker,
2004).
This paper focuses on debate meetings (Cugini
et al., 1997) because of their particular richness in
information concerning the decision-making proc-
ess. We consider that the meeting content can be
organized on three levels: (i) factual level (what
happens: events, timeline, actions, dynamics); (ii)
thematic level (what is said: topics discussed and
details); (iii) argumentative level (which/how com-
mon goals are reached).
The information on the first two levels is ex-
plicit information that can be usually retrieved di-
rectly by searching the meeting records with ap-
propriate IR techniques (i.e., TF-IDF). The third
level, on the contrary, contains more abstract and
tacit information pertaining to how the explicit in-
formation contributes to the rationale of the meet-
ing, and it is not present as such in raw meeting
data: whether or not the meeting goal was reached,
what issues were debated, what proposals were
made, what alternatives were discussed, what ar-
guments were brought, what decisions were made,
what task were assigned, etc.
The motivating scenario is the following: A user
</bodyText>
<page confidence="0.94715">
1008
</page>
<note confidence="0.9249005">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1008–1015,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997856984375">
needs information about a past meeting, either in cussions is of central importance for building pro-
quality of a participant who wants to recollect a ject memories since, in addition to the &amp;quot;strictly fac-
discussion (since the memories of co-participants tual, technical information&amp;quot;, these memories must
are often inconsistent, cf. Banerjee et al., 2005), or also store relevant information about deci-
as a non-participant who missed that meeting. sion-making processes. In a business context, the
Instead of consulting the entire meeting-related information derived from meetings is useful for
information, which is usually heterogeneous and future business processes, as it can explain phe-
scaterred (audio-video recordings, notes, minutes, nomena and past decisions and can support future
e-mails, handouts, etc.), the user asks natural actions by mining and assessment (Pallotta et al.,
language questions to a query engine which 2004). The argumentative structure of meeting dis-
retrieves relevant information from the meeting cussions, possibly visualized in form of argumen-
records. tation diagrams or maps, can be helpful in meeting
In this paper we assess the users&apos; interest in browsing. To our knowledge, there are at least
retrieving argumentative information from three meeting browsers that have adopted argu-
meetings and what kind of knowledge is required mentative structure: ARCHIVUS (Lisowska et al.,
for answering users&apos; queries. Section 2 reviews 2004b), ViCoDe (Marchand-Maillet and Bruno,
previous user requirements studies for the meeting 2005), and the Twente-AMI JFerret browser
domain. Section 3 describes our user requirements (Rienks and Verbree, 2006).
study based on the analysis of elicited user queries,
presents its main findings, and discusses the
implications of these findings for the design of
meeting retrieval systems. Section 4 concludes the
paper and outlines some directions for future work.
2 Argumentative Information in Meeting
Information Retrieval
Depending on the meeting browser type1, different
levels of meeting content become accessible for
information retrieval. Audio and video browsers
deal with factual and thematic information, while
artifact browsers might also touch on deliberative
information, as long as it is present, for instance, in
the meeting minutes. In contrast, derived-data
browsers aim to account for the argumentative in-
formation which is not explicitly present in the
meeting content, but can be inferred from it. If
minutes are likely to contain only the most salient
deliberative facts, the derived-data browsers are
much more useful, in that they offer access to the
full meeting record, and thus to relevant details
about the deliberative information sought.
2.1 Importance of Argumentative Structure
As shown by Rosemberg and Silince (1999), track-
ing argumentative information from meeting dis-
2.2 Query Elicitation Studies
The users&apos; interest in argumentation dimension of
meetings has been highlighted by a series of recent
studies that attempted to elicit the potential user
questions about meetings (Lisowska et al., 2004a;
Benerjee at al., 2005; Cremers et al., 2005).
The study of Lisowska et al. (2004a), part of the
IM2 research project2, was performed in a simu-
lated environment in which users were asked to
imagine themselves in a particular role from a se-
ries of scenarios. The participants were both IM2
members and non-IM2 members and produced
about 300 retrospective queries on recorded meet-
ings. Although this study has been criticized by
Post et al. (2004), Cremers et al. (2005), and Ban-
erjee et al. (2005) for being biased, artificial, ob-
trusive, and not conforming to strong HCI method-
ologies for survey research, it shed light on poten-
tial queries and classified them in two broad cate-
gories, that seem to correspond to our argumenta-
tive/non-argumentative distinction (Lisowska et
</bodyText>
<listItem confidence="0.814391944444445">
al., 2004a: 994):
• “elements related to the interaction among par-
ticipants: acceptance/rejection, agree-
ment/disagreement; proposal, argumentation
(for and against); assertions, statements; deci-
sions; discussions, debates; reactions; ques-
tions; solutions”;
1 (Tucker and Whittaker, 2004) identifies 4 types of meeting
browsers: audio browsers, video browsers, artifacts browsers
(that exploit meeting minutes or other meeting-related docu-
ments), and browsers that work with derived data (such as
discourse and temporal structure information).
2 http://www.im2.ch
1009
• “concepts from the meeting domains: dates,
times; documents; meeting index: current, pre-
vious, sets; participants; presentations, talks;
projects; tasks, responsibilities; topics”.
</listItem>
<bodyText confidence="0.999838309090909">
Unfortunately, the study does not provide precise
information on the relative proportions of queries
for the classification proposed, but simply suggests
that overall more queries belong to the second
category, while queries requiring understanding of
the dialogue structure still comprise a sizeable
proportion.
The survey conducted by Banerjee et al. (2005)
concerned instead real, non-simulated interviews
of busy professionals about actual situations, re-
lated either to meetings in which they previously
participated, or to meetings they missed. More than
half of the information sought by interviewees
concerned, in both cases, the argumentative dimen-
sion of meetings.
For non-missed meetings, 15 out of the 26 in-
stances (i.e., 57.7%) concerned argumentative as-
pects: what the decision was regarding a topic (7);
what task someone was assigned (4); who made a
particular decision (2); what was the participants&apos;
reaction to a particular topic (1); what the future
plan is (1). The other instances (42.3%) relate to
the thematic dimension, i.e., specifics of the dis-
cussion on a topic (11).
As for missed meetings, the argumentative in-
stances were equally represented (18/36): decisions
on a topic (7); what task was assigned to inter-
viewee (4); whether a particular decision was made
(3); what decisions were made (2); reasons for a
decision (1); reactions to a topic (1). The thematic
questions concern topics discussed, announce-
ments made, and background of participants.
The study also showed that the recovery of in-
formation from meeting recordings is significantly
faster when discourse annotations are available,
such as the distinction between discussion, presen-
tation, and briefing.
Another unobtrusive user requirements study
was performed by Cremers et al. (2005) in a &amp;quot;semi-
natural setting&amp;quot; related to the design of a meeting
browser. The top 5 search interests highlighted by
the 60 survey participants were: decisions made,
participants/speakers, topics, agenda items, and
arguments for decision. Of these, the ones shown
in italics are argumentative. In fact, the authors
acknowledge the necessity to include some &amp;quot;func-
tional&amp;quot; categories as innovative search options.
Interestingly, from the user interface evaluation
presented in their paper, one can indirectly infer
how salient the argumentative information is per-
ceived by users: the icons that the authors intended
for emotions, i.e., for a emotion-based search facil-
ity, were actually interpreted by users as referring
to people’s opinion: What is person X&apos;s opinion? –
positive, negative, neutral.
</bodyText>
<sectionHeader confidence="0.979785" genericHeader="method">
3 User Requirements Analysis
</sectionHeader>
<bodyText confidence="0.999051357142857">
The existing query elicitation experiments reported
in Section 2 highlighted a series of question types
that users typically would like to ask about meet-
ings. It also revealed that the information sought
can be classified into two broad categories: argu-
mentative information (about the argumentative
process and the outcome of debate meetings), and
non-argumentative information (factual, i.e., about
the meeting as a physical event, or thematic, i.e.,
about what has been said in terms of topics).
The study we present in this section is aimed at
assessing how difficult it is to answer the questions
that users typically ask about a meeting. Our goal
is to provide insights into:
</bodyText>
<listItem confidence="0.974039285714286">
• how many queries can be answered using stan-
dard IR techniques on meeting artefacts only
(e.g., minutes, written agenda, invitations);
• how many queries can be answered with IR on
meeting recordings;
• what kind of additional information and infer-
ence is needed when IR does not apply or it is
</listItem>
<bodyText confidence="0.956632375">
insufficient (e.g., information about the par-
ticipants and the meeting dynamics, external
information about the meeting’s context such
as the relation to a project, semantic interpreta-
tion of question terms and references, compu-
tation of durations, aggregation of results, etc).
Assessing the level of difficulty of a query based
on the two above-mentioned categories might not
provide insightful results, because these would be
too general, thus less interpretable. Also, the com-
plex queries requiring mixed information would
escape observation because assigned to a too gen-
eral class. We therefore considered it necessary to
perform a separate analysis of each query instance,
as this provides not only detailed, but also trace-
able information.
</bodyText>
<page confidence="0.952974">
1010
</page>
<subsectionHeader confidence="0.990863">
3.1 Data: Collecting User Queries
</subsectionHeader>
<bodyText confidence="0.994403222222222">
Our analysis is based on a heterogeneous collec-
tion of queries for meeting data. In general, an un-
biased queries dataset is difficult to obtain, and the
quality of a dataset can vary if the sample is made
of too homogenous subjects (e.g., people belong-
ing to the same group as members of the same pro-
ject). In order to cope with this problem, our strat-
egy was to use three different datasets collected in
different settings:
</bodyText>
<listItem confidence="0.998763444444444">
• First, we considered the IM2 dataset collected
by Lisowska et al. (2004a), the only set of user
queries on meetings available to date. It com-
prises 270 questions (shortly described in Sec-
tion 2) annotated with a label showing whether
or not the query was produced by an IM2-
member. These queries are introspective and
not related to any particular recorded meeting.
• Second, we cross-validated this dataset with a
</listItem>
<bodyText confidence="0.945683769230769">
large corpus of 294 natural language state-
ments about existing meetings records. This
dataset, called the BET observations (Wellner
et al., 2005), was collected by subjects who
were asked to watch several meeting record-
ings and to report what the meeting partici-
pants appeared to consider interesting. We use
it as a ‘validation’ set for the IM2 queries: an
IM2 query is considered as ‘realistic’ or ‘em-
pirically grounded’ if there is a BET observa-
tion that represents a possible answer to the
query. For instance, the query Why was the
proposal made by X not accepted? matches the
BET observation Denis eliminated Silence of
the Lambs as it was too violent.
• Finally, we collected a new set of ‘real’ queries
by conducting a survey of user requirements
on meeting querying in a natural business set-
ting. The survey involved 3 top managers from
a company and produced 35 queries. We called
this dataset Manager Survey Set (MS-Set).
The queries from the IM2-set (270 queries) and the
MS-Set (35 queries) were analyzed by two differ-
ent teams of two judges. Each team discussed each
query, and classified it along the two main dimen-
sions we are interested in:
</bodyText>
<listItem confidence="0.9991645">
• query type: the type of meeting content to
which the query pertains;
• query difficulty: the type of information re-
quired to provide the answer.
</listItem>
<subsectionHeader confidence="0.997646">
3.2 Query Type Analysis
</subsectionHeader>
<bodyText confidence="0.998696">
Each query was assigned exactly one of the follow-
ing four possible categories (the one perceived as
the most salient):
</bodyText>
<listItem confidence="0.999428">
1. factual: the query pertains to the factual meet-
ing content;
2. thematic: the query pertains to the thematic
meeting content;
3. process: the query pertains to the argumenta-
tive meeting content, more precisely to the ar-
gumentative process;
4. outcome: the query pertains to the argumenta-
tive meeting content, more precisely to the
outcome of the argumentative process.
</listItem>
<table confidence="0.999342625">
Category IM2-set MS-Set
(size:270) (size: 35)
Team1 Team2 Team1 Team2
Factual 24.8% 45.6% 20.0% 20.0%
Thematic 18.5% 20.0% 11.4%
Process 30.0% 32.6% 22.9% 28.6%
Outcome 26.7% 21.8% 37.1% 40.0%
Process+ Outcome 56.7% 54.4% 60.0% 68.6%
</table>
<tableCaption confidence="0.978621">
Table 1. Query classification according to the
meeting content type.
</tableCaption>
<bodyText confidence="0.998663565217391">
Results from this classification task for both query
sets are reported in Table 1. In both sets, the
information most sought was argumentative: about
55% of the IM2-set queries are argumentative
(process or outcome). This invalidates the initial
estimation of Lisowska et al. (2004a:994) that the
non-argumentative queries prevail, and confirms
the figures obtained in (Banerjee et al., 2005), ac-
cording to which 57.7% of the queries are argu-
mentative. In our real managers survey, we ob-
tained even higher percentages for the argumenta-
tive queries (60% or 68.6%, depending on the an-
notation team). The argumentative queries are fol-
lowed by factual and thematic ones in both query
sets, with a slight advantage for factual queries.
The inter-annotator agreement for this first clas-
sification is reported in Table 2. The proportion of
queries on which annotators agree in classifying
them as argumentative is significantly high. We
only report here the agreement results for the indi-
vidual argumentative categories (Process, Out-
come) and both (Process &amp; Outcome). There were
213 queries (in IM2-set) and 30 queries (in MS-
</bodyText>
<page confidence="0.97405">
1011
</page>
<bodyText confidence="0.9942964">
set) that were consistently annotated by the two
teams on both categories. Within this set, a high
percentage of queries were argumentative, that is,
they were annotated as either Process or Outcome
(label AA in the table).
</bodyText>
<table confidence="0.999840375">
Category IM2-set (size: 270) MS-set (size: 35)
ratio kappa ratio kappa
Process 84.8% 82.9% 88.6% 87.8%
Outcome 90.7% 89.6% 91.4% 90.9%
Process &amp; 78.9% 76.2% 85.7% 84.8%
Outcome
AA 117/213 = 19/30 =
54.9% 63.3%
</table>
<tableCaption confidence="0.9200975">
Table 2. Inter-annotator agreement for query-type
classification.
</tableCaption>
<bodyText confidence="0.999940133333333">
Furthermore, we provided a re-assessment of the
proportion of argumentative queries with respect to
query origin for the IM2-set (IM2 members vs.
non-IM2 members): non-IM2 members issued
30.8% of agreed argumentative queries, a propor-
tion that, while smaller compared to that of IM2
members (69.2%), is still non-negligible. This con-
trasts with the opinion expressed in (Lisowska et
al., 2004a) that argumentative queries are almost
exclusively produced by IM2 members.
Among the 90 agreed IM2 queries that were
cross-validated with the BET-observation set,
28.9% were argumentative. We also noted that the
ratio of BET statements that contain argumentative
information is quite high (66.9%).
</bodyText>
<subsectionHeader confidence="0.998499">
3.3 Query Difficulty Analysis
</subsectionHeader>
<bodyText confidence="0.9973404">
In order to assess the difficulty in answering a
query, we used the following categories that the
annotators could assign to each query, according to
the type of information and techniques they judged
necessary for answering it:
</bodyText>
<listItem confidence="0.94734025">
1. Role of IR: states the role of standard3 Informa-
tion Retrieval (in combination with Topic Ex-
traction4) techniques in answering the query.
Possible values:
a. Irrelevant (IR techniques are not appli-
cable). Example: What decisions have
been made?
3 By standard IR we mean techniques based on bag-of-word
search and TF-IDF indexing.
4 Topic extraction techniques are based on topic shift detec-
tion (Galley et al., 2003) and keyword extraction (van der Plas
et al., 2004).
b. successful (IR techniques are sufficient).
Example: Was the budget approved?
c. insufficient (IR techniques are necessary,
but not sufficient alone since they re-
quire additional inference and informa-
tion, such as argumentative, cross-
meeting, external corporate/project
knowledge). Example: Who rejected the
proposal made by X on issue Y?
2. Artefacts: information such as agenda, min-
utes of previous meetings, e-mails, invita-
tions and other documents related and avail-
able before the meeting. Example: Who was
invited to the meeting?
3. Recordings: the meeting recordings (audio,
visual, transcription). This is almost always
true, except for queries where Artefacts or
Metadata are sufficient, such as What was
the agenda?, Who was invited to the meet-
ing?).
4. Metadata: context knowledge kept in static
metadata (e.g., speakers, place, time). Ex-
ample: Who were the participants at the
meeting?
5. Dialogue Acts &amp; Adjacency Pairs: Example:
What was John’s response to my comment
on the last meeting?
6. Argumentation: metadata (annotations)
about the argumentative structure of the
meeting content. Example: Did everybody
agree on the decisions, or were there differ-
ences of opinion?
7. Semantics: semantic interpretation of terms
in the query and reference resolution, in-
cluding deictics (e.g., for how long, usually,
systematically, criticisms; this, about me, I).
Example: What decisions got made easily?
The term requiring semantic interpretation is
underlined.
8. Inference: inference (deriving information
that is implicit), calculation, and aggregation
(e.g., for ‘command’ queries asking for lists
of things – participants, issues, proposals).
Example: What would be required from me?
</listItem>
<page confidence="0.913412">
1012
</page>
<listItem confidence="0.95560275">
9. Multiple meetings: availability of multiple
meeting records. Example: Who usually at-
tends the project meetings?
10. External: related knowledge, not explicitly
</listItem>
<bodyText confidence="0.921638461538462">
present in the meeting records (e.g., infor-
mation about the corporation or the projects
related to the meeting). Example: Did some-
body talk about me or about my work?
Results of annotation reported on the two query
sets are synthesized in Table 3: IR is sufficient for
answering 14.4% of the IM2 queries, and 20% of
the MS-set queries. In 50% and 25.7% of the cases,
respectively, it simply cannot be applied (irrele-
vant). Finally, IR alone is not enough in 35.6% of
the queries from the IM2-set, and in 54.3% of the
MS-set; it has to be complemented with other
techniques.
</bodyText>
<table confidence="0.999403111111111">
IR is: IM2-set MS-set
all AA all AA
queries queries
Sufficient 39/270 = 1/117 = 7/35 = 1/19 =
14.4% 0.8% 20.0% 5.3%
Irrelevant 135/270 = 55/117 = 9/35 = 3/19 =
50.0% 47.0% 25.7% 15.8%
Insufficient 96/270 = 61/117 = 19/35 = 15/19 =
35.6% 52.1% 54.3% 78.9%
</table>
<tableCaption confidence="0.9848375">
Table 3. The role of IR (and topic extraction) in
answering users’ queries.
</tableCaption>
<bodyText confidence="0.990042086956522">
If we consider agreed argumentative queries
(Section 3.2), IR is effective in an extremely low
percentage of cases (0.8% for IM2-set and 5.3%
for MS-Set). IR is insufficient in most of the cases
(52.1% and 78.9%) and inapplicable in the rest of
the cases (47% and 15.8%). Only one argumenta-
tive query from each set was judged as being an-
swerable with IR alone: What were the decisions to
be made (open questions) regarding the topic t1?
When is the NEXT MEETING planned? (e.g. to
follow up on action items).
Table 4 shows the number of queries in each set
that require argumentative information in order to
be answered, distributed according to the query
types. As expected, no argumentation information
is necessary for answering factual queries, but
some thematic queries do need it, such as What
was decided about topic T? (24% in the IM2-set
and 42.9% in the M.S.-set).
Overall, the majority of queries in both sets re-
quire argumentation information in order to be an-
swered (56.3% from IM2 queries, and 65.7% from
MS queries).
</bodyText>
<table confidence="0.99966025">
Category IM2-set, Annotation 1 MS-set, Annotation 1
total Req. Ratio Total Req. Ratio
arg. arg.
Factual 67 0 0% 7 0 0%
Thematic 50 12 24.0% 7 3 42.9%
Process 81 73 90.1% 8 7 87.5%
Outcome 72 67 93.1% 13 13 100%
All 270 152 56.3% 35 23 65.7%
</table>
<tableCaption confidence="0.861661">
Table 4. Queries requiring argumentative informa-
tion.
</tableCaption>
<bodyText confidence="0.9998454">
We finally looked at what kind of information is
needed in those cases where IR is perceived as in-
sufficient or irrelevant. Table 5 lists the most fre-
quent combinations of information types required
for the IM2-set and the MS-set.
</bodyText>
<subsectionHeader confidence="0.997758">
3.4 Summary of Findings
</subsectionHeader>
<bodyText confidence="0.983485">
The analysis of the annotations obtained for the
305 queries (35 from the Manager Survey set, and
270 from the IM2-set) revealed that:
</bodyText>
<listItem confidence="0.99690696">
• The information most sought by users from
meetings is argumentative (i.e., pertains to the
argumentative process and its outcome). It
constitutes more than half of the total queries,
while factual and thematic information are
similar in proportions (Table 1);
• There was no significant difference in this re-
spect between the IM2-set and the MS-set
(Table 1);
• The decision as to whether a query is argumen-
tative or not is easy to draw, as suggested by
the high inter-annotator agreement shown in
Table 2;
• Standard IR and topic extraction techniques
are perceived as insufficient in answering most
of the queries. Only less than 20% of the
whole query set can be answered with IR, and
almost no argumentative question (Table 3).
• Argumentative information is needed in an-
swering the majority of the queries (Table 4);
• When IR alone fails, the information types that
are needed most are (in addition to recordings):
Argumentation, Semantics, Inference, and
Metadata (Table 5); see Section 3.3 for their
description.
</listItem>
<page confidence="0.837151">
1013
</page>
<table confidence="0.999850730769231">
IR alone fails IM2-set
Information types IR insufficient 96 cases 35.6% IR irrelevant 135 cases 50%
Artefacts x
Recordings x x x x x x x x x x x
Meta-data x x x x x x
Dlg acts &amp; Adj. pairs
Argumentation x x x x x x x x x x
Semantics x x x x x x x x x x
Inference x x x x x x x x x
Multiple meetings x x
External
Cases 15 11 9 8 7 5 4 14 9 8 8 7 5
Ratio (%) 15.6 11.5 9.4 8.3 7.3 5.2 4.2 10.4 6.7 5.9 5.9 5.2 3.7
IR alone fails MS-set
Information types IR insufficient 19 cases 54.3% IR irrelevant 9 cases 54.3%
Artefacts x x
Recordings x x x x
Meta-data x x
Dlg acts &amp; Adj. pairs
Argumentation x x x x
Semantics x x x x
Inference x x x x
Multiple meetings
External x
Cases 6 4 2 2 2 2
Ratio (%) 31.6 21 10.5 10.5 22.2 22.2
</table>
<tableCaption confidence="0.950864">
Table 5. Some of the most frequent combinations of information required for answering the queries in the
IM2-Set and in the MS-set when IR alone fails.
</tableCaption>
<sectionHeader confidence="0.677914" genericHeader="method">
3.5 Discussion
</sectionHeader>
<bodyText confidence="0.999926724137931">
Searching relevant information through the re-
corded meeting dialogues poses important prob-
lems when using standard IR indexing techniques
(Baeza-Yates and Ribeiro-Nieto, 2000), because
users ask different types of queries for which a
single retrieval strategy (e.g., keywords-based) is
insufficient. This is the case when looking at an-
swers that require some sort of entailment, such as
inferring that a proposal has been rejected when a
meeting participant says Are you kidding?.
Spoken-language information retrieval (Vinci-
arelli, 2004) and automatic dialogue-act extraction
techniques (Stolke et al., 2000; Clark and Popescu-
Belis, 2004; Ang et al., 2005) have been applied to
meeting recordings and produced good results un-
der the assumption that the user is interested in
retrieving either topic-based or dialog act-based
information. But this assumption is partially in-
validated by our user query elicitation analysis,
which showed that such information is only sought
in a relatively small fraction of the users’ queries.
A particular problem for these approaches is that
the topic looked for is usually not a query itself
(Was topic T mentioned?), but just a parameter in
more structured questions (What was decided
about T?). Moreover, the relevant participants’
contributions (dialog acts) need to be retrieved in
combination, not in isolation (The reactions to the
proposal made by X).
</bodyText>
<sectionHeader confidence="0.995607" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999928055555556">
While most of the research community has ne-
glected the importance of argumentative queries in
meeting information retrieval, we provided evi-
dence that this type of queries is actually very
common. We quantified the proportion of queries
involving the argumentative dimension of the
meeting content by performing an in-depth analy-
sis of queries collected in two different elicitation
surveys. The analysis of the annotations obtained
for the 305 queries (270 from the IM2-set, 35 from
MS-set) was aimed at providing insights into dif-
ferent matters: what type of information is typi-
cally sought by users from meetings; how difficult
it is, and what kind of information and techniques
are needed in order to answer user queries.
This work represents an initial step towards a
better understanding of user queries on the meeting
domain. It could provide useful intuitions about
</bodyText>
<page confidence="0.987098">
1014
</page>
<bodyText confidence="0.9999618">
how to perform the automatic classification of an-
swer types and, more importantly, the automatic
extraction of argumentative features and their rela-
tions with other components of the query (e.g.,
topic, named entities, events).
In the future, we intend to better ground our first
empirical findings by i) running the queries against
a real IR system with indexed meeting transcripts
and evaluate the quality of the obtained answers;
ii) ask judges to manually rank the difficulty of
each query, and iii) compare the two rankings. We
would also like to see how frequent argumentative
queries are in other domains (such as TV talk
shows or political debates) in order to generalize
our results.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999726">
We wish to thank Martin Rajman and Hatem
Ghorbel for their constant and valuable feedback.
This work has been partially supported by the
Swiss National Science Foundation NCCR IM2
and by the SNSF grant no. 200021-116235.
</bodyText>
<sectionHeader confidence="0.99911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999966781609195">
Jeremy Ang, Yang Liu and Elizabeth Shriberg. 2005.
Automatic Dialog Act Segmentation and Classification in
Multiparty Meetings. In Proceedings of IEEE ICASSP
2005, Philadelphia, PA, USA.
Ricardo Baeza-Yates and Berthier Ribeiro-Nieto. 2000.
Modern Information Retrieval. Addison Wesley.
Satanjeev Banerjee, Carolyn Rose and Alexander I. Rudnicky.
2005. The Necessity of a Meeting Recording and Playback
System, and the Benefit of Topic-Level Annotations to
Meeting Browsing. In Proceedings of INTERACT 2005,
Rome, Italy.
Alexander Clark and Andrei Popescu-Belis. 2004. Multi-level
Dialogue Act Tags. In Proceedings of SIGDIAL&apos;04, pages
163–170. Cambridge, MA, USA.
Jeff Conklin. 2006. Dialogue Mapping: Building Shared
Understanding of Wicked Problems. John Wiley &amp; Sons.
Sheila Corrall. 1998. Knowledge management. Are we in the
knowledge management business? ARIADNE: the Web
version, 18.
Anita H.M Cremers, Bart Hilhorst and Arnold P.O.S
Vermeeren. 2005. “What was discussed by whom, how,
when and where?&amp;quot; personalized browsing of annotated
multimedia meeting recordings. In Proceedings of HCI
2005, pages 1–10, Edinburgh, UK.
John Cugini, Laurie Damianos, Lynette Hirschman, Robyn
Kozierok, Jeff Kurtz, Sharon Laskowski and Jean Scholtz.
1997. Methodology for evaluation of collaborative
systems. Technical Report Rev. 3.0, The Evaluation
Working Group of the DARPA Intelligent Collaboration
and Visualization Program.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier and
Hongyan Jing. 2003. Discourse Segmentation of Multi-
Party Conversation. In Proceedings of ACL 2003, pages
562–569, Sapporo, Japan.
Agnes Lisowska, Andrei Popescu-Belis and Susan Armstrong.
2004a. User Query Analysis for the Specification and
Evaluation of a Dialogue Processing and Retrieval System.
In Proceedings LREC 2004, pages 993–996, Lisbon,
Portugal.
Agnes Lisowska, Martin Rajman and Trung H. Bui. 2004b.
ARCHIVUS: A System for Accesssing the Content of
Recorded Multimodal Meetings. In Proceedings of MLMI
2004, Martigny, Switzerland.
Stéphane Marchand-Maillet and Eric Bruno. 2005. Collection
Guiding: A new framework for handling large multimedia
collections. In Proceeding of AVIVDiLib05, Cortona, Italy.
Vincenzo Pallotta, Hatem Ghorbel, Afzal Ballim, Agnes
Lisowska and Stéphane Marchand-Maillet. 2004. Towards
meeting information systems: Meeting knowledge
management. In Proceedings of ICEIS 2005, pages 464–
469, Porto, Portugal.
Lonneke van der Plaas, Vincenzo Pallotta, Martin Rajman and
Hatem Ghorbel. 2004. Automatic keyword extraction from
spoken text: A comparison between two lexical resources:
the EDR and WordNet. In Proceedings of the LREC 2004,
pages 2205–2208, Lisbon, Portugal.
Wilfried M. Post, Anita H.M. Cremers and Olivier Blanson
Henkemans. 2004. A Research Environment for Meeting
Behavior. In Proceedings of the 3rd Workshop on Social
Intelligence Design, pages 159–165, University of Twente,
Enschede, The Netherlands.
Rutger Rienks and Daan Verbree. 2006. About the Usefulness
and Learnability of Argument–Diagrams from Real
Discussions. In Proceedings of MLMI 2006, Washington
DC, USA.
Nicholas C. Romano Jr. and Jay F. Nunamaker Jr. 2001.
Meeting Analysis: Findings from Research and Practice. In
Proceedings of HICSS-34, Maui, HI, IEEE Computer
Society.
Duska Rosemberg and John A.A. Silince. 1999. Common
ground in computer-supported collaborative argumentation.
In Proceedings of the CLSCL99, Stanford, CA, USA.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor,
Rachel Martin, Carol Van Ess-Dykema and Marie Meteer.
2000. Dialog Act Modeling for Automatic Tagging and
Recognition of Conversational Speech. Computational
Linguistics, 26(3):339–373.
Simon Tucker and Steve Whittaker. 2004. Accessing
multimodal meeting data: systems, problems and
possibilities. In Proceedings of MLMI 2004, Martigny,
Switzerland.
Alessandro Vinciarelli. 2004. Noisy text categorization. In
Proceedings of ICPR 2004, Cambridge, UK.
Pierre Wellner, Mike Flynn, Simon Tucker, Steve Whittaker.
2005. A Meeting Browser Evaluation Test. In Proceedings
of CHI 2005, Portand, Oregon, USA.
</reference>
<page confidence="0.993082">
1015
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.138923">
<title confidence="0.998678">User Requirements Analysis for Meeting Information Retrieval Based on Query Elicitation</title>
<author confidence="0.999831">Vincenzo Pallotta</author>
<affiliation confidence="0.999924">Department of Computer Science University of Fribourg</affiliation>
<address confidence="0.591826">Switzerland</address>
<email confidence="0.397857">Vincenzo.Pallotta@unifr.ch</email>
<author confidence="0.866773">Violeta Seretan</author>
<affiliation confidence="0.9998755">Language Technology Laboratory University of Geneva</affiliation>
<address confidence="0.704099">Switzerland</address>
<email confidence="0.921258">seretan@lettres.unige.ch</email>
<author confidence="0.998493">Marita Ailomaa</author>
<affiliation confidence="0.959559333333333">Artificial Intelligence Laboratory Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland</affiliation>
<email confidence="0.851589">Marita.Ailomaa@epfl.ch</email>
<abstract confidence="0.9990654375">We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer. We grounded our work on the empirical analysis of elicited user queries. We found that the majority of elicited queries (around 60%) pertain to argumentative processes and outcomes. Our analysis also suggests that standard keyword-based Information Retrieval can only deal successfully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeremy Ang</author>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Automatic Dialog Act Segmentation and Classification in Multiparty Meetings.</title>
<date>2005</date>
<booktitle>In Proceedings of IEEE ICASSP 2005,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="25883" citStr="Ang et al., 2005" startWordPosition="4096" endWordPosition="4099">he recorded meeting dialogues poses important problems when using standard IR indexing techniques (Baeza-Yates and Ribeiro-Nieto, 2000), because users ask different types of queries for which a single retrieval strategy (e.g., keywords-based) is insufficient. This is the case when looking at answers that require some sort of entailment, such as inferring that a proposal has been rejected when a meeting participant says Are you kidding?. Spoken-language information retrieval (Vinciarelli, 2004) and automatic dialogue-act extraction techniques (Stolke et al., 2000; Clark and PopescuBelis, 2004; Ang et al., 2005) have been applied to meeting recordings and produced good results under the assumption that the user is interested in retrieving either topic-based or dialog act-based information. But this assumption is partially invalidated by our user query elicitation analysis, which showed that such information is only sought in a relatively small fraction of the users’ queries. A particular problem for these approaches is that the topic looked for is usually not a query itself (Was topic T mentioned?), but just a parameter in more structured questions (What was decided about T?). Moreover, the relevant </context>
</contexts>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>Jeremy Ang, Yang Liu and Elizabeth Shriberg. 2005. Automatic Dialog Act Segmentation and Classification in Multiparty Meetings. In Proceedings of IEEE ICASSP 2005, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Nieto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>2000</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="25401" citStr="Baeza-Yates and Ribeiro-Nieto, 2000" startWordPosition="4023" endWordPosition="4026">-set Information types IR insufficient 19 cases 54.3% IR irrelevant 9 cases 54.3% Artefacts x x Recordings x x x x Meta-data x x Dlg acts &amp; Adj. pairs Argumentation x x x x Semantics x x x x Inference x x x x Multiple meetings External x Cases 6 4 2 2 2 2 Ratio (%) 31.6 21 10.5 10.5 22.2 22.2 Table 5. Some of the most frequent combinations of information required for answering the queries in the IM2-Set and in the MS-set when IR alone fails. 3.5 Discussion Searching relevant information through the recorded meeting dialogues poses important problems when using standard IR indexing techniques (Baeza-Yates and Ribeiro-Nieto, 2000), because users ask different types of queries for which a single retrieval strategy (e.g., keywords-based) is insufficient. This is the case when looking at answers that require some sort of entailment, such as inferring that a proposal has been rejected when a meeting participant says Are you kidding?. Spoken-language information retrieval (Vinciarelli, 2004) and automatic dialogue-act extraction techniques (Stolke et al., 2000; Clark and PopescuBelis, 2004; Ang et al., 2005) have been applied to meeting recordings and produced good results under the assumption that the user is interested in</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Nieto, 2000</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribeiro-Nieto. 2000. Modern Information Retrieval. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Carolyn Rose</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>The Necessity of a Meeting Recording and Playback System, and the Benefit of Topic-Level Annotations to Meeting Browsing.</title>
<date>2005</date>
<booktitle>In Proceedings of INTERACT 2005,</booktitle>
<location>Rome, Italy.</location>
<contexts>
<context position="4172" citStr="Banerjee et al., 2005" startWordPosition="610" endWordPosition="613"> task were assigned, etc. The motivating scenario is the following: A user 1008 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1008–1015, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics needs information about a past meeting, either in cussions is of central importance for building proquality of a participant who wants to recollect a ject memories since, in addition to the &amp;quot;strictly facdiscussion (since the memories of co-participants tual, technical information&amp;quot;, these memories must are often inconsistent, cf. Banerjee et al., 2005), or also store relevant information about decias a non-participant who missed that meeting. sion-making processes. In a business context, the Instead of consulting the entire meeting-related information derived from meetings is useful for information, which is usually heterogeneous and future business processes, as it can explain phescaterred (audio-video recordings, notes, minutes, nomena and past decisions and can support future e-mails, handouts, etc.), the user asks natural actions by mining and assessment (Pallotta et al., language questions to a query engine which 2004). The argumentati</context>
<context position="7414" citStr="Banerjee et al. (2005)" startWordPosition="1099" endWordPosition="1103">ed by a series of recent studies that attempted to elicit the potential user questions about meetings (Lisowska et al., 2004a; Benerjee at al., 2005; Cremers et al., 2005). The study of Lisowska et al. (2004a), part of the IM2 research project2, was performed in a simulated environment in which users were asked to imagine themselves in a particular role from a series of scenarios. The participants were both IM2 members and non-IM2 members and produced about 300 retrospective queries on recorded meetings. Although this study has been criticized by Post et al. (2004), Cremers et al. (2005), and Banerjee et al. (2005) for being biased, artificial, obtrusive, and not conforming to strong HCI methodologies for survey research, it shed light on potential queries and classified them in two broad categories, that seem to correspond to our argumentative/non-argumentative distinction (Lisowska et al., 2004a: 994): • “elements related to the interaction among participants: acceptance/rejection, agreement/disagreement; proposal, argumentation (for and against); assertions, statements; decisions; discussions, debates; reactions; questions; solutions”; 1 (Tucker and Whittaker, 2004) identifies 4 types of meeting brow</context>
<context position="8802" citStr="Banerjee et al. (2005)" startWordPosition="1289" endWordPosition="1292">such as discourse and temporal structure information). 2 http://www.im2.ch 1009 • “concepts from the meeting domains: dates, times; documents; meeting index: current, previous, sets; participants; presentations, talks; projects; tasks, responsibilities; topics”. Unfortunately, the study does not provide precise information on the relative proportions of queries for the classification proposed, but simply suggests that overall more queries belong to the second category, while queries requiring understanding of the dialogue structure still comprise a sizeable proportion. The survey conducted by Banerjee et al. (2005) concerned instead real, non-simulated interviews of busy professionals about actual situations, related either to meetings in which they previously participated, or to meetings they missed. More than half of the information sought by interviewees concerned, in both cases, the argumentative dimension of meetings. For non-missed meetings, 15 out of the 26 instances (i.e., 57.7%) concerned argumentative aspects: what the decision was regarding a topic (7); what task someone was assigned (4); who made a particular decision (2); what was the participants&apos; reaction to a particular topic (1); what t</context>
<context position="16166" citStr="Banerjee et al., 2005" startWordPosition="2470" endWordPosition="2473">tual 24.8% 45.6% 20.0% 20.0% Thematic 18.5% 20.0% 11.4% Process 30.0% 32.6% 22.9% 28.6% Outcome 26.7% 21.8% 37.1% 40.0% Process+ Outcome 56.7% 54.4% 60.0% 68.6% Table 1. Query classification according to the meeting content type. Results from this classification task for both query sets are reported in Table 1. In both sets, the information most sought was argumentative: about 55% of the IM2-set queries are argumentative (process or outcome). This invalidates the initial estimation of Lisowska et al. (2004a:994) that the non-argumentative queries prevail, and confirms the figures obtained in (Banerjee et al., 2005), according to which 57.7% of the queries are argumentative. In our real managers survey, we obtained even higher percentages for the argumentative queries (60% or 68.6%, depending on the annotation team). The argumentative queries are followed by factual and thematic ones in both query sets, with a slight advantage for factual queries. The inter-annotator agreement for this first classification is reported in Table 2. The proportion of queries on which annotators agree in classifying them as argumentative is significantly high. We only report here the agreement results for the individual argu</context>
</contexts>
<marker>Banerjee, Rose, Rudnicky, 2005</marker>
<rawString>Satanjeev Banerjee, Carolyn Rose and Alexander I. Rudnicky. 2005. The Necessity of a Meeting Recording and Playback System, and the Benefit of Topic-Level Annotations to Meeting Browsing. In Proceedings of INTERACT 2005, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Multi-level Dialogue Act Tags.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGDIAL&apos;04,</booktitle>
<pages>163--170</pages>
<location>Cambridge, MA, USA.</location>
<marker>Clark, Popescu-Belis, 2004</marker>
<rawString>Alexander Clark and Andrei Popescu-Belis. 2004. Multi-level Dialogue Act Tags. In Proceedings of SIGDIAL&apos;04, pages 163–170. Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Conklin</author>
</authors>
<title>Dialogue Mapping: Building Shared Understanding of Wicked Problems.</title>
<date>2006</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="2093" citStr="Conklin, 2006" startWordPosition="298" endWordPosition="299">ro or con), negotiate alternatives, and make decisions. As outcomes of the meeting, agreements on future action items are reached, tasks are assigned, conflicts are solved, etc. Meeting outcomes have a direct impact on the efficiency of organization and team performance, and the stored and indexed meeting records serve as reference for further processing (Post et al., 2004). They can also be used in future meetings in order to facilitate the decision-making process by accessing relevant information from previous meetings (Cremers et al., 2005), or in order to make the discussion more focused (Conklin, 2006). Meetings constitute a substantial and important source of information that improves corporate organization and performance (Corrall, 1998; Romano and Nunamaker, 2001). Novel multimedia techniques have been dedicated to meeting recording, structuring and content analysis according to the metadata schema, and finally, to accessing the analyzed content via browsing, querying or filtering (Cremers et al., 2005; Tucker and Whittaker, 2004). This paper focuses on debate meetings (Cugini et al., 1997) because of their particular richness in information concerning the decision-making process. We con</context>
</contexts>
<marker>Conklin, 2006</marker>
<rawString>Jeff Conklin. 2006. Dialogue Mapping: Building Shared Understanding of Wicked Problems. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheila Corrall</author>
</authors>
<title>Knowledge management. Are we in the knowledge management business? ARIADNE: the Web version,</title>
<date>1998</date>
<pages>18</pages>
<contexts>
<context position="2232" citStr="Corrall, 1998" startWordPosition="316" endWordPosition="317"> assigned, conflicts are solved, etc. Meeting outcomes have a direct impact on the efficiency of organization and team performance, and the stored and indexed meeting records serve as reference for further processing (Post et al., 2004). They can also be used in future meetings in order to facilitate the decision-making process by accessing relevant information from previous meetings (Cremers et al., 2005), or in order to make the discussion more focused (Conklin, 2006). Meetings constitute a substantial and important source of information that improves corporate organization and performance (Corrall, 1998; Romano and Nunamaker, 2001). Novel multimedia techniques have been dedicated to meeting recording, structuring and content analysis according to the metadata schema, and finally, to accessing the analyzed content via browsing, querying or filtering (Cremers et al., 2005; Tucker and Whittaker, 2004). This paper focuses on debate meetings (Cugini et al., 1997) because of their particular richness in information concerning the decision-making process. We consider that the meeting content can be organized on three levels: (i) factual level (what happens: events, timeline, actions, dynamics); (ii</context>
</contexts>
<marker>Corrall, 1998</marker>
<rawString>Sheila Corrall. 1998. Knowledge management. Are we in the knowledge management business? ARIADNE: the Web version, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anita H M Cremers</author>
<author>Bart Hilhorst</author>
<author>Arnold P O S Vermeeren</author>
</authors>
<title>What was discussed by whom, how, when and where?&amp;quot; personalized browsing of annotated multimedia meeting recordings.</title>
<date>2005</date>
<booktitle>In Proceedings of HCI 2005,</booktitle>
<pages>1--10</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="2028" citStr="Cremers et al., 2005" startWordPosition="285" endWordPosition="288">ress opinions, make suggestions, propose solutions, provide arguments (pro or con), negotiate alternatives, and make decisions. As outcomes of the meeting, agreements on future action items are reached, tasks are assigned, conflicts are solved, etc. Meeting outcomes have a direct impact on the efficiency of organization and team performance, and the stored and indexed meeting records serve as reference for further processing (Post et al., 2004). They can also be used in future meetings in order to facilitate the decision-making process by accessing relevant information from previous meetings (Cremers et al., 2005), or in order to make the discussion more focused (Conklin, 2006). Meetings constitute a substantial and important source of information that improves corporate organization and performance (Corrall, 1998; Romano and Nunamaker, 2001). Novel multimedia techniques have been dedicated to meeting recording, structuring and content analysis according to the metadata schema, and finally, to accessing the analyzed content via browsing, querying or filtering (Cremers et al., 2005; Tucker and Whittaker, 2004). This paper focuses on debate meetings (Cugini et al., 1997) because of their particular richn</context>
<context position="6963" citStr="Cremers et al., 2005" startWordPosition="1022" endWordPosition="1025"> most salient deliberative facts, the derived-data browsers are much more useful, in that they offer access to the full meeting record, and thus to relevant details about the deliberative information sought. 2.1 Importance of Argumentative Structure As shown by Rosemberg and Silince (1999), tracking argumentative information from meeting dis2.2 Query Elicitation Studies The users&apos; interest in argumentation dimension of meetings has been highlighted by a series of recent studies that attempted to elicit the potential user questions about meetings (Lisowska et al., 2004a; Benerjee at al., 2005; Cremers et al., 2005). The study of Lisowska et al. (2004a), part of the IM2 research project2, was performed in a simulated environment in which users were asked to imagine themselves in a particular role from a series of scenarios. The participants were both IM2 members and non-IM2 members and produced about 300 retrospective queries on recorded meetings. Although this study has been criticized by Post et al. (2004), Cremers et al. (2005), and Banerjee et al. (2005) for being biased, artificial, obtrusive, and not conforming to strong HCI methodologies for survey research, it shed light on potential queries and </context>
<context position="10221" citStr="Cremers et al. (2005)" startWordPosition="1508" endWordPosition="1511">ly represented (18/36): decisions on a topic (7); what task was assigned to interviewee (4); whether a particular decision was made (3); what decisions were made (2); reasons for a decision (1); reactions to a topic (1). The thematic questions concern topics discussed, announcements made, and background of participants. The study also showed that the recovery of information from meeting recordings is significantly faster when discourse annotations are available, such as the distinction between discussion, presentation, and briefing. Another unobtrusive user requirements study was performed by Cremers et al. (2005) in a &amp;quot;seminatural setting&amp;quot; related to the design of a meeting browser. The top 5 search interests highlighted by the 60 survey participants were: decisions made, participants/speakers, topics, agenda items, and arguments for decision. Of these, the ones shown in italics are argumentative. In fact, the authors acknowledge the necessity to include some &amp;quot;functional&amp;quot; categories as innovative search options. Interestingly, from the user interface evaluation presented in their paper, one can indirectly infer how salient the argumentative information is perceived by users: the icons that the authors</context>
</contexts>
<marker>Cremers, Hilhorst, Vermeeren, 2005</marker>
<rawString>Anita H.M Cremers, Bart Hilhorst and Arnold P.O.S Vermeeren. 2005. “What was discussed by whom, how, when and where?&amp;quot; personalized browsing of annotated multimedia meeting recordings. In Proceedings of HCI 2005, pages 1–10, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cugini</author>
<author>Laurie Damianos</author>
<author>Lynette Hirschman</author>
<author>Robyn Kozierok</author>
<author>Jeff Kurtz</author>
<author>Sharon Laskowski</author>
<author>Jean Scholtz</author>
</authors>
<title>Methodology for evaluation of collaborative systems.</title>
<date>1997</date>
<booktitle>The Evaluation Working Group of the DARPA Intelligent Collaboration and Visualization Program.</booktitle>
<tech>Technical Report Rev. 3.0,</tech>
<contexts>
<context position="2594" citStr="Cugini et al., 1997" startWordPosition="369" endWordPosition="372">ormation from previous meetings (Cremers et al., 2005), or in order to make the discussion more focused (Conklin, 2006). Meetings constitute a substantial and important source of information that improves corporate organization and performance (Corrall, 1998; Romano and Nunamaker, 2001). Novel multimedia techniques have been dedicated to meeting recording, structuring and content analysis according to the metadata schema, and finally, to accessing the analyzed content via browsing, querying or filtering (Cremers et al., 2005; Tucker and Whittaker, 2004). This paper focuses on debate meetings (Cugini et al., 1997) because of their particular richness in information concerning the decision-making process. We consider that the meeting content can be organized on three levels: (i) factual level (what happens: events, timeline, actions, dynamics); (ii) thematic level (what is said: topics discussed and details); (iii) argumentative level (which/how common goals are reached). The information on the first two levels is explicit information that can be usually retrieved directly by searching the meeting records with appropriate IR techniques (i.e., TF-IDF). The third level, on the contrary, contains more abst</context>
</contexts>
<marker>Cugini, Damianos, Hirschman, Kozierok, Kurtz, Laskowski, Scholtz, 1997</marker>
<rawString>John Cugini, Laurie Damianos, Lynette Hirschman, Robyn Kozierok, Jeff Kurtz, Sharon Laskowski and Jean Scholtz. 1997. Methodology for evaluation of collaborative systems. Technical Report Rev. 3.0, The Evaluation Working Group of the DARPA Intelligent Collaboration and Visualization Program.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Eric Fosler-Lussier and Hongyan Jing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 2003,</booktitle>
<pages>562--569</pages>
<location>Sapporo, Japan.</location>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric Fosler-Lussier and Hongyan Jing. 2003. Discourse Segmentation of MultiParty Conversation. In Proceedings of ACL 2003, pages 562–569, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnes Lisowska</author>
<author>Andrei Popescu-Belis</author>
<author>Susan Armstrong</author>
</authors>
<title>User Query Analysis for the Specification and Evaluation of a Dialogue Processing and Retrieval System.</title>
<date>2004</date>
<booktitle>In Proceedings LREC 2004,</booktitle>
<pages>993--996</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="6916" citStr="Lisowska et al., 2004" startWordPosition="1014" endWordPosition="1017">m it. If minutes are likely to contain only the most salient deliberative facts, the derived-data browsers are much more useful, in that they offer access to the full meeting record, and thus to relevant details about the deliberative information sought. 2.1 Importance of Argumentative Structure As shown by Rosemberg and Silince (1999), tracking argumentative information from meeting dis2.2 Query Elicitation Studies The users&apos; interest in argumentation dimension of meetings has been highlighted by a series of recent studies that attempted to elicit the potential user questions about meetings (Lisowska et al., 2004a; Benerjee at al., 2005; Cremers et al., 2005). The study of Lisowska et al. (2004a), part of the IM2 research project2, was performed in a simulated environment in which users were asked to imagine themselves in a particular role from a series of scenarios. The participants were both IM2 members and non-IM2 members and produced about 300 retrospective queries on recorded meetings. Although this study has been criticized by Post et al. (2004), Cremers et al. (2005), and Banerjee et al. (2005) for being biased, artificial, obtrusive, and not conforming to strong HCI methodologies for survey re</context>
<context position="13307" citStr="Lisowska et al. (2004" startWordPosition="2000" endWordPosition="2003">query instance, as this provides not only detailed, but also traceable information. 1010 3.1 Data: Collecting User Queries Our analysis is based on a heterogeneous collection of queries for meeting data. In general, an unbiased queries dataset is difficult to obtain, and the quality of a dataset can vary if the sample is made of too homogenous subjects (e.g., people belonging to the same group as members of the same project). In order to cope with this problem, our strategy was to use three different datasets collected in different settings: • First, we considered the IM2 dataset collected by Lisowska et al. (2004a), the only set of user queries on meetings available to date. It comprises 270 questions (shortly described in Section 2) annotated with a label showing whether or not the query was produced by an IM2- member. These queries are introspective and not related to any particular recorded meeting. • Second, we cross-validated this dataset with a large corpus of 294 natural language statements about existing meetings records. This dataset, called the BET observations (Wellner et al., 2005), was collected by subjects who were asked to watch several meeting recordings and to report what the meeting </context>
<context position="16055" citStr="Lisowska et al. (2004" startWordPosition="2455" endWordPosition="2458">utcome of the argumentative process. Category IM2-set MS-Set (size:270) (size: 35) Team1 Team2 Team1 Team2 Factual 24.8% 45.6% 20.0% 20.0% Thematic 18.5% 20.0% 11.4% Process 30.0% 32.6% 22.9% 28.6% Outcome 26.7% 21.8% 37.1% 40.0% Process+ Outcome 56.7% 54.4% 60.0% 68.6% Table 1. Query classification according to the meeting content type. Results from this classification task for both query sets are reported in Table 1. In both sets, the information most sought was argumentative: about 55% of the IM2-set queries are argumentative (process or outcome). This invalidates the initial estimation of Lisowska et al. (2004a:994) that the non-argumentative queries prevail, and confirms the figures obtained in (Banerjee et al., 2005), according to which 57.7% of the queries are argumentative. In our real managers survey, we obtained even higher percentages for the argumentative queries (60% or 68.6%, depending on the annotation team). The argumentative queries are followed by factual and thematic ones in both query sets, with a slight advantage for factual queries. The inter-annotator agreement for this first classification is reported in Table 2. The proportion of queries on which annotators agree in classifying</context>
<context position="17790" citStr="Lisowska et al., 2004" startWordPosition="2729" endWordPosition="2732">ratio kappa ratio kappa Process 84.8% 82.9% 88.6% 87.8% Outcome 90.7% 89.6% 91.4% 90.9% Process &amp; 78.9% 76.2% 85.7% 84.8% Outcome AA 117/213 = 19/30 = 54.9% 63.3% Table 2. Inter-annotator agreement for query-type classification. Furthermore, we provided a re-assessment of the proportion of argumentative queries with respect to query origin for the IM2-set (IM2 members vs. non-IM2 members): non-IM2 members issued 30.8% of agreed argumentative queries, a proportion that, while smaller compared to that of IM2 members (69.2%), is still non-negligible. This contrasts with the opinion expressed in (Lisowska et al., 2004a) that argumentative queries are almost exclusively produced by IM2 members. Among the 90 agreed IM2 queries that were cross-validated with the BET-observation set, 28.9% were argumentative. We also noted that the ratio of BET statements that contain argumentative information is quite high (66.9%). 3.3 Query Difficulty Analysis In order to assess the difficulty in answering a query, we used the following categories that the annotators could assign to each query, according to the type of information and techniques they judged necessary for answering it: 1. Role of IR: states the role of standa</context>
</contexts>
<marker>Lisowska, Popescu-Belis, Armstrong, 2004</marker>
<rawString>Agnes Lisowska, Andrei Popescu-Belis and Susan Armstrong. 2004a. User Query Analysis for the Specification and Evaluation of a Dialogue Processing and Retrieval System. In Proceedings LREC 2004, pages 993–996, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnes Lisowska</author>
<author>Martin Rajman</author>
<author>Trung H Bui</author>
</authors>
<title>ARCHIVUS: A System for Accesssing the Content of Recorded Multimodal Meetings.</title>
<date>2004</date>
<booktitle>In Proceedings of MLMI 2004,</booktitle>
<location>Martigny, Switzerland.</location>
<contexts>
<context position="6916" citStr="Lisowska et al., 2004" startWordPosition="1014" endWordPosition="1017">m it. If minutes are likely to contain only the most salient deliberative facts, the derived-data browsers are much more useful, in that they offer access to the full meeting record, and thus to relevant details about the deliberative information sought. 2.1 Importance of Argumentative Structure As shown by Rosemberg and Silince (1999), tracking argumentative information from meeting dis2.2 Query Elicitation Studies The users&apos; interest in argumentation dimension of meetings has been highlighted by a series of recent studies that attempted to elicit the potential user questions about meetings (Lisowska et al., 2004a; Benerjee at al., 2005; Cremers et al., 2005). The study of Lisowska et al. (2004a), part of the IM2 research project2, was performed in a simulated environment in which users were asked to imagine themselves in a particular role from a series of scenarios. The participants were both IM2 members and non-IM2 members and produced about 300 retrospective queries on recorded meetings. Although this study has been criticized by Post et al. (2004), Cremers et al. (2005), and Banerjee et al. (2005) for being biased, artificial, obtrusive, and not conforming to strong HCI methodologies for survey re</context>
<context position="13307" citStr="Lisowska et al. (2004" startWordPosition="2000" endWordPosition="2003">query instance, as this provides not only detailed, but also traceable information. 1010 3.1 Data: Collecting User Queries Our analysis is based on a heterogeneous collection of queries for meeting data. In general, an unbiased queries dataset is difficult to obtain, and the quality of a dataset can vary if the sample is made of too homogenous subjects (e.g., people belonging to the same group as members of the same project). In order to cope with this problem, our strategy was to use three different datasets collected in different settings: • First, we considered the IM2 dataset collected by Lisowska et al. (2004a), the only set of user queries on meetings available to date. It comprises 270 questions (shortly described in Section 2) annotated with a label showing whether or not the query was produced by an IM2- member. These queries are introspective and not related to any particular recorded meeting. • Second, we cross-validated this dataset with a large corpus of 294 natural language statements about existing meetings records. This dataset, called the BET observations (Wellner et al., 2005), was collected by subjects who were asked to watch several meeting recordings and to report what the meeting </context>
<context position="16055" citStr="Lisowska et al. (2004" startWordPosition="2455" endWordPosition="2458">utcome of the argumentative process. Category IM2-set MS-Set (size:270) (size: 35) Team1 Team2 Team1 Team2 Factual 24.8% 45.6% 20.0% 20.0% Thematic 18.5% 20.0% 11.4% Process 30.0% 32.6% 22.9% 28.6% Outcome 26.7% 21.8% 37.1% 40.0% Process+ Outcome 56.7% 54.4% 60.0% 68.6% Table 1. Query classification according to the meeting content type. Results from this classification task for both query sets are reported in Table 1. In both sets, the information most sought was argumentative: about 55% of the IM2-set queries are argumentative (process or outcome). This invalidates the initial estimation of Lisowska et al. (2004a:994) that the non-argumentative queries prevail, and confirms the figures obtained in (Banerjee et al., 2005), according to which 57.7% of the queries are argumentative. In our real managers survey, we obtained even higher percentages for the argumentative queries (60% or 68.6%, depending on the annotation team). The argumentative queries are followed by factual and thematic ones in both query sets, with a slight advantage for factual queries. The inter-annotator agreement for this first classification is reported in Table 2. The proportion of queries on which annotators agree in classifying</context>
<context position="17790" citStr="Lisowska et al., 2004" startWordPosition="2729" endWordPosition="2732">ratio kappa ratio kappa Process 84.8% 82.9% 88.6% 87.8% Outcome 90.7% 89.6% 91.4% 90.9% Process &amp; 78.9% 76.2% 85.7% 84.8% Outcome AA 117/213 = 19/30 = 54.9% 63.3% Table 2. Inter-annotator agreement for query-type classification. Furthermore, we provided a re-assessment of the proportion of argumentative queries with respect to query origin for the IM2-set (IM2 members vs. non-IM2 members): non-IM2 members issued 30.8% of agreed argumentative queries, a proportion that, while smaller compared to that of IM2 members (69.2%), is still non-negligible. This contrasts with the opinion expressed in (Lisowska et al., 2004a) that argumentative queries are almost exclusively produced by IM2 members. Among the 90 agreed IM2 queries that were cross-validated with the BET-observation set, 28.9% were argumentative. We also noted that the ratio of BET statements that contain argumentative information is quite high (66.9%). 3.3 Query Difficulty Analysis In order to assess the difficulty in answering a query, we used the following categories that the annotators could assign to each query, according to the type of information and techniques they judged necessary for answering it: 1. Role of IR: states the role of standa</context>
</contexts>
<marker>Lisowska, Rajman, Bui, 2004</marker>
<rawString>Agnes Lisowska, Martin Rajman and Trung H. Bui. 2004b. ARCHIVUS: A System for Accesssing the Content of Recorded Multimodal Meetings. In Proceedings of MLMI 2004, Martigny, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stéphane Marchand-Maillet</author>
<author>Eric Bruno</author>
</authors>
<title>Collection Guiding: A new framework for handling large multimedia collections.</title>
<date>2005</date>
<booktitle>In Proceeding of AVIVDiLib05,</booktitle>
<location>Cortona, Italy.</location>
<marker>Marchand-Maillet, Bruno, 2005</marker>
<rawString>Stéphane Marchand-Maillet and Eric Bruno. 2005. Collection Guiding: A new framework for handling large multimedia collections. In Proceeding of AVIVDiLib05, Cortona, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincenzo Pallotta</author>
</authors>
<title>Hatem Ghorbel, Afzal Ballim, Agnes Lisowska and Stéphane Marchand-Maillet.</title>
<date>2004</date>
<booktitle>In Proceedings of ICEIS</booktitle>
<pages>464--469</pages>
<location>Porto, Portugal.</location>
<marker>Pallotta, 2004</marker>
<rawString>Vincenzo Pallotta, Hatem Ghorbel, Afzal Ballim, Agnes Lisowska and Stéphane Marchand-Maillet. 2004. Towards meeting information systems: Meeting knowledge management. In Proceedings of ICEIS 2005, pages 464– 469, Porto, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plaas</author>
<author>Vincenzo Pallotta</author>
<author>Martin Rajman</author>
<author>Hatem Ghorbel</author>
</authors>
<title>Automatic keyword extraction from spoken text: A comparison between two lexical resources: the EDR and WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC 2004,</booktitle>
<pages>2205--2208</pages>
<location>Lisbon, Portugal.</location>
<marker>van der Plaas, Pallotta, Rajman, Ghorbel, 2004</marker>
<rawString>Lonneke van der Plaas, Vincenzo Pallotta, Martin Rajman and Hatem Ghorbel. 2004. Automatic keyword extraction from spoken text: A comparison between two lexical resources: the EDR and WordNet. In Proceedings of the LREC 2004, pages 2205–2208, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilfried M Post</author>
<author>Anita H M Cremers</author>
<author>Olivier Blanson Henkemans</author>
</authors>
<title>A Research Environment for Meeting Behavior.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd Workshop on Social Intelligence Design,</booktitle>
<pages>159--165</pages>
<institution>University of Twente,</institution>
<location>Enschede, The Netherlands.</location>
<contexts>
<context position="1855" citStr="Post et al., 2004" startWordPosition="259" endWordPosition="262">ake place that are goal-oriented and where participants perform a series of actions, usually aimed at reaching a common goal: they exchange information, raise issues, express opinions, make suggestions, propose solutions, provide arguments (pro or con), negotiate alternatives, and make decisions. As outcomes of the meeting, agreements on future action items are reached, tasks are assigned, conflicts are solved, etc. Meeting outcomes have a direct impact on the efficiency of organization and team performance, and the stored and indexed meeting records serve as reference for further processing (Post et al., 2004). They can also be used in future meetings in order to facilitate the decision-making process by accessing relevant information from previous meetings (Cremers et al., 2005), or in order to make the discussion more focused (Conklin, 2006). Meetings constitute a substantial and important source of information that improves corporate organization and performance (Corrall, 1998; Romano and Nunamaker, 2001). Novel multimedia techniques have been dedicated to meeting recording, structuring and content analysis according to the metadata schema, and finally, to accessing the analyzed content via brow</context>
<context position="7363" citStr="Post et al. (2004)" startWordPosition="1090" endWordPosition="1093">tation dimension of meetings has been highlighted by a series of recent studies that attempted to elicit the potential user questions about meetings (Lisowska et al., 2004a; Benerjee at al., 2005; Cremers et al., 2005). The study of Lisowska et al. (2004a), part of the IM2 research project2, was performed in a simulated environment in which users were asked to imagine themselves in a particular role from a series of scenarios. The participants were both IM2 members and non-IM2 members and produced about 300 retrospective queries on recorded meetings. Although this study has been criticized by Post et al. (2004), Cremers et al. (2005), and Banerjee et al. (2005) for being biased, artificial, obtrusive, and not conforming to strong HCI methodologies for survey research, it shed light on potential queries and classified them in two broad categories, that seem to correspond to our argumentative/non-argumentative distinction (Lisowska et al., 2004a: 994): • “elements related to the interaction among participants: acceptance/rejection, agreement/disagreement; proposal, argumentation (for and against); assertions, statements; decisions; discussions, debates; reactions; questions; solutions”; 1 (Tucker and </context>
</contexts>
<marker>Post, Cremers, Henkemans, 2004</marker>
<rawString>Wilfried M. Post, Anita H.M. Cremers and Olivier Blanson Henkemans. 2004. A Research Environment for Meeting Behavior. In Proceedings of the 3rd Workshop on Social Intelligence Design, pages 159–165, University of Twente, Enschede, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rutger Rienks</author>
<author>Daan Verbree</author>
</authors>
<title>About the Usefulness and Learnability of Argument–Diagrams from Real Discussions.</title>
<date>2006</date>
<booktitle>In Proceedings of MLMI 2006,</booktitle>
<location>Washington DC, USA.</location>
<contexts>
<context position="5493" citStr="Rienks and Verbree, 2006" startWordPosition="801" endWordPosition="804">sualized in form of argumenrecords. tation diagrams or maps, can be helpful in meeting In this paper we assess the users&apos; interest in browsing. To our knowledge, there are at least retrieving argumentative information from three meeting browsers that have adopted argumeetings and what kind of knowledge is required mentative structure: ARCHIVUS (Lisowska et al., for answering users&apos; queries. Section 2 reviews 2004b), ViCoDe (Marchand-Maillet and Bruno, previous user requirements studies for the meeting 2005), and the Twente-AMI JFerret browser domain. Section 3 describes our user requirements (Rienks and Verbree, 2006). study based on the analysis of elicited user queries, presents its main findings, and discusses the implications of these findings for the design of meeting retrieval systems. Section 4 concludes the paper and outlines some directions for future work. 2 Argumentative Information in Meeting Information Retrieval Depending on the meeting browser type1, different levels of meeting content become accessible for information retrieval. Audio and video browsers deal with factual and thematic information, while artifact browsers might also touch on deliberative information, as long as it is present,</context>
</contexts>
<marker>Rienks, Verbree, 2006</marker>
<rawString>Rutger Rienks and Daan Verbree. 2006. About the Usefulness and Learnability of Argument–Diagrams from Real Discussions. In Proceedings of MLMI 2006, Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay F Nunamaker Jr</author>
</authors>
<title>Meeting Analysis: Findings from Research and Practice.</title>
<date>2001</date>
<booktitle>In Proceedings of HICSS-34,</booktitle>
<publisher>IEEE Computer Society.</publisher>
<location>Maui, HI,</location>
<marker>Jr, 2001</marker>
<rawString>Nicholas C. Romano Jr. and Jay F. Nunamaker Jr. 2001. Meeting Analysis: Findings from Research and Practice. In Proceedings of HICSS-34, Maui, HI, IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duska Rosemberg</author>
<author>John A A Silince</author>
</authors>
<title>Common ground in computer-supported collaborative argumentation.</title>
<date>1999</date>
<booktitle>In Proceedings of the CLSCL99,</booktitle>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="6632" citStr="Rosemberg and Silince (1999)" startWordPosition="972" endWordPosition="975">artifact browsers might also touch on deliberative information, as long as it is present, for instance, in the meeting minutes. In contrast, derived-data browsers aim to account for the argumentative information which is not explicitly present in the meeting content, but can be inferred from it. If minutes are likely to contain only the most salient deliberative facts, the derived-data browsers are much more useful, in that they offer access to the full meeting record, and thus to relevant details about the deliberative information sought. 2.1 Importance of Argumentative Structure As shown by Rosemberg and Silince (1999), tracking argumentative information from meeting dis2.2 Query Elicitation Studies The users&apos; interest in argumentation dimension of meetings has been highlighted by a series of recent studies that attempted to elicit the potential user questions about meetings (Lisowska et al., 2004a; Benerjee at al., 2005; Cremers et al., 2005). The study of Lisowska et al. (2004a), part of the IM2 research project2, was performed in a simulated environment in which users were asked to imagine themselves in a particular role from a series of scenarios. The participants were both IM2 members and non-IM2 membe</context>
</contexts>
<marker>Rosemberg, Silince, 1999</marker>
<rawString>Duska Rosemberg and John A.A. Silince. 1999. Common ground in computer-supported collaborative argumentation. In Proceedings of the CLSCL99, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialog Act Modeling for Automatic Tagging and Recognition of Conversational Speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema and Marie Meteer. 2000. Dialog Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tucker</author>
<author>Steve Whittaker</author>
</authors>
<title>Accessing multimodal meeting data: systems, problems and possibilities.</title>
<date>2004</date>
<booktitle>In Proceedings of MLMI 2004,</booktitle>
<location>Martigny, Switzerland.</location>
<contexts>
<context position="2533" citStr="Tucker and Whittaker, 2004" startWordPosition="359" endWordPosition="362"> to facilitate the decision-making process by accessing relevant information from previous meetings (Cremers et al., 2005), or in order to make the discussion more focused (Conklin, 2006). Meetings constitute a substantial and important source of information that improves corporate organization and performance (Corrall, 1998; Romano and Nunamaker, 2001). Novel multimedia techniques have been dedicated to meeting recording, structuring and content analysis according to the metadata schema, and finally, to accessing the analyzed content via browsing, querying or filtering (Cremers et al., 2005; Tucker and Whittaker, 2004). This paper focuses on debate meetings (Cugini et al., 1997) because of their particular richness in information concerning the decision-making process. We consider that the meeting content can be organized on three levels: (i) factual level (what happens: events, timeline, actions, dynamics); (ii) thematic level (what is said: topics discussed and details); (iii) argumentative level (which/how common goals are reached). The information on the first two levels is explicit information that can be usually retrieved directly by searching the meeting records with appropriate IR techniques (i.e., </context>
<context position="7979" citStr="Tucker and Whittaker, 2004" startWordPosition="1178" endWordPosition="1181"> al. (2004), Cremers et al. (2005), and Banerjee et al. (2005) for being biased, artificial, obtrusive, and not conforming to strong HCI methodologies for survey research, it shed light on potential queries and classified them in two broad categories, that seem to correspond to our argumentative/non-argumentative distinction (Lisowska et al., 2004a: 994): • “elements related to the interaction among participants: acceptance/rejection, agreement/disagreement; proposal, argumentation (for and against); assertions, statements; decisions; discussions, debates; reactions; questions; solutions”; 1 (Tucker and Whittaker, 2004) identifies 4 types of meeting browsers: audio browsers, video browsers, artifacts browsers (that exploit meeting minutes or other meeting-related documents), and browsers that work with derived data (such as discourse and temporal structure information). 2 http://www.im2.ch 1009 • “concepts from the meeting domains: dates, times; documents; meeting index: current, previous, sets; participants; presentations, talks; projects; tasks, responsibilities; topics”. Unfortunately, the study does not provide precise information on the relative proportions of queries for the classification proposed, bu</context>
</contexts>
<marker>Tucker, Whittaker, 2004</marker>
<rawString>Simon Tucker and Steve Whittaker. 2004. Accessing multimodal meeting data: systems, problems and possibilities. In Proceedings of MLMI 2004, Martigny, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Vinciarelli</author>
</authors>
<title>Noisy text categorization.</title>
<date>2004</date>
<booktitle>In Proceedings of ICPR 2004,</booktitle>
<location>Cambridge, UK.</location>
<contexts>
<context position="25764" citStr="Vinciarelli, 2004" startWordPosition="4079" endWordPosition="4081">he queries in the IM2-Set and in the MS-set when IR alone fails. 3.5 Discussion Searching relevant information through the recorded meeting dialogues poses important problems when using standard IR indexing techniques (Baeza-Yates and Ribeiro-Nieto, 2000), because users ask different types of queries for which a single retrieval strategy (e.g., keywords-based) is insufficient. This is the case when looking at answers that require some sort of entailment, such as inferring that a proposal has been rejected when a meeting participant says Are you kidding?. Spoken-language information retrieval (Vinciarelli, 2004) and automatic dialogue-act extraction techniques (Stolke et al., 2000; Clark and PopescuBelis, 2004; Ang et al., 2005) have been applied to meeting recordings and produced good results under the assumption that the user is interested in retrieving either topic-based or dialog act-based information. But this assumption is partially invalidated by our user query elicitation analysis, which showed that such information is only sought in a relatively small fraction of the users’ queries. A particular problem for these approaches is that the topic looked for is usually not a query itself (Was topi</context>
</contexts>
<marker>Vinciarelli, 2004</marker>
<rawString>Alessandro Vinciarelli. 2004. Noisy text categorization. In Proceedings of ICPR 2004, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Wellner</author>
<author>Mike Flynn</author>
<author>Simon Tucker</author>
<author>Steve Whittaker</author>
</authors>
<title>A Meeting Browser Evaluation Test.</title>
<date>2005</date>
<booktitle>In Proceedings of CHI 2005,</booktitle>
<location>Portand, Oregon, USA.</location>
<contexts>
<context position="13797" citStr="Wellner et al., 2005" startWordPosition="2080" endWordPosition="2083">three different datasets collected in different settings: • First, we considered the IM2 dataset collected by Lisowska et al. (2004a), the only set of user queries on meetings available to date. It comprises 270 questions (shortly described in Section 2) annotated with a label showing whether or not the query was produced by an IM2- member. These queries are introspective and not related to any particular recorded meeting. • Second, we cross-validated this dataset with a large corpus of 294 natural language statements about existing meetings records. This dataset, called the BET observations (Wellner et al., 2005), was collected by subjects who were asked to watch several meeting recordings and to report what the meeting participants appeared to consider interesting. We use it as a ‘validation’ set for the IM2 queries: an IM2 query is considered as ‘realistic’ or ‘empirically grounded’ if there is a BET observation that represents a possible answer to the query. For instance, the query Why was the proposal made by X not accepted? matches the BET observation Denis eliminated Silence of the Lambs as it was too violent. • Finally, we collected a new set of ‘real’ queries by conducting a survey of user req</context>
</contexts>
<marker>Wellner, Flynn, Tucker, Whittaker, 2005</marker>
<rawString>Pierre Wellner, Mike Flynn, Simon Tucker, Steve Whittaker. 2005. A Meeting Browser Evaluation Test. In Proceedings of CHI 2005, Portand, Oregon, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>