<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001217">
<title confidence="0.990065">
Recognizing Identical Events with Graph Kernels
</title>
<author confidence="0.996377">
Goran Glavaˇs and Jan ˇSnajder
</author>
<affiliation confidence="0.948005666666667">
University of Zagreb
Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab
</affiliation>
<address confidence="0.909326">
Unska 3, 10000 Zagreb, Croatia
</address>
<email confidence="0.99689">
{goran.glavas,jan.snajder}@fer.hr
</email>
<sectionHeader confidence="0.993813" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997825">
Identifying news stories that discuss the
same real-world events is important for
news tracking and retrieval. Most exist-
ing approaches rely on the traditional vec-
tor space model. We propose an approach
for recognizing identical real-world events
based on a structured, event-oriented doc-
ument representation. We structure docu-
ments as graphs of event mentions and use
graph kernels to measure the similarity be-
tween document pairs. Our experiments
indicate that the proposed graph-based ap-
proach can outperform the traditional vec-
tor space model, and is especially suitable
for distinguishing between topically simi-
lar, yet non-identical events.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920952380952">
News stories typically describe real-world events.
Topic detection and tracking (TDT) aims to de-
tect stories that discuss identical or directly related
events, and track these stories as they evolve over
time (Allan, 2002). Being able to identify the sto-
ries that describe the same real-world event is es-
sential for TDT, and event-based information re-
trieval in general.
In TDT, an event is defined as something hap-
pening in a certain place at a certain time (Yang
et al., 1999), while a topic is defined as a set of
news stories related by some seminal real-world
event (Allan, 2002). To identify news stories on
the same topic, most TDT approaches rely on tra-
ditional vector space models (Salton et al., 1975),
as more sophisticated natural language processing
techniques have not yet proven to be useful for
this task. On the other hand, significant advances
in sentence-level event extraction have been made
over the last decade, in particular as the result of
standardization efforts such as TimeML (Puste-
jovsky et al., 2003a) and TimeBank (Pustejovsky
et al., 2003b), as well as dedicated evaluation tasks
(ACE, 2005; Verhagen et al., 2007; Verhagen et
al., 2010). However, these two lines of research
have largely remained isolated from one another.
In this paper we bridge this gap and address
the task of recognizing stories discussing identical
events by considering structured representations
from sentence-level events. More concretely, we
structure news stories into event graphs built from
individual event mentions extracted from text. To
measure event-based similarity of news stories, we
compare their event graphs using graph kernels
(Borgwardt, 2007). We conduct preliminary ex-
periments on two event-oriented tasks and show
that the proposed approach can outperform tradi-
tional vector space model in recognizing identical
real-world events. Moreover, we demonstrate that
our approach is especially suitable for distinguish-
ing between topically similar, yet non-identical
real-world events.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999741875">
The traditional vector space model (VSM) (Salton
et al., 1975) computes the cosine between bag-of-
words representations of documents. The VSM is
at the core of most approaches that identify same-
topic news stories (Hatzivassiloglou et al., 2000;
Brants et al., 2003; Kumaran and Allan, 2005;
Atkinson and Van der Goot, 2009). However, it
has been observed that some word classes (e.g.,
named entities, noun phrases, collocations) have
more significance than the others. Among them,
named entities have been considered as particu-
larly important, as they often identify the partici-
pants of an event. In view of this, Hatzivassiloglou
et al. (2000) restrict the set of words to be used
for document representation to words constituting
noun phrases and named entities. Makkonen et
</bodyText>
<page confidence="0.960767">
797
</page>
<bodyText confidence="0.938907">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797–803,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
al. (2004) divide document terms into four seman-
tic categories (locations, temporal expressions,
proper names, and general terms) and construct
separate vector for each of them. Kumaran and
Allan (2004) represent news stories with three dif-
ferent vectors, modeling all words, named-entity
words, and all non-named-entity words occurring
in documents. When available, recognition of
identical events can rely on meta-information as-
sociated with news stories, such as document cre-
ation time (DCT). Atkinson and Van der Goot
(2009) combine DCT with VSM, assuming that
temporally distant news stories are unlikely to de-
scribe the same event.
In research on event extraction, the task of rec-
ognizing identical events is known as event coref-
erence resolution (Bejan and Harabagiu, 2010;
Lee et al., 2012). There, however, the aim is to
identify sentence-level event mentions referring to
the same real-world events, and not stories that
discuss identical events.
</bodyText>
<sectionHeader confidence="0.987796" genericHeader="method">
3 Kernels on Event Graphs
</sectionHeader>
<bodyText confidence="0.9999646">
To identify the news describing the same real-
world event, we (1) structure event-oriented in-
formation from text into event graphs and (2) use
graph kernels to measure the similarity between a
pair of event graphs.
</bodyText>
<subsectionHeader confidence="0.999182">
3.1 Event graphs
</subsectionHeader>
<bodyText confidence="0.999986634615385">
An event graph is a vertex- and edge-labeled
mixed graph in which vertices represent individ-
ual event mentions and edges represent temporal
relations between event mentions. We adopt a
generic representation of event mentions, as pro-
posed by Glavaˇs and ˇSnajder (2013): each men-
tion consists of an anchor (a word that conveys
the core meaning) and four types of arguments
(agent, target, time, location). Furthermore, we
consider four types of temporal relations between
event mentions: before, after, overlap, and equal
(Allen, 1983). As relations overlap and equal are
symmetric, whereas before and after are not, an
event graph may contain both directed and undi-
rected edges.
Formally, an event graph G is represented as a
tuple G = (V, E, A, m, r), where V is the set of
vertices, E is the set of undirected edges, A is the
set of directed edges (arcs), m : V -+ M is a
bijection mapping the vertices to event mentions,
and r : E -+ R is the edge-labeling function, as-
signing temporal relations to edges (cf. Fig. 1).
The construction of an event graph from a news
story involves the extraction of event mentions
(anchors and arguments) and the extraction of
temporal relations between mentions. We use a
supervised model (with 80% F1 extraction perfor-
mance) based on a rich set of features similar to
those proposed by Bethard (2008) to extract event
anchors. We then employ a robust, rule-based ap-
proach proposed by Glavaˇs and ˇSnajder (2013) to
extract generic event arguments. Finally, we em-
ploy a supervised model (60% micro-averaged F1
classification performance) with a rich set of fea-
tures, similar to those proposed by Bethard (2008),
to extract temporal relations between event men-
tions. A detailed description of the graph con-
struction steps is outside the scope of this paper.
To compute event graph kernels (cf. Section
3.2), we need to determine whether two event
mentions co-refer. To resolve cross-document
event coreference, we use the model proposed
by Glavaˇs and ˇSnajder (2013). The model de-
termines coreference by comparing factual event
anchors and arguments of four coarse-grained se-
mantic types (agent, target, location, and time),
and achieves an F-score of 67% (79% precision
and 57% recall) on the cross-document mention
pairs from the EventCorefBank dataset (Bejan and
Harabagiu, 2008). In what follows, cf (m1, m2)
denotes whether event mentions m1 and m2 co-
refer (equals 1 if mentions co-refer, 0 otherwise).
</bodyText>
<subsectionHeader confidence="0.999464">
3.2 Graph kernels
</subsectionHeader>
<bodyText confidence="0.987091529411765">
Graph kernels are fast polynomial alternatives
to traditional graph comparison techniques (e.g.,
subgraph isomorphism), which provide an expres-
sive measure of similarity between graphs (Borg-
wardt, 2007). We employ two different graph ker-
nels: product graph kernel and weighted decom-
position kernel. We chose these kernels because
their general forms have intuitive interpretations
for event matching. These particular kernels have
shown to perform well on a number of tasks from
chemoinformatics (Mah´e et al., 2005; Menchetti
et al., 2005).
Product graph kernel. A product graph kernel
(PGK) counts the common walks between two in-
put graphs (G¨artner et al., 2003). The graph prod-
uct of two labeled graphs, G and G�, denoted
GP = G x G&apos;, is a graph with the vertex set
</bodyText>
<equation confidence="0.79529">
VP = {(v, v&apos;)  |v E VG, v&apos; E VGA, S(v, v&apos;)}
</equation>
<page confidence="0.959169">
798
</page>
<bodyText confidence="0.999899272727273">
where S(v, v&apos;) is a predicate that holds when
vertices v and v&apos; are identically labeled (Ham-
mack et al., 2011). Given event graphs G =
(V, E, A, m, r) and G&apos; = (V &apos;, E&apos;, A&apos;, m&apos;, r&apos;), we
consider the vertices to be identically labeled if
the corresponding event mentions co-refer, i.e.,
S(v, v&apos;) .= cf (m(v), m&apos;(v&apos;)). The edge set of the
graph product depends on the type of the product.
We experiment with two different products: ten-
sor product and conormal product. In the tensor
product, an edge is introduced iff the correspond-
ing edges exist in both input graphs and the labels
of those edges match (i.e., both edges represent the
same temporal relation). In the conormal product,
an edge is introduced iff the corresponding edge
exists in at least one input graph. Thus, a conor-
mal product may compensate for omitted temporal
relations in the input graphs.
Let AP be the adjacency matrix of the graph
product GP built from input graphs G and G&apos;. The
product graph kernel that counts common walks in
G and G&apos; can be computed efficiently as:
</bodyText>
<equation confidence="0.9953065">
KPG(G, G&apos;) = � |VP  |[(I − λAP)−1]ij (1)
i,j=1
</equation>
<bodyText confidence="0.998595178571429">
when λ &lt; 1/t, where t is the maximum degree of
a vertex in the graph product GP. In our experi-
ments, we set λ to 1/(t + 1).
Weighted decomposition kernel. A weighted
decomposition kernel (WDK) compares small
graph parts, called selectors, being matched ac-
cording to an equality predicate. The importance
of the match is weighted by the similarity of the
contexts in which the matched selectors occur.
For a description of a general form of WDK, see
Menchetti et al. (2005).
Let S(G) be the set of all pairs (s, z), where s is
the selector (subgraph of interest) and z is the con-
text of s. We decompose event graphs into individ-
ual vertices, i.e., we define selectors to be the indi-
vidual vertices. In this case, similarly as above, the
equality predicate S(v, v&apos;) for two vertices v E G
and v&apos; E G&apos; holds if and only if the correspond-
ing event mentions m(v) and m&apos;(v&apos;) co-refer. Us-
ing selectors that consist of more than one vertex
would require a more complex and perhaps a less
intuitive definition of the equality predicate S. The
selector context Zv of vertex v is a subgraph of G
that contains v and all its immediate neighbors. In
other words, we consider as context all event men-
tions that are in a direct temporal relation with the
selected mention. WDK between event graphs G
and G&apos; is computed as:
</bodyText>
<equation confidence="0.945823">
KWD(G, G&apos;) = � cf (m(v), m&apos;(v&apos;)) n(Zv, Z&apos;v0)
vEVG,v0EVG0
</equation>
<bodyText confidence="0.974089">
(2)
where n(Zv, Z&apos;v0) is the context kernel measuring
the similarity between the context Zv of selector
v E G and the context Z&apos;v0 of selector v&apos; E G&apos;.
We compute the context kernel n as the number of
coreferent mention pairs found between the con-
texts, normalized by the context size:
The intuition behind this is that a pair of corefer-
ent mentions m(v) and m&apos;(v&apos;) should contribute
to the overall event similarity according to the
number of pairs of coreferent mentions, m(w) and
m&apos;(w&apos;), that are in temporal relation with v and v&apos;,
respectively.
Graph kernels example. As an example, con-
sider the following two story snippets describing
the same sets of real-world events:
</bodyText>
<construct confidence="0.578121230769231">
Story 1: A Cezanne masterpiece worth at least $131
million that was the yanked from the wall of a Zurich
art gallery in 2008 has been recovered, Serbian po-
lice said today. Four arrests were made overnight
in connection with the theft, which was one of the
biggest art heists in recent history.
Story 2: Serbian police have recovered a painting
by French impressionist Paul Cezanne worth an esti-
mated 100 million euros (131.7 million U.S. dollars),
media reported on Thursday. The painting ”A boy in
a red vest” was stolen in 2008 from a Zurich museum
by masked perpetrators. Four members of an interna-
tional crime ring were arrested Wednesday.
</construct>
<bodyText confidence="0.999965181818182">
The corresponding event graphs G and G&apos; are
shown in Fig. 1a and 1b, respectively, while their
product is shown in Fig. 1c. There are three pairs
of coreferent event mentions between G and G&apos;:
(yanked, stolen), (recovered, recovered), and (ar-
rests, arrested). Accordingly, the product graph
P has three nodes. The dashed edge between ver-
tices (yanked, stolen) and (arrests, arrested) exists
only in the conormal product graph. By substi-
tuting into (1) the adjacency matrix and maximum
vertex degree of tensor product graph P, we obtain
</bodyText>
<equation confidence="0.966928333333333">
EwEVZv ,w0EVZ0 v0 cf(m(w), m&apos;(w&apos;))
max(|VZv|, |VZ0v0|)
n(Zv, Z&apos;v0) =
</equation>
<page confidence="0.974586">
799
</page>
<figure confidence="0.99468">
(a) Event graph G (Story 1) (b) Event graph G&apos; (Story 2) (c) Product graph P
</figure>
<figureCaption confidence="0.999965">
Figure 1: Example event graphs and their product
</figureCaption>
<bodyText confidence="0.799209">
the tensor PGK score as:
</bodyText>
<equation confidence="0.89507425">
⎡0 0 1 −1
I − 1 0 0 1 ≈ 5.6
3 1 1 0
i,j
</equation>
<bodyText confidence="0.9992585">
Similarly, for the conormal product graph P we
obtain the conormal PGK score of KPG = 9. By
substituting G and G&apos; into (2), we obtain the WDK
score as:
</bodyText>
<equation confidence="0.997239">
�KWD = κ(Zv,Z� v&apos;) = 3 + 4 + 4 ≈ 1.9
2 3 2
(v,v&apos;)EVP
</equation>
<bodyText confidence="0.99962">
where VP contains pairs of coreferent event men-
tions: (yanked, stolen), (recovered, recovered),
and (arrests, arrested).
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999950666666667">
We conducted two preliminary experiments to in-
vestigate whether kernels on event graphs can be
used to recognize identical events.
</bodyText>
<subsectionHeader confidence="0.990371">
4.1 Task 1: Recognizing identical events
</subsectionHeader>
<bodyText confidence="0.999987375">
Dataset. In the first experiment, we classify
pairs of news stories as either describing identical
real-world events or not. For this we need a collec-
tion of stories in which pairs of stories on identi-
cal events have been annotated as such. TDT cor-
pora (Wayne, 2000) is not directly usable because
it has no such annotations. We therefore decided
to build a small annotated dataset.1 To this end,
we use the news clusters of the EMM NewsBrief
service (Steinberger et al., 2009). EMM clusters
news stories from different sources using a docu-
ment similarity score. We acquired 10 randomly
chosen news clusters, manually inspected each of
them, and retained in each cluster only the doc-
uments that describe the same real-world events.
Additionally, we ensured that no documents from
</bodyText>
<footnote confidence="0.955729">
1Datasets for both experiments are available at:
http://takelab.fer.hr/evkernels
</footnote>
<table confidence="0.999939142857143">
Model P R F
Tensor PGK 89.7 82.3 85.8
Conormal PGK 89.3 77.8 83.2
WDK 88.6 73.7 80.5
SVM Graph 91.1 87.6 89.3
SVM Graph + VSM 93.8 96.2 95.0
VSM baseline 90.9 82.9 86.7
</table>
<tableCaption confidence="0.999475">
Table 1: Results for recognition of identical events
</tableCaption>
<bodyText confidence="0.999929766666667">
different clusters discuss the same event. To ob-
tain the gold standard dataset, we build all pairs
of documents. The final dataset consists of 64
documents in 10 clusters, with 195 news pairs
from the same clusters (positive pairs) and 1821
news pairs from different clusters (negative pairs).
We divide the dataset into a train and a test set
(7:3 split ratio). Note that, although our dataset
has ground-truth annotations, it is incomplete in
the sense that some pairs of documents describ-
ing the same events, which were not recognized
as such by the EMM, are not included. Further-
more, because EMM similarity score uses VSM
cosine similarity as one of the features, VSM co-
sine similarity constitutes a competitive baseline
on this dataset.
Results. For each graph kernel and the VSM
baseline, we determine the optimal threshold on
the train set and evaluate the classification per-
formance on the test set. The results are given
in Table 1. The precision is consistently higher
than recall for all kernels and the baseline. High
precision is expected, as clusters represent topi-
cally dissimilar events. PGK models (both ten-
sor and conormal) outperform the WDK model,
indicating that common walks correlate better to
event-based document similarity than common
subgraphs. Individually, none of the graph kernels
outperforms the baseline. To investigate whether
the two kernels complement each other, we fed the
</bodyText>
<equation confidence="0.855111666666667">
3
KPG = �
i,j=1
</equation>
<page confidence="0.91356">
800
</page>
<figure confidence="0.505059555555556">
Original
“Taliban militants have attacked a prison in north-west
Pakistan, freeing at least 380 prisoners... ”
Event-preserving paraphrase
“Taliban militants in northwest Pakistan attacked the
prison, liberated at least 380 prisoners ... ”
Event-shifting paraphrase
“Taliban militants have been arrested in north-west Pak-
istan. At least 380 militants have been arrested... ”
</figure>
<tableCaption confidence="0.989977">
Table 2: Event paraphrasing example
</tableCaption>
<bodyText confidence="0.999652272727273">
individual kernel scores to an SVM model (with
RBF kernel), along with additional graph-based
features such as the number of nodes and the num-
ber of edges (SVM graph model). Finally, we com-
bined the graph-based features with the VSM co-
sine similarity (SVM graph + VSM model). SVM
graph model significantly (at p &lt; 0.05, student’s
2-tailed t-test) outperforms the individual kernel
models and the baseline. The combined model
(SVM graph + VSM) significantly (at p &lt; 0.01)
outperforms the baseline and all kernel models.
</bodyText>
<subsectionHeader confidence="0.995666">
4.2 Task 2: Event-based similarity ranking
</subsectionHeader>
<bodyText confidence="0.999821259259259">
Dataset. In the second experiment we focus
on the task of distinguishing between news sto-
ries that describe topically very similar, yet dis-
tinct events. For this purpose, we use a small
set of event paraphrases, constructed as fol-
lows. We manually selected 10 news stories from
EMM NewsBrief and altered each of them to
obtain two meaning-preserving (event-preserving)
and two meaning-changing (event-shifting) para-
phrases. To obtain the meaning-preserving para-
phrases, we use Google translate and round-trip
translation via two pairs of arbitrarily chosen lan-
guages (Danish/Finnish and Croatian/Hungarian).
Annotators manually corrected lexical and syn-
tactic errors introduced by the round-trip transla-
tion. To obtain meaning-changing paraphrases, we
asked human annotators to alter each story so that
it topically resembles the original, but describes a
different real-world event. The extent of the al-
teration was left to the annotators, i.e., no specific
transformations were proposed. Paraphrase exam-
ples are given in Table 2. The final dataset consists
of 60 news pairs: 30 positive and 30 negative.
Results. For each method we ranked the pairs
based on the assigned similarity scores. An ideal
method would rank all positive pairs above all neg-
ative pairs. We evaluated the performance using
</bodyText>
<table confidence="0.9949312">
Model R-prec. Avg. prec.
Tensor PGK 86.7 96.8
Conormal PGK 93.3 97.5
WDK 86.7 95.7
VSM baseline 80.0 77.1
</table>
<tableCaption confidence="0.998436">
Table 3: Results for event-based similarity ranking
</tableCaption>
<bodyText confidence="0.999156470588235">
two different rank evaluation metrics: R-precision
(precision at rank 30, as there are 30 positive pairs)
and average precision. The performance of graph
kernel models and the VSM baseline is given in
Table 3. We tested the significance of differences
using stratified shuffling (Yeh, 2000). When con-
sidering average precision, all kernel models sig-
nificantly (at p &lt; 0.01) outperform the baseline.
However, when considering R-precision, only the
conormal PGK model significantly (at p &lt; 0.05)
outperforms the baseline. There is no statistical
significance in performance differences between
the considered kernel methods. Inspection of the
rankings reveals that graph kernels assign very low
scores to negative pairs, i.e., they distinguish well
between textual representations of topically simi-
lar, but different real-world events.
</bodyText>
<sectionHeader confidence="0.993862" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999976722222222">
We proposed a novel approach for recognizing
identical events that relies on structured, graph-
based representations of events described in a
document. We use graph kernels as an expres-
sive framework for modeling the similarity be-
tween structured events. Preliminary results on
two event-similarity tasks are encouraging, indi-
cating that our approach can outperform tradi-
tional vector-space model, and is suitable for dis-
tinguishing between topically very similar events.
Further improvements could be obtained by in-
creasing the accuracy of event coreference resolu-
tion, which has a direct influence on graph kernels.
The research opens up many interesting direc-
tions for further research. Besides a systematic
evaluation on larger datasets, we intend to inves-
tigate the applications in event tracking and event-
oriented information retrieval.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99967175">
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croa-
tia under the Grant 036-1300646-1986. We thank
the reviewers for their constructive comments.
</bodyText>
<page confidence="0.992302">
801
</page>
<note confidence="0.713991444444444">
features and clustering algorithms for topical doc-
ument clustering. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 224–231. ACM.
References
ACE. 2005. Evaluation of the detection and recogni-
tion of ACE: Entities, values, temporal expressions,
relations, and events.
</note>
<reference confidence="0.997452621052631">
James Allan. 2002. Topic Detection and Tracking:
Event-based Information Organization, volume 12.
Kluwer Academic Pub.
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832–843.
Martin Atkinson and Erik Van der Goot. 2009. Near
real time information mining in multilingual news.
In Proceedings of the 18th International Conference
on World Wide Web, pages 1153–1154. ACM.
Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A
linguistic resource for discovering event structures
and resolving event coreference. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation (LREC 2008).
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1412–1422. Association for Com-
putational Linguistics.
Steven Bethard. 2008. Finding Event, Temporal and
Causal Structure in Text: A Machine Learning Ap-
proach. Ph.D. thesis, University of Colorado at
Boulder.
Karsten Michael Borgwardt. 2007. Graph Ker-
nels. Ph.D. thesis, Ludwig-Maximilians-Universit¨at
M¨unchen.
Thorsten Brants, Francine Chen, and Ayman Farahat.
2003. A system for new event detection. In Pro-
ceedings of the 26th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, pages 330–337. ACM.
Thomas G¨artner, Peter Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and ef-
ficient alternatives. In Learning Theory and Kernel
Machines, pages 129–143. Springer.
Goran Glava&amp;quot;s and Jan &amp;quot;Snajder. 2013. Exploring coref-
erence uncertainty of generically extracted event
mentions. In Proceedings of 14th International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 408–422. Springer.
Richard Hammack, Wilfried Imrich, and Sandi
Klavz&amp;quot;ar. 2011. Handbook of Product Graphs. Dis-
crete Mathematics and Its Applications. CRC Press.
Vasileios Hatzivassiloglou, Luis Gravano, and Anki-
needu Maganti. 2000. An investigation of linguistic
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detec-
tion. In Proceedings of the 27th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 297–304.
ACM.
Giridhar Kumaran and James Allan. 2005. Using
names and topics for new event detection. In Pro-
ceedings of the Conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 121–128. Association for
Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489–500. Association for Computational Linguis-
tics.
Pierre Mah´e, Nobuhisa Ueda, Tatsuya Akutsu, Jean-
Luc Perret, and Jean-Philippe Vert. 2005. Graph
kernels for molecular structure-activity relationship
analysis with support vector machines. Journal
of Chemical Information and Modeling, 45(4):939–
951.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3):347–
368.
Sauro Menchetti, Fabrizio Costa, and Paolo Frasconi.
2005. Weighted decomposition kernels. In Pro-
ceedings of the 22nd International Conference on
Machine Learning, pages 585–592. ACM.
James Pustejovsky, Jos´e Castano, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir Radev. 2003a. Timeml: Robust
specification of event and temporal expressions in
text. New Directions in Question Answering, 3:28–
34.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003b. The TimeBank corpus. In Cor-
pus Linguistics, volume 2003, page 40.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613–620.
</reference>
<page confidence="0.979437">
802
</page>
<reference confidence="0.999565617647059">
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the euro-
pean media monitor family of applications. In Pro-
ceedings of the Information Access in a Multilin-
gual World-Proceedings of the SIGIR 2009 Work-
shop, pages 1–8.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 Task 15: TempEval tempo-
ral relation identification. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 75–80. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 Task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57–
62. Association for Computational Linguistics.
Charles Wayne. 2000. Multilingual topic detection
and tracking: Successful research enabled by cor-
pora and evaluation. In Proceedings of the Second
International Conference on Language Resources
and Evaluation Conference (LREC 2000), volume
2000, pages 1487–1494.
Yiming Yang, Jaime G Carbonell, Ralf D Brown,
Thomas Pierce, Brian T Archibald, and Xin Liu.
1999. Learning approaches for detecting and track-
ing news events. Intelligent Systems and their Ap-
plications, IEEE, 14(4):32–43.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th Conference on Computational
linguistics, pages 947–953. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.999162">
803
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486548">
<title confidence="0.998367">Recognizing Identical Events with Graph Kernels</title>
<author confidence="0.750574">Glavaˇs</author>
<affiliation confidence="0.883271666666667">University of Faculty of Electrical Engineering and Text Analysis and Knowledge Engineering</affiliation>
<address confidence="0.732867">Unska 3, 10000 Zagreb,</address>
<abstract confidence="0.997300117647059">Identifying news stories that discuss the same real-world events is important for news tracking and retrieval. Most existing approaches rely on the traditional vector space model. We propose an approach for recognizing identical real-world events based on a structured, event-oriented document representation. We structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs. Our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
</authors>
<title>Topic Detection and Tracking: Event-based Information Organization,</title>
<date>2002</date>
<volume>12</volume>
<publisher>Kluwer Academic Pub.</publisher>
<contexts>
<context position="1150" citStr="Allan, 2002" startWordPosition="163" endWordPosition="164">ent-oriented document representation. We structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs. Our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events. 1 Introduction News stories typically describe real-world events. Topic detection and tracking (TDT) aims to detect stories that discuss identical or directly related events, and track these stories as they evolve over time (Allan, 2002). Being able to identify the stories that describe the same real-world event is essential for TDT, and event-based information retrieval in general. In TDT, an event is defined as something happening in a certain place at a certain time (Yang et al., 1999), while a topic is defined as a set of news stories related by some seminal real-world event (Allan, 2002). To identify news stories on the same topic, most TDT approaches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. </context>
</contexts>
<marker>Allan, 2002</marker>
<rawString>James Allan. 2002. Topic Detection and Tracking: Event-based Information Organization, volume 12. Kluwer Academic Pub.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Maintaining knowledge about temporal intervals.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="5668" citStr="Allen, 1983" startWordPosition="864" endWordPosition="865">s to measure the similarity between a pair of event graphs. 3.1 Event graphs An event graph is a vertex- and edge-labeled mixed graph in which vertices represent individual event mentions and edges represent temporal relations between event mentions. We adopt a generic representation of event mentions, as proposed by Glavaˇs and ˇSnajder (2013): each mention consists of an anchor (a word that conveys the core meaning) and four types of arguments (agent, target, time, location). Furthermore, we consider four types of temporal relations between event mentions: before, after, overlap, and equal (Allen, 1983). As relations overlap and equal are symmetric, whereas before and after are not, an event graph may contain both directed and undirected edges. Formally, an event graph G is represented as a tuple G = (V, E, A, m, r), where V is the set of vertices, E is the set of undirected edges, A is the set of directed edges (arcs), m : V -+ M is a bijection mapping the vertices to event mentions, and r : E -+ R is the edge-labeling function, assigning temporal relations to edges (cf. Fig. 1). The construction of an event graph from a news story involves the extraction of event mentions (anchors and argu</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>James Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11):832–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Atkinson</author>
<author>Erik Van der Goot</author>
</authors>
<title>Near real time information mining in multilingual news.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International Conference on World Wide Web,</booktitle>
<pages>1153--1154</pages>
<publisher>ACM.</publisher>
<marker>Atkinson, Van der Goot, 2009</marker>
<rawString>Martin Atkinson and Erik Van der Goot. 2009. Near real time information mining in multilingual news. In Proceedings of the 18th International Conference on World Wide Web, pages 1153–1154. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>A linguistic resource for discovering event structures and resolving event coreference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="7453" citStr="Bejan and Harabagiu, 2008" startWordPosition="1162" endWordPosition="1165">mentions. A detailed description of the graph construction steps is outside the scope of this paper. To compute event graph kernels (cf. Section 3.2), we need to determine whether two event mentions co-refer. To resolve cross-document event coreference, we use the model proposed by Glavaˇs and ˇSnajder (2013). The model determines coreference by comparing factual event anchors and arguments of four coarse-grained semantic types (agent, target, location, and time), and achieves an F-score of 67% (79% precision and 57% recall) on the cross-document mention pairs from the EventCorefBank dataset (Bejan and Harabagiu, 2008). In what follows, cf (m1, m2) denotes whether event mentions m1 and m2 corefer (equals 1 if mentions co-refer, 0 otherwise). 3.2 Graph kernels Graph kernels are fast polynomial alternatives to traditional graph comparison techniques (e.g., subgraph isomorphism), which provide an expressive measure of similarity between graphs (Borgwardt, 2007). We employ two different graph kernels: product graph kernel and weighted decomposition kernel. We chose these kernels because their general forms have intuitive interpretations for event matching. These particular kernels have shown to perform well on </context>
</contexts>
<marker>Bejan, Harabagiu, 2008</marker>
<rawString>Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A linguistic resource for discovering event structures and resolving event coreference. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Unsupervised event coreference resolution with rich linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1412--1422</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4698" citStr="Bejan and Harabagiu, 2010" startWordPosition="708" endWordPosition="711">ector for each of them. Kumaran and Allan (2004) represent news stories with three different vectors, modeling all words, named-entity words, and all non-named-entity words occurring in documents. When available, recognition of identical events can rely on meta-information associated with news stories, such as document creation time (DCT). Atkinson and Van der Goot (2009) combine DCT with VSM, assuming that temporally distant news stories are unlikely to describe the same event. In research on event extraction, the task of recognizing identical events is known as event coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012). There, however, the aim is to identify sentence-level event mentions referring to the same real-world events, and not stories that discuss identical events. 3 Kernels on Event Graphs To identify the news describing the same realworld event, we (1) structure event-oriented information from text into event graphs and (2) use graph kernels to measure the similarity between a pair of event graphs. 3.1 Event graphs An event graph is a vertex- and edge-labeled mixed graph in which vertices represent individual event mentions and edges represent temporal relations between event m</context>
</contexts>
<marker>Bejan, Harabagiu, 2010</marker>
<rawString>Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Unsupervised event coreference resolution with rich linguistic features. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1412–1422. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
</authors>
<title>Finding Event, Temporal and Causal Structure in Text: A Machine Learning Approach.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Colorado at Boulder.</institution>
<contexts>
<context position="6472" citStr="Bethard (2008)" startWordPosition="1012" endWordPosition="1013">a tuple G = (V, E, A, m, r), where V is the set of vertices, E is the set of undirected edges, A is the set of directed edges (arcs), m : V -+ M is a bijection mapping the vertices to event mentions, and r : E -+ R is the edge-labeling function, assigning temporal relations to edges (cf. Fig. 1). The construction of an event graph from a news story involves the extraction of event mentions (anchors and arguments) and the extraction of temporal relations between mentions. We use a supervised model (with 80% F1 extraction performance) based on a rich set of features similar to those proposed by Bethard (2008) to extract event anchors. We then employ a robust, rule-based approach proposed by Glavaˇs and ˇSnajder (2013) to extract generic event arguments. Finally, we employ a supervised model (60% micro-averaged F1 classification performance) with a rich set of features, similar to those proposed by Bethard (2008), to extract temporal relations between event mentions. A detailed description of the graph construction steps is outside the scope of this paper. To compute event graph kernels (cf. Section 3.2), we need to determine whether two event mentions co-refer. To resolve cross-document event core</context>
</contexts>
<marker>Bethard, 2008</marker>
<rawString>Steven Bethard. 2008. Finding Event, Temporal and Causal Structure in Text: A Machine Learning Approach. Ph.D. thesis, University of Colorado at Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karsten Michael Borgwardt</author>
</authors>
<title>Graph Kernels.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Ludwig-Maximilians-Universit¨at M¨unchen.</institution>
<contexts>
<context position="2597" citStr="Borgwardt, 2007" startWordPosition="394" endWordPosition="395">vsky et al., 2003b), as well as dedicated evaluation tasks (ACE, 2005; Verhagen et al., 2007; Verhagen et al., 2010). However, these two lines of research have largely remained isolated from one another. In this paper we bridge this gap and address the task of recognizing stories discussing identical events by considering structured representations from sentence-level events. More concretely, we structure news stories into event graphs built from individual event mentions extracted from text. To measure event-based similarity of news stories, we compare their event graphs using graph kernels (Borgwardt, 2007). We conduct preliminary experiments on two event-oriented tasks and show that the proposed approach can outperform traditional vector space model in recognizing identical real-world events. Moreover, we demonstrate that our approach is especially suitable for distinguishing between topically similar, yet non-identical real-world events. 2 Related Work The traditional vector space model (VSM) (Salton et al., 1975) computes the cosine between bag-ofwords representations of documents. The VSM is at the core of most approaches that identify sametopic news stories (Hatzivassiloglou et al., 2000; B</context>
<context position="7799" citStr="Borgwardt, 2007" startWordPosition="1215" endWordPosition="1217">g factual event anchors and arguments of four coarse-grained semantic types (agent, target, location, and time), and achieves an F-score of 67% (79% precision and 57% recall) on the cross-document mention pairs from the EventCorefBank dataset (Bejan and Harabagiu, 2008). In what follows, cf (m1, m2) denotes whether event mentions m1 and m2 corefer (equals 1 if mentions co-refer, 0 otherwise). 3.2 Graph kernels Graph kernels are fast polynomial alternatives to traditional graph comparison techniques (e.g., subgraph isomorphism), which provide an expressive measure of similarity between graphs (Borgwardt, 2007). We employ two different graph kernels: product graph kernel and weighted decomposition kernel. We chose these kernels because their general forms have intuitive interpretations for event matching. These particular kernels have shown to perform well on a number of tasks from chemoinformatics (Mah´e et al., 2005; Menchetti et al., 2005). Product graph kernel. A product graph kernel (PGK) counts the common walks between two input graphs (G¨artner et al., 2003). The graph product of two labeled graphs, G and G�, denoted GP = G x G&apos;, is a graph with the vertex set VP = {(v, v&apos;) |v E VG, v&apos; E VGA,</context>
</contexts>
<marker>Borgwardt, 2007</marker>
<rawString>Karsten Michael Borgwardt. 2007. Graph Kernels. Ph.D. thesis, Ludwig-Maximilians-Universit¨at M¨unchen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Francine Chen</author>
<author>Ayman Farahat</author>
</authors>
<title>A system for new event detection.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>330--337</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3215" citStr="Brants et al., 2003" startWordPosition="483" endWordPosition="486">). We conduct preliminary experiments on two event-oriented tasks and show that the proposed approach can outperform traditional vector space model in recognizing identical real-world events. Moreover, we demonstrate that our approach is especially suitable for distinguishing between topically similar, yet non-identical real-world events. 2 Related Work The traditional vector space model (VSM) (Salton et al., 1975) computes the cosine between bag-ofwords representations of documents. The VSM is at the core of most approaches that identify sametopic news stories (Hatzivassiloglou et al., 2000; Brants et al., 2003; Kumaran and Allan, 2005; Atkinson and Van der Goot, 2009). However, it has been observed that some word classes (e.g., named entities, noun phrases, collocations) have more significance than the others. Among them, named entities have been considered as particularly important, as they often identify the participants of an event. In view of this, Hatzivassiloglou et al. (2000) restrict the set of words to be used for document representation to words constituting noun phrases and named entities. Makkonen et 797 Proceedings of the 51st Annual Meeting of the Association for Computational Linguis</context>
</contexts>
<marker>Brants, Chen, Farahat, 2003</marker>
<rawString>Thorsten Brants, Francine Chen, and Ayman Farahat. 2003. A system for new event detection. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 330–337. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G¨artner</author>
<author>Peter Flach</author>
<author>Stefan Wrobel</author>
</authors>
<title>On graph kernels: Hardness results and efficient alternatives.</title>
<date>2003</date>
<booktitle>In Learning Theory and Kernel Machines,</booktitle>
<pages>129--143</pages>
<publisher>Springer.</publisher>
<marker>G¨artner, Flach, Wrobel, 2003</marker>
<rawString>Thomas G¨artner, Peter Flach, and Stefan Wrobel. 2003. On graph kernels: Hardness results and efficient alternatives. In Learning Theory and Kernel Machines, pages 129–143. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Goran Glavas</author>
<author>Jan Snajder</author>
</authors>
<title>Exploring coreference uncertainty of generically extracted event mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of 14th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>408--422</pages>
<publisher>Springer.</publisher>
<marker>Glavas, Snajder, 2013</marker>
<rawString>Goran Glava&amp;quot;s and Jan &amp;quot;Snajder. 2013. Exploring coreference uncertainty of generically extracted event mentions. In Proceedings of 14th International Conference on Intelligent Text Processing and Computational Linguistics, pages 408–422. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hammack</author>
<author>Wilfried Imrich</author>
<author>Sandi Klavzar</author>
</authors>
<title>Handbook of Product Graphs. Discrete Mathematics and Its Applications.</title>
<date>2011</date>
<publisher>CRC Press.</publisher>
<contexts>
<context position="8524" citStr="Hammack et al., 2011" startWordPosition="1344" endWordPosition="1348">e these kernels because their general forms have intuitive interpretations for event matching. These particular kernels have shown to perform well on a number of tasks from chemoinformatics (Mah´e et al., 2005; Menchetti et al., 2005). Product graph kernel. A product graph kernel (PGK) counts the common walks between two input graphs (G¨artner et al., 2003). The graph product of two labeled graphs, G and G�, denoted GP = G x G&apos;, is a graph with the vertex set VP = {(v, v&apos;) |v E VG, v&apos; E VGA, S(v, v&apos;)} 798 where S(v, v&apos;) is a predicate that holds when vertices v and v&apos; are identically labeled (Hammack et al., 2011). Given event graphs G = (V, E, A, m, r) and G&apos; = (V &apos;, E&apos;, A&apos;, m&apos;, r&apos;), we consider the vertices to be identically labeled if the corresponding event mentions co-refer, i.e., S(v, v&apos;) .= cf (m(v), m&apos;(v&apos;)). The edge set of the graph product depends on the type of the product. We experiment with two different products: tensor product and conormal product. In the tensor product, an edge is introduced iff the corresponding edges exist in both input graphs and the labels of those edges match (i.e., both edges represent the same temporal relation). In the conormal product, an edge is introduced iff</context>
</contexts>
<marker>Hammack, Imrich, Klavzar, 2011</marker>
<rawString>Richard Hammack, Wilfried Imrich, and Sandi Klavz&amp;quot;ar. 2011. Handbook of Product Graphs. Discrete Mathematics and Its Applications. CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Luis Gravano</author>
<author>Ankineedu Maganti</author>
</authors>
<title>An investigation of linguistic</title>
<date>2000</date>
<contexts>
<context position="3194" citStr="Hatzivassiloglou et al., 2000" startWordPosition="479" endWordPosition="482"> graph kernels (Borgwardt, 2007). We conduct preliminary experiments on two event-oriented tasks and show that the proposed approach can outperform traditional vector space model in recognizing identical real-world events. Moreover, we demonstrate that our approach is especially suitable for distinguishing between topically similar, yet non-identical real-world events. 2 Related Work The traditional vector space model (VSM) (Salton et al., 1975) computes the cosine between bag-ofwords representations of documents. The VSM is at the core of most approaches that identify sametopic news stories (Hatzivassiloglou et al., 2000; Brants et al., 2003; Kumaran and Allan, 2005; Atkinson and Van der Goot, 2009). However, it has been observed that some word classes (e.g., named entities, noun phrases, collocations) have more significance than the others. Among them, named entities have been considered as particularly important, as they often identify the participants of an event. In view of this, Hatzivassiloglou et al. (2000) restrict the set of words to be used for document representation to words constituting noun phrases and named entities. Makkonen et 797 Proceedings of the 51st Annual Meeting of the Association for </context>
</contexts>
<marker>Hatzivassiloglou, Gravano, Maganti, 2000</marker>
<rawString>Vasileios Hatzivassiloglou, Luis Gravano, and Ankineedu Maganti. 2000. An investigation of linguistic</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>James Allan</author>
</authors>
<title>Text classification and named entities for new event detection.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>297--304</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4121" citStr="Kumaran and Allan (2004)" startWordPosition="619" endWordPosition="622"> they often identify the participants of an event. In view of this, Hatzivassiloglou et al. (2000) restrict the set of words to be used for document representation to words constituting noun phrases and named entities. Makkonen et 797 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797–803, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics al. (2004) divide document terms into four semantic categories (locations, temporal expressions, proper names, and general terms) and construct separate vector for each of them. Kumaran and Allan (2004) represent news stories with three different vectors, modeling all words, named-entity words, and all non-named-entity words occurring in documents. When available, recognition of identical events can rely on meta-information associated with news stories, such as document creation time (DCT). Atkinson and Van der Goot (2009) combine DCT with VSM, assuming that temporally distant news stories are unlikely to describe the same event. In research on event extraction, the task of recognizing identical events is known as event coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012). Th</context>
</contexts>
<marker>Kumaran, Allan, 2004</marker>
<rawString>Giridhar Kumaran and James Allan. 2004. Text classification and named entities for new event detection. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 297–304. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>James Allan</author>
</authors>
<title>Using names and topics for new event detection.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>121--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3240" citStr="Kumaran and Allan, 2005" startWordPosition="487" endWordPosition="490">nary experiments on two event-oriented tasks and show that the proposed approach can outperform traditional vector space model in recognizing identical real-world events. Moreover, we demonstrate that our approach is especially suitable for distinguishing between topically similar, yet non-identical real-world events. 2 Related Work The traditional vector space model (VSM) (Salton et al., 1975) computes the cosine between bag-ofwords representations of documents. The VSM is at the core of most approaches that identify sametopic news stories (Hatzivassiloglou et al., 2000; Brants et al., 2003; Kumaran and Allan, 2005; Atkinson and Van der Goot, 2009). However, it has been observed that some word classes (e.g., named entities, noun phrases, collocations) have more significance than the others. Among them, named entities have been considered as particularly important, as they often identify the participants of an event. In view of this, Hatzivassiloglou et al. (2000) restrict the set of words to be used for document representation to words constituting noun phrases and named entities. Makkonen et 797 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797–803, Sofi</context>
</contexts>
<marker>Kumaran, Allan, 2005</marker>
<rawString>Giridhar Kumaran and James Allan. 2005. Using names and topics for new event detection. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 121–128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>489--500</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4717" citStr="Lee et al., 2012" startWordPosition="712" endWordPosition="715">aran and Allan (2004) represent news stories with three different vectors, modeling all words, named-entity words, and all non-named-entity words occurring in documents. When available, recognition of identical events can rely on meta-information associated with news stories, such as document creation time (DCT). Atkinson and Van der Goot (2009) combine DCT with VSM, assuming that temporally distant news stories are unlikely to describe the same event. In research on event extraction, the task of recognizing identical events is known as event coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012). There, however, the aim is to identify sentence-level event mentions referring to the same real-world events, and not stories that discuss identical events. 3 Kernels on Event Graphs To identify the news describing the same realworld event, we (1) structure event-oriented information from text into event graphs and (2) use graph kernels to measure the similarity between a pair of event graphs. 3.1 Event graphs An event graph is a vertex- and edge-labeled mixed graph in which vertices represent individual event mentions and edges represent temporal relations between event mentions. We adopt a</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 489–500. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Mah´e</author>
<author>Nobuhisa Ueda</author>
<author>Tatsuya Akutsu</author>
<author>JeanLuc Perret</author>
<author>Jean-Philippe Vert</author>
</authors>
<title>Graph kernels for molecular structure-activity relationship analysis with support vector machines.</title>
<date>2005</date>
<journal>Journal of Chemical Information and Modeling,</journal>
<volume>45</volume>
<issue>4</issue>
<pages>951</pages>
<marker>Mah´e, Ueda, Akutsu, Perret, Vert, 2005</marker>
<rawString>Pierre Mah´e, Nobuhisa Ueda, Tatsuya Akutsu, JeanLuc Perret, and Jean-Philippe Vert. 2005. Graph kernels for molecular structure-activity relationship analysis with support vector machines. Journal of Chemical Information and Modeling, 45(4):939– 951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juha Makkonen</author>
<author>Helena Ahonen-Myka</author>
<author>Marko Salmenkivi</author>
</authors>
<title>Simple semantics in topic detection and tracking.</title>
<date>2004</date>
<journal>Information Retrieval,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>368</pages>
<marker>Makkonen, Ahonen-Myka, Salmenkivi, 2004</marker>
<rawString>Juha Makkonen, Helena Ahonen-Myka, and Marko Salmenkivi. 2004. Simple semantics in topic detection and tracking. Information Retrieval, 7(3):347– 368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sauro Menchetti</author>
<author>Fabrizio Costa</author>
<author>Paolo Frasconi</author>
</authors>
<title>Weighted decomposition kernels.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning,</booktitle>
<pages>585--592</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8137" citStr="Menchetti et al., 2005" startWordPosition="1266" endWordPosition="1269"> m1 and m2 corefer (equals 1 if mentions co-refer, 0 otherwise). 3.2 Graph kernels Graph kernels are fast polynomial alternatives to traditional graph comparison techniques (e.g., subgraph isomorphism), which provide an expressive measure of similarity between graphs (Borgwardt, 2007). We employ two different graph kernels: product graph kernel and weighted decomposition kernel. We chose these kernels because their general forms have intuitive interpretations for event matching. These particular kernels have shown to perform well on a number of tasks from chemoinformatics (Mah´e et al., 2005; Menchetti et al., 2005). Product graph kernel. A product graph kernel (PGK) counts the common walks between two input graphs (G¨artner et al., 2003). The graph product of two labeled graphs, G and G�, denoted GP = G x G&apos;, is a graph with the vertex set VP = {(v, v&apos;) |v E VG, v&apos; E VGA, S(v, v&apos;)} 798 where S(v, v&apos;) is a predicate that holds when vertices v and v&apos; are identically labeled (Hammack et al., 2011). Given event graphs G = (V, E, A, m, r) and G&apos; = (V &apos;, E&apos;, A&apos;, m&apos;, r&apos;), we consider the vertices to be identically labeled if the corresponding event mentions co-refer, i.e., S(v, v&apos;) .= cf (m(v), m&apos;(v&apos;)). The ed</context>
<context position="9977" citStr="Menchetti et al. (2005)" startWordPosition="1607" endWordPosition="1610"> G and G&apos;. The product graph kernel that counts common walks in G and G&apos; can be computed efficiently as: KPG(G, G&apos;) = � |VP |[(I − λAP)−1]ij (1) i,j=1 when λ &lt; 1/t, where t is the maximum degree of a vertex in the graph product GP. In our experiments, we set λ to 1/(t + 1). Weighted decomposition kernel. A weighted decomposition kernel (WDK) compares small graph parts, called selectors, being matched according to an equality predicate. The importance of the match is weighted by the similarity of the contexts in which the matched selectors occur. For a description of a general form of WDK, see Menchetti et al. (2005). Let S(G) be the set of all pairs (s, z), where s is the selector (subgraph of interest) and z is the context of s. We decompose event graphs into individual vertices, i.e., we define selectors to be the individual vertices. In this case, similarly as above, the equality predicate S(v, v&apos;) for two vertices v E G and v&apos; E G&apos; holds if and only if the corresponding event mentions m(v) and m&apos;(v&apos;) co-refer. Using selectors that consist of more than one vertex would require a more complex and perhaps a less intuitive definition of the equality predicate S. The selector context Zv of vertex v is a s</context>
</contexts>
<marker>Menchetti, Costa, Frasconi, 2005</marker>
<rawString>Sauro Menchetti, Fabrizio Costa, and Paolo Frasconi. 2005. Weighted decomposition kernels. In Proceedings of the 22nd International Conference on Machine Learning, pages 585–592. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jos´e Castano</author>
<author>Robert Ingria</author>
<author>Roser Sauri</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
<author>Dragomir Radev</author>
</authors>
<title>Timeml: Robust specification of event and temporal expressions in text. New Directions in Question Answering,</title>
<date>2003</date>
<volume>3</volume>
<pages>34</pages>
<contexts>
<context position="1957" citStr="Pustejovsky et al., 2003" startWordPosition="298" endWordPosition="302">ething happening in a certain place at a certain time (Yang et al., 1999), while a topic is defined as a set of news stories related by some seminal real-world event (Allan, 2002). To identify news stories on the same topic, most TDT approaches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. On the other hand, significant advances in sentence-level event extraction have been made over the last decade, in particular as the result of standardization efforts such as TimeML (Pustejovsky et al., 2003a) and TimeBank (Pustejovsky et al., 2003b), as well as dedicated evaluation tasks (ACE, 2005; Verhagen et al., 2007; Verhagen et al., 2010). However, these two lines of research have largely remained isolated from one another. In this paper we bridge this gap and address the task of recognizing stories discussing identical events by considering structured representations from sentence-level events. More concretely, we structure news stories into event graphs built from individual event mentions extracted from text. To measure event-based similarity of news stories, we compare their event grap</context>
</contexts>
<marker>Pustejovsky, Castano, Ingria, Sauri, Gaizauskas, Setzer, Katz, Radev, 2003</marker>
<rawString>James Pustejovsky, Jos´e Castano, Robert Ingria, Roser Sauri, Robert Gaizauskas, Andrea Setzer, Graham Katz, and Dragomir Radev. 2003a. Timeml: Robust specification of event and temporal expressions in text. New Directions in Question Answering, 3:28– 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
</authors>
<title>The TimeBank corpus.</title>
<date>2003</date>
<booktitle>In Corpus Linguistics,</booktitle>
<volume>volume</volume>
<pages>40</pages>
<contexts>
<context position="1957" citStr="Pustejovsky et al., 2003" startWordPosition="298" endWordPosition="302">ething happening in a certain place at a certain time (Yang et al., 1999), while a topic is defined as a set of news stories related by some seminal real-world event (Allan, 2002). To identify news stories on the same topic, most TDT approaches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. On the other hand, significant advances in sentence-level event extraction have been made over the last decade, in particular as the result of standardization efforts such as TimeML (Pustejovsky et al., 2003a) and TimeBank (Pustejovsky et al., 2003b), as well as dedicated evaluation tasks (ACE, 2005; Verhagen et al., 2007; Verhagen et al., 2010). However, these two lines of research have largely remained isolated from one another. In this paper we bridge this gap and address the task of recognizing stories discussing identical events by considering structured representations from sentence-level events. More concretely, we structure news stories into event graphs built from individual event mentions extracted from text. To measure event-based similarity of news stories, we compare their event grap</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, et al. 2003b. The TimeBank corpus. In Corpus Linguistics, volume 2003, page 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Anita Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="1639" citStr="Salton et al., 1975" startWordPosition="249" endWordPosition="252">to detect stories that discuss identical or directly related events, and track these stories as they evolve over time (Allan, 2002). Being able to identify the stories that describe the same real-world event is essential for TDT, and event-based information retrieval in general. In TDT, an event is defined as something happening in a certain place at a certain time (Yang et al., 1999), while a topic is defined as a set of news stories related by some seminal real-world event (Allan, 2002). To identify news stories on the same topic, most TDT approaches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. On the other hand, significant advances in sentence-level event extraction have been made over the last decade, in particular as the result of standardization efforts such as TimeML (Pustejovsky et al., 2003a) and TimeBank (Pustejovsky et al., 2003b), as well as dedicated evaluation tasks (ACE, 2005; Verhagen et al., 2007; Verhagen et al., 2010). However, these two lines of research have largely remained isolated from one another. In this paper we bridge this gap and address the task </context>
<context position="3014" citStr="Salton et al., 1975" startWordPosition="451" endWordPosition="454"> stories into event graphs built from individual event mentions extracted from text. To measure event-based similarity of news stories, we compare their event graphs using graph kernels (Borgwardt, 2007). We conduct preliminary experiments on two event-oriented tasks and show that the proposed approach can outperform traditional vector space model in recognizing identical real-world events. Moreover, we demonstrate that our approach is especially suitable for distinguishing between topically similar, yet non-identical real-world events. 2 Related Work The traditional vector space model (VSM) (Salton et al., 1975) computes the cosine between bag-ofwords representations of documents. The VSM is at the core of most approaches that identify sametopic news stories (Hatzivassiloglou et al., 2000; Brants et al., 2003; Kumaran and Allan, 2005; Atkinson and Van der Goot, 2009). However, it has been observed that some word classes (e.g., named entities, noun phrases, collocations) have more significance than the others. Among them, named entities have been considered as particularly important, as they often identify the participants of an event. In view of this, Hatzivassiloglou et al. (2000) restrict the set o</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
<author>Erik Van Der Goot</author>
</authors>
<title>An introduction to the european media monitor family of applications.</title>
<date>2009</date>
<booktitle>In Proceedings of the Information Access in a Multilingual World-Proceedings of the SIGIR 2009 Workshop,</booktitle>
<pages>1--8</pages>
<marker>Steinberger, Pouliquen, Van Der Goot, 2009</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, and Erik Van Der Goot. 2009. An introduction to the european media monitor family of applications. In Proceedings of the Information Access in a Multilingual World-Proceedings of the SIGIR 2009 Workshop, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2007 Task 15: TempEval temporal relation identification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>75--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2073" citStr="Verhagen et al., 2007" startWordPosition="317" endWordPosition="320">ories related by some seminal real-world event (Allan, 2002). To identify news stories on the same topic, most TDT approaches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. On the other hand, significant advances in sentence-level event extraction have been made over the last decade, in particular as the result of standardization efforts such as TimeML (Pustejovsky et al., 2003a) and TimeBank (Pustejovsky et al., 2003b), as well as dedicated evaluation tasks (ACE, 2005; Verhagen et al., 2007; Verhagen et al., 2010). However, these two lines of research have largely remained isolated from one another. In this paper we bridge this gap and address the task of recognizing stories discussing identical events by considering structured representations from sentence-level events. More concretely, we structure news stories into event graphs built from individual event mentions extracted from text. To measure event-based similarity of news stories, we compare their event graphs using graph kernels (Borgwardt, 2007). We conduct preliminary experiments on two event-oriented tasks and show th</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007. Semeval-2007 Task 15: TempEval temporal relation identification. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 75–80. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<title>57– 62. Association for Computational Linguistics.</title>
<date>2010</date>
<booktitle>Semeval-2010 Task 13: TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>pages</pages>
<contexts>
<context position="2097" citStr="Verhagen et al., 2010" startWordPosition="321" endWordPosition="324">eminal real-world event (Allan, 2002). To identify news stories on the same topic, most TDT approaches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. On the other hand, significant advances in sentence-level event extraction have been made over the last decade, in particular as the result of standardization efforts such as TimeML (Pustejovsky et al., 2003a) and TimeBank (Pustejovsky et al., 2003b), as well as dedicated evaluation tasks (ACE, 2005; Verhagen et al., 2007; Verhagen et al., 2010). However, these two lines of research have largely remained isolated from one another. In this paper we bridge this gap and address the task of recognizing stories discussing identical events by considering structured representations from sentence-level events. More concretely, we structure news stories into event graphs built from individual event mentions extracted from text. To measure event-based similarity of news stories, we compare their event graphs using graph kernels (Borgwardt, 2007). We conduct preliminary experiments on two event-oriented tasks and show that the proposed approach</context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. Semeval-2010 Task 13: TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 57– 62. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Wayne</author>
</authors>
<title>Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation Conference (LREC</booktitle>
<volume>volume</volume>
<pages>1487--1494</pages>
<contexts>
<context position="13768" citStr="Wayne, 2000" startWordPosition="2285" endWordPosition="2286"> v&apos;) = 3 + 4 + 4 ≈ 1.9 2 3 2 (v,v&apos;)EVP where VP contains pairs of coreferent event mentions: (yanked, stolen), (recovered, recovered), and (arrests, arrested). 4 Experiments We conducted two preliminary experiments to investigate whether kernels on event graphs can be used to recognize identical events. 4.1 Task 1: Recognizing identical events Dataset. In the first experiment, we classify pairs of news stories as either describing identical real-world events or not. For this we need a collection of stories in which pairs of stories on identical events have been annotated as such. TDT corpora (Wayne, 2000) is not directly usable because it has no such annotations. We therefore decided to build a small annotated dataset.1 To this end, we use the news clusters of the EMM NewsBrief service (Steinberger et al., 2009). EMM clusters news stories from different sources using a document similarity score. We acquired 10 randomly chosen news clusters, manually inspected each of them, and retained in each cluster only the documents that describe the same real-world events. Additionally, we ensured that no documents from 1Datasets for both experiments are available at: http://takelab.fer.hr/evkernels Model</context>
</contexts>
<marker>Wayne, 2000</marker>
<rawString>Charles Wayne. 2000. Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation. In Proceedings of the Second International Conference on Language Resources and Evaluation Conference (LREC 2000), volume 2000, pages 1487–1494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jaime G Carbonell</author>
<author>Ralf D Brown</author>
<author>Thomas Pierce</author>
<author>Brian T Archibald</author>
<author>Xin Liu</author>
</authors>
<title>Learning approaches for detecting and tracking news events. Intelligent Systems and their Applications,</title>
<date>1999</date>
<pages>14--4</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="1406" citStr="Yang et al., 1999" startWordPosition="208" endWordPosition="211">ional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events. 1 Introduction News stories typically describe real-world events. Topic detection and tracking (TDT) aims to detect stories that discuss identical or directly related events, and track these stories as they evolve over time (Allan, 2002). Being able to identify the stories that describe the same real-world event is essential for TDT, and event-based information retrieval in general. In TDT, an event is defined as something happening in a certain place at a certain time (Yang et al., 1999), while a topic is defined as a set of news stories related by some seminal real-world event (Allan, 2002). To identify news stories on the same topic, most TDT approaches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. On the other hand, significant advances in sentence-level event extraction have been made over the last decade, in particular as the result of standardization efforts such as TimeML (Pustejovsky et al., 2003a) and TimeBank (Pustejovsky et al., 2003b), as w</context>
</contexts>
<marker>Yang, Carbonell, Brown, Pierce, Archibald, Liu, 1999</marker>
<rawString>Yiming Yang, Jaime G Carbonell, Ralf D Brown, Thomas Pierce, Brian T Archibald, and Xin Liu. 1999. Learning approaches for detecting and tracking news events. Intelligent Systems and their Applications, IEEE, 14(4):32–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational linguistics,</booktitle>
<pages>947--953</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18737" citStr="Yeh, 2000" startWordPosition="3071" endWordPosition="3072">ked the pairs based on the assigned similarity scores. An ideal method would rank all positive pairs above all negative pairs. We evaluated the performance using Model R-prec. Avg. prec. Tensor PGK 86.7 96.8 Conormal PGK 93.3 97.5 WDK 86.7 95.7 VSM baseline 80.0 77.1 Table 3: Results for event-based similarity ranking two different rank evaluation metrics: R-precision (precision at rank 30, as there are 30 positive pairs) and average precision. The performance of graph kernel models and the VSM baseline is given in Table 3. We tested the significance of differences using stratified shuffling (Yeh, 2000). When considering average precision, all kernel models significantly (at p &lt; 0.01) outperform the baseline. However, when considering R-precision, only the conormal PGK model significantly (at p &lt; 0.05) outperforms the baseline. There is no statistical significance in performance differences between the considered kernel methods. Inspection of the rankings reveals that graph kernels assign very low scores to negative pairs, i.e., they distinguish well between textual representations of topically similar, but different real-world events. 5 Conclusion We proposed a novel approach for recognizin</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th Conference on Computational linguistics, pages 947–953. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>