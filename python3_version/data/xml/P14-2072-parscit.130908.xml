<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015528">
<title confidence="0.988734">
Cross-cultural Deception Detection
</title>
<author confidence="0.990565">
Ver´onica P´erez-Rosas Rada Mihalcea
</author>
<affiliation confidence="0.9981675">
Computer Science and Engineering Computer Science and Engineering
University of North Texas University of Michigan
</affiliation>
<email confidence="0.996931">
veronicaperezrosas@my.unt.edu mihalcea@umich.edu
</email>
<sectionHeader confidence="0.993841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890625">
In this paper, we address the task of
cross-cultural deception detection. Using
crowdsourcing, we collect three deception
datasets, two in English (one originating
from United States and one from India),
and one in Spanish obtained from speakers
from Mexico. We run comparative experi-
ments to evaluate the accuracies of decep-
tion classifiers built for each culture, and
also to analyze classification differences
within and across cultures. Our results
show that we can leverage cross-cultural
information, either through translation or
equivalent semantic categories, and build
deception classifiers with a performance
ranging between 60-70%.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968969696969">
The identification of deceptive behavior is a task
that has gained increasing interest from researchers
in computational linguistics. This is mainly moti-
vated by the rapid growth of deception in written
sources, and in particular in Web content, including
product reviews, online dating profiles, and social
networks posts (Ott et al., 2011).
To date, most of the work presented on deception
detection has focused on the identification of deceit
clues within a specific language, where English is
the most commonly studied language. However, a
large portion of the written communication (e.g.,
e-mail, chats, forums, blogs, social networks) oc-
curs not only between speakers of English, but also
between speakers from other cultural backgrounds,
which poses important questions regarding the ap-
plicability of existing deception tools. Issues such
as language, beliefs, and moral values may influ-
ence the way people deceive, and therefore may
have implications on the construction of tools for
deception detection.
In this paper, we explore within- and across-
culture deception detection for three different cul-
tures, namely United States, India, and Mexico.
Through several experiments, we compare the per-
formance of classifiers that are built separately for
each culture, and classifiers that are applied across
cultures, by using unigrams and word categories
that can act as a cross-lingual bridge. Our results
show that we can achieve accuracies in the range of
60-70%, and that we can leverage resources avail-
able in one language to build deception tools for
another language.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998935481481481">
Research to date on automatic deceit detection has
explored a wide range of applications such as the
identification of spam in e-mail communication,
the detection of deceitful opinions in review web-
sites, and the identification of deceptive behavior
in computer-mediated communication including
chats, blogs, forums and online dating sites (Peng
et al., 2011; Toma et al., 2008; Ott et al., 2011;
Toma and Hancock, 2010; Zhou and Shi, 2008).
Techniques used for deception detection fre-
quently include word-based stylometric analysis.
Linguistic clues such as n-grams, count of used
words and sentences, word diversity, and self-
references are also commonly used to identify de-
ception markers. An important resource that has
been used to represent semantic information for the
deception task is the Linguistic Inquiry and Word
Count (LIWC) dictionary (Pennebaker and Francis,
1999). LIWC provides words grouped into seman-
tic categories relevant to psychological processes,
which have been used successfully to perform lin-
guistic profiling of true tellers and liars (Zhou et al.,
2003; Newman et al., 2003; Rubin, 2010). In addi-
tion to this, features derived from syntactic Context
Free Grammar parse trees, and part of speech have
also been found to aid the deceit detection (Feng et
al., 2012; Xu and Zhao, 2012).
</bodyText>
<page confidence="0.966411">
440
</page>
<bodyText confidence="0.958155387096774">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
While most of the studies have focused on En-
glish, there is a growing interest in studying decep-
tion for other languages. For instance, (Fornaciari
and Poesio, 2013) identified deception in Italian by
analyzing court cases. The authors explored several
strategies for identifying deceptive clues, such as
utterance length, LIWC features, lemmas and part
of speech patterns. (Almela et al., 2012) studied the
deception detection in Spanish text by using SVM
classifiers and linguistic categories, obtained from
the Spanish version of the LIWC dictionary. A
study on Chinese deception is presented in (Zhang
et al., 2009), where the authors built a deceptive
dataset using Internet news and performed machine
learning experiments using a bag-of-words repre-
sentation to train a classifier able to discriminate
between deceptive and truthful cases.
It is also worth mentioning the work conducted
to analyze cross-cultural differences. (Lewis and
George, 2008) presented a study of deception in
social networks sites and face-to-face communi-
cation, where authors compare deceptive behavior
of Korean and American participants, with a sub-
sequent study also considering the differences be-
tween Spanish and American participants (Lewis
and George, 2009). In general, research findings
suggest a strong relation between deception and
cultural aspects, which are worth exploring with
automatic methods.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.999927470588235">
We collect three datasets for three different cul-
tures: United States (English-US), India (English-
India), and Mexico (Spanish-Mexico). Following
(Mihalcea and Strapparava, 2009), we collect short
deceptive and truthful essays for three topics: opin-
ions on Abortion, opinions on Death Penalty, and
feelings about a Best Friend.
For English-US and English-India, we use Ama-
zon Mechanical Turk with a location restriction, so
that all the contributors are from the country of in-
terest (US and India). We collect 100 deceptive and
100 truthful statements for each of the three topics.
To avoid spam, each contribution is manually veri-
fied by one of the authors of this paper.For Spanish-
Mexico, while we initially attempted to collect data
also using Mechanical Turk, we were not able to
receive enough contributions. We therefore cre-
ated a separate web interface to collect data, and
recruited participants through contacts of the pa-
per’s authors. The overall process was significantly
more time consuming than for the other two cul-
tures, and resulted in fewer contributions, namely
39+39 statements for Abortion, 42+42 statements
for Death Penalty, and 94+94 statements for Best
Friend. For all three cultures, the participants first
provided their truthful responses, followed by the
deceptive ones.
Interestingly, for all three cultures, the average
number of words for the deceptive statements (62
words) is significantly smaller than for the truthful
statements (81 words), which may be explained by
the added difficulty of the deceptive process, and
is in line with previous observations about the cues
of deception (DePaulo et al., 2003).
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999986384615385">
Through our experiments, we seek answers to the
following questions. First, what is the perfor-
mance for deception classifiers built for different
cultures? Second, can we use information drawn
from one culture to build a deception classifier for
another culture? Finally, what are the psycholin-
guistic classes most strongly associated with de-
ception/truth, and are there commonalities or dif-
ferences among languages?
In all our experiments, we formulate the decep-
tion detection task in a machine learning frame-
work, where we use an SVM classifier to discrimi-
nate between deceptive and truthful statements.1
</bodyText>
<subsectionHeader confidence="0.613256">
4.1 What is the performance for deception
</subsectionHeader>
<bodyText confidence="0.99789985">
classifiers built for different cultures?
We represent the deceptive and truthful statements
using two different sets of features. First we use
unigrams obtained from the statements correspond-
ing to each topic and each culture. To select the
unigrams, we use a threshold of 10, where all the
unigrams with a frequency less than 10 are dropped.
Since previous research suggested that stopwords
can contain linguistic clues for deception, no stop-
word removal is performed.
Experiments are performed using a ten-fold
cross validation evaluation on each dataset.Using
the same unigram features, we also perform cross-
topic classification, so that we can better under-
stand the topic dependence. For this, we train
the SVM classifier on training data consisting of a
merge of two topics (e.g., Abortion + Best Friend)
and test on the third topic (e.g., Death Penalty). The
results for both within- and cross-topic are shown
in the last two columns of Table 1.
</bodyText>
<footnote confidence="0.8833105">
1We use the SVM classifier implemented in the Weka
toolkit, with its default settings.
</footnote>
<page confidence="0.99183">
441
</page>
<table confidence="0.999708764705882">
Topic LIWC Unigrams
Linguistic Psychological Relativity Personal All Within-topic Cross-topic
English-US
Abortion 72.50% 68.75% 44.37% 67.50% 73.03% 63.75% 80.36%
Best Friend 75.98% 68.62% 58.33% 54.41% 73.03% 74.50% 60.78%
Death Penalty 60.36% 54.50% 49.54% 50.45% 58.10% 58.10% 77.23%
Average 69.61% 63.96% 50.75% 57.45% 69.05% 65.45% 72.79%
English-India
Abortion 56.00% 48.50% 46.50% 48.50% 56.00% 46.00% 50.00%
Best Friend 68.18% 68.62% 54.55% 53.18% 71.36% 60.45% 57.23%
Death Penalty 56.00% 52.84% 57.50% 53.50% 63.50% 57.50% 54.00%
Average 60.06% 59.19% 52.84% 51.72% 63.62% 54.65% 53.74%
Spanish-Mexico
Abortion 73.17% 67.07% 48.78% 51.22% 62.20% 52.46% 57.69%
Best Friend 72.04% 74.19% 67.20% 54.30% 75.27% 66.66% 50.53%
Death Penalty 73.17% 67.07% 48.78% 51.22% 62.20% 54.87% 63.41%
Average 72.79% 69.45% 54.92% 52.25% 67.89% 57.99% 57.21%
</table>
<tableCaption confidence="0.999677">
Table 1: Within-culture classification, using LIWC word classes and unigrams. For LIWC, results are
</tableCaption>
<bodyText confidence="0.954875909090909">
shown for within-topic experiments, with ten-fold cross validation. For unigrams, both within-topic
(ten-fold cross validation on the same topic) and cross-topic (training on two topics and testing on the
third topic) results are reported.
Second, we use the LIWC lexicon to extract fea-
tures corresponding to several word classes. LIWC
was developed as a resource for psycholinguistic
analysis (Pennebaker and Francis, 1999). The 2001
version of LIWC includes about 2,200 words and
word stems grouped into about 70 classes relevant
to psychological processes (e.g., emotion, cogni-
tion), which in turn are grouped into four broad cat-
egories2 namely: linguistic processes, psychologi-
cal processes, relativity, and personal concerns. A
feature is generated for each of the 70 word classes
by counting the total frequency of the words belong-
ing to that class. We perform separate evaluations
using each of the four broad LIWC categories, as
well as using all the categories together. The re-
sults obtained with the SVM classifier are shown
in Table 1.
Overall, the results show that it is possible to
discriminate between deceptive and truthful cases
using machine learning classifiers, with a perfor-
mance superior to a random baseline which for all
datasets is 50% given an even class distribution.
Considering the unigram results, among the three
cultures considered, the deception discrimination
works best for the English-US dataset, and this is
also the dataset that benefits most from the larger
amount of training data brought by the cross-topic
experiments. In general, the cross-topic evaluations
suggest that there is no high topic dependence in
this task, and that using deception data from differ-
</bodyText>
<footnote confidence="0.718812">
2http://www.liwc.net/descriptiontable1.php
</footnote>
<bodyText confidence="0.9854284">
ent topics can lead to results that are comparable
to the within-topic data. Interestingly, among the
three topics considered, the Best Friend topic has
consistently the highest within-topic performance,
which may be explained by the more personal na-
ture of the topic, which can lead to clues that are
useful for the detection of deception (e.g., refer-
ences to the self or personal relationships).
Regarding the LIWC classifiers, the results show
that the use of the LIWC classes can lead to per-
formance that is generally better than the one ob-
tained with the unigram classifiers. The explicit cat-
egorization of words into psycholinguistic classes
seems to be particularly useful for the languages
where the words by themselves did not lead to very
good classification accuracies. Among the four
broad LIWC categories, the linguistic category ap-
pears to lead to the best performance as compared
to the other categories. It is notable that in Spanish,
the linguistic category by itself provides results that
are better than when all the LIWC classes are used,
which may be due to the fact that Spanish has more
explicit lexicalization for clues that may be relevant
to deception (e.g., verb tenses, formality).
4.2 Can we use information drawn from one
culture to build a deception classifier in
another culture?
In the next set of experiments, we explore the de-
tection of deception using training data originating
from a different culture. As with the within-culture
</bodyText>
<page confidence="0.996615">
442
</page>
<table confidence="0.999830666666667">
Topic Linguistic Psychological Relativity Personal All LIWC Unigrams
Training: English-US Test: English-India
Abortion 58.00% 51.00% 48.50% 51.50% 52.25% 57.89%
Best Friend 66.36% 47.27% 48.64% 50.45% 59.54% 51.00%
Death Penalty 54.50% 50.50% 50.00% 48.50% 53.5% 59.00%
Average 59.62% 49.59% 49.05% 50.15% 55.10% 55.96%
Training: English-India Test: English-US
Abortion 71.32% 47.49% 43.38% 45.82% 62.50% 55.51%
Best Friend 59.74% 49.35% 51.94% 49.36% 55.84% 53.20%
Death Penalty 51.47% 44.11% 54.88% 50.98% 39.21% 50.71%
Average 60.87% 46.65% 50.06% 48.72% 52.51% 54.14%
Training: English-US Test: Spanish-Mexico
Abortion 70.51% 46.15% 50.00% 52.56% 53.85% 61.53%
Best Friend 69.35% 52.69% 51.08% 46.77% 67.74% 65.03%
Death Penalty 54.88% 54.88% 53.66% 50.00% 62.19% 59.75%
Average 64.92% 51.24% 51.58% 49.78% 61.26% 62.10%
Training: English-India Test: Spanish-Mexico
Abortion 48.72% 50.00% 47.44% 42.31% 43.58% 55.12 %
Best Friend 68.28% 63.44% 56.45% 54.84% 60.75% 67.20%
Death Penalty 60.98% 53.66% 54.88% 60.98% 59.75% 51.21%
Average 59.32% 55.70% 52.92% 52.71% 54.69% 57.84%
</table>
<tableCaption confidence="0.999274">
Table 2: Cross-cultural experiments using LIWC categories and unigrams
</tableCaption>
<bodyText confidence="0.999080689655172">
experiments, we use unigrams and LIWC features.
For consistency across the experiments, given that
the size of the Spanish dataset is different com-
pared to the other two datasets, we always train on
one of the English datasets.
To enable the unigram based experiments, we
translate the two English datasets into Spanish by
using the Bing API for automatic translation.3 As
before, we extract and keep only the unigrams
with frequency greater or equal to 10. The results
obtained in these cross-cultural experiments are
shown in the last column of Table 2.
In a second set of experiments, we use the LIWC
word classes as a bridge between languages. First,
each deceptive or truthful statement is represented
using features based on the LIWC word classes.
Next, since the same word classes are used in both
the English and the Spanish LIWC lexicons, this
LIWC-based representation is independent of lan-
guage, and therefore can be used to perform cross-
cultural experiments. Table 2 shows the results
obtained with each of the four broad LIWC cate-
gories, as well as with all the LIWC word classes.
We also attempted to combine unigrams and
LIWC features. However, in most cases, no im-
provements were noticed with respect to the use
of unigrams or LIWC features alone. We are not
reporting these results due to space limitation.
These cross-cultural evaluations lead to several
</bodyText>
<footnote confidence="0.70665">
3http://http://http://www.bing.com/dev/en-us/dev-center
</footnote>
<bodyText confidence="0.995805">
findings. First, we can use data from a culture
to build deception classifiers for another culture,
with performance figures better than the random
baseline, but weaker than the results obtained with
within-culture data. An important finding is that
LIWC can be effectively used as a bridge for cross-
cultural classification, with results that are com-
parable to the use of unigrams, which suggests
that such specialized lexicons can be used for
cross-cultural or cross-lingual classification. More-
over, using only the linguistic category from LIWC
brings additional improvements, with absolute im-
provements of 2-4% over the use of unigrams. This
is an encouraging result, as it implies that a seman-
tic bridge such as LIWC can be effectively used
to classify deception data in other languages, in-
stead of using the more costly and time consuming
unigram method based on translations.
4.3 What are the psycholinguistic classes
most strongly associated with
deception/truth?
The final question we address is concerned with
the LIWC classes that are dominant in deceptive
and truthful text for different cultures. We use the
method presented in (Mihalcea and Strapparava,
2009), which consists of a metric that measures the
saliency of LIWC classes in deceptive versus truth-
ful data. Following their strategy, we first create a
corpus of deceptive and truthful text using a mix
of all the topics in each culture. We then calculate
</bodyText>
<page confidence="0.997934">
443
</page>
<table confidence="0.999974695652174">
Class Score Sample words Class Score Sample words
English-US
Deceptive Truthful
Metaph 1.77 Die,died,hell,sin,lord Insight 0.68 Accept,believe,understand
Other 1.46 He,her,herself,him I 0.66 I,me,my,myself,
You 1.41 Thou,you Optimism 0.65 accept, hope, top, best
Othref 1.18 He,her,herself,him We 0.55 Our,ourselves,us,we,
Negemo 1.18 Afraid,agony,awful,bad Friends 0.46 Buddies,friend
English-India
Deceptive Truthful
Negate 1.49 Cannot,neither,no,none Past 0.78 Happened,helped,liked,listened
Physical 1.46 Heart,ill,love,loved, I 0.66 I,me,mine,my
Future 1.42 Be,may,might,will Optimism 0.65 Accept,accepts,best,bold,
Other 1.17 He,she, himself,herself We 0.55 Our,ourselves,us,we
Humans 1.08 Adult,baby,children,human Friends 0.46 Buddies,companion,friend,pal
Spanish
-Mexico
Deceptive Truthful
Certain 1.47 Jam´as(never),siempre(always) Optimism 0.66 Aceptar(accept),animar(cheer)
Humans 1.28 Beb´e(baby),persona(person) Self 0.65 Conmigo(me),tengo(have),soy(am)
You 1.26 Eres(are),estas(be),su(his/her) We 0.58 Estamos(are),somos(be),tenemos(have)
Negate 1.25 Jam´as(never),tampoco(neither) Friends 0.37 Amigo/amiga(friend),amistad(friendship)
Other 1.22 Es(is),esta(are),otro(other) Past 0.32 Compartimos(share),vivimos(lived)
</table>
<tableCaption confidence="0.999954">
Table 3: Top ranked LIWC classes for each culture, along with sample words
</tableCaption>
<bodyText confidence="0.9998389">
the dominance for each LIWC class, and rank the
classes in reversed order of their dominance score.
Table 3 shows the most salient classes for each
culture, along with sample words.
This analysis shows some interesting patterns.
There are several classes that are shared among the
cultures. For instance, the deceivers in all cultures
make use of negation, negative emotions, and refer-
ences to others. Second, true tellers use more opti-
mism and friendship words, as well as references to
themselves. These results are in line with previous
research, which showed that LIWC word classes
exhibit similar trends when distinguishing between
deceptive and non-deceptive text (Newman et al.,
2003). Moreover, there are also word classes that
only appear in some of the cultures; for example,
time classes (Past, Future) appear in English-India
and Spanish-Mexico, but not in English-US, which
in turn contains other classes such as Insight and
Metaph.
</bodyText>
<sectionHeader confidence="0.999631" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999881592592593">
In this paper, we addressed the task of deception
detection within- and across-cultures. Using three
datasets from three different cultures, each cover-
ing three different topics, we conducted several
experiments to evaluate the accuracy of deception
detection when learning from data from the same
culture or from a different culture. In our evalua-
tions, we compared the use of unigrams versus the
use of psycholinguistic word classes.
The main findings from these experiments are:
1) We can build deception classifiers for different
cultures with accuracies ranging between 60-70%,
with better performance obtained when using psy-
cholinguistic word classes as compared to simple
unigrams; 2) The deception classifiers are not sen-
sitive to different topics, with cross-topic classifi-
cation experiments leading to results comparable
to the within-topic experiments; 3) We can use
data originating from one culture to train decep-
tion detection classifiers for another culture; the
use of psycholinguistic classes as a bridge across
languages can be as effective or even more effec-
tive than the use of translated unigrams, with the
added benefit of making the classification process
less costly and less time consuming.
The datasets introduced in this paper are publicly
available from http://nlp.eecs.umich.edu.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999499444444444">
This material is based in part upon work supported
by National Science Foundation awards #1344257
and #1355633 and by DARPA-BAA-12-47 DEFT
grant #12475008. Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion or the Defense Advanced Research Projects
Agency.
</bodyText>
<page confidence="0.998614">
444
</page>
<sectionHeader confidence="0.989901" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997975563218391">
´A. Almela, R. Valencia-Garc´ıa, and P. Cantos. 2012.
Seeing through deception: A computational ap-
proach to deceit detection in written communication.
In Proceedings of the Workshop on Computational
Approaches to Deception Detection, pages 15–22,
Avignon, France, April. Association for Computa-
tional Linguistics.
B. DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck,
K. Charlton, and H. Cooper. 2003. Cues to decep-
tion. Psychological Bulletin, 129(1).
S. Feng, R. Banerjee, and Y. Choi. 2012. Syntactic
stylometry for deception detection. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume
2, ACL ’12, pages 171–175, Stroudsburg, PA, USA.
Association for Computational Linguistics.
T. Fornaciari and M. Poesio. 2013. Automatic decep-
tion detection in italian court cases. Artificial Intelli-
gence and Law, 21(3):303–340.
C. Lewis and J. George. 2008. Cross-cultural de-
ception in social networking sites and face-to-face
communication. Comput. Hum. Behav., 24(6):2945–
2964, September.
C. Lewis and Giordano G. George, J. 2009. A cross-
cultural comparison of computer-mediated decep-
tive communication. In Proceedings of Pacific Asia
Conference on Information Systems.
R. Mihalcea and C. Strapparava. 2009. The lie de-
tector: Explorations in the automatic recognition of
deceptive language. In Proceedings of the Associa-
tion for Computational Linguistics (ACL 2009), Sin-
gapore.
M. Newman, J. Pennebaker, D. Berry, and J. Richards.
2003. Lying words: Predicting deception from lin-
guistic styles. Personality and Social Psychology
Bulletin, 29.
M. Ott, Y. Choi, C. Cardie, and J. Hancock. 2011.
Finding deceptive opinion spam by any stretch of
the imagination. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ’11, pages 309–319, Stroudsburg, PA, USA.
Association for Computational Linguistics.
H. Peng, C. Xiaoling, C. Na, R. Chandramouli, and
P. Subbalakshmi. 2011. Adaptive context mod-
eling for deception detection in emails. In Pro-
ceedings of the 7th international conference on Ma-
chine learning and data mining in pattern recogni-
tion, MLDM’11, pages 458–468, Berlin, Heidelberg.
Springer-Verlag.
J. Pennebaker and M. Francis. 1999. Linguistic in-
quiry and word count: LIWC. Erlbaum Publishers.
V. Rubin. 2010. On deception and deception detec-
tion: Content analysis of computer-mediated stated
beliefs. Proceedings of the American Society for In-
formation Science and Technology, 47(1):1–10.
C. Toma and J. Hancock. 2010. Reading between
the lines: linguistic cues to deception in online dat-
ing profiles. In Proceedings of the 2010 ACM con-
ference on Computer supported cooperative work,
CSCW ’10, pages 5–8, New York, NY, USA. ACM.
C. Toma, J. Hancock, and N. Ellison. 2008. Separating
fact from fiction: An examination of deceptive self-
presentation in online dating profiles. Personality
and Social Psychology Bulletin, 34(8):1023–1036.
Q. Xu and H. Zhao. 2012. Using deep linguistic fea-
tures for finding deceptive opinion spam. In Pro-
ceedings of COLING 2012: Posters, pages 1341–
1350, Mumbai, India, December. The COLING
2012 Organizing Committee.
H. Zhang, S. Wei, H. Tan, and J. Zheng. 2009. Decep-
tion detection based on svm for chinese text in cmc.
In Information Technology: New Generations, 2009.
ITNG ’09. Sixth International Conference on, pages
481–486, April.
L. Zhou and D. Shi, Y.and Zhang. 2008. A statisti-
cal language modeling approach to online deception
detection. IEEE Trans. on Knowl. and Data Eng.,
20(8):1077–1081, August.
L Zhou, D. Twitchell, T Qin, J. Burgoon, and J. Nuna-
maker. 2003. An exploratory study into decep-
tion detection in text-based computer-mediated com-
munication. In Proceedings of the 36th Annual
Hawaii International Conference on System Sci-
ences (HICSS’03) - Track1 - Volume 1, HICSS ’03,
pages 44.2–, Washington, DC, USA. IEEE Com-
puter Society.
</reference>
<page confidence="0.999108">
445
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985752">
<title confidence="0.999626">Cross-cultural Deception Detection</title>
<author confidence="0.99848">Ver´onica P´erez-Rosas Rada Mihalcea</author>
<affiliation confidence="0.9993075">Computer Science and Engineering Computer Science and Engineering University of North Texas University of Michigan</affiliation>
<email confidence="0.999526">veronicaperezrosas@my.unt.edumihalcea@umich.edu</email>
<abstract confidence="0.999337">In this paper, we address the task of cross-cultural deception detection. Using crowdsourcing, we collect three deception datasets, two in English (one originating from United States and one from India), and one in Spanish obtained from speakers from Mexico. We run comparative experiments to evaluate the accuracies of deception classifiers built for each culture, and also to analyze classification differences within and across cultures. Our results show that we can leverage cross-cultural information, either through translation or equivalent semantic categories, and build deception classifiers with a performance ranging between 60-70%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>´A Almela</author>
<author>R Valencia-Garc´ıa</author>
<author>P Cantos</author>
</authors>
<title>Seeing through deception: A computational approach to deceit detection in written communication.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Deception Detection,</booktitle>
<pages>15--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<marker>Almela, Valencia-Garc´ıa, Cantos, 2012</marker>
<rawString>´A. Almela, R. Valencia-Garc´ıa, and P. Cantos. 2012. Seeing through deception: A computational approach to deceit detection in written communication. In Proceedings of the Workshop on Computational Approaches to Deception Detection, pages 15–22, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B DePaulo</author>
<author>J Lindsay</author>
<author>B Malone</author>
<author>L Muhlenbruck</author>
<author>K Charlton</author>
<author>H Cooper</author>
</authors>
<title>Cues to deception.</title>
<date>2003</date>
<journal>Psychological Bulletin,</journal>
<volume>129</volume>
<issue>1</issue>
<contexts>
<context position="7070" citStr="DePaulo et al., 2003" startWordPosition="1057" endWordPosition="1060">r two cultures, and resulted in fewer contributions, namely 39+39 statements for Abortion, 42+42 statements for Death Penalty, and 94+94 statements for Best Friend. For all three cultures, the participants first provided their truthful responses, followed by the deceptive ones. Interestingly, for all three cultures, the average number of words for the deceptive statements (62 words) is significantly smaller than for the truthful statements (81 words), which may be explained by the added difficulty of the deceptive process, and is in line with previous observations about the cues of deception (DePaulo et al., 2003). 4 Experiments Through our experiments, we seek answers to the following questions. First, what is the performance for deception classifiers built for different cultures? Second, can we use information drawn from one culture to build a deception classifier for another culture? Finally, what are the psycholinguistic classes most strongly associated with deception/truth, and are there commonalities or differences among languages? In all our experiments, we formulate the deception detection task in a machine learning framework, where we use an SVM classifier to discriminate between deceptive and</context>
</contexts>
<marker>DePaulo, Lindsay, Malone, Muhlenbruck, Charlton, Cooper, 2003</marker>
<rawString>B. DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck, K. Charlton, and H. Cooper. 2003. Cues to deception. Psychological Bulletin, 129(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feng</author>
<author>R Banerjee</author>
<author>Y Choi</author>
</authors>
<title>Syntactic stylometry for deception detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>171--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3789" citStr="Feng et al., 2012" startWordPosition="562" endWordPosition="565">eception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform linguistic profiling of true tellers and liars (Zhou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and Poesio, 2013) identified deception in Italian by analyzing court cases. The authors explored several strategies for identifying deceptive clues, such as utterance length, LIWC features, lemmas and part of speech</context>
</contexts>
<marker>Feng, Banerjee, Choi, 2012</marker>
<rawString>S. Feng, R. Banerjee, and Y. Choi. 2012. Syntactic stylometry for deception detection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 171–175, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fornaciari</author>
<author>M Poesio</author>
</authors>
<title>Automatic deception detection in italian court cases.</title>
<date>2013</date>
<journal>Artificial Intelligence and Law,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="4191" citStr="Fornaciari and Poesio, 2013" startWordPosition="622" endWordPosition="625">hou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and Poesio, 2013) identified deception in Italian by analyzing court cases. The authors explored several strategies for identifying deceptive clues, such as utterance length, LIWC features, lemmas and part of speech patterns. (Almela et al., 2012) studied the deception detection in Spanish text by using SVM classifiers and linguistic categories, obtained from the Spanish version of the LIWC dictionary. A study on Chinese deception is presented in (Zhang et al., 2009), where the authors built a deceptive dataset using Internet news and performed machine learning experiments using a bag-of-words representation t</context>
</contexts>
<marker>Fornaciari, Poesio, 2013</marker>
<rawString>T. Fornaciari and M. Poesio. 2013. Automatic deception detection in italian court cases. Artificial Intelligence and Law, 21(3):303–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lewis</author>
<author>J George</author>
</authors>
<title>Cross-cultural deception in social networking sites and face-to-face communication.</title>
<date>2008</date>
<journal>Comput. Hum. Behav.,</journal>
<volume>24</volume>
<issue>6</issue>
<pages>2964</pages>
<contexts>
<context position="4981" citStr="Lewis and George, 2008" startWordPosition="738" endWordPosition="741">ures, lemmas and part of speech patterns. (Almela et al., 2012) studied the deception detection in Spanish text by using SVM classifiers and linguistic categories, obtained from the Spanish version of the LIWC dictionary. A study on Chinese deception is presented in (Zhang et al., 2009), where the authors built a deceptive dataset using Internet news and performed machine learning experiments using a bag-of-words representation to train a classifier able to discriminate between deceptive and truthful cases. It is also worth mentioning the work conducted to analyze cross-cultural differences. (Lewis and George, 2008) presented a study of deception in social networks sites and face-to-face communication, where authors compare deceptive behavior of Korean and American participants, with a subsequent study also considering the differences between Spanish and American participants (Lewis and George, 2009). In general, research findings suggest a strong relation between deception and cultural aspects, which are worth exploring with automatic methods. 3 Datasets We collect three datasets for three different cultures: United States (English-US), India (EnglishIndia), and Mexico (Spanish-Mexico). Following (Mihal</context>
</contexts>
<marker>Lewis, George, 2008</marker>
<rawString>C. Lewis and J. George. 2008. Cross-cultural deception in social networking sites and face-to-face communication. Comput. Hum. Behav., 24(6):2945– 2964, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lewis</author>
<author>Giordano G George</author>
<author>J</author>
</authors>
<title>A crosscultural comparison of computer-mediated deceptive communication.</title>
<date>2009</date>
<booktitle>In Proceedings of Pacific Asia Conference on Information Systems.</booktitle>
<marker>Lewis, George, J, 2009</marker>
<rawString>C. Lewis and Giordano G. George, J. 2009. A crosscultural comparison of computer-mediated deceptive communication. In Proceedings of Pacific Asia Conference on Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Strapparava</author>
</authors>
<title>The lie detector: Explorations in the automatic recognition of deceptive language.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="5607" citStr="Mihalcea and Strapparava, 2009" startWordPosition="825" endWordPosition="828">2008) presented a study of deception in social networks sites and face-to-face communication, where authors compare deceptive behavior of Korean and American participants, with a subsequent study also considering the differences between Spanish and American participants (Lewis and George, 2009). In general, research findings suggest a strong relation between deception and cultural aspects, which are worth exploring with automatic methods. 3 Datasets We collect three datasets for three different cultures: United States (English-US), India (EnglishIndia), and Mexico (Spanish-Mexico). Following (Mihalcea and Strapparava, 2009), we collect short deceptive and truthful essays for three topics: opinions on Abortion, opinions on Death Penalty, and feelings about a Best Friend. For English-US and English-India, we use Amazon Mechanical Turk with a location restriction, so that all the contributors are from the country of interest (US and India). We collect 100 deceptive and 100 truthful statements for each of the three topics. To avoid spam, each contribution is manually verified by one of the authors of this paper.For SpanishMexico, while we initially attempted to collect data also using Mechanical Turk, we were not ab</context>
<context position="16701" citStr="Mihalcea and Strapparava, 2009" startWordPosition="2531" endWordPosition="2534"> LIWC brings additional improvements, with absolute improvements of 2-4% over the use of unigrams. This is an encouraging result, as it implies that a semantic bridge such as LIWC can be effectively used to classify deception data in other languages, instead of using the more costly and time consuming unigram method based on translations. 4.3 What are the psycholinguistic classes most strongly associated with deception/truth? The final question we address is concerned with the LIWC classes that are dominant in deceptive and truthful text for different cultures. We use the method presented in (Mihalcea and Strapparava, 2009), which consists of a metric that measures the saliency of LIWC classes in deceptive versus truthful data. Following their strategy, we first create a corpus of deceptive and truthful text using a mix of all the topics in each culture. We then calculate 443 Class Score Sample words Class Score Sample words English-US Deceptive Truthful Metaph 1.77 Die,died,hell,sin,lord Insight 0.68 Accept,believe,understand Other 1.46 He,her,herself,him I 0.66 I,me,my,myself, You 1.41 Thou,you Optimism 0.65 accept, hope, top, best Othref 1.18 He,her,herself,him We 0.55 Our,ourselves,us,we, Negemo 1.18 Afraid,</context>
</contexts>
<marker>Mihalcea, Strapparava, 2009</marker>
<rawString>R. Mihalcea and C. Strapparava. 2009. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of the Association for Computational Linguistics (ACL 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Newman</author>
<author>J Pennebaker</author>
<author>D Berry</author>
<author>J Richards</author>
</authors>
<title>Lying words: Predicting deception from linguistic styles.</title>
<date>2003</date>
<journal>Personality and Social Psychology Bulletin,</journal>
<volume>29</volume>
<contexts>
<context position="3600" citStr="Newman et al., 2003" startWordPosition="529" endWordPosition="532">requently include word-based stylometric analysis. Linguistic clues such as n-grams, count of used words and sentences, word diversity, and selfreferences are also commonly used to identify deception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform linguistic profiling of true tellers and liars (Zhou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and Poesio, 2013) identifi</context>
<context position="18959" citStr="Newman et al., 2003" startWordPosition="2810" endWordPosition="2813">er of their dominance score. Table 3 shows the most salient classes for each culture, along with sample words. This analysis shows some interesting patterns. There are several classes that are shared among the cultures. For instance, the deceivers in all cultures make use of negation, negative emotions, and references to others. Second, true tellers use more optimism and friendship words, as well as references to themselves. These results are in line with previous research, which showed that LIWC word classes exhibit similar trends when distinguishing between deceptive and non-deceptive text (Newman et al., 2003). Moreover, there are also word classes that only appear in some of the cultures; for example, time classes (Past, Future) appear in English-India and Spanish-Mexico, but not in English-US, which in turn contains other classes such as Insight and Metaph. 5 Conclusions In this paper, we addressed the task of deception detection within- and across-cultures. Using three datasets from three different cultures, each covering three different topics, we conducted several experiments to evaluate the accuracy of deception detection when learning from data from the same culture or from a different cultu</context>
</contexts>
<marker>Newman, Pennebaker, Berry, Richards, 2003</marker>
<rawString>M. Newman, J. Pennebaker, D. Berry, and J. Richards. 2003. Lying words: Predicting deception from linguistic styles. Personality and Social Psychology Bulletin, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ott</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>J Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>309--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1245" citStr="Ott et al., 2011" startWordPosition="167" endWordPosition="170">ze classification differences within and across cultures. Our results show that we can leverage cross-cultural information, either through translation or equivalent semantic categories, and build deception classifiers with a performance ranging between 60-70%. 1 Introduction The identification of deceptive behavior is a task that has gained increasing interest from researchers in computational linguistics. This is mainly motivated by the rapid growth of deception in written sources, and in particular in Web content, including product reviews, online dating profiles, and social networks posts (Ott et al., 2011). To date, most of the work presented on deception detection has focused on the identification of deceit clues within a specific language, where English is the most commonly studied language. However, a large portion of the written communication (e.g., e-mail, chats, forums, blogs, social networks) occurs not only between speakers of English, but also between speakers from other cultural backgrounds, which poses important questions regarding the applicability of existing deception tools. Issues such as language, beliefs, and moral values may influence the way people deceive, and therefore may </context>
<context position="2893" citStr="Ott et al., 2011" startWordPosition="422" endWordPosition="425">s a cross-lingual bridge. Our results show that we can achieve accuracies in the range of 60-70%, and that we can leverage resources available in one language to build deception tools for another language. 2 Related Work Research to date on automatic deceit detection has explored a wide range of applications such as the identification of spam in e-mail communication, the detection of deceitful opinions in review websites, and the identification of deceptive behavior in computer-mediated communication including chats, blogs, forums and online dating sites (Peng et al., 2011; Toma et al., 2008; Ott et al., 2011; Toma and Hancock, 2010; Zhou and Shi, 2008). Techniques used for deception detection frequently include word-based stylometric analysis. Linguistic clues such as n-grams, count of used words and sentences, word diversity, and selfreferences are also commonly used to identify deception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used suc</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>M. Ott, Y. Choi, C. Cardie, and J. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 309–319, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Peng</author>
<author>C Xiaoling</author>
<author>C Na</author>
<author>R Chandramouli</author>
<author>P Subbalakshmi</author>
</authors>
<title>Adaptive context modeling for deception detection in emails.</title>
<date>2011</date>
<booktitle>In Proceedings of the 7th international conference on Machine learning and data mining in pattern recognition, MLDM’11,</booktitle>
<pages>458--468</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2856" citStr="Peng et al., 2011" startWordPosition="414" endWordPosition="417">ams and word categories that can act as a cross-lingual bridge. Our results show that we can achieve accuracies in the range of 60-70%, and that we can leverage resources available in one language to build deception tools for another language. 2 Related Work Research to date on automatic deceit detection has explored a wide range of applications such as the identification of spam in e-mail communication, the detection of deceitful opinions in review websites, and the identification of deceptive behavior in computer-mediated communication including chats, blogs, forums and online dating sites (Peng et al., 2011; Toma et al., 2008; Ott et al., 2011; Toma and Hancock, 2010; Zhou and Shi, 2008). Techniques used for deception detection frequently include word-based stylometric analysis. Linguistic clues such as n-grams, count of used words and sentences, word diversity, and selfreferences are also commonly used to identify deception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychologica</context>
</contexts>
<marker>Peng, Xiaoling, Na, Chandramouli, Subbalakshmi, 2011</marker>
<rawString>H. Peng, C. Xiaoling, C. Na, R. Chandramouli, and P. Subbalakshmi. 2011. Adaptive context modeling for deception detection in emails. In Proceedings of the 7th international conference on Machine learning and data mining in pattern recognition, MLDM’11, pages 458–468, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pennebaker</author>
<author>M Francis</author>
</authors>
<title>Linguistic inquiry and word count: LIWC.</title>
<date>1999</date>
<publisher>Erlbaum Publishers.</publisher>
<contexts>
<context position="3377" citStr="Pennebaker and Francis, 1999" startWordPosition="494" endWordPosition="497">in computer-mediated communication including chats, blogs, forums and online dating sites (Peng et al., 2011; Toma et al., 2008; Ott et al., 2011; Toma and Hancock, 2010; Zhou and Shi, 2008). Techniques used for deception detection frequently include word-based stylometric analysis. Linguistic clues such as n-grams, count of used words and sentences, word diversity, and selfreferences are also commonly used to identify deception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform linguistic profiling of true tellers and liars (Zhou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, Baltimore, Maryland, USA, June 23-25 2014. </context>
<context position="10154" citStr="Pennebaker and Francis, 1999" startWordPosition="1519" endWordPosition="1522">3.17% 67.07% 48.78% 51.22% 62.20% 54.87% 63.41% Average 72.79% 69.45% 54.92% 52.25% 67.89% 57.99% 57.21% Table 1: Within-culture classification, using LIWC word classes and unigrams. For LIWC, results are shown for within-topic experiments, with ten-fold cross validation. For unigrams, both within-topic (ten-fold cross validation on the same topic) and cross-topic (training on two topics and testing on the third topic) results are reported. Second, we use the LIWC lexicon to extract features corresponding to several word classes. LIWC was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 classes relevant to psychological processes (e.g., emotion, cognition), which in turn are grouped into four broad categories2 namely: linguistic processes, psychological processes, relativity, and personal concerns. A feature is generated for each of the 70 word classes by counting the total frequency of the words belonging to that class. We perform separate evaluations using each of the four broad LIWC categories, as well as using all the categories together. The results obtained with the SVM classifier</context>
</contexts>
<marker>Pennebaker, Francis, 1999</marker>
<rawString>J. Pennebaker and M. Francis. 1999. Linguistic inquiry and word count: LIWC. Erlbaum Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rubin</author>
</authors>
<title>On deception and deception detection: Content analysis of computer-mediated stated beliefs.</title>
<date>2010</date>
<booktitle>Proceedings of the American Society for Information Science and Technology,</booktitle>
<pages>47--1</pages>
<contexts>
<context position="3614" citStr="Rubin, 2010" startWordPosition="533" endWordPosition="534">d-based stylometric analysis. Linguistic clues such as n-grams, count of used words and sentences, word diversity, and selfreferences are also commonly used to identify deception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform linguistic profiling of true tellers and liars (Zhou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and Poesio, 2013) identified deception i</context>
</contexts>
<marker>Rubin, 2010</marker>
<rawString>V. Rubin. 2010. On deception and deception detection: Content analysis of computer-mediated stated beliefs. Proceedings of the American Society for Information Science and Technology, 47(1):1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Toma</author>
<author>J Hancock</author>
</authors>
<title>Reading between the lines: linguistic cues to deception in online dating profiles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM conference on Computer supported cooperative work, CSCW ’10,</booktitle>
<pages>5--8</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2917" citStr="Toma and Hancock, 2010" startWordPosition="426" endWordPosition="429">bridge. Our results show that we can achieve accuracies in the range of 60-70%, and that we can leverage resources available in one language to build deception tools for another language. 2 Related Work Research to date on automatic deceit detection has explored a wide range of applications such as the identification of spam in e-mail communication, the detection of deceitful opinions in review websites, and the identification of deceptive behavior in computer-mediated communication including chats, blogs, forums and online dating sites (Peng et al., 2011; Toma et al., 2008; Ott et al., 2011; Toma and Hancock, 2010; Zhou and Shi, 2008). Techniques used for deception detection frequently include word-based stylometric analysis. Linguistic clues such as n-grams, count of used words and sentences, word diversity, and selfreferences are also commonly used to identify deception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform lin</context>
</contexts>
<marker>Toma, Hancock, 2010</marker>
<rawString>C. Toma and J. Hancock. 2010. Reading between the lines: linguistic cues to deception in online dating profiles. In Proceedings of the 2010 ACM conference on Computer supported cooperative work, CSCW ’10, pages 5–8, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Toma</author>
<author>J Hancock</author>
<author>N Ellison</author>
</authors>
<title>Separating fact from fiction: An examination of deceptive selfpresentation in online dating profiles.</title>
<date>2008</date>
<journal>Personality and Social Psychology Bulletin,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2875" citStr="Toma et al., 2008" startWordPosition="418" endWordPosition="421">ries that can act as a cross-lingual bridge. Our results show that we can achieve accuracies in the range of 60-70%, and that we can leverage resources available in one language to build deception tools for another language. 2 Related Work Research to date on automatic deceit detection has explored a wide range of applications such as the identification of spam in e-mail communication, the detection of deceitful opinions in review websites, and the identification of deceptive behavior in computer-mediated communication including chats, blogs, forums and online dating sites (Peng et al., 2011; Toma et al., 2008; Ott et al., 2011; Toma and Hancock, 2010; Zhou and Shi, 2008). Techniques used for deception detection frequently include word-based stylometric analysis. Linguistic clues such as n-grams, count of used words and sentences, word diversity, and selfreferences are also commonly used to identify deception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which </context>
</contexts>
<marker>Toma, Hancock, Ellison, 2008</marker>
<rawString>C. Toma, J. Hancock, and N. Ellison. 2008. Separating fact from fiction: An examination of deceptive selfpresentation in online dating profiles. Personality and Social Psychology Bulletin, 34(8):1023–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Xu</author>
<author>H Zhao</author>
</authors>
<title>Using deep linguistic features for finding deceptive opinion spam.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1341--1350</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="3809" citStr="Xu and Zhao, 2012" startWordPosition="566" endWordPosition="569">n important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform linguistic profiling of true tellers and liars (Zhou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and Poesio, 2013) identified deception in Italian by analyzing court cases. The authors explored several strategies for identifying deceptive clues, such as utterance length, LIWC features, lemmas and part of speech patterns. (Almela e</context>
</contexts>
<marker>Xu, Zhao, 2012</marker>
<rawString>Q. Xu and H. Zhao. 2012. Using deep linguistic features for finding deceptive opinion spam. In Proceedings of COLING 2012: Posters, pages 1341– 1350, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>S Wei</author>
<author>H Tan</author>
<author>J Zheng</author>
</authors>
<title>Deception detection based on svm for chinese text in cmc.</title>
<date>2009</date>
<booktitle>In Information Technology: New Generations,</booktitle>
<pages>481--486</pages>
<contexts>
<context position="4645" citStr="Zhang et al., 2009" startWordPosition="690" endWordPosition="693">While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and Poesio, 2013) identified deception in Italian by analyzing court cases. The authors explored several strategies for identifying deceptive clues, such as utterance length, LIWC features, lemmas and part of speech patterns. (Almela et al., 2012) studied the deception detection in Spanish text by using SVM classifiers and linguistic categories, obtained from the Spanish version of the LIWC dictionary. A study on Chinese deception is presented in (Zhang et al., 2009), where the authors built a deceptive dataset using Internet news and performed machine learning experiments using a bag-of-words representation to train a classifier able to discriminate between deceptive and truthful cases. It is also worth mentioning the work conducted to analyze cross-cultural differences. (Lewis and George, 2008) presented a study of deception in social networks sites and face-to-face communication, where authors compare deceptive behavior of Korean and American participants, with a subsequent study also considering the differences between Spanish and American participant</context>
</contexts>
<marker>Zhang, Wei, Tan, Zheng, 2009</marker>
<rawString>H. Zhang, S. Wei, H. Tan, and J. Zheng. 2009. Deception detection based on svm for chinese text in cmc. In Information Technology: New Generations, 2009. ITNG ’09. Sixth International Conference on, pages 481–486, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>D Shi</author>
<author>Y and Zhang</author>
</authors>
<title>A statistical language modeling approach to online deception detection.</title>
<date>2008</date>
<journal>IEEE Trans. on Knowl. and Data Eng.,</journal>
<volume>20</volume>
<issue>8</issue>
<marker>Zhou, Shi, Zhang, 2008</marker>
<rawString>L. Zhou and D. Shi, Y.and Zhang. 2008. A statistical language modeling approach to online deception detection. IEEE Trans. on Knowl. and Data Eng., 20(8):1077–1081, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>D Twitchell</author>
<author>T Qin</author>
<author>J Burgoon</author>
<author>J Nunamaker</author>
</authors>
<title>An exploratory study into deception detection in text-based computer-mediated communication.</title>
<date>2003</date>
<booktitle>In Proceedings of the 36th Annual Hawaii International Conference on System Sciences (HICSS’03) - Track1 - Volume 1, HICSS ’03,</booktitle>
<pages>44--2</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="3579" citStr="Zhou et al., 2003" startWordPosition="525" endWordPosition="528">ception detection frequently include word-based stylometric analysis. Linguistic clues such as n-grams, count of used words and sentences, word diversity, and selfreferences are also commonly used to identify deception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform linguistic profiling of true tellers and liars (Zhou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and P</context>
</contexts>
<marker>Zhou, Twitchell, Qin, Burgoon, Nunamaker, 2003</marker>
<rawString>L Zhou, D. Twitchell, T Qin, J. Burgoon, and J. Nunamaker. 2003. An exploratory study into deception detection in text-based computer-mediated communication. In Proceedings of the 36th Annual Hawaii International Conference on System Sciences (HICSS’03) - Track1 - Volume 1, HICSS ’03, pages 44.2–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>