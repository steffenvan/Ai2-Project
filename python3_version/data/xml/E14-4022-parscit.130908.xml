<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.069739">
<title confidence="0.997317">
Using a Random Forest Classifier to Compile Bilingual Dictionaries of
Technical Terms from Comparable Corpora
</title>
<author confidence="0.99369">
Georgios Kontonatsios1,2 Ioannis Korkontzelos1,2 Jun’ichi Tsujii3 Sophia Ananiadou1,2
</author>
<affiliation confidence="0.9914">
National Centre for Text Mining, University of Manchester, Manchester, UK1
School of Computer Science, University of Manchester, Manchester, UK2
Microsoft Research Asia, Beijing, China3
</affiliation>
<email confidence="0.992388">
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
</email>
<sectionHeader confidence="0.993821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906333333333">
We describe a machine learning approach,
a Random Forest (RF) classifier, that is
used to automatically compile bilingual
dictionaries of technical terms from com-
parable corpora. We evaluate the RF clas-
sifier against a popular term alignment
method, namely context vectors, and we
report an improvement of the translation
accuracy. As an application, we use the
automatically extracted dictionary in com-
bination with a trained Statistical Machine
Translation (SMT) system to more accu-
rately translate unknown terms. The dic-
tionary extraction method described in this
paper is freely available 1.
</bodyText>
<sectionHeader confidence="0.989916" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999860631578948">
Bilingual dictionaries of technical terms are im-
portant resources for many Natural Language
Processing (NLP) tasks including Statistical Ma-
chine Translation (SMT) (Och and Ney, 2003) and
Cross-Language Information Retrieval (Balles-
teros and Croft, 1997). However, manually cre-
ating and updating such resources is an expensive
process. In addition to this, new terms are con-
stantly emerging. Especially in the biomedical
domain, which is the focus of this work, there is
a vast number of neologisms, i.e., newly coined
terms, (Pustejovsky et al., 2001).
Early work on bilingual lexicon extraction
focused on clean, parallel corpora providing
satisfactory results (Melamed, 1997; Kay and
R¨oscheisen, 1993). However, parallel corpora are
expensive to construct and for some domains and
language pairs are scarce resources. For these rea-
sons, the focus has shifted to comparable corpora
</bodyText>
<footnote confidence="0.899387">
1http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/RF-TermAlign.tar.gz
</footnote>
<bodyText confidence="0.999845292682927">
that are more readily available, more up-to-date,
larger and cheaper to construct than parallel data.
Comparable corpora are collections of monolin-
gual documents in a source and target language
that share the same topic, domain and/or docu-
ments are from the same period, genre and so
forth.
Existing methods for bilingual lexicon extrac-
tion from comparable corpora are mainly based
on the same principle. They hypothesise that a
word and its translation tend to appear in simi-
lar lexical context (Fung and Yee, 1998; Rapp,
1999; Morin et al., 2007; Chiao and Zweigen-
baum, 2002). Context vector methods are reported
to achieve robust performance on terms that occur
frequently in the corpus. Chiao and Zweigenbaum
(2002) achieved a performance of 94% accuracy
on the top 20 candidates when translating high fre-
quency, medical terms (frequency of 100 or more).
In contrast, Morin and Daille (2010) reported an
accuracy of 21% for multi-word terms occurring
20 times or less, noting that translating rare terms
is a challenging problem for context vectors.
Kontonatsios et al. (2013) introduced an RF
classifier that is able to automatically learn as-
sociation rules of textual units between a source
and target language. However, they applied their
method only on artificially constructed datasets
containing an equal number of positive and neg-
ative instances. In the case of comparable cor-
pora, the datasets are highly unbalanced (given n,
m source and target terms respectively, we need to
classify n × m instances). In this work, we incor-
porate the classification margin into the RF model,
to allow the method to cope with the skewed dis-
tribution of positive and negative instances that oc-
curs in comparable corpora.
Our proposed method ranks candidate transla-
tions using the classification margin and suggests
as the best translation the candidate with the max-
imum margin. We evaluate our method on an
</bodyText>
<page confidence="0.983838">
111
</page>
<note confidence="0.689586">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 111–116,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999738142857143">
English-Spanish comparable corpus of Wikipedia
articles that are related to the medical sub-domain
of “breast cancer”. Furthermore, we show that dic-
tionaries extracted from comparable corpora can
be used to dynamically augment an SMT sys-
tem in order to better translate Out-of-Vocabulary
(OOV) terms.
</bodyText>
<sectionHeader confidence="0.993315" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999940291666667">
A pair of terms in a source and target language is
represented as a feature vector where each dimen-
sion corresponds to a unique character n-gram.
The value of each dimension is 0 or 1 and desig-
nates the occurrence of the corresponding n-gram
in the input terms. The feature vectors that we
use contain 2q dimensions where the first q dimen-
sions correspond to the n-gram features extracted
from the source terms and the last q dimensions to
those from the target terms. In the reported experi-
ments, we use the 600 (300 source and 300 target)
most frequently occurring n-grams.
The underlying mechanism that allows the RF
method to learn character gram mappings between
terms of a source and target language is the de-
cision trees. A node in the decision tree is a
unique character n-gram. The nodes are linked
through the branches of the trees and therefore the
two sub-spaces of q source and q target charac-
ter grams are combined. Each decision tree in the
forest is constructed as follows: every node is split
by considering |φ |random n-gram features of the
initial feature set Q, and a decision tree is fully
grown. This process is repeated |τ |times and con-
structs |τ |decision trees. We tuned the RF clas-
sifier using 140 random trees where we observed
a plateau in the classification performance. Fur-
thermore, we set the number of random features
using |φ |= log2 |Q |+ 1 as suggested by Breiman
(2001).
The classification margin that we use to rank
the candidate translations is calculated by simply
subtracting the average number of trees predicting
that the input terms are not translations from the
average number of decision trees predicting that
the terms are mutual translations. A larger classi-
fication margin means that more decision trees in
the forest classify an instance as a translation pair.
For training an RF model, we use a bilingual
dictionary of technical terms. When the dictionary
lists more than one translation for an English term,
we randomly select only one. Negative instances
are created by randomly matching non-translation
pairs of terms. We used an equal number of posi-
tive and negative instances for training the model.
Starting from 20, 000 translation pairs we gener-
ated a training dataset of 40, 000 positive and neg-
ative instances.
</bodyText>
<subsectionHeader confidence="0.983377">
2.1 Baseline method
</subsectionHeader>
<bodyText confidence="0.999986606060606">
The context projection method was first pro-
posed by (Fung and Yee, 1998; Rapp, 1999) and
since then different variations have been suggested
(Chiao and Zweigenbaum, 2002; Morin et al.,
2007; Andrade et al., 2010; Morin and Prochas-
son, 2011). Our implementation more closely
follows the context vector method introduced by
(Morin and Prochasson, 2011).
As a preprocessing step, stop words are re-
moved using an online list 2 and lemmatisation
is performed using TreeTagger (Schmid, 1994) on
both the English and Spanish part of the compa-
rable corpus. Afterwards, the method proceeds
in three steps. Firstly, for each source and target
term of the comparable corpus, i.e., i, we collect
all lexical units that: (a) occur within a window
of 3 words around i (a seven-word window) and
(b) are listed in the seed bilingual dictionary. The
lexical units that satisfy the above two conditions
are the dimensions of the context vectors. Each
dimension has a value that indicates the correla-
tion between the context lexical unit and the term
i. In our approach, we use the log-likelihood ra-
tio. In the second step, the seed dictionary is used
to translate the lexical units of the Spanish context
vectors. In this way the Spanish and English vec-
tors become comparable. When several transla-
tions are listed in the seed dictionary, we consider
all of them. In the third step, we compute the con-
text similarity, i.e., distance metric, between the
vector of an English term to be translated with ev-
ery projected, Spanish context vector. For this we
use the cosine similarity.
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9998148">
In this section, we evaluate the two dictionary ex-
traction methods, namely context vectors and RF,
on a comparable corpus of Wikipedia articles.
For the evaluation metric, we use the top-k
translation accuracy 3 and the mean reciprocal
</bodyText>
<footnote confidence="0.999399">
2http://members.unine.ch/jacques.savoy/clef/index.html
3the percentage of English terms whose top k candidates
contain a correct translation
</footnote>
<page confidence="0.994862">
112
</page>
<bodyText confidence="0.9999611875">
rank (MRR) 4 as in previous approaches (Chiao
and Zweigenbaum, 2002; Chiao and Zweigen-
baum, 2002; Morin and Prochasson, 2011; Morin
et al., 2007; Tamura et al., 2012). As a refer-
ence list, we use the UMLS metathesaurus5. In
addition to this, considering that in several cases
the dictionary extraction methods retrieved syn-
onymous translations that do not appear in the ref-
erence list, we manually inspected the answers.
Finally, unlike previous approaches (Chiao and
Zweigenbaum, 2002), we do not restrict the test
list only to those English terms whose Spanish
translations are known to occur in the target cor-
pus. In such cases, the performance of dictionary
extraction methods have been shown to achieve a
lower performance (Tamura et al., 2012).
</bodyText>
<subsectionHeader confidence="0.983557">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999994846153846">
We constructed a comparable corpus of Wikipedia
articles. For this, we used Wikipedia’s search en-
gine 6 and submitted the queries “breast cancer”
and “c´ancer de mama” for English and Spanish
respectively. From the returned list of Wikipedia
pages, we used the 1, 000 top articles for both lan-
guages.
The test list contains 1, 200 English single-word
terms that were extracted by considering all nouns
that occur more than 10 but not more than 200
times and are listed in UMLS. For the Spanish part
of the corpus, we considered all nouns as candi-
date translations (32, 347 in total).
</bodyText>
<subsectionHeader confidence="0.758424">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.8819515">
Table 1 shows the top-k translation accuracy and
the MRR of RF and context vectors.
</bodyText>
<table confidence="0.9992485">
Acc1 Acc10 Acc20 MRR
RF 0.41 0.57 0.59 0.47
Cont. 0.1 0.21 0.26 0.11
Vectors
</table>
<tableCaption confidence="0.911685">
Table 1: top-k translation accuracy and MRR of
RF and context vectors on 1, 200 English terms
</tableCaption>
<bodyText confidence="0.993904">
We observe that the proposed RF method
achieves a considerably better top-k translation ac-
</bodyText>
<equation confidence="0.9584275">
�Q
4MRR = |4 |i=1 ra�ki where Q is the number of
</equation>
<footnote confidence="0.9775648">
English terms for which we are extracting translations and
ranki is the position of the first correct translation from re-
turned list of candidates
5nlm.nih.gov/research/umls
6http://en.wikipedia.org/wiki/Help:Searching
</footnote>
<bodyText confidence="0.9984474">
curacy and MRR than the baseline method. More-
over, we segmented the 1, 200 test terms into 7
frequency ranges 7, from high-frequency to rare
terms. Figure 1 shows the translation accuracy at
top 20 candidates for the two methods. We note
</bodyText>
<figureCaption confidence="0.970032">
Figure 1: Translation accuracy of top 20 candi-
dates on different frequency ranges
</figureCaption>
<bodyText confidence="0.999936818181818">
that for high frequency terms, i.e. [100,200] range,
the performance achieved by the two methods is
similar (53% and 52% for the RF and context vec-
tors respectively). However, for lower frequency
terms, the translation accuracy of the context vec-
tors continuously declines. This confirms that con-
text vectors do not behave robustly for rare terms
(Morin and Daille, 2010). In contrast, the RF
slightly fluctuates over different frequency ranges
and presents approximately the same translation
accuracy for both frequent and rare terms.
</bodyText>
<sectionHeader confidence="0.997809" genericHeader="method">
4 Application
</sectionHeader>
<bodyText confidence="0.99999475">
As an application of our method, we use the pre-
viously extracted dictionaries to on-line augment
the phrase table of an SMT system and observe
the translation performance on test sentences that
contain OOV terms. For the translation probabil-
ities in the phrase table, we use the distance met-
ric given by the dictionary extraction methods i.e.,
classification margin and cosine similarity of RF
and context vectors respectively, normalised by
the uniform probability (if a source term has m
candidate translations, we normalise the distance
metric by dividing by m as in (Wu et al., 2008) .
</bodyText>
<subsectionHeader confidence="0.995955">
4.1 Data and tools
</subsectionHeader>
<bodyText confidence="0.999947166666667">
We construct a parallel, sentence-aligned corpus
from the biomedical domain, following the pro-
cess described in (Wu et al., 2011; Yepes et al.,
2013). The parallel corpus comprises of article ti-
tles indexed by PubMed in both English and Span-
ish. We collect 120K parallel sentences for train-
</bodyText>
<footnote confidence="0.993228">
7each frequency range contains 100 randomly sampled
terms
</footnote>
<page confidence="0.999233">
113
</page>
<bodyText confidence="0.999848">
ing the SMT and 1K sentences for evaluation. The
test sentences contain 1, 200 terms that do not ap-
pear in the training parallel corpus. These terms
occur in the Wikipedia comparable corpus. Hence,
the previously extracted dictionaries list a possible
translation. Using the PubMed parallel corpus, we
train Moses (Koehn et al., 2007), a phrase-based
SMT system.
</bodyText>
<sectionHeader confidence="0.521192" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9995593">
We evaluated the translation performance of the
SMT that uses the dictionary extracted by the RF
against the following baselines: (i) Moses using
only the training parallel data (Moses), (ii) Moses
using the dictionary extracted by context vectors
(Moses+context vector). The evaluation metric is
BLEU (Papineni et al., 2002).
Table 2 shows the BLEU score achieved by the
SMT systems when we append the top-k transla-
tions to the phrase table.
</bodyText>
<table confidence="0.9839995">
BLEU
on top-k translations
1 10 20
Moses 24.22 24.22 24.22
Moses+
RF 25.32 24.626 24.42
Moses+
Context Vectors 23.88 23.69 23.74
</table>
<tableCaption confidence="0.8753725">
Table 2: Translation performance when adding
top-k translations to the phrase table
</tableCaption>
<bodyText confidence="0.9999304375">
We observe that the best performance is
achieved by the RF when we add the top 1 trans-
lation with a total gain of 1.1 BLEU points over
the baseline system. In contrast, context vec-
tors decreased the translation performance of the
SMT system. This indicates that the dictionary ex-
tracted by the context vectors is too noisy and as
a result the translation performance dropped. Fur-
thermore, it is noted that the augmented SMT sys-
tems achieve the highest performance for the top 1
translation while for k greater than 1, the transla-
tion performance decreases. This behaviour is ex-
pected since the target language model was trained
only on the training Spanish sentences of the par-
allel corpus. Hence, the target language model
does not have a prior knowledge of the OOV trans-
lations and as a result it cannot choose the correct
translation among k candidates.
To further investigate the effect of the language
model on the translation performance of the aug-
mented SMT systems, we conducted an oracle ex-
periment. In this ideal setting, we assume a strong
language model, that is trained on both training
and test Spanish sentences of the parallel corpus,
in order to assign a higher probability to a correct
translation if it exists in the deployed dictionary.
As we observe in Table 3, a strong language model
can more accurately select the correct translation
among top-k candidates. The dictionary extracted
by the RF improved the translation performance
by 2.5 BLEU points for the top-10 candidates and
context vectors by 0.45 for the top-20 candidates.
</bodyText>
<table confidence="0.99797225">
BLEU
on top-k translations
1 10 20
Moses 28.85 28.85 28.85
Moses+
RF 30.98 31.35 31.2
Moses+
Context Vectors 28.18 29.17 29.3
</table>
<tableCaption confidence="0.99952">
Table 3: Translation performance when adding
</tableCaption>
<bodyText confidence="0.920868">
top-k translations to the phrase table. SMT sys-
tems use a language model trained on training and
test Spanish sentences of the parallel corpus.
</bodyText>
<sectionHeader confidence="0.998186" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999848315789474">
In this paper, we presented an RF classifier that
is used to extract bilingual dictionaries of techni-
cal terms from comparable corpora. We evaluated
our method on a comparable corpus of Wikipedia
articles. The experimental results showed that our
proposed method performs robustly when translat-
ing both frequent and rare terms.
As an application, we used the automatically
extracted dictionary to augment the phrase table of
an SMT system. The results demonstrated an im-
provement of the overall translation performance.
As future work, we plan to integrate the RF clas-
sifier with context vectors. Intuitively, the two
methods are complementary considering that the
RF exploits the internal structure of terms while
context vectors use the surrounding lexical con-
text. Therefore, it will be interesting to investigate
how we can incorporate the two feature spaces in
a machine learner.
</bodyText>
<page confidence="0.998168">
114
</page>
<sectionHeader confidence="0.998601" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.996619">
This work was funded by the European Commu-
nity’s Seventh Framework Program (FP7/2007-
2013) [grant number 318736 (OSSMETER)].
</bodyText>
<sectionHeader confidence="0.989962" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99936291509434">
Daniel Andrade, Tetsuya Nasukawa, and Jun’ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ’10, pages 19–
27, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84–91. ACM.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5–32.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414–420. Association for Computational Lin-
guistics.
Martin Kay and Martin R¨oscheisen. 1993. Text-
translation alignment. computational Linguistics,
19(1):121–142.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.
Georgios Kontonatsios, Ioannis Korkontzelos, Jun’ichi
Tsujii, and Sophia Ananiadou. 2013. Using ran-
dom forest to recognise translation equivalents of
biomedical terms across languages. In Proceed-
ings of the Sixth Workshop on Building and Using
Comparable Corpora, pages 95–104. Association
for Computational Linguistics, August.
I. Dan Melamed. 1997. A portable algorithm for map-
ping bitext correspondence. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 305–312. Association for
Computational Linguistics.
Emmanuel Morin and B´eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79–95.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27–34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664–
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371–375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519–526. Associ-
ation for Computational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44–49. Manch-
ester, UK.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24–36. Associa-
tion for Computational Linguistics.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume
1, pages 993–1000. Association for Computational
Linguistics.
</reference>
<page confidence="0.988717">
115
</page>
<reference confidence="0.9977548">
Cuijun Wu, Fei Xia, Louise Deleger, and Imre Solti.
2011. Statistical machine translation for biomedical
text: are we there yet? In AMIA Annual Sympo-
sium Proceedings, volume 2011, page 1290. Ameri-
can Medical Informatics Association.
Antonio Jimeno Yepes, ´Elise Prieur-Gaston, and
Aur´elie N´ev´eol. 2013. Combining medline and
publisher data to create parallel corpora for the auto-
matic translation of biomedical text. BMC bioinfor-
matics, 14(1):146.
</reference>
<page confidence="0.999016">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9849205">Using a Random Forest Classifier to Compile Bilingual Dictionaries Technical Terms from Comparable Corpora</title>
<author confidence="0.971086">Ioannis Jun’ichi Sophia</author>
<affiliation confidence="0.838361666666667">Centre for Text Mining, University of Manchester, Manchester, of Computer Science, University of Manchester, Manchester, Research Asia, Beijing,</affiliation>
<email confidence="0.99986">jtsujii@microsoft.com</email>
<abstract confidence="0.980799396067416">We describe a machine learning approach, Forest classifier, that is used to automatically compile bilingual dictionaries of technical terms from comparable corpora. We evaluate the RF classifier against a popular term alignment namely and we report an improvement of the translation accuracy. As an application, we use the automatically extracted dictionary in combination with a trained Statistical Machine Translation (SMT) system to more accutranslate The dictionary extraction method described in this is freely available 1 Background Bilingual dictionaries of technical terms are imresources for many Language tasks including Ma- Translation (Och and Ney, 2003) and Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is vast number of i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains and language pairs are scarce resources. For these reasons, the focus has shifted to comparable corpora ac.uk/postgrad/georgios.kontonatsios/ Software/RF-TermAlign.tar.gz that are more readily available, more up-to-date, larger and cheaper to construct than parallel data. Comparable corpora are collections of monolingual documents in a source and target language that share the same topic, domain and/or documents are from the same period, genre and so forth. Existing methods for bilingual lexicon extraction from comparable corpora are mainly based on the same principle. They hypothesise that a word and its translation tend to appear in similar lexical context (Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002). Context vector methods are reported to achieve robust performance on terms that occur frequently in the corpus. Chiao and Zweigenbaum (2002) achieved a performance of 94% accuracy on the top 20 candidates when translating high frequency, medical terms (frequency of 100 or more). In contrast, Morin and Daille (2010) reported an accuracy of 21% for multi-word terms occurring 20 times or less, noting that translating rare terms is a challenging problem for context vectors. Kontonatsios et al. (2013) introduced an RF classifier that is able to automatically learn association rules of textual units between a source and target language. However, they applied their method only on artificially constructed datasets containing an equal number of positive and negative instances. In the case of comparable corthe datasets are highly unbalanced (given and target terms respectively, we need to In this work, we incorporate the classification margin into the RF model, to allow the method to cope with the skewed distribution of positive and negative instances that occurs in comparable corpora. Our proposed method ranks candidate translations using the classification margin and suggests the best translation the candidate with the max- We evaluate our method on an 111 of the 14th Conference of the European Chapter of the Association for Computational pages 111–116, Sweden, April 26-30 2014. Association for Computational Linguistics English-Spanish comparable corpus of Wikipedia articles that are related to the medical sub-domain of “breast cancer”. Furthermore, we show that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT sysin order to better translate 2 Methodology A pair of terms in a source and target language is represented as a feature vector where each dimension corresponds to a unique character n-gram. value of each dimension is designates the occurrence of the corresponding n-gram in the input terms. The feature vectors that we contain where the first dimensions correspond to the n-gram features extracted the source terms and the last to those from the target terms. In the reported experiments, we use the 600 (300 source and 300 target) most frequently occurring n-grams. The underlying mechanism that allows the RF method to learn character gram mappings between terms of a source and target language is the decision trees. A node in the decision tree is a unique character n-gram. The nodes are linked through the branches of the trees and therefore the sub-spaces of and character grams are combined. Each decision tree in the forest is constructed as follows: every node is split considering n-gram features of the feature set a decision tree is fully This process is repeated and contrees. We tuned the RF classifier using 140 random trees where we observed a plateau in the classification performance. Furthermore, we set the number of random features 1 suggested by Breiman (2001). The classification margin that we use to rank the candidate translations is calculated by simply subtracting the average number of trees predicting that the input terms are not translations from the average number of decision trees predicting that the terms are mutual translations. A larger classification margin means that more decision trees in the forest classify an instance as a translation pair. For training an RF model, we use a bilingual dictionary of technical terms. When the dictionary lists more than one translation for an English term, we randomly select only one. Negative instances are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. from 000 pairs we genera training dataset of 000 and negative instances. 2.1 Baseline method The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are reusing an online list 2and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target of the comparable corpus, i.e., we collect all lexical units that: (a) occur within a window around seven-word window) and (b) are listed in the seed bilingual dictionary. The lexical units that satisfy the above two conditions are the dimensions of the context vectors. Each dimension has a value that indicates the correlation between the context lexical unit and the term In our approach, we use the log-likelihood ratio. In the second step, the seed dictionary is used to translate the lexical units of the Spanish context vectors. In this way the Spanish and English vectors become comparable. When several translations are listed in the seed dictionary, we consider of them. In the third step, we compute the coni.e., distance metric, between the vector of an English term to be translated with every projected, Spanish context vector. For this we use the cosine similarity. 3 Experiments In this section, we evaluate the two dictionary extraction methods, namely context vectors and RF, on a comparable corpus of Wikipedia articles. the evaluation metric, we use the accuracy 3and the mean reciprocal percentage of English terms whose top contain a correct translation 112 (MRR) 4as in previous approaches (Chiao and Zweigenbaum, 2002; Chiao and Zweigenbaum, 2002; Morin and Prochasson, 2011; Morin et al., 2007; Tamura et al., 2012). As a referlist, we use the UMLS In addition to this, considering that in several cases the dictionary extraction methods retrieved synonymous translations that do not appear in the reference list, we manually inspected the answers. Finally, unlike previous approaches (Chiao and Zweigenbaum, 2002), we do not restrict the test list only to those English terms whose Spanish translations are known to occur in the target corpus. In such cases, the performance of dictionary extraction methods have been shown to achieve a lower performance (Tamura et al., 2012). 3.1 Data We constructed a comparable corpus of Wikipedia articles. For this, we used Wikipedia’s search en- 6and submitted the queries “breast cancer” and “c´ancer de mama” for English and Spanish respectively. From the returned list of Wikipedia we used the 000 articles for both languages. test list contains 200 single-word terms that were extracted by considering all nouns occur more than not more than times and are listed in UMLS. For the Spanish part of the corpus, we considered all nouns as canditranslations 347 total). 3.2 Results 1 shows the accuracy and the MRR of RF and context vectors. MRR RF 0.41 0.57 0.59 0.47 Cont. Vectors 0.1 0.21 0.26 0.11 1: accuracy and MRR of and context vectors on 200 terms We observe that the proposed RF method a considerably better acthe number of English terms for which we are extracting translations and the position of the first correct translation from returned list of candidates curacy and MRR than the baseline method. Morewe segmented the 200 terms into ranges from high-frequency to rare terms. Figure 1 shows the translation accuracy at top 20 candidates for the two methods. We note Figure 1: Translation accuracy of top 20 candidates on different frequency ranges that for high frequency terms, i.e. [100,200] range, the performance achieved by the two methods is similar (53% and 52% for the RF and context vectors respectively). However, for lower frequency terms, the translation accuracy of the context vectors continuously declines. This confirms that context vectors do not behave robustly for rare terms (Morin and Daille, 2010). In contrast, the RF slightly fluctuates over different frequency ranges and presents approximately the same translation accuracy for both frequent and rare terms. 4 Application As an application of our method, we use the previously extracted dictionaries to on-line augment the phrase table of an SMT system and observe the translation performance on test sentences that contain OOV terms. For the translation probabilities in the phrase table, we use the distance metric given by the dictionary extraction methods i.e., classification margin and cosine similarity of RF and context vectors respectively, normalised by uniform probability (if a source term has candidate translations, we normalise the distance by dividing by in (Wu et al., 2008) . 4.1 Data and tools We construct a parallel, sentence-aligned corpus from the biomedical domain, following the process described in (Wu et al., 2011; Yepes et al., 2013). The parallel corpus comprises of article titles indexed by PubMed in both English and Span- We collect 120K parallel sentences for trainfrequency range contains sampled terms 113 ing the SMT and 1K sentences for evaluation. The sentences contain 200 that do not appear in the training parallel corpus. These terms occur in the Wikipedia comparable corpus. Hence, the previously extracted dictionaries list a possible translation. Using the PubMed parallel corpus, we train Moses (Koehn et al., 2007), a phrase-based SMT system. 4.2 Results We evaluated the translation performance of the SMT that uses the dictionary extracted by the RF against the following baselines: (i) Moses using only the training parallel data (Moses), (ii) Moses using the dictionary extracted by context vectors (Moses+context vector). The evaluation metric is BLEU (Papineni et al., 2002). Table 2 shows the BLEU score achieved by the systems when we append the translations to the phrase table. BLEU 1 10 20 Moses 24.22 24.22 24.22 Moses+ RF 25.32 24.626 24.42 Moses+ Context Vectors 23.88 23.69 23.74 Table 2: Translation performance when adding to the phrase table We observe that the best performance is achieved by the RF when we add the top 1 transwith a total gain of points over the baseline system. In contrast, context vectors decreased the translation performance of the SMT system. This indicates that the dictionary extracted by the context vectors is too noisy and as a result the translation performance dropped. Furthermore, it is noted that the augmented SMT systems achieve the highest performance for the top 1 while for than 1, the translation performance decreases. This behaviour is expected since the target language model was trained only on the training Spanish sentences of the parallel corpus. Hence, the target language model does not have a prior knowledge of the OOV translations and as a result it cannot choose the correct among To further investigate the effect of the language model on the translation performance of the augmented SMT systems, we conducted an oracle experiment. In this ideal setting, we assume a strong language model, that is trained on both training and test Spanish sentences of the parallel corpus, in order to assign a higher probability to a correct translation if it exists in the deployed dictionary. As we observe in Table 3, a strong language model can more accurately select the correct translation The dictionary extracted by the RF improved the translation performance points for the and vectors by the BLEU 1 10 20 Moses 28.85 28.85 28.85 Moses+ RF 30.98 31.35 31.2 Moses+ Context Vectors 28.18 29.17 29.3 Table 3: Translation performance when adding to the phrase table. SMT systems use a language model trained on training and test Spanish sentences of the parallel corpus. 5 Discussion In this paper, we presented an RF classifier that is used to extract bilingual dictionaries of technical terms from comparable corpora. We evaluated our method on a comparable corpus of Wikipedia articles. The experimental results showed that our proposed method performs robustly when translating both frequent and rare terms. As an application, we used the automatically extracted dictionary to augment the phrase table of an SMT system. The results demonstrated an improvement of the overall translation performance. As future work, we plan to integrate the RF classifier with context vectors. Intuitively, the two methods are complementary considering that the RF exploits the internal structure of terms while context vectors use the surrounding lexical context. Therefore, it will be interesting to investigate how we can incorporate the two feature spaces in a machine learner.</abstract>
<note confidence="0.894264714285714">114 6 Acknowledgements This work was funded by the European Community’s Seventh Framework Program (FP7/2007- 2013) [grant number 318736 (OSSMETER)]. References Daniel Andrade, Tetsuya Nasukawa, and Jun’ichi Tsujii. 2010. Robust measurement and comparison of context similarity for finding translation pairs. In Proceedings of the 23rd International Conference on COLING ’10, pages 19– 27, Stroudsburg, PA, USA. Association for Computational Linguistics. Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal</note>
<abstract confidence="0.541870777777778">translation and query expansion techniques for information retrieval. In SIGIR volume 31, pages 84–91. ACM. Breiman. 2001. Random Forests. Learn- 45:5–32. Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for Candidate Translational Equivalents in Comparable Corpora. In on Computational Pascale Fung and Lo Yuen Yee. 1998. An ir approach for translating new words from nonparallel, compatexts. In of the 17th international on Computational linguistics-Volume pages 414–420. Association for Computational Linguistics. Martin Kay and Martin R¨oscheisen. 1993. Textalignment. 19(1):121–142.</abstract>
<author confidence="0.75045625">Philipp Koehn</author>
<author confidence="0.75045625">Hieu Hoang</author>
<author confidence="0.75045625">Alexandra Birch</author>
<author confidence="0.75045625">Chris Callison-Burch</author>
<author confidence="0.75045625">Marcello Federico</author>
<author confidence="0.75045625">Nicola Bertoldi</author>
<author confidence="0.75045625">Brooke Cowan</author>
<author confidence="0.75045625">Wade Shen</author>
<author confidence="0.75045625">Christine Moran</author>
<author confidence="0.75045625">Richard Zens</author>
<note confidence="0.696316965517241">for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL Interactive Poster and Demonstration pages 177–180. Association for Computational Linguistics. Georgios Kontonatsios, Ioannis Korkontzelos, Jun’ichi Tsujii, and Sophia Ananiadou. 2013. Using random forest to recognise translation equivalents of terms across languages. In Proceedings of the Sixth Workshop on Building and Using pages 95–104. Association for Computational Linguistics, August. I. Dan Melamed. 1997. A portable algorithm for mapbitext correspondence. In of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computapages 305–312. Association for Computational Linguistics. Emmanuel Morin and B´eatrice Daille. 2010. Compositionality and lexical alignment of multi-word Resources and 44(1- 2):79–95. Emmanuel Morin and Emmanuel Prochasson. 2011. Bilingual lexicon extraction from comparable corenhanced with parallel corpora. In of the 4th Workshop on Building and Using Compa- Corpora: Comparable Corpora and the pages 27–34, Portland, Oregon, June. Association</note>
<title confidence="0.532203">for Computational Linguistics.</title>
<author confidence="0.974855">Emmanuel Morin</author>
<author confidence="0.974855">B´eatrice Daille</author>
<author confidence="0.974855">Koichi Takeuchi</author>
<abstract confidence="0.768678258064516">and Kyo Kageura. 2007. Bilingual terminology mining using brain, not brawn comparable corpora. of the 45th Annual Meeting of the Asof Computational pages 664– 671, Prague, Czech Republic, June. Association for Computational Linguistics. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment 29(1):19–51. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic of machine translation. In of the 40th annual meeting on association for compupages 311–318. Association for Computational Linguistics. James Pustejovsky, Jose Castano, Brent Cochran, Maciej Kotecki, and Michael Morrell. 2001. Automatic extraction of acronym-meaning pairs from databases. in health technology and (1):371–375. Reinhard Rapp. 1999. Automatic identification of word translations from unrelated english and german In of the 37th annual meeting of the Association for Computational Linguistics on pages 519–526. Association for Computational Linguistics. Helmut Schmid. 1994. Probabilistic part-of-speech using decision trees. In of the International Conference on New Methods in Lanvolume 12, pages 44–49. Manchester, UK.</abstract>
<note confidence="0.630749884615385">Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2012. Bilingual lexicon extraction from comparacorpora using label propagation. In of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Language pages 24–36. Association for Computational Linguistics. Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual cor- In of the 22nd International Conference on Computational Linguistics-Volume pages 993–1000. Association for Computational Linguistics. 115 Cuijun Wu, Fei Xia, Louise Deleger, and Imre Solti. 2011. Statistical machine translation for biomedical are we there yet? In Annual Sympovolume 2011, page 1290. American Medical Informatics Association. Antonio Jimeno Yepes, ´Elise Prieur-Gaston, and Aur´elie N´ev´eol. 2013. Combining medline and publisher data to create parallel corpora for the autotranslation of biomedical text. bioinfor- 14(1):146. 116</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Andrade</author>
<author>Tetsuya Nasukawa</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Robust measurement and comparison of context similarity for finding translation pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>19--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6997" citStr="Andrade et al., 2010" startWordPosition="1083" endWordPosition="1086">ms. When the dictionary lists more than one translation for an English term, we randomly select only one. Negative instances are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2.1 Baseline method The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word window) and (b) are listed in the seed bili</context>
</contexts>
<marker>Andrade, Nasukawa, Tsujii, 2010</marker>
<rawString>Daniel Andrade, Tetsuya Nasukawa, and Jun’ichi Tsujii. 2010. Robust measurement and comparison of context similarity for finding translation pairs. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 19– 27, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Ballesteros</author>
<author>W Bruce Croft</author>
</authors>
<title>Phrasal translation and query expansion techniques for cross-language information retrieval.</title>
<date>1997</date>
<journal>In ACM SIGIR Forum,</journal>
<volume>31</volume>
<pages>84--91</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1329" citStr="Ballesteros and Croft, 1997" startWordPosition="172" endWordPosition="176">ignment method, namely context vectors, and we report an improvement of the translation accuracy. As an application, we use the automatically extracted dictionary in combination with a trained Statistical Machine Translation (SMT) system to more accurately translate unknown terms. The dictionary extraction method described in this paper is freely available 1. 1 Background Bilingual dictionaries of technical terms are important resources for many Natural Language Processing (NLP) tasks including Statistical Machine Translation (SMT) (Och and Ney, 2003) and Cross-Language Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is a vast number of neologisms, i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains and language pairs are scarce resources. For these reasons, the focus has</context>
</contexts>
<marker>Ballesteros, Croft, 1997</marker>
<rawString>Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal translation and query expansion techniques for cross-language information retrieval. In ACM SIGIR Forum, volume 31, pages 84–91. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random Forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>45--5</pages>
<contexts>
<context position="5900" citStr="Breiman (2001)" startWordPosition="908" endWordPosition="909">odes are linked through the branches of the trees and therefore the two sub-spaces of q source and q target character grams are combined. Each decision tree in the forest is constructed as follows: every node is split by considering |φ |random n-gram features of the initial feature set Q, and a decision tree is fully grown. This process is repeated |τ |times and constructs |τ |decision trees. We tuned the RF classifier using 140 random trees where we observed a plateau in the classification performance. Furthermore, we set the number of random features using |φ |= log2 |Q |+ 1 as suggested by Breiman (2001). The classification margin that we use to rank the candidate translations is calculated by simply subtracting the average number of trees predicting that the input terms are not translations from the average number of decision trees predicting that the terms are mutual translations. A larger classification margin means that more decision trees in the forest classify an instance as a translation pair. For training an RF model, we use a bilingual dictionary of technical terms. When the dictionary lists more than one translation for an English term, we randomly select only one. Negative instance</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random Forests. Machine Learning, 45:5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Looking for Candidate Translational Equivalents in Specialized, Comparable Corpora.</title>
<date>2002</date>
<booktitle>In International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2638" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="367" endWordPosition="371">eorgios.kontonatsios/ Software/RF-TermAlign.tar.gz that are more readily available, more up-to-date, larger and cheaper to construct than parallel data. Comparable corpora are collections of monolingual documents in a source and target language that share the same topic, domain and/or documents are from the same period, genre and so forth. Existing methods for bilingual lexicon extraction from comparable corpora are mainly based on the same principle. They hypothesise that a word and its translation tend to appear in similar lexical context (Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002). Context vector methods are reported to achieve robust performance on terms that occur frequently in the corpus. Chiao and Zweigenbaum (2002) achieved a performance of 94% accuracy on the top 20 candidates when translating high frequency, medical terms (frequency of 100 or more). In contrast, Morin and Daille (2010) reported an accuracy of 21% for multi-word terms occurring 20 times or less, noting that translating rare terms is a challenging problem for context vectors. Kontonatsios et al. (2013) introduced an RF classifier that is able to automatically learn association rules of textual uni</context>
<context position="6955" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="1075" endWordPosition="1078">l, we use a bilingual dictionary of technical terms. When the dictionary lists more than one translation for an English term, we randomly select only one. Negative instances are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2.1 Baseline method The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word w</context>
<context position="8810" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="1377" endWordPosition="1380">, i.e., distance metric, between the vector of an English term to be translated with every projected, Spanish context vector. For this we use the cosine similarity. 3 Experiments In this section, we evaluate the two dictionary extraction methods, namely context vectors and RF, on a comparable corpus of Wikipedia articles. For the evaluation metric, we use the top-k translation accuracy 3 and the mean reciprocal 2http://members.unine.ch/jacques.savoy/clef/index.html 3the percentage of English terms whose top k candidates contain a correct translation 112 rank (MRR) 4 as in previous approaches (Chiao and Zweigenbaum, 2002; Chiao and Zweigenbaum, 2002; Morin and Prochasson, 2011; Morin et al., 2007; Tamura et al., 2012). As a reference list, we use the UMLS metathesaurus5. In addition to this, considering that in several cases the dictionary extraction methods retrieved synonymous translations that do not appear in the reference list, we manually inspected the answers. Finally, unlike previous approaches (Chiao and Zweigenbaum, 2002), we do not restrict the test list only to those English terms whose Spanish translations are known to occur in the target corpus. In such cases, the performance of dictionary extra</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for Candidate Translational Equivalents in Specialized, Comparable Corpora. In International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An ir approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>414--420</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2576" citStr="Fung and Yee, 1998" startWordPosition="357" endWordPosition="360">a 1http://personalpages.manchester. ac.uk/postgrad/georgios.kontonatsios/ Software/RF-TermAlign.tar.gz that are more readily available, more up-to-date, larger and cheaper to construct than parallel data. Comparable corpora are collections of monolingual documents in a source and target language that share the same topic, domain and/or documents are from the same period, genre and so forth. Existing methods for bilingual lexicon extraction from comparable corpora are mainly based on the same principle. They hypothesise that a word and its translation tend to appear in similar lexical context (Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002). Context vector methods are reported to achieve robust performance on terms that occur frequently in the corpus. Chiao and Zweigenbaum (2002) achieved a performance of 94% accuracy on the top 20 candidates when translating high frequency, medical terms (frequency of 100 or more). In contrast, Morin and Daille (2010) reported an accuracy of 21% for multi-word terms occurring 20 times or less, noting that translating rare terms is a challenging problem for context vectors. Kontonatsios et al. (2013) introduced an RF classifier that i</context>
<context position="6857" citStr="Fung and Yee, 1998" startWordPosition="1061" endWordPosition="1064">n trees in the forest classify an instance as a translation pair. For training an RF model, we use a bilingual dictionary of technical terms. When the dictionary lists more than one translation for an English term, we randomly select only one. Negative instances are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2.1 Baseline method The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An ir approach for translating new words from nonparallel, comparable texts. In Proceedings of the 17th international conference on Computational linguistics-Volume 1, pages 414–420. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
<author>Martin R¨oscheisen</author>
</authors>
<title>Texttranslation alignment. computational Linguistics,</title>
<date>1993</date>
<marker>Kay, R¨oscheisen, 1993</marker>
<rawString>Martin Kay and Martin R¨oscheisen. 1993. Texttranslation alignment. computational Linguistics, 19(1):121–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12877" citStr="Koehn et al., 2007" startWordPosition="2044" endWordPosition="2047">l domain, following the process described in (Wu et al., 2011; Yepes et al., 2013). The parallel corpus comprises of article titles indexed by PubMed in both English and Spanish. We collect 120K parallel sentences for train7each frequency range contains 100 randomly sampled terms 113 ing the SMT and 1K sentences for evaluation. The test sentences contain 1, 200 terms that do not appear in the training parallel corpus. These terms occur in the Wikipedia comparable corpus. Hence, the previously extracted dictionaries list a possible translation. Using the PubMed parallel corpus, we train Moses (Koehn et al., 2007), a phrase-based SMT system. 4.2 Results We evaluated the translation performance of the SMT that uses the dictionary extracted by the RF against the following baselines: (i) Moses using only the training parallel data (Moses), (ii) Moses using the dictionary extracted by context vectors (Moses+context vector). The evaluation metric is BLEU (Papineni et al., 2002). Table 2 shows the BLEU score achieved by the SMT systems when we append the top-k translations to the phrase table. BLEU on top-k translations 1 10 20 Moses 24.22 24.22 24.22 Moses+ RF 25.32 24.626 24.42 Moses+ Context Vectors 23.88</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Kontonatsios</author>
<author>Ioannis Korkontzelos</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Using random forest to recognise translation equivalents of biomedical terms across languages.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,</booktitle>
<pages>95--104</pages>
<contexts>
<context position="3141" citStr="Kontonatsios et al. (2013)" startWordPosition="447" endWordPosition="450">tend to appear in similar lexical context (Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002). Context vector methods are reported to achieve robust performance on terms that occur frequently in the corpus. Chiao and Zweigenbaum (2002) achieved a performance of 94% accuracy on the top 20 candidates when translating high frequency, medical terms (frequency of 100 or more). In contrast, Morin and Daille (2010) reported an accuracy of 21% for multi-word terms occurring 20 times or less, noting that translating rare terms is a challenging problem for context vectors. Kontonatsios et al. (2013) introduced an RF classifier that is able to automatically learn association rules of textual units between a source and target language. However, they applied their method only on artificially constructed datasets containing an equal number of positive and negative instances. In the case of comparable corpora, the datasets are highly unbalanced (given n, m source and target terms respectively, we need to classify n × m instances). In this work, we incorporate the classification margin into the RF model, to allow the method to cope with the skewed distribution of positive and negative instance</context>
</contexts>
<marker>Kontonatsios, Korkontzelos, Tsujii, Ananiadou, 2013</marker>
<rawString>Georgios Kontonatsios, Ioannis Korkontzelos, Jun’ichi Tsujii, and Sophia Ananiadou. 2013. Using random forest to recognise translation equivalents of biomedical terms across languages. In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora, pages 95–104. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>A portable algorithm for mapping bitext correspondence.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>305--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1752" citStr="Melamed, 1997" startWordPosition="240" endWordPosition="241">es for many Natural Language Processing (NLP) tasks including Statistical Machine Translation (SMT) (Och and Ney, 2003) and Cross-Language Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is a vast number of neologisms, i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains and language pairs are scarce resources. For these reasons, the focus has shifted to comparable corpora 1http://personalpages.manchester. ac.uk/postgrad/georgios.kontonatsios/ Software/RF-TermAlign.tar.gz that are more readily available, more up-to-date, larger and cheaper to construct than parallel data. Comparable corpora are collections of monolingual documents in a source and target language that share the same topic, domain and/or documents are from the same period, genre and so forth. </context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. A portable algorithm for mapping bitext correspondence. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 305–312. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>B´eatrice Daille</author>
</authors>
<title>Compositionality and lexical alignment of multi-word terms. Language Resources and Evaluation,</title>
<date>2010</date>
<pages>44--1</pages>
<contexts>
<context position="2956" citStr="Morin and Daille (2010)" startWordPosition="418" endWordPosition="421">enre and so forth. Existing methods for bilingual lexicon extraction from comparable corpora are mainly based on the same principle. They hypothesise that a word and its translation tend to appear in similar lexical context (Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002). Context vector methods are reported to achieve robust performance on terms that occur frequently in the corpus. Chiao and Zweigenbaum (2002) achieved a performance of 94% accuracy on the top 20 candidates when translating high frequency, medical terms (frequency of 100 or more). In contrast, Morin and Daille (2010) reported an accuracy of 21% for multi-word terms occurring 20 times or less, noting that translating rare terms is a challenging problem for context vectors. Kontonatsios et al. (2013) introduced an RF classifier that is able to automatically learn association rules of textual units between a source and target language. However, they applied their method only on artificially constructed datasets containing an equal number of positive and negative instances. In the case of comparable corpora, the datasets are highly unbalanced (given n, m source and target terms respectively, we need to classi</context>
<context position="11403" citStr="Morin and Daille, 2010" startWordPosition="1807" endWordPosition="1810">1, 200 test terms into 7 frequency ranges 7, from high-frequency to rare terms. Figure 1 shows the translation accuracy at top 20 candidates for the two methods. We note Figure 1: Translation accuracy of top 20 candidates on different frequency ranges that for high frequency terms, i.e. [100,200] range, the performance achieved by the two methods is similar (53% and 52% for the RF and context vectors respectively). However, for lower frequency terms, the translation accuracy of the context vectors continuously declines. This confirms that context vectors do not behave robustly for rare terms (Morin and Daille, 2010). In contrast, the RF slightly fluctuates over different frequency ranges and presents approximately the same translation accuracy for both frequent and rare terms. 4 Application As an application of our method, we use the previously extracted dictionaries to on-line augment the phrase table of an SMT system and observe the translation performance on test sentences that contain OOV terms. For the translation probabilities in the phrase table, we use the distance metric given by the dictionary extraction methods i.e., classification margin and cosine similarity of RF and context vectors respect</context>
</contexts>
<marker>Morin, Daille, 2010</marker>
<rawString>Emmanuel Morin and B´eatrice Daille. 2010. Compositionality and lexical alignment of multi-word terms. Language Resources and Evaluation, 44(1-2):79–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>Emmanuel Prochasson</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,</booktitle>
<pages>27--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="7026" citStr="Morin and Prochasson, 2011" startWordPosition="1087" endWordPosition="1091">y lists more than one translation for an English term, we randomly select only one. Negative instances are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2.1 Baseline method The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word window) and (b) are listed in the seed bilingual dictionary. The lexical</context>
<context position="8867" citStr="Morin and Prochasson, 2011" startWordPosition="1386" endWordPosition="1389">term to be translated with every projected, Spanish context vector. For this we use the cosine similarity. 3 Experiments In this section, we evaluate the two dictionary extraction methods, namely context vectors and RF, on a comparable corpus of Wikipedia articles. For the evaluation metric, we use the top-k translation accuracy 3 and the mean reciprocal 2http://members.unine.ch/jacques.savoy/clef/index.html 3the percentage of English terms whose top k candidates contain a correct translation 112 rank (MRR) 4 as in previous approaches (Chiao and Zweigenbaum, 2002; Chiao and Zweigenbaum, 2002; Morin and Prochasson, 2011; Morin et al., 2007; Tamura et al., 2012). As a reference list, we use the UMLS metathesaurus5. In addition to this, considering that in several cases the dictionary extraction methods retrieved synonymous translations that do not appear in the reference list, we manually inspected the answers. Finally, unlike previous approaches (Chiao and Zweigenbaum, 2002), we do not restrict the test list only to those English terms whose Spanish translations are known to occur in the target corpus. In such cases, the performance of dictionary extraction methods have been shown to achieve a lower performa</context>
</contexts>
<marker>Morin, Prochasson, 2011</marker>
<rawString>Emmanuel Morin and Emmanuel Prochasson. 2011. Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora. In Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web, pages 27–34, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>B´eatrice Daille</author>
<author>Koichi Takeuchi</author>
<author>Kyo Kageura</author>
</authors>
<title>Bilingual terminology mining - using brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>664--671</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2608" citStr="Morin et al., 2007" startWordPosition="363" endWordPosition="366">er. ac.uk/postgrad/georgios.kontonatsios/ Software/RF-TermAlign.tar.gz that are more readily available, more up-to-date, larger and cheaper to construct than parallel data. Comparable corpora are collections of monolingual documents in a source and target language that share the same topic, domain and/or documents are from the same period, genre and so forth. Existing methods for bilingual lexicon extraction from comparable corpora are mainly based on the same principle. They hypothesise that a word and its translation tend to appear in similar lexical context (Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002). Context vector methods are reported to achieve robust performance on terms that occur frequently in the corpus. Chiao and Zweigenbaum (2002) achieved a performance of 94% accuracy on the top 20 candidates when translating high frequency, medical terms (frequency of 100 or more). In contrast, Morin and Daille (2010) reported an accuracy of 21% for multi-word terms occurring 20 times or less, noting that translating rare terms is a challenging problem for context vectors. Kontonatsios et al. (2013) introduced an RF classifier that is able to automatically learn as</context>
<context position="6975" citStr="Morin et al., 2007" startWordPosition="1079" endWordPosition="1082">ary of technical terms. When the dictionary lists more than one translation for an English term, we randomly select only one. Negative instances are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2.1 Baseline method The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word window) and (b) are l</context>
<context position="8887" citStr="Morin et al., 2007" startWordPosition="1390" endWordPosition="1393">very projected, Spanish context vector. For this we use the cosine similarity. 3 Experiments In this section, we evaluate the two dictionary extraction methods, namely context vectors and RF, on a comparable corpus of Wikipedia articles. For the evaluation metric, we use the top-k translation accuracy 3 and the mean reciprocal 2http://members.unine.ch/jacques.savoy/clef/index.html 3the percentage of English terms whose top k candidates contain a correct translation 112 rank (MRR) 4 as in previous approaches (Chiao and Zweigenbaum, 2002; Chiao and Zweigenbaum, 2002; Morin and Prochasson, 2011; Morin et al., 2007; Tamura et al., 2012). As a reference list, we use the UMLS metathesaurus5. In addition to this, considering that in several cases the dictionary extraction methods retrieved synonymous translations that do not appear in the reference list, we manually inspected the answers. Finally, unlike previous approaches (Chiao and Zweigenbaum, 2002), we do not restrict the test list only to those English terms whose Spanish translations are known to occur in the target corpus. In such cases, the performance of dictionary extraction methods have been shown to achieve a lower performance (Tamura et al., </context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and Kyo Kageura. 2007. Bilingual terminology mining - using brain, not brawn comparable corpora. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 664– 671, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="1258" citStr="Och and Ney, 2003" startWordPosition="164" endWordPosition="167">pora. We evaluate the RF classifier against a popular term alignment method, namely context vectors, and we report an improvement of the translation accuracy. As an application, we use the automatically extracted dictionary in combination with a trained Statistical Machine Translation (SMT) system to more accurately translate unknown terms. The dictionary extraction method described in this paper is freely available 1. 1 Background Bilingual dictionaries of technical terms are important resources for many Natural Language Processing (NLP) tasks including Statistical Machine Translation (SMT) (Och and Ney, 2003) and Cross-Language Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is a vast number of neologisms, i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains an</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13243" citStr="Papineni et al., 2002" startWordPosition="2099" endWordPosition="2102"> terms that do not appear in the training parallel corpus. These terms occur in the Wikipedia comparable corpus. Hence, the previously extracted dictionaries list a possible translation. Using the PubMed parallel corpus, we train Moses (Koehn et al., 2007), a phrase-based SMT system. 4.2 Results We evaluated the translation performance of the SMT that uses the dictionary extracted by the RF against the following baselines: (i) Moses using only the training parallel data (Moses), (ii) Moses using the dictionary extracted by context vectors (Moses+context vector). The evaluation metric is BLEU (Papineni et al., 2002). Table 2 shows the BLEU score achieved by the SMT systems when we append the top-k translations to the phrase table. BLEU on top-k translations 1 10 20 Moses 24.22 24.22 24.22 Moses+ RF 25.32 24.626 24.42 Moses+ Context Vectors 23.88 23.69 23.74 Table 2: Translation performance when adding top-k translations to the phrase table We observe that the best performance is achieved by the RF when we add the top 1 translation with a total gain of 1.1 BLEU points over the baseline system. In contrast, context vectors decreased the translation performance of the SMT system. This indicates that the dic</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jose Castano</author>
<author>Brent Cochran</author>
<author>Maciej Kotecki</author>
<author>Michael Morrell</author>
</authors>
<title>Automatic extraction of acronym-meaning pairs from medline databases.</title>
<date>2001</date>
<booktitle>Studies in health technology and informatics,</booktitle>
<pages>1--371</pages>
<contexts>
<context position="1627" citStr="Pustejovsky et al., 2001" startWordPosition="222" endWordPosition="225">action method described in this paper is freely available 1. 1 Background Bilingual dictionaries of technical terms are important resources for many Natural Language Processing (NLP) tasks including Statistical Machine Translation (SMT) (Och and Ney, 2003) and Cross-Language Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is a vast number of neologisms, i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains and language pairs are scarce resources. For these reasons, the focus has shifted to comparable corpora 1http://personalpages.manchester. ac.uk/postgrad/georgios.kontonatsios/ Software/RF-TermAlign.tar.gz that are more readily available, more up-to-date, larger and cheaper to construct than parallel data. Comparable corpora are collections of monolingual documents in a</context>
</contexts>
<marker>Pustejovsky, Castano, Cochran, Kotecki, Morrell, 2001</marker>
<rawString>James Pustejovsky, Jose Castano, Brent Cochran, Maciej Kotecki, and Michael Morrell. 2001. Automatic extraction of acronym-meaning pairs from medline databases. Studies in health technology and informatics, (1):371–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated english and german corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>519--526</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2588" citStr="Rapp, 1999" startWordPosition="361" endWordPosition="362">ges.manchester. ac.uk/postgrad/georgios.kontonatsios/ Software/RF-TermAlign.tar.gz that are more readily available, more up-to-date, larger and cheaper to construct than parallel data. Comparable corpora are collections of monolingual documents in a source and target language that share the same topic, domain and/or documents are from the same period, genre and so forth. Existing methods for bilingual lexicon extraction from comparable corpora are mainly based on the same principle. They hypothesise that a word and its translation tend to appear in similar lexical context (Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002). Context vector methods are reported to achieve robust performance on terms that occur frequently in the corpus. Chiao and Zweigenbaum (2002) achieved a performance of 94% accuracy on the top 20 candidates when translating high frequency, medical terms (frequency of 100 or more). In contrast, Morin and Daille (2010) reported an accuracy of 21% for multi-word terms occurring 20 times or less, noting that translating rare terms is a challenging problem for context vectors. Kontonatsios et al. (2013) introduced an RF classifier that is able to au</context>
<context position="6870" citStr="Rapp, 1999" startWordPosition="1065" endWordPosition="1066">t classify an instance as a translation pair. For training an RF model, we use a bilingual dictionary of technical terms. When the dictionary lists more than one translation for an English term, we randomly select only one. Negative instances are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2.1 Baseline method The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect </context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated english and german corpora. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 519–526. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<volume>12</volume>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="7271" citStr="Schmid, 1994" startWordPosition="1127" endWordPosition="1128"> from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2.1 Baseline method The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word window) and (b) are listed in the seed bilingual dictionary. The lexical units that satisfy the above two conditions are the dimensions of the context vectors. Each dimension has a value that indicates the correlation between the context lexical unit and the term i. In our approach, we use the log-likelihood ratio. </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, volume 12, pages 44–49. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using label propagation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>24--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8909" citStr="Tamura et al., 2012" startWordPosition="1394" endWordPosition="1397">ish context vector. For this we use the cosine similarity. 3 Experiments In this section, we evaluate the two dictionary extraction methods, namely context vectors and RF, on a comparable corpus of Wikipedia articles. For the evaluation metric, we use the top-k translation accuracy 3 and the mean reciprocal 2http://members.unine.ch/jacques.savoy/clef/index.html 3the percentage of English terms whose top k candidates contain a correct translation 112 rank (MRR) 4 as in previous approaches (Chiao and Zweigenbaum, 2002; Chiao and Zweigenbaum, 2002; Morin and Prochasson, 2011; Morin et al., 2007; Tamura et al., 2012). As a reference list, we use the UMLS metathesaurus5. In addition to this, considering that in several cases the dictionary extraction methods retrieved synonymous translations that do not appear in the reference list, we manually inspected the answers. Finally, unlike previous approaches (Chiao and Zweigenbaum, 2002), we do not restrict the test list only to those English terms whose Spanish translations are known to occur in the target corpus. In such cases, the performance of dictionary extraction methods have been shown to achieve a lower performance (Tamura et al., 2012). 3.1 Data We con</context>
</contexts>
<marker>Tamura, Watanabe, Sumita, 2012</marker>
<rawString>Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2012. Bilingual lexicon extraction from comparable corpora using label propagation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 24–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>993--1000</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12169" citStr="Wu et al., 2008" startWordPosition="1928" endWordPosition="1931">nd rare terms. 4 Application As an application of our method, we use the previously extracted dictionaries to on-line augment the phrase table of an SMT system and observe the translation performance on test sentences that contain OOV terms. For the translation probabilities in the phrase table, we use the distance metric given by the dictionary extraction methods i.e., classification margin and cosine similarity of RF and context vectors respectively, normalised by the uniform probability (if a source term has m candidate translations, we normalise the distance metric by dividing by m as in (Wu et al., 2008) . 4.1 Data and tools We construct a parallel, sentence-aligned corpus from the biomedical domain, following the process described in (Wu et al., 2011; Yepes et al., 2013). The parallel corpus comprises of article titles indexed by PubMed in both English and Spanish. We collect 120K parallel sentences for train7each frequency range contains 100 randomly sampled terms 113 ing the SMT and 1K sentences for evaluation. The test sentences contain 1, 200 terms that do not appear in the training parallel corpus. These terms occur in the Wikipedia comparable corpus. Hence, the previously extracted dic</context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 993–1000. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cuijun Wu</author>
<author>Fei Xia</author>
<author>Louise Deleger</author>
<author>Imre Solti</author>
</authors>
<title>Statistical machine translation for biomedical text: are we there yet?</title>
<date>2011</date>
<booktitle>In AMIA Annual Symposium Proceedings, volume 2011,</booktitle>
<pages>1290</pages>
<publisher>American Medical Informatics Association.</publisher>
<contexts>
<context position="12319" citStr="Wu et al., 2011" startWordPosition="1953" endWordPosition="1956">T system and observe the translation performance on test sentences that contain OOV terms. For the translation probabilities in the phrase table, we use the distance metric given by the dictionary extraction methods i.e., classification margin and cosine similarity of RF and context vectors respectively, normalised by the uniform probability (if a source term has m candidate translations, we normalise the distance metric by dividing by m as in (Wu et al., 2008) . 4.1 Data and tools We construct a parallel, sentence-aligned corpus from the biomedical domain, following the process described in (Wu et al., 2011; Yepes et al., 2013). The parallel corpus comprises of article titles indexed by PubMed in both English and Spanish. We collect 120K parallel sentences for train7each frequency range contains 100 randomly sampled terms 113 ing the SMT and 1K sentences for evaluation. The test sentences contain 1, 200 terms that do not appear in the training parallel corpus. These terms occur in the Wikipedia comparable corpus. Hence, the previously extracted dictionaries list a possible translation. Using the PubMed parallel corpus, we train Moses (Koehn et al., 2007), a phrase-based SMT system. 4.2 Results W</context>
</contexts>
<marker>Wu, Xia, Deleger, Solti, 2011</marker>
<rawString>Cuijun Wu, Fei Xia, Louise Deleger, and Imre Solti. 2011. Statistical machine translation for biomedical text: are we there yet? In AMIA Annual Symposium Proceedings, volume 2011, page 1290. American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Jimeno Yepes</author>
<author>´Elise Prieur-Gaston</author>
<author>Aur´elie N´ev´eol</author>
</authors>
<title>Combining medline and publisher data to create parallel corpora for the automatic translation of biomedical text.</title>
<date>2013</date>
<journal>BMC bioinformatics,</journal>
<volume>14</volume>
<issue>1</issue>
<marker>Yepes, Prieur-Gaston, N´ev´eol, 2013</marker>
<rawString>Antonio Jimeno Yepes, ´Elise Prieur-Gaston, and Aur´elie N´ev´eol. 2013. Combining medline and publisher data to create parallel corpora for the automatic translation of biomedical text. BMC bioinformatics, 14(1):146.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>