<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<note confidence="0.959754666666667">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 63-70
Edmonton, May-June 2003
</note>
<title confidence="0.9996205">
A Weighted Finite State Transducer Implementation of the Alignment
Template Model for Statistical Machine Translation
</title>
<author confidence="0.995244">
Shankar Kumar and William Byrne
</author>
<affiliation confidence="0.8202605">
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD, 21218, USA
</affiliation>
<email confidence="0.997359">
skumar,byrne @jhu.edu
</email>
<sectionHeader confidence="0.993846" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871111111111">
We present a derivation of the alignment tem-
plate model for statistical machine translation
and an implementation of the model using
weighted finite state transducers. The approach
we describe allows us to implement each con-
stituent distribution of the model as a weighted
finite state transducer or acceptor. We show
that bitext word alignment and translation un-
der the model can be performed with standard
FSM operations involving these transducers.
One of the benefits of using this framework
is that it obviates the need to develop special-
ized search procedures, even for the generation
of lattices or N-Best lists of bitext word align-
ments and translation hypotheses. We evaluate
the implementation of the model on the French-
to-English Hansards task and report alignment
and translation performance.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999751954545455">
The Alignment Template Translation Model
(ATTM) (Och et al., 1999) has emerged as a promising
modeling framework for statistical machine translation.
The ATTM attempts to overcome the deficiencies of
word-to-word translation models (Brown et al., 1993)
through the use of phrasal translations. The overall
model is based on a two-level alignment between the
source and the target sentence: a phrase-level alignment
between source and target phrases and a word-level
alignment between words in these phrase pairs.
The goal of this paper is to reformulate the ATTM
so that the operations we intend to perform under a sta-
tistical translation model, namely bitext word alignment
and translation, can be implementation using standard
weighted finite state transducer (WFST) operations. Our
main motivation for a WFST modeling framework lies
in the resulting simplicity of alignment and translation
processes compared to dynamic programming or de-
coders. The WFST implementation allows us to use stan-
dard optimized algorithms available from an off-the-shelf
FSM toolkit (Mohri et al., 1997). This avoids the need to
develop specialized search procedures, even for the gen-
</bodyText>
<sectionHeader confidence="0.9070685" genericHeader="method">
SOURCE LANGUAGE SENTENCE
TARGET LANGUAGE SENTENCE
</sectionHeader>
<figureCaption confidence="0.999482">
Figure 1: ATTM Architecture.
</figureCaption>
<bodyText confidence="0.996549041666667">
eration of lattices or N-best lists of bitext word alignment
or translation hypotheses.
Weighted Finite State Transducers for Statistical Ma-
chine Translation (SMT) have been proposed in the
literature to implement word-to-word translation mod-
els (Knight and Al-Onaizan, 1998) or to perform trans-
lation in an application domain such as the call routing
task (Bangalore and Ricardi, 2001). One of the objec-
tives of these approaches has been to provide an imple-
mentation for SMT that uses standard FSM algorithms
to perform model computations and therefore make SMT
techniques accessible to a wider community. Our WFST
implementation of the ATTM has been developed with
similar objectives.
We start off by presenting a derivation of the ATTM
that identifies the conditional independence assumptions
that underly the model. The derivation allows us to spec-
ify each component distribution of the model and imple-
ment it as a weighted finite state transducer. We then
show that bitext word alignment and translation can be
performed with standard FSM operations involving these
transducers. Finally we report bitext word alignment
and translation performance of the implementation on the
Canadian French-to-English Hansards task.
</bodyText>
<figure confidence="0.987337617647059">
SOURCE
SEGMENTATION
MODEL
TARGET
LANGUAGE MODEL
PERMUTATION
MODEL
TEMPLATE
SEQUENCE
MODEL
PHRASAL
TRANSLATION
MODEL
PHRASE
e1 e2
f f f f f f f7
1 2 3 4 5 6
v
a a a
v2 1 v1 2 v3 3
z 1
u u u
1 2
1
e3
e4 e5 e6 e e e
7 8 9
v2 v3
z2
z3
3
source language phrases
alignment templates
target language phrases
</figure>
<sectionHeader confidence="0.780686" genericHeader="method">
2 Alignment Template Translation Models
</sectionHeader>
<bodyText confidence="0.986419534883721">
We present here a derivation of the alignment template
translation model (ATTM) (Och et al., 1999; Och, 2002)
and give an implementation of the model using weighted
finite state transducers (WFSTs). The finite state model-
ing is performed using the AT&amp;T FSM Toolkit (Mohri et
al., 1997).
In this model, the translation of a source language sen-
tence to a target language sentence is described by a joint
probability distribution over all possible segmentations
and alignments. This distribution is presented in Figure 1
and Equations 1-7. The components of the overall trans-
lation model are the source language model (Term 2),
the source segmentation model (Term 3), the phrase per-
mutation model (Term 4), the template sequence model
(Term 5), the phrasal translation model (Term 6) and the
target language model (Term 7). Each of these condi-
tional distributions is modeled independently and we now
define each in turn and present its implementation as a
weighted finite state acceptor or transducer.
.
Similarly, a phrase in the source language sentence con-
tains words , where is the NULL token.
We assume that each word in each language can be as-
signed to a unique class so that unambiguously spec-
ifies a class sequence and specifies the class se-
quence . Throughout the model, if a sentence is
segmented into phrases , we say to indi-
cate that the words in the phrase sequence agree with the
original sentence.
Source Language Model The model assigns probabil-
ity to any sentence in the source language; this prob-
ability is not actually needed by the translation process
when is given. As the first component in the model, a
finite state acceptor is constructed for.
Source Segmentation Model We introduce the phrase
count random variable which specifies the number of
phrases in a particular segmentation of the source lan-
guage sentence. For a sentence of length, there are
ways to segment it into phrases. Motivated by
this, we choose the distribution as
so that .
We construct a joint distribution over all phrase seg-
mentations as
</bodyText>
<equation confidence="0.735831">
(9)
</equation>
<bodyText confidence="0.937966">
where
The normalization constant
</bodyText>
<equation confidence="0.6344755">
, is chosen so that
.
</equation>
<bodyText confidence="0.939261185185185">
Here, is a “unigram” distribution over source
language phrases; we assume that we have an inventory
of phrases from which this quantity can be estimated. In
this way, the likelihood of a particular segmentation is
determined by the likelihood of the phrases that result.
We now describe the finite state implementation of the
source segmentation model and show how to compute the
most likely segmentation under the model:
.
1. For each source language sentence to be trans-
lated, we implement a weighted finite state trans-
ducer that segments the sentence into all possible
phrase sequences permissible given the inven-
tory of phrases. The score of a segmentation
under is given by . We then generate
a lattice of segmentations of (implemented as an
acceptor ) by composing it with the transducer ,
i.e. .
2. We then decompose into disjoint subsets
so that
contains all segmentations of the source language
sentence with exactly phrases. To construct ,
we create an unweighted acceptor that accepts
any phrase sequence of length ; for efficiency, the
phrase vocabulary is restricted to the phrases in .
is then obtained by the finite state composition
.
</bodyText>
<listItem confidence="0.849184">
3. For
</listItem>
<bodyText confidence="0.996116833333333">
The normalization factors are obtained by sum-
ming the probabilities of all segmentations in .
This sum can be computed efficiently using lattice
forward probabilities (Wessel et al., 1998). For
a fixed , the most likely segmentation in is
found as
</bodyText>
<listItem confidence="0.932080333333333">
(10)
4. Finally we select the optimal segmentation as
(11)
</listItem>
<bodyText confidence="0.962637357142857">
We begin by distinguishing words and phrases. We as-
sume that is a phrase in the target language sentence
that has length and consists of words
(8)
A portion of the segmentation transducer for the
French sentence nous avons une inflation galopante is
presented in Figure 2. When composed with , gen-
erates the following two phrase segmentations: nous
avons une inflation galopante and nous avons une in-
flation galopante. The “ ” symbol is used to indicate
phrases formed by concatenation of consecutive words.
The phrases specified by the source segmentation model
remain in the order that they appear in the source sen-
tence.
</bodyText>
<figureCaption confidence="0.8275415">
Figure 2: A portion of the phrase segmentation transducer
for the sentence “nous avons une inflation galopante”.
</figureCaption>
<bodyText confidence="0.990355258064516">
Phrase Permutation Model We now define a model
for the reordering of phrase sequences as determined
by the previous model. The phrase alignment sequence
specifies a reordering of phrases into target language
phrase order; the words within the phrases remain in the
source language order. The phrase sequence is re-
ordered into . The phrase alignment se-
quence is modeled as a first order Markov process
with . The alignment sequence distri-
bution is constructed to assign decreasing likelihood to
phrase re-orderings that diverge from the original word
order. Suppose and , we set the
Markov chain probabilities as follows (Och et al., 1999)
In the above equations, is a tuning factor and
we normalize the probabilities so that
.
The finite state implementation of this model involves
two acceptors. We first build a unweighted permutation
acceptor that contains all permutations of the phrase
sequence in the source language (Knight and Al-
Onaizan, 1998) . We note that a path through corre-
sponds to an alignment sequence . Figure 3 shows the
acceptor for the source phrase sequence nous avons
une inflation galopante.
A source phrase sequence of length words re-
quires a permutation acceptor of states. For
long phrase sequences we compute a score
for each arc and then prune the arcs by this
score, i.e. phrase alignments containing are in-
cluded only if this score is above a threshold. Pruning
can therefore be applied while is constructed.
</bodyText>
<figureCaption confidence="0.994344">
Figure 3: The permutation acceptor for the
</figureCaption>
<bodyText confidence="0.993798444444445">
source-language phrase sequence nous avons
une inflation galopante.
The second acceptor in the implementation of the
phrase permutation model assigns alignment probabil-
ities (Equation 13) to a given permutation of the
source phrase sequence (Figure 4). In this example,
the phrases in the source phrase sequence are specified as
follows: (nous), (avons) and
(une inflation galopante). We now show the computa-
tion of some of the alignment probabilities (Equation 13)
(❲
in this example )
Normalizing these terms gives
and .
Template Sequence Model Here we describe the
main component of the model. An alignment template
specifies the allowable alignments be-
tween the class sequences and . is a
binary, 0/1 valued matrix which is constructed
as follows: If can be aligned to , then ;
otherwise . This process may allow to align
with the NULL token , i.e. , so that words
can be freely inserted in translation. Given a pair of class
sequences and , we specify exactly one matrix
.
We say that is consistent with the
target language phrase and the source language phrase
</bodyText>
<figure confidence="0.993591357142857">
avons : E
nous : E
une: E
inflation: E
galopante: E
une: E
inflation: E
galopante: E
avons
nous
avons
avons
avons
une_inflation_galopante/0.55
</figure>
<figureCaption confidence="0.99995">
Figure 4: Acceptor that assigns probabilities to per-
</figureCaption>
<bodyText confidence="0.871152">
mutations of the source language phrase sequence nous
</bodyText>
<subsubsectionHeader confidence="0.77532">
avons une inflation galopante (❲ ).
</subsubsectionHeader>
<bodyText confidence="0.995854879310345">
if is the class sequence for and is the class
sequence for.
In Section 4.1, we will outline a procedure to build
a library of alignment templates from bitext word-level
alignments. Each template used in our
model has an index in this template library. Therefore
any operation that involves a mapping to (from) template
sequences will be implemented as a mapping to (from) a
sequence of these indices.
We have described the segmentation and permutation
processes that transform a source language sentence into
phrases in target language phrase order. The next step
is to generate a consistent sequence of alignment tem-
plates. We assume that the templates are conditionally
independent of each other and depend only on the source
language phrase which generated each of them
We will implement this model using the transducer that
maps any permutation of the phrase se-
quence into a template sequence with probability
as in Equation 14. For every phrase, this transducer al-
lows only the templates that are consistent with with
probability , i.e. enforces the consis-
tency between each source phrase and alignment tem-
plate.
Phrasal Translation Model We assume that a tar-
get phrase is generated independently by each alignment
template and source phrase
This allows us to describe the phrase-internal transla-
tion model as follows. We assume that each
word in the target phrase is produced independently and
that the consistency is enforced between the words in
and the class sequence so that if
.
We now introduce the word alignment variables
, which indicates that is aligned to
within and .
We have assumed that , i.e. that
given the template, word alignments do not depend on the
source language phrase.
For a given phrase and a consistent alignment tem-
plate , a weighted acceptor can be
constructed to assign probability to translated phrases ac-
cording to Equations 16 and 17. is constructed from
four component machines , ,and , constructed as
follows.
The first acceptor implements the alignment matrix
. It has states and between any pair of states
and, each arc corresponds to a word alignment vari-
able . Therefore the number of transitions between
statesand is equal to the number of non-zero val-
ues of . The arc from state tohas probability
(Equation 17).
The second machine is an unweighted transducer that
maps the index in the phrase to
the corresponding word .
The third transducer is the lexicon transducer that
maps the source word to the target word
with probability
</bodyText>
<figure confidence="0.804435121212121">
nous/0.33
nous/0.45
avons/0.53
(14)
The term is a translation dictionary (Och and
Ney, 2000) and is obtained as
.
The fourth acceptor is unweighted and allows all tar-
get word sequences which can be specified by the
3
2
1
F
0
Z2
Z1
Z3
ε : have ε : a
/0.42 /0.07
z2:
ε : run ε : away
ε
/0.5 /0.01
z3:
ε
ε: ε
z1 : ε
ε :inflation
/0.44
ε :we
ε: ε
/0.72
ε: ε
</figure>
<figureCaption confidence="0.923111">
Figure 6: Transducer that maps the source template
</figureCaption>
<figure confidence="0.9417596">
sequence into target phrase sequences.
3 /0.5
z
C
Z
3/1.0 0/1.0
i=1 i=2 i=3
inflation: inflation / 0.85
galopante : inflation / 0.04
galopante : run / 0.50
NULL : away / 0.01
0 : NULL
1 : une
2 : inflation
3 : galopante
2/0.5
D
O
r
/0.5 /0.01 /0.44
run away
inflation
run
away
inflation
E = run away inflation
F = une inflation galopante
A
1 2 3
E
</figure>
<figureCaption confidence="0.8201125">
Figure 5: Component transducers to construct the accep-
tor for an alignment template.
</figureCaption>
<bodyText confidence="0.995375404255319">
class sequence . has states. The number
of transitions between states and is equal to the
number of target language words with class specified by
.
Figure 5 shows all the four component FSTs for build-
ing the transducer corresponding to an alignment tem-
plate from our library. Having built these four machines,
we obtain as follows. We first compose the four trans-
ducers, project the resulting transducer onto the output la-
bels, and determinize it under the semiring. This
is implemented using AT&amp;T FSM tools as follows
fsmcompose O I D Cfsmproject -o
fsmrmepsilonfsmdeterminize .
Given an alignment template and a consistent source
phrase, we note that the composition and determiniza-
tion operations assign the probability (Equa-
tion 16) to each consistent target phrase. This summa-
rizes the construction of a transducer for a single align-
ment template.
We now implement a transducer that maps se-
quences of alignment templates to target language word
sequences. We identify all templates consistent with the
phrases in the source language phrase sequence . The
transducer is constructed via the FSM union operation
of the transducers that implement these templates.
For the source phrase sequence (nous avons
une inflation galopante), we show the transducer in
Figure 6. Our example library consists of three tem-
plates, and . maps the source word nous
to the target word we via the word alignment matrix
specified as . maps the source word
avons to the target phrase have a via the word align-
ment matrix specified as
the source phrase une inflation galopante to the target
phrase run away inflation via the word alignment matrix
specified as .
is built out of the three component acceptors ,
, and . The acceptor corresponds to the map-
ping from the template and the source phrase to all
consistent target phrases .
Target Language Model We specify this model as
where enforces the requirement that words
in the translation agree with those in the phrase sequence.
We note that is modeled as a standard backoff
trigram language model (Stolcke, 2002). Such a language
model can be easily compiled as a weighted finite state
acceptor (Mohri et al., 2002).
</bodyText>
<sectionHeader confidence="0.970864" genericHeader="method">
3 Alignment and Translation Via WFSTs
</sectionHeader>
<bodyText confidence="0.988383875">
We will now describe how the alignment template trans-
lation model can be used to perform word-level alignment
of bitexts and translation of source language sentences.
Given a source language sentence and a target sen-
tence, the word-to-word alignment between the sen-
tences can be found as
The variables specify the alignment
between source phrases and target phrases while gives
the word-to-word alignment within the phrase sequences.
Given a source language sentence, the translation
can be found as
. maps
where is the translation of.
We implement the alignment and translation proce-
dures in two steps. We first segment the source sentence
into phrases, as described earlier
</bodyText>
<figure confidence="0.7486074">
(18)
After segmenting the source sentence, the alignment of
a sentence pair is obtained as
(19)
(20)
</figure>
<bodyText confidence="0.997321142857143">
We have described how to compute the optimal seg-
mentation (Equation 18) in Section 2. The seg-
mentation process decomposes the source sentence
into a phrase sequence . This process also tags each
source phrase with its position in the phrase se-
quence. We will now describe the alignment and trans-
lation processes using finite state operations.
</bodyText>
<subsectionHeader confidence="0.999082">
3.1 Bitext Word Alignment
</subsectionHeader>
<bodyText confidence="0.994881547619048">
Given a collection of alignment templates, it is not guar-
anteed that every sentence pair in a bitext can be seg-
mented into phrases for which there exist the consistent
alignment templates needed to create an alignment be-
tween the sentences. We find in practice that this prob-
lem arises frequently enough that most sentence pairs
are assigned a probability of zero under the template
model. To overcome this limitation, we add several types
of “dummy” templates to the library that serve to align
phrases when consistent templates could not otherwise
be found.
The first type of dummy template we introduce al-
lows any source phrase to align with any single word
target phrase . This template is defined as a triple
where and
. All the entries of the matrix are speci-
fied to be ones. The second type of dummy template al-
lows source phrases to be deleted during the alignment
process. For a source phrase we specify this tem-
plate as . The third type
of template allows for insertions of single word target
phrases. For a target phrase we specify this template as
. The probabilities for
these added templates are not estimated; they are fixed as
a global constant which is set so as to discourage their use
except when no other suitable templates are available.
A lattice of possible alignments between and is
then obtained by the finite state composition
(21)
where is an acceptor for the target sentence. We then
compute the ML alignment (Equation 19) by obtain-
ing the path with the highest probability, in . The path
determines three types of alignments: phrasal align-
ment between the source phrase and the target phrase
; deletions of source phrases; and insertions of tar-
get words . To determine the word-level alignment be-
tween the sentencesand,we are primarily interested
in the first of these types of alignments. Given that the
source phrase has aligned to the target phrase, we
look up the hidden template variable that yielded this
alignment. contains the the word-to-word alignment
between these phrases.
</bodyText>
<subsectionHeader confidence="0.999215">
3.2 Translation and Translation Lattices
</subsectionHeader>
<bodyText confidence="0.973212214285714">
The lattice ofpossible translations of is obtained using
the weighted finite state composition:
(22)
The translation with the highest probability (Equa-
tion 20) can now be computed by obtaining the path with
the highest score in .
In terms of AT&amp;T FSM tools, this can be done as fol-
lows
fsmbestpath fsmproject
fsmrmepsilon
A translation lattice (Ueffing et al., 2002) can be gen-
erated by pruning based on likelihoods or number of
states. Similarly, an alignment lattice can be generated
by pruning .
</bodyText>
<sectionHeader confidence="0.982715" genericHeader="method">
4 Translation and Alignment Experiments
</sectionHeader>
<bodyText confidence="0.999815">
We now evaluate this implementation of the alignment
template translation model.
</bodyText>
<subsectionHeader confidence="0.999152">
4.1 Building the Alignment Template Library
</subsectionHeader>
<bodyText confidence="0.997794857142857">
To create the template library, we follow the procedure
reported in Och (2002). We first obtain word alignments
of bitext using IBM-4 translation models trained in each
translation direction (IBM-4 F and IBM-4 E), and then
forming the union of these alignments (IBM-4 ).
We extract the library of alignment templates from the
bitext alignment using the phrase-extract algorithm re-
ported in Och (2002). This procedure identifies several
alignment templates that are consis-
tent with a source phrase. We do not use word classes
in the experiments reported here; therefore templates are
specified by phrases rather than by class sequences. For
a given pair of source and target phrases, we retain only
The translation is the same way as
the matrix of alignments that occurs most frequently in
the training corpus. This is consistent with the intended
application of these templates for translation and align-
ment under the maximum likelihood criterion; in the cur-
rent formulation, only one alignment will survive in any
application of the models and there is no reason to retain
any of the less frequently occuring alignments. We esti-
mate the probability by the relative frequency of
phrasal translations found in bitext alignments. To restrict
the memory requirements of the model, we extract only
the templates which have at most words in the source
phrase. Furthermore, we restrict ourselves to the tem-
plates which have a probability for some
source phrase.
</bodyText>
<subsectionHeader confidence="0.945973">
4.2 Bitext Word Alignment
</subsectionHeader>
<bodyText confidence="0.998937952380952">
We present results on the French-to-English Hansards
translation task (Och and Ney, 2000). We measured
the alignment performance using precision, recall, and
Alignment Error Rate (AER) metrics (Och and Ney,
2000).
Our training set is a subset of the Canadian Hansards
which consists of French-English sentence
pairs (Och and Ney, 2000). The English side of the bitext
had a total of words ( unique tokens) and
the French side contained words (
tokens). Our template library consisted of
templates.
Our test set consists of 500 unseen French sentences
from Hansards for which both reference translations and
word alignments are available (Och and Ney, 2000). We
present the results under the ATTM in Table 1, where we
distinguish word alignments produced by the templates
from the template library against those produced by the
templates introduced for alignment in Section 3.1. For
comparison, we also align the bitext using IBM-4 trans-
lation models.
</bodyText>
<table confidence="0.999479714285714">
Model Alignment Metrics (%)
Precision Recall AER
IBM-4 F 88.9 89.8 10.8
IBM-4 E 89.2 89.4 10.7
IBM-4 84.3 93.8 12.3
ATTM-C 64.2 63.8 36.2
ATTM-A 94.5 55.8 27.3
</table>
<tableCaption confidence="0.94233">
Table 1: Alignment Performance on the French-to-
English Hansards Alignment Task.
</tableCaption>
<bodyText confidence="0.999825545454545">
We first observe that the complete set of word align-
ments generated by the ATTM (ATTM-C) is relatively
poor. However, when we consider only those word align-
ments generated by actual alignment templates (ATTM-
A) (and discard the alignments generated by the dummy
templates introduced as described in Section 3.1), we
obtain very high alignment precision. This implies that
word alignments within the templates are very accurate.
However, the poor performance under the recall measure
suggests that the alignment template library has relatively
poor coverage of the phrases in the alignment test set.
</bodyText>
<subsectionHeader confidence="0.996506">
4.3 Translation and Lattice Quality
</subsectionHeader>
<bodyText confidence="0.999948076923077">
We next measured the translation performance of ATTM
on the same test set. The translation performance was
measured using the BLEU (Papineni et al., 2001) and the
NIST MT-eval metrics (Doddington, 2002), and Word Er-
ror Rate (WER). The target language model was a trigram
language model with modified Kneser-Ney smoothing
trained on the English side of the bitext using the SRILM
tookit (Stolcke, 2002). The performance of the model is
reported in Table 2. For comparison, we also report per-
formance of the IBM-4 translation model trained on the
same corpus. The IBM Model-4 translations were ob-
tained using the ReWrite decoder (Marcu and Germann,
2002). The results in Table 2 show that the alignment
</bodyText>
<table confidence="0.993376">
Model BLEU NIST WER (%)
IBM-4 0.1711 5.0823 67.5
ATTM 0.1941 5.3337 64.7
</table>
<tableCaption confidence="0.9668055">
Table 2: Translation Performance on the French-to-
English Hansards Translation Task.
</tableCaption>
<bodyText confidence="0.999678571428571">
template model outperforms the IBM Model 4 under all
three metrics. This verifies that WFST implementation of
the ATTM can obtain a performance that compares favor-
ably to other well known research tools.
We generate N-best lists from each translation lattice,
and show the variation of their oracle-best BLEU scores
in Table 3. We observe that the oracle-best BLEU score
</bodyText>
<table confidence="0.998012">
Size ofN-best list
1 10 100 400 1000
BLEU 0.1941 0.2264 0.2550 0.2657 0.2735
</table>
<tableCaption confidence="0.7591675">
Table 3: Variation of oracle-Best BLEU scores on N-Best
lists generated by the ATTM.
</tableCaption>
<bodyText confidence="0.998724">
increases with the size of the N-Best List. We can there-
fore expect to rescore these lattices with more sophis-
ticated models and achieve improvements in translation
quality.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="discussions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999053191489362">
The main motivation for our investigation into this WFST
modeling framework for statistical machine translation
lies in the simplicity of the alignment and translation pro-
cesses relative to other dynamic programming or de-
coders (Och, 2002). Once the components of the align-
ment template translation model are implemented as WF-
STs, alignment and translation can be performed using
unique
standard FSM operations that have already been imple-
mented and optimized. It is not necessary to develop spe-
cialized search procedures, even for the generation of lat-
tices and N-best lists of alignment and translation alter-
natives.
The derivation of the ATTM was presented with the in-
tent of clearly identifying the conditional independence
assumptions that underly the WFST implementation.
This approach leads to modular implementations of the
component distributions of the translation model. These
components can be refined and improved by changing the
corresponding transducers without requiring changes to
the overall search procedure. However some of the mod-
eling assumptions are extremely strong. We note in par-
ticular that segmentation and translation are carried out
independently in that phrase segmentation is followed by
phrasal translation; performing these steps independently
can easily lead to search errors.
It is a strength of the ATTM that it can be directly
constructed from available bitext word alignments. How-
ever this construction should only be considered an ini-
tialization of the ATTM model parameters. Alignment
and translation can be expected to improve as the model
is refined and in future work we will investigate iterative
parameter estimation procedures.
We have presented a novel approach to generate align-
ments and alignment lattices under the ATTM. These lat-
tices will likely be very helpful in developing ATTM pa-
rameter estimation procedures, in that they can be used
to provide conditional distributions over the latent model
variables. We have observed that that poor coverage of
the test set by the template library may be why the over-
all word alignments produced by the ATTM are relatively
poor; we will therefore also explore new strategies for
template selection.
The alignment template model is a powerful model-
ing framework for statistical machine translation. It is
our goal to improve its performance through new training
procedures while refining the basic WFST architecture.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999583">
We would like to thank F. J. Och of ISI, USC for pro-
viding us the GIZA++ SMT toolkit, the mkcls toolkit to
train word classes, the Hansards 50K training and test
data, and the reference word alignments and AER met-
ric software. We thank AT&amp;T Labs - Research for use
of the FSM Toolkit and Andreas Stolcke for use of the
SRILM Toolkit. This work was supported by an ONR
MURI grant N00014-01-1-0685.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99949602">
S. Bangalore and G. Ricardi. 2001. A finite-state ap-
proach to machine translation. In Proc. of the North
American Chapter of the Association for Computa-
tional Linguistics, Pittsburgh, PA, USA.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ofHLT 2002, San Diego, CA. USA.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In Proc. of the AMTA Conference,
pages 421–437, Langhorne, PA, USA.
D. Marcu and U. Germann, 2002. The ISI ReWrite
Decoder Release 0.7.0b. http://www.isi.edu/licensed-
sw/rewrite-decoder/.
M. Mohri, F. Pereira, and M. Riley, 1997. ATT
General-purpose finite-state machine software tools.
http://www.research.att.com/sw/tools/fsm/.
M. Mohri, F. Pereira, and M. Riley. 2002. Weighted
finite-state transducers in speech recognition. Com-
puter Speech and Language, 16(1):69–88.
F. Och and H. Ney. 2000. Improved statistical alignment
models. In Proc. ofACL-2000, pages 440–447, Hong
Kong, China.
F. Och, C. Tillmann, and H. Ney. 1999. Improved align-
ment models for statistical machine translation. In
Proc. of the Joint Conf. of Empirical Methods in Nat-
ural Language Processing and Very Large Corpora,
pages 20–28, College Park, MD, USA.
F. Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen, Germany.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division.
A. Stolcke. 2002. SRILM – an extensible language mod-
eling toolkit. In Proc. of the International Conference
on Spoken Language Processing, pages 901–904, Den-
ver, CO, USA. http://www.speech.sri.com/projects/srilm/.
N. Ueffing, F. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of the Conference on Empirical Methods in Natural
Language Processing, pages 156–163, Philadelphia,
PA, USA.
F. Wessel, K. Macherey, and R. Schlueter. 1998. Using
word probabilities as confidence measures. In Proc. of
ICASSP-98, pages 225–228, Seattle, WA, USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.756319">
<note confidence="0.940549666666667">Proceedings of HLT-NAACL 2003 Main Papers , pp. 63-70 Edmonton, May-June 2003</note>
<title confidence="0.9921125">A Weighted Finite State Transducer Implementation of the Alignment Template Model for Statistical Machine Translation</title>
<author confidence="0.979019">Shankar Kumar</author>
<author confidence="0.979019">William</author>
<affiliation confidence="0.948694">Center for Language and Speech Processing, Johns Hopkins</affiliation>
<address confidence="0.999742">3400 North Charles Street, Baltimore, MD, 21218,</address>
<email confidence="0.998691">skumar,byrne@jhu.edu</email>
<abstract confidence="0.999688947368421">We present a derivation of the alignment template model for statistical machine translation and an implementation of the model using weighted finite state transducers. The approach we describe allows us to implement each constituent distribution of the model as a weighted finite state transducer or acceptor. We show that bitext word alignment and translation under the model can be performed with standard FSM operations involving these transducers. One of the benefits of using this framework is that it obviates the need to develop specialized search procedures, even for the generation of lattices or N-Best lists of bitext word alignments and translation hypotheses. We evaluate the implementation of the model on the Frenchto-English Hansards task and report alignment and translation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Ricardi</author>
</authors>
<title>A finite-state approach to machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="2830" citStr="Bangalore and Ricardi, 2001" startWordPosition="420" endWordPosition="423">imized algorithms available from an off-the-shelf FSM toolkit (Mohri et al., 1997). This avoids the need to develop specialized search procedures, even for the genSOURCE LANGUAGE SENTENCE TARGET LANGUAGE SENTENCE Figure 1: ATTM Architecture. eration of lattices or N-best lists of bitext word alignment or translation hypotheses. Weighted Finite State Transducers for Statistical Machine Translation (SMT) have been proposed in the literature to implement word-to-word translation models (Knight and Al-Onaizan, 1998) or to perform translation in an application domain such as the call routing task (Bangalore and Ricardi, 2001). One of the objectives of these approaches has been to provide an implementation for SMT that uses standard FSM algorithms to perform model computations and therefore make SMT techniques accessible to a wider community. Our WFST implementation of the ATTM has been developed with similar objectives. We start off by presenting a derivation of the ATTM that identifies the conditional independence assumptions that underly the model. The derivation allows us to specify each component distribution of the model and implement it as a weighted finite state transducer. We then show that bitext word ali</context>
</contexts>
<marker>Bangalore, Ricardi, 2001</marker>
<rawString>S. Bangalore and G. Ricardi. 2001. A finite-state approach to machine translation. In Proc. of the North American Chapter of the Association for Computational Linguistics, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1453" citStr="Brown et al., 1993" startWordPosition="212" endWordPosition="215">ne of the benefits of using this framework is that it obviates the need to develop specialized search procedures, even for the generation of lattices or N-Best lists of bitext word alignments and translation hypotheses. We evaluate the implementation of the model on the Frenchto-English Hansards task and report alignment and translation performance. 1 Introduction The Alignment Template Translation Model (ATTM) (Och et al., 1999) has emerged as a promising modeling framework for statistical machine translation. The ATTM attempts to overcome the deficiencies of word-to-word translation models (Brown et al., 1993) through the use of phrasal translations. The overall model is based on a two-level alignment between the source and the target sentence: a phrase-level alignment between source and target phrases and a word-level alignment between words in these phrase pairs. The goal of this paper is to reformulate the ATTM so that the operations we intend to perform under a statistical translation model, namely bitext word alignment and translation, can be implementation using standard weighted finite state transducer (WFST) operations. Our main motivation for a WFST modeling framework lies in the resulting</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ofHLT 2002,</booktitle>
<publisher>USA.</publisher>
<location>San Diego, CA.</location>
<contexts>
<context position="24015" citStr="Doddington, 2002" startWordPosition="3972" endWordPosition="3973">ard the alignments generated by the dummy templates introduced as described in Section 3.1), we obtain very high alignment precision. This implies that word alignments within the templates are very accurate. However, the poor performance under the recall measure suggests that the alignment template library has relatively poor coverage of the phrases in the alignment test set. 4.3 Translation and Lattice Quality We next measured the translation performance of ATTM on the same test set. The translation performance was measured using the BLEU (Papineni et al., 2001) and the NIST MT-eval metrics (Doddington, 2002), and Word Error Rate (WER). The target language model was a trigram language model with modified Kneser-Ney smoothing trained on the English side of the bitext using the SRILM tookit (Stolcke, 2002). The performance of the model is reported in Table 2. For comparison, we also report performance of the IBM-4 translation model trained on the same corpus. The IBM Model-4 translations were obtained using the ReWrite decoder (Marcu and Germann, 2002). The results in Table 2 show that the alignment Model BLEU NIST WER (%) IBM-4 0.1711 5.0823 67.5 ATTM 0.1941 5.3337 64.7 Table 2: Translation Perform</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ofHLT 2002, San Diego, CA. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>Y Al-Onaizan</author>
</authors>
<title>Translation with finite-state devices.</title>
<date>1998</date>
<booktitle>In Proc. of the AMTA Conference,</booktitle>
<pages>421--437</pages>
<location>Langhorne, PA, USA.</location>
<contexts>
<context position="2719" citStr="Knight and Al-Onaizan, 1998" startWordPosition="401" endWordPosition="404">on processes compared to dynamic programming or decoders. The WFST implementation allows us to use standard optimized algorithms available from an off-the-shelf FSM toolkit (Mohri et al., 1997). This avoids the need to develop specialized search procedures, even for the genSOURCE LANGUAGE SENTENCE TARGET LANGUAGE SENTENCE Figure 1: ATTM Architecture. eration of lattices or N-best lists of bitext word alignment or translation hypotheses. Weighted Finite State Transducers for Statistical Machine Translation (SMT) have been proposed in the literature to implement word-to-word translation models (Knight and Al-Onaizan, 1998) or to perform translation in an application domain such as the call routing task (Bangalore and Ricardi, 2001). One of the objectives of these approaches has been to provide an implementation for SMT that uses standard FSM algorithms to perform model computations and therefore make SMT techniques accessible to a wider community. Our WFST implementation of the ATTM has been developed with similar objectives. We start off by presenting a derivation of the ATTM that identifies the conditional independence assumptions that underly the model. The derivation allows us to specify each component dist</context>
</contexts>
<marker>Knight, Al-Onaizan, 1998</marker>
<rawString>K. Knight and Y. Al-Onaizan. 1998. Translation with finite-state devices. In Proc. of the AMTA Conference, pages 421–437, Langhorne, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>U Germann</author>
</authors>
<date>2002</date>
<booktitle>The ISI ReWrite Decoder Release 0.7.0b. http://www.isi.edu/licensedsw/rewrite-decoder/.</booktitle>
<contexts>
<context position="24465" citStr="Marcu and Germann, 2002" startWordPosition="4045" endWordPosition="4048">tion performance of ATTM on the same test set. The translation performance was measured using the BLEU (Papineni et al., 2001) and the NIST MT-eval metrics (Doddington, 2002), and Word Error Rate (WER). The target language model was a trigram language model with modified Kneser-Ney smoothing trained on the English side of the bitext using the SRILM tookit (Stolcke, 2002). The performance of the model is reported in Table 2. For comparison, we also report performance of the IBM-4 translation model trained on the same corpus. The IBM Model-4 translations were obtained using the ReWrite decoder (Marcu and Germann, 2002). The results in Table 2 show that the alignment Model BLEU NIST WER (%) IBM-4 0.1711 5.0823 67.5 ATTM 0.1941 5.3337 64.7 Table 2: Translation Performance on the French-toEnglish Hansards Translation Task. template model outperforms the IBM Model 4 under all three metrics. This verifies that WFST implementation of the ATTM can obtain a performance that compares favorably to other well known research tools. We generate N-best lists from each translation lattice, and show the variation of their oracle-best BLEU scores in Table 3. We observe that the oracle-best BLEU score Size ofN-best list 1 10</context>
</contexts>
<marker>Marcu, Germann, 2002</marker>
<rawString>D. Marcu and U. Germann, 2002. The ISI ReWrite Decoder Release 0.7.0b. http://www.isi.edu/licensedsw/rewrite-decoder/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F Pereira</author>
<author>M Riley</author>
</authors>
<date>1997</date>
<note>ATT General-purpose finite-state machine software tools. http://www.research.att.com/sw/tools/fsm/.</note>
<contexts>
<context position="2284" citStr="Mohri et al., 1997" startWordPosition="339" endWordPosition="342">l alignment between words in these phrase pairs. The goal of this paper is to reformulate the ATTM so that the operations we intend to perform under a statistical translation model, namely bitext word alignment and translation, can be implementation using standard weighted finite state transducer (WFST) operations. Our main motivation for a WFST modeling framework lies in the resulting simplicity of alignment and translation processes compared to dynamic programming or decoders. The WFST implementation allows us to use standard optimized algorithms available from an off-the-shelf FSM toolkit (Mohri et al., 1997). This avoids the need to develop specialized search procedures, even for the genSOURCE LANGUAGE SENTENCE TARGET LANGUAGE SENTENCE Figure 1: ATTM Architecture. eration of lattices or N-best lists of bitext word alignment or translation hypotheses. Weighted Finite State Transducers for Statistical Machine Translation (SMT) have been proposed in the literature to implement word-to-word translation models (Knight and Al-Onaizan, 1998) or to perform translation in an application domain such as the call routing task (Bangalore and Ricardi, 2001). One of the objectives of these approaches has been t</context>
<context position="4293" citStr="Mohri et al., 1997" startWordPosition="671" endWordPosition="674">SEGMENTATION MODEL TARGET LANGUAGE MODEL PERMUTATION MODEL TEMPLATE SEQUENCE MODEL PHRASAL TRANSLATION MODEL PHRASE e1 e2 f f f f f f f7 1 2 3 4 5 6 v a a a v2 1 v1 2 v3 3 z 1 u u u 1 2 1 e3 e4 e5 e6 e e e 7 8 9 v2 v3 z2 z3 3 source language phrases alignment templates target language phrases 2 Alignment Template Translation Models We present here a derivation of the alignment template translation model (ATTM) (Och et al., 1999; Och, 2002) and give an implementation of the model using weighted finite state transducers (WFSTs). The finite state modeling is performed using the AT&amp;T FSM Toolkit (Mohri et al., 1997). In this model, the translation of a source language sentence to a target language sentence is described by a joint probability distribution over all possible segmentations and alignments. This distribution is presented in Figure 1 and Equations 1-7. The components of the overall translation model are the source language model (Term 2), the source segmentation model (Term 3), the phrase permutation model (Term 4), the template sequence model (Term 5), the phrasal translation model (Term 6) and the target language model (Term 7). Each of these conditional distributions is modeled independently</context>
</contexts>
<marker>Mohri, Pereira, Riley, 1997</marker>
<rawString>M. Mohri, F. Pereira, and M. Riley, 1997. ATT General-purpose finite-state machine software tools. http://www.research.att.com/sw/tools/fsm/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F Pereira</author>
<author>M Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="16601" citStr="Mohri et al., 2002" startWordPosition="2765" endWordPosition="2768">se une inflation galopante to the target phrase run away inflation via the word alignment matrix specified as . is built out of the three component acceptors , , and . The acceptor corresponds to the mapping from the template and the source phrase to all consistent target phrases . Target Language Model We specify this model as where enforces the requirement that words in the translation agree with those in the phrase sequence. We note that is modeled as a standard backoff trigram language model (Stolcke, 2002). Such a language model can be easily compiled as a weighted finite state acceptor (Mohri et al., 2002). 3 Alignment and Translation Via WFSTs We will now describe how the alignment template translation model can be used to perform word-level alignment of bitexts and translation of source language sentences. Given a source language sentence and a target sentence, the word-to-word alignment between the sentences can be found as The variables specify the alignment between source phrases and target phrases while gives the word-to-word alignment within the phrase sequences. Given a source language sentence, the translation can be found as . maps where is the translation of. We implement the alignme</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>M. Mohri, F. Pereira, and M. Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. ofACL-2000,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="13705" citStr="Och and Ney, 2000" startWordPosition="2244" endWordPosition="2247">The first acceptor implements the alignment matrix . It has states and between any pair of states and, each arc corresponds to a word alignment variable . Therefore the number of transitions between statesand is equal to the number of non-zero values of . The arc from state tohas probability (Equation 17). The second machine is an unweighted transducer that maps the index in the phrase to the corresponding word . The third transducer is the lexicon transducer that maps the source word to the target word with probability nous/0.33 nous/0.45 avons/0.53 (14) The term is a translation dictionary (Och and Ney, 2000) and is obtained as . The fourth acceptor is unweighted and allows all target word sequences which can be specified by the 3 2 1 F 0 Z2 Z1 Z3 ε : have ε : a /0.42 /0.07 z2: ε : run ε : away ε /0.5 /0.01 z3: ε ε: ε z1 : ε ε :inflation /0.44 ε :we ε: ε /0.72 ε: ε Figure 6: Transducer that maps the source template sequence into target phrase sequences. 3 /0.5 z C Z 3/1.0 0/1.0 i=1 i=2 i=3 inflation: inflation / 0.85 galopante : inflation / 0.04 galopante : run / 0.50 NULL : away / 0.01 0 : NULL 1 : une 2 : inflation 3 : galopante 2/0.5 D O r /0.5 /0.01 /0.44 run away inflation run away inflation </context>
<context position="22077" citStr="Och and Ney, 2000" startWordPosition="3662" endWordPosition="3665">e current formulation, only one alignment will survive in any application of the models and there is no reason to retain any of the less frequently occuring alignments. We estimate the probability by the relative frequency of phrasal translations found in bitext alignments. To restrict the memory requirements of the model, we extract only the templates which have at most words in the source phrase. Furthermore, we restrict ourselves to the templates which have a probability for some source phrase. 4.2 Bitext Word Alignment We present results on the French-to-English Hansards translation task (Och and Ney, 2000). We measured the alignment performance using precision, recall, and Alignment Error Rate (AER) metrics (Och and Ney, 2000). Our training set is a subset of the Canadian Hansards which consists of French-English sentence pairs (Och and Ney, 2000). The English side of the bitext had a total of words ( unique tokens) and the French side contained words ( tokens). Our template library consisted of templates. Our test set consists of 500 unseen French sentences from Hansards for which both reference translations and word alignments are available (Och and Ney, 2000). We present the results under th</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. Och and H. Ney. 2000. Improved statistical alignment models. In Proc. ofACL-2000, pages 440–447, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<location>College Park, MD, USA.</location>
<contexts>
<context position="1267" citStr="Och et al., 1999" startWordPosition="186" endWordPosition="189">ted finite state transducer or acceptor. We show that bitext word alignment and translation under the model can be performed with standard FSM operations involving these transducers. One of the benefits of using this framework is that it obviates the need to develop specialized search procedures, even for the generation of lattices or N-Best lists of bitext word alignments and translation hypotheses. We evaluate the implementation of the model on the Frenchto-English Hansards task and report alignment and translation performance. 1 Introduction The Alignment Template Translation Model (ATTM) (Och et al., 1999) has emerged as a promising modeling framework for statistical machine translation. The ATTM attempts to overcome the deficiencies of word-to-word translation models (Brown et al., 1993) through the use of phrasal translations. The overall model is based on a two-level alignment between the source and the target sentence: a phrase-level alignment between source and target phrases and a word-level alignment between words in these phrase pairs. The goal of this paper is to reformulate the ATTM so that the operations we intend to perform under a statistical translation model, namely bitext word a</context>
<context position="4105" citStr="Och et al., 1999" startWordPosition="640" endWordPosition="643">perations involving these transducers. Finally we report bitext word alignment and translation performance of the implementation on the Canadian French-to-English Hansards task. SOURCE SEGMENTATION MODEL TARGET LANGUAGE MODEL PERMUTATION MODEL TEMPLATE SEQUENCE MODEL PHRASAL TRANSLATION MODEL PHRASE e1 e2 f f f f f f f7 1 2 3 4 5 6 v a a a v2 1 v1 2 v3 3 z 1 u u u 1 2 1 e3 e4 e5 e6 e e e 7 8 9 v2 v3 z2 z3 3 source language phrases alignment templates target language phrases 2 Alignment Template Translation Models We present here a derivation of the alignment template translation model (ATTM) (Och et al., 1999; Och, 2002) and give an implementation of the model using weighted finite state transducers (WFSTs). The finite state modeling is performed using the AT&amp;T FSM Toolkit (Mohri et al., 1997). In this model, the translation of a source language sentence to a target language sentence is described by a joint probability distribution over all possible segmentations and alignments. This distribution is presented in Figure 1 and Equations 1-7. The components of the overall translation model are the source language model (Term 2), the source segmentation model (Term 3), the phrase permutation model (Te</context>
<context position="8939" citStr="Och et al., 1999" startWordPosition="1448" endWordPosition="1451">del We now define a model for the reordering of phrase sequences as determined by the previous model. The phrase alignment sequence specifies a reordering of phrases into target language phrase order; the words within the phrases remain in the source language order. The phrase sequence is reordered into . The phrase alignment sequence is modeled as a first order Markov process with . The alignment sequence distribution is constructed to assign decreasing likelihood to phrase re-orderings that diverge from the original word order. Suppose and , we set the Markov chain probabilities as follows (Och et al., 1999) In the above equations, is a tuning factor and we normalize the probabilities so that . The finite state implementation of this model involves two acceptors. We first build a unweighted permutation acceptor that contains all permutations of the phrase sequence in the source language (Knight and AlOnaizan, 1998) . We note that a path through corresponds to an alignment sequence . Figure 3 shows the acceptor for the source phrase sequence nous avons une inflation galopante. A source phrase sequence of length words requires a permutation acceptor of states. For long phrase sequences we compute a</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Statistical Machine Translation: From Single Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen,</institution>
<contexts>
<context position="4117" citStr="Och, 2002" startWordPosition="644" endWordPosition="645">g these transducers. Finally we report bitext word alignment and translation performance of the implementation on the Canadian French-to-English Hansards task. SOURCE SEGMENTATION MODEL TARGET LANGUAGE MODEL PERMUTATION MODEL TEMPLATE SEQUENCE MODEL PHRASAL TRANSLATION MODEL PHRASE e1 e2 f f f f f f f7 1 2 3 4 5 6 v a a a v2 1 v1 2 v3 3 z 1 u u u 1 2 1 e3 e4 e5 e6 e e e 7 8 9 v2 v3 z2 z3 3 source language phrases alignment templates target language phrases 2 Alignment Template Translation Models We present here a derivation of the alignment template translation model (ATTM) (Och et al., 1999; Och, 2002) and give an implementation of the model using weighted finite state transducers (WFSTs). The finite state modeling is performed using the AT&amp;T FSM Toolkit (Mohri et al., 1997). In this model, the translation of a source language sentence to a target language sentence is described by a joint probability distribution over all possible segmentations and alignments. This distribution is presented in Figure 1 and Equations 1-7. The components of the overall translation model are the source language model (Term 2), the source segmentation model (Term 3), the phrase permutation model (Term 4), the t</context>
<context position="20584" citStr="Och (2002)" startWordPosition="3425" endWordPosition="3426">t probability (Equation 20) can now be computed by obtaining the path with the highest score in . In terms of AT&amp;T FSM tools, this can be done as follows fsmbestpath fsmproject fsmrmepsilon A translation lattice (Ueffing et al., 2002) can be generated by pruning based on likelihoods or number of states. Similarly, an alignment lattice can be generated by pruning . 4 Translation and Alignment Experiments We now evaluate this implementation of the alignment template translation model. 4.1 Building the Alignment Template Library To create the template library, we follow the procedure reported in Och (2002). We first obtain word alignments of bitext using IBM-4 translation models trained in each translation direction (IBM-4 F and IBM-4 E), and then forming the union of these alignments (IBM-4 ). We extract the library of alignment templates from the bitext alignment using the phrase-extract algorithm reported in Och (2002). This procedure identifies several alignment templates that are consistent with a source phrase. We do not use word classes in the experiments reported here; therefore templates are specified by phrases rather than by class sequences. For a given pair of source and target phra</context>
<context position="25630" citStr="Och, 2002" startWordPosition="4234" endWordPosition="4235">racle-best BLEU score Size ofN-best list 1 10 100 400 1000 BLEU 0.1941 0.2264 0.2550 0.2657 0.2735 Table 3: Variation of oracle-Best BLEU scores on N-Best lists generated by the ATTM. increases with the size of the N-Best List. We can therefore expect to rescore these lattices with more sophisticated models and achieve improvements in translation quality. 5 Discussion The main motivation for our investigation into this WFST modeling framework for statistical machine translation lies in the simplicity of the alignment and translation processes relative to other dynamic programming or decoders (Och, 2002). Once the components of the alignment template translation model are implemented as WFSTs, alignment and translation can be performed using unique standard FSM operations that have already been implemented and optimized. It is not necessary to develop specialized search procedures, even for the generation of lattices and N-best lists of alignment and translation alternatives. The derivation of the ATTM was presented with the intent of clearly identifying the conditional independence assumptions that underly the WFST implementation. This approach leads to modular implementations of the compone</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>F. Och. 2002. Statistical Machine Translation: From Single Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022),</tech>
<institution>IBM Research Division.</institution>
<contexts>
<context position="23967" citStr="Papineni et al., 2001" startWordPosition="3963" endWordPosition="3966">rated by actual alignment templates (ATTMA) (and discard the alignments generated by the dummy templates introduced as described in Section 3.1), we obtain very high alignment precision. This implies that word alignments within the templates are very accurate. However, the poor performance under the recall measure suggests that the alignment template library has relatively poor coverage of the phrases in the alignment test set. 4.3 Translation and Lattice Quality We next measured the translation performance of ATTM on the same test set. The translation performance was measured using the BLEU (Papineni et al., 2001) and the NIST MT-eval metrics (Doddington, 2002), and Word Error Rate (WER). The target language model was a trigram language model with modified Kneser-Ney smoothing trained on the English side of the bitext using the SRILM tookit (Stolcke, 2002). The performance of the model is reported in Table 2. For comparison, we also report performance of the IBM-4 translation model trained on the same corpus. The IBM Model-4 translations were obtained using the ReWrite decoder (Marcu and Germann, 2002). The results in Table 2 show that the alignment Model BLEU NIST WER (%) IBM-4 0.1711 5.0823 67.5 ATTM</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO, USA. http://www.speech.sri.com/projects/srilm/.</location>
<contexts>
<context position="16498" citStr="Stolcke, 2002" startWordPosition="2749" endWordPosition="2750"> word avons to the target phrase have a via the word alignment matrix specified as the source phrase une inflation galopante to the target phrase run away inflation via the word alignment matrix specified as . is built out of the three component acceptors , , and . The acceptor corresponds to the mapping from the template and the source phrase to all consistent target phrases . Target Language Model We specify this model as where enforces the requirement that words in the translation agree with those in the phrase sequence. We note that is modeled as a standard backoff trigram language model (Stolcke, 2002). Such a language model can be easily compiled as a weighted finite state acceptor (Mohri et al., 2002). 3 Alignment and Translation Via WFSTs We will now describe how the alignment template translation model can be used to perform word-level alignment of bitexts and translation of source language sentences. Given a source language sentence and a target sentence, the word-to-word alignment between the sentences can be found as The variables specify the alignment between source phrases and target phrases while gives the word-to-word alignment within the phrase sequences. Given a source language</context>
<context position="24214" citStr="Stolcke, 2002" startWordPosition="4005" endWordPosition="4006">rate. However, the poor performance under the recall measure suggests that the alignment template library has relatively poor coverage of the phrases in the alignment test set. 4.3 Translation and Lattice Quality We next measured the translation performance of ATTM on the same test set. The translation performance was measured using the BLEU (Papineni et al., 2001) and the NIST MT-eval metrics (Doddington, 2002), and Word Error Rate (WER). The target language model was a trigram language model with modified Kneser-Ney smoothing trained on the English side of the bitext using the SRILM tookit (Stolcke, 2002). The performance of the model is reported in Table 2. For comparison, we also report performance of the IBM-4 translation model trained on the same corpus. The IBM Model-4 translations were obtained using the ReWrite decoder (Marcu and Germann, 2002). The results in Table 2 show that the alignment Model BLEU NIST WER (%) IBM-4 0.1711 5.0823 67.5 ATTM 0.1941 5.3337 64.7 Table 2: Translation Performance on the French-toEnglish Hansards Translation Task. template model outperforms the IBM Model 4 under all three metrics. This verifies that WFST implementation of the ATTM can obtain a performance</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. of the International Conference on Spoken Language Processing, pages 901–904, Denver, CO, USA. http://www.speech.sri.com/projects/srilm/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>156--163</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="20208" citStr="Ueffing et al., 2002" startWordPosition="3365" endWordPosition="3368">pes of alignments. Given that the source phrase has aligned to the target phrase, we look up the hidden template variable that yielded this alignment. contains the the word-to-word alignment between these phrases. 3.2 Translation and Translation Lattices The lattice ofpossible translations of is obtained using the weighted finite state composition: (22) The translation with the highest probability (Equation 20) can now be computed by obtaining the path with the highest score in . In terms of AT&amp;T FSM tools, this can be done as follows fsmbestpath fsmproject fsmrmepsilon A translation lattice (Ueffing et al., 2002) can be generated by pruning based on likelihoods or number of states. Similarly, an alignment lattice can be generated by pruning . 4 Translation and Alignment Experiments We now evaluate this implementation of the alignment template translation model. 4.1 Building the Alignment Template Library To create the template library, we follow the procedure reported in Och (2002). We first obtain word alignments of bitext using IBM-4 translation models trained in each translation direction (IBM-4 F and IBM-4 E), and then forming the union of these alignments (IBM-4 ). We extract the library of align</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>N. Ueffing, F. Och, and H. Ney. 2002. Generation of word graphs in statistical machine translation. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 156–163, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wessel</author>
<author>K Macherey</author>
<author>R Schlueter</author>
</authors>
<title>Using word probabilities as confidence measures.</title>
<date>1998</date>
<booktitle>In Proc. of ICASSP-98,</booktitle>
<pages>225--228</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="7445" citStr="Wessel et al., 1998" startWordPosition="1200" endWordPosition="1203">f (implemented as an acceptor ) by composing it with the transducer , i.e. . 2. We then decompose into disjoint subsets so that contains all segmentations of the source language sentence with exactly phrases. To construct , we create an unweighted acceptor that accepts any phrase sequence of length ; for efficiency, the phrase vocabulary is restricted to the phrases in . is then obtained by the finite state composition . 3. For The normalization factors are obtained by summing the probabilities of all segmentations in . This sum can be computed efficiently using lattice forward probabilities (Wessel et al., 1998). For a fixed , the most likely segmentation in is found as (10) 4. Finally we select the optimal segmentation as (11) We begin by distinguishing words and phrases. We assume that is a phrase in the target language sentence that has length and consists of words (8) A portion of the segmentation transducer for the French sentence nous avons une inflation galopante is presented in Figure 2. When composed with , generates the following two phrase segmentations: nous avons une inflation galopante and nous avons une inflation galopante. The “ ” symbol is used to indicate phrases formed by concatena</context>
</contexts>
<marker>Wessel, Macherey, Schlueter, 1998</marker>
<rawString>F. Wessel, K. Macherey, and R. Schlueter. 1998. Using word probabilities as confidence measures. In Proc. of ICASSP-98, pages 225–228, Seattle, WA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>