<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<note confidence="0.6031665">
EXPLORER: A Natural Language Processing
System for Oil Exploration
</note>
<author confidence="0.617894">
Wendy G. Lehnert
</author>
<affiliation confidence="0.667255333333333">
Department of Computer and Information Sciences
Graduate Research Center
University of Massachusetts
</affiliation>
<reference confidence="0.4095754">
Amherst, Mass. 01003
Steven P. Shwartz
Cognitive Systems Inc.
234 Church Street
New Haven, Ct. 06510
</reference>
<bodyText confidence="0.9830054">
EXPLORER (Lehnert and Shvartz, 1982;
Shwartz 1982) is a non-fragile, &amp;quot;hands-on&amp;quot;
language analysis system that allows oil
explorationists with no knowledge of computers
or computer programming to create customized
maps. Users in Tulsa, Denver, and New Orleans
currently have dial-up access to a DEC-20
where EXPLORER is implemented in TLISP. A
user converses with EXPLORER about a desired
map until both parties have agreed on an
adequate and unambiguous set of
specifications. Another phone link then
carries EXPLORER&apos;s output to an IBM 3033 which
runs database retrieval routines on commercial
well data. When all the information has been
secured from well data, a graphics system
takes over to perform the actual map
generation. EXPLORER is currently undergoing
evaluation, and it is targetted for a 1983
installation in all regional offices of a
major oil company* for restricted use by
geologists and geophysicists.
Since our intended user population is
naive about computers, EXPLORER&apos;s interactive
design is dominated by &amp;quot;user-friendly&amp;quot;
features. EXPLORER processes information
retrieval requests stated in English, without
imposing vocabulary limitations or syntactic
restrictions on the user. Using a 7000-word
dictionary, EXPLORER makes inferences about
what a user is saying and initiates
interactive dialogues when map specifications
are not adequate or potentially ambiguous.
While &amp;quot;user-friendly&amp;quot; system designs
often accomodate novice users at the expense
of efficient interactions with expert users,
EXPLORER naturally tunes itself to both novice
and expert users. An expert can state a
request very concisely, without a long
interactive session, while a novice is guided
by a question-and-answer dialogue. Since all
users are expected to enter mispellings and
typos, an intelligent spelling corrector
enables the user to correct typographical
errors quickly and easily.
*A proprietary agreement forbids us from
identifying them.
Once a request has been analyzed by EXPLORER,
the user is given an opportunity to verify
EXPLORER&apos;s understanding of that request or
make changes as needed.
The following example will illustrate the
complexity of the map requests that EXPLORER
is designed to handle. Following the input,
EXPLORER displays its interpretation of the
request for verification by the user.
EXPLORER Version 02 9/23/81
READY
*Show ma a map of all tight wells drilled
*before May 1, 1980 but since May 1, 1970
*by texaco that show oil deeper than 2000,
*are themselves deeper than 5000, are now
*operated by shell, are wildcat wells where
*the operator reported a drilling problem,
*and have mechanical logs, drill stem tests,
*and a commercial oil analysis, that were
*drilled within the area defined by latitude
*30 deg 20 min 30 sec to 31:20:30 and
*80-81. scale 2000 fest.
By FEST, do you mean FEET (Y or N) *y
</bodyText>
<subsectionHeader confidence="0.583229">
User requests a POSTED map
</subsectionHeader>
<bodyText confidence="0.517011">
latitude 30.34167 - 31.34167
</bodyText>
<equation confidence="0.7844706">
longitude 80 - 81
output medium: PAPER
output device: PHOTODOT
filters: DRILLING DEPTH &gt; 5000 FEET
COMPLETION DATE 5/1/1970
</equation>
<sectionHeader confidence="0.9787488" genericHeader="method">
COMPLETION DATE &lt; 5/1/1980
OIL ANALYSIS AVAILABLE
DRILL STEM TESTS PERFORMED
MECHANICAL LOG FILE WELL
DRILLING PROBLEM
WILDCAT WELL
SHELL CURRENT OPERATOR
WELL SHOW OF OIL &gt; 2000&apos;
TEXACO ORIGINAL OPERATOR
TIGHT HOLE WELL
</sectionHeader>
<bodyText confidence="0.538486">
scale of 1&amp;quot; . 2000.0&apos;
</bodyText>
<page confidence="0.997422">
69
</page>
<bodyText confidence="0.956266625">
As this example shows, EXPLORER can
handle long and ungrammatical requests, make
inferences, and perform complicated word sense
disambiguation. For example, the word &amp;quot;show&amp;quot;
has two distinct meanings in the above
request. The first instance of &amp;quot;show&amp;quot; is
synonymous with &amp;quot;display&amp;quot;, whereas the second
time &amp;quot;show&amp;quot; is used it has a technical meaning
(i.e., an indicator of oil or gas in a well).
The surrounding conceptual context is used to
determine which sense of &amp;quot;show&amp;quot; is appropriate
in each case.
We suspect expert users who utilize
EXPLORER regularly will develop a feel for
EXPLORER&apos;s limitations and flexibilities. If
such a user wants to enter a request as
efficiently as possible, EXPLORER will handle
&amp;quot;telegraphic&amp;quot; English which does not have to
conform to standard syntactic conventions:
EXPLORER Version 02 9/23/81
READY
*tcg bibb georgia ci 100 scale 2000.
User requests a STRUCTURE map
output- medium: PAPER
output device: PHOTODOT
county: BIBB, GEORGIA
on TCC
CI • 100&apos;
scale of 1&amp;quot; • 2000.0&apos;
As long as the request is conceptually
coherent and unambiguous, EXPLORER will be
able to handle a wide range of stylistic
constructs. This last request was
sufficiently specified for EXPLORER to
interpret it without further user interaction,
but many requests (especially those generated
by novice users) are likely to require an
interactive dialog. For example, the
following interaction might take place with an
extremely novice user:
</bodyText>
<sectionHeader confidence="0.5948085" genericHeader="method">
EXPLORER Version 02 9/23/81
READY
</sectionHeader>
<subsectionHeader confidence="0.580193">
*Map the tcg.
</subsectionHeader>
<bodyText confidence="0.871738666666667">
A map region was not specified.
Do you want the same geographic region as the
last request (Y or N)?
*n
Do you wish to specify the map region by
county or by geographic coordinates?
*c
(Al) original input is interpreted correctly
on the first try - perfect performance.
</bodyText>
<figure confidence="0.84866864">
Please input county or counties by name or
number.
*Bibb
Bibb County is in the following states:
ALABAMA GEORGIA
Please input the appropriate state or a new
county specification.
*make it new haven, Ct.
A contour interval was not specified.
Would you like the CI from the previous
request? (Y or N)
Previous CI ■ 100&apos;
*y
A scale was not specified.
Would you like the scale from the previous
request? (Y or N)
Previous scale 2000.0 FT-PER-IN
*y
User requests a STRUCTURE map
output medium: PAPER
output device: photodot
county: NEW HAVEN, CONNECTICUT
on TCG
CI • 100&apos;
scale of 1&amp;quot; = 2000.0&apos;
</figure>
<bodyText confidence="0.99647472">
EXPLORER will query a user as needed to
get missing information and resolve any
ambiguities that may be present. Notice that
EXPLORER naturally offers the user an option
of inheriting many specifications from the
previous map request. Explorationists often
find it useful to examine a sequence of
related maps, so our interface has been
designed to make map sequences easy to
generate.
EXPLORER has been undergoing an initial
test phase since July 13, 1982. During this
time a variety of oil company employees have
dialed up the program and entered map
requests. While we do not yet have enough
test requests for a comprehensive evaluation
of the system, we have analyzed EXPLORER&apos;s
performance over the three-week period from
7/13/82 to 8/6/82 in an effort to assess its
strengths and weaknesses. During this time 39
requests were successfully transmitted to
EXPLORER by 8 different individuals. In order
to evaluate EXPLORER&apos;s performance, we will
consider the following categories of
performance:
</bodyText>
<page confidence="0.998201">
70
</page>
<tableCaption confidence="0.634408">
TABLE - 1
</tableCaption>
<table confidence="0.9977395">
REQUEST TYPE NO. OF REQUESTS SURFACE INTERACTIVE CONCEPTUAL
Al 4 (10%) 19(15-25) 3(3-3) 9(9-10)
A2 26 (67%) 22(1-87) 7(3-14) 11(9-22)
A3 9 (23%) 37(9-57) 8(5-12) 12(10-14)
------ --------- --------
total 39 (100%) 25(1-87) 7(3-14) 11(9-22)
</table>
<bodyText confidence="0.988230740740741">
(A2) original input is interpreted correctly
after one or more clarifying inter-
actions. These interactions may be due
to typing errors, spelling errors,
missing information, or system errors.
(A3) original input is never interpreted
correctly due to a system failure of
some sort.
If a request can be categorized as an Al or A2
request, EXPLORER is fully functional even
though it may make a mistake at some point in
its processing. For example, if EXPLORER does
not recognize a word, it will query the user
for synonyms. If one of the synonyms is
recognized, EXLPORER recovers from its own
recognition error, and the request will be
categorized as an A2 request. When a system
error is fatal in the sense that the user does
not or cannot recover from it, we categorize
the request as an A3 request: an A3 request
should not result in map generation. We have
omitted from this analysis any requests that
were aborted due to transmission errors or
user-initiated interrupts.
In addition to our three performance
categories, we will characterize the general
complexity of a request in three ways:
</bodyText>
<note confidence="0.248922">
(11 Surface Complexity:
</note>
<tableCaption confidence="0.644812666666667">
The number of words in the original
input request.
(2] Interactive Complexity:
The number of complete interactions
between the user and EXPLORER during
a single request dialog.
[3] Conceptual Complexity:
The number of lines generated in the
target query language.
</tableCaption>
<bodyText confidence="0.999443101449275">
We realize that some users will try to
maximize efficient communication by minimizing
the number of complete interactions. At the
same time, still other users will find it
easier to enter a minimal request and let the
system ask for more information as needed. So
while there is an apparent trade-off between
the length of the initial request (surface
complexity) and the number of interactions
needed to fully interpret that request
(interactive complexity), we cannot evaluate
EXPLORER&apos;s effectiveness by trying to minimize
one or the other.
We must also note that conceptual
complexity as it is defined here can only give
a very rough idea of the conceptual content
and information processing involved. It might
be tempting to look for conceptual complexity
as a function of surface complexity and
interactive complexity, but any simple
decomposition along these lines will be
misleading. If a user changes the scale of a
map 10 times, we will see a large interactive
complexity with no change in conceptual
complexity. A more sensitive set of
complexity measures will have to be designed
before we can expect to see correlations
across the various measures.
The results of our trial test period are
summarized in Table 1. We see that the
average surface complexity of all requests is
25 words, with requests ranging from 1 to 87
words in length. Each request averaged 7
complete interactions, with some taking as few
as 3 and others requiring as many as 14
user-interactions. The target query language
requests averaged 11 lines of code, with a
range between 9 and 22 lines.
In terms of performance categories, fully
672 of all requests were A2 requests. Only
10% qualified as Al requests, with the
remaining 23% falling into the A3 category.
A3 requests tended to be slightly more
complicated on average than A2 requests, but
it is important to note that the most complex
requests in terms of all three measures were
nevertheless A2 requests. The relatively
small precentage of Al requests may not be
significant given the size of our sample, but
it is likely that the failed A3 requests would
have been A2 requests had they been processed
successfully. As the system&apos;s hit rate
improves, we expect to see the A2 rate rise
while the Al rate remains stable. It is
interesting to note that the average surface
complexity of the Al requests is very close to
the average surface complexity or the AZ
requests.
Almost all of the errors underlying our
A3 requests were programmer errors due to an
imperfect understanding of user vocabulary or
the target query language. This was expected
and can only be rectified with continued
testing by qualified users. We are extremely
pleased to have a 777. success rate at this
initial stage of program test-development:
EXPLORER&apos;s error rate should decrease over
time as changes are made to correct the errors
we uncover.
</bodyText>
<page confidence="0.994478">
71
</page>
<bodyText confidence="0.999967357142857">
Our experience with EXPLORER suggests
that it is impossible to complete a system of
this complexity without some such testing
phase for feedback purposes. A high degree of
cooperation between program designers and
intended users is therefore critical in these
final stages of system development.
Our next step is to continue testing
revised versions of EXPLORER, expanding our
user population as the system becomes more
competent. At the current rate of user
feedback, we project a 3-6 month period of
system revisions before we freeze the
implementation for a final evaluation.
</bodyText>
<sectionHeader confidence="0.998085" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9914884">
Lehnert, W. and Shwartz, S. (1982). Natural
Language Data Base Access with Pearl.
Proceedings of the Ninth International
Conference on Computational Linguistics.
Prague, Czechoslovakia.
Shwartz, S. (1982). Problems with
domain—independent natural language
database access systems. Proceedings of
the Association for Computational
Linguistics. Toronto, Canada.
</reference>
<page confidence="0.998703">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000216">
<title confidence="0.999819">EXPLORER: A Natural Language Processing System for Oil Exploration</title>
<author confidence="0.999992">Wendy G Lehnert</author>
<affiliation confidence="0.999967666666667">Department of Computer and Information Sciences Graduate Research Center University of Massachusetts</affiliation>
<address confidence="0.999841">Amherst, Mass. 01003</address>
<author confidence="0.999903">Steven P Shwartz</author>
<affiliation confidence="0.999959">Cognitive Systems Inc.</affiliation>
<address confidence="0.989366">234 Church Street New Haven, Ct. 06510</address>
<abstract confidence="0.9661212">EXPLORER (Lehnert and Shvartz, 1982; Shwartz 1982) is a non-fragile, &amp;quot;hands-on&amp;quot; language analysis system that allows oil explorationists with no knowledge of computers or computer programming to create customized maps. Users in Tulsa, Denver, and New Orleans currently have dial-up access to a DEC-20 where EXPLORER is implemented in TLISP. A user converses with EXPLORER about a desired map until both parties have agreed on an adequate and unambiguous set of specifications. Another phone link then carries EXPLORER&apos;s output to an IBM 3033 which runs database retrieval routines on commercial well data. When all the information has been secured from well data, a graphics system takes over to perform the actual map generation. EXPLORER is currently undergoing evaluation, and it is targetted for a 1983 installation in all regional offices of a major oil company* for restricted use by geologists and geophysicists. Since our intended user population is naive about computers, EXPLORER&apos;s interactive design is dominated by &amp;quot;user-friendly&amp;quot; features. EXPLORER processes information retrieval requests stated in English, without imposing vocabulary limitations or syntactic restrictions on the user. Using a 7000-word dictionary, EXPLORER makes inferences about what a user is saying and initiates interactive dialogues when map specifications are not adequate or potentially ambiguous. While &amp;quot;user-friendly&amp;quot; system designs often accomodate novice users at the expense of efficient interactions with expert users, naturally itself to both novice and expert users. An expert can state a request very concisely, without a long interactive session, while a novice is guided by a question-and-answer dialogue. Since all users are expected to enter mispellings and typos, an intelligent spelling corrector enables the user to correct typographical errors quickly and easily. *A proprietary agreement forbids us from identifying them. Once a request has been analyzed by EXPLORER, the user is given an opportunity to verify EXPLORER&apos;s understanding of that request or make changes as needed. The following example will illustrate the complexity of the map requests that EXPLORER is designed to handle. Following the input, EXPLORER displays its interpretation of the request for verification by the user. EXPLORER Version 02 9/23/81 READY *Show ma a map of all tight wells drilled *before May 1, 1980 but since May 1, 1970 *by texaco that show oil deeper than 2000, *are themselves deeper than 5000, are now *operated by shell, are wildcat wells where *the operator reported a drilling problem, *and have mechanical logs, drill stem tests, *and a commercial oil analysis, that were *drilled within the area defined by latitude deg 20 min 30 sec 31:20:30 and *80-81. scale 2000 fest. By FEST, do you mean FEET (Y or N) *y requests POSTED map 30.34167 longitude 80 - 81 PAPER DRILLING DEPTH &gt; FEET</abstract>
<address confidence="0.765466">COMPLETION DATE 5/1/1970</address>
<phone confidence="0.353787">lt; 5/1/1980</phone>
<title confidence="0.734302375">AVAILABLE DRILL STEM TESTS PERFORMED FILE WELL DRILLING PROBLEM WILDCAT WELL SHELL CURRENT OPERATOR WELL SHOW OF OIL &gt; 2000&apos; TEXACO ORIGINAL OPERATOR</title>
<author confidence="0.840589">TIGHT HOLE WELL</author>
<abstract confidence="0.979311012875536">scale of 1&amp;quot; . 2000.0&apos; 69 As this example shows, EXPLORER can handle long and ungrammatical requests, make inferences, and perform complicated word sense disambiguation. For example, the word &amp;quot;show&amp;quot; has two distinct meanings in the above request. The first instance of &amp;quot;show&amp;quot; is synonymous with &amp;quot;display&amp;quot;, whereas the second time &amp;quot;show&amp;quot; is used it has a technical meaning (i.e., an indicator of oil or gas in a well). The surrounding conceptual context is used to determine which sense of &amp;quot;show&amp;quot; is appropriate in each case. We suspect expert users who utilize EXPLORER regularly will develop a feel for EXPLORER&apos;s limitations and flexibilities. If such a user wants to enter a request as efficiently as possible, EXPLORER will handle &amp;quot;telegraphic&amp;quot; English which does not have to conform to standard syntactic conventions: Version 02 READY *tcg bibb georgia ci 100 scale 2000. User requests a STRUCTURE map outputmedium: PAPER output device: PHOTODOT county: BIBB, GEORGIA on TCC CI • 100&apos; scale of 1&amp;quot; • 2000.0&apos; long as the request is coherent and unambiguous, EXPLORER will be to handle a wide range stylistic This last was sufficiently specified for EXPLORER to interpret it without further user interaction, but many requests (especially those generated novice users) are to require an interactive dialog. For example, the following interaction might take place with an user: EXPLORER Version 02 9/23/81 READY *Map the tcg. region was not specified. Do you want the same geographic region as the request N)? *n you wish to specify the map by county or by geographic coordinates? *c original input is interpreted on the first try perfect performance. Please input county or counties by name or number. *Bibb Bibb County is in the following states: ALABAMA GEORGIA Please input the appropriate state or a new county specification. *make it new haven, Ct. contour interval was specified. Would you like the CI from the previous request? (Y or N) Previous CI ■ 100&apos; *y A scale was not specified. Would you like the scale from the previous request? (Y or N) Previous scale 2000.0 FT-PER-IN *y User requests a STRUCTURE map output medium: PAPER output device: photodot county: NEW HAVEN, CONNECTICUT on TCG CI • 100&apos; scale of 1&amp;quot; = 2000.0&apos; EXPLORER will query a user as needed to get missing information and resolve any that may be present. naturally offers the user option inheriting specifications from the map request. Explorationists it to examine a sequence of related maps, so our interface has been to make map sequences to generate. has been undergoing an since July 13, 1982. During a of employees dialed up the program and entered map requests. While we do not yet have enough requests a of the system, we have analyzed EXPLORER&apos;s performance over the three-week period from to 8/6/82 in an effort to its strengths and weaknesses. During this time 39 requests were successfully transmitted to by 8 different In order evaluate performance, we consider the following categories of performance: 70 TABLE - 1 REQUEST TYPE NO. OF REQUESTS SURFACE INTERACTIVE CONCEPTUAL Al 4 (10%) 19(15-25) 3(3-3) 9(9-10) A2 26 (67%) 22(1-87) 7(3-14) 11(9-22) A3 9 (23%) 37(9-57) 8(5-12) 12(10-14) ------ --------- -------total 39 (100%) 25(1-87) 7(3-14) 11(9-22) (A2) original input is interpreted correctly after one or more clarifying interactions. These interactions may be due to typing errors, spelling errors, missing information, or system errors. (A3) original input is never interpreted correctly due to a system failure of some sort. If a request can be categorized as an Al or A2 request, EXPLORER is fully functional even though it may make a mistake at some point in its processing. For example, if EXPLORER does not recognize a word, it will query the user for synonyms. If one of the synonyms is recognized, EXLPORER recovers from its own recognition error, and the request will be categorized as an A2 request. When a system error is fatal in the sense that the user does not or cannot recover from it, we categorize the request as an A3 request: an A3 request should not result in map generation. We have omitted from this analysis any requests that were aborted due to transmission errors or user-initiated interrupts. In addition to our three performance we will characterize general complexity of a request in three ways: (11 Surface Complexity: The number of words in the original input request. (2] Interactive Complexity: The number of complete interactions between the user and EXPLORER during a single request dialog. [3] Conceptual Complexity: The number of lines generated in the target query language. We realize that some users will try to maximize efficient communication by minimizing the number of complete interactions. At the same time, still other users will find it easier to enter a minimal request and let the system ask for more information as needed. So while there is an apparent trade-off between the length of the initial request (surface complexity) and the number of interactions to fully request (interactive complexity), we cannot evaluate by trying to minimize one or the other. We must also note that conceptual complexity as it is defined here can only give a very rough idea of the conceptual content and information processing involved. It might be tempting to look for conceptual complexity as a function of surface complexity and interactive complexity, but any simple decomposition along these lines will be misleading. If a user changes the scale of a map 10 times, we will see a large interactive complexity with no change in conceptual complexity. A more sensitive set of complexity measures will have to be designed before we can expect to see correlations across the various measures. The results of our trial test period are summarized in Table 1. We see that the surface complexity all requests is 25 words, with requests ranging from 1 to 87 words in length. Each request averaged 7 complete interactions, with some taking as few as 3 and others requiring as many as 14 user-interactions. The target query language requests averaged 11 lines of code, with a range between 9 and 22 lines. terms of performance categories, all requests were A2 requests. as Al requests, with the 23% falling the requests tended to slightly complicated on average than A2 requests, but is important to note that most requests in terms of all three measures were A2 requests. relatively precentage of Al not be given the size our but it is likely that the failed A3 requests would been A2 requests had been processed successfully. As the system&apos;s hit rate we expect to see the A2 rise the Al rate remains stable. It is to note that the surface of the is very close to the average surface complexity or the AZ requests. all of errors underlying our were programmer due to an imperfect understanding of user vocabulary or the target query language. This was expected and can only be rectified with continued testing by qualified users. We are extremely pleased to have a 777. success rate at this program test-development: EXPLORER&apos;s error rate should decrease over time as changes are made to correct the errors we uncover. 71 Our experience with EXPLORER suggests that it is impossible to complete a system of this complexity without some such testing phase for feedback purposes. A high degree of cooperation between program designers and intended users is therefore critical in these final stages of system development. Our next step is to continue testing revised versions of EXPLORER, expanding our user population as the system becomes more competent. At the current rate of user we a 3-6 month period of system revisions before we freeze the implementation for a final evaluation.</abstract>
<note confidence="0.859419916666667">REFERENCES Lehnert, W. and Shwartz, S. (1982). Natural Language Data Base Access with Pearl. Proceedings of the Ninth International Conference on Computational Linguistics. Prague, Czechoslovakia. Shwartz, S. (1982). Problems with domain—independent natural language database access systems. Proceedings of the Association for Computational Linguistics. Toronto, Canada. 72</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<pages>01003</pages>
<location>Amherst, Mass.</location>
<marker></marker>
<rawString>Amherst, Mass. 01003</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Steven</author>
</authors>
<title>Shwartz Cognitive Systems Inc.</title>
<booktitle>234 Church</booktitle>
<pages>06510</pages>
<location>Street New Haven, Ct.</location>
<marker>Steven, </marker>
<rawString>Steven P. Shwartz Cognitive Systems Inc. 234 Church Street New Haven, Ct. 06510</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>S Shwartz</author>
</authors>
<title>Natural Language Data Base Access with Pearl.</title>
<date>1982</date>
<booktitle>Proceedings of the Ninth International Conference on Computational Linguistics.</booktitle>
<location>Prague, Czechoslovakia.</location>
<marker>Lehnert, Shwartz, 1982</marker>
<rawString>Lehnert, W. and Shwartz, S. (1982). Natural Language Data Base Access with Pearl. Proceedings of the Ninth International Conference on Computational Linguistics. Prague, Czechoslovakia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shwartz</author>
</authors>
<title>Problems with domain—independent natural language database access systems.</title>
<date>1982</date>
<booktitle>Proceedings of the Association for Computational Linguistics.</booktitle>
<location>Toronto, Canada.</location>
<marker>Shwartz, 1982</marker>
<rawString>Shwartz, S. (1982). Problems with domain—independent natural language database access systems. Proceedings of the Association for Computational Linguistics. Toronto, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>