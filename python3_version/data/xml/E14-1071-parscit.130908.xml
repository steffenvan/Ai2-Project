<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.999425">
Automatic Food Categorization from Large Unlabeled Corpora and Its
Impact on Relation Extraction
</title>
<author confidence="0.98945">
Michael Wiegand and Benjamin Roth and Dietrich Klakow
</author>
<affiliation confidence="0.985039">
Spoken Language Systems
Saarland University
</affiliation>
<address confidence="0.763106">
D-66123 Saarbr¨ucken, Germany
</address>
<email confidence="0.997748">
{Michael.Wiegand|Benjamin.Roth|Dietrich.Klakow}@lsv.uni-saarland.de
</email>
<sectionHeader confidence="0.997367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979875">
We present a weakly-supervised induc-
tion method to assign semantic informa-
tion to food items. We consider two tasks
of categorizations being food-type classi-
fication and the distinction of whether a
food item is composite or not. The cate-
gorizations are induced by a graph-based
algorithm applied on a large unlabeled
domain-specific corpus. We show that the
usage of a domain-specific corpus is vi-
tal. We do not only outperform a manually
designed open-domain ontology but also
prove the usefulness of these categoriza-
tions in relation extraction, outperforming
state-of-the-art features that include syn-
tactic information and Brown clustering.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982177777778">
In view of the large interest in food in many parts
of the population and the ever increasing amount
of new dishes/food items, there is a need of au-
tomatic knowledge acquisition. We approach this
task with the help of natural language processing.
We investigate different methods to assign cate-
gories to food items. We focus on two categoriza-
tions, being a classification of food items to cat-
egories of the Food Guide Pyramid (U.S. Depart-
ment of Agriculture, 1992) and a categorization of
whether a food item is composite or not.
We present a semi-supervised graph-based ap-
proach to induce these food categorizations from
an unlabeled domain-specific text corpus crawled
from the Web. The method only requires mini-
mal manual guidance for the initialization of the
algorithm with seed terms. It depends, however,
on an automatically constructed high-quality sim-
ilarity graph. For that we choose a pattern-based
representation that outperforms a distributional-
based representation. For initialization, we ex-
amine some manually compiled seed words and
a very few simple surface patterns to automati-
cally induce such expressions. As a hard baseline,
we compare the effectiveness of using a general-
purpose ontology for the same types of categoriza-
tions. Apart from an intrinsic evaluation, we also
examine the categories in relation extraction.
The contributions of this paper are a method re-
quiring minimal supervision for a comprehensive
classification of food items and a proof of con-
cept that the knowledge that can thus be gained is
beneficial for relation extraction. Even though we
focus on a specific domain, the induction method
can be easily translated to other domains. In par-
ticular, other life-style domains, such as fashion,
cosmetics or home &amp; gardening, show parallels
since comparable textual web data are available
and similar relation types (e.g. that two items fit
together or can be substituted by each other) exist.
Our experiments are carried out on German data
but our findings should carry over to other lan-
guages since the issues we address are (mostly)
language universal. For general accessibility, all
examples are given as English translations.
</bodyText>
<sectionHeader confidence="0.992149" genericHeader="method">
2 Data &amp; Annotation
</sectionHeader>
<subsectionHeader confidence="0.996628">
2.1 Domain-Specific Text Corpus
</subsectionHeader>
<bodyText confidence="0.9999612">
In order to generate a dataset for our experiments,
we used a crawl of chefkoch.de1 (Wiegand et al.,
2012b) consisting of 418,558 webpages of food-
related forum entries. chefkoch.de is the largest
German web portal for food-related issues.
</bodyText>
<subsectionHeader confidence="0.97736">
2.2 Food Categorization
</subsectionHeader>
<bodyText confidence="0.9998105">
As a food vocabulary, we employ a list of 1888
food items: 1104 items were directly extracted
from GermaNet (Hamp and Feldweg, 1997), the
German version of WordNet (Miller et al., 1990).
The items were identified by extracting all hy-
ponyms of the synset lahrung (English: food). By
</bodyText>
<footnote confidence="0.974284">
1www.chefkoch.de
</footnote>
<page confidence="0.959561">
673
</page>
<note confidence="0.980541071428571">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673–682,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
Class Description Size Perc.
MEAT meat and fish (products) 394 20.87
BEVERAGE beverages (incl. alcoholic drinks) 298 15.78
VEGE vegetables (incl. salads) 231 12.24
SWEET sweets, pastries and snack mixes 228 12.08
SPICE spices and sauces 216 11.44
STARCH starch-based side dishes 185 9.80
MILK milk products 104 5.51
FRUIT fruits 94 4.98
GRAIN grains, nuts and seeds 77 4.08
FAT fat 41 2.18
EGG eggs 20 1.06
</note>
<tableCaption confidence="0.998578">
Table 1: The different food types (gold standard).
</tableCaption>
<bodyText confidence="0.9273">
consulting the relation tuples from Wiegand et al.
(2012c) a further 784 items were added. We man-
ually annotated this vocabulary w.r.t. two tasks:
</bodyText>
<subsubsectionHeader confidence="0.549614">
2.2.1 Task I: Food Types
</subsubsectionHeader>
<bodyText confidence="0.999976857142857">
The food type categories we chose are mainly in-
spired by the Food Guide Pyramid (U.S. Depart-
ment of Agriculture, 1992) that divides food items
into categories with similar nutritional properties.
This categorization scheme not only divides the
set of food items in many intuitive homogeneous
classes but it is also the scheme that is most com-
monly agreed upon. Table 1 lists the specific cat-
egories we use. For category assignment of com-
plex dishes comprising different food items we ap-
plied a heuristics: we always assign the category
that dominates the dish. A meat sauce, for exam-
ple, would thus be assigned MEAT (even though
there may be other ingredients than meat).
</bodyText>
<subsubsectionHeader confidence="0.658758">
2.2.2 Task II: Dishes vs. Atomic Food Items
</subsubsectionHeader>
<bodyText confidence="0.999983846153846">
In addition to Task I, we include another catego-
rization that divides food items into dishes and
atomic food items (Table 2). By dish, we mainly
understand food items that are composite food
items made of other (atomic) food items. This
categorization is orthogonal to the previous clas-
sification of food items. We refrained from adding
dishes as a further category of food types in §2.2.1,
as we would have ended up with a very heteroge-
neous class in the set of homogeneous food type
categories. Thus, dishes that differ greatly in nu-
trient content, such as Waldorfsalad and chocolate
cake, would have been subsumed by one class.
</bodyText>
<sectionHeader confidence="0.998421" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.993991">
3.1 Graph-based Induction
</subsectionHeader>
<bodyText confidence="0.992197">
We propose a semi-supervised graph-based ap-
proach to label food items with their respective
</bodyText>
<table confidence="0.988031666666667">
Class Description Examples Perc.
DISH composite food items cake, falafel, meat loaf 32.10
ATOM non-composite food items apple, steak, potato 67.90
</table>
<tableCaption confidence="0.974756">
Table 2: Distribution of dishes and atomic food
</tableCaption>
<bodyText confidence="0.976079928571428">
items among the food vocabulary (gold standard).
food categories. The underlying data structure
is a similarity graph connecting different food
items. Food items that belong to the same category
should be connected by highly weighted edges. In
order to infer the labels for each respective food
item, one first needs to specify a small set of seeds
for each category and then apply a graph-based
clustering method that divides the graph into clus-
ters that represent distinct food categories. Our
method is a low-resource approach that can also
be easily adapted to other domains. The only
domain-specific information required are an unla-
beled corpus and a set of seeds.
</bodyText>
<subsectionHeader confidence="0.935041">
3.1.1 Construction of the Similarity Graph
</subsectionHeader>
<bodyText confidence="0.999969666666667">
To enable a graph-based induction, we generate a
similarity graph that connects similar food items.
For that purpose, a list of domain-independent
similarity-patterns was compiled. Each pattern is a
lexical sequence that connects the mention of two
food items (Table 3). Each pair of food items ob-
served with any of those patterns is connected via
a weighted edge (the different patterns are treated
equally). The weight is the total frequency of all
patterns co-occurring with a particular food pair.
Due to the high precision of our patterns, with
one or a few prototypical seeds we cannot expect
to find all items of a food category within the set
of items to which the seeds are directly connected.
Instead, one also needs to consider transitive con-
nectedness within the graph. For example, in Fig-
ure 1 banana and redberry are not directly con-
nected but they can be reached via pear or rasp-
berry. However, by considering mediate relation-
ships it becomes more difficult to determine the
most appropriate category for each food item since
most food items are connected to food items of dif-
ferent categories (in Figure 1, there are not only
edges between banana and other types of fruits
but there is also some edge to some sweet, i.e.
chocolate). For a unique class assignment, we ap-
ply a robust graph-based clustering algorithm. (It
will figure out that banana, pear, raspberry and
redberry belong to the same category and choco-
late belongs to another category, since it is mostly
</bodyText>
<page confidence="0.998687">
674
</page>
<table confidence="0.983593666666667">
Patterns food item1 (or|or rather|instead of|“(”) food item2
Example {apple: pineapple, pear, fruit, strawberry, kiwi} {steak:
schnitzel, sausage, roast, meat loaf, cutlet}
</table>
<tableCaption confidence="0.9864125">
Table 3: Domain-independent patterns for build-
ing the similarity graph.
</tableCaption>
<figureCaption confidence="0.998965">
Figure 1: Illustration of the similarity graph.
</figureCaption>
<bodyText confidence="0.676252">
linked to many other food items not being fruits.)
</bodyText>
<subsectionHeader confidence="0.845359">
3.1.2 Semi-Supervised Graph Optimization
</subsectionHeader>
<bodyText confidence="0.977196571428572">
Our semi-supervised graph optimization (Belkin
and Niyogi, 2004) is a robust algorithm that was
primarily chosen since it only contains few free
parameters to adjust. It is based on two principles:
First, similar data points should be assigned simi-
lar labels, as expressed by a similarity graph of la-
beled and unlabeled data. Second, for labeled data
points the prediction of the learnt classifier should
be consistent with the (actual) gold labels.
We construct a weighted transition matrix W
of the graph by normalization of the matrix with
co-occurrence counts C which we obtain from the
similarity graph (§3.1.1). We use the common
normalization by a power of the degree function
di = Ej Cij: it defines Wij = d dλ if i �
i j
and Wii = 0. The normalization weight A is the
first of two parameters used in our experiments for
semi-supervised graph optimization. For learning
the semi-supervised classifier, we use the method
of Zhou et al. (2004) to find a classifying function
which is sufficiently smooth with respect to both
the structure of unlabeled and labeled points.
Given a set of data points X = {x1, ... , xn}
and label set L = {1,... , c}, with xi:1&lt;i&lt;l labeled
as yi E L and xi:l+1&lt;i&lt;n unlabeled. For predic-
tion, a vectorial function F : X → lR,c is estimated
assigning a vector Fi of label scores to every xi.
The predicted labeling follows from these scores
as ˆyi = arg maxj&lt;c Fij. Conversely, the gold la-
beling matrix Y is a n x c matrix with Yij = 1 if
xi is labeled as yi = j and Yij = 0 otherwise.
Minimizing the cost function Q aims at a trade-
off between information from neighbours and ini-
tial labeling information, controlled by parameter
</bodyText>
<table confidence="0.9902844">
Patterns Categorization Examples
patthearst Food Types food item is some food type,
food type such as food item, .. .
pattdishes Dishes recipefor food item
pattatom Atomic Food Items made of|contains food item
</table>
<tableCaption confidence="0.998586">
Table 4: List of patterns to extract seeds.
</tableCaption>
<bodyText confidence="0.480892">
p (the second parameter used in our experiments):
</bodyText>
<equation confidence="0.943736333333333">
� �
� � � + µ En i=1 kFi − Yik
�
</equation>
<bodyText confidence="0.999502785714286">
where Si is the degree function of W.
The first term in Q is the smoothness constraint,
its minimization leads to adjacent edges having
similar labels. The second term is the fitting con-
straint, its minimization leads to consistency of the
function F with the labeling of the data. The solu-
tion to the above cost function is found by solving
a system of linear equations (Zhou et al., 2004).
As we do not possess development data for this
work, we set the two free parameters A = 0.5 and
p = 0.01. This setting is used for both induction
tasks and all configurations. It is a setting that pro-
vided reasonable results without any notable bias
for any particular configuration we examine.
</bodyText>
<subsectionHeader confidence="0.775898">
3.1.3 Manually vs. Automatically Extracted
Seeds
</subsectionHeader>
<bodyText confidence="0.987242380952381">
We explore two types of seed initializations: (a)
a manually compiled list of seed food items and
(b) a small set of patterns (Table 4) by the help of
which such seeds are automatically extracted.
In order to extract seeds for Task I with the
pattern-based approach, we apply the patterns
from Hearst (1992). These patterns have been de-
signed for the acquisition of hyponyms. Task I can
also be regarded as some type of hyponym extrac-
tion. The food types (fruit, meat, sweets) repre-
sent the hypernyms for which we extract seed hy-
ponyms (banana, beef, chocolate).
In order to extract seeds for Task II, we apply
two domain-specific sets of patterns (pattdish and
pattatom). We rank the food items according to the
frequency of occurring with the respective pattern
set. Since food items may occur in both rankings,
we merge the two rankings in the following way:
score(food item) = #pattdish(food it.) − #pattatom(food it.)
The top end of this ranking represents dishes
while the bottom end represents atoms.
</bodyText>
<subsectionHeader confidence="0.999941">
3.2 Using a General-Purpose Ontology
</subsectionHeader>
<bodyText confidence="0.9996265">
As a hard baseline, we also make use of the seman-
tic relationships encoded in GermaNet. Our two
</bodyText>
<equation confidence="0.987219333333333">
1 n i,j=1
Wij
2
&apos;1δ. Fi − �a Fj
t �
II� � � � �
</equation>
<page confidence="0.987532">
675
</page>
<bodyText confidence="0.999940222222222">
types of food categorization schemes can be ap-
proximated with the hypernymy graph in that on-
tology: We manually identify nodes that resemble
our food categories (e.g. fruit, meat or dish) and
label any food item that is an immediate or a me-
diate hyponym of these nodes (e.g. apple for fruit)
with the respective category label. The downside
of this method is that a large amount of food items
is missing from the GermaNet-database (§2.2).
</bodyText>
<subsectionHeader confidence="0.998132">
3.3 Other Baselines &amp; Post-Processing
</subsectionHeader>
<bodyText confidence="0.999100428571429">
In addition to the previous methods we imple-
ment a heuristic baseline (HEUR) that rests on the
observation that German food items of the same
food category often share the same suffix, e.g.
Schokoladenkuchen (English: chocolate cake) and
Apfelkuchen (English: apple pie). For HEUR, we
manually compiled a set of few typical suffixes for
each food type/dish category (ranging from 3 to 8
suffixes per category). For classification of a food
item, we assign the food item the category label
whose suffix matched with the food item.2
We also examine an unsupervised baseline
(UNSUP) that applies spectral clustering on the
similarity graph following von Luxburg (2007):
</bodyText>
<listItem confidence="0.992186142857143">
• Input: a similarity matrix W and the number of categories to detect k.
• The laplacian L is constructed from W. It is the symmetric laplacian
L = I − D1/2WD1/2, where D is a diagonal degree matrix.3
• A matrix U ∈ Rn×k is constructed that contains as columns the first
k eigenvectors u1, ... , uk of L.
• The rows of U are interpreted as the new data points. The final cluster-
ing is obtained by k-means clustering of the rows of U.
</listItem>
<bodyText confidence="0.985279761904762">
UNSUP (which is completely parameter-free)
gives some indication about the intrinsic expres-
siveness of the similarity graph as it lacks any
guidance towards the categories to be predicted.
In graph-based food categorization, one can
only make predictions for food items that are con-
nected (be it directly or indirectly) to seed food
items within the similarity graph. To expand labels
to unconnected food items, we apply some post-
processing (POSTP). Similarly to HEUR, it ex-
ploits the suffix-similarity of food items. It assigns
each unconnected food item the label of the food
item (that could be labeled by the graph optimiza-
tion) that shares the longest suffix. Due to their
similar nature, we refrain from applying POSTP
on HEUR as it would produce no changes.
2Unlike German food items, English food items are of-
ten multi-word expressions. Therefore, we assume that for
English, instead of analyzing suffixes the usage of the head
of a multiword expression (i.e. chocolate cake) would be an
appropriate basis for a similar heuristic.
</bodyText>
<footnote confidence="0.811472">
3That is, Dii equals to the sum of the ith row.
</footnote>
<table confidence="0.999088307692308">
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
UNSUP ✓ 46.2 43.1 35.7 36.0 56.1 41.0 42.5 38.4
HEUR (plain) 25.5 87.9 32.2 42.9 N/A N/A N/A N/A
HEUR ✓ 56.4 73.6 52.1 54.7 68.7 72.3 64.3 60.7
PAT-Top1 ✓ 52.4 60.2 51.2 52.5 64.5 58.2 62.9 57.4
PAT-Top5 ✓ 61.1 70.7 61.9 64.4 74.5 67.9 76.0 69.7
PAT-Top10 ✓ 60.2 69.6 60.5 62.2 73.4 66.7 74.2 67.3
1-PROTO ✓ 58.0 68.0 58.0 59.5 70.2 64.1 71.0 63.8
5-PROTO ✓ 64.5 76.6 63.7 68.6 78.6 73.8 78.5 75.2
10-PROTO ✓ 65.8 79.0 65.5 71.0 80.2 75.9 80.6 77.7
GermaNet (plain) 52.1 94.0 52.0 65.7 75.4 73.2 75.0 72.4
GermaNet ✓ 68.3 84.7 63.4 71.6 82.7 81.8 77.7 79.1
</table>
<tableCaption confidence="0.9952265">
Table 5: Comparison of different food-type classi-
fiers (graph indicates graph-based optimization).
</tableCaption>
<sectionHeader confidence="0.999031" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999332">
We report precision, recall and F-score and accu-
racy.4 For precision, recall and F-score, we list the
macro-averaged score.
</bodyText>
<subsectionHeader confidence="0.9993045">
4.1 Evaluation of Food Categorization
4.1.1 Detection of Food Types
</subsectionHeader>
<bodyText confidence="0.999863318181818">
Table 5 compares different classifiers and configu-
rations for the prediction of food types (against the
gold standard from Table 1). Apart from the pre-
viously described baselines, we consider n man-
ually selected prototypes (n-PROTO) and the top
n food items produced by Hearst-patterns (PAT-
Topn) as seeds for graph-based optimization. The
table shows that the semi-supervised graph-based
approach with these seeds outperforms the base-
lines UNSUP and HEUR. Only as few as 5
prototypical seeds (per category) are required to
obtain performance that is even better than us-
ing plain GermaNet. The table also shows that
post-processing (with our suffix-heuristics) con-
sistently improves performance. Manually choos-
ing prototypes is more effective than instantiating
seeds via Hearst-patterns. The quality of the out-
put of Hearst-patterns degrades from top 10 on-
wards. However, considering that PAT-Topn does
not include any manual intervention, it already
produces decent results. Finally, even GermaNet
can be effectively used as seeds.
</bodyText>
<subsectionHeader confidence="0.622784">
4.1.2 Detection of Dishes
</subsectionHeader>
<tableCaption confidence="0.908590333333333">
Table 6 compares different classifiers for the de-
tection of dishes (against the gold standard from
Table 2). Dishes and atomic food items are very
</tableCaption>
<footnote confidence="0.994483666666667">
4All manually labeled resources are available at:
www.lsv.uni-saarland.de/personalPages/
michael/relFood.html
</footnote>
<page confidence="0.985708">
676
</page>
<table confidence="0.999571307692308">
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
UNSUP ✓ 54.5 59.6 40.2 37.3 67.9 59.0 50.0 40.6
HEUR (plain) 74.1 84.3 59.9 58.6 N/A N/A N/A N/A
PAT-Top25 ✓ 59.7 72.2 54.6 61.9 74.1 70.1 67.6 68.4
PAT-Top50 ✓ 60.9 74.4 55.6 63.1 75.9 72.7 69.2 70.3
PAT-Top100 ✓ 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0
PAT-Top250 ✓ 59.6 71.8 55.1 62.2 74.2 70.3 68.7 69.3
RAND-25 ✓ 61.4 77.1 54.3 61.8 76.1 74.4 67.1 68.4
RAND-50 ✓ 62.6 76.3 60.1 67.2 77.2 74.0 76.8 74.4
RAND-100 ✓ 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1
GermaNet (plain) 49.5 81.3 46.5 59.3 79.0 75.9 75.5 75.7
GermaNet ✓ 60.8 79.4 51.3 57.6 75.9 78.2 64.4 65.4
</table>
<tableCaption confidence="0.97805">
Table 6: Comparison of different classifiers dis-
tinguishing between dishes and atomic food items
(graph indicates graph-based optimization).
</tableCaption>
<table confidence="0.999730833333333">
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
PAT-Top100 (plain) 9.5 89.5 10.5 18.6 63.6 61.5 63.5 61.3
PAT-Top100 ✓ 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0
RAND-100 (plain) 10.6 100.0 12.2 21.4 70.2 69.7 69.0 69.0
RAND-100 ✓ 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1
</table>
<tableCaption confidence="0.8965995">
Table 7: Impact of graph-based optimization
(graph) for the detection of dishes.
</tableCaption>
<bodyText confidence="0.99960636">
heterogeneous classes which is why more seeds
are required for initialization. This means that
we cannot look for prototypes. For simplicity,
we resorted to randomly sample seeds from our
gold standard (RAND-n). For HEUR, we could
not find a small and intuitive set of suffixes that
are shared by many atomic food types, therefore
we considered all food types from our vocabulary
whose suffix did not match a typical dish suffix as
atomic. As this leaves no unspecified food items in
our vocabulary, we cannot use the output of HEUR
as seeds for graph-based optimization.
In contrast to the previous experiment, HEUR is
a more robust baseline. But again, post-processing
mostly improves performance, and patterns are not
as good as manual (random) seeds yet the former
are notably better than HEUR w.r.t. F-Score. Un-
like in the food-type classification, graph-based
optimization applied on GermaNet does not result
in some improvement. We assume that the preci-
sion of plain GermaNet with 81.3% is too low.5
Since GermaNet cannot effectively be used as
seeds for the graph-based optimization and post-
processing has already a strong positive effect, we
may wonder how effective the actual graph-based
</bodyText>
<footnote confidence="0.9574905">
5For other seeds for which it worked, we usually mea-
sured a precision of 90% or higher.
</footnote>
<bodyText confidence="0.999358666666667">
optimization is for this classification task. Af-
ter all, significantly more seeds are required for
this classification task than for the previous task,
so we need to show that it is not the mere seeds
(+post-processing) that are required for a reason-
able categorization. Table 7 examines two key
configurations with and without graph-based op-
timization. It shows that also for this classification
task, graph-based optimization produces a catego-
rization superior to the mere seeds. Moreover, the
suffix-based post-processing is complementary to
the improvement by the graph-based optimization.
</bodyText>
<subsectionHeader confidence="0.925406">
4.1.3 Comparison of Initialization Methods
</subsectionHeader>
<bodyText confidence="0.999954071428572">
Table 8 compares for each food type 5 manually
selected prototypical seeds (i.e. 5-PROTO) and
the 5 food items most frequently been observed
with patthearst (Table 4). While the manually cho-
sen seeds represent the spectrum of food items
within each particular class (e.g. for STARCH,
some type of pasta, rice and potato was chosen),
it is not possible to enforce such diversity with
the automatically extracted seeds. However, most
food items are correct. Table 9 displays the 10
most highly ranked dishes and atomic food items
extracted with pattdish and pattatom (Table 4). Un-
like the previous task (Table 8), we obtain more
heterogeneous seeds within the same class.
</bodyText>
<subsubsectionHeader confidence="0.653022">
4.1.4 Distributional Similarity
</subsubsectionHeader>
<bodyText confidence="0.9999725">
Since many recent methods for related tasks, such
as noun classification, are based on so-called dis-
tributional similarity (Riloff and Shepherd, 1997;
Lin, 1998; Snow et al., 2004; Weeds et al., 2004;
Yamada et al., 2009; Huang and Riloff, 2010;
Lenci and Benotto, 2012), we also examine this as
an alternative representation to the pattern-based
similarity graph (Table 3). We represent each food
item as a vector which itself is an aggregate of
the contexts of all mentions of a particular food
item. We weighted the individual (context) words
co-occurring with the food item at a fixed window
size of 5 words with tf-idf. We can now apply
graph-based optimization on the similarity matrix
encoding the cosine similarities between any pos-
sible pair of vectors representing two food items.
As seeds, we use the best configuration (not em-
ploying GermaNet), i.e. 10-PROTO for food type
classification and RAND-100 for the dish classi-
fication. Since, however, the graph clustering is
not actually necessary, as we have a full similar-
ity matrix (rather than a sparse graph) that also al-
</bodyText>
<page confidence="0.998006">
677
</page>
<note confidence="0.86192575">
Class 5 Manually Chosen Seeds (5-PROTO) 5 Hearst-Pattern Seeds (PAT-Top5)
MEAT schnitzel, rissole, bologna, redfish, trout salmon, beef, chicken, turkey hen, poultry
BEVERAGE coffee, tea, water, beer, coke coffee, beer, mineral water, lemonade, tea
VEGE peas, green salad, tomato, cauliflower, carrot zucchini, lamb’s salad, broccoli, leek, cauliflower
SWEET chocolate, torte, popcorn, apple pie, potato crisps wine gum, marzipan, custard, pancake, biscuits
SPICE pepper, cinnamon, salt, gravy, remoulade cinnamon, laurel, clove, tomato sauce, basil
STARCH spaghetti, basmati rice, white bread, potato, french fries au gratin potatoes, jacketpotato, potato, pita, jam
MILK yoghurt, gouda, cream cheese, cream, butter milk butter milk, bovine milk, soured milk, goat cheese, sour cream
FRUIT banana, apple, strawberries, apricot, orange banana, strawberries, pear, melon, kiwi
GRAIN hazelnut, pumpkin seed, rye flour, semolina, wheat sesame, spelt, wheat, millet, barley
FAT margarine, lard, colza oil, spread, butter margarine, lard, resolidified butter, coconut oil, tartar
EGG scrambled eggs, fried eggs, chicken egg, omelette, pickled egg yolk, fried eggs, albumen, offal, easter egg
</note>
<tableCaption confidence="0.97274">
Table 8: Comparison of different seed initializations for the food type categorization task (underlined
food items represent erroneously extracted food items).
</tableCaption>
<bodyText confidence="0.997864166666667">
lows us to compare any arbitrary pair of food items
directly, we also employ a second classifier (for
comparison) based on the nearest neighbour prin-
ciple. We assign each food item the label of the
most similar seed food item.
Table 10 compares these two classifiers with the
best previous result. It shows that the pattern-
based representation consistently outperforms the
distributional representation. The former may be
sparse but it produces high-precision similarity
links.6 The vector representation, on the other
hand, may not be sparse but it contains a high
degree of noise. The major problem is that not
only vectors of similar food items, such as chips
(fries), potatoes and rice, are similar to each other,
but also vectors of different food items that are
typically consumed with each other (e.g. fish
and chips). This is because of their frequent co-
occurrence (as in collocations like fish &amp; chips).
Unfortunately, these pairs belong to different food
types. For the dish classification, however, the
vector representation is less of a problem.7
The distributional representation works better
with the simple nearest neighbour classifier. We
assume that graph-based optimization adds further
noise to the classification since, unlike the nearest
neighbour which only calculates the direct similar-
ity between two vectors, it also incorporates indi-
rect relationships (which may be more error-prone
than the direct relationships) between food items.
</bodyText>
<subsectionHeader confidence="0.578038">
4.1.5 Do we need a domain-specific corpus?
</subsectionHeader>
<bodyText confidence="0.984281666666667">
In this section, we want to provide evidence that
apart from the similarity graph and seeds the tex-
tual source for the graph, i.e. our domain-specific
</bodyText>
<footnote confidence="0.9873665">
6By the label propagation within the graph-based opti-
mization, the sparsity problem is also mitigated.
7Fish and chips are both atoms, so in the dish classifica-
tion, it is no mistake to consider them similar food items.
</footnote>
<table confidence="0.73255">
Class 10 Seeds Extracted with Patterns (PAT-Top10)
DISH cookies, cake, praline, bread dumpling, jam, biscuit, cheese
cake, black-and-whites, onion tart, pasta salad
ATOM marzipan, flour, potato, olive oil, water, sugar, cream, choco-
late, milk, tomato
</table>
<tableCaption confidence="0.987897">
Table 9: Illustration of seed initialization for the
distinction between dishes and atomic food items.
</tableCaption>
<table confidence="0.999831571428571">
Task Similarity Classifier Acc F1
Food Type distributional nearest neighbour 53.4 51.1
distributional graph 25.6 25.6
pattern-based graph 80.2 77.7
Dish distributional nearest neighbour 76.8 75.2
distributional graph 71.5 71.2
pattern-based graph 83.0 80.1
</table>
<tableCaption confidence="0.998619">
Table 10: Impact of the similarity representation.
</tableCaption>
<bodyText confidence="0.9999088">
corpus (chefkoch.de), is also important. For that
purpose, we compare our current corpus against
an open-domain corpus. We consider the German
version of Wikipedia since this resource also con-
tains encyclopedic knowledge about food items.
Table 11 compares the graph-based induction. As
in the previous section, we only consider the best
previous configuration. The table clearly shows
that our domain-specific text corpus is a more ef-
fective resource for our purpose than Wikipedia.
</bodyText>
<subsectionHeader confidence="0.801631">
4.2 Evaluation for Relation Extraction
</subsectionHeader>
<bodyText confidence="0.9998516">
We now examine whether automatic food cate-
gorization can be harnessed for relation extrac-
tion. The task is to detect instances of the relation
types SuitsTo, SubstitutedBy and IngredientOf in-
troduced Wiegand et al. (2012b) (repeated in Ta-
ble 12) and motivated in Wiegand et al. (2012a).
These relation types are highly relevant for cus-
tomer advice/product recommendation. In partic-
ular, SuitsTo and SubstitutedBy are fairly domain-
independent relation types. Customers want to
</bodyText>
<page confidence="0.997933">
678
</page>
<bodyText confidence="0.991775470588235">
know which items can be used together (SuitsTo),
be it two food items that can be used as a meal
or two fashion items that can be worn together.
Substitutes are also relevant for situations in which
item A is out of stock but item B can be offered as
an alternative. Therefore, insights from this work
should carry over to other domains.
We randomly extracted 1500 sentences from
our text corpus (§2.1) in which (at least) two food
items co-occur. Each food pair mention was man-
ually assigned one label. In addition to the three
relation types from above, we introduce the la-
bel Other for cases in which either another rela-
tion between the target food items is expressed or
the co-occurrence is co-incidental. On a subset of
200 sentences, we measured a substantial inter-
annotation agreement of Cohen’s κ = 0.67 (Lan-
dis and Koch, 1977).
We train a supervised classifier and incorporate
the knowledge induced from our domain-specific
corpus as features. We chose Support Vector Ma-
chines with 5-fold cross-validation using SVMlight-
multi-class (Joachims, 1999).
Table 13 displays all features that we examine
for supervised classification. Most features are
widely used throughout different NLP tasks. One
special feature brown takes into consideration the
output of Brown clustering (Brown et al., 1992)
which like our graph-based optimization produces
a corpus-driven categorization of words. Simi-
lar to UNSUP, this method is unsupervised but it
considers the entire vocabulary of our text corpus
rather than only food items. Therefore, this in-
formation can be considered as a generalization
of all contextual words. Such type of informa-
tion has been shown to be useful for named-entity
recognition (Turian et al., 2010) and relation ex-
traction (Plank and Moschitti, 2013).
For syntactic parsing, Stanford Parser (Rafferty
and Manning, 2008) was used. For Brown cluster-
ing, the SRILM-toolkit (Stolcke, 2002) was used.
Following Turian et al. (2010), we induced 1000
clusters (from our domain-specific corpus §2.1).
4.2.1 Why should food categories be helpful
for relation extraction?
All relation types we consider comprise pairs of
two food items which makes these relation types
likely to be confused. Contextual information may
be used for disambiguation but there may also be
frequent contexts that are not sufficiently informa-
tive. For example, 25% of the instances of Ingre-
</bodyText>
<table confidence="0.995432333333333">
PLAIN +POSTP
Task Corpus graph Acc F1 Acc F1
Wikipedia ✓ 40.3 49.4 61.4 59.8
Food Type chefkoch.de ✓ 65.8 71.0 80.2 77.7
Wikipedia ✓ 50.4 53.1 75.4 71.1
Dish chefkoch.de ✓ 66.5 71.3 83.0 80.1
</table>
<tableCaption confidence="0.994698">
Table 11: Comparison of Wikipedia and domain-
</tableCaption>
<bodyText confidence="0.66517625">
specific corpus as a source for the similarity graph.
dientOf follow the lexical pattern food items with
food item2 (1). However, the same pattern also
covers 15% of the instances of SuitsTo (2).
</bodyText>
<listItem confidence="0.901173">
(1) We had a stew with red lentils. (Relation: IngredientOf)
(2) We had salmon with broccoli. (Relation: SuitsTo)
</listItem>
<bodyText confidence="0.999498">
The food type information we learned from our
text corpus might tell us which of the food items
are dishes. Only in (1), there is a dish, i.e. stew.
So, one may infer that the presence of dishes is
indicative of IngredientOf rather than SuitsTo.
food items and food item2 is another ambigu-
ous context. It cannot only be observed with the
relation SuitsTo, as in (3) (66% of all instantia-
tions of that pattern), but also SubstitutedBy (20%
of all mentions of that relation match that pattern),
as in (4). For SuitsTo, two food items that belong
to two different classes (e.g. MEAT and STARCH
or MEAT and VEGE) are quite characteristic. For
SubstitutedBy, the two food items are very often of
the same category of the Food Guide Pyramid.
</bodyText>
<listItem confidence="0.997229666666667">
(3) I very often eat fish and chips. (Relation: SuitsTo)
(4) For these types of dishes you can offer both Burgundy wine and
Champagne. (Relation: SubstitutedBy)
</listItem>
<bodyText confidence="0.9998359">
Since the second ambiguous context involves
the two general relation types SuitsTo and Substi-
tutedBy, resolving this ambiguity with automati-
cally induced type information has some signifi-
cance for other domains. In particular, for other
life-style domains, domain-specific type informa-
tion could be obtained following our method from
§3.1. The disambiguation rule that two entities of
the same type imply SubstitutedBy otherwise they
imply SuitsTo should also be widely applicable.
</bodyText>
<sectionHeader confidence="0.760738" genericHeader="evaluation">
4.2.2 Results
</sectionHeader>
<bodyText confidence="0.999349714285714">
Table 14 displays the performance of the different
feature sets for relation extraction. The features
designed from graph-based induction (i.e. graph)
work slightly better than GermaNet. The perfor-
mance of patt is not impressively high. However,
one should consider that patt can be used directly
without a supervised classifier (as each pattern is
</bodyText>
<page confidence="0.997606">
679
</page>
<table confidence="0.987427166666667">
Relation Description Example Freq. Perc.
SuitsTo food items that are typically consumed together My kids love the simple combination offish fingers 633 42.20
with mashed potatoes.
SubstitutedBy similar food items commonly consumed in the same situations We usually buy margarine instead of butter. 336 22.40
IngredientOf ingredient of a particular dish Falafel is made of chickpeas. 246 16.40
Other other relation or co-occurrence of food items are co-incidental On my shopping list, I’ve got bread, cauliflower, ... 285 19.00
</table>
<tableCaption confidence="0.996652">
Table 12: The different relation types and their respective frequency on our dataset.
</tableCaption>
<table confidence="0.998891466666667">
Features Description
patt lexical surface patterns used in Wiegand et al. (2012b)
word bag-of-words features: all words within the sentence
brown features using Brown clustering: all features from word but
words are replaced by induced clusters
pos part-of-speech sequence between target food items and tags
of the words immediately preceding and following them
synt path from syntactic parse tree from first target food item to
second target food item
conj conjunctive features: patt with brown classes of target food
items; pos sequence with brown classes of target food items;
synt with brown classes of target food items
graph semantic food information induced by graph optimization
(config.: 10-PROTO(+POSTP) and RAND-100(+POSTP))
germanet semantic food information derived from (plain) GermaNet
</table>
<tableCaption confidence="0.99955">
Table 13: Description of the feature set.
</tableCaption>
<bodyText confidence="0.9998634375">
designed for a particular relation type, one can
read off from the matching pattern which class is
predicted). word is slightly better but, unlike patt,
it is dependent on supervised learning.
The only feature that individually manages to
significantly outperform word is graph. The tra-
ditional features (i.e. pos, synt and brown) only
produce some mild improvement when added
jointly to word along some conjunctive fea-
tures. When graph is added to this feature set
(i.e. word+patt+pos+synt+brown+conj), we ob-
tain another significant improvement. In con-
clusion, the information we induced from our
domain-specific corpus cannot be obtained by
other NLP-features, including other state-of-the-
art induction methods such as Brown clustering.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9993675">
While many of the previous works on noun catego-
rization also address the task of hypernym classifi-
cation (Hearst, 1992; Caraballo, 1999; Widdows,
2003; Kozareva et al., 2008; Huang and Riloff,
2010; Lenci and Benotto, 2012) and some include
examples involving food items (Widdows and
Dorow, 2002; Cederberg and Widdows, 2003),
only van Hage et al. (2005) and van Hage et al.
(2006) specifically focus on the classification of
food items. van Hage et al. (2005) deal with on-
tology mapping whereas van Hage et al. (2006)
explore part-whole relations.
</bodyText>
<table confidence="0.999609533333333">
Features Acc Prec Rec F1
germanet 45.3 41.3 37.2 37.3
graph 46.0 39.4 39.7 38.6
patt 59.8 49.8 41.1 38.7
word 60.1 56.9 54.5 55.1
word+patt 60.3 57.3 54.9 55.5
word+brown 59.5 56.1 54.6 54.9
word+synt 60.3 57.7 55.4 56.0
word+pos 59.8 56.6 54.6 55.1
word+germanet 61.3 58.6 56.0 56.7
word+graph 62.9 59.2 57.6 58.1°
word+patt+brown+synt+pos 60.4 57.3 56.2 56.5
word+patt+brown+synt+pos+conj 61.7 59.0 57.8 58.2*
word+patt+brown+synt+pos+conj+germanet 63.1 60.2 58.6 59.1°
word+patt+brown+synt+pos+conj+graph 64.7 62.1 60.3 60.9°†
</table>
<tableCaption confidence="0.9611105">
statistical significance testing (paired t-test): better than word * at p &lt; 0.1/
° at p &lt; 0.05; † better than word+patt+brown+synt+pos+conj at p &lt; 0.05
Table 14: Comparison of various features (Ta-
ble 13) for (unrestricted) relation extraction.
</tableCaption>
<bodyText confidence="0.998671727272727">
The task of data-driven lexicon expansion has
also been explored before (Kanayama and Na-
sukawa, 2006; Das and Smith, 2012), however,
our paper presents the first attempt to carry out
a comprehensive categorization for the food do-
main. For the first time, we also show that type
information can effectively improve the extraction
of very common relations. For the twitter domain,
the usage of type information based on cluster-
ing has already been found effective for supervised
learning (Bergsma et al., 2013).
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989846153846">
We presented an induction method to assign se-
mantic information to food items. We considered
two types of categorizations being food-type infor-
mation and information about whether a food item
is composite or not. The categorization is induced
by graph-based optimization applied on a large
unlabeled domain-specific text corpus. We pro-
duce categorizations that outperform a manually
compiled resource. The usage of such a domain-
specific corpus based on a pattern-based represen-
tation is vital and largely outperforms other text
corpora or a distributional representation. The in-
duced knowledge improves relation extraction.
</bodyText>
<page confidence="0.970996">
680
</page>
<bodyText confidence="0.896225636363636">
Acknowledgements putational Linguistics (COLING), pages 539–545,
Nantes, France.
This work was performed in the context of the Software-
Cluster project SINNODIUM. Michael Wiegand was funded
by the German Federal Ministry of Education and Research
(BMBF) under grant no. 01IC12SO1X. Benjamin Roth is
a recipient of the Google Europe Fellowship in Natural Lan-
guage Processing, and this research is supported in part by
this Google Fellowship. The authors would like to thank
Stephanie K¨oser for annotating the dataset presented in this
paper.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999721053763441">
Mikhail Belkin and Partha Niyogi. 2004. Semi-
supervised learning on Riemannian manifolds. Ma-
chine Learning, 56(1-3):209–239.
Shane Bergsma, Mark Dredze, Benjamin Van
Durme, Theresa Wilson, and David Yarowsky.
2013. Broadly Improving User Classification
via Communication-Based Name and Location
Clustering on Twitter. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), pages
1010–1019, Atlanta, GA, USA.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467–479.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages
120–126, College Park, MD, USA.
Scott Cederberg and Dominic Widdows. 2003. Us-
ing LSA and Noun Coordination Information to Im-
prove the Precision and Recall of Automatic Hy-
ponymy Extraction. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL), pages 111–118, Edmonton, Alberta,
Canada.
Dipanjan Das and Noah A. Smith. 2012. Graph-
Based Lexicon Expansion with Sparsity-Inducing
Penalties. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the ACL (HLT/NAACL), pages 677–
687, Montr´eal, Quebec, Canada.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In Proceedings
ofACL workshop Automatic Information Extraction
and Building ofLexical Semantic Resourcesfor NLP
Applications, pages 9–15, Madrid, Spain.
Marti A. Hearst. 1992. Automatic Acquisition of
Hyponyms from Large Text Corpora. In Pro-
ceedings of the International Conference on Com-
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-specific Semantic Class Taggers from (al-
most) Nothing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 275–285, Uppsala, Sweden.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Sch¨olkopf, C. Burges,
and A. Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 169–184. MIT
Press.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully Automatic Lexicon Expansion for Domain-
oriented Sentiment Analysis. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 355–363, Syd-
ney, Australia.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic Class Learning from the Web
with Hyponym Pattern Linkage Graphs. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1048–
1056, Columbus, OH, USA.
J. Richard Landis and Gary G. Koch. 1977. The
Measurement of Observer Agreement for Categor-
ical Data. Biometrics, 33(1):159–174.
Alessandro Lenci and Guilia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of the Joint Conference on Lexical
and Computational Semantics (*SEM), pages 75–
79, Montr´eal, Quebec, Canada.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics and International Conference on Computa-
tional Linguistics (ACL/COLING), pages 768–774,
Montreal, Quebec, Canada.
George Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Introduction to WordNet: An On-line Lexical
Database. International Journal of Lexicography,
3:235–244.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adapation of Relation Extraction. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1498–
1507, Sofia, Bulgaria.
Anna Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the ACL
Workshop on Parsing German (PaGe), pages 40–46,
Columbus, OH, USA.
</reference>
<page confidence="0.978269">
681
</page>
<reference confidence="0.99965">
Ellen Riloff and Jessica Shepherd. 1997. A
Corpus-Based Approach for Building Semantic
Lexicons. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 117–124, Providence, RI, USA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning Syntactic Patterns for Automatic Hyper-
nym Discovery. In Advances in Neural Informa-
tion Processing Systems (NIPS), Vancouver, British
Columbia, Canada.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901–904, Denver, CO, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-supervised Learning. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 384–394,
Uppsala, Sweden.
Human Nutrition Information Service U.S. Depart-
ment of Agriculture. 1992. The Food Guide Pyra-
mid. Home and Garden Bulletin 252, Washington,
D.C., USA.
Willem Robert van Hage, Sophia Katrenko, and Guus
Schreiber. 2005. A Method to Combine Linguis-
tic Ontology-Mapping Techniques. In Proceedings
of International Semantic Web Conference (ISWC),
pages 732 – 744, Galway, Ireland. Springer.
Willem Robert van Hage, Hap Kolb, and Guus
Schreiber. 2006. A Method for Learning Part-
Whole Relations. In Proceedings of International
Semantic Web Conference (ISWC), pages 723 – 735,
Athens, GA, USA. Springer.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17:395–416.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of the International Con-
ference on Computational Linguistics (COLING),
pages 1015–1021, Geneva, Switzerland.
Dominic Widdows and Beate Dorow. 2002. A
Graph Model for Unsupervised Lexical Acquisition.
In Proceedings of the International Conference on
Computational Linguistics (COLING), pages 1093–
1099, Taipei, Taiwan.
Dominic Widdows. 2003. Unsupervised methods for
developing taxonomies by combining syntactic and
statistical information. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), pages
197–204, Edmonton, Alberta, Canada.
Michael Wiegand, Benjamin Roth, and Dietrich
Klakow. 2012a. Knowledge Acquisition with Nat-
ural Language Processing in the Food Domain: Po-
tential and Challenges. In Proceedings of the ECAI-
Workshop on Cooking with Computers (CWC),
pages 46–51, Montpellier, France.
Michael Wiegand, Benjamin Roth, and Dietrich
Klakow. 2012b. Web-based Relation Extraction
for the Food Domain. In Proceedings of the In-
ternational Conference on Applications of Natu-
ral Language Processing to Information Systems
(NLDB), pages 222–227, Groningen, the Nether-
lands. Springer.
Michael Wiegand, Benjamin Roth, Eva Lasarcyk,
Stephanie K¨oser, and Dietrich Klakow. 2012c. A
Gold Standard for Relation Extraction in the Food
Domain. In Proceedings of the Conference on
Language Resources and Evaluation (LREC), pages
507–514, Istanbul, Turkey.
Ichiro Yamada, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym Dis-
covery Based on Distributional Similarity and Hi-
erarchical Structures. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 929–927, Singapore.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Sch¨olkopf. 2004.
Learning with Local and Global Consistency. In
Advances in Neural Information Processing Systems
(NIPS), Vancouver and Whistler, British Columbia,
Canada.
</reference>
<page confidence="0.997949">
682
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.434128">
<title confidence="0.999188">Automatic Food Categorization from Large Unlabeled Corpora and Impact on Relation Extraction</title>
<author confidence="0.966239">Wiegand Roth</author>
<affiliation confidence="0.7951355">Spoken Language Saarland</affiliation>
<address confidence="0.721423">D-66123 Saarbr¨ucken,</address>
<abstract confidence="0.993682705882353">We present a weakly-supervised induction method to assign semantic information to food items. We consider two tasks of categorizations being food-type classification and the distinction of whether a food item is composite or not. The categorizations are induced by a graph-based algorithm applied on a large unlabeled domain-specific corpus. We show that the usage of a domain-specific corpus is vital. We do not only outperform a manually designed open-domain ontology but also prove the usefulness of these categorizations in relation extraction, outperforming state-of-the-art features that include syntactic information and Brown clustering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
</authors>
<title>Semisupervised learning on Riemannian manifolds.</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<pages>56--1</pages>
<contexts>
<context position="8911" citStr="Belkin and Niyogi, 2004" startWordPosition="1410" endWordPosition="1413">stering algorithm. (It will figure out that banana, pear, raspberry and redberry belong to the same category and chocolate belongs to another category, since it is mostly 674 Patterns food item1 (or|or rather|instead of|“(”) food item2 Example {apple: pineapple, pear, fruit, strawberry, kiwi} {steak: schnitzel, sausage, roast, meat loaf, cutlet} Table 3: Domain-independent patterns for building the similarity graph. Figure 1: Illustration of the similarity graph. linked to many other food items not being fruits.) 3.1.2 Semi-Supervised Graph Optimization Our semi-supervised graph optimization (Belkin and Niyogi, 2004) is a robust algorithm that was primarily chosen since it only contains few free parameters to adjust. It is based on two principles: First, similar data points should be assigned similar labels, as expressed by a similarity graph of labeled and unlabeled data. Second, for labeled data points the prediction of the learnt classifier should be consistent with the (actual) gold labels. We construct a weighted transition matrix W of the graph by normalization of the matrix with co-occurrence counts C which we obtain from the similarity graph (§3.1.1). We use the common normalization by a power of </context>
</contexts>
<marker>Belkin, Niyogi, 2004</marker>
<rawString>Mikhail Belkin and Partha Niyogi. 2004. Semisupervised learning on Riemannian manifolds. Machine Learning, 56(1-3):209–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Mark Dredze</author>
<author>Benjamin Van Durme</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Broadly Improving User Classification via Communication-Based Name and Location Clustering on Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT/NAACL),</booktitle>
<pages>1010--1019</pages>
<location>Atlanta, GA, USA.</location>
<marker>Bergsma, Dredze, Van Durme, Wilson, Yarowsky, 2013</marker>
<rawString>Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, and David Yarowsky. 2013. Broadly Improving User Classification via Communication-Based Name and Location Clustering on Twitter. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT/NAACL), pages 1010–1019, Atlanta, GA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="28728" citStr="Brown et al., 1992" startWordPosition="4654" endWordPosition="4657">ccurrence is co-incidental. On a subset of 200 sentences, we measured a substantial interannotation agreement of Cohen’s κ = 0.67 (Landis and Koch, 1977). We train a supervised classifier and incorporate the knowledge induced from our domain-specific corpus as features. We chose Support Vector Machines with 5-fold cross-validation using SVMlightmulti-class (Joachims, 1999). Table 13 displays all features that we examine for supervised classification. Most features are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, </context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>120--126</pages>
<location>College Park, MD, USA.</location>
<contexts>
<context position="34442" citStr="Caraballo, 1999" startWordPosition="5551" endWordPosition="5552">aph. The traditional features (i.e. pos, synt and brown) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. 5 Related Work While many of the previous works on noun categorization also address the task of hypernym classification (Hearst, 1992; Caraballo, 1999; Widdows, 2003; Kozareva et al., 2008; Huang and Riloff, 2010; Lenci and Benotto, 2012) and some include examples involving food items (Widdows and Dorow, 2002; Cederberg and Widdows, 2003), only van Hage et al. (2005) and van Hage et al. (2006) specifically focus on the classification of food items. van Hage et al. (2005) deal with ontology mapping whereas van Hage et al. (2006) explore part-whole relations. Features Acc Prec Rec F1 germanet 45.3 41.3 37.2 37.3 graph 46.0 39.4 39.7 38.6 patt 59.8 49.8 41.1 38.7 word 60.1 56.9 54.5 55.1 word+patt 60.3 57.3 54.9 55.5 word+brown 59.5 56.1 54.6 </context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 120–126, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Cederberg</author>
<author>Dominic Widdows</author>
</authors>
<title>Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>111--118</pages>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="34632" citStr="Cederberg and Widdows, 2003" startWordPosition="5578" endWordPosition="5581">is feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. 5 Related Work While many of the previous works on noun categorization also address the task of hypernym classification (Hearst, 1992; Caraballo, 1999; Widdows, 2003; Kozareva et al., 2008; Huang and Riloff, 2010; Lenci and Benotto, 2012) and some include examples involving food items (Widdows and Dorow, 2002; Cederberg and Widdows, 2003), only van Hage et al. (2005) and van Hage et al. (2006) specifically focus on the classification of food items. van Hage et al. (2005) deal with ontology mapping whereas van Hage et al. (2006) explore part-whole relations. Features Acc Prec Rec F1 germanet 45.3 41.3 37.2 37.3 graph 46.0 39.4 39.7 38.6 patt 59.8 49.8 41.1 38.7 word 60.1 56.9 54.5 55.1 word+patt 60.3 57.3 54.9 55.5 word+brown 59.5 56.1 54.6 54.9 word+synt 60.3 57.7 55.4 56.0 word+pos 59.8 56.6 54.6 55.1 word+germanet 61.3 58.6 56.0 56.7 word+graph 62.9 59.2 57.6 58.1° word+patt+brown+synt+pos 60.4 57.3 56.2 56.5 word+patt+brown</context>
</contexts>
<marker>Cederberg, Widdows, 2003</marker>
<rawString>Scott Cederberg and Dominic Widdows. 2003. Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL), pages 111–118, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>GraphBased Lexicon Expansion with Sparsity-Inducing Penalties.</title>
<date>2012</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT/NAACL),</booktitle>
<pages>677--687</pages>
<location>Montr´eal, Quebec, Canada.</location>
<contexts>
<context position="35752" citStr="Das and Smith, 2012" startWordPosition="5751" endWordPosition="5754">.0 56.7 word+graph 62.9 59.2 57.6 58.1° word+patt+brown+synt+pos 60.4 57.3 56.2 56.5 word+patt+brown+synt+pos+conj 61.7 59.0 57.8 58.2* word+patt+brown+synt+pos+conj+germanet 63.1 60.2 58.6 59.1° word+patt+brown+synt+pos+conj+graph 64.7 62.1 60.3 60.9°† statistical significance testing (paired t-test): better than word * at p &lt; 0.1/ ° at p &lt; 0.05; † better than word+patt+brown+synt+pos+conj at p &lt; 0.05 Table 14: Comparison of various features (Table 13) for (unrestricted) relation extraction. The task of data-driven lexicon expansion has also been explored before (Kanayama and Nasukawa, 2006; Das and Smith, 2012), however, our paper presents the first attempt to carry out a comprehensive categorization for the food domain. For the first time, we also show that type information can effectively improve the extraction of very common relations. For the twitter domain, the usage of type information based on clustering has already been found effective for supervised learning (Bergsma et al., 2013). 6 Conclusion We presented an induction method to assign semantic information to food items. We considered two types of categorizations being food-type information and information about whether a food item is comp</context>
</contexts>
<marker>Das, Smith, 2012</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2012. GraphBased Lexicon Expansion with Sparsity-Inducing Penalties. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT/NAACL), pages 677– 687, Montr´eal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>GermaNet -a Lexical-Semantic Net for German.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL workshop Automatic Information Extraction and Building ofLexical Semantic Resourcesfor NLP Applications,</booktitle>
<pages>9--15</pages>
<location>Madrid,</location>
<contexts>
<context position="3580" citStr="Hamp and Feldweg, 1997" startWordPosition="545" endWordPosition="548">gs should carry over to other languages since the issues we address are (mostly) language universal. For general accessibility, all examples are given as English translations. 2 Data &amp; Annotation 2.1 Domain-Specific Text Corpus In order to generate a dataset for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012b) consisting of 418,558 webpages of foodrelated forum entries. chefkoch.de is the largest German web portal for food-related issues. 2.2 Food Categorization As a food vocabulary, we employ a list of 1888 food items: 1104 items were directly extracted from GermaNet (Hamp and Feldweg, 1997), the German version of WordNet (Miller et al., 1990). The items were identified by extracting all hyponyms of the synset lahrung (English: food). By 1www.chefkoch.de 673 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673–682, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Class Description Size Perc. MEAT meat and fish (products) 394 20.87 BEVERAGE beverages (incl. alcoholic drinks) 298 15.78 VEGE vegetables (incl. salads) 231 12.24 SWEET sweets, pastries and snack mixes 228 12.08 SPICE</context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. GermaNet -a Lexical-Semantic Net for German. In Proceedings ofACL workshop Automatic Information Extraction and Building ofLexical Semantic Resourcesfor NLP Applications, pages 9–15, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the International Conference on Com-</booktitle>
<contexts>
<context position="11892" citStr="Hearst (1992)" startWordPosition="1949" endWordPosition="1950">for this work, we set the two free parameters A = 0.5 and p = 0.01. This setting is used for both induction tasks and all configurations. It is a setting that provided reasonable results without any notable bias for any particular configuration we examine. 3.1.3 Manually vs. Automatically Extracted Seeds We explore two types of seed initializations: (a) a manually compiled list of seed food items and (b) a small set of patterns (Table 4) by the help of which such seeds are automatically extracted. In order to extract seeds for Task I with the pattern-based approach, we apply the patterns from Hearst (1992). These patterns have been designed for the acquisition of hyponyms. Task I can also be regarded as some type of hyponym extraction. The food types (fruit, meat, sweets) represent the hypernyms for which we extract seed hyponyms (banana, beef, chocolate). In order to extract seeds for Task II, we apply two domain-specific sets of patterns (pattdish and pattatom). We rank the food items according to the frequency of occurring with the respective pattern set. Since food items may occur in both rankings, we merge the two rankings in the following way: score(food item) = #pattdish(food it.) − #pat</context>
<context position="34425" citStr="Hearst, 1992" startWordPosition="5549" endWordPosition="5550">orm word is graph. The traditional features (i.e. pos, synt and brown) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. 5 Related Work While many of the previous works on noun categorization also address the task of hypernym classification (Hearst, 1992; Caraballo, 1999; Widdows, 2003; Kozareva et al., 2008; Huang and Riloff, 2010; Lenci and Benotto, 2012) and some include examples involving food items (Widdows and Dorow, 2002; Cederberg and Widdows, 2003), only van Hage et al. (2005) and van Hage et al. (2006) specifically focus on the classification of food items. van Hage et al. (2005) deal with ontology mapping whereas van Hage et al. (2006) explore part-whole relations. Features Acc Prec Rec F1 germanet 45.3 41.3 37.2 37.3 graph 46.0 39.4 39.7 38.6 patt 59.8 49.8 41.1 38.7 word 60.1 56.9 54.5 55.1 word+patt 60.3 57.3 54.9 55.5 word+brow</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Proceedings of the International Conference on Com-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Inducing Domain-specific Semantic Class Taggers from (almost) Nothing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>275--285</pages>
<location>Uppsala,</location>
<contexts>
<context position="21698" citStr="Huang and Riloff, 2010" startWordPosition="3578" endWordPosition="3581">chosen), it is not possible to enforce such diversity with the automatically extracted seeds. However, most food items are correct. Table 9 displays the 10 most highly ranked dishes and atomic food items extracted with pattdish and pattatom (Table 4). Unlike the previous task (Table 8), we obtain more heterogeneous seeds within the same class. 4.1.4 Distributional Similarity Since many recent methods for related tasks, such as noun classification, are based on so-called distributional similarity (Riloff and Shepherd, 1997; Lin, 1998; Snow et al., 2004; Weeds et al., 2004; Yamada et al., 2009; Huang and Riloff, 2010; Lenci and Benotto, 2012), we also examine this as an alternative representation to the pattern-based similarity graph (Table 3). We represent each food item as a vector which itself is an aggregate of the contexts of all mentions of a particular food item. We weighted the individual (context) words co-occurring with the food item at a fixed window size of 5 words with tf-idf. We can now apply graph-based optimization on the similarity matrix encoding the cosine similarities between any possible pair of vectors representing two food items. As seeds, we use the best configuration (not employin</context>
<context position="34504" citStr="Huang and Riloff, 2010" startWordPosition="5559" endWordPosition="5562">) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. 5 Related Work While many of the previous works on noun categorization also address the task of hypernym classification (Hearst, 1992; Caraballo, 1999; Widdows, 2003; Kozareva et al., 2008; Huang and Riloff, 2010; Lenci and Benotto, 2012) and some include examples involving food items (Widdows and Dorow, 2002; Cederberg and Widdows, 2003), only van Hage et al. (2005) and van Hage et al. (2006) specifically focus on the classification of food items. van Hage et al. (2005) deal with ontology mapping whereas van Hage et al. (2006) explore part-whole relations. Features Acc Prec Rec F1 germanet 45.3 41.3 37.2 37.3 graph 46.0 39.4 39.7 38.6 patt 59.8 49.8 41.1 38.7 word 60.1 56.9 54.5 55.1 word+patt 60.3 57.3 54.9 55.5 word+brown 59.5 56.1 54.6 54.9 word+synt 60.3 57.7 55.4 56.0 word+pos 59.8 56.6 54.6 55.</context>
</contexts>
<marker>Huang, Riloff, 2010</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2010. Inducing Domain-specific Semantic Class Taggers from (almost) Nothing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 275–285, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>169--184</pages>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="28484" citStr="Joachims, 1999" startWordPosition="4620" endWordPosition="4621">ccur. Each food pair mention was manually assigned one label. In addition to the three relation types from above, we introduce the label Other for cases in which either another relation between the target food items is expressed or the co-occurrence is co-incidental. On a subset of 200 sentences, we measured a substantial interannotation agreement of Cohen’s κ = 0.67 (Landis and Koch, 1977). We train a supervised classifier and incorporate the knowledge induced from our domain-specific corpus as features. We chose Support Vector Machines with 5-fold cross-validation using SVMlightmulti-class (Joachims, 1999). Table 13 displays all features that we examine for supervised classification. Most features are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown t</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making Large-Scale SVM Learning Practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 169–184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Tetsuya Nasukawa</author>
</authors>
<title>Fully Automatic Lexicon Expansion for Domainoriented Sentiment Analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>355--363</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="35730" citStr="Kanayama and Nasukawa, 2006" startWordPosition="5746" endWordPosition="5750">.1 word+germanet 61.3 58.6 56.0 56.7 word+graph 62.9 59.2 57.6 58.1° word+patt+brown+synt+pos 60.4 57.3 56.2 56.5 word+patt+brown+synt+pos+conj 61.7 59.0 57.8 58.2* word+patt+brown+synt+pos+conj+germanet 63.1 60.2 58.6 59.1° word+patt+brown+synt+pos+conj+graph 64.7 62.1 60.3 60.9°† statistical significance testing (paired t-test): better than word * at p &lt; 0.1/ ° at p &lt; 0.05; † better than word+patt+brown+synt+pos+conj at p &lt; 0.05 Table 14: Comparison of various features (Table 13) for (unrestricted) relation extraction. The task of data-driven lexicon expansion has also been explored before (Kanayama and Nasukawa, 2006; Das and Smith, 2012), however, our paper presents the first attempt to carry out a comprehensive categorization for the food domain. For the first time, we also show that type information can effectively improve the extraction of very common relations. For the twitter domain, the usage of type information based on clustering has already been found effective for supervised learning (Bergsma et al., 2013). 6 Conclusion We presented an induction method to assign semantic information to food items. We considered two types of categorizations being food-type information and information about wheth</context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully Automatic Lexicon Expansion for Domainoriented Sentiment Analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 355–363, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1048--1056</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="34480" citStr="Kozareva et al., 2008" startWordPosition="5555" endWordPosition="5558">.e. pos, synt and brown) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. 5 Related Work While many of the previous works on noun categorization also address the task of hypernym classification (Hearst, 1992; Caraballo, 1999; Widdows, 2003; Kozareva et al., 2008; Huang and Riloff, 2010; Lenci and Benotto, 2012) and some include examples involving food items (Widdows and Dorow, 2002; Cederberg and Widdows, 2003), only van Hage et al. (2005) and van Hage et al. (2006) specifically focus on the classification of food items. van Hage et al. (2005) deal with ontology mapping whereas van Hage et al. (2006) explore part-whole relations. Features Acc Prec Rec F1 germanet 45.3 41.3 37.2 37.3 graph 46.0 39.4 39.7 38.6 patt 59.8 49.8 41.1 38.7 word 60.1 56.9 54.5 55.1 word+patt 60.3 57.3 54.9 55.5 word+brown 59.5 56.1 54.6 54.9 word+synt 60.3 57.7 55.4 56.0 wor</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1048– 1056, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="28262" citStr="Landis and Koch, 1977" startWordPosition="4587" endWordPosition="4591">of stock but item B can be offered as an alternative. Therefore, insights from this work should carry over to other domains. We randomly extracted 1500 sentences from our text corpus (§2.1) in which (at least) two food items co-occur. Each food pair mention was manually assigned one label. In addition to the three relation types from above, we introduce the label Other for cases in which either another relation between the target food items is expressed or the co-occurrence is co-incidental. On a subset of 200 sentences, we measured a substantial interannotation agreement of Cohen’s κ = 0.67 (Landis and Koch, 1977). We train a supervised classifier and incorporate the knowledge induced from our domain-specific corpus as features. We chose Support Vector Machines with 5-fold cross-validation using SVMlightmulti-class (Joachims, 1999). Table 13 displays all features that we examine for supervised classification. Most features are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervis</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Guilia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>75--79</pages>
<location>Montr´eal, Quebec, Canada.</location>
<contexts>
<context position="21724" citStr="Lenci and Benotto, 2012" startWordPosition="3582" endWordPosition="3585">ble to enforce such diversity with the automatically extracted seeds. However, most food items are correct. Table 9 displays the 10 most highly ranked dishes and atomic food items extracted with pattdish and pattatom (Table 4). Unlike the previous task (Table 8), we obtain more heterogeneous seeds within the same class. 4.1.4 Distributional Similarity Since many recent methods for related tasks, such as noun classification, are based on so-called distributional similarity (Riloff and Shepherd, 1997; Lin, 1998; Snow et al., 2004; Weeds et al., 2004; Yamada et al., 2009; Huang and Riloff, 2010; Lenci and Benotto, 2012), we also examine this as an alternative representation to the pattern-based similarity graph (Table 3). We represent each food item as a vector which itself is an aggregate of the contexts of all mentions of a particular food item. We weighted the individual (context) words co-occurring with the food item at a fixed window size of 5 words with tf-idf. We can now apply graph-based optimization on the similarity matrix encoding the cosine similarities between any possible pair of vectors representing two food items. As seeds, we use the best configuration (not employing GermaNet), i.e. 10-PROTO</context>
<context position="34530" citStr="Lenci and Benotto, 2012" startWordPosition="5563" endWordPosition="5566"> improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. 5 Related Work While many of the previous works on noun categorization also address the task of hypernym classification (Hearst, 1992; Caraballo, 1999; Widdows, 2003; Kozareva et al., 2008; Huang and Riloff, 2010; Lenci and Benotto, 2012) and some include examples involving food items (Widdows and Dorow, 2002; Cederberg and Widdows, 2003), only van Hage et al. (2005) and van Hage et al. (2006) specifically focus on the classification of food items. van Hage et al. (2005) deal with ontology mapping whereas van Hage et al. (2006) explore part-whole relations. Features Acc Prec Rec F1 germanet 45.3 41.3 37.2 37.3 graph 46.0 39.4 39.7 38.6 patt 59.8 49.8 41.1 38.7 word 60.1 56.9 54.5 55.1 word+patt 60.3 57.3 54.9 55.5 word+brown 59.5 56.1 54.6 54.9 word+synt 60.3 57.7 55.4 56.0 word+pos 59.8 56.6 54.6 55.1 word+germanet 61.3 58.6 </context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Guilia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of the Joint Conference on Lexical and Computational Semantics (*SEM), pages 75– 79, Montr´eal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics (ACL/COLING),</booktitle>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="21614" citStr="Lin, 1998" startWordPosition="3564" endWordPosition="3565">icular class (e.g. for STARCH, some type of pasta, rice and potato was chosen), it is not possible to enforce such diversity with the automatically extracted seeds. However, most food items are correct. Table 9 displays the 10 most highly ranked dishes and atomic food items extracted with pattdish and pattatom (Table 4). Unlike the previous task (Table 8), we obtain more heterogeneous seeds within the same class. 4.1.4 Distributional Similarity Since many recent methods for related tasks, such as noun classification, are based on so-called distributional similarity (Riloff and Shepherd, 1997; Lin, 1998; Snow et al., 2004; Weeds et al., 2004; Yamada et al., 2009; Huang and Riloff, 2010; Lenci and Benotto, 2012), we also examine this as an alternative representation to the pattern-based similarity graph (Table 3). We represent each food item as a vector which itself is an aggregate of the contexts of all mentions of a particular food item. We weighted the individual (context) words co-occurring with the food item at a fixed window size of 5 words with tf-idf. We can now apply graph-based optimization on the similarity matrix encoding the cosine similarities between any possible pair of vector</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics (ACL/COLING), pages 768–774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--235</pages>
<contexts>
<context position="3633" citStr="Miller et al., 1990" startWordPosition="554" endWordPosition="557"> we address are (mostly) language universal. For general accessibility, all examples are given as English translations. 2 Data &amp; Annotation 2.1 Domain-Specific Text Corpus In order to generate a dataset for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012b) consisting of 418,558 webpages of foodrelated forum entries. chefkoch.de is the largest German web portal for food-related issues. 2.2 Food Categorization As a food vocabulary, we employ a list of 1888 food items: 1104 items were directly extracted from GermaNet (Hamp and Feldweg, 1997), the German version of WordNet (Miller et al., 1990). The items were identified by extracting all hyponyms of the synset lahrung (English: food). By 1www.chefkoch.de 673 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673–682, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Class Description Size Perc. MEAT meat and fish (products) 394 20.87 BEVERAGE beverages (incl. alcoholic drinks) 298 15.78 VEGE vegetables (incl. salads) 231 12.24 SWEET sweets, pastries and snack mixes 228 12.08 SPICE spices and sauces 216 11.44 STARCH starch-based side</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Introduction to WordNet: An On-line Lexical Database. International Journal of Lexicography, 3:235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding Semantic Similarity in Tree Kernels for Domain Adapation of Relation Extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1498--1507</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="29198" citStr="Plank and Moschitti, 2013" startWordPosition="4728" endWordPosition="4731">tures are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, 2002) was used. Following Turian et al. (2010), we induced 1000 clusters (from our domain-specific corpus §2.1). 4.2.1 Why should food categories be helpful for relation extraction? All relation types we consider comprise pairs of two food items which makes these relation types likely to be confused. Contextual information may be used for disambiguation but there may also be frequent contexts that are not sufficiently informative. For example, 25% of the instances o</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding Semantic Similarity in Tree Kernels for Domain Adapation of Relation Extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1498– 1507, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL Workshop on Parsing German (PaGe),</booktitle>
<pages>40--46</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="29267" citStr="Rafferty and Manning, 2008" startWordPosition="4737" endWordPosition="4740">ature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, 2002) was used. Following Turian et al. (2010), we induced 1000 clusters (from our domain-specific corpus §2.1). 4.2.1 Why should food categories be helpful for relation extraction? All relation types we consider comprise pairs of two food items which makes these relation types likely to be confused. Contextual information may be used for disambiguation but there may also be frequent contexts that are not sufficiently informative. For example, 25% of the instances of IngrePLAIN +POSTP Task Corpus graph Acc F1 Acc F1 Wikipedia ✓ 40.3 </context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>Anna Rafferty and Christopher D. Manning. 2008. Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines. In Proceedings of the ACL Workshop on Parsing German (PaGe), pages 40–46, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jessica Shepherd</author>
</authors>
<title>A Corpus-Based Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>117--124</pages>
<location>Providence, RI, USA.</location>
<contexts>
<context position="21603" citStr="Riloff and Shepherd, 1997" startWordPosition="3560" endWordPosition="3563">food items within each particular class (e.g. for STARCH, some type of pasta, rice and potato was chosen), it is not possible to enforce such diversity with the automatically extracted seeds. However, most food items are correct. Table 9 displays the 10 most highly ranked dishes and atomic food items extracted with pattdish and pattatom (Table 4). Unlike the previous task (Table 8), we obtain more heterogeneous seeds within the same class. 4.1.4 Distributional Similarity Since many recent methods for related tasks, such as noun classification, are based on so-called distributional similarity (Riloff and Shepherd, 1997; Lin, 1998; Snow et al., 2004; Weeds et al., 2004; Yamada et al., 2009; Huang and Riloff, 2010; Lenci and Benotto, 2012), we also examine this as an alternative representation to the pattern-based similarity graph (Table 3). We represent each food item as a vector which itself is an aggregate of the contexts of all mentions of a particular food item. We weighted the individual (context) words co-occurring with the food item at a fixed window size of 5 words with tf-idf. We can now apply graph-based optimization on the similarity matrix encoding the cosine similarities between any possible pai</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Ellen Riloff and Jessica Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 117–124, Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning Syntactic Patterns for Automatic Hypernym Discovery.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="21633" citStr="Snow et al., 2004" startWordPosition="3566" endWordPosition="3569">s (e.g. for STARCH, some type of pasta, rice and potato was chosen), it is not possible to enforce such diversity with the automatically extracted seeds. However, most food items are correct. Table 9 displays the 10 most highly ranked dishes and atomic food items extracted with pattdish and pattatom (Table 4). Unlike the previous task (Table 8), we obtain more heterogeneous seeds within the same class. 4.1.4 Distributional Similarity Since many recent methods for related tasks, such as noun classification, are based on so-called distributional similarity (Riloff and Shepherd, 1997; Lin, 1998; Snow et al., 2004; Weeds et al., 2004; Yamada et al., 2009; Huang and Riloff, 2010; Lenci and Benotto, 2012), we also examine this as an alternative representation to the pattern-based similarity graph (Table 3). We represent each food item as a vector which itself is an aggregate of the contexts of all mentions of a particular food item. We weighted the individual (context) words co-occurring with the food item at a fixed window size of 5 words with tf-idf. We can now apply graph-based optimization on the similarity matrix encoding the cosine similarities between any possible pair of vectors representing two </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004. Learning Syntactic Patterns for Automatic Hypernym Discovery. In Advances in Neural Information Processing Systems (NIPS), Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>901--904</pages>
<location>Denver, CO, USA.</location>
<contexts>
<context position="29333" citStr="Stolcke, 2002" startWordPosition="4749" endWordPosition="4750">l., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, 2002) was used. Following Turian et al. (2010), we induced 1000 clusters (from our domain-specific corpus §2.1). 4.2.1 Why should food categories be helpful for relation extraction? All relation types we consider comprise pairs of two food items which makes these relation types likely to be confused. Contextual information may be used for disambiguation but there may also be frequent contexts that are not sufficiently informative. For example, 25% of the instances of IngrePLAIN +POSTP Task Corpus graph Acc F1 Acc F1 Wikipedia ✓ 40.3 49.4 61.4 59.8 Food Type chefkoch.de ✓ 65.8 71.0 80.2 77.7 Wikiped</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 901–904, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>384--394</pages>
<location>Uppsala,</location>
<contexts>
<context position="29146" citStr="Turian et al., 2010" startWordPosition="4720" endWordPosition="4723">xamine for supervised classification. Most features are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, 2002) was used. Following Turian et al. (2010), we induced 1000 clusters (from our domain-specific corpus §2.1). 4.2.1 Why should food categories be helpful for relation extraction? All relation types we consider comprise pairs of two food items which makes these relation types likely to be confused. Contextual information may be used for disambiguation but there may also be frequent contexts that are not sufficien</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-supervised Learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 384–394, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<title>Human Nutrition Information Service U.S. Department of Agriculture.</title>
<date>1992</date>
<booktitle>The Food Guide Pyramid. Home and Garden Bulletin 252,</booktitle>
<location>Washington, D.C., USA.</location>
<contexts>
<context position="11892" citStr="(1992)" startWordPosition="1950" endWordPosition="1950">s work, we set the two free parameters A = 0.5 and p = 0.01. This setting is used for both induction tasks and all configurations. It is a setting that provided reasonable results without any notable bias for any particular configuration we examine. 3.1.3 Manually vs. Automatically Extracted Seeds We explore two types of seed initializations: (a) a manually compiled list of seed food items and (b) a small set of patterns (Table 4) by the help of which such seeds are automatically extracted. In order to extract seeds for Task I with the pattern-based approach, we apply the patterns from Hearst (1992). These patterns have been designed for the acquisition of hyponyms. Task I can also be regarded as some type of hyponym extraction. The food types (fruit, meat, sweets) represent the hypernyms for which we extract seed hyponyms (banana, beef, chocolate). In order to extract seeds for Task II, we apply two domain-specific sets of patterns (pattdish and pattatom). We rank the food items according to the frequency of occurring with the respective pattern set. Since food items may occur in both rankings, we merge the two rankings in the following way: score(food item) = #pattdish(food it.) − #pat</context>
</contexts>
<marker>1992</marker>
<rawString>Human Nutrition Information Service U.S. Department of Agriculture. 1992. The Food Guide Pyramid. Home and Garden Bulletin 252, Washington, D.C., USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Robert van Hage</author>
<author>Sophia Katrenko</author>
<author>Guus Schreiber</author>
</authors>
<title>A Method to Combine Linguistic Ontology-Mapping Techniques.</title>
<date>2005</date>
<booktitle>In Proceedings of International Semantic Web Conference (ISWC), pages 732 – 744,</booktitle>
<publisher>Springer.</publisher>
<location>Galway, Ireland.</location>
<marker>van Hage, Katrenko, Schreiber, 2005</marker>
<rawString>Willem Robert van Hage, Sophia Katrenko, and Guus Schreiber. 2005. A Method to Combine Linguistic Ontology-Mapping Techniques. In Proceedings of International Semantic Web Conference (ISWC), pages 732 – 744, Galway, Ireland. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Robert van Hage</author>
<author>Hap Kolb</author>
<author>Guus Schreiber</author>
</authors>
<title>A Method for Learning PartWhole Relations.</title>
<date>2006</date>
<booktitle>In Proceedings of International Semantic Web Conference (ISWC), pages 723 – 735,</booktitle>
<publisher>Springer.</publisher>
<location>Athens, GA, USA.</location>
<marker>van Hage, Kolb, Schreiber, 2006</marker>
<rawString>Willem Robert van Hage, Hap Kolb, and Guus Schreiber. 2006. A Method for Learning PartWhole Relations. In Proceedings of International Semantic Web Conference (ISWC), pages 723 – 735, Athens, GA, USA. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike von Luxburg</author>
</authors>
<date>2007</date>
<booktitle>A Tutorial on Spectral Clustering. Statistics and Computing,</booktitle>
<pages>17--395</pages>
<marker>von Luxburg, 2007</marker>
<rawString>Ulrike von Luxburg. 2007. A Tutorial on Spectral Clustering. Statistics and Computing, 17:395–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising Measures of Lexical Distributional Similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1015--1021</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="21653" citStr="Weeds et al., 2004" startWordPosition="3570" endWordPosition="3573"> some type of pasta, rice and potato was chosen), it is not possible to enforce such diversity with the automatically extracted seeds. However, most food items are correct. Table 9 displays the 10 most highly ranked dishes and atomic food items extracted with pattdish and pattatom (Table 4). Unlike the previous task (Table 8), we obtain more heterogeneous seeds within the same class. 4.1.4 Distributional Similarity Since many recent methods for related tasks, such as noun classification, are based on so-called distributional similarity (Riloff and Shepherd, 1997; Lin, 1998; Snow et al., 2004; Weeds et al., 2004; Yamada et al., 2009; Huang and Riloff, 2010; Lenci and Benotto, 2012), we also examine this as an alternative representation to the pattern-based similarity graph (Table 3). We represent each food item as a vector which itself is an aggregate of the contexts of all mentions of a particular food item. We weighted the individual (context) words co-occurring with the food item at a fixed window size of 5 words with tf-idf. We can now apply graph-based optimization on the similarity matrix encoding the cosine similarities between any possible pair of vectors representing two food items. As seeds</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising Measures of Lexical Distributional Similarity. In Proceedings of the International Conference on Computational Linguistics (COLING), pages 1015–1021, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>A Graph Model for Unsupervised Lexical Acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1093--1099</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="34602" citStr="Widdows and Dorow, 2002" startWordPosition="5574" endWordPosition="5577">When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. 5 Related Work While many of the previous works on noun categorization also address the task of hypernym classification (Hearst, 1992; Caraballo, 1999; Widdows, 2003; Kozareva et al., 2008; Huang and Riloff, 2010; Lenci and Benotto, 2012) and some include examples involving food items (Widdows and Dorow, 2002; Cederberg and Widdows, 2003), only van Hage et al. (2005) and van Hage et al. (2006) specifically focus on the classification of food items. van Hage et al. (2005) deal with ontology mapping whereas van Hage et al. (2006) explore part-whole relations. Features Acc Prec Rec F1 germanet 45.3 41.3 37.2 37.3 graph 46.0 39.4 39.7 38.6 patt 59.8 49.8 41.1 38.7 word 60.1 56.9 54.5 55.1 word+patt 60.3 57.3 54.9 55.5 word+brown 59.5 56.1 54.6 54.9 word+synt 60.3 57.7 55.4 56.0 word+pos 59.8 56.6 54.6 55.1 word+germanet 61.3 58.6 56.0 56.7 word+graph 62.9 59.2 57.6 58.1° word+patt+brown+synt+pos 60.4 </context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Dominic Widdows and Beate Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. In Proceedings of the International Conference on Computational Linguistics (COLING), pages 1093– 1099, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT/NAACL),</booktitle>
<pages>197--204</pages>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="34457" citStr="Widdows, 2003" startWordPosition="5553" endWordPosition="5554">nal features (i.e. pos, synt and brown) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. 5 Related Work While many of the previous works on noun categorization also address the task of hypernym classification (Hearst, 1992; Caraballo, 1999; Widdows, 2003; Kozareva et al., 2008; Huang and Riloff, 2010; Lenci and Benotto, 2012) and some include examples involving food items (Widdows and Dorow, 2002; Cederberg and Widdows, 2003), only van Hage et al. (2005) and van Hage et al. (2006) specifically focus on the classification of food items. van Hage et al. (2005) deal with ontology mapping whereas van Hage et al. (2006) explore part-whole relations. Features Acc Prec Rec F1 germanet 45.3 41.3 37.2 37.3 graph 46.0 39.4 39.7 38.6 patt 59.8 49.8 41.1 38.7 word 60.1 56.9 54.5 55.1 word+patt 60.3 57.3 54.9 55.5 word+brown 59.5 56.1 54.6 54.9 word+synt </context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. 2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT/NAACL), pages 197–204, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
</authors>
<title>Knowledge Acquisition with Natural Language Processing in the Food Domain: Potential and Challenges.</title>
<date>2012</date>
<booktitle>In Proceedings of the ECAIWorkshop on Cooking with Computers (CWC),</booktitle>
<pages>46--51</pages>
<location>Montpellier, France.</location>
<contexts>
<context position="3290" citStr="Wiegand et al., 2012" startWordPosition="500" endWordPosition="503"> domains, such as fashion, cosmetics or home &amp; gardening, show parallels since comparable textual web data are available and similar relation types (e.g. that two items fit together or can be substituted by each other) exist. Our experiments are carried out on German data but our findings should carry over to other languages since the issues we address are (mostly) language universal. For general accessibility, all examples are given as English translations. 2 Data &amp; Annotation 2.1 Domain-Specific Text Corpus In order to generate a dataset for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012b) consisting of 418,558 webpages of foodrelated forum entries. chefkoch.de is the largest German web portal for food-related issues. 2.2 Food Categorization As a food vocabulary, we employ a list of 1888 food items: 1104 items were directly extracted from GermaNet (Hamp and Feldweg, 1997), the German version of WordNet (Miller et al., 1990). The items were identified by extracting all hyponyms of the synset lahrung (English: food). By 1www.chefkoch.de 673 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673–682, Gothenburg, Swe</context>
<context position="27167" citStr="Wiegand et al. (2012" startWordPosition="4400" endWordPosition="4403">consider the German version of Wikipedia since this resource also contains encyclopedic knowledge about food items. Table 11 compares the graph-based induction. As in the previous section, we only consider the best previous configuration. The table clearly shows that our domain-specific text corpus is a more effective resource for our purpose than Wikipedia. 4.2 Evaluation for Relation Extraction We now examine whether automatic food categorization can be harnessed for relation extraction. The task is to detect instances of the relation types SuitsTo, SubstitutedBy and IngredientOf introduced Wiegand et al. (2012b) (repeated in Table 12) and motivated in Wiegand et al. (2012a). These relation types are highly relevant for customer advice/product recommendation. In particular, SuitsTo and SubstitutedBy are fairly domainindependent relation types. Customers want to 678 know which items can be used together (SuitsTo), be it two food items that can be used as a meal or two fashion items that can be worn together. Substitutes are also relevant for situations in which item A is out of stock but item B can be offered as an alternative. Therefore, insights from this work should carry over to other domains. We</context>
<context position="32789" citStr="Wiegand et al. (2012" startWordPosition="5302" endWordPosition="5305">ically consumed together My kids love the simple combination offish fingers 633 42.20 with mashed potatoes. SubstitutedBy similar food items commonly consumed in the same situations We usually buy margarine instead of butter. 336 22.40 IngredientOf ingredient of a particular dish Falafel is made of chickpeas. 246 16.40 Other other relation or co-occurrence of food items are co-incidental On my shopping list, I’ve got bread, cauliflower, ... 285 19.00 Table 12: The different relation types and their respective frequency on our dataset. Features Description patt lexical surface patterns used in Wiegand et al. (2012b) word bag-of-words features: all words within the sentence brown features using Brown clustering: all features from word but words are replaced by induced clusters pos part-of-speech sequence between target food items and tags of the words immediately preceding and following them synt path from syntactic parse tree from first target food item to second target food item conj conjunctive features: patt with brown classes of target food items; pos sequence with brown classes of target food items; synt with brown classes of target food items graph semantic food information induced by graph optim</context>
</contexts>
<marker>Wiegand, Roth, Klakow, 2012</marker>
<rawString>Michael Wiegand, Benjamin Roth, and Dietrich Klakow. 2012a. Knowledge Acquisition with Natural Language Processing in the Food Domain: Potential and Challenges. In Proceedings of the ECAIWorkshop on Cooking with Computers (CWC), pages 46–51, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
</authors>
<title>Web-based Relation Extraction for the Food Domain.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Applications of Natural Language Processing to Information Systems (NLDB),</booktitle>
<pages>222--227</pages>
<publisher>Springer.</publisher>
<location>Groningen, the Netherlands.</location>
<contexts>
<context position="3290" citStr="Wiegand et al., 2012" startWordPosition="500" endWordPosition="503"> domains, such as fashion, cosmetics or home &amp; gardening, show parallels since comparable textual web data are available and similar relation types (e.g. that two items fit together or can be substituted by each other) exist. Our experiments are carried out on German data but our findings should carry over to other languages since the issues we address are (mostly) language universal. For general accessibility, all examples are given as English translations. 2 Data &amp; Annotation 2.1 Domain-Specific Text Corpus In order to generate a dataset for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012b) consisting of 418,558 webpages of foodrelated forum entries. chefkoch.de is the largest German web portal for food-related issues. 2.2 Food Categorization As a food vocabulary, we employ a list of 1888 food items: 1104 items were directly extracted from GermaNet (Hamp and Feldweg, 1997), the German version of WordNet (Miller et al., 1990). The items were identified by extracting all hyponyms of the synset lahrung (English: food). By 1www.chefkoch.de 673 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673–682, Gothenburg, Swe</context>
<context position="27167" citStr="Wiegand et al. (2012" startWordPosition="4400" endWordPosition="4403">consider the German version of Wikipedia since this resource also contains encyclopedic knowledge about food items. Table 11 compares the graph-based induction. As in the previous section, we only consider the best previous configuration. The table clearly shows that our domain-specific text corpus is a more effective resource for our purpose than Wikipedia. 4.2 Evaluation for Relation Extraction We now examine whether automatic food categorization can be harnessed for relation extraction. The task is to detect instances of the relation types SuitsTo, SubstitutedBy and IngredientOf introduced Wiegand et al. (2012b) (repeated in Table 12) and motivated in Wiegand et al. (2012a). These relation types are highly relevant for customer advice/product recommendation. In particular, SuitsTo and SubstitutedBy are fairly domainindependent relation types. Customers want to 678 know which items can be used together (SuitsTo), be it two food items that can be used as a meal or two fashion items that can be worn together. Substitutes are also relevant for situations in which item A is out of stock but item B can be offered as an alternative. Therefore, insights from this work should carry over to other domains. We</context>
<context position="32789" citStr="Wiegand et al. (2012" startWordPosition="5302" endWordPosition="5305">ically consumed together My kids love the simple combination offish fingers 633 42.20 with mashed potatoes. SubstitutedBy similar food items commonly consumed in the same situations We usually buy margarine instead of butter. 336 22.40 IngredientOf ingredient of a particular dish Falafel is made of chickpeas. 246 16.40 Other other relation or co-occurrence of food items are co-incidental On my shopping list, I’ve got bread, cauliflower, ... 285 19.00 Table 12: The different relation types and their respective frequency on our dataset. Features Description patt lexical surface patterns used in Wiegand et al. (2012b) word bag-of-words features: all words within the sentence brown features using Brown clustering: all features from word but words are replaced by induced clusters pos part-of-speech sequence between target food items and tags of the words immediately preceding and following them synt path from syntactic parse tree from first target food item to second target food item conj conjunctive features: patt with brown classes of target food items; pos sequence with brown classes of target food items; synt with brown classes of target food items graph semantic food information induced by graph optim</context>
</contexts>
<marker>Wiegand, Roth, Klakow, 2012</marker>
<rawString>Michael Wiegand, Benjamin Roth, and Dietrich Klakow. 2012b. Web-based Relation Extraction for the Food Domain. In Proceedings of the International Conference on Applications of Natural Language Processing to Information Systems (NLDB), pages 222–227, Groningen, the Netherlands. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Benjamin Roth</author>
<author>Eva Lasarcyk</author>
<author>Stephanie K¨oser</author>
<author>Dietrich Klakow</author>
</authors>
<title>A Gold Standard for Relation Extraction in the Food Domain.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>507--514</pages>
<location>Istanbul, Turkey.</location>
<marker>Wiegand, Roth, Lasarcyk, K¨oser, Klakow, 2012</marker>
<rawString>Michael Wiegand, Benjamin Roth, Eva Lasarcyk, Stephanie K¨oser, and Dietrich Klakow. 2012c. A Gold Standard for Relation Extraction in the Food Domain. In Proceedings of the Conference on Language Resources and Evaluation (LREC), pages 507–514, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ichiro Yamada</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Kazama</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Stijn De Saeger</author>
<author>Francis Bond</author>
<author>Asuka Sumida</author>
</authors>
<title>Hypernym Discovery Based on Distributional Similarity and Hierarchical Structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>929--927</pages>
<marker>Yamada, Torisawa, Kazama, Kuroda, Murata, De Saeger, Bond, Sumida, 2009</marker>
<rawString>Ichiro Yamada, Kentaro Torisawa, Jun’ichi Kazama, Kow Kuroda, Masaki Murata, Stijn De Saeger, Francis Bond, and Asuka Sumida. 2009. Hypernym Discovery Based on Distributional Similarity and Hierarchical Structures. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 929–927, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Olivier Bousquet</author>
<author>Thomas Navin Lal</author>
<author>Jason Weston</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>Learning with Local and Global Consistency.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS), Vancouver and</booktitle>
<location>Whistler, British Columbia, Canada.</location>
<marker>Zhou, Bousquet, Lal, Weston, Sch¨olkopf, 2004</marker>
<rawString>Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch¨olkopf. 2004. Learning with Local and Global Consistency. In Advances in Neural Information Processing Systems (NIPS), Vancouver and Whistler, British Columbia, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>