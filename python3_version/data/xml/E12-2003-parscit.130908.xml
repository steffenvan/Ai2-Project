<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005967">
<title confidence="0.984705">
Collaborative Machine Translation Service for Scientific texts
</title>
<author confidence="0.984765">
Patrik Lambert Jean Senellart
</author>
<affiliation confidence="0.985892">
University of Le Mans Systran SA
</affiliation>
<email confidence="0.970279">
patrik.lambert@lium.univ-lemans.fr senellart@systran.fr
</email>
<author confidence="0.964739">
Laurent Romary Holger Schwenk
</author>
<affiliation confidence="0.840854">
Humboldt Universit¨at Berlin / University of Le Mans
</affiliation>
<email confidence="0.822162">
INRIA Saclay - Ile de France holger.schwenk@lium.univ-lemans.fr
laurent.romary@inria.fr
</email>
<author confidence="0.838754">
Florian Zipser Patrice Lopez Fr´ed´eric Blain
</author>
<bodyText confidence="0.2531325">
Humboldt Universit¨at Berlin Humboldt Universit¨at Berlin / Systran SA /
f.zipser@gmx.de INRIA Saclay - Ile de France University of Le Mans
patrice.lopez@inria.fr frederic.blain@
lium.univ-lemans.fr
</bodyText>
<sectionHeader confidence="0.978915" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986345">
French researchers are required to fre-
quently translate into French the descrip-
tion of their work published in English. At
the same time, the need for French people
to access articles in English, or to interna-
tional researchers to access theses or pa-
pers in French, is incorrectly resolved via
the use of generic translation tools. We
propose the demonstration of an end-to-end
tool integrated in the HAL open archive for
enabling efficient translation for scientific
texts. This tool can give translation sugges-
tions adapted to the scientific domain, im-
proving by more than 10 points the BLEU
score of a generic system. It also provides
a post-edition service which captures user
post-editing data that can be used to incre-
mentally improve the translations engines.
Thus it is helpful for users which need to
translate or to access scientific texts.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985210526316">
Due to the globalisation of research, the English
language is today the universal language of sci-
entific communication. In France, regulations re-
quire the use of the French language in progress
reports, academic dissertations, manuscripts, and
French is the official educational language of the
country. This situation forces researchers to fre-
quently translate their own articles, lectures, pre-
sentations, reports, and abstracts between English
and French. In addition, students and the general
public are also challenged by language, when it
comes to find published articles in English or to
understand these articles. Finally, international
scientists not even consider to look for French
publications (for instance PhD theses) because
they are not available in their native languages.
This problem, incorrectly resolved through the
use of generic translation tools, actually reveals
an interesting generic problem where a commu-
nity of specialists are regularly performing trans-
lations tasks on a very limited domain. At the
same time, other communities of users seek trans-
lations for the same type of documents. Without
appropriate tools, the expertise and time spent for
translation activity by the first community is lost
and do not benefit to translation requests of the
other communities.
We propose the demonstration of an end-to-end
tool for enabling efficient translation for scientific
texts. This system, developed for the COSMAT
ANR project,1 is closely integrated into the HAL
open archive,2 a multidisciplinary open-access
archive which was created in 2006 to archive pub-
lications from all the French scientific commu-
nity. The tool deals with handling of source doc-
ument format, generally a pdf file, specialised
translation of the content, and user-friendly user-
interface allowing to post-edit the output. Behind
</bodyText>
<footnote confidence="0.9996405">
1http://www.cosmat.fr/
2http://hal.archives-ouvertes.fr/?langue=en
</footnote>
<page confidence="0.992566">
11
</page>
<bodyText confidence="0.966390043478261">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 11–15,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
the scene, the post-editing tool captures user post-
editing data which are used to incrementally im-
prove the translations engines. The only equip-
ment required by this demonstration is a computer
with an Internet browser installed and an Internet
connection.
In this paper, we first describe the complete
work-flow from data acquisition to final post-
editing. Then we focus on the text extraction pro-
cedure. In Section 4, we give details about the
translation system. Then in section 5, we present
the translation and post-editing interface. We fi-
nally give some concluding remarks.
The system will be demonstrated at EACL in
his tight integration with the HAL paper deposit
system. If the organizers agree, we would like to
offer the use of our system during the EACL con-
ference. It would automatically translate all the
abstracts of the accepted papers and also offers
the possibility to correct the outputs. This result-
ing data would be made freely available.
</bodyText>
<sectionHeader confidence="0.93075" genericHeader="method">
2 Complete Processing Work-flow
</sectionHeader>
<bodyText confidence="0.999755142857143">
The entry point for the system are “ready to pub-
lish” scientific papers. The goal of our system
was to extract content keeping as many meta-
information as possible from the document, to
translate the content, to allow the user to perform
post-editing, and to render the result in a format as
close as possible to the source format. To train our
system, we collected from the HAL archive more
than 40 000 documents in physics and computer
science, including articles, PhD theses or research
reports (see Section 4). This material was used to
train the translation engines and to extract domain
bilingual terminology.
The user scenario is the following:
</bodyText>
<listItem confidence="0.95597725">
• A user uploads an article in PDF format3 on
the system.
• The document is processed by the open-
source Grobid tool (see section 3) to extract
</listItem>
<bodyText confidence="0.987333692307692">
3The commonly used publishing format is PDF files
while authoring format is principally a mix of Microsoft
Word file and LaTeX documents using a variety of styles.
The originality of our approach is to work on the PDF file
and not on these source formats. The rationale being that 1/
the source format is almost never available, 2/ even if we had
access to the source format, we would need to implement a
filter specific to each individual template required by such or
such conference for a good quality content extraction
the content. The extracted paper is structured
in the TEI format where title, authors, refer-
ences, footnotes, figure captions are identi-
fied with a very high accuracy.
</bodyText>
<listItem confidence="0.998429814814815">
• An entity recognition process is performed
for markup of domain entities such as:
chemical compounds for chemical papers,
mathematical formulas, pseudo-code and ob-
ject references in computer science papers,
but also miscellaneous acronyms commonly
used in scientific communication.
• Specialised terminology is then recognised
using the Termsciences4 reference termi-
nology database, completed with terminol-
ogy automatically extracted from the train-
ing corpus. The actual translation of the pa-
per is performed using adapted translation as
described in Section 4.
• The translation process generates a bilingual
TEI format preserving the source structure
and integrating the entity annotation, multi-
ple terminology choices when available, and
the token alignment between source and tar-
get sentences.
• The translation is proposed to the user for
post-editing through a rich interactive inter-
face described in Section 5.
• The final version of the document is then
archived in TEI format and available for dis-
play in HTML using dedicated XSLT style
sheets.
</listItem>
<sectionHeader confidence="0.951425" genericHeader="method">
3 The Grobid System
</sectionHeader>
<bodyText confidence="0.999684416666667">
Based on state-of-the-art machine learning tech-
niques, Grobid (Lopez, 2009) performs reliable
bibliographic data extraction from scholar articles
combined with multi-level term extraction. These
two types of extraction present synergies and cor-
respond to complementary descriptions of an arti-
cle.
This tool parses and converts scientific arti-
cles in PDF format into a structured TEI docu-
ment5 compliant with the good practices devel-
oped within the European PEER project (Bretel et
al., 2010). Grobid is trained on a set of annotated
</bodyText>
<footnote confidence="0.999984">
4http://www.termsciences.fr
5http://www.tei-c.org
</footnote>
<page confidence="0.997297">
12
</page>
<bodyText confidence="0.996760333333333">
scientific article and can be re-trained to fit tem-
plates used for a specific conference or to extract
additional fields.
</bodyText>
<sectionHeader confidence="0.822294" genericHeader="method">
4 Translation of Scientific Texts
</sectionHeader>
<bodyText confidence="0.999923340909091">
The translation system used is a Hybrid Machine
Translation (HMT) system from French to En-
glish and from English to French, adapted to
translate scientific texts in several domains (so
far physics and computer science). This sys-
tem is composed of a statistical engine, cou-
pled with rule-based modules to translate spe-
cial parts of the text such as mathematical for-
mulas, chemical compounds, pseudo-code, and
enriched with domain bilingual terminology (see
Section 2). Large amounts of monolingual and
parallel data are available to train a SMT system
between French and English, but not in the scien-
tific domain. In order to improve the performance
of our translation system in this task, we extracted
in-domain monolingual and parallel data from the
HAL archive. All the PDF files deposited in HAL
in computer science and physics were made avail-
able to us. These files were then converted to
plain text using the Grobid tool, as described in
the previous section. We extracted text from all
the documents from HAL that were made avail-
able to us to train our language model. We built
a small parallel corpus from the abstracts of the
PhD theses from French universities, which must
include both an abstract in French and in English.
Table 1 presents statistics of these in-domain data.
The data extracted from HAL were used to
adapt a generic system to the scientific litera-
ture domain. The generic system was mostly
trained on data provided for the shared task of
Sixth Workshop on Statistical Machine Transla-
tion6 (WMT 2011), described in Table 2.
Table 3 presents results showing, in the
English–French direction, the impact on the sta-
tistical engine of introducing the resources ex-
tracted from HAL, as well as the impact of do-
main adaptation techniques. The baseline statis-
tical engine is a standard PBSMT system based
on Moses (Koehn et al., 2007) and the SRILM
tookit (Stolcke, 2002). Is was trained and tuned
only on WMT11 data (out-of-domain). Incorpo-
rating the HAL data into the language model and
tuning the system on the HAL development set,
</bodyText>
<footnote confidence="0.944791">
6http://www.statmt.org/wmt11/translation-task.html
</footnote>
<table confidence="0.999747941176471">
Set Domain Lg Sent. Words Vocab.
Parallel data
Train cs+phys En 55.9 k 1.41 M 43.3 k
Fr 55.9 k 1.63 M 47.9 k
Dev cs En 1100 25.8 k 4.6 k
Fr 1100 28.7 k 5.1 k
phys En 1000 26.1 k 5.1 k
Fr 1000 29.1 k 5.6 k
Test cs En 1100 26.1 k 4.6 k
Fr 1100 29.2 k 5.2 k
phys En 1000 25.9 k 5.1 k
Fr 1000 28.8 k 5.5 k
Monolingual data
Train cs En 2.5 M 54 M 457 k
Fr 761 k 19 M 274 k
phys En 2.1 M 50 M 646 k
Fr 662 k 17 M 292 k
</table>
<tableCaption confidence="0.6676269">
Table 1: Statistics for the parallel training, develop-
ment, and test data sets extracted from thesis abstracts
contained in HAL, as well as monolingual data ex-
tracted from all documents in HAL, in computer sci-
ence (cs) and physics (phys). The following statistics
are given for the English (En) and French (Fr) sides
(Lg) of the corpus: the number of sentences, the num-
ber of running words (after tokenisation) and the num-
ber of words in the vocabulary (M and k stand for mil-
lions and thousands, respectively).
</tableCaption>
<bodyText confidence="0.9999478">
yielded a gain of more than 7 BLEU points, in
both domains (computer science and physics). In-
cluding the theses abstracts in the parallel training
corpus, a further gain of 2.3 BLEU points is ob-
served for computer science, and 3.1 points for
physics. The last experiment performed aims at
increasing the amount of in-domain parallel texts
by translating automatically in-domain monolin-
gual data, as suggested by Schwenk (2008). The
synthesised bitext does not bring new words into
the system, but increases the probability of in-
domain bilingual phrases. By adding a synthetic
bitext of 12 million words to the parallel training
data, we observed a gain of 0.5 BLEU point for
computer science, and 0.7 points for physics.
Although not shown here, similar results were
obtained in the French–English direction. The
French–English system is actually slightly bet-
ter than the English–French one as it is an easier
translation direction.
</bodyText>
<page confidence="0.997219">
13
</page>
<table confidence="0.9992535">
Translation Model Language Model Tuning Domain CS PHYS
words (M) Bleu words (M) Bleu
wmt11 wmt11 wmt11 371 27.3 371 27.1
wmt11 wmt11+hal hal 371 36.0 371 36.2
wmt11+hal wmt11+hal hal 287 38.3 287 39.3
wmt11+hal+adapted wmt11+hal hal 299 38.8 307 40.0
</table>
<tableCaption confidence="0.93471025">
Table 3: Results (BLEU score) for the English–French systems. The type of parallel data used to train the
translation model or language model are indicated, as well as the set (in-domain or out-of-domain) used to tune
the models. Finally, the number of words in the parallel corpus and the BLEU score on the in-domain test set are
indicated for each domain: computer science and physics.
</tableCaption>
<figureCaption confidence="0.994785">
Figure 1: Translation and post-editing interface.
</figureCaption>
<table confidence="0.999659363636364">
Corpus English French
Bitexts: 50.5M 54.4M
Europarl 2.9M 3.3M
News Commentary 667M 794M
Crawled (109 bitexts)
Development data: 65k 73k
newstest2009 62k 71k
newstest2010
Monolingual data: 4.1G 920M
LDC Gigaword 2.6G 612M
Crawled news
</table>
<tableCaption confidence="0.9966955">
Table 2: Out-of-domain development and training data
used (number of words after tokenisation).
</tableCaption>
<sectionHeader confidence="0.996998" genericHeader="method">
5 Post-editing Interface
</sectionHeader>
<bodyText confidence="0.99900775">
The collaborative aspect of the demonstrated ma-
chine translation service is based on a post-editing
tool, whose interface is shown in Figure 1. This
tool provides the following features:
</bodyText>
<listItem confidence="0.996888428571429">
• WYSIWYG display of the source and target
texts (Zones 1+2)
• Alignment at the sentence level (Zone 3)
• Zone to review the translation with align-
ment of source and target terms (Zone 4) and
terminology reference (Zone 5)
• Alternative translations (Zone 6)
</listItem>
<bodyText confidence="0.9999566">
The tool allows the user to perform sentence
level post-editing and records details of post-
editing activity, such as keystrokes, terminology
selection, actual edits and time log for the com-
plete action.
</bodyText>
<sectionHeader confidence="0.995565" genericHeader="conclusions">
6 Conclusions and Perspectives
</sectionHeader>
<bodyText confidence="0.9991655">
We proposed the demonstration of an end-to-end
tool integrated into the HAL archive and enabling
</bodyText>
<page confidence="0.99667">
14
</page>
<bodyText confidence="0.999933285714286">
efficient translation for scientific texts. This tool
consists of a high-accuracy PDF extractor, a hy-
brid machine translation engine adapted to the sci-
entific domain and a post-edition tool. Thanks to
in-domain data collected from HAL, the statisti-
cal engine was improved by more than 10 BLEU
points with respect to a generic system trained on
WMT11 data.
Our system was deployed for a physic confer-
ence organised in Paris in Sept 2011. All accepted
abstracts were translated into author’s native lan-
guages (around 70% of them) and proposed for
post-editing. The experience was promoted by
the organisation committee and 50 scientists vol-
unteered (34 finally performed their post-editing).
The same experience will be proposed for authors
of the LREC conference. We would like to offer
a complete demonstration of the system at EACL.
The goal of these experiences is to collect and dis-
tribute detailed ”post-editing” data for enabling
research on this activity.
</bodyText>
<sectionHeader confidence="0.99604" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999906">
This work has been partially funded by the French
Government under the project COSMAT (ANR
ANR-09-CORD-004).
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999783888888889">
Foudil Bretel, Patrice Lopez, Maud Medves, Alain
Monteil, and Laurent Romary. 2010. Back to
meaning – information structuring in the PEER
project. In TEI Conference, Zadar, Croatie.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (Demo and Poster Sessions), pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Patrice Lopez. 2009. GROBID: Combining auto-
matic bibliographic data recognition and term ex-
traction for scholarship publications. In Proceed-
ings of ECDL 2009, 13th European Conference on
Digital Library, Corfu, Greece.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In IWSLT, pages 182–189.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901–904, Denver,
CO.
</reference>
<page confidence="0.99785">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.069357">
<title confidence="0.999514">Collaborative Machine Translation Service for Scientific texts</title>
<author confidence="0.99986">Patrik Lambert Jean Senellart</author>
<affiliation confidence="0.993464">University of Le Mans Systran SA</affiliation>
<email confidence="0.546117">patrik.lambert@lium.univ-lemans.frsenellart@systran.fr</email>
<author confidence="0.700336">Laurent Romary Holger Schwenk Humboldt Universit¨at Berlin University of Le_Mans Saclay</author>
<email confidence="0.985707">laurent.romary@inria.fr</email>
<author confidence="0.580351666666667">Florian Patrice Lopez Fr´ed´eric Humboldt Universit¨at Berlin Humboldt Universit¨at Berlin INRIA Saclay</author>
<email confidence="0.913354">lium.univ-lemans.fr</email>
<abstract confidence="0.999055857142857">French researchers are required to frequently translate into French the description of their work published in English. At the same time, the need for French people to access articles in English, or to international researchers to access theses or papers in French, is incorrectly resolved via the use of generic translation tools. We propose the demonstration of an end-to-end tool integrated in the HAL open archive for enabling efficient translation for scientific texts. This tool can give translation suggestions adapted to the scientific domain, improving by more than 10 points the BLEU score of a generic system. It also provides a post-edition service which captures user post-editing data that can be used to incrementally improve the translations engines. Thus it is helpful for users which need to translate or to access scientific texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Foudil Bretel</author>
<author>Patrice Lopez</author>
<author>Maud Medves</author>
<author>Alain Monteil</author>
<author>Laurent Romary</author>
</authors>
<title>Back to meaning – information structuring in the PEER project.</title>
<date>2010</date>
<booktitle>In TEI Conference,</booktitle>
<location>Zadar, Croatie.</location>
<contexts>
<context position="7625" citStr="Bretel et al., 2010" startWordPosition="1169" endWordPosition="1172">document is then archived in TEI format and available for display in HTML using dedicated XSLT style sheets. 3 The Grobid System Based on state-of-the-art machine learning techniques, Grobid (Lopez, 2009) performs reliable bibliographic data extraction from scholar articles combined with multi-level term extraction. These two types of extraction present synergies and correspond to complementary descriptions of an article. This tool parses and converts scientific articles in PDF format into a structured TEI document5 compliant with the good practices developed within the European PEER project (Bretel et al., 2010). Grobid is trained on a set of annotated 4http://www.termsciences.fr 5http://www.tei-c.org 12 scientific article and can be re-trained to fit templates used for a specific conference or to extract additional fields. 4 Translation of Scientific Texts The translation system used is a Hybrid Machine Translation (HMT) system from French to English and from English to French, adapted to translate scientific texts in several domains (so far physics and computer science). This system is composed of a statistical engine, coupled with rule-based modules to translate special parts of the text such as m</context>
</contexts>
<marker>Bretel, Lopez, Medves, Monteil, Romary, 2010</marker>
<rawString>Foudil Bretel, Patrice Lopez, Maud Medves, Alain Monteil, and Laurent Romary. 2010. Back to meaning – information structuring in the PEER project. In TEI Conference, Zadar, Croatie.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics (Demo and Poster Sessions),</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="9728" citStr="Koehn et al., 2007" startWordPosition="1516" endWordPosition="1519">Table 1 presents statistics of these in-domain data. The data extracted from HAL were used to adapt a generic system to the scientific literature domain. The generic system was mostly trained on data provided for the shared task of Sixth Workshop on Statistical Machine Translation6 (WMT 2011), described in Table 2. Table 3 presents results showing, in the English–French direction, the impact on the statistical engine of introducing the resources extracted from HAL, as well as the impact of domain adaptation techniques. The baseline statistical engine is a standard PBSMT system based on Moses (Koehn et al., 2007) and the SRILM tookit (Stolcke, 2002). Is was trained and tuned only on WMT11 data (out-of-domain). Incorporating the HAL data into the language model and tuning the system on the HAL development set, 6http://www.statmt.org/wmt11/translation-task.html Set Domain Lg Sent. Words Vocab. Parallel data Train cs+phys En 55.9 k 1.41 M 43.3 k Fr 55.9 k 1.63 M 47.9 k Dev cs En 1100 25.8 k 4.6 k Fr 1100 28.7 k 5.1 k phys En 1000 26.1 k 5.1 k Fr 1000 29.1 k 5.6 k Test cs En 1100 26.1 k 4.6 k Fr 1100 29.2 k 5.2 k phys En 1000 25.9 k 5.1 k Fr 1000 28.8 k 5.5 k Monolingual data Train cs En 2.5 M 54 M 457 k </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics (Demo and Poster Sessions), pages 177– 180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrice Lopez</author>
</authors>
<title>GROBID: Combining automatic bibliographic data recognition and term extraction for scholarship publications.</title>
<date>2009</date>
<booktitle>In Proceedings of ECDL 2009, 13th European Conference on Digital Library, Corfu,</booktitle>
<contexts>
<context position="7209" citStr="Lopez, 2009" startWordPosition="1108" endWordPosition="1109"> described in Section 4. • The translation process generates a bilingual TEI format preserving the source structure and integrating the entity annotation, multiple terminology choices when available, and the token alignment between source and target sentences. • The translation is proposed to the user for post-editing through a rich interactive interface described in Section 5. • The final version of the document is then archived in TEI format and available for display in HTML using dedicated XSLT style sheets. 3 The Grobid System Based on state-of-the-art machine learning techniques, Grobid (Lopez, 2009) performs reliable bibliographic data extraction from scholar articles combined with multi-level term extraction. These two types of extraction present synergies and correspond to complementary descriptions of an article. This tool parses and converts scientific articles in PDF format into a structured TEI document5 compliant with the good practices developed within the European PEER project (Bretel et al., 2010). Grobid is trained on a set of annotated 4http://www.termsciences.fr 5http://www.tei-c.org 12 scientific article and can be re-trained to fit templates used for a specific conference </context>
</contexts>
<marker>Lopez, 2009</marker>
<rawString>Patrice Lopez. 2009. GROBID: Combining automatic bibliographic data recognition and term extraction for scholarship publications. In Proceedings of ECDL 2009, 13th European Conference on Digital Library, Corfu, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Investigations on large-scale lightly-supervised training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In IWSLT,</booktitle>
<pages>182--189</pages>
<contexts>
<context position="11330" citStr="Schwenk (2008)" startWordPosition="1821" endWordPosition="1822">f the corpus: the number of sentences, the number of running words (after tokenisation) and the number of words in the vocabulary (M and k stand for millions and thousands, respectively). yielded a gain of more than 7 BLEU points, in both domains (computer science and physics). Including the theses abstracts in the parallel training corpus, a further gain of 2.3 BLEU points is observed for computer science, and 3.1 points for physics. The last experiment performed aims at increasing the amount of in-domain parallel texts by translating automatically in-domain monolingual data, as suggested by Schwenk (2008). The synthesised bitext does not bring new words into the system, but increases the probability of indomain bilingual phrases. By adding a synthetic bitext of 12 million words to the parallel training data, we observed a gain of 0.5 BLEU point for computer science, and 0.7 points for physics. Although not shown here, similar results were obtained in the French–English direction. The French–English system is actually slightly better than the English–French one as it is an easier translation direction. 13 Translation Model Language Model Tuning Domain CS PHYS words (M) Bleu words (M) Bleu wmt11</context>
</contexts>
<marker>Schwenk, 2008</marker>
<rawString>Holger Schwenk. 2008. Investigations on large-scale lightly-supervised training for statistical machine translation. In IWSLT, pages 182–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="9765" citStr="Stolcke, 2002" startWordPosition="1524" endWordPosition="1525">main data. The data extracted from HAL were used to adapt a generic system to the scientific literature domain. The generic system was mostly trained on data provided for the shared task of Sixth Workshop on Statistical Machine Translation6 (WMT 2011), described in Table 2. Table 3 presents results showing, in the English–French direction, the impact on the statistical engine of introducing the resources extracted from HAL, as well as the impact of domain adaptation techniques. The baseline statistical engine is a standard PBSMT system based on Moses (Koehn et al., 2007) and the SRILM tookit (Stolcke, 2002). Is was trained and tuned only on WMT11 data (out-of-domain). Incorporating the HAL data into the language model and tuning the system on the HAL development set, 6http://www.statmt.org/wmt11/translation-task.html Set Domain Lg Sent. Words Vocab. Parallel data Train cs+phys En 55.9 k 1.41 M 43.3 k Fr 55.9 k 1.63 M 47.9 k Dev cs En 1100 25.8 k 4.6 k Fr 1100 28.7 k 5.1 k phys En 1000 26.1 k 5.1 k Fr 1000 29.1 k 5.6 k Test cs En 1100 26.1 k 4.6 k Fr 1100 29.2 k 5.2 k phys En 1000 25.9 k 5.1 k Fr 1000 28.8 k 5.5 k Monolingual data Train cs En 2.5 M 54 M 457 k Fr 761 k 19 M 274 k phys En 2.1 M 50 </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM: an extensible language modeling toolkit. In Proc. of the Int. Conf. on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>