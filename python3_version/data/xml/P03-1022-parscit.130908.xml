<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000593">
<title confidence="0.998831">
A Machine Learning Approach to Pronoun Resolution in Spoken Dialogue
</title>
<author confidence="0.978977">
Michael Strube and Christoph M¨uller
</author>
<affiliation confidence="0.962667">
European Media Laboratory GmbH
</affiliation>
<address confidence="0.869673">
Villa Bosch
Schloß-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
</address>
<email confidence="0.998256">
michael.strube|christoph.mueller @eml.villa-bosch.de
</email>
<sectionHeader confidence="0.995628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9964738">
We apply a decision tree based approach
to pronoun resolution in spoken dialogue.
Our system deals with pronouns with NP-
and non-NP-antecedents. We present a set
of features designed for pronoun resolu-
tion in spoken dialogue and determine the
most promising features. We evaluate the
system on twenty Switchboard dialogues
and show that it compares well to Byron’s
(2002) manually tuned system.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999475733333333">
Corpus-based methods and machine learning tech-
niques have been applied to anaphora resolution in
written text with considerable success (Soon et al.,
2001; Ng &amp; Cardie, 2002, among others). It has
been demonstrated that systems based on these ap-
proaches achieve a performance that is comparable
to hand-crafted systems. Since they can easily be
applied to new domains it seems also feasible to
port a given corpus-based anaphora resolution sys-
tem from written text to spoken dialogue. This pa-
per describes the extensions and adaptations needed
for applying our anaphora resolution system (M¨uller
et al., 2002; Strube et al., 2002) to pronoun resolu-
tion in spoken dialogue.
There are important differences between written
text and spoken dialogue which have to be accounted
for. The most obvious difference is that in spo-
ken dialogue there is an abundance of (personal and
demonstrative) pronouns with non-NP-antecedents
or no antecedents at all. Corpus studies have shown
that a significant amount of pronouns in spoken di-
alogue have non-NP-antecedents: Byron &amp; Allen
(1998) report that about 50% of the pronouns in the
TRAINS93 corpus have non-NP-antecedents. Eck-
ert &amp; Strube (2000) note that only about 45% of
the pronouns in a set of Switchboard dialogues have
NP-antecedents. The remainder consists of 22%
which have non-NP-antecedents and 33% without
antecedents. These studies suggest that the perfor-
mance of a pronoun resolution algorithm can be im-
proved considerably by enabling it to resolve also
pronouns with non-NP-antecedents.
Because of the difficulties a pronoun resolution
algorithm encounters in spoken dialogue, previous
approaches were applied only to tiny domains, they
needed deep semantic analysis and discourse pro-
cessing and relied on hand-crafted knowledge bases.
In contrast, we build on our existing anaphora res-
olution system and incrementally add new features
specifically devised for spoken dialogue. That way
we are able to determine relatively powerful yet
computationally cheap features. To our knowledge
the work presented here describes the first imple-
mented system for corpus-based anaphora resolution
dealing also with non-NP-antecedents.
</bodyText>
<sectionHeader confidence="0.731117" genericHeader="method">
2 NP- vs. Non-NP-Antecedents
</sectionHeader>
<bodyText confidence="0.95331797368421">
Spoken dialogue contains more pronouns with non-
NP-antecedents than written text does. However,
pronouns with NP-antecedents (like 3rd pers. mas-
culine/feminine pronouns, cf. he in the example be-
low) still constitute the largest fraction of all coref-
erential pronouns in the Switchboard corpus.
In spoken dialogue there are considerable num-
bers of pronouns that pick up different kinds of
abstract objects from the previous discourse, e.g.
events, states, concepts, propositions or facts (Web-
ber, 1991; Asher, 1993). These anaphors then have
VP-antecedents (“it ” in (B6) below) or sentential
antecedents (“that ” in (B5)).
A1: ... [he] ’s nine months old....
A2: [He] likes to dig around a little bit.
A3: [His] mother comes in and says, why did you let [him]
[play in the dirt] ,
A:4 I guess [[he] ’s enjoying himself] .
B5: [That] ’s right.
B&amp; [It] ’s healthy, .. .
A major problem for pronoun resolution in spo-
ken dialogue is the large number of personal and
demonstrative pronouns which are either not refer-
ential at all (e.g. expletive pronouns) or for which a
particular antecedent cannot easily be determined by
humans (called vague anaphors by Eckert &amp; Strube
(2000)).
In the following example, the “that ” in utter-
ance (A3) refers back to utterance (A1). As for
the first two pronouns in (B4), following Eckert &amp;
Strube (2000) and Byron (2002) we assume that re-
ferring expressions in disfluencies, abandoned utter-
ances etc. are excluded from the resolution. The
third pronoun in (B4) is an expletive. The pronoun
in (A5) is different in that it is indeed referential: it
refers back to“that ” from (A3).
A1: ... [There is a lot of theft, a lot of assault dealing with, uh,
people trying to get money for drugs. ]
</bodyText>
<footnote confidence="0.95732325">
B2: Yeah.
A3: And, uh, I think [that ]’s a national problem, though.
B4: It, it, it’s pretty bad here, too.
A5: [It ]’s not unique ...
</footnote>
<bodyText confidence="0.988390818181818">
Pronoun resolution in spoken dialogue also has
to deal with the whole range of difficulties that
come with processing spoken language: disfluen-
cies, hesitations, abandoned utterances, interrup-
tions, backchannels, etc. These phenomena have to
be taken into account when formulating constraints
on e.g. the search space in which an anaphor looks
for its antecedent. E.g., utterance (B2) in the previ-
ous example does not contain any referring expres-
sions. So the demonstrative pronoun in (A3) has to
have access not only to (B2) but also to (A1).
</bodyText>
<sectionHeader confidence="0.992532" genericHeader="method">
3 Data
</sectionHeader>
<subsectionHeader confidence="0.996339">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999966925">
Our work is based on twenty randomly chosen
Switchboard dialogues. Taken together, the dia-
logues contain 30810 tokens (words and punctua-
tion) in 3275 sentences / 1771 turns. The annotation
consists of 16601 markables, i.e. sequences of words
and attributes associated with them. On the top level,
different types of markables are distinguished: NP-
markables identify referring expressions like noun
phrases, pronouns and proper names. Some of
the attributes for these markables are derived from
the Penn Treebank version of the Switchboard dia-
logues, e.g. grammatical function, NP form, gram-
matical case and depth of embedding in the syn-
tactical structure. VP-markables are verb phrases,
S-markables sentences. Disfluency-markables are
noun phrases or pronouns which occur in unfin-
ished or abandoned utterances. Among other (type-
dependent) attributes, markables contain a member
attribute with the ID of the coreference class they
are part of (if any). If an expression is used to re-
fer to an entity that is not referred to by any other
expression, it is considered a singleton.
Table 1 gives the distribution of the npform at-
tribute for NP-markables. The second and third row
give the number of non-singletons and singletons re-
spectively that add up to the total number given in
the first row.
Table 2 shows the distribution of the agreement
attribute (i.e. person, gender, and number) for the
pronominal expressions in our corpus. The left fig-
ure in each cell gives the total number of expres-
sions, the right figure gives the number of non-
singletons. Note the relatively high number of sin-
gletons among the personal and demonstrative pro-
nouns (223 for it, 60 for they and 82 for that). These
pronouns are either expletive or vague, and cause
the most trouble for a pronoun resolution algorithm,
which will usually attempt to find an antecedent
nonetheless. Singleton they pronouns, in particu-
lar, are typical for spoken language (as opposed to
</bodyText>
<table confidence="0.98087375">
defNP indefNP NNP prp prp$ dtpro
Total 1080 1899 217 1075 70 392
In coreference relation 219 163 94 786 56 309
Singletons 861 1736 123 289 14 83
</table>
<tableCaption confidence="0.984517">
Table 1: Distribution of npform Feature on Markables (w/o 1st and 2nd Persons)
</tableCaption>
<table confidence="0.9991782">
3m 3f 3n 3p
prp 67 63 49 47 541 318 418 358
prp$ 18 15 14 11 3 3 35 27
dtpro 0 0 0 0 380 298 12 11
85 78 63 58 924 619 465 396
</table>
<tableCaption confidence="0.998355">
Table 2: Distribution of Agreement Feature on Pronominal Expressions
</tableCaption>
<bodyText confidence="0.999926666666667">
written text). The same is true for anaphors with
non-NP-antecedents. However, while they are far
more frequent in spoken language than in written
text, they still constitute only a fraction of all coref-
erential expressions in our corpus. This defines an
upper limit for what the resolution of these kinds of
anaphors can contribute at all. These facts have to be
kept in mind when comparing our results to results
of coreference resolution in written text.
</bodyText>
<subsectionHeader confidence="0.999697">
3.2 Data Generation
</subsectionHeader>
<bodyText confidence="0.999990428571429">
Training and test data instances were generated from
our corpus as follows. All markables were sorted
in document order, and markables for first and sec-
ond person pronouns were removed. The resulting
list was then processed from top to bottom. If the
list contained an NP-markable at the current posi-
tion and if this markable was not an indefinite noun
phrase, it was considered a potential anaphor. In
that case, pairs of potentially coreferring expressions
were generated by combining the potential anaphor
with each compatible1 NP-markable preceding2 it
in the list. The resulting pairs were labelled P if
both markables had the same (non-empty) value in
their member attribute, N otherwise. For anaphors
with non-NP-antecedents, additional training and
test data instances had to be generated. This process
was triggered by the markable at the current position
being it or that. In that case, a small set of poten-
tial non-NP-antecedents was generated by selecting
S- and VP-markables from the last two valid sen-
tences preceding the potential anaphor. The choice
</bodyText>
<footnote confidence="0.981463666666667">
1Markables are considered compatible if they do not mis-
match in terms of agreement.
2We disregard the phenomenon of cataphor here.
</footnote>
<bodyText confidence="0.999919733333333">
of the last two sentences was motivated pragmat-
ically by considerations to keep the search space
(and the number of instances) small. A sentence
was considered valid if it was neither unfinished
nor a backchannel utterance (like e.g. ”Uh-huh”,
”Yeah”, etc.). From the selected markables, inac-
cessible non-NP-expressions were automatically re-
moved. We considered an expression inaccessible
if it ended before the sentence in which it was con-
tained. This was intended to be a rough approxi-
mation of the concept of the right frontier (Webber,
1991). The remaining expressions were then com-
bined with the potential anaphor. Finally, the result-
ing pairs were labelled P or N and added to the in-
stances generated with NP-antecedents.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.996296444444444">
We distinguish two classes of features: NP-level
features specify e.g. the grammatical function, NP
form, morpho-syntax, grammatical case and the
depth of embedding in the syntactical structure.
For these features, each instance contains one
value for the antecedent and one for the anaphor.
Coreference-level features, on the other hand, de-
scribe the relation between antecedent and anaphor
in terms of e.g. distance (in words, markables and
sentences), compatibility in terms of agreement and
identity of syntactic function. For these features,
each instance contains only one value.
In addition, we introduce a set of features which
is partly tailored to the processing of spoken dia-
logue. The feature ante exp type (17) is a rather
obvious yet useful feature to distinguish NP- from
non-NP-antecedents. The features ana np , vp and
NP-level features
</bodyText>
<listItem confidence="0.957326333333333">
1. ante gram func grammatical function of antecedent
2. ante npform form of antecedent
3. ante agree person, gender, number
4. ante case grammatical case of antecedent
5. ante s depth the level of embedding in a sentence
6. ana gram func grammatical function of anaphor
7. ana npform form of anaphor
8. ana agree person, gender, number
9. ana case grammatical case of anaphor
</listItem>
<figure confidence="0.969721555555555">
10. ana s depth the level of embedding in a sentence
Coreference-level features
11. agree comp compatibility in agreement between anaphor and antecedent
12. npform comp compatibilty in NP form between anaphor and antecedent
13. wdist distance between anaphor and antecedent in words
14. mdist distance between anaphor and antecedent in markables
15. sdist distance between anaphor and antecedent in sentences
16. syn par anaphor and antecedent have the same grammatical function (yes, no)
Features introduced for spoken dialogue
17. ante exp type type of antecedent (NP, S, VP)
18. ana np pref preference for NP arguments
19. ana vp pref preference for VP arguments
20. ana s pref preference for S arguments
21. mdist 3mf3p (see text)
22. mdist 3n (see text)
23. ante tfidf (see text)
24. ante ic (see text)
25. wdist ic (see text)
</figure>
<tableCaption confidence="0.985983">
Table 3: Our Features
</tableCaption>
<bodyText confidence="0.999244136363637">
s pref (18, 19, 20) describe a verb’s preference for
arguments of a particular type. Inspired by the
work of Eckert &amp; Strube (2000) and Byron (2002),
these features capture preferences for NP- or non-
NP-antecedents by taking a pronoun’s predicative
context into account. The underlying assumption is
that if a verb preceding a personal or demonstrative
pronoun preferentially subcategorizes sentences or
VPs, then the pronoun will be likely to have a non-
NP-antecedent. The features are based on a verb list
compiled from 553 Switchboard dialogues.3 For ev-
ery verb occurring in the corpus, this list contains
up to three entries giving the absolute count of cases
where the verb has a direct argument of type NP, VP
or S. When the verb list was produced, pronominal
arguments were ignored. The features mdist 3mf3p
and mdist 3n (21, 22) are refinements of the mdist
feature. They measure the distance in markables be-
tween antecedent and anaphor, but in doing so they
take the agreement value of the anaphor into ac-
count. For anaphors with an agreement value of 3mf
or 3p, mdist 3mf3p is measured as D = 1 + the num-
</bodyText>
<footnote confidence="0.8187175">
3It seemed preferable to compile our own list instead of us-
ing existing ones like Briscoe &amp; Carroll (1997).
</footnote>
<bodyText confidence="0.999693238095238">
ber of NP-markables between anaphor and potential
antecedent. Anaphors with an agreement value of
3n, (i.e. it or that), on the other hand, potentially
have non-NP-antecedents, so mdist 3n is measured
as D + the number of anaphorically accessible4 S-
and VP-markables between anaphor and potential
antecedent.
The feature ante tfifd (23) is supposed to capture
the relative importance of an expression for a dia-
logue. The underlying assumption is that the higher
the importance of a non-NP expression, the higher
the probability of its being referred back to. For
our purposes, we calculated TF for every word by
counting its frequency in each of our twenty Switch-
board dialogues separately. The calculation of IDF
was based on a set of 553 Switchboard dialogues.
For every word, we calculated IDF as log(553/N ),
with N =number of documents containing the word.
For every non-NP-markable, an average TF*IDF
value was calculated as the TF*IDF sum of all words
comprising the markable, divided by the number of
</bodyText>
<footnote confidence="0.704708666666667">
4As mentioned earlier, the definition of accessibility of non-
NP-antecedents is inspired by the concept of the right frontier
(Webber, 1991).
</footnote>
<bodyText confidence="0.9999565625">
words in the markable. The feature ante ic (24) as
an alternative to ante tfidf is based on the same as-
sumptions as the former. The information content of
a non-NP-markable is calculated as follows, based
on a set of 553 Switchboard dialogues: For each
word in the markable, the IC value was calculated
as the negative log of the total frequency of the word
divided by the total number of words in all 553 dia-
logues. The average IC value was then calculated as
the IC sum of all words in the markable, divided by
the number of words in the markable. Finally, the
feature wdist ic (25) measures the word-based dis-
tance between two expressions. It does so in terms
of the sum of the individual words’ IC. The calcula-
tion of the IC was done as described for the ante ic
feature.
</bodyText>
<sectionHeader confidence="0.998604" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.875896">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9998951">
All experiments were performed using the decision
tree learner RPART (Therneau &amp; Atkinson, 1997),
which is a CART (Breiman et al., 1984) reimple-
mentation for the S-Plus and R statistical comput-
ing environments (we use R, Ihaka &amp; Gentleman
(1996), see http://www.r-project.org). We used the
standard pruning and control settings for RPART
(cp=0.0001, minsplit=20, minbucket=7). All results
reported were obtained by performing 20-fold cross-
validation.
In the prediction phase, the trained classifier is ex-
posed to unlabeled instances of test data. The classi-
fier’s task is to label each instance. When an instance
is labeled as coreferring, the IDs of the anaphor and
antecedent are kept in a response list for the evalua-
tion according to Vilain et al. (1995).
For determining the relevant feature set we fol-
lowed an iterative procedure similar to the wrap-
per approach for feature selection (Kohavi &amp; John,
1997). We start with a model based on a set of prede-
fined baseline features. Then we train models com-
bining the baseline with all additional features sep-
arately. We choose the best performing feature (f-
measure according to Vilain et al. (1995)), adding
it to the model. We then train classifiers combining
the enhanced model with each of the remaining fea-
tures separately. We again choose the best perform-
ing classifier and add the corresponding new feature
to the model. This process is repeated as long as
significant improvement can be observed.
</bodyText>
<subsectionHeader confidence="0.786063">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999929295454545">
In our experiments we split the data in three sets ac-
cording to the agreement of the anaphor: third per-
son masculine and feminine pronouns (3mf), third
person neuter pronouns (3n), and third person plural
pronouns (3p). Since only 3n-pronouns have non-
NP-antecedents, we were mainly interested in im-
provements in this data set.
We used the same baseline model for each data
set. The baseline model corresponds to a pronoun
resolution algorithm commonly applied to written
text, i.e., it uses only the features in the first two
parts of Table 3. For the baseline model we gener-
ated training and test data which included only NP-
antecedents.
Then we performed experiments using the fea-
tures introduced for spoken dialogue. The training
and test data for the models using additional features
included NP- and non-NP-antecedents. For each
data set we followed the iterative procedure outlined
in Section 5.1.
In the following tables we present the results of
our experiments. The first column gives the number
of coreference links correctly found by the classifier,
the second column gives the number of all corefer-
ence links found. The third column gives the total
number of coreference links (1250) in the corpus.
During evaluation, the list of all correct links is used
as the key list against which the response list pro-
duced by the classifier (cf. above) is compared. The
remaining three columns show precision, recall and
f-measure, respectively.
Table 4 gives the results for 3mf pronouns. The
baseline model performs very well on this data set
(the low recall figure is due to the fact that the 3mf
data set contains only a small subset of the coref-
erence links expected by the evaluation). The re-
sults are comparable to any pronoun resolution al-
gorithm dealing with written text. This shows that
our pronoun resolution system could be ported to the
spoken dialogue domain without sacrificing perfor-
mance.
Table 5 shows the results for 3n pronouns. The
baseline model does not perform very well. As men-
tioned above, for evaluating the performance of the
</bodyText>
<table confidence="0.559796">
correct found total found total correct precision recall f-measure
baseline, features 1-16 120 150 1250 80.00 9.60 17.14
plus mdist 3mf3p 121 153 1250 79.08 9.68 17.25
</table>
<tableCaption confidence="0.942426">
Table 4: Results for Third Person Masculine and Feminine Pronouns (3mf)
</tableCaption>
<table confidence="0.996376636363636">
correct found total found total correct precision recall f-measure
109 235 1250 46.38 8.72 14.68
97 232 1250 41.81 7.76 13.09
137 359 1250 38.16 10.96 17.03
154 389 1250 39.59 12.32 18.79
158 391 1250 40.41 12.64 19.26
baseline, features 1-16
plus none
plus ante exp type
plus wdist ic
plus ante tfidf
</table>
<tableCaption confidence="0.997764">
Table 5: Results for Third Person Neuter Pronouns (3n)
</tableCaption>
<bodyText confidence="0.996683622222222">
baseline model we removed all potential non-NP-
antecedents from the data. This corresponds to a
naive application of a model developed for written
text to spoken dialogue.
First, we applied the same model to the data set
containing all kinds of antecedents. The perfor-
mance drops somewhat as the classifier is exposed
to non-NP-antecedents without being able to differ-
entiate between NP- and non-NP-antecedents. By
adding the feature ante exp type the classifier is en-
abled to address NP- and non-NP-antecedents dif-
ferently, which results in a considerable gain in per-
formance. Substituting the wdist feature with the
wdist ic feature also improves the performance con-
siderably. The ante tfidf feature only contributes
marginally to the overall performance. – These re-
sults show that it pays off to consider features par-
ticularly designed for spoken dialogue.
Table 6 presents the results for 3p pronouns,
which do not have non-NP-antecedents. Many of
these pronouns do not have an antecedent at all. Oth-
ers are vague in that human annotators felt them
to be referential, but could not determine an an-
tecedent. Since we did not address that issue in
depth, the classifier tries to find antecedents for these
pronouns indiscriminately, which results in rather
low precision figures, as compared to e.g. those for
3mf. Only the feature wdist ic leads to an improve-
ment over the baseline.
Table 7 shows the results for the combined clas-
sifiers. The improvement in f-measure is due to the
increase in recall while the precision shows only a
slight decrease.
Though some of the features of the baseline
model (features 1-16) still occur in the decision
tree learned, the feature ante exp type divides ma-
jor parts of the tree quite nicely (see Figure 1). Be-
low that node the feature ana npform is used to dis-
tinguish between negative (personal pronouns) and
potential positive cases (demonstrative pronouns).
This confirms the hypothesis by Eckert &amp; Strube
(2000) and Byron (2002) to give high priority to
these features. The decision tree fragment in Figure
1 correctly assigns the P label to 23-7=16 sentential
antecedents.
</bodyText>
<table confidence="0.440694">
split, n, loss, yval
* denotes terminal node
...
</table>
<construct confidence="0.885758166666667">
anteexptype=s,vp 1110 55 N
ananpform=prp 747,11 N *
ananpform=dtpro 363 44 N
anteexptype=vp 177 3 N *
anteexptype=s 186 41 N
udist&gt;=1.5 95 14 N *
</construct>
<equation confidence="0.949824888888889">
udist&lt;1.5 91 27 N
wdistic&lt;43.32 33 4 N *
wdistic&gt;=43.32 58 23 N
anasdepth&gt;=2.5 23 4 N *
anasdepth&lt;2.5 35 16 N
wdistic&gt;=63.62 24 11 N
wdistic&lt;80.60 12 3 N *
wdistic&gt;=80.60 12 4 P *
wdistic&lt;63.62 11 3 P *
</equation>
<figureCaption confidence="0.999389">
Figure 1: Decision Tree Fragment
</figureCaption>
<bodyText confidence="0.999802625">
However, the most important problem is the large
amount of pronouns without antecedents. The
model does find (wrong) antecedents for a lot of pro-
nouns which should not have one. Only a small frac-
tion of these pronouns are true expletives (i.e., they
precede a “weather” verb or are in constructions like
“It seems that... ”. The majority of these cases are
referential, but have no antecedent in the data (i.e.,
</bodyText>
<table confidence="0.81100375">
correct found total found total correct precision recall f-measure
baseline, features 1-16 227 354 1250 64.12 18.16 28.30
plus wdist ic 230 353 1250 65.16 18.40 28.70
Table 6: Results for Third Person Plural Pronouns (3p)
</table>
<tableCaption confidence="0.989818">
Table 7: Combined Results for All Pronouns
</tableCaption>
<table confidence="0.7661005">
correct total found total correct precision recall f-measure
found
baseline, features 1-16 456 739 1250 61.71 36.48 45.85
combined 509 897 1250 56.74 40.72 47.42
</table>
<bodyText confidence="0.978018888888889">
they are vague pronouns).
The overall numbers for precision, recall and f-
measure are fairly low. One reason is that we did not
attempt to resolve anaphoric definite NPs and proper
names though these coreference links are contained
in the evaluation key list. If we removed them from
there, the recall of our experiments would approach
the 51% Byron (2002) mentioned for her system us-
ing only domain-independent semantic restrictions.
</bodyText>
<sectionHeader confidence="0.990206" genericHeader="method">
6 Comparison to Related Work
</sectionHeader>
<bodyText confidence="0.999981865384616">
Our approach for determining the feature set for pro-
noun resolution resembles the so-called wrapper ap-
proach for feature selection (Kohavi &amp; John, 1997).
This is in contrast to the majority of other work on
feature selection for anaphora resolution, which was
hardly ever done systematically. E.g. Soon et al.
(2001) only compared baseline systems consisting
of one feature each, only three of which yielded an
f-measure greater than zero. Then they combined
these features and achieved results which were close
to the best overall results they report. While this tells
us which features contribute a lot, it does not give
any information about potential (positive or nega-
tive) influence of the rest. Ng &amp; Cardie (2002) select
the set of features by hand, giving a preference to
high precision features. They admit that this method
is quite subjective.
Corpus-based work about pronoun resolution in
spoken dialogue is almost non-existent. However,
there are a few papers dealing with neuter pronouns
with NP-antecedents. E.g., Dagan &amp; Itai (1991) pre-
sented a corpus-based approach to the resolution of
the pronoun it, but they use a written text corpus and
do not mention non-NP-antecedents at all. Paul et al.
(1999) presented a corpus-based anaphora resolu-
tion algorithm for spoken dialogue. For their exper-
iments, however, they restricted anaphoric relations
to those with NP-antecedents.
Byron (2002) presented a symbolic approach
which resolves pronouns with NP- and non-NP-
antecedents in spoken dialogue in the TRAINS do-
main. Byron extends a pronoun resolution al-
gorithm (Tetrault, 2001) with semantic filtering,
thus enabling it to resolve anaphors with non-NP-
antecedents as well. Semantic filtering relies on
knowledge about semantic restrictions associated
with verbs, like semantic compatibility between sub-
ject and predicative noun or predicative adjective.
An evaluation on ten TRAINS93 dialogues with
80 3rd person pronouns and 100 demonstrative pro-
nouns shows that semantic filtering and the im-
plementation of different search strategies for per-
sonal and demonstrative pronouns yields a suc-
cess rate of 72%. As Byron admits, the ma-
jor limitation of her algorithm is its dependence
on domain-dependent resources which cover the
domain entirely. When evaluating her algorithm
with only domain-independent semantics, Byron
achieved 51% success rate. What is problematic
with her approach is that she assumes the input to
her algorithm to be only referential pronouns. This
simplifies the task considerably.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999963805555556">
We presented a machine learning approach to pro-
noun resolution in spoken dialogue. We built upon
a system we used for anaphora resolution in writ-
ten text and extended it with a set of features de-
signed for spoken dialogue. We refined distance
features and used metrics from information retrieval
for determining non-NP-antecedents. Inspired by
the more linguistically oriented work by Eckert &amp;
Strube (2000) and Byron (2002) we also evaluated
the contribution of features which used the predica-
tive context of the pronoun to be resolved. However,
these features did not show up in the final models
since they did not lead to an improvement. Instead,
rather simple distance metrics were preferred. While
we were (almost) satisfied with the performance of
these features, the major problem for a spoken dia-
logue pronoun resolution algorithm is the abundance
of pronouns without antecedents. Previous research
could avoid dealing with this phenomenon by either
applying the algorithm by hand (Eckert &amp; Strube,
2000) or excluding these cases (Byron, 2002) from
the evaluation. Because we included these cases
in our evaluation we consider our approach at least
comparable to Byron’s system when she uses only
domain-independent semantics. We believe that our
system is more robust than hers and that it can more
easily be ported to new domains.
Acknowledgements. The work presented here has
been partially funded by the German Ministry of
Research and Technology as part of the EMBASSI
project (01 IL 904 D/2) and by the Klaus Tschira
Foundation. We would like to thank Susanne
Wilhelm and Lutz Wind for doing the annota-
tions, Kerstin Sch¨urmann, Torben Pastuch and Klaus
Rothenh¨ausler for helping with the data prepara-
tion.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999538681159421">
Asher, Nicholas (1993). Reference to Abstract Objects in Dis-
course. Dordrecht, The Netherlands: Kluwer.
Breiman, Leo, Jerome H. Friedman, Charles J. Stone &amp; R.A.
Olshen (1984). Classification and Regression Trees. Bel-
mont, Cal.: Wadsworth and Brooks/Cole.
Briscoe, Ted &amp; John Carroll (1997). Automatic extraction
of subcategorization from corpora. In Proceedings of the
5th Conference on Applied Natural Language Processing,
Washington, D.C., 31 March – 3 April 1997, pp. 356–363.
Byron, Donna K. (2002). Resolving pronominal reference to
abstract entities. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, Philadel-
phia, Penn., 7–12 July 2002, pp. 80–87.
Byron, Donna K. &amp; James F. Allen (1998). Resolving demon-
strative pronouns in the TRAINS93 corpus. In New Ap-
proaches to Discourse Anaphora: Proceedings of the Sec-
ond Colloquium on Discourse Anaphora and Anaphor Res-
olution (DAARC2), pp. 68–81.
Dagan, Ido &amp; Alon Itai (1991). A statistical filter for resolv-
ing pronoun references. In Y.A. Feldman &amp; A. Bruckstein
(Eds.), Artificial Intelligence and Computer Vision, pp. 125–
135. Amsterdam: Elsevier.
Eckert, Miriam &amp; Michael Strube (2000). Dialogue acts, syn-
chronising units and anaphora resolution. Journal of Seman-
tics, 17(1):51–89.
Ihaka, Ross &amp; Robert Gentleman (1996). R: A language for
data analysis and graphics. Journal of Computational and
Graphical Statistics, 5:299–314.
Kohavi, Ron &amp; George H. John (1997). Wrappers for fea-
ture subset selection. Artificial Intelligence Journal, 97(1-
2):273–324.
M¨uller, Christoph, Stefan Rapp &amp; Michael Strube (2002). Ap-
plying Co-Training to reference resolution. In Proceedings
of the 40th Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia, Penn., 7–12 July 2002, pp.
352–359.
Ng, Vincent &amp; Claire Cardie (2002). Improving machine learn-
ing approaches to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia, Penn., 7–12 July 2002, pp.
104–111.
Paul, Michael, Kazuhide Yamamoto &amp; Eiichiro Sumita (1999).
Corpus-based anaphora resolution towards antecedent pref-
erence. In Proc. of the 37th ACL, Workshop Coreference and
Its Applications, College Park, Md., 1999, pp. 47–52.
Soon, Wee Meng, Hwee Tou Ng &amp; Daniel Chung Yong Lim
(2001). A machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics, 27(4):521–
544.
Strube, Michael, Stefan Rapp &amp; Christoph M¨uller (2002). The
influence of minimum edit distance on reference resolution.
In Proceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing, Philadelphia, Pa., 6–7
July 2002, pp. 312–319.
Tetrault, Joel R. (2001). A corpus-based evaluation of cen-
tering and pronoun resolution. Computational Linguistics,
27(4):507–520.
Therneau, Terry M. &amp; Elizabeth J. Atkinson (1997). An intro-
duction to recursive partitioning using the RPART routines.
Technical Report: Mayo Foundation. Distributed with the
RPART package.
Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly &amp;
Lynette Hirschman (1995). A model-theoretic coreference
scoring scheme. In Proceedings of the 6th Message Under-
standing Conference (MUC-6), pp. 45–52. San Mateo, Cal.:
Morgan Kaufmann.
Webber, Bonnie L. (1991). Structure and ostension in the inter-
pretation of discourse deixis. Language and Cognitive Pro-
cesses, 6(2):107–135.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510096">
<title confidence="0.905832666666667">A Machine Learning Approach to Pronoun Resolution in Spoken Dialogue Strube M¨uller European Media Laboratory GmbH</title>
<author confidence="0.895413">Villa Bosch</author>
<address confidence="0.866606">Schloß-Wolfsbrunnenweg 33 69118 Heidelberg, Germany</address>
<email confidence="0.956502">michael.strube|christoph.mueller@eml.villa-bosch.de</email>
<abstract confidence="0.996774454545454">We apply a decision tree based approach to pronoun resolution in spoken dialogue. Our system deals with pronouns with NPand non-NP-antecedents. We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features. We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron’s (2002) manually tuned system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>Reference to Abstract Objects in Discourse.</title>
<date>1993</date>
<publisher>Kluwer.</publisher>
<location>Dordrecht, The Netherlands:</location>
<contexts>
<context position="3391" citStr="Asher, 1993" startWordPosition="503" endWordPosition="504">orpus-based anaphora resolution dealing also with non-NP-antecedents. 2 NP- vs. Non-NP-Antecedents Spoken dialogue contains more pronouns with nonNP-antecedents than written text does. However, pronouns with NP-antecedents (like 3rd pers. masculine/feminine pronouns, cf. he in the example below) still constitute the largest fraction of all coreferential pronouns in the Switchboard corpus. In spoken dialogue there are considerable numbers of pronouns that pick up different kinds of abstract objects from the previous discourse, e.g. events, states, concepts, propositions or facts (Webber, 1991; Asher, 1993). These anaphors then have VP-antecedents (“it ” in (B6) below) or sentential antecedents (“that ” in (B5)). A1: ... [he] ’s nine months old.... A2: [He] likes to dig around a little bit. A3: [His] mother comes in and says, why did you let [him] [play in the dirt] , A:4 I guess [[he] ’s enjoying himself] . B5: [That] ’s right. B&amp; [It] ’s healthy, .. . A major problem for pronoun resolution in spoken dialogue is the large number of personal and demonstrative pronouns which are either not referential at all (e.g. expletive pronouns) or for which a particular antecedent cannot easily be determine</context>
</contexts>
<marker>Asher, 1993</marker>
<rawString>Asher, Nicholas (1993). Reference to Abstract Objects in Discourse. Dordrecht, The Netherlands: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
<author>Jerome H Friedman</author>
<author>Charles J Stone</author>
<author>R A Olshen</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<location>Belmont, Cal.: Wadsworth</location>
<contexts>
<context position="15489" citStr="Breiman et al., 1984" startWordPosition="2542" endWordPosition="2545">uency of the word divided by the total number of words in all 553 dialogues. The average IC value was then calculated as the IC sum of all words in the markable, divided by the number of words in the markable. Finally, the feature wdist ic (25) measures the word-based distance between two expressions. It does so in terms of the sum of the individual words’ IC. The calculation of the IC was done as described for the ante ic feature. 5 Experiments and Results 5.1 Experimental Setup All experiments were performed using the decision tree learner RPART (Therneau &amp; Atkinson, 1997), which is a CART (Breiman et al., 1984) reimplementation for the S-Plus and R statistical computing environments (we use R, Ihaka &amp; Gentleman (1996), see http://www.r-project.org). We used the standard pruning and control settings for RPART (cp=0.0001, minsplit=20, minbucket=7). All results reported were obtained by performing 20-fold crossvalidation. In the prediction phase, the trained classifier is exposed to unlabeled instances of test data. The classifier’s task is to label each instance. When an instance is labeled as coreferring, the IDs of the anaphor and antecedent are kept in a response list for the evaluation according t</context>
</contexts>
<marker>Breiman, Friedman, Stone, Olshen, 1984</marker>
<rawString>Breiman, Leo, Jerome H. Friedman, Charles J. Stone &amp; R.A. Olshen (1984). Classification and Regression Trees. Belmont, Cal.: Wadsworth and Brooks/Cole.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<volume>3</volume>
<pages>356--363</pages>
<location>Washington, D.C., 31</location>
<contexts>
<context position="13377" citStr="Briscoe &amp; Carroll (1997)" startWordPosition="2182" endWordPosition="2185">ist contains up to three entries giving the absolute count of cases where the verb has a direct argument of type NP, VP or S. When the verb list was produced, pronominal arguments were ignored. The features mdist 3mf3p and mdist 3n (21, 22) are refinements of the mdist feature. They measure the distance in markables between antecedent and anaphor, but in doing so they take the agreement value of the anaphor into account. For anaphors with an agreement value of 3mf or 3p, mdist 3mf3p is measured as D = 1 + the num3It seemed preferable to compile our own list instead of using existing ones like Briscoe &amp; Carroll (1997). ber of NP-markables between anaphor and potential antecedent. Anaphors with an agreement value of 3n, (i.e. it or that), on the other hand, potentially have non-NP-antecedents, so mdist 3n is measured as D + the number of anaphorically accessible4 Sand VP-markables between anaphor and potential antecedent. The feature ante tfifd (23) is supposed to capture the relative importance of an expression for a dialogue. The underlying assumption is that the higher the importance of a non-NP expression, the higher the probability of its being referred back to. For our purposes, we calculated TF for e</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Briscoe, Ted &amp; John Carroll (1997). Automatic extraction of subcategorization from corpora. In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington, D.C., 31 March – 3 April 1997, pp. 356–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna K Byron</author>
</authors>
<title>Resolving pronominal reference to abstract entities.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<location>Philadelphia, Penn.,</location>
<contexts>
<context position="4229" citStr="Byron (2002)" startWordPosition="653" endWordPosition="654">y did you let [him] [play in the dirt] , A:4 I guess [[he] ’s enjoying himself] . B5: [That] ’s right. B&amp; [It] ’s healthy, .. . A major problem for pronoun resolution in spoken dialogue is the large number of personal and demonstrative pronouns which are either not referential at all (e.g. expletive pronouns) or for which a particular antecedent cannot easily be determined by humans (called vague anaphors by Eckert &amp; Strube (2000)). In the following example, the “that ” in utterance (A3) refers back to utterance (A1). As for the first two pronouns in (B4), following Eckert &amp; Strube (2000) and Byron (2002) we assume that referring expressions in disfluencies, abandoned utterances etc. are excluded from the resolution. The third pronoun in (B4) is an expletive. The pronoun in (A5) is different in that it is indeed referential: it refers back to“that ” from (A3). A1: ... [There is a lot of theft, a lot of assault dealing with, uh, people trying to get money for drugs. ] B2: Yeah. A3: And, uh, I think [that ]’s a national problem, though. B4: It, it, it’s pretty bad here, too. A5: [It ]’s not unique ... Pronoun resolution in spoken dialogue also has to deal with the whole range of difficulties tha</context>
<context position="12307" citStr="Byron (2002)" startWordPosition="1999" endWordPosition="2000">edent in sentences 16. syn par anaphor and antecedent have the same grammatical function (yes, no) Features introduced for spoken dialogue 17. ante exp type type of antecedent (NP, S, VP) 18. ana np pref preference for NP arguments 19. ana vp pref preference for VP arguments 20. ana s pref preference for S arguments 21. mdist 3mf3p (see text) 22. mdist 3n (see text) 23. ante tfidf (see text) 24. ante ic (see text) 25. wdist ic (see text) Table 3: Our Features s pref (18, 19, 20) describe a verb’s preference for arguments of a particular type. Inspired by the work of Eckert &amp; Strube (2000) and Byron (2002), these features capture preferences for NP- or nonNP-antecedents by taking a pronoun’s predicative context into account. The underlying assumption is that if a verb preceding a personal or demonstrative pronoun preferentially subcategorizes sentences or VPs, then the pronoun will be likely to have a nonNP-antecedent. The features are based on a verb list compiled from 553 Switchboard dialogues.3 For every verb occurring in the corpus, this list contains up to three entries giving the absolute count of cases where the verb has a direct argument of type NP, VP or S. When the verb list was produ</context>
<context position="21446" citStr="Byron (2002)" startWordPosition="3530" endWordPosition="3531">ment over the baseline. Table 7 shows the results for the combined classifiers. The improvement in f-measure is due to the increase in recall while the precision shows only a slight decrease. Though some of the features of the baseline model (features 1-16) still occur in the decision tree learned, the feature ante exp type divides major parts of the tree quite nicely (see Figure 1). Below that node the feature ana npform is used to distinguish between negative (personal pronouns) and potential positive cases (demonstrative pronouns). This confirms the hypothesis by Eckert &amp; Strube (2000) and Byron (2002) to give high priority to these features. The decision tree fragment in Figure 1 correctly assigns the P label to 23-7=16 sentential antecedents. split, n, loss, yval * denotes terminal node ... anteexptype=s,vp 1110 55 N ananpform=prp 747,11 N * ananpform=dtpro 363 44 N anteexptype=vp 177 3 N * anteexptype=s 186 41 N udist&gt;=1.5 95 14 N * udist&lt;1.5 91 27 N wdistic&lt;43.32 33 4 N * wdistic&gt;=43.32 58 23 N anasdepth&gt;=2.5 23 4 N * anasdepth&lt;2.5 35 16 N wdistic&gt;=63.62 24 11 N wdistic&lt;80.60 12 3 N * wdistic&gt;=80.60 12 4 P * wdistic&lt;63.62 11 3 P * Figure 1: Decision Tree Fragment However, the most impor</context>
<context position="23217" citStr="Byron (2002)" startWordPosition="3833" endWordPosition="3834">le 6: Results for Third Person Plural Pronouns (3p) Table 7: Combined Results for All Pronouns correct total found total correct precision recall f-measure found baseline, features 1-16 456 739 1250 61.71 36.48 45.85 combined 509 897 1250 56.74 40.72 47.42 they are vague pronouns). The overall numbers for precision, recall and fmeasure are fairly low. One reason is that we did not attempt to resolve anaphoric definite NPs and proper names though these coreference links are contained in the evaluation key list. If we removed them from there, the recall of our experiments would approach the 51% Byron (2002) mentioned for her system using only domain-independent semantic restrictions. 6 Comparison to Related Work Our approach for determining the feature set for pronoun resolution resembles the so-called wrapper approach for feature selection (Kohavi &amp; John, 1997). This is in contrast to the majority of other work on feature selection for anaphora resolution, which was hardly ever done systematically. E.g. Soon et al. (2001) only compared baseline systems consisting of one feature each, only three of which yielded an f-measure greater than zero. Then they combined these features and achieved resul</context>
<context position="24729" citStr="Byron (2002)" startWordPosition="4070" endWordPosition="4071">n features. They admit that this method is quite subjective. Corpus-based work about pronoun resolution in spoken dialogue is almost non-existent. However, there are a few papers dealing with neuter pronouns with NP-antecedents. E.g., Dagan &amp; Itai (1991) presented a corpus-based approach to the resolution of the pronoun it, but they use a written text corpus and do not mention non-NP-antecedents at all. Paul et al. (1999) presented a corpus-based anaphora resolution algorithm for spoken dialogue. For their experiments, however, they restricted anaphoric relations to those with NP-antecedents. Byron (2002) presented a symbolic approach which resolves pronouns with NP- and non-NPantecedents in spoken dialogue in the TRAINS domain. Byron extends a pronoun resolution algorithm (Tetrault, 2001) with semantic filtering, thus enabling it to resolve anaphors with non-NPantecedents as well. Semantic filtering relies on knowledge about semantic restrictions associated with verbs, like semantic compatibility between subject and predicative noun or predicative adjective. An evaluation on ten TRAINS93 dialogues with 80 3rd person pronouns and 100 demonstrative pronouns shows that semantic filtering and the</context>
<context position="26296" citStr="Byron (2002)" startWordPosition="4309" endWordPosition="4310"> What is problematic with her approach is that she assumes the input to her algorithm to be only referential pronouns. This simplifies the task considerably. 7 Conclusions and Future Work We presented a machine learning approach to pronoun resolution in spoken dialogue. We built upon a system we used for anaphora resolution in written text and extended it with a set of features designed for spoken dialogue. We refined distance features and used metrics from information retrieval for determining non-NP-antecedents. Inspired by the more linguistically oriented work by Eckert &amp; Strube (2000) and Byron (2002) we also evaluated the contribution of features which used the predicative context of the pronoun to be resolved. However, these features did not show up in the final models since they did not lead to an improvement. Instead, rather simple distance metrics were preferred. While we were (almost) satisfied with the performance of these features, the major problem for a spoken dialogue pronoun resolution algorithm is the abundance of pronouns without antecedents. Previous research could avoid dealing with this phenomenon by either applying the algorithm by hand (Eckert &amp; Strube, 2000) or excludin</context>
</contexts>
<marker>Byron, 2002</marker>
<rawString>Byron, Donna K. (2002). Resolving pronominal reference to abstract entities. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, Penn., 7–12 July 2002, pp. 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna K Byron</author>
<author>James F Allen</author>
</authors>
<title>Resolving demonstrative pronouns in the TRAINS93 corpus.</title>
<date>1998</date>
<booktitle>In New Approaches to Discourse Anaphora: Proceedings of the Second Colloquium on Discourse Anaphora and Anaphor Resolution (DAARC2),</booktitle>
<pages>68--81</pages>
<contexts>
<context position="1747" citStr="Byron &amp; Allen (1998)" startWordPosition="258" endWordPosition="261">tten text to spoken dialogue. This paper describes the extensions and adaptations needed for applying our anaphora resolution system (M¨uller et al., 2002; Strube et al., 2002) to pronoun resolution in spoken dialogue. There are important differences between written text and spoken dialogue which have to be accounted for. The most obvious difference is that in spoken dialogue there is an abundance of (personal and demonstrative) pronouns with non-NP-antecedents or no antecedents at all. Corpus studies have shown that a significant amount of pronouns in spoken dialogue have non-NP-antecedents: Byron &amp; Allen (1998) report that about 50% of the pronouns in the TRAINS93 corpus have non-NP-antecedents. Eckert &amp; Strube (2000) note that only about 45% of the pronouns in a set of Switchboard dialogues have NP-antecedents. The remainder consists of 22% which have non-NP-antecedents and 33% without antecedents. These studies suggest that the performance of a pronoun resolution algorithm can be improved considerably by enabling it to resolve also pronouns with non-NP-antecedents. Because of the difficulties a pronoun resolution algorithm encounters in spoken dialogue, previous approaches were applied only to tin</context>
</contexts>
<marker>Byron, Allen, 1998</marker>
<rawString>Byron, Donna K. &amp; James F. Allen (1998). Resolving demonstrative pronouns in the TRAINS93 corpus. In New Approaches to Discourse Anaphora: Proceedings of the Second Colloquium on Discourse Anaphora and Anaphor Resolution (DAARC2), pp. 68–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>A statistical filter for resolving pronoun references.</title>
<date>1991</date>
<booktitle>In Y.A. Feldman &amp; A. Bruckstein (Eds.), Artificial Intelligence and Computer Vision,</booktitle>
<pages>125--135</pages>
<publisher>Elsevier.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="24371" citStr="Dagan &amp; Itai (1991)" startWordPosition="4013" endWordPosition="4016">r than zero. Then they combined these features and achieved results which were close to the best overall results they report. While this tells us which features contribute a lot, it does not give any information about potential (positive or negative) influence of the rest. Ng &amp; Cardie (2002) select the set of features by hand, giving a preference to high precision features. They admit that this method is quite subjective. Corpus-based work about pronoun resolution in spoken dialogue is almost non-existent. However, there are a few papers dealing with neuter pronouns with NP-antecedents. E.g., Dagan &amp; Itai (1991) presented a corpus-based approach to the resolution of the pronoun it, but they use a written text corpus and do not mention non-NP-antecedents at all. Paul et al. (1999) presented a corpus-based anaphora resolution algorithm for spoken dialogue. For their experiments, however, they restricted anaphoric relations to those with NP-antecedents. Byron (2002) presented a symbolic approach which resolves pronouns with NP- and non-NPantecedents in spoken dialogue in the TRAINS domain. Byron extends a pronoun resolution algorithm (Tetrault, 2001) with semantic filtering, thus enabling it to resolve </context>
</contexts>
<marker>Dagan, Itai, 1991</marker>
<rawString>Dagan, Ido &amp; Alon Itai (1991). A statistical filter for resolving pronoun references. In Y.A. Feldman &amp; A. Bruckstein (Eds.), Artificial Intelligence and Computer Vision, pp. 125– 135. Amsterdam: Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Eckert</author>
<author>Michael Strube</author>
</authors>
<title>Dialogue acts, synchronising units and anaphora resolution.</title>
<date>2000</date>
<journal>Journal of Semantics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="1856" citStr="Eckert &amp; Strube (2000)" startWordPosition="275" endWordPosition="279">naphora resolution system (M¨uller et al., 2002; Strube et al., 2002) to pronoun resolution in spoken dialogue. There are important differences between written text and spoken dialogue which have to be accounted for. The most obvious difference is that in spoken dialogue there is an abundance of (personal and demonstrative) pronouns with non-NP-antecedents or no antecedents at all. Corpus studies have shown that a significant amount of pronouns in spoken dialogue have non-NP-antecedents: Byron &amp; Allen (1998) report that about 50% of the pronouns in the TRAINS93 corpus have non-NP-antecedents. Eckert &amp; Strube (2000) note that only about 45% of the pronouns in a set of Switchboard dialogues have NP-antecedents. The remainder consists of 22% which have non-NP-antecedents and 33% without antecedents. These studies suggest that the performance of a pronoun resolution algorithm can be improved considerably by enabling it to resolve also pronouns with non-NP-antecedents. Because of the difficulties a pronoun resolution algorithm encounters in spoken dialogue, previous approaches were applied only to tiny domains, they needed deep semantic analysis and discourse processing and relied on hand-crafted knowledge b</context>
<context position="4051" citStr="Eckert &amp; Strube (2000)" startWordPosition="619" endWordPosition="622">nts (“it ” in (B6) below) or sentential antecedents (“that ” in (B5)). A1: ... [he] ’s nine months old.... A2: [He] likes to dig around a little bit. A3: [His] mother comes in and says, why did you let [him] [play in the dirt] , A:4 I guess [[he] ’s enjoying himself] . B5: [That] ’s right. B&amp; [It] ’s healthy, .. . A major problem for pronoun resolution in spoken dialogue is the large number of personal and demonstrative pronouns which are either not referential at all (e.g. expletive pronouns) or for which a particular antecedent cannot easily be determined by humans (called vague anaphors by Eckert &amp; Strube (2000)). In the following example, the “that ” in utterance (A3) refers back to utterance (A1). As for the first two pronouns in (B4), following Eckert &amp; Strube (2000) and Byron (2002) we assume that referring expressions in disfluencies, abandoned utterances etc. are excluded from the resolution. The third pronoun in (B4) is an expletive. The pronoun in (A5) is different in that it is indeed referential: it refers back to“that ” from (A3). A1: ... [There is a lot of theft, a lot of assault dealing with, uh, people trying to get money for drugs. ] B2: Yeah. A3: And, uh, I think [that ]’s a national </context>
<context position="12290" citStr="Eckert &amp; Strube (2000)" startWordPosition="1994" endWordPosition="1997">e between anaphor and antecedent in sentences 16. syn par anaphor and antecedent have the same grammatical function (yes, no) Features introduced for spoken dialogue 17. ante exp type type of antecedent (NP, S, VP) 18. ana np pref preference for NP arguments 19. ana vp pref preference for VP arguments 20. ana s pref preference for S arguments 21. mdist 3mf3p (see text) 22. mdist 3n (see text) 23. ante tfidf (see text) 24. ante ic (see text) 25. wdist ic (see text) Table 3: Our Features s pref (18, 19, 20) describe a verb’s preference for arguments of a particular type. Inspired by the work of Eckert &amp; Strube (2000) and Byron (2002), these features capture preferences for NP- or nonNP-antecedents by taking a pronoun’s predicative context into account. The underlying assumption is that if a verb preceding a personal or demonstrative pronoun preferentially subcategorizes sentences or VPs, then the pronoun will be likely to have a nonNP-antecedent. The features are based on a verb list compiled from 553 Switchboard dialogues.3 For every verb occurring in the corpus, this list contains up to three entries giving the absolute count of cases where the verb has a direct argument of type NP, VP or S. When the ve</context>
<context position="21429" citStr="Eckert &amp; Strube (2000)" startWordPosition="3525" endWordPosition="3528">dist ic leads to an improvement over the baseline. Table 7 shows the results for the combined classifiers. The improvement in f-measure is due to the increase in recall while the precision shows only a slight decrease. Though some of the features of the baseline model (features 1-16) still occur in the decision tree learned, the feature ante exp type divides major parts of the tree quite nicely (see Figure 1). Below that node the feature ana npform is used to distinguish between negative (personal pronouns) and potential positive cases (demonstrative pronouns). This confirms the hypothesis by Eckert &amp; Strube (2000) and Byron (2002) to give high priority to these features. The decision tree fragment in Figure 1 correctly assigns the P label to 23-7=16 sentential antecedents. split, n, loss, yval * denotes terminal node ... anteexptype=s,vp 1110 55 N ananpform=prp 747,11 N * ananpform=dtpro 363 44 N anteexptype=vp 177 3 N * anteexptype=s 186 41 N udist&gt;=1.5 95 14 N * udist&lt;1.5 91 27 N wdistic&lt;43.32 33 4 N * wdistic&gt;=43.32 58 23 N anasdepth&gt;=2.5 23 4 N * anasdepth&lt;2.5 35 16 N wdistic&gt;=63.62 24 11 N wdistic&lt;80.60 12 3 N * wdistic&gt;=80.60 12 4 P * wdistic&lt;63.62 11 3 P * Figure 1: Decision Tree Fragment Howeve</context>
<context position="26279" citStr="Eckert &amp; Strube (2000)" startWordPosition="4304" endWordPosition="4307"> achieved 51% success rate. What is problematic with her approach is that she assumes the input to her algorithm to be only referential pronouns. This simplifies the task considerably. 7 Conclusions and Future Work We presented a machine learning approach to pronoun resolution in spoken dialogue. We built upon a system we used for anaphora resolution in written text and extended it with a set of features designed for spoken dialogue. We refined distance features and used metrics from information retrieval for determining non-NP-antecedents. Inspired by the more linguistically oriented work by Eckert &amp; Strube (2000) and Byron (2002) we also evaluated the contribution of features which used the predicative context of the pronoun to be resolved. However, these features did not show up in the final models since they did not lead to an improvement. Instead, rather simple distance metrics were preferred. While we were (almost) satisfied with the performance of these features, the major problem for a spoken dialogue pronoun resolution algorithm is the abundance of pronouns without antecedents. Previous research could avoid dealing with this phenomenon by either applying the algorithm by hand (Eckert &amp; Strube, </context>
</contexts>
<marker>Eckert, Strube, 2000</marker>
<rawString>Eckert, Miriam &amp; Michael Strube (2000). Dialogue acts, synchronising units and anaphora resolution. Journal of Semantics, 17(1):51–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Ihaka</author>
<author>Robert Gentleman</author>
</authors>
<title>R: A language for data analysis and graphics.</title>
<date>1996</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<pages>5--299</pages>
<contexts>
<context position="15598" citStr="Ihaka &amp; Gentleman (1996)" startWordPosition="2560" endWordPosition="2563"> calculated as the IC sum of all words in the markable, divided by the number of words in the markable. Finally, the feature wdist ic (25) measures the word-based distance between two expressions. It does so in terms of the sum of the individual words’ IC. The calculation of the IC was done as described for the ante ic feature. 5 Experiments and Results 5.1 Experimental Setup All experiments were performed using the decision tree learner RPART (Therneau &amp; Atkinson, 1997), which is a CART (Breiman et al., 1984) reimplementation for the S-Plus and R statistical computing environments (we use R, Ihaka &amp; Gentleman (1996), see http://www.r-project.org). We used the standard pruning and control settings for RPART (cp=0.0001, minsplit=20, minbucket=7). All results reported were obtained by performing 20-fold crossvalidation. In the prediction phase, the trained classifier is exposed to unlabeled instances of test data. The classifier’s task is to label each instance. When an instance is labeled as coreferring, the IDs of the anaphor and antecedent are kept in a response list for the evaluation according to Vilain et al. (1995). For determining the relevant feature set we followed an iterative procedure similar t</context>
</contexts>
<marker>Ihaka, Gentleman, 1996</marker>
<rawString>Ihaka, Ross &amp; Robert Gentleman (1996). R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5:299–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kohavi</author>
<author>George H John</author>
</authors>
<title>Wrappers for feature subset selection.</title>
<date>1997</date>
<journal>Artificial Intelligence Journal,</journal>
<pages>97--1</pages>
<contexts>
<context position="16264" citStr="Kohavi &amp; John, 1997" startWordPosition="2665" endWordPosition="2668">tandard pruning and control settings for RPART (cp=0.0001, minsplit=20, minbucket=7). All results reported were obtained by performing 20-fold crossvalidation. In the prediction phase, the trained classifier is exposed to unlabeled instances of test data. The classifier’s task is to label each instance. When an instance is labeled as coreferring, the IDs of the anaphor and antecedent are kept in a response list for the evaluation according to Vilain et al. (1995). For determining the relevant feature set we followed an iterative procedure similar to the wrapper approach for feature selection (Kohavi &amp; John, 1997). We start with a model based on a set of predefined baseline features. Then we train models combining the baseline with all additional features separately. We choose the best performing feature (fmeasure according to Vilain et al. (1995)), adding it to the model. We then train classifiers combining the enhanced model with each of the remaining features separately. We again choose the best performing classifier and add the corresponding new feature to the model. This process is repeated as long as significant improvement can be observed. 5.2 Results In our experiments we split the data in thre</context>
<context position="23477" citStr="Kohavi &amp; John, 1997" startWordPosition="3870" endWordPosition="3873">7.42 they are vague pronouns). The overall numbers for precision, recall and fmeasure are fairly low. One reason is that we did not attempt to resolve anaphoric definite NPs and proper names though these coreference links are contained in the evaluation key list. If we removed them from there, the recall of our experiments would approach the 51% Byron (2002) mentioned for her system using only domain-independent semantic restrictions. 6 Comparison to Related Work Our approach for determining the feature set for pronoun resolution resembles the so-called wrapper approach for feature selection (Kohavi &amp; John, 1997). This is in contrast to the majority of other work on feature selection for anaphora resolution, which was hardly ever done systematically. E.g. Soon et al. (2001) only compared baseline systems consisting of one feature each, only three of which yielded an f-measure greater than zero. Then they combined these features and achieved results which were close to the best overall results they report. While this tells us which features contribute a lot, it does not give any information about potential (positive or negative) influence of the rest. Ng &amp; Cardie (2002) select the set of features by ha</context>
</contexts>
<marker>Kohavi, John, 1997</marker>
<rawString>Kohavi, Ron &amp; George H. John (1997). Wrappers for feature subset selection. Artificial Intelligence Journal, 97(1-2):273–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Stefan Rapp</author>
<author>Michael Strube</author>
</authors>
<title>Applying Co-Training to reference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>352--359</pages>
<location>Philadelphia, Penn.,</location>
<marker>M¨uller, Rapp, Strube, 2002</marker>
<rawString>M¨uller, Christoph, Stefan Rapp &amp; Michael Strube (2002). Applying Co-Training to reference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, Penn., 7–12 July 2002, pp. 352–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>Philadelphia, Penn.,</location>
<contexts>
<context position="845" citStr="Ng &amp; Cardie, 2002" startWordPosition="116" endWordPosition="119">eller @eml.villa-bosch.de Abstract We apply a decision tree based approach to pronoun resolution in spoken dialogue. Our system deals with pronouns with NPand non-NP-antecedents. We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features. We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron’s (2002) manually tuned system. 1 Introduction Corpus-based methods and machine learning techniques have been applied to anaphora resolution in written text with considerable success (Soon et al., 2001; Ng &amp; Cardie, 2002, among others). It has been demonstrated that systems based on these approaches achieve a performance that is comparable to hand-crafted systems. Since they can easily be applied to new domains it seems also feasible to port a given corpus-based anaphora resolution system from written text to spoken dialogue. This paper describes the extensions and adaptations needed for applying our anaphora resolution system (M¨uller et al., 2002; Strube et al., 2002) to pronoun resolution in spoken dialogue. There are important differences between written text and spoken dialogue which have to be accounted</context>
<context position="24044" citStr="Ng &amp; Cardie (2002)" startWordPosition="3963" endWordPosition="3966">pproach for feature selection (Kohavi &amp; John, 1997). This is in contrast to the majority of other work on feature selection for anaphora resolution, which was hardly ever done systematically. E.g. Soon et al. (2001) only compared baseline systems consisting of one feature each, only three of which yielded an f-measure greater than zero. Then they combined these features and achieved results which were close to the best overall results they report. While this tells us which features contribute a lot, it does not give any information about potential (positive or negative) influence of the rest. Ng &amp; Cardie (2002) select the set of features by hand, giving a preference to high precision features. They admit that this method is quite subjective. Corpus-based work about pronoun resolution in spoken dialogue is almost non-existent. However, there are a few papers dealing with neuter pronouns with NP-antecedents. E.g., Dagan &amp; Itai (1991) presented a corpus-based approach to the resolution of the pronoun it, but they use a written text corpus and do not mention non-NP-antecedents at all. Paul et al. (1999) presented a corpus-based anaphora resolution algorithm for spoken dialogue. For their experiments, ho</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Ng, Vincent &amp; Claire Cardie (2002). Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, Penn., 7–12 July 2002, pp. 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Kazuhide Yamamoto &amp; Eiichiro Sumita</title>
<date>1999</date>
<booktitle>In Proc. of the 37th ACL, Workshop Coreference and Its Applications,</booktitle>
<pages>47--52</pages>
<location>College Park, Md.,</location>
<marker>Paul, 1999</marker>
<rawString>Paul, Michael, Kazuhide Yamamoto &amp; Eiichiro Sumita (1999). Corpus-based anaphora resolution towards antecedent preference. In Proc. of the 37th ACL, Workshop Coreference and Its Applications, College Park, Md., 1999, pp. 47–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
</authors>
<title>Hwee Tou Ng &amp; Daniel Chung Yong Lim</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>544</pages>
<marker>Soon, 2001</marker>
<rawString>Soon, Wee Meng, Hwee Tou Ng &amp; Daniel Chung Yong Lim (2001). A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521– 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Stefan Rapp</author>
<author>Christoph M¨uller</author>
</authors>
<title>The influence of minimum edit distance on reference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>312--319</pages>
<location>Philadelphia, Pa.,</location>
<marker>Strube, Rapp, M¨uller, 2002</marker>
<rawString>Strube, Michael, Stefan Rapp &amp; Christoph M¨uller (2002). The influence of minimum edit distance on reference resolution. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Philadelphia, Pa., 6–7 July 2002, pp. 312–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetrault</author>
</authors>
<title>A corpus-based evaluation of centering and pronoun resolution.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="24917" citStr="Tetrault, 2001" startWordPosition="4099" endWordPosition="4100">g with neuter pronouns with NP-antecedents. E.g., Dagan &amp; Itai (1991) presented a corpus-based approach to the resolution of the pronoun it, but they use a written text corpus and do not mention non-NP-antecedents at all. Paul et al. (1999) presented a corpus-based anaphora resolution algorithm for spoken dialogue. For their experiments, however, they restricted anaphoric relations to those with NP-antecedents. Byron (2002) presented a symbolic approach which resolves pronouns with NP- and non-NPantecedents in spoken dialogue in the TRAINS domain. Byron extends a pronoun resolution algorithm (Tetrault, 2001) with semantic filtering, thus enabling it to resolve anaphors with non-NPantecedents as well. Semantic filtering relies on knowledge about semantic restrictions associated with verbs, like semantic compatibility between subject and predicative noun or predicative adjective. An evaluation on ten TRAINS93 dialogues with 80 3rd person pronouns and 100 demonstrative pronouns shows that semantic filtering and the implementation of different search strategies for personal and demonstrative pronouns yields a success rate of 72%. As Byron admits, the major limitation of her algorithm is its dependenc</context>
</contexts>
<marker>Tetrault, 2001</marker>
<rawString>Tetrault, Joel R. (2001). A corpus-based evaluation of centering and pronoun resolution. Computational Linguistics, 27(4):507–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry M Therneau</author>
<author>Elizabeth J Atkinson</author>
</authors>
<title>An introduction to recursive partitioning using the RPART routines. Technical Report: Mayo Foundation. Distributed with the RPART package.</title>
<date>1997</date>
<contexts>
<context position="15449" citStr="Therneau &amp; Atkinson, 1997" startWordPosition="2534" endWordPosition="2537">culated as the negative log of the total frequency of the word divided by the total number of words in all 553 dialogues. The average IC value was then calculated as the IC sum of all words in the markable, divided by the number of words in the markable. Finally, the feature wdist ic (25) measures the word-based distance between two expressions. It does so in terms of the sum of the individual words’ IC. The calculation of the IC was done as described for the ante ic feature. 5 Experiments and Results 5.1 Experimental Setup All experiments were performed using the decision tree learner RPART (Therneau &amp; Atkinson, 1997), which is a CART (Breiman et al., 1984) reimplementation for the S-Plus and R statistical computing environments (we use R, Ihaka &amp; Gentleman (1996), see http://www.r-project.org). We used the standard pruning and control settings for RPART (cp=0.0001, minsplit=20, minbucket=7). All results reported were obtained by performing 20-fold crossvalidation. In the prediction phase, the trained classifier is exposed to unlabeled instances of test data. The classifier’s task is to label each instance. When an instance is labeled as coreferring, the IDs of the anaphor and antecedent are kept in a resp</context>
</contexts>
<marker>Therneau, Atkinson, 1997</marker>
<rawString>Therneau, Terry M. &amp; Elizabeth J. Atkinson (1997). An introduction to recursive partitioning using the RPART routines. Technical Report: Mayo Foundation. Distributed with the RPART package.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, Cal.:</location>
<contexts>
<context position="16111" citStr="Vilain et al. (1995)" startWordPosition="2640" endWordPosition="2643">eimplementation for the S-Plus and R statistical computing environments (we use R, Ihaka &amp; Gentleman (1996), see http://www.r-project.org). We used the standard pruning and control settings for RPART (cp=0.0001, minsplit=20, minbucket=7). All results reported were obtained by performing 20-fold crossvalidation. In the prediction phase, the trained classifier is exposed to unlabeled instances of test data. The classifier’s task is to label each instance. When an instance is labeled as coreferring, the IDs of the anaphor and antecedent are kept in a response list for the evaluation according to Vilain et al. (1995). For determining the relevant feature set we followed an iterative procedure similar to the wrapper approach for feature selection (Kohavi &amp; John, 1997). We start with a model based on a set of predefined baseline features. Then we train models combining the baseline with all additional features separately. We choose the best performing feature (fmeasure according to Vilain et al. (1995)), adding it to the model. We then train classifiers combining the enhanced model with each of the remaining features separately. We again choose the best performing classifier and add the corresponding new fe</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly &amp; Lynette Hirschman (1995). A model-theoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pp. 45–52. San Mateo, Cal.: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie L Webber</author>
</authors>
<title>Structure and ostension in the interpretation of discourse deixis.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--2</pages>
<contexts>
<context position="3377" citStr="Webber, 1991" startWordPosition="500" endWordPosition="502">d system for corpus-based anaphora resolution dealing also with non-NP-antecedents. 2 NP- vs. Non-NP-Antecedents Spoken dialogue contains more pronouns with nonNP-antecedents than written text does. However, pronouns with NP-antecedents (like 3rd pers. masculine/feminine pronouns, cf. he in the example below) still constitute the largest fraction of all coreferential pronouns in the Switchboard corpus. In spoken dialogue there are considerable numbers of pronouns that pick up different kinds of abstract objects from the previous discourse, e.g. events, states, concepts, propositions or facts (Webber, 1991; Asher, 1993). These anaphors then have VP-antecedents (“it ” in (B6) below) or sentential antecedents (“that ” in (B5)). A1: ... [he] ’s nine months old.... A2: [He] likes to dig around a little bit. A3: [His] mother comes in and says, why did you let [him] [play in the dirt] , A:4 I guess [[he] ’s enjoying himself] . B5: [That] ’s right. B&amp; [It] ’s healthy, .. . A major problem for pronoun resolution in spoken dialogue is the large number of personal and demonstrative pronouns which are either not referential at all (e.g. expletive pronouns) or for which a particular antecedent cannot easil</context>
<context position="9881" citStr="Webber, 1991" startWordPosition="1601" endWordPosition="1602"> terms of agreement. 2We disregard the phenomenon of cataphor here. of the last two sentences was motivated pragmatically by considerations to keep the search space (and the number of instances) small. A sentence was considered valid if it was neither unfinished nor a backchannel utterance (like e.g. ”Uh-huh”, ”Yeah”, etc.). From the selected markables, inaccessible non-NP-expressions were automatically removed. We considered an expression inaccessible if it ended before the sentence in which it was contained. This was intended to be a rough approximation of the concept of the right frontier (Webber, 1991). The remaining expressions were then combined with the potential anaphor. Finally, the resulting pairs were labelled P or N and added to the instances generated with NP-antecedents. 4 Features We distinguish two classes of features: NP-level features specify e.g. the grammatical function, NP form, morpho-syntax, grammatical case and the depth of embedding in the syntactical structure. For these features, each instance contains one value for the antecedent and one for the anaphor. Coreference-level features, on the other hand, describe the relation between antecedent and anaphor in terms of e.</context>
<context position="14526" citStr="Webber, 1991" startWordPosition="2370" endWordPosition="2371">being referred back to. For our purposes, we calculated TF for every word by counting its frequency in each of our twenty Switchboard dialogues separately. The calculation of IDF was based on a set of 553 Switchboard dialogues. For every word, we calculated IDF as log(553/N ), with N =number of documents containing the word. For every non-NP-markable, an average TF*IDF value was calculated as the TF*IDF sum of all words comprising the markable, divided by the number of 4As mentioned earlier, the definition of accessibility of nonNP-antecedents is inspired by the concept of the right frontier (Webber, 1991). words in the markable. The feature ante ic (24) as an alternative to ante tfidf is based on the same assumptions as the former. The information content of a non-NP-markable is calculated as follows, based on a set of 553 Switchboard dialogues: For each word in the markable, the IC value was calculated as the negative log of the total frequency of the word divided by the total number of words in all 553 dialogues. The average IC value was then calculated as the IC sum of all words in the markable, divided by the number of words in the markable. Finally, the feature wdist ic (25) measures the </context>
</contexts>
<marker>Webber, 1991</marker>
<rawString>Webber, Bonnie L. (1991). Structure and ostension in the interpretation of discourse deixis. Language and Cognitive Processes, 6(2):107–135.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>