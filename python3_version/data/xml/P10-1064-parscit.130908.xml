<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000752">
<title confidence="0.92229">
Bridging SMT and TM with Translation Recommendation
</title>
<author confidence="0.996244">
Yifan He Yanjun Ma Josef van Genabith Andy Way
</author>
<affiliation confidence="0.990190666666667">
Centre for Next Generation Localisation
School of Computing
Dublin City University
</affiliation>
<email confidence="0.99796">
{yhe,yma,josef,away}@computing.dcu.ie
</email>
<sectionHeader confidence="0.994775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940761904762">
We propose a translation recommendation
framework to integrate Statistical Machine
Translation (SMT) output with Transla-
tion Memory (TM) systems. The frame-
work recommends SMT outputs to a TM
user when it predicts that SMT outputs are
more suitable for post-editing than the hits
provided by the TM. We describe an im-
plementation of this framework using an
SVM binary classifier. We exploit meth-
ods to fine-tune the classifier and inves-
tigate a variety of features of different
types. We rely on automatic MT evalua-
tion metrics to approximate human judge-
ments in our experiments. Experimental
results show that our system can achieve
0.85 precision at 0.89 recall, excluding ex-
act matches. Furthermore, it is possible for
the end-user to achieve a desired balance
between precision and recall by adjusting
confidence levels.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935666666667">
Recent years have witnessed rapid developments
in statistical machine translation (SMT), with con-
siderable improvements in translation quality. For
certain language pairs and applications, automated
translations are now beginning to be considered
acceptable, especially in domains where abundant
parallel corpora exist.
However, these advances are being adopted
only slowly and somewhat reluctantly in profes-
sional localization and post-editing environments.
Post-editors have long relied on translation memo-
ries (TMs) as the main technology assisting trans-
lation, and are understandably reluctant to give
them up. There are several simple reasons for
this: 1) TMs are useful; 2) TMs represent con-
siderable effort and investment by a company or
(even more so) an individual translator; 3) the
fuzzy match score used in TMs offers a good ap-
proximation of post-editing effort, which is useful
both for translators and translation cost estimation
and, 4) current SMT translation confidence esti-
mation measures are not as robust as TM fuzzy
match scores and professional translators are thus
not ready to replace fuzzy match scores with SMT
internal quality measures.
There has been some research to address this is-
sue, see e.g. (Specia et al., 2009a) and (Specia et
al., 2009b). However, to date most of the research
has focused on better confidence measures for MT,
e.g. based on training regression models to per-
form confidence estimation on scores assigned by
post-editors (cf. Section 2).
In this paper, we try to address the problem
from a different perspective. Given that most post-
editing work is (still) based on TM output, we pro-
pose to recommend MT outputs which are better
than TM hits to post-editors. In this framework,
post-editors still work with the TM while benefit-
ing from (better) SMT outputs; the assets in TMs
are not wasted and TM fuzzy match scores can
still be used to estimate (the upper bound of) post-
editing labor.
There are three specific goals we need to
achieve within this framework. Firstly, the rec-
ommendation should have high precision, other-
wise it would be confusing for post-editors and
may negatively affect the lower bound of the post-
editing effort. Secondly, although we have full
access to the SMT system used in this paper,
our method should be able to generalize to cases
where SMT is treated as a black-box, which is of-
</bodyText>
<page confidence="0.966486">
622
</page>
<note confidence="0.9438955">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622–630,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999900086956522">
ten the case in the translation industry. Finally,
post-editors should be able to easily adjust the rec-
ommendation threshold to particular requirements
without having to retrain the model.
In our framework, we recast translation recom-
mendation as a binary classification (rather than
regression) problem using SVMs, perform RBF
kernel parameter optimization, employ posterior
probability-based confidence estimation to sup-
port user-based tuning for precision and recall, ex-
periment with feature sets involving MT-, TM- and
system-independent features, and use automatic
MT evaluation metrics to simulate post-editing ef-
fort.
The rest of the paper is organized as follows: we
first briefly introduce related research in Section 2,
and review the classification SVMs in Section 3.
We formulate the classification model in Section 4
and present experiments in Section 5. In Section
6, we analyze the post-editing effort approximated
by the TER metric (Snover et al., 2006). Section
7 concludes the paper and points out avenues for
future research.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999944333333334">
Previous research relating to this work mainly fo-
cuses on predicting the MT quality.
The first strand is confidence estimation for MT,
initiated by (Ueffing et al., 2003), in which pos-
terior probabilities on the word graph or N-best
list are used to estimate the quality of MT out-
puts. The idea is explored more comprehensively
in (Blatz et al., 2004). These estimations are often
used to rerank the MT output and to optimize it
directly. Extensions of this strand are presented
in (Quirk, 2004) and (Ueffing and Ney, 2005).
The former experimented with confidence esti-
mation with several different learning algorithms;
the latter uses word-level confidence measures to
determine whether a particular translation choice
should be accepted or rejected in an interactive
translation system.
The second strand of research focuses on com-
bining TM information with an SMT system, so
that the SMT system can produce better target lan-
guage output when there is an exact or close match
in the TM (Simard and Isabelle, 2009). This line
of research is shown to help the performance of
MT, but is less relevant to our task in this paper.
A third strand of research tries to incorporate
confidence measures into a post-editing environ-
ment. To the best of our knowledge, the first paper
in this area is (Specia et al., 2009a). Instead of
modeling on translation quality (often measured
by automatic evaluation scores), this research uses
regression on both the automatic scores and scores
assigned by post-editors. The method is improved
in (Specia et al., 2009b), which applies Inductive
Confidence Machines and a larger set of features
to model post-editors’ judgement of the translation
quality between ‘good’ and ‘bad’, or among three
levels of post-editing effort.
Our research is more similar in spirit to the third
strand. However, we use outputs and features from
the TM explicitly; therefore instead of having to
solve a regression problem, we only have to solve
a much easier binary prediction problem which
can be integrated into TMs in a straightforward
manner. Because of this, the precision and recall
scores reported in this paper are not directly com-
parable to those in (Specia et al., 2009b) as the lat-
ter are computed on a pure SMT system without a
TM in the background.
</bodyText>
<sectionHeader confidence="0.80428" genericHeader="method">
3 Support Vector Machines for
Translation Quality Estimation
</sectionHeader>
<bodyText confidence="0.999488">
SVMs (Cortes and Vapnik, 1995) are binary clas-
sifiers that classify an input instance based on de-
cision rules which minimize the regularized error
function in (1):
</bodyText>
<equation confidence="0.99940825">
�WTW + C
1
s. t. yi(WTO(Xi) + b) &gt; 1 − ξi
ξi &gt; 0
</equation>
<bodyText confidence="0.9988751">
where (xi, yi) E Rn x {+1, −1} are l training
instances that are mapped by the function ϕ to a
higher dimensional space. w is the weight vec-
tor, ξ is the relaxation variable and C &gt; 0 is the
penalty parameter.
Solving SVMs is viable using the ‘kernel
trick’: finding a kernel function K in (1) with
K(xi,xj) _ b(xi)T b(xj). We perform our ex-
periments with the Radial Basis Function (RBF)
kernel, as /in (2):
</bodyText>
<equation confidence="0.932782">
K(Xi, Xj) = exp(−γ||Xi − Xj||2), γ &gt; 0 (2)
</equation>
<bodyText confidence="0.9976184">
When using SVMs with the RBF kernel, we
have two free parameters to tune on: the cost pa-
rameter C in (1) and the radius parameter -y in (2).
In each of our experimental settings, the param-
eters C and -y are optimized by a brute-force grid
</bodyText>
<equation confidence="0.924124333333333">
min
w,b,ξ
ξi
∑l
i=1
(1)
</equation>
<page confidence="0.993289">
623
</page>
<bodyText confidence="0.998624">
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
</bodyText>
<sectionHeader confidence="0.8138545" genericHeader="method">
4 Translation Recommendation as
Binary Classification
</sectionHeader>
<bodyText confidence="0.9999875">
We use an SVM binary classifier to predict the rel-
ative quality of the SMT output to make a recom-
mendation. The SVM classifier uses features from
the SMT system, the TM and additional linguis-
tic features to estimate whether the SMT output is
better than the hit from the TM.
</bodyText>
<subsectionHeader confidence="0.997874">
4.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.999876571428571">
As we treat translation recommendation as a bi-
nary classification problem, we have a pair of out-
puts from TM and MT for each sentence. Ideally
the classifier will recommend the output that needs
less post-editing effort. As large-scale annotated
data is not yet available for this task, we use auto-
matic TER scores (Snover et al., 2006) as the mea-
sure for the required post-editing effort. In the fu-
ture, we hope to train our system on HTER (TER
with human targeted references) scores (Snover et
al., 2006) once the necessary human annotations
are in place. In the meantime we use TER, as TER
is shown to have high correlation with HTER.
We label the training examples as in (3):
</bodyText>
<equation confidence="0.988337">
{
= +1 if TER(MT) &lt; TER(TM)
y
−1 if TER(MT) &gt; TER(TM) (3)
</equation>
<bodyText confidence="0.994908333333333">
Each instance is associated with a set of features
from both the MT and TM outputs, which are dis-
cussed in more detail in Section 4.3.
</bodyText>
<subsectionHeader confidence="0.997597">
4.2 Recommendation Confidence Estimation
</subsectionHeader>
<bodyText confidence="0.999884357142857">
In classical settings involving SVMs, confidence
levels are represented as margins of binary predic-
tions. However, these margins provide little in-
sight for our application because the numbers are
only meaningful when compared to each other.
What is more preferable is a probabilistic confi-
dence score (e.g. 90% confidence) which is better
understood by post-editors and translators.
We use the techniques proposed by (Platt, 1999)
and improved by (Lin et al., 2007) to obtain the
posterior probability of a classification, which is
used as the confidence score in our system.
Platt’s method estimates the posterior probabil-
ity with a sigmod function, as in (4):
</bodyText>
<equation confidence="0.9985845">
1
Pr(y = 1|x) � PA,B(f) =1 + exp(Af + B) (4)
</equation>
<bodyText confidence="0.99991675">
where f = f(x) is the decision function of the
estimated SVM. A and B are parameters that min-
imize the cross-entropy error function F on the
training data, as in Eq. (5):
</bodyText>
<equation confidence="0.9298785">
(tilog(pi) + (1 − ti)log(1 − pi)),
{N++1
N++2 if yi = +1
where pi = PA,B(fi), and ti = 1
N_+2 if yi = −1
(5)
</equation>
<bodyText confidence="0.9999776">
where z = (A, B) is a parameter setting, and
N+ and N_ are the numbers of observed positive
and negative examples, respectively, for the label
yi. These numbers are obtained using an internal
cross-validation on the training set.
</bodyText>
<subsectionHeader confidence="0.998755">
4.3 The Feature Set
</subsectionHeader>
<bodyText confidence="0.999663333333333">
We use three types of features in classification: the
MT system features, the TM feature and system-
independent features.
</bodyText>
<subsectionHeader confidence="0.839947">
4.3.1 The MT System Features
</subsectionHeader>
<bodyText confidence="0.9999908">
These features include those typically used in
SMT, namely the phrase-translation model scores,
the language model probability, the distance-based
reordering score, the lexicalized reordering model
scores, and the word penalty.
</bodyText>
<subsectionHeader confidence="0.70784">
4.3.2 The TM Feature
</subsectionHeader>
<bodyText confidence="0.9999271">
The TM feature is the fuzzy match (Sikes, 2007)
cost of the TM hit. The calculation of fuzzy match
score itself is one of the core technologies in TM
systems and varies among different vendors. We
compute fuzzy match cost as the minimum Edit
Distance (Levenshtein, 1966) between the source
and TM entry, normalized by the length of the
source as in (6), as most of the current implemen-
tations are based on edit distance while allowing
some additional flexible matching.
</bodyText>
<equation confidence="0.9923735">
EditDistance(s, e) (6)
Len(s)
</equation>
<bodyText confidence="0.999894625">
where s is the source side of t, the sentence to
translate, and a is the source side of an entry in the
TM. For fuzzy match scores F, this fuzzy match
cost hf,,t roughly corresponds to 1−F. The differ-
ence in calculation does not influence classifica-
tion, and allows direct comparison between a pure
TM system and a translation recommendation sys-
tem in Section 5.4.2.
</bodyText>
<equation confidence="0.989650285714286">
∑�
i=1
F(z) = −
min
z=(A,B)
hf,,,,(t) = min
e
</equation>
<page confidence="0.98544">
624
</page>
<subsubsectionHeader confidence="0.500733">
4.3.3 System-Independent Features
</subsubsectionHeader>
<bodyText confidence="0.996872936170213">
We use several features that are independent of
the translation system, which are useful when a
third-party translation service is used or the MT
system is simply treated as a black-box. These
features are source and target side LM scores,
pseudo source fuzzy match scores and IBM model
1 scores.
Source-Side Language Model Score and Per-
plexity. We compute the language model (LM)
score and perplexity of the input source sentence
on a LM trained on the source-side training data of
the SMT system. The inputs that have lower per-
plexity or higher LM score are more similar to the
dataset on which the SMT system is built.
Target-Side Language Model Perplexity. We
compute the LM probability and perplexity of the
target side as a measure of fluency. Language
model perplexity of the MT outputs are calculated,
and LM probability is already part of the MT sys-
tems scores. LM scores on TM outputs are also
computed, though they are not as informative as
scores on the MT side, since TM outputs should
be grammatically perfect.
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM system performs
well enough, these two sentences should be the
same or very similar. Therefore, the fuzzy match
score here gives an estimation of the confidence
level of the output. We compute this score for both
the MT output and the TM hit.
The IBM Model 1 Score. The fuzzy match
score does not measure whether the hit could be
a correct translation, i.e. it does not take into ac-
count the correspondence between the source and
target, but rather only the source-side information.
For the TM hit, the IBM Model 1 score (Brown
et al., 1993) serves as a rough estimation of how
good a translation it is on the word level; for the
MT output, on the other hand, it is a black-box
feature to estimate translation quality when the in-
formation from the translation model is not avail-
able. We compute bidirectional (source-to-target
and target-to-source) model 1 scores on both TM
and MT outputs.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995859">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999946324324325">
Our raw data set is an English–French translation
memory with technical translation from Syman-
tec, consisting of 51K sentence pairs. We ran-
domly selected 43K to train an SMT system and
translated the English side of the remaining 8K
sentence pairs. The average sentence length of
the training set is 13.5 words and the size of the
training set is comparable to the (larger) TMs used
in the industry. Note that we remove the exact
matches in the TM from our dataset, because ex-
act matches will be reused and not presented to the
post-editor in a typical TM setting.
As for the SMT system, we use a stan-
dard log-linear PB-SMT model (Och and Ney,
2002): GIZA++ implementation of IBM word
alignment model 4,1 the refinement and phrase-
extraction heuristics described in (Koehn et
al., 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained with
SRILM (Stolcke, 2002) on the English side of the
training data, and Moses (Koehn et al., 2007) to
decode. We train a system in the opposite direc-
tion using the same data to produce the pseudo-
source sentences.
We train the SVM classifier using the lib-
SVM (Chang and Lin, 2001) toolkit. The SVM-
training and testing is performed on the remaining
8K sentences with 4-fold cross validation. We also
report 95% confidence intervals.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 4-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameter C in (1) we search in the range
[2−5, 215], and for parameter γ (2) we search in the
range [2−15, 23]. The step size is 2 on the expo-
nent.
</bodyText>
<subsectionHeader confidence="0.998109">
5.2 The Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.937218">
We measure the quality of the classification by
precision and recall. Let A be the set of recom-
mended MT outputs, and B be the set of MT out-
puts that have lower TER than TM hits. We stan-
dardly define precision P, recall R and F-value as
in (7):
&apos;More specifically, we performed 5 iterations of Model 1,
5 iterations of FMM, 3 iterations of Model 3, and 3 iterations
of Model 4.
</bodyText>
<page confidence="0.994767">
625
</page>
<subsectionHeader confidence="0.99626">
5.3 Recommendation Results
</subsectionHeader>
<bodyText confidence="0.9974408">
In Table 1, we report recommendation perfor-
mance using MT and TM system features (SYS),
system features plus system-independent features
(ALL:SYS+SI), and system-independent features
only (SI).
</bodyText>
<tableCaption confidence="0.998593">
Table 1: Recommendation Results
</tableCaption>
<table confidence="0.99922875">
Precision Recall F-Score
SYS 82.53±1.17 96.44±0.68 88.95±.56
SI 82.56±1.46 95.83±0.52 88.70±.65
ALL 83.45±1.33 95.56±1.33 89.09±.24
</table>
<bodyText confidence="0.998673555555555">
From Table 1, we observe that MT and TM
system-internal features are very useful for pro-
ducing a stable (as indicated by the smaller con-
fidence interval) recommendation system (SYS).
Interestingly, only using some simple system-
external features as described in Section 4.3.3 can
also yield a system with reasonably good per-
formance (SI). We expect that the performance
can be further boosted by adding more syntactic
and semantic features. Combining all the system-
internal and -external features leads to limited
gains in Precision and F-score compared to using
only system-internal features (SYS) only. This in-
dicates that at the default confidence level, current
system-external (resp. system-internal) features
can only play a limited role in informing the sys-
tem when current system-internal (resp. system-
external) features are available. We show in Sec-
tion 5.4.2 that combing both system-internal and -
external features can yield higher, more stable pre-
cision when adjusting the confidence levels of the
classifier. Additionally, the performance of system
SI is promising given the fact that we are using
only a limited number of simple features, which
demonstrates a good prospect of applying our rec-
ommendation system to MT systems where we do
not have access to their internal features.
</bodyText>
<sectionHeader confidence="0.768394" genericHeader="method">
5.4 Further Improving Recommendation
Precision
</sectionHeader>
<bodyText confidence="0.9992875">
Table 1 shows that classification recall is very
high, which suggests that precision can still be im-
proved, even though the F-score is not low. Con-
sidering that TM is the dominant technology used
by post-editors, a recommendation to replace the
hit from the TM would require more confidence,
i.e. higher precision. Ideally our aim is to obtain
a level of 0.9 precision at the cost of some recall,
if necessary. We propose two methods to achieve
this goal.
</bodyText>
<subsubsectionHeader confidence="0.592374">
5.4.1 Classifier Margins
</subsubsectionHeader>
<bodyText confidence="0.999978083333333">
We experiment with different margins on the train-
ing data to tune precision and recall in order to
obtain a desired balance. In the basic case, the
training example would be marked as in (3). If we
label both the training and test sets with this rule,
the accuracy of the prediction will be maximized.
We try to achieve higher precision by enforc-
ing a larger bias towards negative examples in the
training set so that some borderline positive in-
stances would actually be labeled as negative, and
the classifier would have higher precision in the
prediction stage as in (8).
</bodyText>
<equation confidence="0.992263">
{
= +1 if TER(SMT) + b &lt; TER(TM)
y
−1 if TER(SMT) + b &gt;, TER(TM)
(g)
</equation>
<bodyText confidence="0.999853">
We experiment with b in [0, 0.25] using MT sys-
tem features and TM features. Results are reported
in Table 2.
</bodyText>
<tableCaption confidence="0.990615">
Table 2: Classifier margins
</tableCaption>
<table confidence="0.995594857142857">
Precision Recall
TER+0 83.45 ±1.33 95.56 ±1.33
TER+0.05 82.41 ±1.23 94.41 ±1.01
TER+0.10 84.53 ±0.98 88.81 ±0.89
TER+0.15 85.24 ±0.91 87.08 ±2.38
TER+0.20 87.59 ±0.57 75.86 ±2.70
TER+0.25 89.29 ±0.93 66.67 ±2.53
</table>
<bodyText confidence="0.999646285714286">
The highest accuracy and F-value is achieved
by TER + 0, as all other settings are trained
on biased margins. Except for a small drop in
TER+0.05, other configurations all obtain higher
precision than TER + 0. We note that we can ob-
tain 0.85 precision without a big sacrifice in recall
with b=0.15, but for larger improvements on pre-
cision, recall will drop more rapidly.
When we use b beyond 0.25, the margin be-
comes less reliable, as the number of positive
examples becomes too small. In particular, this
causes the SVM parameters we tune on in the first
fold to become less applicable to the other folds.
This is one limitation of using biased margins to
</bodyText>
<equation confidence="0.999019666666667">
P = |A ∩ B|
|A |, |B |P + R (7)
R = |A ∩ B |and F = 2PR
</equation>
<page confidence="0.992639">
626
</page>
<bodyText confidence="0.9998385">
obtain high precision. The method presented in
Section 5.4.2 is less influenced by this limitation.
</bodyText>
<subsectionHeader confidence="0.966583">
5.4.2 Adjusting Confidence Levels
</subsectionHeader>
<bodyText confidence="0.977244304347826">
An alternative to using a biased margin is to output
a confidence score during prediction and to thresh-
old on the confidence score. It is also possible to
add this method to the SVM model trained with a
biased margin.
We use the SVM confidence estimation tech-
niques in Section 4.2 to obtain the confidence
level of the recommendation, and change the con-
fidence threshold for recommendation when nec-
essary. This also allows us to compare directly
against a simple baseline inspired by TM users. In
a TM environment, some users simply ignore TM
hits below a certain fuzzy match score F (usually
from 0.7 to 0.8). This fuzzy match score reflects
the confidence of recommending the TM hits. To
obtain the confidence of recommending an SMT
output, our baseline (FM) uses fuzzy match costs
hFM Pz� 1−F (cf. Section 4.3.2) for the TM hits as
the level of confidence. In other words, the higher
the fuzzy match cost of the TM hit is (lower fuzzy
match score), the higher the confidence of recom-
mending the SMT output. We compare this base-
line with the three settings in Section 5.
</bodyText>
<figureCaption confidence="0.822983">
Figure 1: Precision Changes with Confidence
</figureCaption>
<bodyText confidence="0.986984533333333">
Level
Figure 1 shows that the precision curve of FM
is low and flat when the fuzzy match costs are
low (from 0 to 0.6), indicating that it is unwise to
recommend an SMT output when the TM hit has
a low fuzzy match cost (corresponding to higher
fuzzy match score, from 0.4 to 1). We also observe
that the precision of the recommendation receives
a boost when the fuzzy match costs for the TM
hits are above 0.7 (fuzzy match score lower than
0.3), indicating that SMT output should be recom-
mended when the TM hit has a high fuzzy match
cost (low fuzzy match score). With this boost, the
precision of the baseline system can reach 0.85,
demonstrating that a proper thresholding of fuzzy
match scores can be used effectively to discrimi-
nate the recommendation of the TM hit from the
recommendation of the SMT output.
However, using the TM information only does
not always find the easiest-to-edit translation. For
example, an excellent SMT output should be rec-
ommended even if there exists a good TM hit (e.g.
fuzzy match score is 0.7 or more). On the other
hand, a misleading SMT output should not be rec-
ommended if there exists a poor but useful TM
match (e.g. fuzzy match score is 0.2).
Our system is able to tackle these complica-
tions as it incorporates features from the MT and
the TM systems simultaneously. Figure 1 shows
that both the SYS and the ALL setting consistently
outperform FM, indicating that our classification
scheme can better integrate the MT output into the
TM system than this naive baseline.
The SI feature set does not perform well when
the confidence level is set above 0.85 (cf. the de-
scending tail of the SI curve in Figure 1). This
might indicate that this feature set is not reliable
enough to extract the best translations. How-
ever, when the requirement on precision is not that
high, and the MT-internal features are not avail-
able, it would still be desirable to obtain transla-
tion recommendations with these black-box fea-
tures. The difference between SYS and ALL is
generally small, but ALL performs steadily better
in [0.5, 0,8].
</bodyText>
<tableCaption confidence="0.998655">
Table 3: Recall at Fixed Precision
</tableCaption>
<table confidence="0.989804">
Recall
SYS @85PREC 88.12±1.32
SYS @90PREC 52.73±2.31
SI @85PREC 87.33±1.53
ALL @85PREC 88.57±1.95
ALL @90PREC 51.92±4.28
</table>
<subsectionHeader confidence="0.979161">
5.5 Precision Constraints
</subsectionHeader>
<bodyText confidence="0.999525166666667">
In Table 3 we also present the recall scores at 0.85
and 0.9 precision for SYS, SI and ALL models to
demonstrate our system’s performance when there
is a hard constraint on precision. Note that our
system will return the TM entry when there is an
exact match, so the overall precision of the system
</bodyText>
<figure confidence="0.993138214285714">
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Confidence
Precision
0.95
0.85
0.75
0.9
0.8
0.7
1
SI
Sys
All
FM
</figure>
<page confidence="0.990458">
627
</page>
<bodyText confidence="0.996871333333333">
is above the precision score we set here in a ma-
ture TM environment, as a significant portion of
the material to be translated will have a complete
match in the TM system.
In Table 3 for MODEL@K, the recall scores are
achieved when the prediction precision is better
than K with 0.95 confidence. For each model, pre-
cision at 0.85 can be obtained without a very big
loss on recall. However, if we want to demand
further recommendation precision (more conser-
vative in recommending SMT output), the recall
level will begin to drop more quickly. If we use
only system-independent features (SI), we cannot
achieve as high precision as with other models
even if we sacrifice more recall.
Based on these results, the users of the TM sys-
tem can choose between precision and recall ac-
cording to their own needs. As the threshold does
not involve training of the SMT system or the
SVM classifier, the user is able to determine this
trade-off at runtime.
</bodyText>
<tableCaption confidence="0.997899">
Table 4: Contribution of Features
</tableCaption>
<table confidence="0.9997532">
Precision Recall F Score
SYS 82.53±1.17 96.44±0.68 88.95±.56
+M1 82.87±1.26 96.23±0.53 89.05±.52
+LM 82.82±1.16 96.20±1.14 89.01±.23
+PS 83.21±1.33 96.61±0.44 89.41±.84
</table>
<subsectionHeader confidence="0.998907">
5.6 Contribution of Features
</subsectionHeader>
<bodyText confidence="0.990687642857143">
In Section 4.3.3 we suggested three sets of
system-independent features: features based on
the source- and target-side language model (LM),
the IBM Model 1 (M1) and the fuzzy match scores
on pseudo-source (PS). We compare the contribu-
tion of these features in Table 4.
In sum, all the three sets of system-independent
features improve the precision and F-scores of the
MT and TM system features. The improvement
is not significant, but improvement on every set of
system-independent features gives some credit to
the capability of SI features, as does the fact that
SI features perform close to SYS features in Table
1.
</bodyText>
<subsectionHeader confidence="0.582281">
6 Analysis of Post-Editing Effort
</subsectionHeader>
<bodyText confidence="0.999981153846154">
A natural question on the integration models is
whether the classification reduces the effort of the
translators and post-editors: after reading these
recommendations, will they translate/edit less than
they would otherwise have to? Ideally this ques-
tion would be answered by human post-editors in
a large-scale experimental setting. As we have
not yet conducted a manual post-editing experi-
ment, we conduct two sets of analyses, trying to
show which type of edits will be required for dif-
ferent recommendation confidence levels. We also
present possible methods for human evaluation at
the end of this section.
</bodyText>
<subsectionHeader confidence="0.999542">
6.1 Edit Statistics
</subsectionHeader>
<bodyText confidence="0.999939884615385">
We provide the statistics of the number of edits
for each sentence with 0.95 confidence intervals,
sorted by TER edit types. Statistics of positive in-
stances in classification (i.e. the instances in which
MT output is recommended over the TM hit) are
given in Table 5.
When an MT output is recommended, its TM
counterpart will require a larger average number
of total edits than the MT output, as we expect. If
we drill down, however, we also observe that many
of the saved edits come from the Substitution cat-
egory, which is the most costly operation from the
post-editing perspective. In this case, the recom-
mended MT output actually saves more effort for
the editors than what is shown by the TER score.
It reflects the fact that TM outputs are not actual
translations, and might need heavier editing.
Table 6 shows the statistics of negative instances
in classification (i.e. the instances in which MT
output is not recommended over the TM hit). In
this case, the MT output requires considerably
more edits than the TM hits in terms of all four
TER edit types, i.e. insertion, substitution, dele-
tion and shift. This reflects the fact that some high
quality TM matches can be very useful as a trans-
lation.
</bodyText>
<subsectionHeader confidence="0.9983865">
6.2 Edit Statistics on Recommendations of
Higher Confidence
</subsectionHeader>
<bodyText confidence="0.999554909090909">
We present the edit statistics of recommendations
with higher confidence in Table 7. Comparing Ta-
bles 5 and 7, we see that if recommended with
higher confidence, the MT output will need sub-
stantially less edits than the TM output: e.g. 3.28
fewer substitutions on average.
From the characteristics of the high confidence
recommendations, we suspect that these mainly
comprise harder to translate (i.e. different from
the SMT training set/TM database) sentences, as
indicated by the slightly increased edit operations
</bodyText>
<page confidence="0.998964">
628
</page>
<tableCaption confidence="0.997649">
Table 5: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.5
</tableCaption>
<table confidence="0.999318333333333">
Insertion Substitution Deletion Shift
MT 0.9849 f 0.0408 2.2881 f 0.0672 0.8686 f 0.0370 1.2500 f 0.0598
TM 0.7762 f 0.0408 4.5841 f 0.1036 3.1567 f 0.1120 1.2096 f 0.0554
</table>
<tableCaption confidence="0.865716">
Table 6: Edit Statistics when NOT Recommending MT Outputs in Classification, confidence=0.5
</tableCaption>
<table confidence="0.999736666666667">
Insertion Substitution Deletion Shift
MT 1.0830 f 0.1167 2.2885 f 0.1376 1.0964 f 0.1137 1.5381 f 0.1962
TM 0.7554 f 0.0376 1.5527 f 0.1584 1.0090 f 0.1850 0.4731 f 0.1083
</table>
<tableCaption confidence="0.966548">
Table 7: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.85
</tableCaption>
<table confidence="0.995438">
Insertion Substitution Deletion Shift
MT 1.1665 f 0.0615 2.7334 f 0.0969 1.0277 f 0.0544 1.5549 f 0.0899
TM 0.8894 f 0.0594 6.0085 f 0.1501 4.1770 f 0.1719 1.6727 f 0.0846
</table>
<bodyText confidence="0.9989622">
on the MT side. TM produces much worse edit-
candidates for such sentences, as indicated by
the numbers in Table 7, since TM does not have
the ability to automatically reconstruct an output
through the combination of several segments.
</bodyText>
<subsectionHeader confidence="0.947616">
6.3 Plan for Human Evaluation
</subsectionHeader>
<bodyText confidence="0.879490105263158">
Evaluation with human post-editors is crucial to
validate and improve translation recommendation.
There are two possible avenues to pursue:
• Test our system on professional post-editors.
By providing them with the TM output, the
MT output and the one recommended to edit,
we can measure the true accuracy of our
recommendation, as well as the post-editing
time we save for the post-editors;
• Apply the presented method on open do-
main data and evaluate it using crowd-
sourcing. It has been shown that crowd-
sourcing tools, such as the Amazon Me-
chanical Turk (Callison-Burch, 2009), can
help developers to obtain good human judge-
ments on MT output quality both cheaply and
quickly. Given that our problem is related to
MT quality estimation in nature, it can poten-
tially benefit from such tools as well.
</bodyText>
<sectionHeader confidence="0.996242" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999733625">
In this paper we present a classification model to
integrate SMT into a TM system, in order to facili-
tate the work of post-editors. Insodoing we handle
the problem of MT quality estimation as binary
prediction instead of regression. From the post-
editors’ perspective, they can continue to work in
their familiar TM environment, use the same cost-
estimation methods, and at the same time bene-
fit from the power of state-of-the-art MT. We use
SVMs to make these predictions, and use grid
search to find better RBF kernel parameters.
We explore features from inside the MT sys-
tem, from the TM, as well as features that make
no assumption on the translation model for the bi-
nary classification. With these features we make
glass-box and black-box predictions. Experiments
show that the models can achieve 0.85 precision at
a level of 0.89 recall, and even higher precision if
we sacrifice more recall. With this guarantee on
precision, our method can be used in a TM envi-
ronment without changing the upper-bound of the
related cost estimation.
Finally, we analyze the characteristics of the in-
tegrated outputs. We present results to show that,
if measured by number, type and content of ed-
its in TER, the recommended sentences produced
by the classification model would bring about less
post-editing effort than the TM outputs.
This work can be extended in the following
ways. Most importantly, it is useful to test the
model in user studies, as proposed in Section 6.3.
A user study can serve two purposes: 1) it can
validate the effectiveness of the method by mea-
suring the amount of edit effort it saves; and 2)
the byproduct of the user study – post-edited sen-
tences – can be used to generate HTER scores
to train a better recommendation model. Further-
more, we want to experiment and improve on the
adaptability of this method, as the current experi-
ment is on a specific domain and language pair.
</bodyText>
<page confidence="0.998838">
629
</page>
<sectionHeader confidence="0.997881" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995734">
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank Symantec for providing the TM database and the
anonymous reviewers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.994262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999336579999999">
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for ma-
chine translation. In The 20th International Conference
on Computational Linguistics (Coling-2004), pages 315 –
321, Geneva, Switzerland.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263 – 311.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Mechani-
cal Turk. In The 2009 Conference on Empirical Methods
in Natural Language Processing (EMNLP-2009), pages
286 – 295, Singapore.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
˜cjlin/libsvm.
Corinna Cortes and Vladimir Vapnik. 1995. Support-vector
networks. Machine learning, 20(3):273 – 297.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In The 1995 International
Conference on Acoustics, Speech, and Signal Processing
(ICASSP-95), pages 181 – 184, Detroit, MI.
Philipp. Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In The 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technol-
ogy (NAACL/HLT-2003), pages 48 – 54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In The 45th Annual Meet-
ing of the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster Ses-
sions (ACL-2007), pages 177 – 180, Prague, Czech Re-
public.
Vladimir Iosifovich Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10(8):707 – 710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt’s probabilistic outputs for support vector
machines. Machine Learning, 68(3):267 – 276.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics (ACL-
2002), pages 295 – 302, Philadelphia, PA.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In The 41st Annual Meet-
ing on Association for Computational Linguistics (ACL-
2003), pages 160 – 167.
John C. Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood meth-
ods. Advances in Large Margin Classifiers, pages 61 – 74.
Christopher B. Quirk. 2004. Training a sentence-level ma-
chine translation confidence measure. In The Fourth In-
ternational Conference on Language Resources and Eval-
uation (LREC-2004), pages 825 – 828, Lisbon, Portugal.
Richard Sikes. 2007. Fuzzy matching in theory and practice.
Multilingual, 18(6):39 – 43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation en-
vironment. In The Twelfth Machine Translation Sum-
mit (MT Summit XII), pages 120 – 127, Ottawa, Ontario,
Canada.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In The 2006
conference of the Association for Machine Translation in
the Americas (AMTA-2006), pages 223 – 231, Cambridge,
MA.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco
Turchi, and Nello Cristianini. 2009a. Estimating the
sentence-level quality of machine translation systems. In
The 13th Annual Conference of the European Association
for Machine Translation (EAMT-2009), pages 28 – 35,
Barcelona, Spain.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang,
and John Shawe-Taylor. 2009b. Improving the confidence
of machine translation quality estimates. In The Twelfth
Machine Translation Summit (MT Summit XII), pages 136
– 143, Ottawa, Ontario, Canada.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In The Seventh International Confer-
ence on Spoken Language Processing, volume 2, pages
901 – 904, Denver, CO.
Nicola Ueffing and Hermann Ney. 2005. Application
of word-level confidence measures in interactive statisti-
cal machine translation. In The Ninth Annual Confer-
ence of the European Association for Machine Translation
(EAMT-2005), pages 262 – 270, Budapest, Hungary.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation.
In The Ninth Machine Translation Summit (MT Summit
IX), pages 394 – 401, New Orleans, LA.
</reference>
<page confidence="0.997546">
630
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986137">
<title confidence="0.99994">Bridging SMT and TM with Translation Recommendation</title>
<author confidence="0.999894">Yifan He Yanjun Ma Josef van_Genabith Andy Way</author>
<affiliation confidence="0.997970666666667">Centre for Next Generation Localisation School of Computing Dublin City University</affiliation>
<abstract confidence="0.999609272727273">We propose a translation recommendation framework to integrate Statistical Machine Translation (SMT) output with Translation Memory (TM) systems. The framework recommends SMT outputs to a TM user when it predicts that SMT outputs are more suitable for post-editing than the hits provided by the TM. We describe an implementation of this framework using an SVM binary classifier. We exploit methods to fine-tune the classifier and investigate a variety of features of different types. We rely on automatic MT evaluation metrics to approximate human judgements in our experiments. Experimental results show that our system can achieve 0.85 precision at 0.89 recall, excluding exact matches. Furthermore, it is possible for the end-user to achieve a desired balance between precision and recall by adjusting confidence levels.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In The 20th International Conference on Computational Linguistics (Coling-2004),</booktitle>
<pages>315--321</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="5030" citStr="Blatz et al., 2004" startWordPosition="785" endWordPosition="788">the classification model in Section 4 and present experiments in Section 5. In Section 6, we analyze the post-editing effort approximated by the TER metric (Snover et al., 2006). Section 7 concludes the paper and points out avenues for future research. 2 Related Work Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there </context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In The 20th International Conference on Computational Linguistics (Coling-2004), pages 315 – 321, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="13690" citStr="Brown et al., 1993" startWordPosition="2288" endWordPosition="2291">ore between the original source sentence and this pseudo-source. If the MT/TM system performs well enough, these two sentences should be the same or very similar. Therefore, the fuzzy match score here gives an estimation of the confidence level of the output. We compute this score for both the MT output and the TM hit. The IBM Model 1 Score. The fuzzy match score does not measure whether the hit could be a correct translation, i.e. it does not take into account the correspondence between the source and target, but rather only the source-side information. For the TM hit, the IBM Model 1 score (Brown et al., 1993) serves as a rough estimation of how good a translation it is on the word level; for the MT output, on the other hand, it is a black-box feature to estimate translation quality when the information from the translation model is not available. We compute bidirectional (source-to-target and target-to-source) model 1 scores on both TM and MT outputs. 5 Experiments 5.1 Experimental Settings Our raw data set is an English–French translation memory with technical translation from Symantec, consisting of 51K sentence pairs. We randomly selected 43K to train an SMT system and translated the English si</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263 – 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In The 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-2009),</booktitle>
<pages>286--295</pages>
<contexts>
<context position="29836" citStr="Callison-Burch, 2009" startWordPosition="5023" endWordPosition="5024">l segments. 6.3 Plan for Human Evaluation Evaluation with human post-editors is crucial to validate and improve translation recommendation. There are two possible avenues to pursue: • Test our system on professional post-editors. By providing them with the TM output, the MT output and the one recommended to edit, we can measure the true accuracy of our recommendation, as well as the post-editing time we save for the post-editors; • Apply the presented method on open domain data and evaluate it using crowdsourcing. It has been shown that crowdsourcing tools, such as the Amazon Mechanical Turk (Callison-Burch, 2009), can help developers to obtain good human judgements on MT output quality both cheaply and quickly. Given that our problem is related to MT quality estimation in nature, it can potentially benefit from such tools as well. 7 Conclusions and Future Work In this paper we present a classification model to integrate SMT into a TM system, in order to facilitate the work of post-editors. Insodoing we handle the problem of MT quality estimation as binary prediction instead of regression. From the posteditors’ perspective, they can continue to work in their familiar TM environment, use the same costes</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In The 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-2009), pages 286 – 295, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ ˜cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="15269" citStr="Chang and Lin, 2001" startWordPosition="2558" endWordPosition="2561">or the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4,1 the refinement and phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudosource sentences. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. The SVMtraining and testing is performed on the remaining 8K sentences with 4-fold cross validation. We also report 95% confidence intervals. The SVM hyper-parameters are tuned using the training data of the first fold in the 4-fold cross validation via a brute force grid search. More specifically, for parameter C in (1) we search in the range [2−5, 215], and for parameter γ (2) we search in the range [2−15, 23]. The step size is 2 on the exponent. 5.2 The Evaluation Metrics We measure the quality of the classification by precision and recall. Let A be the set of recommended MT outpu</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ ˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine learning, 20(3):273 –</booktitle>
<pages>297</pages>
<contexts>
<context position="7046" citStr="Cortes and Vapnik, 1995" startWordPosition="1115" endWordPosition="1118">iting effort. Our research is more similar in spirit to the third strand. However, we use outputs and features from the TM explicitly; therefore instead of having to solve a regression problem, we only have to solve a much easier binary prediction problem which can be integrated into TMs in a straightforward manner. Because of this, the precision and recall scores reported in this paper are not directly comparable to those in (Specia et al., 2009b) as the latter are computed on a pure SMT system without a TM in the background. 3 Support Vector Machines for Translation Quality Estimation SVMs (Cortes and Vapnik, 1995) are binary classifiers that classify an input instance based on decision rules which minimize the regularized error function in (1): �WTW + C 1 s. t. yi(WTO(Xi) + b) &gt; 1 − ξi ξi &gt; 0 where (xi, yi) E Rn x {+1, −1} are l training instances that are mapped by the function ϕ to a higher dimensional space. w is the weight vector, ξ is the relaxation variable and C &gt; 0 is the penalty parameter. Solving SVMs is viable using the ‘kernel trick’: finding a kernel function K in (1) with K(xi,xj) _ b(xi)T b(xj). We perform our experiments with the Radial Basis Function (RBF) kernel, as /in (2): K(Xi, Xj)</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine learning, 20(3):273 – 297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In The 1995 International Conference on Acoustics, Speech, and Signal Processing (ICASSP-95),</booktitle>
<pages>181--184</pages>
<location>Detroit, MI.</location>
<contexts>
<context position="14980" citStr="Kneser and Ney, 1995" startWordPosition="2505" endWordPosition="2508"> the training set is 13.5 words and the size of the training set is comparable to the (larger) TMs used in the industry. Note that we remove the exact matches in the TM from our dataset, because exact matches will be reused and not presented to the post-editor in a typical TM setting. As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4,1 the refinement and phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudosource sentences. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. The SVMtraining and testing is performed on the remaining 8K sentences with 4-fold cross validation. We also report 95% confidence intervals. The SVM hyper-parameters are tuned using the training data of the first fold in the 4-fold cross validation via a brute force grid search. More specifically, f</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In The 1995 International Conference on Acoustics, Speech, and Signal Processing (ICASSP-95), pages 181 – 184, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och Koehn</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In The 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL/HLT-2003), pages 48 – 54,</booktitle>
<location>Edmonton, Alberta, Canada.</location>
<marker>Koehn, Marcu, 2003</marker>
<rawString>Philipp. Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In The 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL/HLT-2003), pages 48 – 54, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In The 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions (ACL-2007),</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15088" citStr="Koehn et al., 2007" startWordPosition="2524" endWordPosition="2527"> industry. Note that we remove the exact matches in the TM from our dataset, because exact matches will be reused and not presented to the post-editor in a typical TM setting. As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4,1 the refinement and phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudosource sentences. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. The SVMtraining and testing is performed on the remaining 8K sentences with 4-fold cross validation. We also report 95% confidence intervals. The SVM hyper-parameters are tuned using the training data of the first fold in the 4-fold cross validation via a brute force grid search. More specifically, for parameter C in (1) we search in the range [2−5, 215], and for parameter γ (2) we search in the range [2−1</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In The 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions (ACL-2007), pages 177 – 180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Iosifovich Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<pages>710</pages>
<contexts>
<context position="11234" citStr="Levenshtein, 1966" startWordPosition="1865" endWordPosition="1866">he MT system features, the TM feature and systemindependent features. 4.3.1 The MT System Features These features include those typically used in SMT, namely the phrase-translation model scores, the language model probability, the distance-based reordering score, the lexicalized reordering model scores, and the word penalty. 4.3.2 The TM Feature The TM feature is the fuzzy match (Sikes, 2007) cost of the TM hit. The calculation of fuzzy match score itself is one of the core technologies in TM systems and varies among different vendors. We compute fuzzy match cost as the minimum Edit Distance (Levenshtein, 1966) between the source and TM entry, normalized by the length of the source as in (6), as most of the current implementations are based on edit distance while allowing some additional flexible matching. EditDistance(s, e) (6) Len(s) where s is the source side of t, the sentence to translate, and a is the source side of an entry in the TM. For fuzzy match scores F, this fuzzy match cost hf,,t roughly corresponds to 1−F. The difference in calculation does not influence classification, and allows direct comparison between a pure TM system and a translation recommendation system in Section 5.4.2. ∑� </context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Iosifovich Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707 – 710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsuan-Tien Lin</author>
<author>Chih-Jen Lin</author>
<author>Ruby C Weng</author>
</authors>
<title>A note on platt’s probabilistic outputs for support vector machines.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>68</volume>
<issue>3</issue>
<pages>276</pages>
<contexts>
<context position="9793" citStr="Lin et al., 2007" startWordPosition="1611" endWordPosition="1614">ted with a set of features from both the MT and TM outputs, which are discussed in more detail in Section 4.3. 4.2 Recommendation Confidence Estimation In classical settings involving SVMs, confidence levels are represented as margins of binary predictions. However, these margins provide little insight for our application because the numbers are only meaningful when compared to each other. What is more preferable is a probabilistic confidence score (e.g. 90% confidence) which is better understood by post-editors and translators. We use the techniques proposed by (Platt, 1999) and improved by (Lin et al., 2007) to obtain the posterior probability of a classification, which is used as the confidence score in our system. Platt’s method estimates the posterior probability with a sigmod function, as in (4): 1 Pr(y = 1|x) � PA,B(f) =1 + exp(Af + B) (4) where f = f(x) is the decision function of the estimated SVM. A and B are parameters that minimize the cross-entropy error function F on the training data, as in Eq. (5): (tilog(pi) + (1 − ti)log(1 − pi)), {N++1 N++2 if yi = +1 where pi = PA,B(fi), and ti = 1 N_+2 if yi = −1 (5) where z = (A, B) is a parameter setting, and N+ and N_ are the numbers of obse</context>
</contexts>
<marker>Lin, Lin, Weng, 2007</marker>
<rawString>Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007. A note on platt’s probabilistic outputs for support vector machines. Machine Learning, 68(3):267 – 276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL2002),</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="14729" citStr="Och and Ney, 2002" startWordPosition="2470" endWordPosition="2473">French translation memory with technical translation from Symantec, consisting of 51K sentence pairs. We randomly selected 43K to train an SMT system and translated the English side of the remaining 8K sentence pairs. The average sentence length of the training set is 13.5 words and the size of the training set is comparable to the (larger) TMs used in the industry. Note that we remove the exact matches in the TM from our dataset, because exact matches will be reused and not presented to the post-editor in a typical TM setting. As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4,1 the refinement and phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudosource sentences. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. The SVMtraining and testing is performed on the re</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL2002), pages 295 – 302, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In The 41st Annual Meeting on Association for Computational Linguistics (ACL2003),</booktitle>
<pages>160--167</pages>
<contexts>
<context position="14906" citStr="Och, 2003" startWordPosition="2496" endWordPosition="2497">the remaining 8K sentence pairs. The average sentence length of the training set is 13.5 words and the size of the training set is comparable to the (larger) TMs used in the industry. Note that we remove the exact matches in the TM from our dataset, because exact matches will be reused and not presented to the post-editor in a typical TM setting. As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4,1 the refinement and phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudosource sentences. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. The SVMtraining and testing is performed on the remaining 8K sentences with 4-fold cross validation. We also report 95% confidence intervals. The SVM hyper-parameters are tuned using the training data of the first fold in the 4</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In The 41st Annual Meeting on Association for Computational Linguistics (ACL2003), pages 160 – 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<contexts>
<context position="9758" citStr="Platt, 1999" startWordPosition="1606" endWordPosition="1607">) (3) Each instance is associated with a set of features from both the MT and TM outputs, which are discussed in more detail in Section 4.3. 4.2 Recommendation Confidence Estimation In classical settings involving SVMs, confidence levels are represented as margins of binary predictions. However, these margins provide little insight for our application because the numbers are only meaningful when compared to each other. What is more preferable is a probabilistic confidence score (e.g. 90% confidence) which is better understood by post-editors and translators. We use the techniques proposed by (Platt, 1999) and improved by (Lin et al., 2007) to obtain the posterior probability of a classification, which is used as the confidence score in our system. Platt’s method estimates the posterior probability with a sigmod function, as in (4): 1 Pr(y = 1|x) � PA,B(f) =1 + exp(Af + B) (4) where f = f(x) is the decision function of the estimated SVM. A and B are parameters that minimize the cross-entropy error function F on the training data, as in Eq. (5): (tilog(pi) + (1 − ti)log(1 − pi)), {N++1 N++2 if yi = +1 where pi = PA,B(fi), and ti = 1 N_+2 if yi = −1 (5) where z = (A, B) is a parameter setting, an</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, pages 61 – 74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher B Quirk</author>
</authors>
<title>Training a sentence-level machine translation confidence measure.</title>
<date>2004</date>
<booktitle>In The Fourth International Conference on Language Resources and Evaluation (LREC-2004),</booktitle>
<pages>825--828</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="5174" citStr="Quirk, 2004" startWordPosition="811" endWordPosition="812">tric (Snover et al., 2006). Section 7 concludes the paper and points out avenues for future research. 2 Related Work Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less rele</context>
</contexts>
<marker>Quirk, 2004</marker>
<rawString>Christopher B. Quirk. 2004. Training a sentence-level machine translation confidence measure. In The Fourth International Conference on Language Resources and Evaluation (LREC-2004), pages 825 – 828, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sikes</author>
</authors>
<title>Fuzzy matching in theory and practice.</title>
<date>2007</date>
<journal>Multilingual,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="11011" citStr="Sikes, 2007" startWordPosition="1827" endWordPosition="1828">ositive and negative examples, respectively, for the label yi. These numbers are obtained using an internal cross-validation on the training set. 4.3 The Feature Set We use three types of features in classification: the MT system features, the TM feature and systemindependent features. 4.3.1 The MT System Features These features include those typically used in SMT, namely the phrase-translation model scores, the language model probability, the distance-based reordering score, the lexicalized reordering model scores, and the word penalty. 4.3.2 The TM Feature The TM feature is the fuzzy match (Sikes, 2007) cost of the TM hit. The calculation of fuzzy match score itself is one of the core technologies in TM systems and varies among different vendors. We compute fuzzy match cost as the minimum Edit Distance (Levenshtein, 1966) between the source and TM entry, normalized by the length of the source as in (6), as most of the current implementations are based on edit distance while allowing some additional flexible matching. EditDistance(s, e) (6) Len(s) where s is the source side of t, the sentence to translate, and a is the source side of an entry in the TM. For fuzzy match scores F, this fuzzy ma</context>
</contexts>
<marker>Sikes, 2007</marker>
<rawString>Richard Sikes. 2007. Fuzzy matching in theory and practice. Multilingual, 18(6):39 – 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
</authors>
<title>Phrase-based machine translation in a computer-assisted translation environment.</title>
<date>2009</date>
<booktitle>In The Twelfth Machine Translation Summit (MT Summit XII),</booktitle>
<pages>120--127</pages>
<location>Ottawa, Ontario, Canada.</location>
<contexts>
<context position="5694" citStr="Simard and Isabelle, 2009" startWordPosition="891" endWordPosition="894">rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. A third strand of research tries to incorporate confidence measures into a post-editing environment. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a). Instead of modeling on translation quality (often measured by automatic evaluation scores), this research uses regression on both the automatic scores and scores assigned by post-editors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of</context>
</contexts>
<marker>Simard, Isabelle, 2009</marker>
<rawString>Michel Simard and Pierre Isabelle. 2009. Phrase-based machine translation in a computer-assisted translation environment. In The Twelfth Machine Translation Summit (MT Summit XII), pages 120 – 127, Ottawa, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In The 2006 conference of the Association for Machine Translation in the Americas (AMTA-2006),</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="4588" citStr="Snover et al., 2006" startWordPosition="709" endWordPosition="712">mization, employ posterior probability-based confidence estimation to support user-based tuning for precision and recall, experiment with feature sets involving MT-, TM- and system-independent features, and use automatic MT evaluation metrics to simulate post-editing effort. The rest of the paper is organized as follows: we first briefly introduce related research in Section 2, and review the classification SVMs in Section 3. We formulate the classification model in Section 4 and present experiments in Section 5. In Section 6, we analyze the post-editing effort approximated by the TER metric (Snover et al., 2006). Section 7 concludes the paper and points out avenues for future research. 2 Related Work Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing </context>
<context position="8750" citStr="Snover et al., 2006" startWordPosition="1432" endWordPosition="1435">inary classifier to predict the relative quality of the SMT output to make a recommendation. The SVM classifier uses features from the SMT system, the TM and additional linguistic features to estimate whether the SMT output is better than the hit from the TM. 4.1 Problem Formulation As we treat translation recommendation as a binary classification problem, we have a pair of outputs from TM and MT for each sentence. Ideally the classifier will recommend the output that needs less post-editing effort. As large-scale annotated data is not yet available for this task, we use automatic TER scores (Snover et al., 2006) as the measure for the required post-editing effort. In the future, we hope to train our system on HTER (TER with human targeted references) scores (Snover et al., 2006) once the necessary human annotations are in place. In the meantime we use TER, as TER is shown to have high correlation with HTER. We label the training examples as in (3): { = +1 if TER(MT) &lt; TER(TM) y −1 if TER(MT) &gt; TER(TM) (3) Each instance is associated with a set of features from both the MT and TM outputs, which are discussed in more detail in Section 4.3. 4.2 Recommendation Confidence Estimation In classical settings </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In The 2006 conference of the Association for Machine Translation in the Americas (AMTA-2006), pages 223 – 231, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In The 13th Annual Conference of the European Association for Machine Translation (EAMT-2009),</booktitle>
<pages>28--35</pages>
<location>Barcelona,</location>
<contexts>
<context position="2313" citStr="Specia et al., 2009" startWordPosition="347" endWordPosition="350">everal simple reasons for this: 1) TMs are useful; 2) TMs represent considerable effort and investment by a company or (even more so) an individual translator; 3) the fuzzy match score used in TMs offers a good approximation of post-editing effort, which is useful both for translators and translation cost estimation and, 4) current SMT translation confidence estimation measures are not as robust as TM fuzzy match scores and professional translators are thus not ready to replace fuzzy match scores with SMT internal quality measures. There has been some research to address this issue, see e.g. (Specia et al., 2009a) and (Specia et al., 2009b). However, to date most of the research has focused on better confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors (cf. Section 2). In this paper, we try to address the problem from a different perspective. Given that most postediting work is (still) based on TM output, we propose to recommend MT outputs which are better than TM hits to post-editors. In this framework, post-editors still work with the TM while benefiting from (better) SMT outputs; the assets in TMs are not wasted an</context>
<context position="5989" citStr="Specia et al., 2009" startWordPosition="945" endWordPosition="948">articular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. A third strand of research tries to incorporate confidence measures into a post-editing environment. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a). Instead of modeling on translation quality (often measured by automatic evaluation scores), this research uses regression on both the automatic scores and scores assigned by post-editors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of features to model post-editors’ judgement of the translation quality between ‘good’ and ‘bad’, or among three levels of post-editing effort. Our research is more similar in spirit to the third strand. However, we use outputs and features from the TM explicitly; therefore instead of having to s</context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009a. Estimating the sentence-level quality of machine translation systems. In The 13th Annual Conference of the European Association for Machine Translation (EAMT-2009), pages 28 – 35, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Craig Saunders</author>
<author>Marco Turchi</author>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Improving the confidence of machine translation quality estimates.</title>
<date>2009</date>
<booktitle>In The Twelfth Machine Translation Summit (MT Summit XII),</booktitle>
<pages>136--143</pages>
<location>Ottawa, Ontario, Canada.</location>
<contexts>
<context position="2313" citStr="Specia et al., 2009" startWordPosition="347" endWordPosition="350">everal simple reasons for this: 1) TMs are useful; 2) TMs represent considerable effort and investment by a company or (even more so) an individual translator; 3) the fuzzy match score used in TMs offers a good approximation of post-editing effort, which is useful both for translators and translation cost estimation and, 4) current SMT translation confidence estimation measures are not as robust as TM fuzzy match scores and professional translators are thus not ready to replace fuzzy match scores with SMT internal quality measures. There has been some research to address this issue, see e.g. (Specia et al., 2009a) and (Specia et al., 2009b). However, to date most of the research has focused on better confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors (cf. Section 2). In this paper, we try to address the problem from a different perspective. Given that most postediting work is (still) based on TM output, we propose to recommend MT outputs which are better than TM hits to post-editors. In this framework, post-editors still work with the TM while benefiting from (better) SMT outputs; the assets in TMs are not wasted an</context>
<context position="5989" citStr="Specia et al., 2009" startWordPosition="945" endWordPosition="948">articular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. A third strand of research tries to incorporate confidence measures into a post-editing environment. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a). Instead of modeling on translation quality (often measured by automatic evaluation scores), this research uses regression on both the automatic scores and scores assigned by post-editors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of features to model post-editors’ judgement of the translation quality between ‘good’ and ‘bad’, or among three levels of post-editing effort. Our research is more similar in spirit to the third strand. However, we use outputs and features from the TM explicitly; therefore instead of having to s</context>
</contexts>
<marker>Specia, Saunders, Turchi, Wang, Shawe-Taylor, 2009</marker>
<rawString>Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang, and John Shawe-Taylor. 2009b. Improving the confidence of machine translation quality estimates. In The Twelfth Machine Translation Summit (MT Summit XII), pages 136 – 143, Ottawa, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In The Seventh International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="15015" citStr="Stolcke, 2002" startWordPosition="2512" endWordPosition="2513">ze of the training set is comparable to the (larger) TMs used in the industry. Note that we remove the exact matches in the TM from our dataset, because exact matches will be reused and not presented to the post-editor in a typical TM setting. As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4,1 the refinement and phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudosource sentences. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. The SVMtraining and testing is performed on the remaining 8K sentences with 4-fold cross validation. We also report 95% confidence intervals. The SVM hyper-parameters are tuned using the training data of the first fold in the 4-fold cross validation via a brute force grid search. More specifically, for parameter C in (1) we search in </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In The Seventh International Conference on Spoken Language Processing, volume 2, pages 901 – 904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Application of word-level confidence measures in interactive statistical machine translation.</title>
<date>2005</date>
<booktitle>In The Ninth Annual Conference of the European Association for Machine Translation (EAMT-2005),</booktitle>
<pages>262--270</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="5202" citStr="Ueffing and Ney, 2005" startWordPosition="814" endWordPosition="817">., 2006). Section 7 concludes the paper and points out avenues for future research. 2 Related Work Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this pap</context>
</contexts>
<marker>Ueffing, Ney, 2005</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2005. Application of word-level confidence measures in interactive statistical machine translation. In The Ninth Annual Conference of the European Association for Machine Translation (EAMT-2005), pages 262 – 270, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Klaus Macherey</author>
<author>Hermann Ney</author>
</authors>
<title>Confidence measures for statistical machine translation.</title>
<date>2003</date>
<booktitle>In The Ninth Machine Translation Summit (MT Summit IX),</booktitle>
<pages>394--401</pages>
<location>New Orleans, LA.</location>
<contexts>
<context position="4849" citStr="Ueffing et al., 2003" startWordPosition="752" endWordPosition="755">editing effort. The rest of the paper is organized as follows: we first briefly introduce related research in Section 2, and review the classification SVMs in Section 3. We formulate the classification model in Section 4 and present experiments in Section 5. In Section 6, we analyze the post-editing effort approximated by the TER metric (Snover et al., 2006). Section 7 concludes the paper and points out avenues for future research. 2 Related Work Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive t</context>
</contexts>
<marker>Ueffing, Macherey, Ney, 2003</marker>
<rawString>Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003. Confidence measures for statistical machine translation. In The Ninth Machine Translation Summit (MT Summit IX), pages 394 – 401, New Orleans, LA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>