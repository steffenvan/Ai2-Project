<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011658">
<title confidence="0.974889">
The Role of Implicit Argumentation in Nominal SRL
</title>
<author confidence="0.997549">
Matt Gerber
</author>
<affiliation confidence="0.998115">
Dept. of Computer Science
Michigan State University
</affiliation>
<email confidence="0.995331">
gerberm2@msu.edu
</email>
<author confidence="0.977032">
Joyce Y. Chai
</author>
<affiliation confidence="0.9976595">
Dept. of Computer Science
Michigan State University
</affiliation>
<email confidence="0.996938">
jchai@cse.msu.edu
</email>
<author confidence="0.996153">
Adam Meyers
</author>
<affiliation confidence="0.8218435">
Dept. of Computer Science
New York University
</affiliation>
<email confidence="0.998982">
meyers@cs.nyu.edu
</email>
<sectionHeader confidence="0.993739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9930215625">
Nominals frequently surface without overtly
expressed arguments. In order to measure the
potential benefit of nominal SRL for down-
stream processes, such nominals must be ac-
counted for. In this paper, we show that a
state-of-the-art nominal SRL system with an
overall argument Fl of 0.76 suffers a perfor-
mance loss of more than 9% when nominals
with implicit arguments are included in the
evaluation. We then develop a system that
takes implicit argumentation into account, im-
proving overall performance by nearly 5%.
Our results indicate that the degree of implicit
argumentation varies widely across nominals,
making automated detection of implicit argu-
mentation an important step for nominal SRL.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995244375">
In the past few years, a number of studies have
focused on verbal semantic role labeling (SRL).
Driven by annotation resources such as FrameNet
(Baker et al., 1998) and PropBank (Palmer et al.,
2005), many systems developed in these studies
have achieved argument F1 scores near 80% in
large-scale evaluations such as the one reported by
Carreras and M`arquez (2005).
More recently, the automatic identification of
nominal argument structure has received increased
attention due to the release of the NomBank cor-
pus (Meyers, 2007a). NomBank annotates predicat-
ing nouns in the same way that PropBank annotates
predicating verbs. Consider the following example
of the verbal predicate distribute from the PropBank
corpus:
</bodyText>
<listItem confidence="0.9557125">
(1) Freeport-McMoRan Energy Partners will be
liquidated and [Arg1 shares of the new
</listItem>
<bodyText confidence="0.96661775">
company] [Predicate distributed] [Arg2 to the
partnership’s unitholders].
The NomBank corpus contains a similar instance of
the deverbal nominalization distribution:
</bodyText>
<listItem confidence="0.97504825">
(2) Searle will give [Arg0 pharmacists] [Arg1
brochures] [Arg1 on the use of prescription
drugs] for [Predicate distribution] [Location in
their stores].
</listItem>
<bodyText confidence="0.959554391304348">
This instance demonstrates the annotation of split ar-
guments (Arg1) and modifying adjuncts (Location),
which are also annotated in PropBank. In cases
where a nominal has a verbal counterpart, the inter-
pretation of argument positions Arg0-Arg5 is con-
sistent between the two corpora.
In addition to deverbal (i.e., event-based) nomi-
nalizations, NomBank annotates a wide variety of
nouns that are not derived from verbs and do not de-
note events. An example is given below of the parti-
tive noun percent:
(3) Hallwood owns about 11 [Predicate %] [Arg1 of
Integra].
In this case, the noun phrase headed by the predicate
% (i.e., “about 11% of Integra”) denotes a fractional
part of the argument in position Arg1.
Since NomBank’s release, a number of studies
have applied verbal SRL techniques to the task of
nominal SRL. For example, Liu and Ng (2007) re-
ported an argument F1 of 0.7283. Although this
result is encouraging, it does not take into account
nominals that surface without overt arguments. Con-
sider the following example:
</bodyText>
<listItem confidence="0.911416333333333">
(4) The [Predicate distribution] represents [NP
available cash flow] [PP from the partnership]
[PP between Aug. 1 and Oct. 31].
</listItem>
<page confidence="0.981478">
146
</page>
<note confidence="0.890072">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 146–154,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958925">
As in (2), distribution in (4) has a noun phrase and
multiple prepositional phrases in its environment,
but not one of these constituents is an argument to
distribution in (4); rather, any arguments are implic-
itly supplied by the surrounding discourse. As de-
scribed by Meyers (2007a), instances such as (2) are
called “markable” because they contain overt argu-
ments, and instances such as (4) are called “unmark-
able” because they do not. In the NomBank corpus,
only markable instances have been annotated.
Previous evaluations (e.g., those by Jiang and
Ng (2006) and Liu and Ng (2007)) have been based
on markable instances, which constitute 57% of all
instances of nominals from the NomBank lexicon.
In order to use nominal SRL systems for down-
stream processing, it is important to develop and
evaluate techniques that can handle markable as well
as unmarkable nominal instances. To address this
issue, we investigate the role of implicit argumenta-
tion for nominal SRL. This is, in part, inspired by the
recent CoNLL Shared Task (Surdeanu et al., 2008),
which was the first evaluation of syntactic and se-
mantic dependency parsing to include unmarkable
nominals. In this paper, we extend this task to con-
stituent parsing with techniques and evaluations that
focus specifically on implicit argumentation in nom-
inals.
We first present our NomBank SRL system,
which improves the best reported argument F1 score
in the markable-only evaluation from 0.7283 to
0.7630 using a single-stage classification approach.
We show that this system, when applied to all nomi-
nal instances, achieves an argument F1 score of only
0.6895, a loss of more than 9%. We then present
a model of implicit argumentation that reduces this
loss by 46%, resulting in an F1 score of 0.7235 on
the more complete evaluation task. In our analyses,
we find that SRL performance varies widely among
specific classes of nominals, suggesting interesting
directions for future work.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999940369565217">
Nominal SRL is related to nominal relation interpre-
tation as evaluated in SemEval (Girju et al., 2007).
Both tasks identify semantic relations between a
head noun and other constituents; however, the tasks
focus on different relations. Nominal SRL focuses
primarily on relations that hold between nominaliza-
tions and their arguments, whereas the SemEval task
focuses on a range of semantic relations, many of
which are not applicable to nominal argument struc-
ture.
Early work in identifying the argument struc-
ture of deverbal nominalizations was primarily rule-
based, using rule sets to associate syntactic con-
stituents with semantic roles (Dahl et al., 1987;
Hull and Gomez, 1996; Meyers et al., 1998). La-
pata (2000) developed a statistical model to classify
modifiers of deverbal nouns as underlying subjects
or underlying objects, where subject and object de-
note the grammatical position of the modifier when
linked to a verb.
FrameNet and NomBank have facilitated machine
learning approaches to nominal argument struc-
ture. Gildea and Jurafsky (2002) presented an early
FrameNet-based SRL system that targeted both ver-
bal and nominal predicates. Jiang and Ng (2006)
and Liu and Ng (2007) have tested the hypothe-
sis that methodologies and representations used in
PropBank SRL (Pradhan et al., 2005) can be ported
to the task of NomBank SRL. These studies report
argument F1 scores of 0.6914 and 0.7283, respec-
tively. Both studies also investigated the use of fea-
tures specific to the task of NomBank SRL, but ob-
served only marginal performance gains.
NomBank argument structure has also been used
in the recent CoNLL Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies (Surdeanu
et al., 2008). In this task, systems were required to
identify syntactic dependencies, verbal and nominal
predicates, and semantic dependencies (i.e., argu-
ments) for the predicates. For nominals, the best se-
mantic F1 score was 0.7664 (Surdeanu et al., 2008);
however this score is not directly comparable to the
NomBank SRL results of Liu and Ng (2007) or the
results in this paper due to a focus on different as-
pects of the problem (see the end of section 5.2 for
details).
</bodyText>
<sectionHeader confidence="0.99743" genericHeader="method">
3 NomBank SRL
</sectionHeader>
<bodyText confidence="0.99996125">
Given a nominal predicate, an SRL system attempts
to assign surrounding spans of text to one of 23
classes representing core arguments, adjunct argu-
ments, and the null or non-argument. Similarly to
</bodyText>
<page confidence="0.997227">
147
</page>
<bodyText confidence="0.999873">
verbal SRL, this task is traditionally formulated as
a two-stage classification problem over nodes in the
syntactic parse tree of the sentence containing the
predicate.1 In the first stage, each parse tree node is
assigned a binary label indicating whether or not it
is an argument. In the second stage, argument nodes
are assigned one of the 22 non-null argument types.
Spans of text subsumed by labeled parse tree nodes
constitute arguments of the predication.
</bodyText>
<subsectionHeader confidence="0.999768">
3.1 An improved NomBank SRL baseline
</subsectionHeader>
<bodyText confidence="0.999982733333333">
To investigate the effects of implicit argumenta-
tion, we first developed a system based on previ-
ous markable-only approaches. Our system follows
many of the traditions above, but differs in the fol-
lowing ways. First, we replace the standard two-
stage pipeline with a single-stage logistic regression
model2 that predicts arguments directly. Second,
we model incorporated arguments (i.e., predicates
that are also arguments) with a simple maximum
likelihood model that predicts the most likely argu-
ment label for a predicate based on counts from the
training data. Third, we use the following heuris-
tics to resolve argument conflicts: (1) If two argu-
ments overlap, the one with the higher probability is
kept. (2) If two non-overlapping arguments are of
the same type, the one with the higher probability
is kept unless the two nodes are siblings, in which
case both are kept. Heuristic (2) accounts for split
argument constructions.
Our NomBank SRL system uses features that are
selected with a greedy forward search strategy sim-
ilar to the one used by Jiang and Ng (2006). The
top half of Table 2 (next page) lists the selected ar-
gument features.3 We extracted training nodes from
sections 2-21 of NomBank, used section 24 for de-
velopment and section 23 for testing. All parse
trees were generated by Charniak’s re-ranking syn-
tactic parser (Charniak and Johnson, 2005). Follow-
ing the evaluation methodology used by Jiang and
Ng (2006) and Liu and Ng (2007), we obtained sig-
</bodyText>
<footnote confidence="0.915953571428571">
1The syntactic parse can be based on ground-truth annota-
tion or derived automatically, depending on the evaluation.
2We use LibLinear (Fan et al., 2008).
3For features requiring the identification of support verbs,
we use the annotations provided in NomBank. Preliminary ex-
periments show a small loss when using automatic support verb
identification.
</footnote>
<table confidence="0.96119125">
Dev. Fl Testing Fl
Jiang and Ng (2006) 0.6677 0.6914
Liu and Ng (2007) - 0.7283
This paper 0.7454 0.7630
</table>
<tableCaption confidence="0.9899442">
Table 1: Markable-only NomBank SRL results for ar-
gument prediction using automatically generated parse
trees. The f-measure statistics were calculated by ag-
gregating predictions across all classes. “-” indicates
that the result was not reported.
</tableCaption>
<table confidence="0.9997755">
Markable-only All-token % loss
P 0.7955 0.6577 -17.32
R 0.7330 0.7247 -1.13
Fl 0.7630 0.6895 -9.63
</table>
<tableCaption confidence="0.9904505">
Table 3: Comparison of the markable-only and all-
token evaluations of the baseline argument model.
</tableCaption>
<bodyText confidence="0.767766">
nificantly better results, as shown in Table 1 above.4
</bodyText>
<subsectionHeader confidence="0.992406">
3.2 The effect of implicit nominal arguments
</subsectionHeader>
<bodyText confidence="0.999973294117647">
The presence of implicit nominal arguments
presents challenges that are not taken into account
by the evaluation described above. To assess the im-
pact of implicit arguments, we evaluated our Nom-
Bank SRL system over each token in the testing
section. The system attempts argument identifica-
tion for all singular and plural nouns that have at
least one annotated instance in the training portion
of the NomBank corpus (morphological variations
included).
Table 3 gives a comparison of the results from the
markable-only and all-token evaluations. As can be
seen, assuming that all known nouns take overt argu-
ments results in a significant performance loss. This
loss is due primarily to a drop in precision caused by
false positive argument predictions made for nomi-
nals with implicit arguments.
</bodyText>
<sectionHeader confidence="0.9360735" genericHeader="method">
4 Accounting for implicit arguments in
nominal SRL
</sectionHeader>
<bodyText confidence="0.999870666666667">
A natural solution to the problem described above
is to first distinguish nominals that bear overt
arguments from those that do not. We treat this
</bodyText>
<footnote confidence="0.52000875">
4As noted by Carreras and M`arquez (2005), the discrepancy
between the development and testing results is likely due to
poorer syntactic parsing performance on the development sec-
tion.
</footnote>
<page confidence="0.986574">
148
</page>
<table confidence="0.99154906">
# Description N S
Argument features 1 12 &amp; parse tree path from n to pred
2 Position of n relative to pred &amp; parse tree path from n to pred *
3 First word subsumed by n
4 12 &amp; position of n relative to pred
5 12 &amp; 14
6 Head word of n’s parent *
7 Last word subsumed n
8 n’s syntactic category &amp; length of parse tree path from n to pred
9 First word of n’s right sibling * *
10 Production rule that expands the parent of pred
11 Head word of the right-most NP in n if n is a PP *
12 Stem of pred
13 Parse tree path from n to the lowest common ancestor of n and pred
14 Head word of n
15 12 &amp; n’s syntactic category
16 Production rule that expands n’s parent * *
17 Parse tree path from n to the nearest support verb *
18 Last part of speech (POS) subsumed by n *
19 Production rule that expands n’s left sibling *
20 Head word of n, if the parent of n is a PP
21 The POS of the head word of the right-most NP under n if n is a PP
... Features 22-31 are available upon request 0 3
Nominal features 1 n’s ancestor subcategorization frames (ASF) (see section 4) *
2 n’s word
3 Syntactic category of n’s right sibling
4 Parse tree paths from n to each support verb *
5 Last word of n’s left sibling * *
6 Parse tree path from n to previous nominal, with lexicalized source (see section 4) *
7 Last word of n’s right sibling *
8 Production rule that expands n’s left sibling * *
9 Syntactic category of n *
10 PropBank markability score (see section 4) *
11 Parse tree path from n to previous nominal, with lexicalized source and destination *
12 Whether or not n is followed by PP *
13 Parse tree path from n to previous nominal, with lexicalized destination *
14 Head word of n’s parent *
15 Whether or not n surfaces before a passive verb * *
16 First word of n’s left sibling *
17 Parse tree path from n to closest support verb, with lexicalized destination *
18 Whether or not n is a head *
19 Head word of n’s right sibling
20 Production rule that expands n’s parent * *
21 Parse tree paths from n to all support verbs, with lexicalized destinations *
22 First word of n’s right sibling * *
23 Head word of n’s left sibling *
24 If n is followed by a PP, the head of that PP’s object *
25 Parse tree path from n to previous nominal *
26 Token distance from n to previous nominal *
27 Production rule that expands n’s grandparent *
</table>
<tableCaption confidence="0.998494">
Table 2: Features, sorted by gain in selection algorithm. &amp; denotes concatenation. The last two columns indicate
(N)ew features (not used in Liu and Ng (2007)) and features (S)hared by the argument and nominal models.
</tableCaption>
<page confidence="0.997946">
149
</page>
<bodyText confidence="0.999002954545454">
as a binary classification task over token nodes.
Once a nominal has been identified as bearing
overt arguments, it is processed with the argument
identification model developed in the previous
section. To classify nominals, we use the features
shown in the bottom half of Table 2, which were
selected with the same algorithm used for the
argument classification model. As shown by Table
2, the sets of features selected for argument and
nominal classification are quite different, and many
of the features used for nominal classification have
not been previously used. Below, we briefly explain
a few of these features.
Ancestor subcategorization frames (ASF)
As shown in Table 2, the most informative feature
is ASF. For a given token t, ASF is actually a set
of sub-features, one for each parse tree node above
t. Each sub-feature is indexed (i.e., named) by its
distance from t. The value of an ASF sub-feature
is the production rule that expands the correspond-
ing node in the tree. An ASF feature with two
sub-features is depicted below for the token “sale”:
</bodyText>
<equation confidence="0.985025333333333">
VP: A5F2 = V P —* V, NP
V (made) NP: A5F1 = NP —* Det, N
Det (a) N (sale)
</equation>
<bodyText confidence="0.999071352941177">
Parse tree path lexicalization A lexicalized parse
tree path is one in which surface tokens from the
beginning or end of the path are included in the path.
This is a finer-grained version of the traditional
parse tree path that captures the joint behavior of
the path and the tokens it connects. For example,
in the tree above, the path from “sale” to “made”
with a lexicalized source and destination would be
sale: N T NP T V P 1 V : made. Lexicalization
increases sparsity; however, it is often preferred
by the feature selection algorithm, as shown in the
bottom half of Table 2.
PropBank markability score This feature is
the probability that the context (f 5 words) of a de-
verbal nominal is generated by a unigram language
model trained over the PropBank argument words
for the corresponding verb. Entities are normalized
</bodyText>
<table confidence="0.99994">
Precision Recall Fl
Baseline 0.5555 0.9784 0.7086
MLE 0.6902 0.8903 0.7776
LibLinear 0.8989 0.8927 0.8958
</table>
<tableCaption confidence="0.996387">
Table 4: Evaluation results for identifying nominals
with explicit arguments.
</tableCaption>
<bodyText confidence="0.999889571428571">
to their entity type using BBN’s IdentiFinder, and
adverbs are normalized to their related adjective us-
ing the ADJADV dictionary provided by NomBank.
The normalization of adverbs is motivated by the
fact that adverbial modifiers of verbs typically have
a corresponding adjectival modifier for deverbal
nominals.
</bodyText>
<sectionHeader confidence="0.993142" genericHeader="evaluation">
5 Evaluation results
</sectionHeader>
<bodyText confidence="0.9995708">
Our evaluation methodology reflects a practical sce-
nario in which the nominal SRL system must pro-
cess each token in a sentence. The system can-
not safely assume that each token bears overt argu-
ments; rather, this decision must be made automat-
ically. In section 5.1, we present results for the au-
tomatic identification of nominals with overt argu-
ments. Then, in section 5.2, we present results for
the combined task in which nominal classification is
followed by argument identification.
</bodyText>
<subsectionHeader confidence="0.965167">
5.1 Nominal classification
</subsectionHeader>
<bodyText confidence="0.993927071428572">
Following standard practice, we train the nomi-
nal classifier over NomBank sections 2-21 using
LibLinear and automatically generated syntactic
parse trees. The prediction threshold is set to the
value that maximizes the nominal F1 score on
development section (24), and the resulting model
is tested over section 23. For comparison, we
implemented the following simple classifiers.
Baseline nominal classifier Classifies a token
as overtly bearing arguments if it is a singular or
plural noun that is markable in the training data.
As shown in Table 4, this classifier achieves nearly
perfect recall.5
MLE nominal classifier Operates similarly to
</bodyText>
<footnote confidence="0.995871">
5Recall is less than 100% due to (1) part-of-speech errors
from the syntactic parser and (2) nominals that were not anno-
tated in the training data but exist in the testing data.
</footnote>
<page confidence="0.990733">
150
</page>
<figure confidence="0.998333384615384">
% of nominal instances
0.06
0.09
0.08
0.07
0.05
0.04
0.03
0.02
0.01
0.1
0
Observed markable probability
</figure>
<figureCaption confidence="0.93566775">
(a) Distribution of nominals. Each interval on the x-axis denotes a set of nominals that are markable between (x−5)%
and x% of the time in the training data. The y-axis denotes the percentage of all nominal instances in TreeBank that
is occupied by nominals in the interval. Quartiles are marked below the intervals. For example, quartile 0.25 indicates
that one quarter of all nominal instances are markable 35% of the time or less.
</figureCaption>
<figure confidence="0.999887125">
Predicate nominal F1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Observed markable probability
Baseline
LibLinear
Argument F1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.
0
1
1
Observed markable probability
Baseline
MLE
LibLinear
</figure>
<figureCaption confidence="0.951283857142857">
(b) Nominal classification performance with respect to the
distribution in Figure 1a. The y-axis denotes the combined
Fl for nominals in the interval.
(c) All-token argument classification performance with re-
spect to the distribution in Figure 1a. The y-axis denotes the
combined Fl for nominals in the interval.
Figure 1: Evaluation results with respect to the distribution of nominals in TreeBank.
</figureCaption>
<bodyText confidence="0.999845230769231">
the baseline classifier, but also produces a score
for the classification. The value of the score is
equal to the probability that the nominal bears overt
arguments, as observed in the training data. A
prediction threshold is imposed on this score as
determined by the development data (t = 0.23).
As shown by Table 4, this exchanges recall for
precision and leads to a significant increase in the
overall F1 score.
The last row in Table 4 shows the results for
the LibLinear nominal classifier, which significantly
outperforms the others, achieving balanced preci-
sion and recall scores near 0.9. In addition, it is
able to recover from part-of-speech errors because
it does not filter out non-noun instances; rather, it
combines part-of-speech information with other lex-
ical and syntactic features to classify nominals.
Interesting observations can be made by grouping
nominals according to the probability with which
they are markable in the corpus. Figure 1a gives
the overall distribution of markable nominals in the
training data. As shown, 50% of nominal instances
are markable only 65% of the time or less, making
nominal classification an important first step. Using
this view of the data, Figure 1b presents the over-
all F1 scores for the baseline and LibLinear nominal
</bodyText>
<page confidence="0.996665">
151
</page>
<bodyText confidence="0.999868">
classifiers.6 As expected, gains in nominal classi-
fication diminish as nominals become more overtly
associated with arguments. Furthermore, nominals
that are rarely markable (i.e., those in interval 0.05)
remain problematic due to a lack of positive training
instances and the unbalanced nature of the classifi-
cation task.
</bodyText>
<subsectionHeader confidence="0.999725">
5.2 Combined nominal-argument classification
</subsectionHeader>
<bodyText confidence="0.999941864864865">
We now turn to the task of combined nominal-
argument classification. In this task, systems must
first identify nominals that bear overt arguments. We
evaluated three configurations based on the nominal
classifiers from the previous section. Each config-
uration uses the argument classification model from
section 3.
As shown in Table 3, overall argument classifi-
cation F1 suffers a loss of more than 9% under the
assumption that all known nouns bear overt argu-
ments. This corresponds precisely to using the base-
line nominal classifier in the combined nominal-
argument task. The MLE nominal classifier is able
to reduce this loss by 25% to an F1 of 0.7080. The
LibLinear nominal classifier reduces this loss by
46%, resulting in an overall argument classification
F1 of 0.7235. This improvement is the direct result
of filtering out nominal instances that do not bear
overt arguments.
Similarly to the nominal evaluation, we can view
argument classification performance with respect to
the probability that a nominal bears overt arguments.
This is shown in Figure 1c for the three configura-
tions. The configuration using the MLE nominal
classifier obtains an argument F1 of zero for nom-
inals below its prediction threshold. Compared to
the baseline nominal classifier, the LibLinear clas-
sifier achieves argument classification gains as large
as 150.94% (interval 0.05), with an average gain of
52.87% for intervals 0.05 to 0.4. As with nomi-
nal classification, argument classification gains di-
minish for nominals that express arguments more
overtly - we observe an average gain of only 2.15%
for intervals 0.45 to 1.00. One possible explana-
tion for this is that the argument prediction model
has substantially more training data for the nomi-
nals in intervals 0.45 to 1.00. Thus, even if the nom-
</bodyText>
<footnote confidence="0.83706">
6Baseline and MLE are identical above the MLE threshold.
</footnote>
<table confidence="0.999377333333333">
Nominals
Deverbal Deverbal-like Other
Baseline 0.7975 0.6789 0.6757
MLE 0.8298 0.7332 0.7486
LibLinear 0.9261 0.8826 0.8905
Arguments
Baseline 0.7059 0.6738 0.7454
MLE 0.7206 0.6641 0.7675
LibLinear 0.7282 0.7178 0.7847
</table>
<tableCaption confidence="0.975528">
Table 5: Nominal and argument Fl scores for dever-
bal, deverbal-like, and other nominals in the all-token
evaluation.
</tableCaption>
<bodyText confidence="0.999926285714286">
inal classifier makes a false positive prediction in the
0.45 to 1.00 interval range, the argument model may
correctly avoid labeling any arguments.
As noted in section 2, these results are not di-
rectly comparable to the results of the recent CoNLL
Shared Task (Surdeanu et al., 2008). This is due to
the fact that the semantic labeled F1 in the Shared
Task combines predicate and argument predictions
into a single score. The same combined F1 score for
our best two-stage nominal SRL system (logistic re-
gression nominal and argument models) is 0.7806;
however, this result is not precisely comparable be-
cause we do not identify the predicate role set as re-
quired by the CoNLL Shared Task.
</bodyText>
<subsectionHeader confidence="0.99736">
5.3 NomLez-based analysis of results
</subsectionHeader>
<bodyText confidence="0.999970263157895">
As demonstrated in section 1, NomBank annotates
many classes of deverbal and non-deverbal nomi-
nals, which have been categorized on syntactic and
semantic bases in NomLex-PLUS (Meyers, 2007b).
To help understand what types of nominals are par-
ticularly affected by implicit argumentation, we fur-
ther analyzed performance with respect to these
classes.
Figure 2a shows the distribution of nominals
across classes defined by the NomLex resource. As
shown in Figure 2b, many of the most frequent
classes exhibit significant gains. For example, the
classification of partitive nominals (13% of all nom-
inal instances) with the LibLinear classifier results
in gains of 55.45% and 33.72% over the baseline
and MLE classifiers, respectively. For the 5 most
common classes, which constitute 82% of all nomi-
nals instances, we observe average gains of 27.47%
and 19.30% over the baseline and MLE classifiers,
</bodyText>
<page confidence="0.984548">
152
</page>
<figure confidence="0.94082692">
(a) Distribution of nominals across the NomLex classes. The
y-axis denotes the percentage of all nominal instances that is
occupied by nominals in the class.
(b) Nominal classification performance with respect to the
NomLex classes in Figure 2a. The y-axis denotes the com-
bined Fl for nominals in the class.
NomLex class
% of nominal instances
0.45
0.35
0.25
0.15
0.05
0.5
0.4
0.3
0.2
0.
0
1
Predicate nominal F1
NomLex class
Baseline
MLI
LibLinear
</figure>
<figureCaption confidence="0.999801">
Figure 2: Evaluation results with respect to NomLex classes.
</figureCaption>
<bodyText confidence="0.999765894736842">
respectively.
Table 5 separates nominal and argument classifi-
cation results into sets of deverbal (NomLex class
nom), deverbal-like (NomLex class nom-like), and
all other nominalizations. A deverbal-like nominal
is closely related to some verb, although not mor-
phologically. For example, the noun accolade shares
argument interpretation with award, but the two are
not morphologically related. As shown by Table 5,
nominal classification tends to be easier - and ar-
gument classification harder - for deverbals when
compared to other types of nominals. The differ-
ence in argument F1 between deverbal/deverbal-like
nominals and the others is due primarily to relational
nominals, which are relatively easy to classify (Fig-
ure 2b); additionally, relational nominals exhibit a
high rate of argument incorporation, which is eas-
ily handled by the maximum-likelihood model de-
scribed in section 3.1.
</bodyText>
<sectionHeader confidence="0.996359" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999969323529412">
The application of nominal SRL to practical NLP
problems requires a system that is able to accurately
process each token it encounters. Previously, it was
unclear whether the models proposed by Jiang and
Ng (2006) and Liu and Ng (2007) would operate ef-
fectively in such an environment. The systems de-
scribed by Surdeanu et al. (2008) are designed with
this environment in mind, but their evaluation did
not focus on the issue of implicit argumentation.
These two problems motivate the work presented in
this paper.
Our contribution is three-fold. First, we improve
upon previous nominal SRL results using a single-
stage classifier with additional new features. Sec-
ond, we show that this model suffers a substantial
performance degradation when evaluated over nom-
inals with implicit arguments. Finally, we identify a
set of features - many of them new - that can be used
to reliably detect nominals with explicit arguments,
thus significantly increasing the performance of the
nominal SRL system.
Our results also suggest interesting directions for
future work. As described in section 5.2, many nom-
inals do not have enough labeled training data to
produce accurate argument models. The general-
ization procedures developed by Gordon and Swan-
son (2007) for PropBank SRL and Pad´o et al. (2008)
for NomBank SRL might alleviate this problem.
Additionally, instead of ignoring nominals with im-
plicit arguments, we would prefer to identify the im-
plicit arguments using information contained in the
surrounding discourse. Such inferences would help
connect entities and events across sentences, provid-
ing a fuller interpretation of the text.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997708">
The authors would like to thank the anonymous re-
viewers for their helpful suggestions. The first two
authors were supported by NSF grants IIS-0535112
and IIS-0347548, and the third author was supported
by NSF grant IIS-0534700.
</bodyText>
<page confidence="0.998878">
153
</page>
<sectionHeader confidence="0.995897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936577464788">
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Christian Boitet
and Pete Whitelock, editors, Proceedings of the Thirty-
Sixth Annual Meeting of the Association for Computa-
tional Linguistics and Seventeenth International Con-
ference on Computational Linguistics, pages 86–90,
San Francisco, California. Morgan Kaufmann Publish-
ers.
Xavier Carreras and Lluis M`arquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245–288.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of the 4th
International Workshop on Semantic Evaluations.
A. Gordon and R. Swanson. 2007. Generalizing seman-
tic role annotations across syntactically similar verbs.
In Proceedings ofACL, pages 192–199.
Z. Jiang and H. Ng. 2006. Semantic role labeling of
nombank: A maximum entropy approach. In Proceed-
ings of the 2006 Conference on Empirical Methods in
Natural Language Processing.
Maria Lapata. 2000. The automatic interpretation
of nominalizations. In Proceedings of the Seven-
teenth National Conference on Artificial Intelligence
and Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 716–721. AAAI Press /
The MIT Press.
Chang Liu and Hwee Ng. 2007. Learning predictive
structures for semantic role labeling of nombank. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 208–215,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Adam Meyers. 2007a. Annotation guidelines for nom-
bank - noun argument structure for propbank. Techni-
cal report, New York University.
Adam Meyers. 2007b. Those other nombank dictionar-
ies. Technical report, New York University.
Sebastian Pad´o, Marco Pennacchiotti, and Caroline
Sporleder. 2008. Semantic role assignment for event
nominalisations by leveraging verbal data. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 665–
672, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
106.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2005. Towards robust semantic role labeling. In Asso-
ciation for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159–177, Manchester,
England, August. Coling 2008 Organizing Committee.
</reference>
<page confidence="0.999771">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.317011">
<title confidence="0.999644">The Role of Implicit Argumentation in Nominal SRL</title>
<author confidence="0.996247">Matt</author>
<affiliation confidence="0.925726">Dept. of Computer Michigan State</affiliation>
<email confidence="0.992473">gerberm2@msu.edu</email>
<author confidence="0.98324">Y Joyce</author>
<affiliation confidence="0.8611985">Dept. of Computer Michigan State</affiliation>
<email confidence="0.999232">jchai@cse.msu.edu</email>
<author confidence="0.863355">Adam</author>
<affiliation confidence="0.994416">Dept. of Computer</affiliation>
<address confidence="0.678381">New York</address>
<email confidence="0.999824">meyers@cs.nyu.edu</email>
<abstract confidence="0.993976470588235">Nominals frequently surface without overtly expressed arguments. In order to measure the potential benefit of nominal SRL for downstream processes, such nominals must be accounted for. In this paper, we show that a state-of-the-art nominal SRL system with an argument 0.76 suffers a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation. We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%. Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argumentation an important step for nominal SRL.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles Fillmore</author>
<author>John Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>Proceedings of the ThirtySixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics,</booktitle>
<pages>86--90</pages>
<editor>In Christian Boitet and Pete Whitelock, editors,</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, California.</location>
<contexts>
<context position="1178" citStr="Baker et al., 1998" startWordPosition="175" endWordPosition="178">overall argument Fl of 0.76 suffers a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation. We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%. Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argumentation an important step for nominal SRL. 1 Introduction In the past few years, a number of studies have focused on verbal semantic role labeling (SRL). Driven by annotation resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), many systems developed in these studies have achieved argument F1 scores near 80% in large-scale evaluations such as the one reported by Carreras and M`arquez (2005). More recently, the automatic identification of nominal argument structure has received increased attention due to the release of the NomBank corpus (Meyers, 2007a). NomBank annotates predicating nouns in the same way that PropBank annotates predicating verbs. Consider the following example of the verbal predicate distribute from the PropBank corpus: (1) Freeport-McMoRan Energy Partners will be</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles Fillmore, and John Lowe. 1998. The Berkeley FrameNet project. In Christian Boitet and Pete Whitelock, editors, Proceedings of the ThirtySixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics, pages 86–90, San Francisco, California. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Lluis M`arquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9703" citStr="Charniak and Johnson, 2005" startWordPosition="1541" endWordPosition="1544">nts are of the same type, the one with the higher probability is kept unless the two nodes are siblings, in which case both are kept. Heuristic (2) accounts for split argument constructions. Our NomBank SRL system uses features that are selected with a greedy forward search strategy similar to the one used by Jiang and Ng (2006). The top half of Table 2 (next page) lists the selected argument features.3 We extracted training nodes from sections 2-21 of NomBank, used section 24 for development and section 23 for testing. All parse trees were generated by Charniak’s re-ranking syntactic parser (Charniak and Johnson, 2005). Following the evaluation methodology used by Jiang and Ng (2006) and Liu and Ng (2007), we obtained sig1The syntactic parse can be based on ground-truth annotation or derived automatically, depending on the evaluation. 2We use LibLinear (Fan et al., 2008). 3For features requiring the identification of support verbs, we use the annotations provided in NomBank. Preliminary experiments show a small loss when using automatic support verb identification. Dev. Fl Testing Fl Jiang and Ng (2006) 0.6677 0.6914 Liu and Ng (2007) - 0.7283 This paper 0.7454 0.7630 Table 1: Markable-only NomBank SRL resu</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="9960" citStr="Fan et al., 2008" startWordPosition="1584" endWordPosition="1587"> search strategy similar to the one used by Jiang and Ng (2006). The top half of Table 2 (next page) lists the selected argument features.3 We extracted training nodes from sections 2-21 of NomBank, used section 24 for development and section 23 for testing. All parse trees were generated by Charniak’s re-ranking syntactic parser (Charniak and Johnson, 2005). Following the evaluation methodology used by Jiang and Ng (2006) and Liu and Ng (2007), we obtained sig1The syntactic parse can be based on ground-truth annotation or derived automatically, depending on the evaluation. 2We use LibLinear (Fan et al., 2008). 3For features requiring the identification of support verbs, we use the annotations provided in NomBank. Preliminary experiments show a small loss when using automatic support verb identification. Dev. Fl Testing Fl Jiang and Ng (2006) 0.6677 0.6914 Liu and Ng (2007) - 0.7283 This paper 0.7454 0.7630 Table 1: Markable-only NomBank SRL results for argument prediction using automatically generated parse trees. The f-measure statistics were calculated by aggregating predictions across all classes. “-” indicates that the result was not reported. Markable-only All-token % loss P 0.7955 0.6577 -17</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--245</pages>
<contexts>
<context position="6500" citStr="Gildea and Jurafsky (2002)" startWordPosition="1012" endWordPosition="1015">licable to nominal argument structure. Early work in identifying the argument structure of deverbal nominalizations was primarily rulebased, using rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Sy</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28:245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations.</booktitle>
<contexts>
<context position="5550" citStr="Girju et al., 2007" startWordPosition="866" endWordPosition="869">om 0.7283 to 0.7630 using a single-stage classification approach. We show that this system, when applied to all nominal instances, achieves an argument F1 score of only 0.6895, a loss of more than 9%. We then present a model of implicit argumentation that reduces this loss by 46%, resulting in an F1 score of 0.7235 on the more complete evaluation task. In our analyses, we find that SRL performance varies widely among specific classes of nominals, suggesting interesting directions for future work. 2 Related work Nominal SRL is related to nominal relation interpretation as evaluated in SemEval (Girju et al., 2007). Both tasks identify semantic relations between a head noun and other constituents; however, the tasks focus on different relations. Nominal SRL focuses primarily on relations that hold between nominalizations and their arguments, whereas the SemEval task focuses on a range of semantic relations, many of which are not applicable to nominal argument structure. Early work in identifying the argument structure of deverbal nominalizations was primarily rulebased, using rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998).</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In Proceedings of the 4th International Workshop on Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gordon</author>
<author>R Swanson</author>
</authors>
<title>Generalizing semantic role annotations across syntactically similar verbs.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="27659" citStr="Gordon and Swanson (2007)" startWordPosition="4538" endWordPosition="4542">ifier with additional new features. Second, we show that this model suffers a substantial performance degradation when evaluated over nominals with implicit arguments. Finally, we identify a set of features - many of them new - that can be used to reliably detect nominals with explicit arguments, thus significantly increasing the performance of the nominal SRL system. Our results also suggest interesting directions for future work. As described in section 5.2, many nominals do not have enough labeled training data to produce accurate argument models. The generalization procedures developed by Gordon and Swanson (2007) for PropBank SRL and Pad´o et al. (2008) for NomBank SRL might alleviate this problem. Additionally, instead of ignoring nominals with implicit arguments, we would prefer to identify the implicit arguments using information contained in the surrounding discourse. Such inferences would help connect entities and events across sentences, providing a fuller interpretation of the text. Acknowledgments The authors would like to thank the anonymous reviewers for their helpful suggestions. The first two authors were supported by NSF grants IIS-0535112 and IIS-0347548, and the third author was support</context>
</contexts>
<marker>Gordon, Swanson, 2007</marker>
<rawString>A. Gordon and R. Swanson. 2007. Generalizing semantic role annotations across syntactically similar verbs. In Proceedings ofACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Jiang</author>
<author>H Ng</author>
</authors>
<title>Semantic role labeling of nombank: A maximum entropy approach.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4051" citStr="Jiang and Ng (2006)" startWordPosition="622" endWordPosition="625">e 2009. c�2009 Association for Computational Linguistics As in (2), distribution in (4) has a noun phrase and multiple prepositional phrases in its environment, but not one of these constituents is an argument to distribution in (4); rather, any arguments are implicitly supplied by the surrounding discourse. As described by Meyers (2007a), instances such as (2) are called “markable” because they contain overt arguments, and instances such as (4) are called “unmarkable” because they do not. In the NomBank corpus, only markable instances have been annotated. Previous evaluations (e.g., those by Jiang and Ng (2006) and Liu and Ng (2007)) have been based on markable instances, which constitute 57% of all instances of nominals from the NomBank lexicon. In order to use nominal SRL systems for downstream processing, it is important to develop and evaluate techniques that can handle markable as well as unmarkable nominal instances. To address this issue, we investigate the role of implicit argumentation for nominal SRL. This is, in part, inspired by the recent CoNLL Shared Task (Surdeanu et al., 2008), which was the first evaluation of syntactic and semantic dependency parsing to include unmarkable nominals.</context>
<context position="6615" citStr="Jiang and Ng (2006)" startWordPosition="1030" endWordPosition="1033">imarily rulebased, using rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008). In this task, systems were required to identify syntacti</context>
<context position="9406" citStr="Jiang and Ng (2006)" startWordPosition="1492" endWordPosition="1495">ood model that predicts the most likely argument label for a predicate based on counts from the training data. Third, we use the following heuristics to resolve argument conflicts: (1) If two arguments overlap, the one with the higher probability is kept. (2) If two non-overlapping arguments are of the same type, the one with the higher probability is kept unless the two nodes are siblings, in which case both are kept. Heuristic (2) accounts for split argument constructions. Our NomBank SRL system uses features that are selected with a greedy forward search strategy similar to the one used by Jiang and Ng (2006). The top half of Table 2 (next page) lists the selected argument features.3 We extracted training nodes from sections 2-21 of NomBank, used section 24 for development and section 23 for testing. All parse trees were generated by Charniak’s re-ranking syntactic parser (Charniak and Johnson, 2005). Following the evaluation methodology used by Jiang and Ng (2006) and Liu and Ng (2007), we obtained sig1The syntactic parse can be based on ground-truth annotation or derived automatically, depending on the evaluation. 2We use LibLinear (Fan et al., 2008). 3For features requiring the identification o</context>
<context position="26623" citStr="Jiang and Ng (2006)" startWordPosition="4373" endWordPosition="4376">ompared to other types of nominals. The difference in argument F1 between deverbal/deverbal-like nominals and the others is due primarily to relational nominals, which are relatively easy to classify (Figure 2b); additionally, relational nominals exhibit a high rate of argument incorporation, which is easily handled by the maximum-likelihood model described in section 3.1. 6 Conclusions and future work The application of nominal SRL to practical NLP problems requires a system that is able to accurately process each token it encounters. Previously, it was unclear whether the models proposed by Jiang and Ng (2006) and Liu and Ng (2007) would operate effectively in such an environment. The systems described by Surdeanu et al. (2008) are designed with this environment in mind, but their evaluation did not focus on the issue of implicit argumentation. These two problems motivate the work presented in this paper. Our contribution is three-fold. First, we improve upon previous nominal SRL results using a singlestage classifier with additional new features. Second, we show that this model suffers a substantial performance degradation when evaluated over nominals with implicit arguments. Finally, we identify </context>
</contexts>
<marker>Jiang, Ng, 2006</marker>
<rawString>Z. Jiang and H. Ng. 2006. Semantic role labeling of nombank: A maximum entropy approach. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The automatic interpretation of nominalizations.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>716--721</pages>
<publisher>AAAI Press / The MIT Press.</publisher>
<contexts>
<context position="6164" citStr="Lapata (2000)" startWordPosition="963" endWordPosition="965"> Both tasks identify semantic relations between a head noun and other constituents; however, the tasks focus on different relations. Nominal SRL focuses primarily on relations that hold between nominalizations and their arguments, whereas the SemEval task focuses on a range of semantic relations, many of which are not applicable to nominal argument structure. Early work in identifying the argument structure of deverbal nominalizations was primarily rulebased, using rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to</context>
</contexts>
<marker>Lapata, 2000</marker>
<rawString>Maria Lapata. 2000. The automatic interpretation of nominalizations. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 716–721. AAAI Press / The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Hwee Ng</author>
</authors>
<title>Learning predictive structures for semantic role labeling of nombank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>208--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2983" citStr="Liu and Ng (2007)" startWordPosition="454" endWordPosition="457">ions Arg0-Arg5 is consistent between the two corpora. In addition to deverbal (i.e., event-based) nominalizations, NomBank annotates a wide variety of nouns that are not derived from verbs and do not denote events. An example is given below of the partitive noun percent: (3) Hallwood owns about 11 [Predicate %] [Arg1 of Integra]. In this case, the noun phrase headed by the predicate % (i.e., “about 11% of Integra”) denotes a fractional part of the argument in position Arg1. Since NomBank’s release, a number of studies have applied verbal SRL techniques to the task of nominal SRL. For example, Liu and Ng (2007) reported an argument F1 of 0.7283. Although this result is encouraging, it does not take into account nominals that surface without overt arguments. Consider the following example: (4) The [Predicate distribution] represents [NP available cash flow] [PP from the partnership] [PP between Aug. 1 and Oct. 31]. 146 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 146–154, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics As in (2), distribution in (4) has a noun phrase and multiple prepositional phrases in its env</context>
<context position="6637" citStr="Liu and Ng (2007)" startWordPosition="1035" endWordPosition="1038"> rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008). In this task, systems were required to identify syntactic dependencies, verbal</context>
<context position="9791" citStr="Liu and Ng (2007)" startWordPosition="1557" endWordPosition="1560">lings, in which case both are kept. Heuristic (2) accounts for split argument constructions. Our NomBank SRL system uses features that are selected with a greedy forward search strategy similar to the one used by Jiang and Ng (2006). The top half of Table 2 (next page) lists the selected argument features.3 We extracted training nodes from sections 2-21 of NomBank, used section 24 for development and section 23 for testing. All parse trees were generated by Charniak’s re-ranking syntactic parser (Charniak and Johnson, 2005). Following the evaluation methodology used by Jiang and Ng (2006) and Liu and Ng (2007), we obtained sig1The syntactic parse can be based on ground-truth annotation or derived automatically, depending on the evaluation. 2We use LibLinear (Fan et al., 2008). 3For features requiring the identification of support verbs, we use the annotations provided in NomBank. Preliminary experiments show a small loss when using automatic support verb identification. Dev. Fl Testing Fl Jiang and Ng (2006) 0.6677 0.6914 Liu and Ng (2007) - 0.7283 This paper 0.7454 0.7630 Table 1: Markable-only NomBank SRL results for argument prediction using automatically generated parse trees. The f-measure sta</context>
<context position="14477" citStr="Liu and Ng (2007)" startWordPosition="2416" endWordPosition="2419">not n is a head * 19 Head word of n’s right sibling 20 Production rule that expands n’s parent * * 21 Parse tree paths from n to all support verbs, with lexicalized destinations * 22 First word of n’s right sibling * * 23 Head word of n’s left sibling * 24 If n is followed by a PP, the head of that PP’s object * 25 Parse tree path from n to previous nominal * 26 Token distance from n to previous nominal * 27 Production rule that expands n’s grandparent * Table 2: Features, sorted by gain in selection algorithm. &amp; denotes concatenation. The last two columns indicate (N)ew features (not used in Liu and Ng (2007)) and features (S)hared by the argument and nominal models. 149 as a binary classification task over token nodes. Once a nominal has been identified as bearing overt arguments, it is processed with the argument identification model developed in the previous section. To classify nominals, we use the features shown in the bottom half of Table 2, which were selected with the same algorithm used for the argument classification model. As shown by Table 2, the sets of features selected for argument and nominal classification are quite different, and many of the features used for nominal classificati</context>
<context position="26645" citStr="Liu and Ng (2007)" startWordPosition="4378" endWordPosition="4381">f nominals. The difference in argument F1 between deverbal/deverbal-like nominals and the others is due primarily to relational nominals, which are relatively easy to classify (Figure 2b); additionally, relational nominals exhibit a high rate of argument incorporation, which is easily handled by the maximum-likelihood model described in section 3.1. 6 Conclusions and future work The application of nominal SRL to practical NLP problems requires a system that is able to accurately process each token it encounters. Previously, it was unclear whether the models proposed by Jiang and Ng (2006) and Liu and Ng (2007) would operate effectively in such an environment. The systems described by Surdeanu et al. (2008) are designed with this environment in mind, but their evaluation did not focus on the issue of implicit argumentation. These two problems motivate the work presented in this paper. Our contribution is three-fold. First, we improve upon previous nominal SRL results using a singlestage classifier with additional new features. Second, we show that this model suffers a substantial performance degradation when evaluated over nominals with implicit arguments. Finally, we identify a set of features - ma</context>
</contexts>
<marker>Liu, Ng, 2007</marker>
<rawString>Chang Liu and Hwee Ng. 2007. Learning predictive structures for semantic role labeling of nombank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208–215, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
</authors>
<title>Annotation guidelines for nombank - noun argument structure for propbank.</title>
<date>2007</date>
<tech>Technical report,</tech>
<location>New York University.</location>
<contexts>
<context position="1543" citStr="Meyers, 2007" startWordPosition="233" endWordPosition="234">ection of implicit argumentation an important step for nominal SRL. 1 Introduction In the past few years, a number of studies have focused on verbal semantic role labeling (SRL). Driven by annotation resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), many systems developed in these studies have achieved argument F1 scores near 80% in large-scale evaluations such as the one reported by Carreras and M`arquez (2005). More recently, the automatic identification of nominal argument structure has received increased attention due to the release of the NomBank corpus (Meyers, 2007a). NomBank annotates predicating nouns in the same way that PropBank annotates predicating verbs. Consider the following example of the verbal predicate distribute from the PropBank corpus: (1) Freeport-McMoRan Energy Partners will be liquidated and [Arg1 shares of the new company] [Predicate distributed] [Arg2 to the partnership’s unitholders]. The NomBank corpus contains a similar instance of the deverbal nominalization distribution: (2) Searle will give [Arg0 pharmacists] [Arg1 brochures] [Arg1 on the use of prescription drugs] for [Predicate distribution] [Location in their stores]. This </context>
<context position="3770" citStr="Meyers (2007" startWordPosition="579" endWordPosition="580">ample: (4) The [Predicate distribution] represents [NP available cash flow] [PP from the partnership] [PP between Aug. 1 and Oct. 31]. 146 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 146–154, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics As in (2), distribution in (4) has a noun phrase and multiple prepositional phrases in its environment, but not one of these constituents is an argument to distribution in (4); rather, any arguments are implicitly supplied by the surrounding discourse. As described by Meyers (2007a), instances such as (2) are called “markable” because they contain overt arguments, and instances such as (4) are called “unmarkable” because they do not. In the NomBank corpus, only markable instances have been annotated. Previous evaluations (e.g., those by Jiang and Ng (2006) and Liu and Ng (2007)) have been based on markable instances, which constitute 57% of all instances of nominals from the NomBank lexicon. In order to use nominal SRL systems for downstream processing, it is important to develop and evaluate techniques that can handle markable as well as unmarkable nominal instances. </context>
<context position="24266" citStr="Meyers, 2007" startWordPosition="4005" endWordPosition="4006">e fact that the semantic labeled F1 in the Shared Task combines predicate and argument predictions into a single score. The same combined F1 score for our best two-stage nominal SRL system (logistic regression nominal and argument models) is 0.7806; however, this result is not precisely comparable because we do not identify the predicate role set as required by the CoNLL Shared Task. 5.3 NomLez-based analysis of results As demonstrated in section 1, NomBank annotates many classes of deverbal and non-deverbal nominals, which have been categorized on syntactic and semantic bases in NomLex-PLUS (Meyers, 2007b). To help understand what types of nominals are particularly affected by implicit argumentation, we further analyzed performance with respect to these classes. Figure 2a shows the distribution of nominals across classes defined by the NomLex resource. As shown in Figure 2b, many of the most frequent classes exhibit significant gains. For example, the classification of partitive nominals (13% of all nominal instances) with the LibLinear classifier results in gains of 55.45% and 33.72% over the baseline and MLE classifiers, respectively. For the 5 most common classes, which constitute 82% of a</context>
</contexts>
<marker>Meyers, 2007</marker>
<rawString>Adam Meyers. 2007a. Annotation guidelines for nombank - noun argument structure for propbank. Technical report, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
</authors>
<title>Those other nombank dictionaries.</title>
<date>2007</date>
<tech>Technical report,</tech>
<location>New York University.</location>
<contexts>
<context position="1543" citStr="Meyers, 2007" startWordPosition="233" endWordPosition="234">ection of implicit argumentation an important step for nominal SRL. 1 Introduction In the past few years, a number of studies have focused on verbal semantic role labeling (SRL). Driven by annotation resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), many systems developed in these studies have achieved argument F1 scores near 80% in large-scale evaluations such as the one reported by Carreras and M`arquez (2005). More recently, the automatic identification of nominal argument structure has received increased attention due to the release of the NomBank corpus (Meyers, 2007a). NomBank annotates predicating nouns in the same way that PropBank annotates predicating verbs. Consider the following example of the verbal predicate distribute from the PropBank corpus: (1) Freeport-McMoRan Energy Partners will be liquidated and [Arg1 shares of the new company] [Predicate distributed] [Arg2 to the partnership’s unitholders]. The NomBank corpus contains a similar instance of the deverbal nominalization distribution: (2) Searle will give [Arg0 pharmacists] [Arg1 brochures] [Arg1 on the use of prescription drugs] for [Predicate distribution] [Location in their stores]. This </context>
<context position="3770" citStr="Meyers (2007" startWordPosition="579" endWordPosition="580">ample: (4) The [Predicate distribution] represents [NP available cash flow] [PP from the partnership] [PP between Aug. 1 and Oct. 31]. 146 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 146–154, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics As in (2), distribution in (4) has a noun phrase and multiple prepositional phrases in its environment, but not one of these constituents is an argument to distribution in (4); rather, any arguments are implicitly supplied by the surrounding discourse. As described by Meyers (2007a), instances such as (2) are called “markable” because they contain overt arguments, and instances such as (4) are called “unmarkable” because they do not. In the NomBank corpus, only markable instances have been annotated. Previous evaluations (e.g., those by Jiang and Ng (2006) and Liu and Ng (2007)) have been based on markable instances, which constitute 57% of all instances of nominals from the NomBank lexicon. In order to use nominal SRL systems for downstream processing, it is important to develop and evaluate techniques that can handle markable as well as unmarkable nominal instances. </context>
<context position="24266" citStr="Meyers, 2007" startWordPosition="4005" endWordPosition="4006">e fact that the semantic labeled F1 in the Shared Task combines predicate and argument predictions into a single score. The same combined F1 score for our best two-stage nominal SRL system (logistic regression nominal and argument models) is 0.7806; however, this result is not precisely comparable because we do not identify the predicate role set as required by the CoNLL Shared Task. 5.3 NomLez-based analysis of results As demonstrated in section 1, NomBank annotates many classes of deverbal and non-deverbal nominals, which have been categorized on syntactic and semantic bases in NomLex-PLUS (Meyers, 2007b). To help understand what types of nominals are particularly affected by implicit argumentation, we further analyzed performance with respect to these classes. Figure 2a shows the distribution of nominals across classes defined by the NomLex resource. As shown in Figure 2b, many of the most frequent classes exhibit significant gains. For example, the classification of partitive nominals (13% of all nominal instances) with the LibLinear classifier results in gains of 55.45% and 33.72% over the baseline and MLE classifiers, respectively. For the 5 most common classes, which constitute 82% of a</context>
</contexts>
<marker>Meyers, 2007</marker>
<rawString>Adam Meyers. 2007b. Those other nombank dictionaries. Technical report, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Caroline Sporleder</author>
</authors>
<title>Semantic role assignment for event nominalisations by leveraging verbal data.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>665--672</pages>
<location>Manchester, UK,</location>
<marker>Pad´o, Pennacchiotti, Sporleder, 2008</marker>
<rawString>Sebastian Pad´o, Marco Pennacchiotti, and Caroline Sporleder. 2008. Semantic role assignment for event nominalisations by leveraging verbal data. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 665– 672, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="1213" citStr="Palmer et al., 2005" startWordPosition="181" endWordPosition="184">s a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation. We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%. Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argumentation an important step for nominal SRL. 1 Introduction In the past few years, a number of studies have focused on verbal semantic role labeling (SRL). Driven by annotation resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), many systems developed in these studies have achieved argument F1 scores near 80% in large-scale evaluations such as the one reported by Carreras and M`arquez (2005). More recently, the automatic identification of nominal argument structure has received increased attention due to the release of the NomBank corpus (Meyers, 2007a). NomBank annotates predicating nouns in the same way that PropBank annotates predicating verbs. Consider the following example of the verbal predicate distribute from the PropBank corpus: (1) Freeport-McMoRan Energy Partners will be liquidated and [Arg1 shares of the</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6747" citStr="Pradhan et al., 2005" startWordPosition="1052" endWordPosition="1055">6; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008). In this task, systems were required to identify syntactic dependencies, verbal and nominal predicates, and semantic dependencies (i.e., arguments) for the predicates. For nominals, the bes</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2005</marker>
<rawString>Sameer Pradhan, Wayne Ward, and James H. Martin. 2005. Towards robust semantic role labeling. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<location>Manchester, England,</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177, Manchester, England, August. Coling 2008 Organizing Committee.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>