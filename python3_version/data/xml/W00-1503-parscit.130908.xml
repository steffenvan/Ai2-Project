<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000288">
<title confidence="0.994239666666667">
An Experiment in Unifying Audio-Visual and Textual
Infrastructures for Language Processing Research and
Development
</title>
<author confidence="0.9655315">
Kalina Bontchevat, Hennie Brugmant, Albert Russelt,
Peter Wittenburgt, Hamish Cunninghamt
</author>
<affiliation confidence="0.968041">
t Department of Computer Science, University of Sheffield, Sheffield, UK
</affiliation>
<email confidence="0.454078">
&lt;kalina,hamishndcs.shef.ac.uk
</email>
<affiliation confidence="0.656224">
t Max-Plank Institute for Psycholinguistics, Nijmegen, The Netherlands
</affiliation>
<email confidence="0.304929">
&lt;firstname.lastnamenmpi.n1
</email>
<sectionHeader confidence="0.982216" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755666666667">
This paper describes an experimental integration of
two infrastructures (Eudico and GATE) which were
developed independently of each other; for different
media (video/speech vs. text) and applications. The
integration resulted into gaining an in-depth under-
standing of the functionality and operation of each
of the two systems in isolation, and the benefits of
their combined use. It also highlighted some issues
(e.g., distributed access) which need to be addressed
in future work. The experiment also showed clearly
the advantages of modularity and generality adopted
in both systems.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998384898305085">
This paper describes the integration of two infras-
tructures (Eudico and GATE) which were developed
independently of each other; and for different me-
dia (video/speech vs. text) and applications. Such
integration was needed in order to have an appli-
cation where the end users (linguists and language
engineers) who annotate video and speech corpora
with textual transcriptions in Eudico, can also ben-
efit from language processing tools and viewers from
GATE.
Eudico (European Distributed Corpora) is a dis-
tributed multimedia infrastructure supporting cre-
ation, presentation and analysis of annotations of
speech and video corpora (Brugman et al., 1998).
Annotations of all kinds of user-definable types can
be time-aligned with the speech/video data so that
dynamic and simultaneous viewing is possible. For
example, when the user sets a new media time this
time is reflected in all annotation viewers, and, vice
versa, when the user selects an annotation this time
selection is shown in all viewers, including the media
player: viewing of media and transcription data is
synchronised.
GATE (General Architecture for Text Engineer-
ing) (Cunningham et al., 1997; Cunningham et
al., 1999) is an architecture, framework, and devel-
opment environment, providing representation and
storage of language data together with infrastruc-
tural support for building and deploying language
engineering applications. Its plug-and-play support
for processing modules and data viewers lowers the
overhead of building such applications and facilitates
code-reuse.
The idea behind this experiment is to combine
and build on the strengths of these two architec-
tures, thus bridging the gap between transcriptions
of speech and video, and language engineering tools.
From the user&apos;s perspective, this entails simultane-
ous manipulation of media, transcriptions and lin-
guistic data in a uniform and synchronised way. The
integration with GATE provides Eudico applications
with ways to represent, display and manipulate lin-
guistic data and, more importantly, to run GATE
language processing modules on the text transcrip-
tions (e.g., part-of-speech tagger, name-entity recog-
niser). In this way, linguists and language engineers
developing speech/video corpora are assisted in the
corpus annotation task by language processing tools
and viewers.
The main question that had to be answered is
whether it is possible to integrate into one appli-
cation two separate architectures. The major differ-
ence between the two comes from the media struc-
turing: speech/video annotation in Eudico is time-
based while text annotation in GATE is offset-based.
Therefore, we had to find a way of storing and ac-
cessing time information for offset-based linguistic
objects. From an implementational viewpoint, this
entailed:
</bodyText>
<listItem confidence="0.991789">
1. Mapping objects from the GATE world to ob-
jects in the Eudico world in such a way that Eu-
dico&apos;s views give a meaningful representation of
GATE data, while keeping their dynamic and
synchronised nature.
2. Embedding GATE viewers in Eudico viewers
and making them time aware with minimal
rewriting of existing code.
</listItem>
<bodyText confidence="0.99995725">
The next three sections are devoted to discussing
the design and implementational aspects of these
two problems. Section 2 describes how the ac-
tual mapping of objects between the architectures is
done. Section 3 describes the synchronisation mech-
anism allowing GATE and Eudico viewers to operate
and update in parallel with the media being played.
Section 4 describes the specific GATE viewers used
in this project. The pilot application is described in
section 5. Section 6 concludes this report by sum-
marising the outcome of this work and pointing to a
set of open issues.
</bodyText>
<sectionHeader confidence="0.656915" genericHeader="method">
2 Integrating the Two Data Models
</sectionHeader>
<subsectionHeader confidence="0.896569">
2.1 Eudico Data Model
</subsectionHeader>
<bodyText confidence="0.999836714285714">
The key concepts underlying Eudico&apos;s data model
are:
Corpus - a collection of Transcriptions (or of sub-
corpora).
Transcription - all annotations that refer to
one media file, or that describe one uninterrupted
recorded event.
Tier - a collection of Tags that are strictly con-
secutive in time and that annotate one specific phe-
nomenon. Tiers can also have meta information (e.g.
name, transcriber).
Tag - a typed set of values applying to a time
interval. A Tag is part of exactly one Tier. Tags
on the same Tier may not overlap in time. Tags are
either explicitly linked to media time or ordered in
time.
These concepts are part of Eudico&apos;s Abstract Cor-
pus Model (ACM). The ACM was designed to ab-
stract the specifics of corpus annotation formats
from the tools that work on the annotation data,
thus making Eudico corpus format independent.
</bodyText>
<subsectionHeader confidence="0.986777">
2.2 GATE Data Model
</subsectionHeader>
<bodyText confidence="0.995756736842105">
GATE&apos;s data model is based on the TIPSTER ar-
chitecture (Grishman, 1996). The main classes are:
Collection - a set of Documents which can be
loaded, stored, and processed together.
Document - consists of a document content (e.g.
text) and a set of Annotation objects associated to
the document content by means of spans (objects
specifying the begin and end offset of the annota-
tion).
Annotation - has a set of spans (specify the text
parts covered by this annotation), type (e.g. part-of-
speech (POS)), and a set of Attribute objects which
hold further information about the annotation (e.g.
time,12345, category,&amp;quot; Noun&amp;quot;).
Attribute- a feature-value pair which can hold any
type of data as a value. Attributes can be associated
to any of the above classes - collection (e.g., creator),
document (e.g., language, media type), and annota-
tion (see above).
</bodyText>
<subsectionHeader confidence="0.946167">
2.3 Mapping between the two worlds
</subsectionHeader>
<bodyText confidence="0.999959028571428">
There is a two-way mapping between Eudico
and GATE objects since GATE objects are con-
structed from Eudico ones (with a special &apos;Eudico-
to-Gate&apos;tool) that were initialised from another ex-
isting corpus, and, vice versa, Eudico objects are
created from GATE ones when a stored Gate col-
lection is loaded in Eudico for further processing.
This mapping is realised by implementing the proper
parts of the Eudico&apos;s ACM using GATE&apos;s API and
data structures. We chose to specify the mapping
only for a set of data that can be treated in a mean-
ingful way in both environments, namely transcribed
speech utterances.
Eudico corpora are mapped to GATE collections.
All transcribed utterances on a given media file are
ordered according to their place in the original file
and form a Document. All the information in the
document is accessed through time attributes and
annotation spans. In this way, only the relevant doc-
ument part (s) are manipulated at each given time
point. All Tags from the Tiers will are added as
Annotations of type utterance. Tier information
is encoded as an Attribute on the utterance anno-
tations created for the Tags. Tier objects can be
re-created by selecting all utterance annotations
which have the same value for the attribute tier
(see the example below). In order to provide a two
way link between Tags and utterance annotations,
all Tags have associated span information and all
utterances have associated time information.
Based on the utterance annotations, which pro-
vide a link to Eudico&apos;s time-based world, new lin-
guistic data in the form of other annotations can be
added now to the media corpus.
We experimented with two types of such data:
</bodyText>
<listItem confidence="0.9991045">
• POS - part of speech annotations which are cur-
rently obtained from hand-annotation.
• syntaxTree - syntax trees obtained by manual
annotation.
</listItem>
<subsectionHeader confidence="0.967707">
2.4 Example
</subsectionHeader>
<bodyText confidence="0.966200294117647">
The following example of the mapping is based on
an example of a time-aligned file from the CHILDES
Corpus (MacWhinney, 1999).
@Begin
@Filename: boys73.cha
@Participants
ROS Ross Child, MAR Mark Child,
FAT Brian Father, MOT Mary Mother
*ROS: yahoo.
Yosnd: &amp;quot;boys73a.aiff&amp;quot; 7349 8338
*FAT: you got a lot more to do # don&apos;t you?
Yosnd: &amp;quot;boys73a.aiff&amp;quot; 8607 9999
*MAR: yeah.
Yosnd: &amp;quot;boys73a.aiff&amp;quot; 10482 10839
*MAR: because I&apos;m not ready to
go to &lt;the bathroom&gt;[&gt;] +/.
Yosnd: &amp;quot;boys73a.aiff&amp;quot; 11621 13784
</bodyText>
<listItem confidence="0.663662">
• Associate the span of this annotation with a
</listItem>
<bodyText confidence="0.952161941176471">
time interval that is taken from a time selection
set with the Eudico media player.
This file is first parsed and Eudico Tiers and Tags
are created as follows:
Tiers: ROS, MAR, FAT, MOT. 3.2 Using Time Information to Synchronise
Tags for ROS: {(7349, 8338, &amp;quot;yahoo&amp;quot;)} GATE Viewers
Tags for FAT: {(8607, 9999, Eudico has a time-handling mechanism where, at the
you got a lot more to do# don&apos;t you?&amp;quot;} time of creation of a new viewer, all relevant time
Tags for MAR: {(10482, 10839, &amp;quot;yeah&amp;quot;), n- points are registered with the media player. These
(11621, 13784, because. .+/.
Speech utterances are combined into a GATE doc-
ument:
yahoo. You got a lot more to do# don&apos;t you?
0...15...110..115..120..125..130..135..140..
yeah. because I&apos;m not ready to go to...
45..150..155..160..165..170..175..180..
Utterance annotations are created for each Tag:
</bodyText>
<table confidence="0.9964773">
Id Type Span Span Attribute
start end
1 utterance 0 5 Tier=ROS,
Time=(7349,8338)
2 utterance 7 43 Tier=FAT,
Time=(8607,9999)
3 utterance 45 49 Tier=MAR,
Time=(10482,10839)
4 utterance 51 102 Tier=MAR,
Time=(11621,13784)
</table>
<sectionHeader confidence="0.986636" genericHeader="method">
3 Time-based Synchronisation
</sectionHeader>
<subsectionHeader confidence="0.9965035">
3.1 Encoding Time Information for
Linguistic Objects
</subsectionHeader>
<bodyText confidence="0.9999235">
Eudico objects can be time-aligned which provides
a way of synchronising all Eudico viewers by al-
ways showing the information relevant to the current
point of time in the media. Therefore it is desirable
for GATE linguistic objects to be time-aligned as
well when such information is available.
This is achieved by encoding the time in millisec-
onds as a value of an Attrzbute object, which is then
associated with linguistic Annotatzon objects (e.g.
POS annotations). In this way, annotations with such
an attribute are linked to the media time and can
be manipulated and displayed in the same way as
Eudico objects.
For example, given a transcription tier for one
speaker, POS annotations are added using GATE.
Each POS annotation has to be associated with both
a time interval and a text span.
The procedure is as follows:
</bodyText>
<listItem confidence="0.95512675">
• Use GATE to add POS Annotations
• Display the POS annotations in a GATE viewer
• Select an Annotation in this viewer (for multi-
span annotations, select one of the spans)
</listItem>
<bodyText confidence="0.999514117647059">
time points are derived from the time information
of the annotation objects that are to be displayed
in the viewer. During media playback an event is
generated for each timepoint and based on these
events, the viewers update themselves to reflect the
current time in their own specific way. This mech-
anism was extended to the domain of GATE view-
ers in a non-trivial way. Extra timepoints are taken
from the attributes of time-aligned annotations and
also registered with the media player. Additional
events are generated at play back time and passed
on by the Eudico viewer to the proper embedded
GATE viewer. These events can then be used by the
GATE viewer to show/highlight annotations at the
appropriate time, resulting in time synchronisation
between all Eudico and GATE viewers. Examples
are given below.
</bodyText>
<sectionHeader confidence="0.958746" genericHeader="method">
4 Re-using GATE Viewers inside
the Eudico Interface
</sectionHeader>
<subsectionHeader confidence="0.9636215">
4.1 Disguising GATE Viewers as &amp;quot;Native&amp;quot;
Eudico GUI classes
</subsectionHeader>
<bodyText confidence="0.999974269230769">
In order to provide creation, editing, and visuali-
sation of linguistic data, we embedded GATE GUI
modules into classes implementing Eudico&apos;s inter-
face for a panel displaying a single Tag. In this way,
the various Eudico viewers can manipulate them in
the same way as the &amp;quot;native&amp;quot; Eudico ones. The
wrapper classes also provide the time synchronisa-
tion functionality described in the previous section.
We experimented with two GATE viewers - POS
and SyntaxTree viewers - corresponding to the cho-
sen linguistic annotations.
The POS viewer works at the level of orthographic
transcription because that is a type of text that can
be handled meaningfully in both the GATE and Eu-
dico domains. Utterances at this level are almost
all already time-aligned which means that the POS
viewer can operate sensibly even if no finer time
alignment exists. Words in the utterances can be
selected for manual annotation with part of speech
data (the annotation dialog is shown in Figure 1).
These annotations are then visualised by textually
aligning them with the proper span of the speech
utterance.
The TreeViewer displays an utterance together
with one or more (partial) syntax trees of sentences
within this utterance (see Figure 2). The annota-
</bodyText>
<figure confidence="0.998839333333334">
NAnnotation Viewer AIM -10IX
yel rd th e pietureT ghow kerne
Conj DET N V
type:
text:
span :
pos :
time:
Fran
Proprl
VTRANS
AuW
Carl
ADV
RI POS Entry Dialog
pos (Part Of Speech)
some
I 385 : 389
</figure>
<figureCaption confidence="0.999989">
Figure 1: The Part-Of-Speech (POS) viewer
</figureCaption>
<table confidence="0.984484733333333">
VP
NP
V
NP
PropN
VTRANS
PP
Conj
AdJP
I- VP —1
ADV
ADJ
PREP
Pron DET Pron Adj
This is This another
</table>
<figure confidence="0.93007">
a test sentence. is
idDET
</figure>
<figureCaption confidence="0.999963">
Figure 2: The TreeViewer
</figureCaption>
<bodyText confidence="0.955384">
tion process starts from the words and proceeds up-
wards. The appropriate category can be selected
from a menu of availabe categories which is con-
structed from the corresponding stereotype (see fol-
lowing section). Non-terminal nodes can be com-
bined into higher-level non-terminals (e.g. V and
</bodyText>
<footnote confidence="0.5068525">
NP into a VP) by selecting all relevant nodes for the
new category and selecting the category itself from
the menu. Deletion of selected nodes is avaialble too.
Similar to all GATE viewers, both the POS
and Syntax Tree viewers are implemented as Java
Beans which enables their easy reuse and config-
</footnote>
<bodyText confidence="0.996421666666667">
uration. On-line demonstrations of GATE appli-
cations embedding these viewers are available at
http://www.gate.ac.ukl.
</bodyText>
<subsectionHeader confidence="0.9455955">
4.2 Configuring Viewers for Types of
Annotations
</subsectionHeader>
<bodyText confidence="0.999345416666667">
Annotations in GATE are flexible and unconstrained
data structures. This has the advantage that the
architecture is theory-neutral, and can represent a
wide range of data. The disadvantage is that the ap-
plication using them needs meta-information about
annotations in order to allow proper creation, edit-
ing, and visualisation. Therefore, annotation stereo-
types are used for encoding this information, which
is then used for configuring the behaviour of the
GATE viewers and editors.
These stereotypes (AnnotationStereotype class)
have
</bodyText>
<listItem confidence="0.990683">
• annotation type (e.g. POS, SyntTree)
• annotation structure type (single span, multiple
span, tree or graph)
• a set of attribute stereotypes specifying the
name of the attribute feature, the type of per-
missible values (e.g., String), and a list of per-
missible values where applicable.
</listItem>
<bodyText confidence="0.998371785714286">
For example, the stereotype of POS annotations
is given in Table 4.1.
Viewers use stereotypes to define what annota-
tion types they can visualise. For example, the syn-
tax tree viewer can register itself as supporting an-
notations with structure type tree, or even more
specifically, annotations of type SyntaxTree. Cre-
ation/editing of annotations also uses stereotypes to
control what attributes and values can be entered.
For instance, in the POSEntryDialog in Figure 1 the
list of permissible categories is taken from the per-
missible values for the attribute cat as defined in
the POS stereotype (see Table 4.1 for some example
values)2.
</bodyText>
<sectionHeader confidence="0.937745" genericHeader="method">
5 The Showcase Application
</sectionHeader>
<bodyText confidence="0.999881333333333">
The publicly-available showcase application&apos; cur-
rently demonstrates the time syncronisation and
viewing of linguistic data provided by the integration
of the two architectures. The application uses sound
media annotated with utterances for each speaker4.
The data is taken from the ESF corpus&apos; and im-
</bodyText>
<footnote confidence="0.999250818181818">
1The demos require a browser (e.g., Netscape, Internet
Explorer) with enabled Java and Java applets.
2Eudico&apos;s Abstract Corpus Model includes comparable
concepts, but no effort to integrate at this level was made
in the scope of the pilot project.
3 Available for download under demos at
http: //www . gate .ac .uk.
4 Video data could have been used just as easily, but at
the cost of having the user install the Java Media Framework
(JMF) on her system
5http://www.mpi.nl/world/tg/lapp/esf/esf.html
</footnote>
<bodyText confidence="0.99663096">
ported via Eudico into a GATE document with an-
notations. Afterwards some utterances were manu-
ally annotated with part-of-speech information using
the GATE POS Viewer (see the figure above). The
result is stored using GATE&apos;s persistence mechanism
and is read every time the application runs.
The showcase application allows playback and
viewing of media and associated linguistic data. It
demonstrates the synchronysed operation of GATE&apos;s
linguistic viewers embedded into Eudico&apos;s media and
annotation viewers. The screen shot below shows
two types of Eudico viewers used - subtitle and tag
list viewer&apos;. The subtitle viewer (the window with
lielal lp.wav caption) shows the current utterance
for each speaker (in this case, INN and SLA) and
controls the media playback. The other two Eudico
viewers (the windows with GATE List View cap-
tions) are tag list viewers, one for each speaker, and
integrate many instances of the GATE&apos;s POS viewer.
Each instance displays a particular utterance and all
POS annotations related to it.
The embedding Eudico viewers take care of layout,
scrolling, update and time synchronisation of the
embedded GATE components. They obtain the rel-
evant time data from each GATE POS viewer&apos;s an-
notation data and register it with Eudico&apos;s time syn-
chronisation mechanism. They also receive all time
events and direct them to the appropriate GATE
viewer which then highlights the word which is cur-
rently played as a sound by the media player.
In the example screen shot in Figure 3, the cur-
rent word is day. A few milliseconds later, another
time event would cause the highlight to be moved
to the word and, which is the next word to be spo-
ken. When no POS data is available (as for the
word this), the last word remains highlighted until
the next time-aligned annotation becomes current
or the end of the utterance is reached. The Eudico
viewers also take care of displaying a moving red bar
in front of the currently playing utterance.
In addition to this synchronous playing behaviour,
time synchronisation is exploited in some other
ways: it is possible to select an utterance in one
of the tag list viewers. This (time) selection is then
reflected in each of the other viewers (by means of
a blue bar in front of the overlapping utterances).
The media time is automatically reset to the begin
time of the selection. Manipulating the media time
by dragging the media player&apos;s slider is reflected by
the red bar and text highlight in the other viewers.
</bodyText>
<sectionHeader confidence="0.992568" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.956513666666667">
This paper described the integration of two infras-
tructures (Eudico and GATE) which were devel-
oped independently of each other; for different me-
</bodyText>
<footnote confidence="0.9638835">
6For examples and a discussion of all Eudico viewers see
http://www.mpi.nl/world/tg/lapp/eudico/eudico.html
</footnote>
<figure confidence="0.913412860465116">
omap alp jo 40LIS 1100.13S V : anam
!lel&amp; p.wav PIEM
000010.800 I.&gt; 111
00.00.10.9g /00:00:40.90
utterance erm this is an indian man yes?
POS Pron V DET ADJ N ADV
O and i would like you this is what he does every day # and i would like lfdyou 3:4# i would like you to tell me.
Conj Pron AuJN V Pron Pron V WV Pron V ADJ N Conj Pron AuJN V Pron Pron AuW V Pron V Pron
utterance what he is doing.
pos Pron Pron V N
1 utterance
utterance this HI is an indian man.
pOS Pron V DET ADJ N
utterance yes and the pictures show some of the things he does every day.
.. ri T NI Prnn V h n I NI
.onc-aro_c
ks-LA - GATE List Vio■fir 1111712
utterance yes 00:00:07.120 - 00:00:07.458
pos ADV 00:00:19.537 - 0000-19.917
utterance 00:00:21.072 - 00:00:22.896
P05 0000:23.169 - 0000:24.556
utterance 00:00:30.014 - 00:00:30.815
pOS
utterance
P05
utterance
pos
yes.
ADV
excuse me i dont understand
V Pron Pron AuW V
okay [1].
yes yes
ADV ADV
INN
INN and i would like you this is what he does every day# and i would I
00:00:07.980 - 00:00:19.438
SLA
SLA
annotationType POS
AnnotationStructureType Single span
AttributeStereotypes cat String det, n, adj, v, prep, conj, aux
Time Long
</figure>
<tableCaption confidence="0.98586">
Table 1: Stereotype for POS annotations
</tableCaption>
<bodyText confidence="0.999988097560976">
dia (video/speech vs. text) and applications. Such
integration was needed in order to have an appli-
cation where the end users (linguists and language
engineers) who annotate video and speech corpora
with textual transcriptions in Eudico, can also ben-
efit from language processing tools and viewers from
GATE.
The experiment lead to gaining an in-depth under-
standing of the functionality and operation of each
of the two frameworks in isolation, and the benefits
of their combined use. The showcase application ex-
emplifies how the strengths of each framework have
been combined to achieve seamless integration be-
tween speech, transcribed utterances, and linguistic
data. The novel aspect of this integration is the
resulting close inter-operation of the two infrastruc-
tures, which allows bi-directional data exchange and
embedding of GUI components. Technically this is
much more difficult to achieve than the usual case
where one system uses the other through wrapper
code. The application also proved the feasibility of
the data- and GUI-reuse emphasis in GATE, as well
as the extendibility of Eudico&apos;s dynamic viewers.
The inter-operation was made possible by the
openness and flexibility of the underlying data mod-
els. Eudico&apos;s abstract corpus model showed its
generality by the straightforward implementation of
GATE/TIPSTER support. In turn, the generality
of the GATE/TIPSTER model allowed efficient en-
coding and manipulation of transcribed speech data
and the associated media time information.
From end-user perspective, the Eudico/GATE in-
tegrated application has introduced language en-
gineering to the domain of spoken language re-
search. In this way, linguists collecting and annotat-
ing speech/video corpora can also encode, manipu-
late and view linguistic data. From GATE program-
ming perspective, the application highlighted the
need for supporting different media and distributed
processing. GATE version 2 which is currently un-
der development, is aiming to address these issues.
</bodyText>
<sectionHeader confidence="0.997914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736142857143">
H. Brugman, A. Russel, P. Wittenburg, and
R. Piepenbrock. 1998. Corpus-based Research
using the Internet. In Workshop on Distribut-
ing and Accessing Linguistic Resources, pages
8-15, Granada. http://www.dcs.shef.ac.uk/-
Thamish/dalr/.
H. Cunningham, K. Humphreys, R. Gaizauskas,
and Y. Wilks. 1997. Software Infrastructure
for Natural Language Processing. In Proceed-
ings of the Fifth Conference on Applied Nat-
ural Language Processing (ANLP-97), March.
http://xxx.lanl.gov/abs/cs.CL/9702005.
H. Cunningham, R.G. Gaizauskas, K. Humphreys,
and Y. Wilks. 1999. Experience with a Language
Engineering Architecture: Three Years of GATE.
In Proceedings of the AISB&apos;99 Workshop on Ref-
erence Architectures and Data Standards for NLP,
Edinburgh, U.K., April. The Society for the Study
of Artificial Intelligence and Simulation of Be-
haviour.
R. Grishman. 1996. The TIPSTER Text Phase
II Architecture Design. Document Version 2.2.
Technical report, Department of Computer
Science, New York University, September.
http://www.cs.nyu.edu/pub/n1p/tipster/152.ps .
B. MacWhinney. 1999. The CHILDES Project:
Tools for Analysing Talk (second ed.). Lawrence
Arlbaum, Hillsdale, N.J.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.346585">
<title confidence="0.991124">An Experiment in Unifying Audio-Visual and Infrastructures for Language Processing Research Development</title>
<author confidence="0.9593085">Kalina Bontchevat</author>
<author confidence="0.9593085">Hennie Brugmant</author>
<author confidence="0.9593085">Albert Peter Wittenburgt</author>
<author confidence="0.9593085">Hamish</author>
<affiliation confidence="0.8818965">t Department of Computer Science, University of Sheffield, Sheffield, Institute for Psycholinguistics, Nijmegen, The</affiliation>
<address confidence="0.487739">lt;firstname.lastnamenmpi.n1</address>
<abstract confidence="0.999118461538462">This paper describes an experimental integration of two infrastructures (Eudico and GATE) which were developed independently of each other; for different media (video/speech vs. text) and applications. The integration resulted into gaining an in-depth understanding of the functionality and operation of each of the two systems in isolation, and the benefits of their combined use. It also highlighted some issues (e.g., distributed access) which need to be addressed in future work. The experiment also showed clearly the advantages of modularity and generality adopted in both systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Brugman</author>
<author>A Russel</author>
<author>P Wittenburg</author>
<author>R Piepenbrock</author>
</authors>
<title>Corpus-based Research using the Internet.</title>
<date>1998</date>
<booktitle>In Workshop on Distributing and Accessing Linguistic Resources,</booktitle>
<pages>8--15</pages>
<location>Granada. http://www.dcs.shef.ac.uk/-Thamish/dalr/.</location>
<contexts>
<context position="1669" citStr="Brugman et al., 1998" startWordPosition="228" endWordPosition="231">gration of two infrastructures (Eudico and GATE) which were developed independently of each other; and for different media (video/speech vs. text) and applications. Such integration was needed in order to have an application where the end users (linguists and language engineers) who annotate video and speech corpora with textual transcriptions in Eudico, can also benefit from language processing tools and viewers from GATE. Eudico (European Distributed Corpora) is a distributed multimedia infrastructure supporting creation, presentation and analysis of annotations of speech and video corpora (Brugman et al., 1998). Annotations of all kinds of user-definable types can be time-aligned with the speech/video data so that dynamic and simultaneous viewing is possible. For example, when the user sets a new media time this time is reflected in all annotation viewers, and, vice versa, when the user selects an annotation this time selection is shown in all viewers, including the media player: viewing of media and transcription data is synchronised. GATE (General Architecture for Text Engineering) (Cunningham et al., 1997; Cunningham et al., 1999) is an architecture, framework, and development environment, provid</context>
</contexts>
<marker>Brugman, Russel, Wittenburg, Piepenbrock, 1998</marker>
<rawString>H. Brugman, A. Russel, P. Wittenburg, and R. Piepenbrock. 1998. Corpus-based Research using the Internet. In Workshop on Distributing and Accessing Linguistic Resources, pages 8-15, Granada. http://www.dcs.shef.ac.uk/-Thamish/dalr/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>K Humphreys</author>
<author>R Gaizauskas</author>
<author>Y Wilks</author>
</authors>
<title>Software Infrastructure for Natural Language Processing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP-97),</booktitle>
<contexts>
<context position="2176" citStr="Cunningham et al., 1997" startWordPosition="308" endWordPosition="311">ure supporting creation, presentation and analysis of annotations of speech and video corpora (Brugman et al., 1998). Annotations of all kinds of user-definable types can be time-aligned with the speech/video data so that dynamic and simultaneous viewing is possible. For example, when the user sets a new media time this time is reflected in all annotation viewers, and, vice versa, when the user selects an annotation this time selection is shown in all viewers, including the media player: viewing of media and transcription data is synchronised. GATE (General Architecture for Text Engineering) (Cunningham et al., 1997; Cunningham et al., 1999) is an architecture, framework, and development environment, providing representation and storage of language data together with infrastructural support for building and deploying language engineering applications. Its plug-and-play support for processing modules and data viewers lowers the overhead of building such applications and facilitates code-reuse. The idea behind this experiment is to combine and build on the strengths of these two architectures, thus bridging the gap between transcriptions of speech and video, and language engineering tools. From the user&apos;s </context>
</contexts>
<marker>Cunningham, Humphreys, Gaizauskas, Wilks, 1997</marker>
<rawString>H. Cunningham, K. Humphreys, R. Gaizauskas, and Y. Wilks. 1997. Software Infrastructure for Natural Language Processing. In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP-97), March. http://xxx.lanl.gov/abs/cs.CL/9702005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>R G Gaizauskas</author>
<author>K Humphreys</author>
<author>Y Wilks</author>
</authors>
<title>Experience with a Language Engineering Architecture: Three Years of GATE.</title>
<date>1999</date>
<booktitle>In Proceedings of the AISB&apos;99 Workshop on Reference Architectures and Data Standards for NLP,</booktitle>
<location>Edinburgh, U.K.,</location>
<contexts>
<context position="2202" citStr="Cunningham et al., 1999" startWordPosition="312" endWordPosition="315">presentation and analysis of annotations of speech and video corpora (Brugman et al., 1998). Annotations of all kinds of user-definable types can be time-aligned with the speech/video data so that dynamic and simultaneous viewing is possible. For example, when the user sets a new media time this time is reflected in all annotation viewers, and, vice versa, when the user selects an annotation this time selection is shown in all viewers, including the media player: viewing of media and transcription data is synchronised. GATE (General Architecture for Text Engineering) (Cunningham et al., 1997; Cunningham et al., 1999) is an architecture, framework, and development environment, providing representation and storage of language data together with infrastructural support for building and deploying language engineering applications. Its plug-and-play support for processing modules and data viewers lowers the overhead of building such applications and facilitates code-reuse. The idea behind this experiment is to combine and build on the strengths of these two architectures, thus bridging the gap between transcriptions of speech and video, and language engineering tools. From the user&apos;s perspective, this entails </context>
</contexts>
<marker>Cunningham, Gaizauskas, Humphreys, Wilks, 1999</marker>
<rawString>H. Cunningham, R.G. Gaizauskas, K. Humphreys, and Y. Wilks. 1999. Experience with a Language Engineering Architecture: Three Years of GATE. In Proceedings of the AISB&apos;99 Workshop on Reference Architectures and Data Standards for NLP, Edinburgh, U.K., April. The Society for the Study of Artificial Intelligence and Simulation of Behaviour.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
</authors>
<title>The TIPSTER Text Phase II Architecture Design. Document Version 2.2.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, New York University,</institution>
<note>http://www.cs.nyu.edu/pub/n1p/tipster/152.ps .</note>
<contexts>
<context position="5670" citStr="Grishman, 1996" startWordPosition="867" endWordPosition="868">cific phenomenon. Tiers can also have meta information (e.g. name, transcriber). Tag - a typed set of values applying to a time interval. A Tag is part of exactly one Tier. Tags on the same Tier may not overlap in time. Tags are either explicitly linked to media time or ordered in time. These concepts are part of Eudico&apos;s Abstract Corpus Model (ACM). The ACM was designed to abstract the specifics of corpus annotation formats from the tools that work on the annotation data, thus making Eudico corpus format independent. 2.2 GATE Data Model GATE&apos;s data model is based on the TIPSTER architecture (Grishman, 1996). The main classes are: Collection - a set of Documents which can be loaded, stored, and processed together. Document - consists of a document content (e.g. text) and a set of Annotation objects associated to the document content by means of spans (objects specifying the begin and end offset of the annotation). Annotation - has a set of spans (specify the text parts covered by this annotation), type (e.g. part-ofspeech (POS)), and a set of Attribute objects which hold further information about the annotation (e.g. time,12345, category,&amp;quot; Noun&amp;quot;). Attribute- a feature-value pair which can hold an</context>
</contexts>
<marker>Grishman, 1996</marker>
<rawString>R. Grishman. 1996. The TIPSTER Text Phase II Architecture Design. Document Version 2.2. Technical report, Department of Computer Science, New York University, September. http://www.cs.nyu.edu/pub/n1p/tipster/152.ps .</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analysing Talk</title>
<date>1999</date>
<editor>(second ed.). Lawrence Arlbaum,</editor>
<location>Hillsdale, N.J.</location>
<contexts>
<context position="8437" citStr="MacWhinney, 1999" startWordPosition="1327" endWordPosition="1328">nce annotations, all Tags have associated span information and all utterances have associated time information. Based on the utterance annotations, which provide a link to Eudico&apos;s time-based world, new linguistic data in the form of other annotations can be added now to the media corpus. We experimented with two types of such data: • POS - part of speech annotations which are currently obtained from hand-annotation. • syntaxTree - syntax trees obtained by manual annotation. 2.4 Example The following example of the mapping is based on an example of a time-aligned file from the CHILDES Corpus (MacWhinney, 1999). @Begin @Filename: boys73.cha @Participants ROS Ross Child, MAR Mark Child, FAT Brian Father, MOT Mary Mother *ROS: yahoo. Yosnd: &amp;quot;boys73a.aiff&amp;quot; 7349 8338 *FAT: you got a lot more to do # don&apos;t you? Yosnd: &amp;quot;boys73a.aiff&amp;quot; 8607 9999 *MAR: yeah. Yosnd: &amp;quot;boys73a.aiff&amp;quot; 10482 10839 *MAR: because I&apos;m not ready to go to &lt;the bathroom&gt;[&gt;] +/. Yosnd: &amp;quot;boys73a.aiff&amp;quot; 11621 13784 • Associate the span of this annotation with a time interval that is taken from a time selection set with the Eudico media player. This file is first parsed and Eudico Tiers and Tags are created as follows: Tiers: ROS, MAR, FAT, </context>
</contexts>
<marker>MacWhinney, 1999</marker>
<rawString>B. MacWhinney. 1999. The CHILDES Project: Tools for Analysing Talk (second ed.). Lawrence Arlbaum, Hillsdale, N.J.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>