<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003788">
<title confidence="0.991139">
Finding Bursty Topics from Microblogs
</title>
<author confidence="0.998783">
Qiming Diao, Jing Jiang, Feida Zhu, Ee-Peng Lim
</author>
<affiliation confidence="0.957028666666667">
Living Analytics Research Centre
School of Information Systems
Singapore Management University
</affiliation>
<email confidence="0.979945">
{qiming.diao.2010, jingjiang, fdzhu, eplim}@smu.edu.sg
</email>
<sectionHeader confidence="0.995227" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999389178571429">
Microblogs such as Twitter reflect the general
public’s reactions to major events. Bursty top-
ics from microblogs reveal what events have
attracted the most online attention. Although
bursty event detection from text streams has
been studied before, previous work may not
be suitable for microblogs because compared
with other text streams such as news articles
and scientific publications, microblog posts
are particularly diverse and noisy. To find top-
ics that have bursty patterns on microblogs,
we propose a topic model that simultaneous-
ly captures two observations: (1) posts pub-
lished around the same time are more like-
ly to have the same topic, and (2) posts pub-
lished by the same user are more likely to have
the same topic. The former helps find event-
driven posts while the latter helps identify and
filter out “personal” posts. Our experiments
on a large Twitter dataset show that there are
more meaningful and unique bursty topics in
the top-ranked results returned by our mod-
el than an LDA baseline and two degenerate
variations of our model. We also show some
case studies that demonstrate the importance
of considering both the temporal information
and users’ personal interests for bursty topic
detection from microblogs.
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792230769231">
With the fast growth of Web 2.0, a vast amount of
user-generated content has accumulated on the so-
cial Web. In particular, microblogging sites such
as Twitter allow users to easily publish short in-
stant posts about any topic to be shared with the
general public. The textual content coupled with
the temporal patterns of these microblog posts pro-
vides important insight into the general public’s in-
terest. A sudden increase of topically similar posts
usually indicates a burst of interest in some event
that has happened offline (such as a product launch
or a natural disaster) or online (such as the spread
of a viral video). Finding bursty topics from mi-
croblogs therefore can help us identify the most pop-
ular events that have drawn the public’s attention. In
this paper, we study the problem of finding bursty
topics from a stream of microblog posts generated
by different users. We focus on retrospective detec-
tion, where the text stream within a certain period is
analyzed in its entirety.
Retrospective bursty event detection from tex-
t streams is not new (Kleinberg, 2002; Fung et al.,
2005; Wang et al., 2007), but finding bursty topic-
s from microblog steams has not been well studied.
In his seminal work, Kleinberg (2002) proposed a s-
tate machine to model the arrival times of documents
in a stream in order to identify bursts. This model
has been widely used. However, this model assumes
that documents in the stream are all about a given
topic. In contrast, discovering interesting topics that
have drawn bursts of interest from a stream of top-
ically diverse microblog posts is itself a challenge.
To discover topics, we can certainly apply standard
topic models such as LDA (Blei et al., 2003), but
with standard LDA temporal information is lost dur-
ing topic discovery. For microblogs, where posts are
short and often event-driven, temporal information
can sometimes be critical in determining the topic of
a post. For example, typically a post containing the
</bodyText>
<page confidence="0.990244">
536
</page>
<note confidence="0.9864895">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 536–544,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.995197902439025">
word “jobs” is likely to be about employment, but
right after October 5, 2011, a post containing “jobs”
is more likely to be related to Steve Jobs’ death. Es-
sentially, we expect that on microblogs, posts pub-
lished around the same time have a higher probabil-
ity to belong to the same topic.
To capture this intuition, one solution is to assume
that posts published within the same short time win-
dow follow the same topic distribution. Wang et
al. (2007) proposed a PLSA-based topic model that
exploits this idea to find correlated bursty patterns
across multiple text streams. However, their model
is not immediately applicable for our problem. First,
their model assumes multiple text streams where
word distributions for the same topic are different
on different streams. More importantly, their model
was applied to news articles and scientific publica-
tions, where most documents follow the global top-
ical trends. On microblogs, besides talking about
global popular events, users also often talk about
their daily lives and personal interests. In order to
detect global bursty events from microblog posts, it
is important to filter out these “personal” posts.
In this paper, we propose a topic model designed
for finding bursty topics from microblogs. Our mod-
el is based on the following two assumptions: (1) If
a post is about a global event, it is likely to follow
a global topic distribution that is time-dependent.
(2) If a post is about a personal topic, it is likely
to follow a personal topic distribution that is more
or less stable over time. Separation of “global” and
“personal” posts is done in an unsupervised manner
through hidden variables. Finally, we apply a state
machine to detect bursts from the discovered topics.
We evaluate our model on a large Twitter dataset.
We find that compared with bursty topics discovered
by standard LDA and by two degenerate variations
of our model, bursty topics discovered by our model
are more accurate and less redundant within the top-
ranked results. We also use some example bursty
topics to explain the advantages of our model.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999869557692308">
To find bursty patterns from data streams, Kleinberg
(2002) proposed a state machine to model the ar-
rival times of documents in a stream. Different states
generate time gaps according to exponential density
functions with different expected values, and bursty
intervals can be discovered from the underlying state
sequence. A similar approach by Ihler et al. (2006)
models a sequence of count data using Poisson dis-
tributions. To apply these methods to find bursty
topics, the data stream used must represent a single
topic.
Fung et al. (2005) proposed a method that iden-
tifies both topics and bursts from document stream-
s. The method first finds individual words that have
bursty patterns. It then finds groups of words that
tend to share bursty periods and co-occur in the same
documents to form topics. Weng and Lee (2011)
proposed a similar method that first characterizes the
temporal patterns of individual words using wavelet-
s and then groups words into topics. A major prob-
lem with these methods is that the word clustering
step can be expensive when the number of bursty
words is large. We find that the method by Fung
et al. (2005) cannot be applied to our dataset be-
cause their word clustering algorithm does not scale
up. Weng and Lee (2011) applied word clustering
to only the top bursty words within a single day, and
subsequently their topics mostly consist of two or
three words. In contrast, our method is scalable and
each detected bursty topic is directly associated with
a word distribution and a set of tweets (see Table 3),
which makes it easier to interpret the topic.
Topic models provide a principled and elegan-
t way to discover hidden topics from large docu-
ment collections. Standard topic models do not con-
sider temporal information. A number of temporal
topic models have been proposed to consider topic
changes over time. Some of these models focus on
the change of topic composition, i.e. word distri-
butions, which is not relevant to bursty topic detec-
tion (Blei and Lafferty, 2006; Nallapati et al., 2007;
Wang et al., 2008). Some other work looks at the
temporal evolution of topics, but the focus is not on
bursty patterns (Wang and McCallum, 2006; Ahmed
and Xing, 2008; Masada et al., 2009; Ahmed and X-
ing, 2010; Hong et al., 2011).
The model proposed by Wang et al. (2007) is the
most relevant to ours. But as we have pointed out
in Section 1, they do not need to handle the sep-
aration of “personal” documents from event-driven
documents. As we will show later in our experi-
ments, for microblogs it is critical to model users’
</bodyText>
<page confidence="0.994324">
537
</page>
<bodyText confidence="0.996706583333333">
personal interests in addition to global topical trend-
s.
To capture users’ interests, Rosen-Zvi et al.
(2004) expand topic distributions from document-
level to user-level in order to capture users’ specif-
ic interests. But on microblogs, posts are short and
noisy, so Zhao et al. (2011) further assume that each
post is assigned a single topic and some words can
be background words. However, these studies do not
aim to detect bursty patterns. Our work is novel in
that it combines users’ interests and temporal infor-
mation to detect bursty topics.
</bodyText>
<sectionHeader confidence="0.997119" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.998763">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.99996472">
We first introduce the notation used in this paper and
formally formulate our problem. We assume that
we have a stream of D microblog posts, denoted as
d1, d2,. .. , dD. Each post di is generated by a user
ui, where ui is an index between 1 and U, and U is
the total number of users. Each di is also associat-
ed with a discrete timestamp ti, where ti is an index
between 1 and T, and T is the total number of time
points we consider. Each di contains a bag of word-
s, denoted as {wi,1, wi,2, ... , wi,Ni1, where wi,j is
an index between 1 and V , and V is the vocabulary
size. Ni is the number of words in di.
We define a bursty topic b as a word distri-
bution coupled with a bursty interval, denoted as
(Ob, tbs, tbe), where Ob is a multinomial distribution
over the vocabulary, and tbs and tbe (1 &lt; tbs � tbe &lt; T)
are the start and the end timestamps of the bursty in-
terval, respectively. Our task is to find meaningful
bursty topics from the input text stream.
Our method consists of a topic discovery step and
a burst detection step. At the topic discovery step,
we propose a topic model that considers both users’
topical interests and the global topic trends. Burst
detection is done through a standard state machine
method.
</bodyText>
<subsectionHeader confidence="0.998198">
3.2 Our Topic Model
</subsectionHeader>
<bodyText confidence="0.99998420754717">
We assume that there are C (latent) topics in the text
stream, where each topic c has a word distribution
Oc. Note that not every topic has a bursty interval.
On the other hand, a topic may have multiple bursty
intervals and hence leads to multiple bursty topics.
We also assume a background word distribution OB
that captures common words. All posts are assumed
to be generated from some mixture of these C + 1
underlying topics.
In standard LDA, a document contains a mixture
of topics, represented by a topic distribution, and
each word has a hidden topic label. While this is a
reasonable assumption for long documents, for short
microblog posts, a single post is most likely to be
about a single topic. We therefore associate a single
hidden variable with each post to indicate its topic.
Similar idea of assigning a single topic to a short se-
quence of words has been used before (Gruber et al.,
2007; Zhao et al., 2011). As we will see very soon,
this treatment also allows us to model topic distribu-
tions at time window level and user level.
As we have discussed in Section 1, an importan-
t observation we have is that when everything else
is equal, a pair of posts published around the same
time is more likely to be about the same topic than a
random pair of posts. To model this observation, we
assume that there is a global topic distribution Bt for
each time point t. Presumably Bt has a high prob-
ability for a topic that is popular in the microblog-
sphere at time t.
Unlike news articles from traditional media,
which are mostly about current affairs, an important
property of microblog posts is that many posts are
about users’ personal encounters and interests rather
than global events. Since our focus is to find popular
global events, we need to separate out these “person-
al” posts. To do this, an intuitive idea is to compare
a post with its publisher’s general topical interests
observed over a long time. If a post does not match
the user’s long term interests, it is more likely re-
lated to a global event. We therefore introduce a
time-independent topic distribution ηu for each us-
er to capture her long term topical interests.
We assume the following generation process for
all the posts in the stream. When user u publishes
a post at time point t, she first decides whether to
write about a global trendy topic or a personal top-
ic. If she chooses the former, she then selects a topic
according to Bt. Otherwise, she selects a topic ac-
cording to her own topic distribution ηu. With the
chosen topic, words in the post are generated from
the word distribution for that topic or from the back-
ground word distribution that captures white noise.
</bodyText>
<page confidence="0.977331">
538
</page>
<listItem confidence="0.988316882352941">
1. Draw ϕB ∼ Dirichlet(β), π ∼ Beta(γ), ρ ∼
Beta(λ)
2. For each time point t = 1, ... , T
(a) draw θt ∼ Dirichlet(α)
3. For each user u = 1, ... , U
(a) draw ηu ∼ Dirichlet(α)
4. For each topic c = 1, ... , C,
(a) draw ϕc ∼ Dirichlet(β)
5. For each post i = 1, ... , D,
(a) draw yi ∼ Bernoulli(π)
(b) draw zi ∼ Multinomial(ηui) if yi = 0 or
zi ∼ Multinomial(θti) if yi = 1
(c) for each word j = 1, ... , Ni
i. draw xi,j ∼ Bernoulli(ρ)
ii. draw wi,j ∼ Multinomial(ϕB) if
xi,j = 0 or wi,j ∼ Multinomial(ϕzi)
if xi,j = 1
</listItem>
<figureCaption confidence="0.998516">
Figure 2: The generation process for all posts.
</figureCaption>
<bodyText confidence="0.9839445">
We use π to denote the probability of choosing to
talk about a global topic rather than a personal topic.
</bodyText>
<figureCaption confidence="0.7869275">
Formally, the generation process is summarized in
Figure 2. The model is also depicted in Figure 1(a).
</figureCaption>
<bodyText confidence="0.999070266666667">
There are two degenerate variations of our model
that we also consider in our experiments. The first
one is depicted in Figure 1(b). In this model, we only
consider the time-dependent topic distributions that
capture the global topical trends. This model can be
seen as a direct application of the model by Wang
et al. (2007). The second one is depicted in Fig-
ure 1(c). In this model, we only consider the users’
personal interests but not the global topical trends,
and therefore temporal information is not used. We
refer to our complete model as TimeUserLDA, the
model in Figure 1(b) as TimeLDA and the model in
Figure 1(c) as UserLDA. We also consider a standard
LDA model in our experiments, where each word is
associated with a hidden topic.
</bodyText>
<subsectionHeader confidence="0.684378">
Learning
</subsectionHeader>
<bodyText confidence="0.999931636363636">
We use collapsed Gibbs sampling to obtain sam-
ples of the hidden variable assignment and to esti-
mate the model parameters from these samples. Due
to space limit, we only show the derived Gibbs sam-
pling formulas as follows.
First, for the i-th post, we know its publisher ui
and timestamp ti. We can jointly sample yi and zi
based on the values of all other hidden variables. Let
us use y to denote the set of all hidden variables y
and y¬i to denote all y except yi. We use similar
symbols for other variables. We then have
</bodyText>
<equation confidence="0.994175">
p(yi = p, zi = c|z¬i, y¬i, x, w) ∝
Ml (c) + α l lv 1 l lk () 1(Mc(v) + k + β)
Ml(·) + Cα 7�7E . -1 (1)
l lk(p (M() + k + V β)
</equation>
<bodyText confidence="0.964300285714286">
where l = ui when p = 0 and l = ti when p =
1. Here every M is a counter. Mπ(0) is the number
of posts generated by personal interests, while Mπ (1)
is the number of posts coming from global topical
trends. Mπ(·) = Mπ0 + Mπ1 . Mui (c)is the number of
posts by user ui and assigned to topic c, and Mui
(·) is
the total number of posts by ui. Mti (c)is the number
of posts assigned to topic c at time point ti, and Mti
(·)
is the total number of posts at ti. E(v) is the number
of times word v occurs in the i-th post and is labeled
as a topic word, while E(·) is the total number of
topic words in the i-th post. Here, topic words refer
to words whose latent variable x equals 1. Mc(v) is
the number of times word v is assigned to topic c,
and Mc(·) is the total number of words assigned to
topic c. All the counters M mentioned above are
calculated with the i-th post excluded.
We sample xi,j for each word wi,j in the i-th post
using
</bodyText>
<equation confidence="0.984834">
p(xi,j = q|y, z, x¬{i,j}, w)
l
M(wi,3) + β(2)
Ml(·) + V β ,
</equation>
<bodyText confidence="0.991073909090909">
where l = B when q = 0 and l = zi when q = 1.
Mρ(0) and Mρ(1) are counters to record the numbers
of words assigned to the background model and any
topic, respectively, and Mρ(·) = Mρ(0)+Mρ(1). MB (wi,j)
is the number of times word wi,j occurs as a back-
ground word. Mzi (wi,j) counts the number of times
word wi,j is assigned to topic zi, and Mzi
(·) is the to-
tal number of words assigned to topic zi. Again, all
counters are calculated with the current word wi,j
excluded.
</bodyText>
<equation confidence="0.999642">
Mπ (p) + γ
Mπ (·) + &amp;quot;γ
Mρ (q) + γ
∝ ρ ),
) + ,
M(· 2
</equation>
<page confidence="0.996644">
539
</page>
<figureCaption confidence="0.9997715">
Figure 1: (a) Our topic model for burst detection. (b) A variation of our model where we only consider global topical
trends. (c) A variation of our model where we only consider users’ personal topical interests.
</figureCaption>
<subsectionHeader confidence="0.998716">
3.3 Burst Detection
</subsectionHeader>
<bodyText confidence="0.999899192307692">
Just like standard LDA, our topic model itself finds a
set of topics represented by ϕc but does not directly
generate bursty topics. To identify bursty topics, we
use the following mechanism, which is based on the
idea by Kleinberg (2002) and Ihler et al. (2006). In
our experiments, when we compare different mod-
els, we also use the same burst detection mechanism
for other models.
We assume that after topic modeling, for each dis-
covered topic c, we can obtain a series of counts
(mc1, mc2, ... , mcT) representing the intensity of the
topic at different time points. For LDA, these
are the numbers of words assigned to topic c.
For TimeUserLDA, these are the numbers of posts
which are in topic c and generated by the global top-
ic distribution θt�, i.e whose hidden variable yi is 1.
For other models, these are the numbers of posts in
topic c.
We assume that these counts are generated by two
Poisson distributions corresponding to a bursty state
and a normal state, respectively. Let µ0 denote the
expected count for the normal state and µ1 for the
bursty state. Let vt denote the state for time point t,
where vt = 0 indicates the normal state and vt = 1
indicates the bursty state. The probability of observ-
ing a count of mct is as follows:
</bodyText>
<table confidence="0.998897">
Method P@5 P@10 P@20 P@30
LDA 0.600 0.800 0.750 N/A
TimeLDA 0.800 0.700 0.600 0.633
UserLDA 0.800 0.700 0.850 0.833
TimeUserLDA 1.000 1.000 0.900 0.800
</table>
<tableCaption confidence="0.999304">
Table 1: Precision at K for the various models.
</tableCaption>
<table confidence="0.9997798">
Method P@5 P@10 P@20 P@30
LDA 0.600 0.800 0.700 N/A
TimeLDA 0.400 0.500 0.500 0.567
UserLDA 0.800 0.500 0.500 0.600
TimeUserLDA 1.000 0.900 0.850 0.767
</table>
<tableCaption confidence="0.993293">
Table 2: Precision at K for the various models after we
remove redundant bursty topics.
</tableCaption>
<bodyText confidence="0.986573">
where l is either 0 or 1.
µ0 and µ1 are topic specific. In our experiments,
we set µ0 = T ∑t mct, that is, µ0 is the average
count over time. We set µ1 = 3µ0. For transition
probabilities, we empirically set σ0 = 0.9 and σ1 =
0.6 for all topics.
We can use dynamic programming to uncover the
underlying state sequence for a series of counts. Fi-
nally, a burst is marked by a consecutive subse-
quence of bursty states.
mct, ,
where l is either 0 or 1. The state sequence
(v0, v1, ... , vT) is a Markov chain with the follow-
ing transition probabilities:
</bodyText>
<equation confidence="0.963654">
p(vt = l|vt−1 = l) = σl,
</equation>
<sectionHeader confidence="0.999512" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.990158">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.9997846">
We use a Twitter data set to evaluate our models.
The original data set contains 151,055 Twitter users
based in Singapore and their tweets. These Twitter
users were obtained by starting from a set of seed
Singapore users who are active online and tracing
</bodyText>
<figure confidence="0.6120696">
e
p(mc t|vt = l) =
mt
µ
l
</figure>
<page confidence="0.944819">
540
</page>
<table confidence="0.819332235294117">
Bursty Period Top Words Example Tweets Label
Nov 29 vote, big, awards, (1) why didnt 2ne1 win this time! Mnet Asian
bang, mama, win, (2) 2ne1. you deserved that urgh! Music Awards
2ne1, award, won (3) watching mama. whoohoo (MAMA)
Oct 5 — Oct 8 steve, jobs, apple, (1) breaking: apple says steve jobs has passed away! Steve Jobs
iphone, rip, world, (2) google founders: steve jobs was an inspiration! death
changed, 4s, siri (3) apple 4 life thankyousteve
Nov 1 —Nov 3 reservior, bedok, adlyn, (1) this adelyn totally disgust me. slap her mum? girl slapping
slap, found, body, queen of cine? joke please can. mom
mom, singapore, steven (2) she slapped her mum and boasted about it on fb
(3) adelyn lives in woodlands , later she slap me how?
Nov 5 reservior, bedok, adlyn, (1) bedok = bodies either drowned or killed. suicide near
slap, found, body, (2) another body found, in bedok reservoir? bedok reservoir
mom, singapore, steven (3) so many bodies found at bedok reservoir. alamak.
Oct 23 man, arsenal, united, (1) damn you man city! we will get you next time! football game
liverpool, chelsea, city, (2) wtf 90min goal!
goal, game, match (3) 6-1 to city. unbelievable.
</table>
<tableCaption confidence="0.947824">
Table 3: Top-5 bursty topics ranked by TimeUserLDA. The labels are manually given. The 3rd and the 4th bursty
topics come from the same topic but have different bursty periods.
</tableCaption>
<table confidence="0.998807833333333">
Rank LDA UserLDA TimeLDA
1 Steve Jobs’ death MAMA MAMA
2 MAMA football game MAMA
3 N/A #zamanprimaryschool MAMA
4 girl slapping mom N/A girl slapping mom
5 N/A iphone 4s N/A
</table>
<tableCaption confidence="0.999816">
Table 4: Top-5 bursty topics ranked by other models. N/A indicates a meaningless burst.
</tableCaption>
<bodyText confidence="0.99965">
their follower/followee links by two hops. Because
this data set is huge, we randomly sampled 2892
users from this data set and extracted their tweets
between September 1 and November 30, 2011 (91
days in total). We use one day as our time window.
Therefore our timestamps range from 1 to 91. We
then removed stop words and words containing non-
standard characters. Tweets containing less than 3
words were also discarded. After preprocessing, we
obtained the final data set with 3,967,927 tweets and
24,280,638 tokens.
</bodyText>
<subsectionHeader confidence="0.998869">
4.2 Ground Truth Generation
</subsectionHeader>
<bodyText confidence="0.999995666666667">
To compare our model with other alternative models,
we perform both quantitative and qualitative evalua-
tion. As we have explained in Section 3, each mod-
el gives us time series data for a number of topics,
and by applying a Poisson-based state machine, we
can obtain a set of bursty topics. For each method,
we rank the obtained bursty topics by the number
of tweets (or words in the case of the LDA model)
assigned to the topics and take the top-30 bursty top-
ics from each model. In the case of the LDA mod-
el, only 23 bursty topics were detected. We merged
these topics and asked two human judges to judge
their quality by assigning a score of either 0 or 1.
The judges are graduate students living in Singapore
and not involved in this project. The judges were
given the bursty period and 100 randomly selected
tweets for the given topic within that period for each
bursty topic. They can consult external resources to
help make judgment. A bursty topic was scored 1
if the 100 tweets coherently describe a bursty even-
t based on the human judge’s understanding. The
inter-annotator agreement score is 0.649 using Co-
hen’s kappa, showing substantial agreement. For
ground truth, we consider a bursty topic to be cor-
rect if both human judges have scored it 1. Since
some models gave redundant bursty topics, we al-
so asked one of the judges to identify unique bursty
</bodyText>
<page confidence="0.993982">
541
</page>
<bodyText confidence="0.821194">
topics from the ground truth bursty topics.
</bodyText>
<subsectionHeader confidence="0.986062">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999948041666667">
In this section, we show the quantitative evalua-
tion of the four models we consider, namely, LDA,
TimeLDA, UserLDA and TimeUserLDA. For each
model, we set the number of topics C to 80, α to c
and Q to 0.01 after some preliminary experiments.
Each model was run for 500 iterations of Gibbs sam-
pling. We take 40 samples with a gap of 5 iterations
in the last 200 iterations to help us assign values to
all the hidden variables.
Table 1 shows the comparison between these
models in terms of the precision of the top-K result-
s. As we can see, our model outperforms all other
models for K &lt;= 20. For K = 30, the UserLDA
model performs the best followed by our model.
As we have pointed out, some of the bursty topics
are redundant, i.e. they are about the same bursty
event. We therefore also calculated precision at K
for unique topics, where for redundant topics the one
ranked the highest is scored 1 and the other ones
are scored 0. The comparison of the performance
is shown in Table 2. As we can see, in this case,
our model outperforms other models with all K. We
will further discuss redundant bursty topics in the
next section.
</bodyText>
<subsectionHeader confidence="0.999941">
4.4 Sample Results and Discussions
</subsectionHeader>
<bodyText confidence="0.995423223880597">
In this section, we show some sample results from
our experiments and discuss some case studies that
illustrate the advantages of our model.
First, we show the top-5 bursty topics discovered
by the TimeUserLDA model in Table 3. As we can
see, all these bursty topics are meaningful. Some of
these events are global major events such as Steve
Jobs’ death, while some others are related to online
events such as the scandal of a girl boasting about
slapping her mother on Facebook. For comparison,
we also show the top-5 bursty topics discovered by
other models in Table 4. As we can see, some of
them are not meaningful events while some of them
are redundant.
Next, we show two case studies to demonstrate
the effectiveness of our model.
Effectiveness of Temporal Models: Both
TimeLDA and TimeUserLDA tend to group posts
published on the same day into the same topic. We
find that this can help separate bursty topics from
general ones. An example is the topic on the Circle
Line. The Circle Line is one of the subway lines of
Singapore’s mass transit system. There were a few
incidents of delays or breakdowns during the period
between September and November, 2011. We show
the time series data of the topic related to the Circle
Line of UserLDA, TimeLDA and TimeUserLDA in
Figure 3. As we can see, the UserLDA model de-
tects a much large volume of tweets related to this
topic. A close inspection tells us that the topic under
UserLDA is actually related to the subway systems
in Singapore in general, which include a few other
subway lines, and the Circle Line topic is merged
with this general topic. On the other hand, TimeL-
DA and TimeUserLDA are both able to separate the
Circle Line topic from the general subway topic be-
cause the Circle Line has several bursts. What is
shown in Figure 3 for TimeLDA and TimeUserLDA
is only the topic on the Circle Line, therefore the
volume is much smaller. We can see that TimeLDA
and TimeUserLDA show clearer bursty patterns than
UserLDA for this topic. The bursts around day 20,
day 44 and day 85 are all real events based on our
ground truth.
Effectiveness of User Models: We have stat-
ed that it is important to filter out users’ “person-
al” posts in order to find meaningful global events.
We find that our results also support this hypothesis.
Let us look at the example of the topic on the Mnet
Asian Music Awards, which is a major music award
show that is held by Mnet Media annually. In 2011,
this event took place in Singapore on November 29.
Because Korean pop music is very popular in Singa-
pore, many Twitter users often tweet about Korean
pop music bands and singers in general. All our top-
ic models give multiple topics related to Korean pop
music, and many of them have a burst on Novem-
ber 29, 2011. Under the TimeLDA and UserLDA
models, this leads to several redundant bursty top-
ics for the MAMA event ranked within the top-30.
For TimeUserLDA, however, although the MAMA
event is also ranked the top, there is no redundan-
t one within the top-30 results. We find that this is
because with TimeUserLDA, we can remove tweet-
s that are considered personal and therefore do not
contribute to bursty topic ranking. We show the top-
ic intensity of a topic about a Korean pop singer in
</bodyText>
<page confidence="0.988589">
542
</page>
<figure confidence="0.4346766">
UserLDA TimeLDA TimeUserLDA
m 1200 m 1200 m 1200
1000 1000 1000
800 800 800
600 600 600
400 400 400
200 200 200
0 0 0
10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 90
t t t
</figure>
<figureCaption confidence="0.978857">
Figure 3: Topic intensity over time for the topic on the Circle Line.
</figureCaption>
<figure confidence="0.992554272727273">
UserLDA TimeLDA TimeUserLDA
m 7000 m 7000 m 7000
6000 6000 6000
5000 5000 5000
4000 4000 4000
3000 3000 3000
2000 2000 2000
1000 1000 1000
0 0 0
10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 90
t t t
</figure>
<figureCaption confidence="0.998056">
Figure 4: Topic intensity over time for the topic about a Korean pop singer. The dotted curves show the topic on Steve
Jobs’ death.
</figureCaption>
<bodyText confidence="0.9755215">
Figure 4. For reference, we also show the intensity
of the topic on Steve Jobs’ death under each mod-
el. We can see that because this topic is related to
Korean pop music, it has a burst on day 90 (Novem-
ber 29). But if we consider the relative intensity of
this burst compared with Steve Jobs’ death, under
TimeLDA and UserLDA, this topic is still strong but
under TimeUserLDA its intensity can almost be ig-
nored. This is why with TimeLDA and UserLDA
this topic leads to a redundant burst within the top-
30 results but with TimeUserLDA the burst is not
ranked high.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999976807692308">
In this paper, we studied the problem of finding
bursty topics from the text streams on microblogs.
Because existing work on burst detection from tex-
t streams may not be suitable for microblogs, we
proposed a new topic model that considers both the
temporal information of microblog posts and user-
s’ personal interests. We then applied a Poisson-
based state machine to identify bursty periods from
the topics discovered by our model. We compared
our model with standard LDA as well as two de-
generate variations of our model on a real Twitter
dataset. Our quantitative evaluation showed that our
model could more accurately detect unique bursty
topics among the top ranked results. We also used
two case studies to illustrate the effectiveness of the
temporal factor and the user factor of our model.
Our method currently can only detect bursty top-
ics in a retrospective and offline manner. A more in-
teresting and useful task is to detect realtime bursts
in an online fashion. This is one of the directions we
plan to study in the future. Another limitation of the
current method is that the number of topics is pre-
determined. We also plan to look into methods that
allow appearance and disappearance of topics along
the timeline, such as the model by Ahmed and Xing
(2010).
</bodyText>
<sectionHeader confidence="0.998362" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982534">
This research is supported by the Singapore Nation-
al Research Foundation under its International Re-
search Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office. We
thank the reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.994591" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.532055">
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
</reference>
<page confidence="0.992145">
543
</page>
<reference confidence="0.999618923076923">
restaurant process: with applications to evolutionary
clustering. In Proceedings of the SIAM International
Conference on Data Mining, pages 219–230.
Amr Ahmed and Eric P. Xing. 2010. Timeline: A dy-
namic hierarchical Dirichlet process model for recov-
ering birth/death and evolution of topics in text stream.
In Proceedings of the 26th Conference on Uncertainty
in Artificial Intelligence, pages 20–29.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd International
Conference on Machine Learning.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In Proceedings of the 31st
International Conference on Very Large Data Bases,
pages 181–192.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic Markov model. In Proceedings of the
International Conference on Artificial Intelligence and
Statistics.
Liangjie Hong, Byron Dom, Siva Gurumurthy, and
Kostas Tsioutsiouliklis. 2011. A time-dependent top-
ic model for multiple text streams. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 832–
840.
Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 207–216.
Jon Kleinberg. 2002. Bursty and hierarchical structure in
streams. In Proceedings of the 8th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 91–101.
Tomonari Masada, Daiji Fukagawa, Atsuhiro Takasu,
Tsuyoshi Hamada, Yuichiro Shibata, and Kiyoshi
Oguri. 2009. Dynamic hyperparameter optimization
for bayesian topical trend analysis. In Proceedings of
the 18th ACM Conference on Information and knowl-
edge management, pages 1831–1834.
Ramesh M. Nallapati, Susan Ditmore, John D. Lafferty,
and Kin Ung. 2007. Multiscale topic tomography. In
Proceedings of the 13th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 520–529.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence, pages
487–494.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 424–433.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pattern-
s from coordinated text streams. In Proceedings of
the 13th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 784–
793.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, pages 579–586.
Jianshu Weng and Francis Lee. 2011. Event detection in
Twitter. In Proceedings of the 5th International AAAI
Conference on Weblogs and Social Media.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In Proceedings of the 33rd European confer-
ence on Advances in information retrieval, pages 338–
349.
</reference>
<page confidence="0.998253">
544
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520899">
<title confidence="0.999926">Finding Bursty Topics from Microblogs</title>
<author confidence="0.781219">Qiming Diao</author>
<author confidence="0.781219">Jing Jiang</author>
<author confidence="0.781219">Feida Zhu</author>
<author confidence="0.781219">Ee-Peng</author>
<affiliation confidence="0.852483666666667">Living Analytics Research School of Information Singapore Management</affiliation>
<email confidence="0.926771">jingjiang,fdzhu,</email>
<abstract confidence="0.998763275862069">Microblogs such as Twitter reflect the general public’s reactions to major events. Bursty topics from microblogs reveal what events have attracted the most online attention. Although bursty event detection from text streams has been studied before, previous work may not be suitable for microblogs because compared with other text streams such as news articles and scientific publications, microblog posts are particularly diverse and noisy. To find topics that have bursty patterns on microblogs, we propose a topic model that simultaneously captures two observations: (1) posts published around the same time are more likely to have the same topic, and (2) posts published by the same user are more likely to have the same topic. The former helps find eventdriven posts while the latter helps identify and filter out “personal” posts. Our experiments on a large Twitter dataset show that there are more meaningful and unique bursty topics in the top-ranked results returned by our model than an LDA baseline and two degenerate variations of our model. We also show some case studies that demonstrate the importance of considering both the temporal information and users’ personal interests for bursty topic detection from microblogs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Dynamic nonparametric mixture models and the recurrent Chinese restaurant process: with applications to evolutionary clustering.</title>
<date>2008</date>
<booktitle>In Proceedings of the SIAM International Conference on Data Mining,</booktitle>
<pages>219--230</pages>
<contexts>
<context position="7970" citStr="Ahmed and Xing, 2008" startWordPosition="1311" endWordPosition="1314">he topic. Topic models provide a principled and elegant way to discover hidden topics from large document collections. Standard topic models do not consider temporal information. A number of temporal topic models have been proposed to consider topic changes over time. Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture users’ interests, Rosen-Zvi et al. (2004) expand topic distributions from documentlevel to user-level in order to capture users’ specific interests. But on microblogs, </context>
</contexts>
<marker>Ahmed, Xing, 2008</marker>
<rawString>Amr Ahmed and Eric P. Xing. 2008. Dynamic nonparametric mixture models and the recurrent Chinese restaurant process: with applications to evolutionary clustering. In Proceedings of the SIAM International Conference on Data Mining, pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream.</title>
<date>2010</date>
<booktitle>In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>20--29</pages>
<contexts>
<context position="8013" citStr="Ahmed and Xing, 2010" startWordPosition="1319" endWordPosition="1323"> and elegant way to discover hidden topics from large document collections. Standard topic models do not consider temporal information. A number of temporal topic models have been proposed to consider topic changes over time. Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture users’ interests, Rosen-Zvi et al. (2004) expand topic distributions from documentlevel to user-level in order to capture users’ specific interests. But on microblogs, posts are short and noisy, so Zhao et al. (</context>
</contexts>
<marker>Ahmed, Xing, 2010</marker>
<rawString>Amr Ahmed and Eric P. Xing. 2010. Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream. In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, pages 20–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning.</booktitle>
<contexts>
<context position="7779" citStr="Blei and Lafferty, 2006" startWordPosition="1277" endWordPosition="1280"> words. In contrast, our method is scalable and each detected bursty topic is directly associated with a word distribution and a set of tweets (see Table 3), which makes it easier to interpret the topic. Topic models provide a principled and elegant way to discover hidden topics from large document collections. Standard topic models do not consider temporal information. A number of temporal topic models have been proposed to consider topic changes over time. Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topi</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David M. Blei and John D. Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="3202" citStr="Blei et al., 2003" startWordPosition="518" endWordPosition="521">al., 2005; Wang et al., 2007), but finding bursty topics from microblog steams has not been well studied. In his seminal work, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream in order to identify bursts. This model has been widely used. However, this model assumes that documents in the stream are all about a given topic. In contrast, discovering interesting topics that have drawn bursts of interest from a stream of topically diverse microblog posts is itself a challenge. To discover topics, we can certainly apply standard topic models such as LDA (Blei et al., 2003), but with standard LDA temporal information is lost during topic discovery. For microblogs, where posts are short and often event-driven, temporal information can sometimes be critical in determining the topic of a post. For example, typically a post containing the 536 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 536–544, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics word “jobs” is likely to be about employment, but right after October 5, 2011, a post containing “jobs” is more likely to be related to</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Pui Cheong Fung</author>
<author>Jeffrey Xu Yu</author>
<author>Philip S Yu</author>
<author>Hongjun Lu</author>
</authors>
<title>Parameter free bursty events detection in text streams.</title>
<date>2005</date>
<booktitle>In Proceedings of the 31st International Conference on Very Large Data Bases,</booktitle>
<pages>181--192</pages>
<contexts>
<context position="2593" citStr="Fung et al., 2005" startWordPosition="413" endWordPosition="416">interest in some event that has happened offline (such as a product launch or a natural disaster) or online (such as the spread of a viral video). Finding bursty topics from microblogs therefore can help us identify the most popular events that have drawn the public’s attention. In this paper, we study the problem of finding bursty topics from a stream of microblog posts generated by different users. We focus on retrospective detection, where the text stream within a certain period is analyzed in its entirety. Retrospective bursty event detection from text streams is not new (Kleinberg, 2002; Fung et al., 2005; Wang et al., 2007), but finding bursty topics from microblog steams has not been well studied. In his seminal work, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream in order to identify bursts. This model has been widely used. However, this model assumes that documents in the stream are all about a given topic. In contrast, discovering interesting topics that have drawn bursts of interest from a stream of topically diverse microblog posts is itself a challenge. To discover topics, we can certainly apply standard topic models such as LDA (Blei et a</context>
<context position="6311" citStr="Fung et al. (2005)" startWordPosition="1024" endWordPosition="1027">ample bursty topics to explain the advantages of our model. 2 Related Work To find bursty patterns from data streams, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream. Different states generate time gaps according to exponential density functions with different expected values, and bursty intervals can be discovered from the underlying state sequence. A similar approach by Ihler et al. (2006) models a sequence of count data using Poisson distributions. To apply these methods to find bursty topics, the data stream used must represent a single topic. Fung et al. (2005) proposed a method that identifies both topics and bursts from document streams. The method first finds individual words that have bursty patterns. It then finds groups of words that tend to share bursty periods and co-occur in the same documents to form topics. Weng and Lee (2011) proposed a similar method that first characterizes the temporal patterns of individual words using wavelets and then groups words into topics. A major problem with these methods is that the word clustering step can be expensive when the number of bursty words is large. We find that the method by Fung et al. (2005) c</context>
</contexts>
<marker>Fung, Yu, Yu, Lu, 2005</marker>
<rawString>Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu, and Hongjun Lu. 2005. Parameter free bursty events detection in text streams. In Proceedings of the 31st International Conference on Very Large Data Bases, pages 181–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden topic Markov model.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="11062" citStr="Gruber et al., 2007" startWordPosition="1879" endWordPosition="1882">round word distribution OB that captures common words. All posts are assumed to be generated from some mixture of these C + 1 underlying topics. In standard LDA, a document contains a mixture of topics, represented by a topic distribution, and each word has a hidden topic label. While this is a reasonable assumption for long documents, for short microblog posts, a single post is most likely to be about a single topic. We therefore associate a single hidden variable with each post to indicate its topic. Similar idea of assigning a single topic to a short sequence of words has been used before (Gruber et al., 2007; Zhao et al., 2011). As we will see very soon, this treatment also allows us to model topic distributions at time window level and user level. As we have discussed in Section 1, an important observation we have is that when everything else is equal, a pair of posts published around the same time is more likely to be about the same topic than a random pair of posts. To model this observation, we assume that there is a global topic distribution Bt for each time point t. Presumably Bt has a high probability for a topic that is popular in the microblogsphere at time t. Unlike news articles from t</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007. Hidden topic Markov model. In Proceedings of the International Conference on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangjie Hong</author>
<author>Byron Dom</author>
<author>Siva Gurumurthy</author>
<author>Kostas Tsioutsiouliklis</author>
</authors>
<title>A time-dependent topic model for multiple text streams.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>832--840</pages>
<contexts>
<context position="8033" citStr="Hong et al., 2011" startWordPosition="1324" endWordPosition="1327">scover hidden topics from large document collections. Standard topic models do not consider temporal information. A number of temporal topic models have been proposed to consider topic changes over time. Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture users’ interests, Rosen-Zvi et al. (2004) expand topic distributions from documentlevel to user-level in order to capture users’ specific interests. But on microblogs, posts are short and noisy, so Zhao et al. (2011) further assume</context>
</contexts>
<marker>Hong, Dom, Gurumurthy, Tsioutsiouliklis, 2011</marker>
<rawString>Liangjie Hong, Byron Dom, Siva Gurumurthy, and Kostas Tsioutsiouliklis. 2011. A time-dependent topic model for multiple text streams. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 832– 840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Ihler</author>
<author>Jon Hutchins</author>
<author>Padhraic Smyth</author>
</authors>
<title>Adaptive event detection with time-varying poisson processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="6133" citStr="Ihler et al. (2006)" startWordPosition="993" endWordPosition="996">ard LDA and by two degenerate variations of our model, bursty topics discovered by our model are more accurate and less redundant within the topranked results. We also use some example bursty topics to explain the advantages of our model. 2 Related Work To find bursty patterns from data streams, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream. Different states generate time gaps according to exponential density functions with different expected values, and bursty intervals can be discovered from the underlying state sequence. A similar approach by Ihler et al. (2006) models a sequence of count data using Poisson distributions. To apply these methods to find bursty topics, the data stream used must represent a single topic. Fung et al. (2005) proposed a method that identifies both topics and bursts from document streams. The method first finds individual words that have bursty patterns. It then finds groups of words that tend to share bursty periods and co-occur in the same documents to form topics. Weng and Lee (2011) proposed a similar method that first characterizes the temporal patterns of individual words using wavelets and then groups words into topi</context>
<context position="17025" citStr="Ihler et al. (2006)" startWordPosition="3043" endWordPosition="3046">, all counters are calculated with the current word wi,j excluded. Mπ (p) + γ Mπ (·) + &amp;quot;γ Mρ (q) + γ ∝ ρ ), ) + , M(· 2 539 Figure 1: (a) Our topic model for burst detection. (b) A variation of our model where we only consider global topical trends. (c) A variation of our model where we only consider users’ personal topical interests. 3.3 Burst Detection Just like standard LDA, our topic model itself finds a set of topics represented by ϕc but does not directly generate bursty topics. To identify bursty topics, we use the following mechanism, which is based on the idea by Kleinberg (2002) and Ihler et al. (2006). In our experiments, when we compare different models, we also use the same burst detection mechanism for other models. We assume that after topic modeling, for each discovered topic c, we can obtain a series of counts (mc1, mc2, ... , mcT) representing the intensity of the topic at different time points. For LDA, these are the numbers of words assigned to topic c. For TimeUserLDA, these are the numbers of posts which are in topic c and generated by the global topic distribution θt�, i.e whose hidden variable yi is 1. For other models, these are the numbers of posts in topic c. We assume that</context>
</contexts>
<marker>Ihler, Hutchins, Smyth, 2006</marker>
<rawString>Alexander Ihler, Jon Hutchins, and Padhraic Smyth. 2006. Adaptive event detection with time-varying poisson processes. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 207–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
</authors>
<title>Bursty and hierarchical structure in streams.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>91--101</pages>
<contexts>
<context position="2574" citStr="Kleinberg, 2002" startWordPosition="411" endWordPosition="412">cates a burst of interest in some event that has happened offline (such as a product launch or a natural disaster) or online (such as the spread of a viral video). Finding bursty topics from microblogs therefore can help us identify the most popular events that have drawn the public’s attention. In this paper, we study the problem of finding bursty topics from a stream of microblog posts generated by different users. We focus on retrospective detection, where the text stream within a certain period is analyzed in its entirety. Retrospective bursty event detection from text streams is not new (Kleinberg, 2002; Fung et al., 2005; Wang et al., 2007), but finding bursty topics from microblog steams has not been well studied. In his seminal work, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream in order to identify bursts. This model has been widely used. However, this model assumes that documents in the stream are all about a given topic. In contrast, discovering interesting topics that have drawn bursts of interest from a stream of topically diverse microblog posts is itself a challenge. To discover topics, we can certainly apply standard topic models suc</context>
<context position="5827" citStr="Kleinberg (2002)" startWordPosition="947" endWordPosition="948">ver time. Separation of “global” and “personal” posts is done in an unsupervised manner through hidden variables. Finally, we apply a state machine to detect bursts from the discovered topics. We evaluate our model on a large Twitter dataset. We find that compared with bursty topics discovered by standard LDA and by two degenerate variations of our model, bursty topics discovered by our model are more accurate and less redundant within the topranked results. We also use some example bursty topics to explain the advantages of our model. 2 Related Work To find bursty patterns from data streams, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream. Different states generate time gaps according to exponential density functions with different expected values, and bursty intervals can be discovered from the underlying state sequence. A similar approach by Ihler et al. (2006) models a sequence of count data using Poisson distributions. To apply these methods to find bursty topics, the data stream used must represent a single topic. Fung et al. (2005) proposed a method that identifies both topics and bursts from document streams. The method first finds individual w</context>
<context position="17001" citStr="Kleinberg (2002)" startWordPosition="3040" endWordPosition="3041">ed to topic zi. Again, all counters are calculated with the current word wi,j excluded. Mπ (p) + γ Mπ (·) + &amp;quot;γ Mρ (q) + γ ∝ ρ ), ) + , M(· 2 539 Figure 1: (a) Our topic model for burst detection. (b) A variation of our model where we only consider global topical trends. (c) A variation of our model where we only consider users’ personal topical interests. 3.3 Burst Detection Just like standard LDA, our topic model itself finds a set of topics represented by ϕc but does not directly generate bursty topics. To identify bursty topics, we use the following mechanism, which is based on the idea by Kleinberg (2002) and Ihler et al. (2006). In our experiments, when we compare different models, we also use the same burst detection mechanism for other models. We assume that after topic modeling, for each discovered topic c, we can obtain a series of counts (mc1, mc2, ... , mcT) representing the intensity of the topic at different time points. For LDA, these are the numbers of words assigned to topic c. For TimeUserLDA, these are the numbers of posts which are in topic c and generated by the global topic distribution θt�, i.e whose hidden variable yi is 1. For other models, these are the numbers of posts in</context>
</contexts>
<marker>Kleinberg, 2002</marker>
<rawString>Jon Kleinberg. 2002. Bursty and hierarchical structure in streams. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 91–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomonari Masada</author>
<author>Daiji Fukagawa</author>
</authors>
<title>Atsuhiro Takasu, Tsuyoshi Hamada, Yuichiro Shibata, and Kiyoshi Oguri.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and knowledge management,</booktitle>
<pages>1831--1834</pages>
<marker>Masada, Fukagawa, 2009</marker>
<rawString>Tomonari Masada, Daiji Fukagawa, Atsuhiro Takasu, Tsuyoshi Hamada, Yuichiro Shibata, and Kiyoshi Oguri. 2009. Dynamic hyperparameter optimization for bayesian topical trend analysis. In Proceedings of the 18th ACM Conference on Information and knowledge management, pages 1831–1834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh M Nallapati</author>
<author>Susan Ditmore</author>
<author>John D Lafferty</author>
<author>Kin Ung</author>
</authors>
<title>Multiscale topic tomography.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>520--529</pages>
<contexts>
<context position="7803" citStr="Nallapati et al., 2007" startWordPosition="1281" endWordPosition="1284">method is scalable and each detected bursty topic is directly associated with a word distribution and a set of tweets (see Table 3), which makes it easier to interpret the topic. Topic models provide a principled and elegant way to discover hidden topics from large document collections. Standard topic models do not consider temporal information. A number of temporal topic models have been proposed to consider topic changes over time. Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture u</context>
</contexts>
<marker>Nallapati, Ditmore, Lafferty, Ung, 2007</marker>
<rawString>Ramesh M. Nallapati, Susan Ditmore, John D. Lafferty, and Kin Ung. 2007. Multiscale topic tomography. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 520–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th conference on Uncertainty in artificial intelligence,</booktitle>
<pages>487--494</pages>
<contexts>
<context position="8443" citStr="Rosen-Zvi et al. (2004)" startWordPosition="1397" endWordPosition="1400">08). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture users’ interests, Rosen-Zvi et al. (2004) expand topic distributions from documentlevel to user-level in order to capture users’ specific interests. But on microblogs, posts are short and noisy, so Zhao et al. (2011) further assume that each post is assigned a single topic and some words can be background words. However, these studies do not aim to detect bursty patterns. Our work is novel in that it combines users’ interests and temporal information to detect bursty topics. 3 Method 3.1 Preliminaries We first introduce the notation used in this paper and formally formulate our problem. We assume that we have a stream of D microblog </context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2004. The author-topic model for authors and documents. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 487–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
</authors>
<title>Topics over time: a non-Markov continuous-time model of topical trends.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>424--433</pages>
<contexts>
<context position="7948" citStr="Wang and McCallum, 2006" startWordPosition="1307" endWordPosition="1310"> it easier to interpret the topic. Topic models provide a principled and elegant way to discover hidden topics from large document collections. Standard topic models do not consider temporal information. A number of temporal topic models have been proposed to consider topic changes over time. Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture users’ interests, Rosen-Zvi et al. (2004) expand topic distributions from documentlevel to user-level in order to capture users’ specific interest</context>
</contexts>
<marker>Wang, McCallum, 2006</marker>
<rawString>Xuerui Wang and Andrew McCallum. 2006. Topics over time: a non-Markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 424–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuanhui Wang</author>
<author>ChengXiang Zhai</author>
<author>Xiao Hu</author>
<author>Richard Sproat</author>
</authors>
<title>Mining correlated bursty topic patterns from coordinated text streams.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>784--793</pages>
<contexts>
<context position="2613" citStr="Wang et al., 2007" startWordPosition="417" endWordPosition="420">ent that has happened offline (such as a product launch or a natural disaster) or online (such as the spread of a viral video). Finding bursty topics from microblogs therefore can help us identify the most popular events that have drawn the public’s attention. In this paper, we study the problem of finding bursty topics from a stream of microblog posts generated by different users. We focus on retrospective detection, where the text stream within a certain period is analyzed in its entirety. Retrospective bursty event detection from text streams is not new (Kleinberg, 2002; Fung et al., 2005; Wang et al., 2007), but finding bursty topics from microblog steams has not been well studied. In his seminal work, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream in order to identify bursts. This model has been widely used. However, this model assumes that documents in the stream are all about a given topic. In contrast, discovering interesting topics that have drawn bursts of interest from a stream of topically diverse microblog posts is itself a challenge. To discover topics, we can certainly apply standard topic models such as LDA (Blei et al., 2003), but with </context>
<context position="4119" citStr="Wang et al. (2007)" startWordPosition="667" endWordPosition="670">al Meeting of the Association for Computational Linguistics, pages 536–544, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics word “jobs” is likely to be about employment, but right after October 5, 2011, a post containing “jobs” is more likely to be related to Steve Jobs’ death. Essentially, we expect that on microblogs, posts published around the same time have a higher probability to belong to the same topic. To capture this intuition, one solution is to assume that posts published within the same short time window follow the same topic distribution. Wang et al. (2007) proposed a PLSA-based topic model that exploits this idea to find correlated bursty patterns across multiple text streams. However, their model is not immediately applicable for our problem. First, their model assumes multiple text streams where word distributions for the same topic are different on different streams. More importantly, their model was applied to news articles and scientific publications, where most documents follow the global topical trends. On microblogs, besides talking about global popular events, users also often talk about their daily lives and personal interests. In ord</context>
<context position="8075" citStr="Wang et al. (2007)" startWordPosition="1332" endWordPosition="1335">ollections. Standard topic models do not consider temporal information. A number of temporal topic models have been proposed to consider topic changes over time. Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture users’ interests, Rosen-Zvi et al. (2004) expand topic distributions from documentlevel to user-level in order to capture users’ specific interests. But on microblogs, posts are short and noisy, so Zhao et al. (2011) further assume that each post is assigned a single topic</context>
<context position="13924" citStr="Wang et al. (2007)" startWordPosition="2418" endWordPosition="2421">wi,j ∼ Multinomial(ϕzi) if xi,j = 1 Figure 2: The generation process for all posts. We use π to denote the probability of choosing to talk about a global topic rather than a personal topic. Formally, the generation process is summarized in Figure 2. The model is also depicted in Figure 1(a). There are two degenerate variations of our model that we also consider in our experiments. The first one is depicted in Figure 1(b). In this model, we only consider the time-dependent topic distributions that capture the global topical trends. This model can be seen as a direct application of the model by Wang et al. (2007). The second one is depicted in Figure 1(c). In this model, we only consider the users’ personal interests but not the global topical trends, and therefore temporal information is not used. We refer to our complete model as TimeUserLDA, the model in Figure 1(b) as TimeLDA and the model in Figure 1(c) as UserLDA. We also consider a standard LDA model in our experiments, where each word is associated with a hidden topic. Learning We use collapsed Gibbs sampling to obtain samples of the hidden variable assignment and to estimate the model parameters from these samples. Due to space limit, we only</context>
</contexts>
<marker>Wang, Zhai, Hu, Sproat, 2007</marker>
<rawString>Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard Sproat. 2007. Mining correlated bursty topic patterns from coordinated text streams. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 784– 793.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>David M Blei</author>
<author>David Heckerman</author>
</authors>
<title>Continuous time dynamic topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>579--586</pages>
<contexts>
<context position="7823" citStr="Wang et al., 2008" startWordPosition="1285" endWordPosition="1288">ach detected bursty topic is directly associated with a word distribution and a set of tweets (see Table 3), which makes it easier to interpret the topic. Topic models provide a principled and elegant way to discover hidden topics from large document collections. Standard topic models do not consider temporal information. A number of temporal topic models have been proposed to consider topic changes over time. Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008). Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture users’ interests, Ros</context>
</contexts>
<marker>Wang, Blei, Heckerman, 2008</marker>
<rawString>Chong Wang, David M. Blei, and David Heckerman. 2008. Continuous time dynamic topic models. In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence, pages 579–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianshu Weng</author>
<author>Francis Lee</author>
</authors>
<title>Event detection in Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="6593" citStr="Weng and Lee (2011)" startWordPosition="1073" endWordPosition="1076"> functions with different expected values, and bursty intervals can be discovered from the underlying state sequence. A similar approach by Ihler et al. (2006) models a sequence of count data using Poisson distributions. To apply these methods to find bursty topics, the data stream used must represent a single topic. Fung et al. (2005) proposed a method that identifies both topics and bursts from document streams. The method first finds individual words that have bursty patterns. It then finds groups of words that tend to share bursty periods and co-occur in the same documents to form topics. Weng and Lee (2011) proposed a similar method that first characterizes the temporal patterns of individual words using wavelets and then groups words into topics. A major problem with these methods is that the word clustering step can be expensive when the number of bursty words is large. We find that the method by Fung et al. (2005) cannot be applied to our dataset because their word clustering algorithm does not scale up. Weng and Lee (2011) applied word clustering to only the top bursty words within a single day, and subsequently their topics mostly consist of two or three words. In contrast, our method is sc</context>
</contexts>
<marker>Weng, Lee, 2011</marker>
<rawString>Jianshu Weng and Francis Lee. 2011. Event detection in Twitter. In Proceedings of the 5th International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Jianshu Weng</author>
<author>Jing He</author>
<author>Ee-Peng Lim</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Comparing twitter and traditional media using topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd European conference on Advances in information retrieval,</booktitle>
<pages>338--349</pages>
<contexts>
<context position="8618" citStr="Zhao et al. (2011)" startWordPosition="1426" endWordPosition="1429">nd Xing, 2010; Hong et al., 2011). The model proposed by Wang et al. (2007) is the most relevant to ours. But as we have pointed out in Section 1, they do not need to handle the separation of “personal” documents from event-driven documents. As we will show later in our experiments, for microblogs it is critical to model users’ 537 personal interests in addition to global topical trends. To capture users’ interests, Rosen-Zvi et al. (2004) expand topic distributions from documentlevel to user-level in order to capture users’ specific interests. But on microblogs, posts are short and noisy, so Zhao et al. (2011) further assume that each post is assigned a single topic and some words can be background words. However, these studies do not aim to detect bursty patterns. Our work is novel in that it combines users’ interests and temporal information to detect bursty topics. 3 Method 3.1 Preliminaries We first introduce the notation used in this paper and formally formulate our problem. We assume that we have a stream of D microblog posts, denoted as d1, d2,. .. , dD. Each post di is generated by a user ui, where ui is an index between 1 and U, and U is the total number of users. Each di is also associate</context>
<context position="11082" citStr="Zhao et al., 2011" startWordPosition="1883" endWordPosition="1886">on OB that captures common words. All posts are assumed to be generated from some mixture of these C + 1 underlying topics. In standard LDA, a document contains a mixture of topics, represented by a topic distribution, and each word has a hidden topic label. While this is a reasonable assumption for long documents, for short microblog posts, a single post is most likely to be about a single topic. We therefore associate a single hidden variable with each post to indicate its topic. Similar idea of assigning a single topic to a short sequence of words has been used before (Gruber et al., 2007; Zhao et al., 2011). As we will see very soon, this treatment also allows us to model topic distributions at time window level and user level. As we have discussed in Section 1, an important observation we have is that when everything else is equal, a pair of posts published around the same time is more likely to be about the same topic than a random pair of posts. To model this observation, we assume that there is a global topic distribution Bt for each time point t. Presumably Bt has a high probability for a topic that is popular in the microblogsphere at time t. Unlike news articles from traditional media, wh</context>
</contexts>
<marker>Zhao, Jiang, Weng, He, Lim, Yan, Li, 2011</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011. Comparing twitter and traditional media using topic models. In Proceedings of the 33rd European conference on Advances in information retrieval, pages 338– 349.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>