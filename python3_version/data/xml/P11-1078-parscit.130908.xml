<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.980565">
Extracting Social Power Relationships from Natural Language
</title>
<author confidence="0.881359">
Philip Bramsen
</author>
<affiliation confidence="0.521326">
Louisville, KY
</affiliation>
<email confidence="0.927174">
bramsen@alum.mit.edu*
</email>
<author confidence="0.99172">
Ami Patel
</author>
<affiliation confidence="0.999124">
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.902165">
Cambridge, MA
</address>
<email confidence="0.987286">
ampatel@mit.edu*
</email>
<sectionHeader confidence="0.30979" genericHeader="abstract">
Martha Escobar-Molano
</sectionHeader>
<address confidence="0.790732">
San Diego, CA
</address>
<email confidence="0.970523">
mescobar@asgard.com*
</email>
<author confidence="0.946972">
Rafael Alonso
</author>
<affiliation confidence="0.877696">
SET Corporation
</affiliation>
<address confidence="0.894347">
Arlington, VA
</address>
<email confidence="0.998267">
ralonso@setcorp.com
</email>
<sectionHeader confidence="0.998597" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999716545454546">
Sociolinguists have long argued that social
context influences language use in all manner
of ways, resulting in lects&apos;. This paper ex-
plores a text classification problem we will
call lect modeling, an example of what has
been termed computational sociolinguistics. In
particular, we use machine learning techniques
to identify social power relationships between
members of a social network, based purely on
the content of their interpersonal communica-
tion. We rely on statistical methods, as op-
posed to language-specific engineering, to
extract features which represent vocabulary
and grammar usage indicative of social power
lect. We then apply support vector machines to
model the social power lects representing su-
perior-subordinate communication in the En-
ron email corpus. *ur results validate the
treatment of lect modeling as a text classifica-
tion problem — albeit a hard one — and consti-
tute a case for future research in computational
sociolinguistics.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998558">
Linguists in sociolinguistics, pragmatics and re-
lated fields have analyzed the influence of social
context on language and have catalogued countless
phenomena that are influenced by it, confirming
many with qualitative and quantitative studies. In-
</bodyText>
<footnote confidence="0.9192955">
* This work was done while these authors were at SET Corpo-
ration, an SAIC Company.
1 Fields that deal with society and language have inconsistent
terminology; &amp;quot;lect&amp;quot; is chosen here because &amp;quot;lect&amp;quot; has no other
English definitions and the etymology of the word gives it the
sense we consider most relevant.
</footnote>
<bodyText confidence="0.973376142857143">
deed, social context and function influence lan-
guage at every level — morphologically, lexically,
syntactically, and semantically, through discourse
structure, and through higher-level abstractions
such as pragmatics.
Considered together, the extent to which speak-
ers modify their language for a social context
amounts to an identifiable variation on language,
which we call a lect. Lect is a backformation from
words such as dialect (geographically defined lan-
guage) and ethnolect (language defined by ethnic
context).
In this paper, we describe lect classifiers for so-
cial power relationships. We refer to these lects as:
</bodyText>
<listItem confidence="0.999155166666667">
• UpSpeak: Communication directed to
someone with greater social authority.
• DownSpeak: Communication directed to
someone with less social authority.
• PeerSpeak: Communication to someone of
equal social authority.
</listItem>
<bodyText confidence="0.99990575">
We call the problem of modeling these lects Social
Power Modeling (SPM). The experiments reported
in this paper focused primarily on modeling Up-
Speak and DownSpeak.
Manually constructing tools that effectively
model specific linguistic phenomena suggested by
sociolinguistics would be a Herculean effort.
Moreover, it would be necessary to repeat the ef-
fort in every language! *ur approach first identi-
fies statistically salient phrases of words and parts
of speech — known as n-grams — in training texts
generated in conditions where the social power
</bodyText>
<page confidence="0.98258">
773
</page>
<note confidence="0.979503">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 773–782,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958590909091">
relationship is known. Then, we apply machine
learning to train classifiers with groups of these n-
grams as features. The classifiers assign the Up-
Speak and DownSpeak labels to unseen text. This
methodology is a cost-effective approach to model-
ing social information and requires no language- or
culture-specific feature engineering, although we
believe sociolinguistics-inspired features hold
promise.
When applied to the corpus of emails sent and
received by Enron employees (CAL* Project
2009), this approach produced solid results, despite
a limited number of training and test instances.
This has many implications. Since manually de-
termining the power structure of social networks is
a time-consuming process, even for an expert, ef-
fective SPM could support data driven socio-
cultural research and greatly aid analysts doing
national intelligence work. Social network analysis
(SNA) presupposes a collection of individuals,
whereas a social power lect classifier, once trained,
would provide useful information about individual
author-recipient links. *n networks where SNA
already has traction, SPM could provide comple-
mentary information based on the content of com-
munications.
If SPM were yoked with sentiment analysis, we
might identify which opinions belong to respected
members of online communities or lay the
groundwork for understanding how respect is
earned in social networks.
More broadly, computational sociolinguistics is
a nascent field with significant potential to aid in
modeling and understanding human relationships.
The results in this paper suggest that successes to
date modeling authorship, sentiment, emotion, and
personality extend to social power modeling, and
our approach may well be applicable to other di-
mensions of social meaning.
In the coming sections, we first establish the
Related Work, primarily from Statistical NLP.
We then cover our Approach, the Evaluation,
and, finally, the Conclusions and Future Re-
search.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997914833333334">
The feasibility of Social Power Modeling is sup-
ported by sociolinguistic research identifying spe-
cific ways in which a person&apos;s language reflects his
relative power over others. Fairclough&apos;s classic
work Language and Power explores how
&amp;quot;sociolinguistic conventions ... arise out of -- and
give rise to — particular relations of power&amp;quot; (Fair-
clough, 1989). Brown and Levinson created a the-
ory of politeness, articulating a set of strategies
which people employ to demonstrate different lev-
els of politeness (Brown &amp; Levinson, 1987). Mo-
rand drew upon this theory in his analysis of
emails sent within a corporate hierarchy; in it, he
quantitatively showed that emails from subordi-
nates to superiors are, in fact, perceived as more
polite, and that this perceived politeness is corre-
lated with specific linguistic tactics, including ones
set out by Brown and Levinson (Morand, 2000).
Similarly, Erikson et al identified measurable char-
acteristics of the speech of witnesses in a court-
room setting which were directly associated with
the witness&apos;s level of social power (Erikson, 1978).
Given, then, that there are distinct differences
among what we term UpSpeak and DownSpeak,
we treat Social Power Modeling as an instance of
text classification (or categorization): we seek to
assign a class (UpSpeak or DownSpeak) to a text
sample. Closely related natural language process-
ing problems are authorship attribution, sentiment
analysis, emotion detection, and personality classi-
fication: all aim to extract higher-level information
from language.
Authorship attribution in computational linguis-
tics is the task of identifying the author of a text.
The earliest modern authorship attribution work
was (Mosteller &amp; Wallace, 1964), although foren-
sic authorship analysis has been around much
longer. Mosteller and Wallace used statistical lan-
guage-modeling techniques to measure the similar-
ity of disputed Federalist Papers to samples of
known authorship. Since then, authorship identifi-
cation has become a mature area productively ex-
ploring a broad spectrum of features (stylistic,
lexical, syntactic, and semantic) and many genera-
tive and discriminative modeling approaches (Sta-
matatos, 2009). The generative models of
authorship identification motivated our statistically
extracted lexical and grammatical features, and
future work should consider these language model-
ing (a.k.a. compression) approaches.
Sentiment analysis, which strives to determine
the attitude of an author from text, has recently
garnered much attention (e.g. Pang, Lee, &amp; Vai-
thyanathan, 2002; Kim &amp; Hovy, 2004; Breck, Choi
</bodyText>
<page confidence="0.99677">
774
</page>
<bodyText confidence="0.99997198989899">
&amp; Cardie, 2007). For example, one problem is
classifying user reviews as positive, negative or
neutral. Typically, polarity lexicons (each term is
labeled as positive, negative or neutral) help de-
termine attitudes in text (Hiroya &amp; Takamura,
2005, Ravichandran 2009, Choi &amp; Cardie 2009).
The polarity of an expression can be determined
based on the polarity of its component lexical
items (Choi &amp; Cardie 2008). For example, the po-
larity of the expression is determined by the major-
ity polarity of its lexical items or by rules applied
to syntactic patterns of expressions on how to de-
termine the polarity from its lexical components.
McDonald et al studied models that classify senti-
ment on multiple levels of granularity: sentence
and document-level (McDonald, 2007). Their work
jointly classifies sentiment at both levels instead of
using independent classifiers for each level or cas-
caded classifiers. Similar to our techniques, these
studies determine the polarity of text based on its
component lexical and grammatical sequences.
Unlike their works, our text classification tech-
niques take into account the frequency of occur-
rence of word n-grams and part-of-speech (P*S)
tag sequences, and other measures of statistical
salience in training data.
Text-based emotion prediction is another in-
stance of text classification, where the goal is to
detect the emotion appropriate to a text (Alm, Roth
&amp; Sproat, 2005) or provoked by an author, for ex-
ample (Strapparava &amp; Mihalcea, 2008). Alm, Roth,
and Sproat explored a broad array of lexical and
syntactic features, reminiscent of those of author-
ship attribution, as well as features related to story
structure. A Winnow-based learning algorithm
trained on these features convincingly predicted an
appropriate emotion for individual sentences of
narrative text. Strapparava and Mihalcea try to
predict the emotion the author of a headline intends
to provoke by leveraging words with known affec-
tive sense and by expanding those words&apos; syno-
nyms. They used a Naive Bayes classifier trained
on short blogposts of known emotive sense. The
knowledge engineering approaches were generally
superior to the Naive Bayes approach. *ur ap-
proach is corpus-driven like the Naive Bayes ap-
proach, but we interject statistically driven feature
selection between the corpus and the machine
learning classifiers.
In personality classification, a person&apos;s lan-
guage is used to classify him on different personal-
ity dimensions, such as extraversion or neuroticism
(*berlander &amp; Nowson, 2006; Mairesse &amp; Walker;
2006). The goal is to recover the more permanent
traits of a person, rather than fleeting characteris-
tics such as sentiment or emotion. *berlander and
Nowson explore using a Naive Bayes and an SVM
classifier to perform binary classification of text on
each personality dimension. For example, one clas-
sifier might determine if a person displays a high
or low level of extraversion. Their attempt to clas-
sify each personality trait as either &amp;quot;high&amp;quot; or &amp;quot;low&amp;quot;
echoes early sentiment analysis work that reduced
sentiments to either positive or negative (Pang,
Lee, &amp; Vaithyanathan, 2002), and supports ini-
tially treating Social Power Modeling as a binary
classification task. Personality classification seems
to be the application of text classification which is
the most relevant to Social Power Modeling. As
Mairesse and Walker note, certain personality
traits are indicative of leaders. Thus, the ability to
model personality suggests an ability to model so-
cial power lects as well.
Apart from text classification, work from the
topic modeling community is also closely related
to Social Power Modeling. Andrew McCallum ex-
tended Latent Dirichlet Allocation to model the
author and recipient dependencies of per-message
topic distributions with an Author-Recipient-Topic
(ART) model (McCallum, Wang, &amp; Corrada-
Emmanuel, 2007). This was the first significant
work to model the content and relationships of
communication in a social network. McCallum et
al applied ART to the Enron email corpus to show
that the resulting topics are strongly tied to role.
They suggest that clustering these topic distribu-
tions would yield roles and argue that the person-
to-person similarity matrix yielded by this ap-
proach has advantages over those of canonical so-
cial network analysis. The same authors proposed
several Role-Author-Recipient-Topic (RART)
models to model authors, roles and words simulta-
neously. With a RART modeling roles-per-word,
they produced per-author distributions of generated
roles that appeared reasonable (e.g. they labeled
Role 10 as `grant issues&apos; and Role 2 as `natural
language researcher&apos;).
We have a similar emphasis on statistically
modeling language and interpersonal communica-
</bodyText>
<page confidence="0.990641">
775
</page>
<bodyText confidence="0.999965222222222">
tion. However, we model social power relation-
ships, not roles or topics, and our approach pro-
duces discriminative classifiers, not generative
models, which enables more concrete evaluation.
Namata, Getoor, and Diehl effectively applied
role modeling to the Enron email corpus, allowing
them to infer the social hierarchy structure of En-
ron (Namata et al., 2006). They applied machine
learning classifiers to map individuals to their roles
in the hierarchy based on features related to email
traffic patterns. They also attempt to identify cases
of manager-subordinate relationships within the
email domain by ranking emails using traffic-based
and content-based features (Diehl et al., 2007).
While their task is similar to ours, our goal is to
classify any case in which one person has more
social power than the other, not just identify in-
stances of direct reporting.
</bodyText>
<sectionHeader confidence="0.9022675" genericHeader="method">
3 Approach
3*1 Feature Set-Up
</sectionHeader>
<bodyText confidence="0.999702967741935">
Previous work in traditional text classification and
its variants — such as sentiment analysis — has
achieved successful results by using the bag-of-
words representation; that is, by treating text as a
collection of words with no interdependencies,
training a classifier on a large feature set of word
unigrams which appear in the corpus. However,
our hypothesis was that this approach would not be
the best for SPM. Morand&apos;s study, for instance,
identified specific features that correlate with the
direction of communication within a social hierar-
chy (Morand, 2000). Few of these tactics would be
effectively encapsulated by word unigrams. Many
would be better modeled by P*S tag unigrams
(with no word information) or by longer n-grams
consisting of either words, P*S tags, or a combina-
tion of the two. &amp;quot;Uses subjunctive&amp;quot; and &amp;quot;Uses past
tense&amp;quot; are examples. Because considering such
features would increase the size of the feature
space, we suspected that including these features
would also benefit from algorithmic means of se-
lecting n-grams that are indicative of particular
lects, and even from binning these relevant n-
grams into sets to be used as features.
Therefore, we focused on an approach where
each feature is associated with a set of one or more
n-grams. Each n-gram is a sequence of words, P*S
tags or a combination of words and P*S tags
(&amp;quot;mixed&amp;quot; n-grams). Let S represent a set In,, .. ,
nk} of n-grams. The feature associated with S on
text T would be:
</bodyText>
<equation confidence="0.996590666666667">
k
f (S, T) = L freq(ni,T)
1
</equation>
<bodyText confidence="0.99936">
where freq(ni,T) is the relative frequency (de-
fined later) of ni in text T. Let ni represent the
sequence s1 ... sm where sj specifies either a word
or a P*S tag. Let T represent the text consisting of
the sequence of tagged-word tokens t1 ... tl .
freq(ni,T) is then defined as follows:
</bodyText>
<equation confidence="0.985198">
freq n T = freq s ... s m T
( i , ) ( 1 , )
{ tb+ 1 ... tb+m : V 1≤p≤m (tb+p = sp )}
l m
− +1
where:
word (ti) = sj if sj is a word
tag Q)= sj if sj is a tag
</equation>
<bodyText confidence="0.986834857142857">
To illustrate, consider the following feature set, a
bigram and a trigram (each term in the n-gram ei-
ther has the form word or ^tag):
{please ^VB, please ^`comma&apos; ^VB/
The tag &amp;quot;VB&amp;quot; denotes a verb. Suppose T consists
of the following tokenized and tagged text (sen-
tence initial and final tokens are not shown):
</bodyText>
<equation confidence="0.995507333333333">
please^RB bring^VB the^DET report^NN
to^T0 our^PRP$ next^JJ weekly^JJ meet-
ing^NN .�.
</equation>
<bodyText confidence="0.929465571428571">
The first n-gram of the set, please ^VB, would
match please^RB bring^VB from the text. The fre-
quency of this n-gram in T would then be 1/9,
where 1 is the number of substrings in Tthat match
2 To distinguish a comma separating elements of a set with a
comma as part of an ngram, we use `comma&apos; to denote the
punctuation mark `,&apos; as part of the ngram.
</bodyText>
<equation confidence="0.985652">
i
=
=
ti = sj ��
↔ � ��
</equation>
<page confidence="0.98171">
776
</page>
<bodyText confidence="0.999923111111111">
please ^VB and 9 is the number of bigrams in T,
excluding sentence initial and final markers. The
other n-gram, the trigram please ^`comma&apos; ^VB,
does not have any match, so the final value of the
feature is 1/9.
Defining features in this manner allows us to
both explore the bag-of-words representation as
well as use groups of n-grams as features, which
we believed would be a better fit for this problem.
</bodyText>
<sectionHeader confidence="0.652705" genericHeader="method">
3*2 N-Gram Selection
</sectionHeader>
<bodyText confidence="0.9999006">
To identify n-grams which would be useful fea-
tures, frequencies of n-grams in only the training
set are considered. Different types of frequency
measures were explored to capture different types
of information about an n-gram&apos;s usage. These are:
</bodyText>
<listItem confidence="0.8160343">
• Absolute frequency: The total number of
times a particular n-gram occurs in the text
of a given class (social power lect).
• Relative frequency: The total number of
times a particular n-gram occurs in a given
class, divided by the total number of n-
grams in that class. Normalization by the
size of the class makes relative frequency a
better metric for comparing n-gram usage
across classes.
</listItem>
<bodyText confidence="0.902562">
We then used the following frequency-based met-
rics to select n-grams:
</bodyText>
<listItem confidence="0.9183134">
• We set a minimum threshold for the abso-
lute frequency of the n-gram in a class.
This helps weed out extremely infrequent
words and spelling errors.
• We require that the ratio of the relative
frequency of the n-gram in one class to its
relative frequency in the other class is also
greater than a threshold. This is a simple
means of selecting n-grams indicative of
lect.
</listItem>
<bodyText confidence="0.99969225">
In experiments based on the bag-of-words model,
we only consider an absolute frequency threshold,
whereas in later experiments, we also take into ac-
count the relative frequency ratio threshold.
</bodyText>
<sectionHeader confidence="0.661734" genericHeader="method">
3*3 N-gram Binning
</sectionHeader>
<bodyText confidence="0.999967">
In experiments in which we bin n-grams, selected
n-grams are assigned to the class in which their
relative frequency is highest. For example, an n-
gram whose relative frequency in UpSpeak text is
twice that in DownSpeak text would be assigned to
the class UpSpeak.
N-grams assigned to a class are then partitioned
into sets of n-grams. Each of these sets of n-grams
is associated with a feature. This partition is based
on the n-gram type, the length of n-grams and the
relative frequency ratio of the n-grams. While the
n-grams composing a set may themselves be in-
dicative of social power lects, this method of
grouping them makes no guarantees as to how in-
dicative the overall set is. Therefore, we experi-
mented with filtering out sets which had a
negligible information gain. Information gain is an
information theoretic concept measuring how
much the probability distributions for a feature dif-
fer among the different classes. A small informa-
tion gain suggests that a feature may not be
effective at discriminating between classes.
Although this approach to partitioning is simple
and worthy of improvement, it effectively reduced
the dimensionality of the feature space.
</bodyText>
<sectionHeader confidence="0.720284" genericHeader="method">
3*4 Classification
</sectionHeader>
<bodyText confidence="0.999954">
*nce features are selected, a classifier is trained on
these features. Many features are weak on their
own; they either occur rarely or occur frequently
but only hint weakly at social information. There-
fore, we experimented with classifiers friendly to
weak features, such as Adaboost and Logistic Re-
gression (MaxEnt). However, we generally
achieved the best results using support vector ma-
chines, a machine learning classifier which has
been successfully applied to many previous text
classification problems. We used Weka&apos;s opti-
mized SVMs (SM*) (Witten 2005, Platt 1998) and
default parameters, except where noted.
</bodyText>
<sectionHeader confidence="0.9272915" genericHeader="method">
4 Evaluation
4*1 Data
</sectionHeader>
<bodyText confidence="0.9999585">
To validate our supervised learning approach, we
sought an adequately large English corpus of per-
son-to-person communication labeled with the
ground truth. For this, we used the publicly avail-
</bodyText>
<page confidence="0.985439">
777
</page>
<bodyText confidence="0.999883078947369">
able Enron corpus. After filtering for duplicates
and removing empty or otherwise unusable emails,
the total number of emails is 245K, containing
roughly 90 million words. However, this total in-
cludes emails to non-Enron employees, such as
family members and employees of other corpora-
tions, emails to multiple people, and emails re-
ceived from Enron employees without a known
corporate role. Because the author-recipient rela-
tionships of these emails could not be established,
they were not included in our experiments.
Building upon previous annotation done on the
corpus, we were able to ascertain the corporate role
(CE*, Manager, Employee, etc.) of many email
authors and recipients. From this information, we
determined the author-recipient relationship by
applying general rules about the structure of a cor-
porate hierarchy (an email from an Employee to a
CE*, for instance, is UpSpeak). This annotation
method does not take into account promotions over
time, secretaries speaking on behalf of their super-
visors, or other causes of relationship irregularities.
However, this misinformation would, if anything,
generally hurt our classifiers.
The emails were pre-processed to eliminate text
not written by the author, such as forwarded text
and email headers. As our approach requires text to
be P*S-tagged, we employed Stanford&apos;s P*S tag-
ger (http://nlp.stanford.edu/software/tagger.shtml).
In addition, text was regularized by conversion to
lower case and tokenized to improve counts.
To create training and test sets, we partitioned
the authors of text from the corpus into two sets: A
and B. Then, we used text authored by individuals
in A as a training set and text authored by indi-
viduals in B as a test set. The training set is used to
determine discriminating features upon which clas-
sifiers are built and applied to the test set. We
</bodyText>
<table confidence="0.99829725">
UpSpeak DownSpeak
Links Words Links Words
Training 431 136K 328 63K
Test 232 74K 148 27K
</table>
<tableCaption confidence="0.82321625">
Table 1. Author-based Training and Test partitions. The
number of author-recipient pairs (links) and the number
of words in text labeled as UpSpeak and DownSpeak
are shown.
</tableCaption>
<bodyText confidence="0.999541155555556">
found that partitioning by authors was necessary to
avoid artificially inflated scores, because the clas-
sifiers pick up aspects of particular authors&apos; lan-
guage (idiolect) in addition to social power lect
information. It was not necessary to account for
recipients because the emails did not contain text
from the recipients. Table 1 summarizes the text
partitions.
Because preliminary experiments suggested that
smaller text samples were harder to classify, the
classifiers we describe in this paper were both
trained and tested on a subset of the Enron corpus
where at least 500 words of text was communi-
cated from a specific author to a specific recipient.
This subset contained 142 links, 40% of which
were used as the test set.
Weighting for Cost-Sensitive Learning: The
original corpus was not balanced: the number of
UpSpeak links was greater than the number of
DownSpeak links. Varying the weight given to
training instances is a technique for creating a clas-
sifier that is cost-sensitive, since a classifier built
on an unbalanced training set can be biased to-
wards avoiding errors on the overrepresented class
(Witten, 2005). We wanted misclassifying Up-
Speak as DownSpeak to have the same cost as mis-
classifying DownSpeak as UpSpeak. To do this,
we assigned weights to each instance in the train-
ing set. UpSpeak instances were weighted less than
DownSpeak instances, creating a training set that
was balanced between UpSpeak and DownSpeak.
Balancing the training set generally improved re-
sults.
Weighting the test set in the same manner al-
lowed us to evaluate the performance of the classi-
fier in a situation in which the numbers of
UpSpeak and DownSpeak instances were equal. A
baseline classifier that always predicted the major-
ity class would, on its own, achieve an accuracy of
74% on UpSpeak/DownSpeak classification of
unweighted test set instances with a minimum
length of 500 words. However, results on the
weighted test set are properly compared to a base-
line of 50%. We include both approaches to scor-
ing in this paper.
</bodyText>
<sectionHeader confidence="0.706693" genericHeader="method">
4*2 UpSpeak/DownSpeak Classifiers
</sectionHeader>
<bodyText confidence="0.9998358">
In this section, we describe experiments on classi-
fication of interpersonal email communication into
UpSpeak and DownSpeak. For these experiments,
only emails exchanged between two people related
by a superior/subordinate power relationship were
</bodyText>
<page confidence="0.993164">
778
</page>
<table confidence="0.790379642857143">
Features # of # of Cross-Validation Test Set Test Set
features n-grams (weighted) (unweighted)
Acc (%) F-score Acc (%) F-score Acc (%) F-score
(1) Word unigrams 3899 3899 55.4 .481 62.1 .567 78.9 .748
(2) Word bigrams 3740 3740 54.5 .457 56.4 .498 73.7 .693
(3) Word unigrams + 7639 7639 51.8 .398 63.3 .576 80.7 .762
word bigrams
(4) (3) + tag unigrams 9014 9014 51.8 .398 58.8 .515 77.2 .719
+ tag bigrams
(5) Binned n-grams 8 106 83.0 .830 78*1 *781 77.2 .783
(6) N-grams from (5), 106 106 83.0 .828 60.5 .587 70.2 .698
separated
(7) (5) + polite 9 108 83.9 .839 77.1 .771 78*9 *797
imperatives
</table>
<tableCaption confidence="0.956697333333333">
Table 2. Experiment Results. Accuracies/F-Scores with an SVM classifier for 10-fold cross validation on the
weighted training set and evaluation against the weighted and unweighted test sets. Note that the baseline accu-
racy against the unweighted test set is 74%, but 50% for the weighted test set and cross-validation.
</tableCaption>
<bodyText confidence="0.9960129375">
Human-Engineered Features: Before examin-
ing the data itself, we identified some features
which we thought would be predictive of UpSpeak
or DownSpeak, and which could be fairly accu-
rately modeled by mixed n-grams. These features
included the use of different types of imperatives.
We also thought that the type of greeting or sig-
nature used in the email might be reflective of
formality, and therefore of UpSpeak and Down-
Speak. For example, subordinates might be more
likely to use an honorific when addressing a supe-
rior, or to sign an email with &amp;quot;Thanks.&amp;quot; We pre-
formed some preliminary experiments using these
features. While the feature set was too small to
produce notable results, we identified which fea-
tures actually were indicative of lect. *ne such
feature was polite imperatives (imperatives pre-
ceded by the word &amp;quot;please&amp;quot;). The polite imperative
feature was represented by the n-gram set:
{please &apos;VB, please &apos;`comma&amp;quot; &apos;VB).
Unigrams and Bigrams: As a different sort of
baseline, we considered the results of a bag-of-
words based classifier. Features used in these ex-
periments consist of single words which occurred a
minimum of four times in the relevant lects (Up-
Speak and DownSpeak) of the training set. The
results of the SVM classifier, shown in line (1) of
Table 2, were fairly poor. We then performed ex-
periments with word bigrams, selecting as features
those which occurred at least seven times in the
relevant lects of the training set. This threshold for
bigram frequency minimized the difference in the
number of features between the unigram and bi-
gram experiments. While the bigrams on their own
were less successful than the unigrams, as seen in
line (2), adding them to the unigram features im-
proved accuracy against the test set, shown in line
(3).
As we had speculated that including surface-
level grammar information in the form of tag n-
grams would be beneficial to our problem, we per-
formed experiments using all tag unigrams and all
tag bigrams occurring in the training set as fea-
tures. The results are shown in line (4) of Table 2.
The results of these experiments were not particu-
larly strong, likely owing to the increased sparsity
of the feature vectors.
Binning: Next, we wished to explore longer n-
grams of words or P*S tags and to reduce the
sparsity of the feature vectors. We therefore ex-
perimented with our method of binning the indi-
vidual n-grams to be used as features. We binned
features by their relative frequency ratios. In addi-
tion to binning, we also reduced the total number
of n-grams by setting higher frequency thresholds
and relative frequency ratio thresholds.
When selecting n-grams for this experiment, we
considered only word n-grams and tag n-grams —
not mixed n-grams, which are a combination of
words and tags. These mixed n-grams, while useful
for specifying human-defined features, largely in-
creased the dimensionality of the feature search
space and did not provide significant benefit in
preliminary experiments. For the word sequences,
</bodyText>
<page confidence="0.996234">
779
</page>
<bodyText confidence="0.992538777777778">
we set an absolute frequency threshold that de-
pended on class. The frequency of a word n-gram
in a particular class was required to be 0.18 *
nrlinks / n, where nrlinks is the number of links in
each class (431 for UpSpeak and 328 for Down-
Speak), and n is the number of words in the class.
The relative frequency ratio was required to be at
least 1.5. The tag sequences were required to meet
an absolute frequency threshold of 20, but the
same relative frequency ratio of 1.5.
Binning the n-grams into features was done
based on both the length of the n-gram and the rel-
ative frequency ratio. For example, one feature
might represent the set of all word unigrams which
have a relative frequency ratio between 1.5 and
1.6.
We explored possible feature sets with cross va-
lidation. Before filtering for low information gain,
we used six word n-gram bins per class (relative
frequency ratios of 1.5, 1.6 ..., 1.9 and 2.0+), one
tag n-gram bin for UpSpeak (2.0+), and three tag
n-gram bins for DownSpeak (2.0+, 5.0+, 10.0+).
Even with the weighted training set, DownSpeak
instances were generally harder to identify and
likely benefited from additional representation.
Grouping features by length was a simple but arbi-
trary method for reducing dimensionality, yet
sometimes produced small bins of otherwise good
features. Therefore, as we explored the feature
space, small bins of different n-gram lengths were
merged. We then employed Weka&apos;s InfoGain fea-
ture selection tool to remove those features with a
low information gain3, which removed all but eight
features. The results of this experiment are shown
in line (5) of Table 2. It far outperforms the bag-of-
words baselines, despite significantly fewer fea-
tures.
To ascertain which feature reduction method had
the greatest effect on performance — binning or
setting a relative frequency ratio threshold — we
performed an experiment in which all the n-grams
that we used in the previous experiment were their
own features. Line (6) of Table 2 shows that while
this approach is an improvement over the basic
bag-of-words method, grouping features still im-
proves results.
3 In Weka, features (`attributes&apos;) with a sufficiently low in-
formation gain have this value rounded down to &amp;quot;0&amp;quot;; these are
the features we removed.
*ur goal was to have successful results using
only statistically extracted features; however, we
examined the effect of augmenting this feature set
with the most indicative of the human-identified
feature — polite imperatives. The results, in line (7),
show a slight improvement in both the cross vali-
dation accuracy, and the accuracy against the un-
weighted test set increases to 78*9%4. However,
among the weighted test sets, the highest accuracy
was 78*1%, with the features in line (5).
We report the scores for cross-validation on the
training set for these features; however, because
the features were selected with knowledge of their
per-class distribution in the training set, these
cross-validation scores should not be seen as the
classifier&apos;s true accuracy.
Self-Training: Besides sparse feature vectors,
another factor likely to be hurting our classifier
was the limited amount of training data. We at-
tempted to increase the training set size by per-
forming exploratory experiments with self-
training, an iterative semi-supervised learning me-
thod (Zhu, 2005) with the feature set from (7). *n
the first iteration, we trained the classifier on the
labeled training set, classified the instances of the
unlabeled test set, and then added the instances of
the test set along with their predicted class to the
training set to be used for the next iteration. After
three iterations, the accuracy of the classifier when
evaluated on the weighted test set improved to
82%, suggesting that our classifiers would benefit
from more data.
</bodyText>
<subsectionHeader confidence="0.563303">
Impact of Cost-Sensitive Learning: Without
</subsectionHeader>
<bodyText confidence="0.9999306">
cost-sensitive learning, the classifiers were heavily
biased towards UpSpeak, tending to classify both
DownSpeak and UpSpeak test instances as Up-
Speak. With cost-sensitive training, overall per-
formance improved and classifier performance on
DownSpeak instances improved dramatically. In
(5) of Table 2, DownSpeak classifier accuracy
even edged out the accuracy for UpSpeak. We
expect that on a larger dataset behavior with un-
weighted training and test data would improve.
</bodyText>
<sectionHeader confidence="0.999195" genericHeader="method">
5 Conclusions and Future Research
</sectionHeader>
<bodyText confidence="0.999596666666667">
We presented a corpus-based statistical learning
approach to modeling social power relationships
and experimental results for our methods. To our
</bodyText>
<sectionHeader confidence="0.399386" genericHeader="conclusions">
4 The associated p-value is 6.56E-6.
</sectionHeader>
<page confidence="0.988141">
780
</page>
<bodyText confidence="0.999766">
knowledge, this is the first corpus-based approach
to learning social power lects beyond those in di-
rect reporting relationships.
*ur work strongly suggests that statistically ex-
tracted features are an efficient and effective ap-
proach to modeling social information. *ur
methods exploit many aspects of language use and
effectively model social power information while
using statistical methods at every stage to tease out
the information we seek, significantly reducing
language-, culture-, and lect-specific engineering
needs. *ur feature selection method picks up on
indicators suggested by sociolinguistics, and it also
allows for the identification of features that are not
obviously characteristic of UpSpeak or Down-
Speak. Some easily recognizable features include:
</bodyText>
<subsectionHeader confidence="0.712881">
Lect Ngram Example
</subsectionHeader>
<bodyText confidence="0.458943333333333">
UpSpeak if you &amp;quot;Let me know ifyou need any-
thing.&amp;quot;
&amp;quot;Please call me ifyou have any
questions.&amp;quot;
Down- give me &amp;quot;Read this over and give me a
Speak call.&amp;quot;
&amp;quot;Please give me your comments
next week.&amp;quot;
*n the other hand, other features are less intuitive:
</bodyText>
<subsectionHeader confidence="0.95661">
Lect Ngram Example
</subsectionHeader>
<bodyText confidence="0.997368510638298">
UpSpeak I&apos;ll, we&apos;ll &amp;quot;I&apos;ll let you know the final re-
sults soon&amp;quot;
&amp;quot;Everyone is very excited [...]
and we&apos;re confident we&apos;ll be
successful&amp;quot;
DownSpeak that is, &amp;quot;Neither does any other group
this is but that is not my problem&amp;quot;
&amp;quot;I think this is an excellent let-
ter&amp;quot;
We hope to improve our methods for selecting
and binning features with information theoretic
selection metrics and clustering algorithms.
We also have begun work on 3-way, UpSpeak/
DownSpeak/PeerSpeak classification. Training a
multiclass SVM on the binned n-gram features
from (5) produces 51*6% cross-validation accu-
racy on training data and 44*4% accuracy on the
weighted test set (both numbers should be com-
pared to a 33% baseline). That classifier contained
no n-gram features selected from the PeerSpeak
class. Preliminary experiments incorporating
PeerSpeak n-grams yield slightly better numbers.
However, early results also suggest that the three-
way classification problem is made more tractable
with cascaded two-way classifiers; feature selec-
tion was more manageable with binary problems.
For example, one classifier determines whether an
instance is UpSpeak; if it is not, a second classifier
distinguishes between DownSpeak and PeerSpeak.
*ur text classification problem is similar to senti-
ment analysis in that there are class dependencies;
for example, DownSpeak is more closely related to
PeerSpeak than to UpSpeak. We might attempt to
exploit these dependencies in a manner similar to
Pang and Lee (2005) to improve three-way classi-
fication.
In addition, we had promising early results for
classification of author-recipient links with 200 to
500 words, so we plan to explore performance im-
provements for links of few words.
In early, unpublished work, we had promising
results with generative model-based approach to
SPM, and we plan to revisit it; language models
are a natural fit for lect modeling. Finally, we hope
to investigate how SPM and SNA can enhance one
another, and explore other lect classification prob-
lems for which the ground truth can be found.
</bodyText>
<sectionHeader confidence="0.999083" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996124823529412">
Dr. Richard Sproat contributed time, valuable in-
sights, and wise counsel on several occasions dur-
ing the course of the research. Dr. Lillian Lee and
her students in Natural Language Processing and
Social Interaction reviewed the paper, offering
valuable feedback and helpful leads.
*ur colleague, Diane Bramsen, created an ex-
cellent graphical interface for probing and under-
standing the results. Jeff Lau guided and advised
throughout the project.
We thank our anonymous reviewers for prudent
advice.
This work was funded by the Army Studies
Board and sponsored by Col. Timothy Hill of the
United Stated Army Intelligence and Security
Command (INSC*M) Futures Directorate under
contract W911W4-08-D-0011.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997548">
Cecilia *vesdotter Alm, Dan Roth and Richard Sproat.
2005. Emotions from text: machine learning for text-
based emotion prediction. HLTIEMNLP 2005. *cto-
ber 6-8, 2005, Vancouver.
</reference>
<page confidence="0.970907">
781
</page>
<reference confidence="0.99927621978022">
Penelope Brown and Stephen C. Levinson. 1987. Po-
liteness: Some universals in language usage. Cam-
bridge: Cambridge University Press.
Eric Breck, Yejin Choi and Claire Cardie. 2007. Identi-
fying expressions of opinion in context.
In Proceedings of the Twentieth International Joint
Conference on Artificial Intelligence (IJCAI-2007)
CAL* Project. 2009. Enron E-Mail Dataset.
http://www.cs.cmu.edu/-enron/.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Honolulu, Hawaii: ACM. 793-801.
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. Empirical Methods
in Natural Language Processing (EMNLP).
Christopher P. Diehl, Galileo Namata, and Lise Getoor.
2007. Relationship identification for social network
discovery. AAAI &apos;07: Proceedings of the 22nd Na-
tional Conference on Artificial Intelligence.
Bonnie Erickson, et al. 1978. Speech style and impres-
sion formation in a court setting: The effects of&apos;pow-
erful&apos; and &apos;powerless&apos; speech. Journal ofExperimental
Social Psychology 14: 266-79.
Norman Fairclough. 1989. Language and power. Lon-
don: Longman.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Exploration (1): Issue 1.
JHU Center for Imaging Science. 2005. Scan Statistics
on Enron Graphs. http://cisjhu.edu/-parky/Enron/
Soo-min Kim and Eduard Hovy. 2004. Determining the
Sentiment of *pinions. Proceedings of the C0LING
Conference. Geneva, Switzerland.
Francois Mairesse and Marilyn Walker. 2006. Auto-
matic recognition of personality in conversation. Pro-
ceedings of HLT-NAACL. New York City, New York.
Galileo Mark S. Namata Jr., Lise Getoor, and Christo-
pher P. Diehl. 2006. Inferring organizational titles in
online communication. ICML 2006, 179-181.
Andrew McCallum, Xuerui Wang, and Andres Corrada-
Emmanuel. 2007. Topic and role discovery in social
networks with experiments on Enron and academic e-
Mail. Journal ofArtificial Intelligence Research 29.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. Proceedings of the
ACL.
David Morand. 2000. Language and power: An empiri-
cal analysis of linguistic strategies used in supe-
rior/subordinate communication. Journal of
0rganizational Behavior, 21:235-248.
Frederick Mosteller and David L. Wallace. 1964. Infer-
ence and disputed authorship: The Federalist. Addi-
son-Wesley, Reading, Mass.
Jon *berlander and Scott Nowson. 2006. Whose thumb
is it anyway? Classifying author personality from we-
blog text. Proceedings of CoLing/ACL. Sydney, Aus-
tralia.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. Proceedings ofEMNLP,
79-86.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. Proceedings of the ACL.
John Platt. 1998. Sequential minimal optimization: A
fast algorithm for training support vector machines. In
Technical Report MST-TR-98-14. Microsoft Re-
search.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. European
Chapter of the Association for Computational Lin-
guistics.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. JASIST 60(3): 538-556.
Carol Strapparava and Rada Mihalcea. 2008. Learning
to identify emotions in text. SAC 2008: 1556-1560
Hiroya Takamura, Takashi Inui, and Manabu *kumura.
2005. Semantic *rientations of Words using Spin
Model. Annual Meeting of the Association for Com-
putational Linguistics.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kauffman.
Xiaojin Zhu. 2005. Semi-supervised learning literature
survey. Technical Report 1530, Department of Com-
puter Sciences, University of Wisconsin, Madison.
</reference>
<page confidence="0.997373">
782
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.031581">
<title confidence="0.999551">Extracting Social Power Relationships from Natural Language</title>
<author confidence="0.965922">Philip</author>
<affiliation confidence="0.832436666666667">Louisville, KY Ami Massachusetts Institute of</affiliation>
<address confidence="0.522829">Cambridge,</address>
<author confidence="0.381212333333333">Martha San Diego</author>
<author confidence="0.381212333333333">Rafael</author>
<affiliation confidence="0.804403">SET Arlington,</affiliation>
<email confidence="0.999882">ralonso@setcorp.com</email>
<abstract confidence="0.997716608695652">Sociolinguists have long argued that social context influences language use in all manner ways, resulting in This paper explores a text classification problem we will an example of what has been termed computational sociolinguistics. In particular, we use machine learning techniques identify power relationships members of a social network, based purely on the content of their interpersonal communication. We rely on statistical methods, as opposed to language-specific engineering, to extract features which represent vocabulary and grammar usage indicative of social power lect. We then apply support vector machines to model the social power lects representing superior-subordinate communication in the Enron email corpus. *ur results validate the treatment of lect modeling as a text classification problem — albeit a hard one — and constitute a case for future research in computational sociolinguistics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cecilia vesdotter Alm</author>
<author>Dan Roth</author>
<author>Richard Sproat</author>
</authors>
<title>Emotions from text: machine learning for textbased emotion prediction. HLTIEMNLP</title>
<date>2005</date>
<pages>6--8</pages>
<location>Vancouver.</location>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>Cecilia *vesdotter Alm, Dan Roth and Richard Sproat. 2005. Emotions from text: machine learning for textbased emotion prediction. HLTIEMNLP 2005. *ctober 6-8, 2005, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Brown</author>
<author>Stephen C Levinson</author>
</authors>
<title>Politeness: Some universals in language usage. Cambridge:</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5933" citStr="Brown &amp; Levinson, 1987" startWordPosition="869" endWordPosition="872">r Approach, the Evaluation, and, finally, the Conclusions and Future Research. 2 Related Work The feasibility of Social Power Modeling is supported by sociolinguistic research identifying specific ways in which a person&apos;s language reflects his relative power over others. Fairclough&apos;s classic work Language and Power explores how &amp;quot;sociolinguistic conventions ... arise out of -- and give rise to — particular relations of power&amp;quot; (Fairclough, 1989). Brown and Levinson created a theory of politeness, articulating a set of strategies which people employ to demonstrate different levels of politeness (Brown &amp; Levinson, 1987). Morand drew upon this theory in his analysis of emails sent within a corporate hierarchy; in it, he quantitatively showed that emails from subordinates to superiors are, in fact, perceived as more polite, and that this perceived politeness is correlated with specific linguistic tactics, including ones set out by Brown and Levinson (Morand, 2000). Similarly, Erikson et al identified measurable characteristics of the speech of witnesses in a courtroom setting which were directly associated with the witness&apos;s level of social power (Erikson, 1978). Given, then, that there are distinct difference</context>
</contexts>
<marker>Brown, Levinson, 1987</marker>
<rawString>Penelope Brown and Stephen C. Levinson. 1987. Politeness: Some universals in language usage. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI-2007)</booktitle>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI-2007)</rawString>
</citation>
<citation valid="true">
<authors>
<author>CAL Project</author>
</authors>
<date>2009</date>
<note>Enron E-Mail Dataset. http://www.cs.cmu.edu/-enron/.</note>
<contexts>
<context position="3929" citStr="Project 2009" startWordPosition="572" endWordPosition="573">onal Linguistics, pages 773–782, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics relationship is known. Then, we apply machine learning to train classifiers with groups of these ngrams as features. The classifiers assign the UpSpeak and DownSpeak labels to unseen text. This methodology is a cost-effective approach to modeling social information and requires no language- or culture-specific feature engineering, although we believe sociolinguistics-inspired features hold promise. When applied to the corpus of emails sent and received by Enron employees (CAL* Project 2009), this approach produced solid results, despite a limited number of training and test instances. This has many implications. Since manually determining the power structure of social networks is a time-consuming process, even for an expert, effective SPM could support data driven sociocultural research and greatly aid analysts doing national intelligence work. Social network analysis (SNA) presupposes a collection of individuals, whereas a social power lect classifier, once trained, would provide useful information about individual author-recipient links. *n networks where SNA already has tract</context>
</contexts>
<marker>Project, 2009</marker>
<rawString>CAL* Project. 2009. Enron E-Mail Dataset. http://www.cs.cmu.edu/-enron/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>793--801</pages>
<publisher>ACM.</publisher>
<location>Honolulu, Hawaii:</location>
<contexts>
<context position="8406" citStr="Choi &amp; Cardie 2008" startWordPosition="1239" endWordPosition="1242">n) approaches. Sentiment analysis, which strives to determine the attitude of an author from text, has recently garnered much attention (e.g. Pang, Lee, &amp; Vaithyanathan, 2002; Kim &amp; Hovy, 2004; Breck, Choi 774 &amp; Cardie, 2007). For example, one problem is classifying user reviews as positive, negative or neutral. Typically, polarity lexicons (each term is labeled as positive, negative or neutral) help determine attitudes in text (Hiroya &amp; Takamura, 2005, Ravichandran 2009, Choi &amp; Cardie 2009). The polarity of an expression can be determined based on the polarity of its component lexical items (Choi &amp; Cardie 2008). For example, the polarity of the expression is determined by the majority polarity of its lexical items or by rules applied to syntactic patterns of expressions on how to determine the polarity from its lexical components. McDonald et al studied models that classify sentiment on multiple levels of granularity: sentence and document-level (McDonald, 2007). Their work jointly classifies sentiment at both levels instead of using independent classifiers for each level or cascaded classifiers. Similar to our techniques, these studies determine the polarity of text based on its component lexical a</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. Proceedings of the Conference on Empirical Methods in Natural Language Processing. Honolulu, Hawaii: ACM. 793-801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification.</title>
<date>2009</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="8283" citStr="Choi &amp; Cardie 2009" startWordPosition="1218" endWordPosition="1221">ally extracted lexical and grammatical features, and future work should consider these language modeling (a.k.a. compression) approaches. Sentiment analysis, which strives to determine the attitude of an author from text, has recently garnered much attention (e.g. Pang, Lee, &amp; Vaithyanathan, 2002; Kim &amp; Hovy, 2004; Breck, Choi 774 &amp; Cardie, 2007). For example, one problem is classifying user reviews as positive, negative or neutral. Typically, polarity lexicons (each term is labeled as positive, negative or neutral) help determine attitudes in text (Hiroya &amp; Takamura, 2005, Ravichandran 2009, Choi &amp; Cardie 2009). The polarity of an expression can be determined based on the polarity of its component lexical items (Choi &amp; Cardie 2008). For example, the polarity of the expression is determined by the majority polarity of its lexical items or by rules applied to syntactic patterns of expressions on how to determine the polarity from its lexical components. McDonald et al studied models that classify sentiment on multiple levels of granularity: sentence and document-level (McDonald, 2007). Their work jointly classifies sentiment at both levels instead of using independent classifiers for each level or cas</context>
</contexts>
<marker>Choi, Cardie, 2009</marker>
<rawString>Yejin Choi and Claire Cardie. 2009. Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification. Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher P Diehl</author>
<author>Galileo Namata</author>
<author>Lise Getoor</author>
</authors>
<title>Relationship identification for social network discovery.</title>
<date>2007</date>
<booktitle>AAAI &apos;07: Proceedings of the 22nd National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="13424" citStr="Diehl et al., 2007" startWordPosition="2003" endWordPosition="2006"> and our approach produces discriminative classifiers, not generative models, which enables more concrete evaluation. Namata, Getoor, and Diehl effectively applied role modeling to the Enron email corpus, allowing them to infer the social hierarchy structure of Enron (Namata et al., 2006). They applied machine learning classifiers to map individuals to their roles in the hierarchy based on features related to email traffic patterns. They also attempt to identify cases of manager-subordinate relationships within the email domain by ranking emails using traffic-based and content-based features (Diehl et al., 2007). While their task is similar to ours, our goal is to classify any case in which one person has more social power than the other, not just identify instances of direct reporting. 3 Approach 3*1 Feature Set-Up Previous work in traditional text classification and its variants — such as sentiment analysis — has achieved successful results by using the bag-ofwords representation; that is, by treating text as a collection of words with no interdependencies, training a classifier on a large feature set of word unigrams which appear in the corpus. However, our hypothesis was that this approach would </context>
</contexts>
<marker>Diehl, Namata, Getoor, 2007</marker>
<rawString>Christopher P. Diehl, Galileo Namata, and Lise Getoor. 2007. Relationship identification for social network discovery. AAAI &apos;07: Proceedings of the 22nd National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Erickson</author>
</authors>
<title>Speech style and impression formation in a court setting: The effects of&apos;powerful&apos; and &apos;powerless&apos; speech.</title>
<date>1978</date>
<journal>Journal ofExperimental Social Psychology</journal>
<volume>14</volume>
<pages>266--79</pages>
<marker>Erickson, 1978</marker>
<rawString>Bonnie Erickson, et al. 1978. Speech style and impression formation in a court setting: The effects of&apos;powerful&apos; and &apos;powerless&apos; speech. Journal ofExperimental Social Psychology 14: 266-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman Fairclough</author>
</authors>
<title>Language and power.</title>
<date>1989</date>
<location>London: Longman.</location>
<contexts>
<context position="5757" citStr="Fairclough, 1989" startWordPosition="842" endWordPosition="844">ay well be applicable to other dimensions of social meaning. In the coming sections, we first establish the Related Work, primarily from Statistical NLP. We then cover our Approach, the Evaluation, and, finally, the Conclusions and Future Research. 2 Related Work The feasibility of Social Power Modeling is supported by sociolinguistic research identifying specific ways in which a person&apos;s language reflects his relative power over others. Fairclough&apos;s classic work Language and Power explores how &amp;quot;sociolinguistic conventions ... arise out of -- and give rise to — particular relations of power&amp;quot; (Fairclough, 1989). Brown and Levinson created a theory of politeness, articulating a set of strategies which people employ to demonstrate different levels of politeness (Brown &amp; Levinson, 1987). Morand drew upon this theory in his analysis of emails sent within a corporate hierarchy; in it, he quantitatively showed that emails from subordinates to superiors are, in fact, perceived as more polite, and that this perceived politeness is correlated with specific linguistic tactics, including ones set out by Brown and Levinson (Morand, 2000). Similarly, Erikson et al identified measurable characteristics of the spe</context>
</contexts>
<marker>Fairclough, 1989</marker>
<rawString>Norman Fairclough. 1989. Language and power. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Exploration (1): Issue</journal>
<volume>1</volume>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Exploration (1): Issue 1.</rawString>
</citation>
<citation valid="true">
<title>Scan Statistics on Enron Graphs.</title>
<date>2005</date>
<institution>JHU Center for Imaging Science.</institution>
<note>http://cisjhu.edu/-parky/Enron/</note>
<contexts>
<context position="35531" citStr="(2005)" startWordPosition="5645" endWordPosition="5645">arly results also suggest that the threeway classification problem is made more tractable with cascaded two-way classifiers; feature selection was more manageable with binary problems. For example, one classifier determines whether an instance is UpSpeak; if it is not, a second classifier distinguishes between DownSpeak and PeerSpeak. *ur text classification problem is similar to sentiment analysis in that there are class dependencies; for example, DownSpeak is more closely related to PeerSpeak than to UpSpeak. We might attempt to exploit these dependencies in a manner similar to Pang and Lee (2005) to improve three-way classification. In addition, we had promising early results for classification of author-recipient links with 200 to 500 words, so we plan to explore performance improvements for links of few words. In early, unpublished work, we had promising results with generative model-based approach to SPM, and we plan to revisit it; language models are a natural fit for lect modeling. Finally, we hope to investigate how SPM and SNA can enhance one another, and explore other lect classification problems for which the ground truth can be found. Acknowledgments Dr. Richard Sproat contr</context>
</contexts>
<marker>2005</marker>
<rawString>JHU Center for Imaging Science. 2005. Scan Statistics on Enron Graphs. http://cisjhu.edu/-parky/Enron/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the Sentiment of *pinions.</title>
<date>2004</date>
<booktitle>Proceedings of the C0LING Conference.</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="7979" citStr="Kim &amp; Hovy, 2004" startWordPosition="1171" endWordPosition="1174">uthorship identification has become a mature area productively exploring a broad spectrum of features (stylistic, lexical, syntactic, and semantic) and many generative and discriminative modeling approaches (Stamatatos, 2009). The generative models of authorship identification motivated our statistically extracted lexical and grammatical features, and future work should consider these language modeling (a.k.a. compression) approaches. Sentiment analysis, which strives to determine the attitude of an author from text, has recently garnered much attention (e.g. Pang, Lee, &amp; Vaithyanathan, 2002; Kim &amp; Hovy, 2004; Breck, Choi 774 &amp; Cardie, 2007). For example, one problem is classifying user reviews as positive, negative or neutral. Typically, polarity lexicons (each term is labeled as positive, negative or neutral) help determine attitudes in text (Hiroya &amp; Takamura, 2005, Ravichandran 2009, Choi &amp; Cardie 2009). The polarity of an expression can be determined based on the polarity of its component lexical items (Choi &amp; Cardie 2008). For example, the polarity of the expression is determined by the majority polarity of its lexical items or by rules applied to syntactic patterns of expressions on how to </context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-min Kim and Eduard Hovy. 2004. Determining the Sentiment of *pinions. Proceedings of the C0LING Conference. Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Mairesse</author>
<author>Marilyn Walker</author>
</authors>
<title>Automatic recognition of personality in conversation.</title>
<date>2006</date>
<booktitle>Proceedings of HLT-NAACL.</booktitle>
<location>New York City, New York.</location>
<marker>Mairesse, Walker, 2006</marker>
<rawString>Francois Mairesse and Marilyn Walker. 2006. Automatic recognition of personality in conversation. Proceedings of HLT-NAACL. New York City, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galileo Mark S Namata Jr</author>
<author>Lise Getoor</author>
<author>Christopher P Diehl</author>
</authors>
<title>Inferring organizational titles in online communication. ICML</title>
<date>2006</date>
<pages>179--181</pages>
<marker>Jr, Getoor, Diehl, 2006</marker>
<rawString>Galileo Mark S. Namata Jr., Lise Getoor, and Christopher P. Diehl. 2006. Inferring organizational titles in online communication. ICML 2006, 179-181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Xuerui Wang</author>
<author>Andres CorradaEmmanuel</author>
</authors>
<title>Topic and role discovery in social networks with experiments on Enron and academic eMail.</title>
<date>2007</date>
<journal>Journal ofArtificial Intelligence Research</journal>
<volume>29</volume>
<contexts>
<context position="11858" citStr="McCallum, Wang, &amp; CorradaEmmanuel, 2007" startWordPosition="1767" endWordPosition="1772">y classification seems to be the application of text classification which is the most relevant to Social Power Modeling. As Mairesse and Walker note, certain personality traits are indicative of leaders. Thus, the ability to model personality suggests an ability to model social power lects as well. Apart from text classification, work from the topic modeling community is also closely related to Social Power Modeling. Andrew McCallum extended Latent Dirichlet Allocation to model the author and recipient dependencies of per-message topic distributions with an Author-Recipient-Topic (ART) model (McCallum, Wang, &amp; CorradaEmmanuel, 2007). This was the first significant work to model the content and relationships of communication in a social network. McCallum et al applied ART to the Enron email corpus to show that the resulting topics are strongly tied to role. They suggest that clustering these topic distributions would yield roles and argue that the personto-person similarity matrix yielded by this approach has advantages over those of canonical social network analysis. The same authors proposed several Role-Author-Recipient-Topic (RART) models to model authors, roles and words simultaneously. With a RART modeling roles-pe</context>
</contexts>
<marker>McCallum, Wang, CorradaEmmanuel, 2007</marker>
<rawString>Andrew McCallum, Xuerui Wang, and Andres CorradaEmmanuel. 2007. Topic and role discovery in social networks with experiments on Enron and academic eMail. Journal ofArtificial Intelligence Research 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>Proceedings of the ACL.</booktitle>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Morand</author>
</authors>
<title>Language and power: An empirical analysis of linguistic strategies used in superior/subordinate communication.</title>
<date>2000</date>
<journal>Journal of 0rganizational Behavior,</journal>
<pages>21--235</pages>
<contexts>
<context position="6282" citStr="Morand, 2000" startWordPosition="928" endWordPosition="929">... arise out of -- and give rise to — particular relations of power&amp;quot; (Fairclough, 1989). Brown and Levinson created a theory of politeness, articulating a set of strategies which people employ to demonstrate different levels of politeness (Brown &amp; Levinson, 1987). Morand drew upon this theory in his analysis of emails sent within a corporate hierarchy; in it, he quantitatively showed that emails from subordinates to superiors are, in fact, perceived as more polite, and that this perceived politeness is correlated with specific linguistic tactics, including ones set out by Brown and Levinson (Morand, 2000). Similarly, Erikson et al identified measurable characteristics of the speech of witnesses in a courtroom setting which were directly associated with the witness&apos;s level of social power (Erikson, 1978). Given, then, that there are distinct differences among what we term UpSpeak and DownSpeak, we treat Social Power Modeling as an instance of text classification (or categorization): we seek to assign a class (UpSpeak or DownSpeak) to a text sample. Closely related natural language processing problems are authorship attribution, sentiment analysis, emotion detection, and personality classificati</context>
<context position="14199" citStr="Morand, 2000" startWordPosition="2132" endWordPosition="2133">irect reporting. 3 Approach 3*1 Feature Set-Up Previous work in traditional text classification and its variants — such as sentiment analysis — has achieved successful results by using the bag-ofwords representation; that is, by treating text as a collection of words with no interdependencies, training a classifier on a large feature set of word unigrams which appear in the corpus. However, our hypothesis was that this approach would not be the best for SPM. Morand&apos;s study, for instance, identified specific features that correlate with the direction of communication within a social hierarchy (Morand, 2000). Few of these tactics would be effectively encapsulated by word unigrams. Many would be better modeled by P*S tag unigrams (with no word information) or by longer n-grams consisting of either words, P*S tags, or a combination of the two. &amp;quot;Uses subjunctive&amp;quot; and &amp;quot;Uses past tense&amp;quot; are examples. Because considering such features would increase the size of the feature space, we suspected that including these features would also benefit from algorithmic means of selecting n-grams that are indicative of particular lects, and even from binning these relevant ngrams into sets to be used as features. T</context>
</contexts>
<marker>Morand, 2000</marker>
<rawString>David Morand. 2000. Language and power: An empirical analysis of linguistic strategies used in superior/subordinate communication. Journal of 0rganizational Behavior, 21:235-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Inference and disputed authorship: The Federalist.</title>
<date>1964</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Mass.</location>
<contexts>
<context position="7125" citStr="Mosteller &amp; Wallace, 1964" startWordPosition="1050" endWordPosition="1053">that there are distinct differences among what we term UpSpeak and DownSpeak, we treat Social Power Modeling as an instance of text classification (or categorization): we seek to assign a class (UpSpeak or DownSpeak) to a text sample. Closely related natural language processing problems are authorship attribution, sentiment analysis, emotion detection, and personality classification: all aim to extract higher-level information from language. Authorship attribution in computational linguistics is the task of identifying the author of a text. The earliest modern authorship attribution work was (Mosteller &amp; Wallace, 1964), although forensic authorship analysis has been around much longer. Mosteller and Wallace used statistical language-modeling techniques to measure the similarity of disputed Federalist Papers to samples of known authorship. Since then, authorship identification has become a mature area productively exploring a broad spectrum of features (stylistic, lexical, syntactic, and semantic) and many generative and discriminative modeling approaches (Stamatatos, 2009). The generative models of authorship identification motivated our statistically extracted lexical and grammatical features, and future w</context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1964. Inference and disputed authorship: The Federalist. Addison-Wesley, Reading, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon berlander</author>
<author>Scott Nowson</author>
</authors>
<title>Whose thumb is it anyway? Classifying author personality from weblog text.</title>
<date>2006</date>
<booktitle>Proceedings of CoLing/ACL.</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="10516" citStr="berlander &amp; Nowson, 2006" startWordPosition="1564" endWordPosition="1567">s to provoke by leveraging words with known affective sense and by expanding those words&apos; synonyms. They used a Naive Bayes classifier trained on short blogposts of known emotive sense. The knowledge engineering approaches were generally superior to the Naive Bayes approach. *ur approach is corpus-driven like the Naive Bayes approach, but we interject statistically driven feature selection between the corpus and the machine learning classifiers. In personality classification, a person&apos;s language is used to classify him on different personality dimensions, such as extraversion or neuroticism (*berlander &amp; Nowson, 2006; Mairesse &amp; Walker; 2006). The goal is to recover the more permanent traits of a person, rather than fleeting characteristics such as sentiment or emotion. *berlander and Nowson explore using a Naive Bayes and an SVM classifier to perform binary classification of text on each personality dimension. For example, one classifier might determine if a person displays a high or low level of extraversion. Their attempt to classify each personality trait as either &amp;quot;high&amp;quot; or &amp;quot;low&amp;quot; echoes early sentiment analysis work that reduced sentiments to either positive or negative (Pang, Lee, &amp; Vaithyanathan, 2</context>
</contexts>
<marker>berlander, Nowson, 2006</marker>
<rawString>Jon *berlander and Scott Nowson. 2006. Whose thumb is it anyway? Classifying author personality from weblog text. Proceedings of CoLing/ACL. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>Proceedings ofEMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="7961" citStr="Pang, Lee, &amp; Vaithyanathan, 2002" startWordPosition="1165" endWordPosition="1170">of known authorship. Since then, authorship identification has become a mature area productively exploring a broad spectrum of features (stylistic, lexical, syntactic, and semantic) and many generative and discriminative modeling approaches (Stamatatos, 2009). The generative models of authorship identification motivated our statistically extracted lexical and grammatical features, and future work should consider these language modeling (a.k.a. compression) approaches. Sentiment analysis, which strives to determine the attitude of an author from text, has recently garnered much attention (e.g. Pang, Lee, &amp; Vaithyanathan, 2002; Kim &amp; Hovy, 2004; Breck, Choi 774 &amp; Cardie, 2007). For example, one problem is classifying user reviews as positive, negative or neutral. Typically, polarity lexicons (each term is labeled as positive, negative or neutral) help determine attitudes in text (Hiroya &amp; Takamura, 2005, Ravichandran 2009, Choi &amp; Cardie 2009). The polarity of an expression can be determined based on the polarity of its component lexical items (Choi &amp; Cardie 2008). For example, the polarity of the expression is determined by the majority polarity of its lexical items or by rules applied to syntactic patterns of expr</context>
<context position="11119" citStr="Pang, Lee, &amp; Vaithyanathan, 2002" startWordPosition="1660" endWordPosition="1664">sm (*berlander &amp; Nowson, 2006; Mairesse &amp; Walker; 2006). The goal is to recover the more permanent traits of a person, rather than fleeting characteristics such as sentiment or emotion. *berlander and Nowson explore using a Naive Bayes and an SVM classifier to perform binary classification of text on each personality dimension. For example, one classifier might determine if a person displays a high or low level of extraversion. Their attempt to classify each personality trait as either &amp;quot;high&amp;quot; or &amp;quot;low&amp;quot; echoes early sentiment analysis work that reduced sentiments to either positive or negative (Pang, Lee, &amp; Vaithyanathan, 2002), and supports initially treating Social Power Modeling as a binary classification task. Personality classification seems to be the application of text classification which is the most relevant to Social Power Modeling. As Mairesse and Walker note, certain personality traits are indicative of leaders. Thus, the ability to model personality suggests an ability to model social power lects as well. Apart from text classification, work from the topic modeling community is also closely related to Social Power Modeling. Andrew McCallum extended Latent Dirichlet Allocation to model the author and re</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. Proceedings ofEMNLP, 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>Proceedings of the ACL.</booktitle>
<contexts>
<context position="35531" citStr="Pang and Lee (2005)" startWordPosition="5642" endWordPosition="5645">s. However, early results also suggest that the threeway classification problem is made more tractable with cascaded two-way classifiers; feature selection was more manageable with binary problems. For example, one classifier determines whether an instance is UpSpeak; if it is not, a second classifier distinguishes between DownSpeak and PeerSpeak. *ur text classification problem is similar to sentiment analysis in that there are class dependencies; for example, DownSpeak is more closely related to PeerSpeak than to UpSpeak. We might attempt to exploit these dependencies in a manner similar to Pang and Lee (2005) to improve three-way classification. In addition, we had promising early results for classification of author-recipient links with 200 to 500 words, so we plan to explore performance improvements for links of few words. In early, unpublished work, we had promising results with generative model-based approach to SPM, and we plan to revisit it; language models are a natural fit for lect modeling. Finally, we hope to investigate how SPM and SNA can enhance one another, and explore other lect classification problems for which the ground truth can be found. Acknowledgments Dr. Richard Sproat contr</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Sequential minimal optimization: A fast algorithm for training support vector machines. In</title>
<date>1998</date>
<tech>Technical Report MST-TR-98-14. Microsoft Research.</tech>
<contexts>
<context position="19843" citStr="Platt 1998" startWordPosition="3124" endWordPosition="3125"> the feature space. 3*4 Classification *nce features are selected, a classifier is trained on these features. Many features are weak on their own; they either occur rarely or occur frequently but only hint weakly at social information. Therefore, we experimented with classifiers friendly to weak features, such as Adaboost and Logistic Regression (MaxEnt). However, we generally achieved the best results using support vector machines, a machine learning classifier which has been successfully applied to many previous text classification problems. We used Weka&apos;s optimized SVMs (SM*) (Witten 2005, Platt 1998) and default parameters, except where noted. 4 Evaluation 4*1 Data To validate our supervised learning approach, we sought an adequately large English corpus of person-to-person communication labeled with the ground truth. For this, we used the publicly avail777 able Enron corpus. After filtering for duplicates and removing empty or otherwise unusable emails, the total number of emails is 245K, containing roughly 90 million words. However, this total includes emails to non-Enron employees, such as family members and employees of other corporations, emails to multiple people, and emails receive</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>John Platt. 1998. Sequential minimal optimization: A fast algorithm for training support vector machines. In Technical Report MST-TR-98-14. Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semisupervised polarity lexicon induction.</title>
<date>2009</date>
<journal>European Chapter of the Association for Computational Linguistics.</journal>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>A survey of modern authorship attribution methods.</title>
<date>2009</date>
<journal>JASIST</journal>
<volume>60</volume>
<issue>3</issue>
<pages>538--556</pages>
<contexts>
<context position="7588" citStr="Stamatatos, 2009" startWordPosition="1117" endWordPosition="1119">computational linguistics is the task of identifying the author of a text. The earliest modern authorship attribution work was (Mosteller &amp; Wallace, 1964), although forensic authorship analysis has been around much longer. Mosteller and Wallace used statistical language-modeling techniques to measure the similarity of disputed Federalist Papers to samples of known authorship. Since then, authorship identification has become a mature area productively exploring a broad spectrum of features (stylistic, lexical, syntactic, and semantic) and many generative and discriminative modeling approaches (Stamatatos, 2009). The generative models of authorship identification motivated our statistically extracted lexical and grammatical features, and future work should consider these language modeling (a.k.a. compression) approaches. Sentiment analysis, which strives to determine the attitude of an author from text, has recently garnered much attention (e.g. Pang, Lee, &amp; Vaithyanathan, 2002; Kim &amp; Hovy, 2004; Breck, Choi 774 &amp; Cardie, 2007). For example, one problem is classifying user reviews as positive, negative or neutral. Typically, polarity lexicons (each term is labeled as positive, negative or neutral) he</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>Efstathios Stamatatos. 2009. A survey of modern authorship attribution methods. JASIST 60(3): 538-556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Strapparava</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning to identify emotions in text. SAC</title>
<date>2008</date>
<pages>1556--1560</pages>
<contexts>
<context position="9481" citStr="Strapparava &amp; Mihalcea, 2008" startWordPosition="1408" endWordPosition="1411">assifiers for each level or cascaded classifiers. Similar to our techniques, these studies determine the polarity of text based on its component lexical and grammatical sequences. Unlike their works, our text classification techniques take into account the frequency of occurrence of word n-grams and part-of-speech (P*S) tag sequences, and other measures of statistical salience in training data. Text-based emotion prediction is another instance of text classification, where the goal is to detect the emotion appropriate to a text (Alm, Roth &amp; Sproat, 2005) or provoked by an author, for example (Strapparava &amp; Mihalcea, 2008). Alm, Roth, and Sproat explored a broad array of lexical and syntactic features, reminiscent of those of authorship attribution, as well as features related to story structure. A Winnow-based learning algorithm trained on these features convincingly predicted an appropriate emotion for individual sentences of narrative text. Strapparava and Mihalcea try to predict the emotion the author of a headline intends to provoke by leveraging words with known affective sense and by expanding those words&apos; synonyms. They used a Naive Bayes classifier trained on short blogposts of known emotive sense. The</context>
</contexts>
<marker>Strapparava, Mihalcea, 2008</marker>
<rawString>Carol Strapparava and Rada Mihalcea. 2008. Learning to identify emotions in text. SAC 2008: 1556-1560</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu kumura</author>
</authors>
<title>Semantic *rientations of Words using Spin Model. Annual Meeting of the Association for Computational Linguistics.</title>
<date>2005</date>
<marker>Takamura, Inui, kumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu *kumura. 2005. Semantic *rientations of Words using Spin Model. Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kauffman.</publisher>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kauffman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>Department of Computer Sciences, University of Wisconsin, Madison.</institution>
<contexts>
<context position="31790" citStr="Zhu, 2005" startWordPosition="5072" endWordPosition="5073">, with the features in line (5). We report the scores for cross-validation on the training set for these features; however, because the features were selected with knowledge of their per-class distribution in the training set, these cross-validation scores should not be seen as the classifier&apos;s true accuracy. Self-Training: Besides sparse feature vectors, another factor likely to be hurting our classifier was the limited amount of training data. We attempted to increase the training set size by performing exploratory experiments with selftraining, an iterative semi-supervised learning method (Zhu, 2005) with the feature set from (7). *n the first iteration, we trained the classifier on the labeled training set, classified the instances of the unlabeled test set, and then added the instances of the test set along with their predicted class to the training set to be used for the next iteration. After three iterations, the accuracy of the classifier when evaluated on the weighted test set improved to 82%, suggesting that our classifiers would benefit from more data. Impact of Cost-Sensitive Learning: Without cost-sensitive learning, the classifiers were heavily biased towards UpSpeak, tending t</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Xiaojin Zhu. 2005. Semi-supervised learning literature survey. Technical Report 1530, Department of Computer Sciences, University of Wisconsin, Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>