<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019903">
<title confidence="0.9979435">
IXAGroupEHUDiac: A Multiple Approach System towards the Diachronic
Evaluation of Texts
</title>
<author confidence="0.997104">
Haritz Salaberri†, Iker Salaberri‡, Olatz Arregi†, Befiat Zapirain††IXA Group - Faculty of Computer Sciences, ‡Faculty of Arts
</author>
<affiliation confidence="0.994385">
University of the Basque Country, Spain
</affiliation>
<email confidence="0.988478">
firstname.lastname@ehu.eus
</email>
<sectionHeader confidence="0.99552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999082">
This paper presents our contribution to the
SemEval-2015 Task 7. The task was subdi-
vided into three subtasks that consisted of au-
tomatically identifying the time period when
a piece of news was written (1,2) as well as
automatically determining whether a specific
phrase in a sentence is relevant or not for a
given period of time (3). Our system tackles
the resolution of all three subtasks. With this
purpose in mind multiple approaches are un-
dertaken that use resources such as Wikipedia
or Google NGrams. Final results are obtained
by combining the output from all approaches.
The texts used for the task are written in En-
glish and range from the years 1700 to 2000.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993525">
According to Mihalcea and Nastase (2012) cur-
rent applications within human language technol-
ogy work with languages as if they were constant.
However, changes in language are taking place con-
stantly, for example: new meanings for old words
are coined; metaphoric and metonymic uses become
so ingrained that they are considered literal from one
specific point in time on; new words are constantly
being created.
These changes in language are what in part has
motivated the task addressed by our system. In fact,
subtasks (1) and (2) tackle the problem of computa-
tionally identifying the time period in which a piece
of news was written. This is undertaken based on,
among other things, the changes that take place in
language over time. The difference between sub-
tasks (1) and (2) is that the texts in subtask (1) con-
tain clear references to time anchors. This means
that e.g. historical events, relevant people, commer-
cial products etc. are mentioned in the text that are
specific to the period of time in which the texts were
written. Subtask (3), on the other hand, consists of
determining whether a phrase within a clause is spe-
cific or not to the period of time in which the text
was written. The training corpus for this subtask is
made up of the texts from other subtasks. As a con-
sequence our system will be able to use information
on both time anchors and language changes in order
to generate the results for subtask (3).
This paper is organized as follows: section 2
presents the resources available to the diachronic
evaluation of texts; section 3, on the other hand,
shortly reviews the relevant literature on this mat-
ter. Section 4 makes a description of the developed
system; results are then described in section 5 and,
finally, our conclusions are given in section 6.
</bodyText>
<sectionHeader confidence="0.996121" genericHeader="introduction">
2 Resources
</sectionHeader>
<bodyText confidence="0.999977363636364">
To the extent of our knowledge, there exist two main
resources as of today for computationally address-
ing the diachronic evaluation of texts as defined in
task 7: Google NGrams and Wikipedia. The for-
mer holds statistics on word usage on Google Books,
a textual corpus consisting of books written in En-
glish and printed between 1505 and 2008. Google
NGrams can be used to map language changes to
specific time periods. The latter requires no presen-
tation as it is a well-known resource; it can be used
to establish the period a time anchor belongs to.
</bodyText>
<page confidence="0.970279">
840
</page>
<note confidence="0.6064415">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 840–845,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999729" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.997718595744681">
To the best of our knowledge several techniques
have been previously used to computationally ad-
dress language-change. We consider it important
to note that the motivation to study the language-
change phenomena differs from one work to an-
other: Some of the techniques make use of it in or-
der to establish the period of time in which a text
was produced (Jong et al., 2005; Dalli and Wilks,
2006), which is our main concern; others, on the
other hand, use the phenomena in order to study top-
ics such as the changes that have taken place in cul-
ture (Juola, 2013; Michel et al., 2011).
Some of the techniques used so far to address the
task of temporal classification are based on language
models built from texts belonging to a same period
of time. This way the task of temporally classifying
texts consists basically of identifying the model that
best fits the text that wants to be classified. Some
of the systems that follow this approach are Kumar
(2011) and Wang et al. (2012).
Another relevant class of models for temporal
classification is based on the idea that the change of
word meaning and word usage over time can help
determine the period of time in which a text was
written. Normally the resource used by the systems
that are based on this approach is Google NGrams
(see section 2). Some example models that use
this approach are presented in Mihalcea and Nastase
(2012) and Popescu and Strapparava (2013).
Other systems that can be brought up in this
section make use of stylistic and readability fea-
tures (Štajner and Zampieri, 2013), neural nets (Kim
et al., 2014) and lexical features (Dalli and Wilks,
2006).
From the approaches here presented we decided
to implement our system using, among others, the
change of word usage and word meaning over time
approach (see subsection 4.1.3) and the lexical and
stylistic features approach (see subsection 4.1.4) as
we believe both to have reported good performance
in previous works (Mihalcea and Nastase, 2012; Šta-
jner and Zampieri, 2013; Dalli and Wilks, 2006).
Although we think that the approach to epoch de-
limitation based on using language models can also
come up with good results, we have not used it as
we believe that the training set is too limited for this
approach to be effective.
</bodyText>
<sectionHeader confidence="0.977838" genericHeader="method">
4 System Description
</sectionHeader>
<bodyText confidence="0.9997845">
The way in which our system deals with temporal
text classification (subtasks (1) and (2)) is described
under subsection 4.1. The way in which our system
deals with recognizing time-specific phrases (sub-
task (3)), on the other hand, is presented under sub-
section 4.2.
</bodyText>
<subsectionHeader confidence="0.998692">
4.1 Temporal Text Classification
</subsectionHeader>
<bodyText confidence="0.999995083333333">
Four different approaches are undertaken in order
to automatically determine the period of time in
which a piece of news was written: the first ap-
proach consists of searching for the mentioned time
period within the text. The second approach, on the
other hand, consists of searching for named entities
present in the text and then establishing the period
of time by linking these to Wikipedia. The third ap-
proach uses Google NGrams and, to conclude, the
fourth approach consists of using linguistic features
that are significant with respect to language change
in combination with machine learning.
</bodyText>
<subsubsectionHeader confidence="0.556569">
4.1.1 Year Entity Detection
</subsubsectionHeader>
<bodyText confidence="0.999993882352941">
The present approach was implemented based on
the observation made upon the training texts, in the
development of which we have realized that the pe-
riod of time that corresponds to a text is present
within the text. This approach is characterized by
a very high precision and a very low recall as only
10% of the training texts contain a period of time
and in 85% of the cases these are the ones that cor-
respond to texts. In order to establish the time pe-
riod, year entities are detected by our system using
the Apache OpenNLP name finder tool Baldridge
(2005).
It is considered here that this approach is strongly
dependent on the domain; in fact, if historical texts
(or texts that in general describe past events) were
to be diachronically evaluated, the precision would
drop and recall would improve considerably.
</bodyText>
<subsectionHeader confidence="0.526871">
4.1.2 Wikipedia Entity Linking
</subsectionHeader>
<bodyText confidence="0.99987925">
For the second approach our system detects
named entities that correspond to persons and or-
ganizations within the texts; the Apache OpenNLP
name finder tool and the pre-trained models for this
</bodyText>
<page confidence="0.988563">
841
</page>
<bodyText confidence="0.991123090909091">
type of entities are used. After named entities are
recognized, these are searched for in Wikipedia; if a
named entity can be found, then year entities are de-
tected in the corresponding entry: with this purpose
in mind the OpenNLP name finder tool is used and
tuned as in 4.1.1. Finally, an average of all years
(which stem from the Wikipedia entries that corre-
spond to the named entities in the text) is calculated
for every text and the time period that corresponds
to the average assigned. The workflow for this ap-
proach can be seen in figure 1:
</bodyText>
<figureCaption confidence="0.999353">
Figure 1: Wikipedia Entity Linking.
</figureCaption>
<bodyText confidence="0.999266">
Different scenarios are possible concerning this
approach: some texts do not contain named enti-
ties and some others have many of them, and some-
times entities are not detected or can not be found
in Wikipedia. For these reasons not all texts are as-
signed a time period by this approach1.
</bodyText>
<subsectionHeader confidence="0.718935">
4.1.3 Google NGrams
</subsectionHeader>
<bodyText confidence="0.998150666666667">
The Google NGrams 1-gram corpus is used for
the third approach. We consider all nouns (proper
and common) within the texts to be of interest as we
consider these to be the kind of words that change
most across time and as a result provide the high-
est amount of information on the time in which a
piece of news was written. In order to identify these
nouns the ClearNLP PoS tagger and lemmatizer is
used Choi and Palmer (2012). The system computes
for each noun the percentage of occurrences that that
1Our approach does not handle cases where more than one
Wikipedia pages match a name.
noun has in a year with respect to the sum of words
available for that year (normalization). The amount
of published data in Google Books is not the same
for all years; in fact, it grows exponentially from the
second half of the 20th century on. For this rea-
son the percentage of occurrences with respect to
the sum of words needs to be calculated, rather than
simply using the amount of occurrences.
When percentages for all nouns in a text are cal-
culated, the year that corresponds to the highest per-
centage is associated to each noun. Then, the aver-
age value for these years is calculated. If the year as-
sociated to a noun differs in 40 or more years from
the average value, our system considers this noun
to be period-specific. Consequently, the time period
that includes this year is assigned to the text. Period-
specific nouns are determined locally within a given
text since the same noun might be period-specific in
one text but not in another.
If there is more than one noun that is considered
to be period-specific, the average value of the years
that correspond to these nouns is used. If there are
no detected period-specific nouns, on the other hand,
the average value calculated for all nouns is used.
</bodyText>
<subsectionHeader confidence="0.800253">
4.1.4 Language Change
</subsectionHeader>
<bodyText confidence="0.997854727272727">
The fourth approach used by our system consists
of using linguistic features (patterns or tendencies)
that are significant regarding language change in
combination with machine learning. For this pur-
pose the different diachronic or language-change
tendencies that are observable in the training data
have been studied. These tendencies include both
linguistic and extra-linguistic factors, and they af-
fect different areas of grammar such as orthography,
lexicon, semantics, morphology or syntax. Some ex-
amples can be seen in figure 2.
The patterns resulting from the study are classi-
fied into six different fifty-year periods ranging from
the years 1700 to 2000 as we consider these to be
the finest grain period patterns can be classified into.
Said patterns are used as features for the learning
algorithm; some examples include: the loss of sub-
junctive mood in subordinate clauses, the arisal of
do so-verbal substitution and the extinction of post-
positions and of various inflectional morphemes. In
spite of the richness of extracted linguistic change
patterns, this approach has proved in any case to be
</bodyText>
<page confidence="0.996909">
842
</page>
<figureCaption confidence="0.999841">
Figure 2: Some of the language-change patterns used by our system.
</figureCaption>
<bodyText confidence="0.998586888888889">
much less effective when compared to the other ap-
proaches.
The classifier used by the approach here described
is a standard multi-class Support Vector Machine
classifier implemented using the SVM-multiclass
package in Joachims (1999). The decision of using
a standard SVM learning algorithm comes from our
experience on classification tasks with such a large
number of classes.
</bodyText>
<subsectionHeader confidence="0.458885">
4.1.5 Final Decision
</subsectionHeader>
<bodyText confidence="0.999968466666667">
In order to ultimately determine the period of time
in which a text was written the system follows a pro-
cedure that takes into account the precision given by
each approach (since the systems seeks maximum
precision). We consider the year entity detection ap-
proach to be the one with the highest precision, fol-
lowed by the Wikipedia entity linking, the Google
NGrams and the language-change approaches. The
present procedure establishes that the period of time
yielded by the approach with the maximum preci-
sion that is available must be set to the text. It must
be kept in mind that both the year entity detection
and the Wikipedia entity linking approaches have a
low recall as only some of the texts are assigned a
period of time by these approaches.
</bodyText>
<page confidence="0.997085">
843
</page>
<table confidence="0.999082333333333">
Grain Coarse Medium Fine
Subtask
Precision Score Precision Score Precision Score
1 0.0902 0.5575 0.0413 0.3672 0.0225 0.187
2 0.0987 0.6225 0.0677 0.428 0.0377 0.2618
3 0.5739
</table>
<tableCaption confidence="0.999016">
Table 1: Official results reported for our system for all three subtasks.
</tableCaption>
<subsectionHeader confidence="0.999942">
4.2 Recognizing Time-Specific Phrases
</subsectionHeader>
<bodyText confidence="0.999963913043478">
We consider that determining whether the phrases
within a sentence are particularly relevant or not for
the period of time in which the sentence was written
can be viewed as a two-step procedure: first, mark-
able phrases need to be detected, and then it must
be decided whether these phrases are indicative fea-
tures for the period of time or not. Our system per-
forms just the classification step since the markable
phrases are provided by the task organizers. This is
achieved by making use of the period-specific words
identified in the Google NGrams approach described
in 4.1.3. Our system marks the set of consecutive
words that start and end with period-specific words
as a relevant phrase for the period of time in which
the text was written. This procedure is followed if
there is no punctuation mark between the words and
the distance is not greater than four words.
The decision to consider phrases that have a max-
imum of four words is based upon observation. We
consider this to be the appropriate number of words
in order not to miss too many relevant phrases. The
system can be easily tuned for phrases with a greater
or a smaller number of words.
</bodyText>
<sectionHeader confidence="0.999821" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999935340909091">
Table 1 contains the official results reported for our
system. In order to evaluate subtasks (1) and (2)
three configurations are considered: a fine-graded
evaluation were periods of time span two years in
subtask (1) and six years in subtask (2); a medium-
graded evaluation were periods of time span six
years for subtask (1) and twelve years for subtask (2)
and a coarse-graded evaluation were periods of time
span twelve years in subtask (1) and twenty years in
subtask (2).
There is no fine-, medium- or coarse-graded eval-
uation for subtask (3). Certain phrases from a piece
of news are selected by the task organizers and
marked as yes or no by our system according to their
relevance for the period of time when the news was
produced (the period of time is also provided by the
organizers). The score for this subtask is computed
by counting the number of times our system has cor-
rectly marked the phrases.
As far as we know the only works that bear a
slight resemblance to what is proposed in the tem-
poral text classification subtasks (subtasks (1) and
(2)) are Mihalcea and Nastase (2012) and Popescu
and Strapparava (2013), in which computational ap-
proaches to temporal classification of words are pre-
sented. We consider that our results cannot be even
loosely compared to the results in the cited papers as
there is too little resemblance between temporal text
classification and temporal word classification. We
are not aware of any work that performs recognition
of time-specific phrases (subtasks (3)).
As can be observed in table 1, the scores for sub-
task (2) are higher than the scores reported for sub-
task (1); however, we find that establishing the pe-
riod of time when a piece of news was written is
more complicated for the texts in subtask (1) as it
mainly depends on a correct exploitation of time an-
chors. For this reason, we understand that the per-
formance of our system is higher in subtask (1) than
in subtask (2). Finally, we believe that the score ob-
tained for the third subtask (0.5739) can be under-
stood as an indicator of high performance as the dif-
ficulty of the subtask is, in our opinion, higher than
that of other subtasks.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="conclusions">
6 Conclusions and Future Works
</sectionHeader>
<bodyText confidence="0.9980475">
In this paper we have presented our system for the
diachronic evaluation of English texts, which has
taken part in the SemEval-2015 task 7. Our sys-
tem has been the only participant system that has
reported results for the three subtasks that compre-
hended the task. We believe that many issues still
</bodyText>
<page confidence="0.992863">
844
</page>
<bodyText confidence="0.9676084">
need to be reviewed.
We intend to improve the overall performance of
the system in the near future by trying out new tech-
niques that we have not been able to implement due
to time limitations.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991988">
Haritz Salaberri holds a PhD grant from the Univer-
sity of the Basque Country (UPV/EHU)(IXA Group,
Research Group of type A (2010-2015)(IT34410)).
In addition, this work has been supported by the FP7
NewsReader project (Grant No. 316404).
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982410466666667">
Jason Baldridge. 2005. The opennlp project. URL:
http://opennlp. apache. org/index. html,(accessed
2 February 2012).
Jinho D. Choi and Martha Palmer. 2012. Fast
and robust part-of-speech tagging using dynamic
model selection. In Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics: Short Papers-Volume 2, pages
363–367. Association for Computational Linguis-
tics.
Angelo Dalli and Yorick Wilks. 2006. Automatic
dating of documents and temporal text classifi-
cation. In Proceedings of the Workshop on An-
notating and Reasoning about Time and Events,
pages 17–22. Association for Computational Lin-
guistics.
Thorsten Joachims. 1999. Making large scale SVM
learning practical. Universität Dortmund.
F. de Jong, Henning Rode, Djoerd Hiemstra. 2005.
Temporal language models for the disclosure of
historical text. Royal Netherlands Academy of
Arts and Sciences.
Patrick Juola. 2013. Using the Google N-Gram cor-
pus to measure cultural complexity. In Literary
and linguistic computing 28(4), pages 668–675.
Association for Computational Linguistics.
Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan
Hegde, Slav Petrov. 2014. Temporal analysis of
language through neural language models. arXiv
preprint arXiv:1405.3515.
Abhimanu Kumar, Matthew Lease, Jason Baldridge.
2011. Supervised language modeling for tem-
poral resolution of texts. In Proceedings of the
20th ACM international conference on Informa-
tion and knowledge management, pages 2069–
2072. Association for Computational Linguistics.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, Joseph P.
Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig,
Jon Orwant and others. 2011. Quantitative anal-
ysis of culture using millions of digitized books.
Science, 331(6014):176–182.
Rada Mihalcea and Vivi Nastase. 2012. Word epoch
disambiguation: Finding how words change over
time. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2, pages 259–263. Associa-
tion for Computational Linguistics.
Octavian Popescu and Carlo Strapparava. 2013. Be-
hind the times: Detecting epoch changes using
large corpora. In Proc. of IJCNLP.
Octavian Popescu and Carlo Strapparava. 2014.
Time corpora: Epochs, opinions and changes.
Knowledge-Based Systems.
Sanja Štajner and Marcos Zampieri. 2013. Stylis-
tic changes for temporal text classification. Text,
Speech, and Dialogue, pages 519–526. Springer.
Chong Wang, David Blei, David Heckerman. 2012.
Continuous time dynamic topic models. arXiv
preprint arXiv:1206.3298.
</reference>
<page confidence="0.998848">
845
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364766">
<title confidence="0.9545315">IXAGroupEHUDiac: A Multiple Approach System towards the Diachronic Evaluation of Texts</title>
<author confidence="0.61391">Faculty of Computer Sciences</author>
<author confidence="0.61391">of</author>
<affiliation confidence="0.993753">University of the Basque Country,</affiliation>
<email confidence="0.98051">firstname.lastname@ehu.eus</email>
<abstract confidence="0.999426">This paper presents our contribution to the SemEval-2015 Task 7. The task was subdivided into three subtasks that consisted of automatically identifying the time period when a piece of news was written (1,2) as well as automatically determining whether a specific phrase in a sentence is relevant or not for a given period of time (3). Our system tackles the resolution of all three subtasks. With this purpose in mind multiple approaches are undertaken that use resources such as Wikipedia or Google NGrams. Final results are obtained by combining the output from all approaches.</abstract>
<note confidence="0.7256615">The texts used for the task are written in English and range from the years 1700 to 2000.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
</authors>
<title>The opennlp project.</title>
<date>2005</date>
<note>URL: http://opennlp. apache. org/index. html,(accessed 2</note>
<contexts>
<context position="7287" citStr="Baldridge (2005)" startWordPosition="1231" endWordPosition="1232">tion with machine learning. 4.1.1 Year Entity Detection The present approach was implemented based on the observation made upon the training texts, in the development of which we have realized that the period of time that corresponds to a text is present within the text. This approach is characterized by a very high precision and a very low recall as only 10% of the training texts contain a period of time and in 85% of the cases these are the ones that correspond to texts. In order to establish the time period, year entities are detected by our system using the Apache OpenNLP name finder tool Baldridge (2005). It is considered here that this approach is strongly dependent on the domain; in fact, if historical texts (or texts that in general describe past events) were to be diachronically evaluated, the precision would drop and recall would improve considerably. 4.1.2 Wikipedia Entity Linking For the second approach our system detects named entities that correspond to persons and organizations within the texts; the Apache OpenNLP name finder tool and the pre-trained models for this 841 type of entities are used. After named entities are recognized, these are searched for in Wikipedia; if a named en</context>
</contexts>
<marker>Baldridge, 2005</marker>
<rawString>Jason Baldridge. 2005. The opennlp project. URL: http://opennlp. apache. org/index. html,(accessed 2 February 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Fast and robust part-of-speech tagging using dynamic model selection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>363--367</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9090" citStr="Choi and Palmer (2012)" startWordPosition="1543" endWordPosition="1546">thers have many of them, and sometimes entities are not detected or can not be found in Wikipedia. For these reasons not all texts are assigned a time period by this approach1. 4.1.3 Google NGrams The Google NGrams 1-gram corpus is used for the third approach. We consider all nouns (proper and common) within the texts to be of interest as we consider these to be the kind of words that change most across time and as a result provide the highest amount of information on the time in which a piece of news was written. In order to identify these nouns the ClearNLP PoS tagger and lemmatizer is used Choi and Palmer (2012). The system computes for each noun the percentage of occurrences that that 1Our approach does not handle cases where more than one Wikipedia pages match a name. noun has in a year with respect to the sum of words available for that year (normalization). The amount of published data in Google Books is not the same for all years; in fact, it grows exponentially from the second half of the 20th century on. For this reason the percentage of occurrences with respect to the sum of words needs to be calculated, rather than simply using the amount of occurrences. When percentages for all nouns in a t</context>
</contexts>
<marker>Choi, Palmer, 2012</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2012. Fast and robust part-of-speech tagging using dynamic model selection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 363–367. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelo Dalli</author>
<author>Yorick Wilks</author>
</authors>
<title>Automatic dating of documents and temporal text classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Annotating and Reasoning about Time and Events,</booktitle>
<pages>17--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3917" citStr="Dalli and Wilks, 2006" startWordPosition="653" endWordPosition="656"> a time anchor belongs to. 840 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 840–845, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 Related Work To the best of our knowledge several techniques have been previously used to computationally address language-change. We consider it important to note that the motivation to study the languagechange phenomena differs from one work to another: Some of the techniques make use of it in order to establish the period of time in which a text was produced (Jong et al., 2005; Dalli and Wilks, 2006), which is our main concern; others, on the other hand, use the phenomena in order to study topics such as the changes that have taken place in culture (Juola, 2013; Michel et al., 2011). Some of the techniques used so far to address the task of temporal classification are based on language models built from texts belonging to a same period of time. This way the task of temporally classifying texts consists basically of identifying the model that best fits the text that wants to be classified. Some of the systems that follow this approach are Kumar (2011) and Wang et al. (2012). Another releva</context>
<context position="5146" citStr="Dalli and Wilks, 2006" startWordPosition="868" endWordPosition="871">s of models for temporal classification is based on the idea that the change of word meaning and word usage over time can help determine the period of time in which a text was written. Normally the resource used by the systems that are based on this approach is Google NGrams (see section 2). Some example models that use this approach are presented in Mihalcea and Nastase (2012) and Popescu and Strapparava (2013). Other systems that can be brought up in this section make use of stylistic and readability features (Štajner and Zampieri, 2013), neural nets (Kim et al., 2014) and lexical features (Dalli and Wilks, 2006). From the approaches here presented we decided to implement our system using, among others, the change of word usage and word meaning over time approach (see subsection 4.1.3) and the lexical and stylistic features approach (see subsection 4.1.4) as we believe both to have reported good performance in previous works (Mihalcea and Nastase, 2012; Štajner and Zampieri, 2013; Dalli and Wilks, 2006). Although we think that the approach to epoch delimitation based on using language models can also come up with good results, we have not used it as we believe that the training set is too limited for </context>
</contexts>
<marker>Dalli, Wilks, 2006</marker>
<rawString>Angelo Dalli and Yorick Wilks. 2006. Automatic dating of documents and temporal text classification. In Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 17–22. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large scale SVM learning practical.</title>
<date>1999</date>
<institution>Universität Dortmund.</institution>
<contexts>
<context position="11902" citStr="Joachims (1999)" startWordPosition="2011" endWordPosition="2012">algorithm; some examples include: the loss of subjunctive mood in subordinate clauses, the arisal of do so-verbal substitution and the extinction of postpositions and of various inflectional morphemes. In spite of the richness of extracted linguistic change patterns, this approach has proved in any case to be 842 Figure 2: Some of the language-change patterns used by our system. much less effective when compared to the other approaches. The classifier used by the approach here described is a standard multi-class Support Vector Machine classifier implemented using the SVM-multiclass package in Joachims (1999). The decision of using a standard SVM learning algorithm comes from our experience on classification tasks with such a large number of classes. 4.1.5 Final Decision In order to ultimately determine the period of time in which a text was written the system follows a procedure that takes into account the precision given by each approach (since the systems seeks maximum precision). We consider the year entity detection approach to be the one with the highest precision, followed by the Wikipedia entity linking, the Google NGrams and the language-change approaches. The present procedure establishe</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large scale SVM learning practical. Universität Dortmund.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F de Jong</author>
<author>Henning Rode</author>
<author>Djoerd Hiemstra</author>
</authors>
<title>Temporal language models for the disclosure of historical text. Royal Netherlands Academy of Arts and Sciences.</title>
<date>2005</date>
<marker>de Jong, Rode, Hiemstra, 2005</marker>
<rawString>F. de Jong, Henning Rode, Djoerd Hiemstra. 2005. Temporal language models for the disclosure of historical text. Royal Netherlands Academy of Arts and Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Juola</author>
</authors>
<title>Using the Google N-Gram corpus to measure cultural complexity.</title>
<date>2013</date>
<booktitle>In Literary and linguistic computing 28(4),</booktitle>
<pages>668--675</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4081" citStr="Juola, 2013" startWordPosition="687" endWordPosition="688">ociation for Computational Linguistics 3 Related Work To the best of our knowledge several techniques have been previously used to computationally address language-change. We consider it important to note that the motivation to study the languagechange phenomena differs from one work to another: Some of the techniques make use of it in order to establish the period of time in which a text was produced (Jong et al., 2005; Dalli and Wilks, 2006), which is our main concern; others, on the other hand, use the phenomena in order to study topics such as the changes that have taken place in culture (Juola, 2013; Michel et al., 2011). Some of the techniques used so far to address the task of temporal classification are based on language models built from texts belonging to a same period of time. This way the task of temporally classifying texts consists basically of identifying the model that best fits the text that wants to be classified. Some of the systems that follow this approach are Kumar (2011) and Wang et al. (2012). Another relevant class of models for temporal classification is based on the idea that the change of word meaning and word usage over time can help determine the period of time i</context>
</contexts>
<marker>Juola, 2013</marker>
<rawString>Patrick Juola. 2013. Using the Google N-Gram corpus to measure cultural complexity. In Literary and linguistic computing 28(4), pages 668–675. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, Slav Petrov.</title>
<date>2014</date>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, Slav Petrov. 2014. Temporal analysis of language through neural language models. arXiv preprint arXiv:1405.3515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhimanu Kumar</author>
<author>Matthew Lease</author>
<author>Jason Baldridge</author>
</authors>
<title>Supervised language modeling for temporal resolution of texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management,</booktitle>
<pages>pages</pages>
<marker>Kumar, Lease, Baldridge, 2011</marker>
<rawString>Abhimanu Kumar, Matthew Lease, Jason Baldridge. 2011. Supervised language modeling for temporal resolution of texts. In Proceedings of the 20th ACM international conference on Information and knowledge management, pages 2069– 2072. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Michel</author>
<author>Yuan Kui Shen</author>
<author>Aviva Presser Aiden</author>
<author>Adrian Veres</author>
<author>Matthew K Gray</author>
<author>Joseph P Pickett</author>
</authors>
<title>Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant and others.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>331</volume>
<issue>6014</issue>
<contexts>
<context position="4103" citStr="Michel et al., 2011" startWordPosition="689" endWordPosition="692">Computational Linguistics 3 Related Work To the best of our knowledge several techniques have been previously used to computationally address language-change. We consider it important to note that the motivation to study the languagechange phenomena differs from one work to another: Some of the techniques make use of it in order to establish the period of time in which a text was produced (Jong et al., 2005; Dalli and Wilks, 2006), which is our main concern; others, on the other hand, use the phenomena in order to study topics such as the changes that have taken place in culture (Juola, 2013; Michel et al., 2011). Some of the techniques used so far to address the task of temporal classification are based on language models built from texts belonging to a same period of time. This way the task of temporally classifying texts consists basically of identifying the model that best fits the text that wants to be classified. Some of the systems that follow this approach are Kumar (2011) and Wang et al. (2012). Another relevant class of models for temporal classification is based on the idea that the change of word meaning and word usage over time can help determine the period of time in which a text was wri</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, Pickett, 2011</marker>
<rawString>Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant and others. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Vivi Nastase</author>
</authors>
<title>Word epoch disambiguation: Finding how words change over time.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>259--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1016" citStr="Mihalcea and Nastase (2012)" startWordPosition="156" endWordPosition="159"> subtasks that consisted of automatically identifying the time period when a piece of news was written (1,2) as well as automatically determining whether a specific phrase in a sentence is relevant or not for a given period of time (3). Our system tackles the resolution of all three subtasks. With this purpose in mind multiple approaches are undertaken that use resources such as Wikipedia or Google NGrams. Final results are obtained by combining the output from all approaches. The texts used for the task are written in English and range from the years 1700 to 2000. 1 Introduction According to Mihalcea and Nastase (2012) current applications within human language technology work with languages as if they were constant. However, changes in language are taking place constantly, for example: new meanings for old words are coined; metaphoric and metonymic uses become so ingrained that they are considered literal from one specific point in time on; new words are constantly being created. These changes in language are what in part has motivated the task addressed by our system. In fact, subtasks (1) and (2) tackle the problem of computationally identifying the time period in which a piece of news was written. This </context>
<context position="4904" citStr="Mihalcea and Nastase (2012)" startWordPosition="828" endWordPosition="831"> the task of temporally classifying texts consists basically of identifying the model that best fits the text that wants to be classified. Some of the systems that follow this approach are Kumar (2011) and Wang et al. (2012). Another relevant class of models for temporal classification is based on the idea that the change of word meaning and word usage over time can help determine the period of time in which a text was written. Normally the resource used by the systems that are based on this approach is Google NGrams (see section 2). Some example models that use this approach are presented in Mihalcea and Nastase (2012) and Popescu and Strapparava (2013). Other systems that can be brought up in this section make use of stylistic and readability features (Štajner and Zampieri, 2013), neural nets (Kim et al., 2014) and lexical features (Dalli and Wilks, 2006). From the approaches here presented we decided to implement our system using, among others, the change of word usage and word meaning over time approach (see subsection 4.1.3) and the lexical and stylistic features approach (see subsection 4.1.4) as we believe both to have reported good performance in previous works (Mihalcea and Nastase, 2012; Štajner an</context>
<context position="15376" citStr="Mihalcea and Nastase (2012)" startWordPosition="2608" endWordPosition="2611">ask (2). There is no fine-, medium- or coarse-graded evaluation for subtask (3). Certain phrases from a piece of news are selected by the task organizers and marked as yes or no by our system according to their relevance for the period of time when the news was produced (the period of time is also provided by the organizers). The score for this subtask is computed by counting the number of times our system has correctly marked the phrases. As far as we know the only works that bear a slight resemblance to what is proposed in the temporal text classification subtasks (subtasks (1) and (2)) are Mihalcea and Nastase (2012) and Popescu and Strapparava (2013), in which computational approaches to temporal classification of words are presented. We consider that our results cannot be even loosely compared to the results in the cited papers as there is too little resemblance between temporal text classification and temporal word classification. We are not aware of any work that performs recognition of time-specific phrases (subtasks (3)). As can be observed in table 1, the scores for subtask (2) are higher than the scores reported for subtask (1); however, we find that establishing the period of time when a piece of</context>
</contexts>
<marker>Mihalcea, Nastase, 2012</marker>
<rawString>Rada Mihalcea and Vivi Nastase. 2012. Word epoch disambiguation: Finding how words change over time. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 259–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
<author>Carlo Strapparava</author>
</authors>
<title>Behind the times: Detecting epoch changes using large corpora.</title>
<date>2013</date>
<booktitle>In Proc. of IJCNLP.</booktitle>
<contexts>
<context position="4939" citStr="Popescu and Strapparava (2013)" startWordPosition="833" endWordPosition="836">ing texts consists basically of identifying the model that best fits the text that wants to be classified. Some of the systems that follow this approach are Kumar (2011) and Wang et al. (2012). Another relevant class of models for temporal classification is based on the idea that the change of word meaning and word usage over time can help determine the period of time in which a text was written. Normally the resource used by the systems that are based on this approach is Google NGrams (see section 2). Some example models that use this approach are presented in Mihalcea and Nastase (2012) and Popescu and Strapparava (2013). Other systems that can be brought up in this section make use of stylistic and readability features (Štajner and Zampieri, 2013), neural nets (Kim et al., 2014) and lexical features (Dalli and Wilks, 2006). From the approaches here presented we decided to implement our system using, among others, the change of word usage and word meaning over time approach (see subsection 4.1.3) and the lexical and stylistic features approach (see subsection 4.1.4) as we believe both to have reported good performance in previous works (Mihalcea and Nastase, 2012; Štajner and Zampieri, 2013; Dalli and Wilks, </context>
<context position="15411" citStr="Popescu and Strapparava (2013)" startWordPosition="2613" endWordPosition="2616">um- or coarse-graded evaluation for subtask (3). Certain phrases from a piece of news are selected by the task organizers and marked as yes or no by our system according to their relevance for the period of time when the news was produced (the period of time is also provided by the organizers). The score for this subtask is computed by counting the number of times our system has correctly marked the phrases. As far as we know the only works that bear a slight resemblance to what is proposed in the temporal text classification subtasks (subtasks (1) and (2)) are Mihalcea and Nastase (2012) and Popescu and Strapparava (2013), in which computational approaches to temporal classification of words are presented. We consider that our results cannot be even loosely compared to the results in the cited papers as there is too little resemblance between temporal text classification and temporal word classification. We are not aware of any work that performs recognition of time-specific phrases (subtasks (3)). As can be observed in table 1, the scores for subtask (2) are higher than the scores reported for subtask (1); however, we find that establishing the period of time when a piece of news was written is more complicat</context>
</contexts>
<marker>Popescu, Strapparava, 2013</marker>
<rawString>Octavian Popescu and Carlo Strapparava. 2013. Behind the times: Detecting epoch changes using large corpora. In Proc. of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
<author>Carlo Strapparava</author>
</authors>
<title>Time corpora: Epochs, opinions and changes. Knowledge-Based Systems.</title>
<date>2014</date>
<marker>Popescu, Strapparava, 2014</marker>
<rawString>Octavian Popescu and Carlo Strapparava. 2014. Time corpora: Epochs, opinions and changes. Knowledge-Based Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanja Štajner</author>
<author>Marcos Zampieri</author>
</authors>
<title>Stylistic changes for temporal text classification. Text, Speech, and Dialogue,</title>
<date>2013</date>
<pages>519--526</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5069" citStr="Štajner and Zampieri, 2013" startWordPosition="855" endWordPosition="858">ollow this approach are Kumar (2011) and Wang et al. (2012). Another relevant class of models for temporal classification is based on the idea that the change of word meaning and word usage over time can help determine the period of time in which a text was written. Normally the resource used by the systems that are based on this approach is Google NGrams (see section 2). Some example models that use this approach are presented in Mihalcea and Nastase (2012) and Popescu and Strapparava (2013). Other systems that can be brought up in this section make use of stylistic and readability features (Štajner and Zampieri, 2013), neural nets (Kim et al., 2014) and lexical features (Dalli and Wilks, 2006). From the approaches here presented we decided to implement our system using, among others, the change of word usage and word meaning over time approach (see subsection 4.1.3) and the lexical and stylistic features approach (see subsection 4.1.4) as we believe both to have reported good performance in previous works (Mihalcea and Nastase, 2012; Štajner and Zampieri, 2013; Dalli and Wilks, 2006). Although we think that the approach to epoch delimitation based on using language models can also come up with good results</context>
</contexts>
<marker>Štajner, Zampieri, 2013</marker>
<rawString>Sanja Štajner and Marcos Zampieri. 2013. Stylistic changes for temporal text classification. Text, Speech, and Dialogue, pages 519–526. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>David Blei</author>
<author>David Heckerman</author>
</authors>
<title>Continuous time dynamic topic models. arXiv preprint arXiv:1206.3298.</title>
<date>2012</date>
<contexts>
<context position="4501" citStr="Wang et al. (2012)" startWordPosition="758" endWordPosition="761"> al., 2005; Dalli and Wilks, 2006), which is our main concern; others, on the other hand, use the phenomena in order to study topics such as the changes that have taken place in culture (Juola, 2013; Michel et al., 2011). Some of the techniques used so far to address the task of temporal classification are based on language models built from texts belonging to a same period of time. This way the task of temporally classifying texts consists basically of identifying the model that best fits the text that wants to be classified. Some of the systems that follow this approach are Kumar (2011) and Wang et al. (2012). Another relevant class of models for temporal classification is based on the idea that the change of word meaning and word usage over time can help determine the period of time in which a text was written. Normally the resource used by the systems that are based on this approach is Google NGrams (see section 2). Some example models that use this approach are presented in Mihalcea and Nastase (2012) and Popescu and Strapparava (2013). Other systems that can be brought up in this section make use of stylistic and readability features (Štajner and Zampieri, 2013), neural nets (Kim et al., 2014)</context>
</contexts>
<marker>Wang, Blei, Heckerman, 2012</marker>
<rawString>Chong Wang, David Blei, David Heckerman. 2012. Continuous time dynamic topic models. arXiv preprint arXiv:1206.3298.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>