<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000143">
<title confidence="0.998429">
A Generative Model for User Simulation in a Spatial Navigation Domain
</title>
<author confidence="0.999392">
Aciel Eshky&apos;, Ben Allison&apos;, Subramanian Ramamoorthy&apos;, and Mark Steedman&apos;
</author>
<affiliation confidence="0.9936565">
&apos;School of Informatics, University of Edinburgh, UK
&apos;Actual Analytics Ltd., Edinburgh, UK
</affiliation>
<email confidence="0.975074">
{a.eshky,s.ramamoorthy,steedman}@ed.ac.uk
ballison@actualanalytics.com
</email>
<sectionHeader confidence="0.993401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999">
We propose the use of a generative model
to simulate user behaviour in a novel task-
oriented dialog domain, where user goals
are spatial routes across artificial land-
scapes. We show how to derive an effi-
cient feature-based representation of spa-
tial goals, admitting exact inference and
generalising to new routes. The use of
a generative model allows us to capture
a range of plausible behaviour given the
same underlying goal. We evaluate intrin-
sically using held-out probability and per-
plexity, and find a substantial reduction in
uncertainty brought by our spatial repre-
sentation. We evaluate extrinsically in a
human judgement task and find that our
model’s behaviour does not differ signif-
icantly from the behaviour of real users.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999909170731707">
Automated dialog management is an area of re-
search that has undergone rapid advancement in
the last decade. The driving force of this innova-
tion has been the rise of the statistical paradigm
for monitoring dialog state, reasoning about the
effects of possible dialog moves, and planning fu-
ture actions (Young et al., 2013). Statistical di-
alog management treats conversations as Markov
Decision Processes, where dialog moves are as-
sociated with a utility, estimated online by inter-
acting with a simulated user (Levin et al., 1998;
Roy et al., 2000; Singh et al., 2002; Williams and
Young, 2007; Henderson and Lemon, 2008). Slot-
filling domains have been the subject of most of
this research, with the exception of work on trou-
bleshooting domains (Williams, 2007) and rela-
tional domains (Lison, 2013).
Although navigational dialogs have received
much attention in studies of human conversational
behaviour (Anderson et al., 1991; Thompson et
al., 1993; Reitter and Moore, 2007), they have not
been the subject of statistical dialog management
research, and existing systems addressing naviga-
tional domains remain largely hand crafted (Ja-
narthanam et al., 2013). Navigational domains
present an interesting challenge, due to the dispar-
ity between the spatial goals and their grounding
as utterances. This disparity renders much of the
statistical management literature inapplicable. In
this paper, we address this deficiency.
We focus on the task of simulating user be-
haviour, both because of the important role sim-
ulators plays in the induction of dialog managers,
and because it provides a self-contained means of
developing the domain representations which fa-
cilitate dialog reasoning. We show how a genera-
tive model of user behaviour can be induced from
data, alleviating the manual effort typically in-
volved in the development of simulators, and pro-
viding an elegant mechanism for reproducing the
natural variability observed in human behaviour.
</bodyText>
<subsectionHeader confidence="0.996211">
1.1 Spatial Goals of Users
</subsectionHeader>
<bodyText confidence="0.999897823529412">
Users in task-oriented domains are goal-directed,
with a persistent notion of what they wish to ac-
complish from the dialog. In slot-filling domains,
goals are comprised of a group of categorical en-
tities, represented as slot-value pairs. These en-
tities can be placed directly into the user’s utter-
ance. For example, in a flight booking domain, if
a user’s goal is to fly to London from New York
on the 3rd of November, then the goal takes the
form: {origin=“New York”, dest=“London”, de-
part date=“03-11-13”}, and expressing the desti-
nation takes the form: Provide dest=“London”.
In contrast, consider the task of navigating
somebody across a landscape. Figure 1 shows a
pair of maps taken from a spatial navigation do-
main, the Map Task. Because the Giver aims to
communicate their route, one can view the route
</bodyText>
<page confidence="0.982415">
626
</page>
<note confidence="0.9982952">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 626–635,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
Natural Language Semantic Representation
G: you are above the camera shop Instruct POSITION(ABOVE, LM)
F: yeah Acknowledge
G: go left jus– just to the side of the paper, Instruct MOVE(TO, PAGE LEFT) �
then south, Instruct MOVE(TOWARDS, ABSOLUTE SOUTH)
under the parked van o Instruct MOVE(UNDER, LM) o
you have a parked van? Query-yn
F: a parked van no Reply-n
G: you go– you just go west, Clarify MOVE(TOWARDS, ABSOLUTE WEST) �
and down, Clarify MOVE(TOWARDS, ABSOLUTE SOUTH)
and then you go along to the– you go east o Clarify MOVE(TOWARDS, ABSOLUTE EAST) o
F: south then east Check
G: yeah Reply-y
</note>
<tableCaption confidence="0.987804">
Table 1: A Giver (G) and a Follower (F) alternating turns in a dialog concerning the maps in Figure 1.
</tableCaption>
<bodyText confidence="0.997741066666667">
The utterances are shown in natural language (left), and the semantic equivalent (right), which is com-
posed of Dialog Acts and SEMANTIC UNITS. Utterances marked * demonstrate a plausible variability in
expressing the same part of the route on the Giver’s map, and similarly those marked o. We model the
Giver’s behaviour, conditioned on the Follower’s, at the semantic level.
as the Giver’s goal for the dialog. However, unlike
goals in slot-filling domains, it is unclear whether
the route can be represented categorically in a
form that would allow the giver to communicate
it by placing it directly into an utterance. As raw
data, a specific route is represented numerically as
a series of pixel coordinates. Before modelling in-
terlocutors in this domain, we must derive a mean-
ingful representation for the spatial goals, and then
devise a mechanism that takes us from the spatial
goals to the utterances which express them.
</bodyText>
<subsectionHeader confidence="0.995948">
1.2 Utterance Variability for the Same Goal
</subsectionHeader>
<bodyText confidence="0.999816928571429">
In addition to making sensible utterances, a con-
cern for user simulation is providing plausible
variability in utterances, to provide dialog man-
agers with realistic training scenarios. Consider
the dialog in Table 1, resulting from the maps in
Figure 1. Utterances marked * (and similarly those
marked o) illustrate how the same route can be
described in different ways, not only at the natu-
ral language level, but also at the semantic level1.
A model providing a 1-to-1 mapping from spatial
routes to semantic utterances would fail to capture
this phenomenon. Instead, we need to be able to
account for plausible variability in expressing the
underlying spatial route as semantic utterances.
</bodyText>
<footnote confidence="0.978549">
1Route descriptor TOWARDS indicates a movement in the
direction of the referent ABS WEST, whereas TO indicates a
movement until the referent is reached.
</footnote>
<figureCaption confidence="0.7892205">
Figure 1: In the Map Task, the instruction Giver’s
task is to communicate a route to a Follower,
</figureCaption>
<bodyText confidence="0.937322333333333">
whose map may differ. The route can be seen as
the Giver’s goal which the Follower tries to infer.
A corresponding dialog is shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.999461">
1.3 Overview of Approach
</subsectionHeader>
<bodyText confidence="0.9999136">
In order to perform efficient reasoning, we pro-
pose a new feature-based representation of spatial
goals, transforming them from coordinate space to
a low-dimensional feature space. This groups sim-
ilar routes together intelligently, permitting exact
inference, and generalising to new routes. To ad-
dress the problem of variability of utterances given
the same underlying route, we learn a distribution
over possible utterances given the feature vector
derived from a route, with probability proportional
to the plausibility of the utterance.
Because this domain has not been previously
addressed in the context of dialog management or
user simulation, there is no directly comparable
prior work. We thus conduct several novel evalu-
</bodyText>
<figure confidence="0.3431495">
Follower
Giver
</figure>
<page confidence="0.987692">
627
</page>
<bodyText confidence="0.999963181818182">
ations to validate our model. We first use intrinsic
information theoretic measures, which compute
the extent of the reduction in uncertainty brought
by our feature-based representation of the spatial
goals. We then evaluate extrinsically by gener-
ating utterances from our model, and comparing
them to held-out utterance of real humans in the
test data. We also utilise human judgements for
the task, where the judges score the output of the
different models and the human utterances based
on their suitability to a particular route.
</bodyText>
<sectionHeader confidence="0.99984" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.854367">
2.1 Related Work on the Map Task
</subsectionHeader>
<bodyText confidence="0.999982">
To our knowledge, there are no attempts to model
instruction Givers as users in the Map Task do-
main. Two studies model the Follower, in the con-
text of understanding natural language instructions
and interpreting them by drawing a route (Levit
and Roy, 2007; Vogel and Jurafsky, 2010). Both
studies exclude dialog from their modelling. Al-
though their work is not directly comparable to
ours, they provide a corpus suitable for our task.
</bodyText>
<subsectionHeader confidence="0.841434">
2.2 Related Work on User Simulation
</subsectionHeader>
<bodyText confidence="0.999153714285714">
Early user simulation techniques are based on N-
grams (Eckert et al., 1997; Levin and Pieraccini,
2000; Georgila et al., 2005; Georgila et al., 2006),
ensuring that simulator responses to a machine ut-
terance are sensible locally. However, they do not
enforce user consistency throughout the dialog.
Deterministic simulators with trainable parame-
ters mitigate the lack of consistency using rules in
conjunction with explicit goals or agendas (Schef-
fler and Young, 2002; Rieser and Lemon, 2006;
Pietquin, 2006; Ai and Litman, 2007; Schatzmann
and Young, 2009). However, they require large
amounts of hand crafting and restrict the variabil-
ity in user responses, which by extension restricts
the access of the dialog manager to potentially in-
teresting states. An alternative approach to dealing
with the lack of consistency is to extend N-grams
to explicitly model user goals and condition utter-
ances on them (Pietquin, 2004; Cuay´ahuitl et al.,
2005; Pietquin and Dutoit, 2006; Rossignol et al.,
2010; Rossignol et al., 2011; Eshky et al., 2012).
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="method">
3 The Model
</sectionHeader>
<bodyText confidence="0.884355">
Our task is to model the Giver’s utterances in re-
sponse to the Follower’s, at the semantic level. A
Giver’s utterance takes the form:
</bodyText>
<equation confidence="0.92822">
g = Instruct, u = MOVE (UNDER, LM)
</equation>
<bodyText confidence="0.999926166666667">
consisting of a dialog act g and a semantic unit u2.
Aligned with u, is an ordered set of waypoints W,
corresponding only to part of the route u describes.
Figure 2(a) shows an example of such a sub-route.
The point-set W can be seen as the Giver’s current
goal on which they base their behaviour. Because
the routes are drawn on the Giver’s maps, we treat
W as observed.
To model some of the interaction between the
Giver and the Follower, we additionally consider
in our model the previous dialog act of the Fol-
lower, which could for example be:
</bodyText>
<equation confidence="0.546023">
f = Acknowledge
</equation>
<bodyText confidence="0.999793">
Given point-set W and preceding Follower act
f, as the giver, we need to determine a procedure
for choosing which dialog act g and semantic unit
u to produce. In other words, we are interested in
the following distribution:
</bodyText>
<equation confidence="0.907953">
p(g, u|f, W) (1)
</equation>
<bodyText confidence="0.999932461538462">
which says that, as the Giver, we select our utter-
ances on the basis of what the Follower says, and
on the set of waypoints we next wish to describe.
To formalise this idea into a generative model,
we assume that the Giver act g depends only on
the Follower act f. We further assume that the se-
mantic unit u depends on the set of waypoints W
which it describes, and on the Giver’s choice of
dialog act g. Thus, u and f are conditionally in-
dependent given g. This provides a simple way of
incorporating the different sources of information
into a complete generative model3. Using Bayes’
theorem, we can rewrite Equation (1) as:
</bodyText>
<equation confidence="0.998168">
p(g, u|f, W) =
p(u) p(g|u) p(f|g) p(W |u)
Eg/u/ p(u&apos;) p(g&apos;|u&apos;) p(f|g&apos;) p(W |u&apos;) (2)
</equation>
<bodyText confidence="0.999567166666667">
requiring four distributions: p(u), p(g|u), p(f|g),
and p(W |u). The first three become the seman-
tic component of our model, to which we dedi-
cate Section 3.1. The fourth is the spatio-semantic
component, to which we dedicate Sections 3.2–
3.4.
</bodyText>
<footnote confidence="0.81502975">
2We align g and u in a preprocessing step, and store the
names of landmarks which the units abstract away from.
3Further advancements to this work would investigate the
effects of relaxing the conditional independence assumption.
</footnote>
<page confidence="0.989438">
628
</page>
<subsectionHeader confidence="0.995844">
3.1 The Semantic Component
</subsectionHeader>
<bodyText confidence="0.999977166666667">
The semantic component concerns only the cate-
gorical variables, f, g, and u, and addresses how
the Giver selects their semantic utterances based
on what the Follower says. We model the distri-
butions u, g|u, and f|g from Equation (2) as cate-
gorical distributions with uniform Dirichlet priors:
</bodyText>
<equation confidence="0.999364666666667">
u — Cat(α) α — Dir(c) (3a)
g|u — Cat(β) β — Dir(κ) (3b)
f|g — Cat(γ) γ — Dir(λ) (3c)
</equation>
<bodyText confidence="0.99718">
We use point estimates for α, β and γ, fixing them
at their posterior means in the following manner:
</bodyText>
<equation confidence="0.999861">
Count(g, u) + 1
βgu = p(g|u) = Eg, Count(g&apos;, u) + L (4)
</equation>
<bodyText confidence="0.9846">
and similarly for αˆ and γˆ (L = size of vector β).
</bodyText>
<subsectionHeader confidence="0.999293">
3.2 Spatial Goal Abstraction
</subsectionHeader>
<bodyText confidence="0.999993214285714">
Each ordered point-set W on some given map can
be seen as the Giver’s current goal, on which they
base their behaviour. Let W = {wi; 0 &lt; i &lt; n},
where wi = (xi, yi) is a waypoint, and xi, yi are
pixel coordinates on the map, typically obtained
through a vision processing step.
Given this goal formulation, from Equation (2)
we require p(W|u), i.e. the probability of a set
of waypoints given a semantic unit. However,
there are two problems with deriving a generative
model directly over W. Firstly, the length of W
varies from one point-set to the next, making it
hard to compare probabilities with different num-
bers of observations. Secondly, deriving a model
directly over x, y coordinates introduces sparsity
problems, as we are highly unlikely to encounter
the same set of coordinates multiple times. We
thus require an abstraction away from the space of
pixel coordinates.
Our approach is to extract feature vectors of
fixed length from the point-sets, and then derive a
generative model over the feature vectors instead
of the point-sets. Feature extraction allows point-
sets with similar characteristics, rather than exact
pixel values, to give rise to similar distributions
over units, thus enabling the model to reason given
previously unseen point-sets. The features we ex-
tract are detailed in Section 3.4.
</bodyText>
<subsectionHeader confidence="0.995891">
3.3 The Multivariate Normal Distribution
</subsectionHeader>
<bodyText confidence="0.9997913">
Let M be an unordered point-set describing map
elements, such as landmark locations and map
boundary information. M = {mj; 0 &lt; j &lt; k},
where mj = (xj, yj) is a map element with pixel
coordinates xj and yj. We define a spatial feature
function ψ : W, M —* Rn which captures, as fea-
ture values, the characteristics of the point-set W
in relation to elements in M. Let the spatial fea-
ture vector, extracted from the point-set W and the
map elements M, be:
</bodyText>
<equation confidence="0.949236">
v = ψ(W, M) (5)
</equation>
<bodyText confidence="0.9963724">
Figure 2(b) illustrates the feature extraction pro-
cess. We now define a distribution over the feature
vector v given the semantic unit u. We model v|u
as a multivariate normal distribution (recall that v
is in Rn):
</bodyText>
<equation confidence="0.97054">
v|u — N (µu, Eu) (6)
</equation>
<bodyText confidence="0.9999720625">
where µ and E are the mean vectors and covari-
ance matrices respectively. Subscript u indicates
that there is one such parameter for each unit u.
Since the alignments between units u and point-
sets W are fully observed, parameter estimation
is a question of estimating the mean vectors µu,
and the covariance matrices Eu, from the point-
sets co-occuring with unit u&apos;. We use maximum
likelihood estimators. To avoid issues with de-
generate covariance matrices resulting from small
amounts of data, we consider diagonal covariance
matrices. Because v|u is normally distributed, in-
ference, both for parameters and conditional distri-
butions over units, can be performed exactly, and
so the model is exceptionally quick to learn and
perform inference.
</bodyText>
<subsectionHeader confidence="0.973484">
3.4 The Spatial Feature Sets
</subsectionHeader>
<bodyText confidence="0.997127666666667">
We derive four feature sets from the ordered point-
set W, while considering the map elements in the
unordered point-set M:
</bodyText>
<listItem confidence="0.9814766">
1. Absolute features capture directions and dis-
tances of movement. We compute the distance
between the first and last points in W, and com-
pute the angle between unit vector &lt;0,-1&gt; and
the line connecting first and last points in W
2. Polynomial features capture shapes of move-
ments as straight lines or curves. We compute
the mean residual of a degree one polynomial
fit to the points in W (linear), and a degree two
polynomial (quadratic)4
</listItem>
<footnote confidence="0.9515195">
4These features are computed quickly and efficiently, re-
quiring only the solution to a least squares problem.
</footnote>
<page confidence="0.993819">
629
</page>
<figureCaption confidence="0.98892525">
Figure 2: (a) At training time, a Giver’s semantic unit u is aligned with an ordered point-set W, repre-
senting a sub-route. (b) We extract a spatial feature vector v of fixed length, from point-sets W and M
of varying lengths. (c) We define a generative model of the Giver, over Giver act g and semantic unit u,
preceding Follower act f, and spatial feature vector v. Latent parameters and priors are shown.
</figureCaption>
<figure confidence="0.994878461538461">
f yu Eu
g v
u
Y
P
(a) sub-route aligned with u (b) spatial feature extraction (c) the model
α
E
w0
w0
w6
u = MOVE(UNDER, LM)
w6
W=
M=
w0
w1
:
wn
m0
m1
:
mk
v =
X
X
</figure>
<listItem confidence="0.999279416666667">
3. Landmark features capture how close the
route takes the Follower to the nearest land-
mark. We compute the distance between the
end-point in W and the nearest landmark in M,
and compute the angle between the route taken
in W and the line connecting the start point in
to the nearest landmark
4. Edge features capture the relationship between
the movement and the map edges. We compute
the distance from the start-point in W to the
nearest edge and corner in M, and similarly for
the end-point in W
</listItem>
<subsectionHeader confidence="0.945497">
3.5 The Complete Generative Model
</subsectionHeader>
<bodyText confidence="0.999988888888889">
Our complete generative model of the Giver is a
distribution over Giver act g and semantic unit u,
given the preceding Follower act f and the spatial
feature vector v. Vector v is the result of apply-
ing the feature extraction function ψ over W and
M, where W is the ordered point-set describing
the sub-route aligned with u, and M is the point-
set describing landmark locations and map edge
information. We rewrite Equation (2) as:
</bodyText>
<equation confidence="0.9918215">
p(g, u|f, v) =
p(u) p(g|u) p(f|g) p(v|u)
</equation>
<sectionHeader confidence="0.888027" genericHeader="method">
4 Corpus Statistics and State Space
</sectionHeader>
<bodyText confidence="0.999968588235294">
We conduct our experiments on the Map Task cor-
pus (Anderson et al., 1991), a collection of cooper-
ative human-human dialogs arising from the task
explained in Figure 1 and Table 1. The original
corpus was labelled with dialog acts, such as Ac-
knowledge and Instruct. The semantic units can
be obtained through a semantic parse of the nat-
ural language utterances, while the spatial infor-
mation can be obtained through vision processing
of the maps. We use an existing extension of the
corpus by Levit and Roy (2007), which is seman-
tically and spatially annotated. The spatial anno-
tation are x, y pixel coordinates of landmark loca-
tions and evenly spaces points on the routes. All
15 maps were annotated. The semantic units take
the predicates MOVE, TURN, POSITION, or ORI-
ENTATION, and two arguments: a route descrip-
tor and a referent. The semantic annotations were
restricted to the Giver’s Instruct, Clarify, and Ex-
plain acts. Out of the original 128 dialogs, 25 were
semantically annotated.
For our experiments, we use all 15 pairs of
maps, and all 25 semantically annotated dialogs.
A dialog on average contain 57.5 instances, where
an instance is an occurrence of f, g, u, and W.
We find 87 unique semantic units u in our data,
however, according to the semantic representation,
there can be 456 distinct possible values for u5.
As for the rest of the variables, f takes 15 values,
g takes 4, and v is a real-valued vector of length
10, extracted from the real valued sets W and M
of varying lengths. We thus reason in a semantic
state space of 87 × 15 × 4 = 5220, and an infinite
spatial state space.
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="method">
5 Intrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.858987285714286">
Our first evaluation metric is an information theo-
retic one, based on notion that better models find
new instances of data (not used to train them) to be
more predictable. One such metric is the probabil-
ity a model assigns to the data, (higher is better). A
520×2 for TURN and ORIENTATION, + 208×2 for
MOVE and POSITION.
</bodyText>
<equation confidence="0.861677">
Eg/u/ p(u&apos;) p(g&apos;|u&apos;) p(f|g&apos;) p(v|u&apos;) (7)
</equation>
<bodyText confidence="0.999945">
We call our model the Spatio-Semantic Model,
SSM, and depict it in Figure 2(c).
</bodyText>
<page confidence="0.994675">
630
</page>
<bodyText confidence="0.999965784313726">
second metric is perplexity, which computes how
surprising a model finds the data (lower is better).
Both metrics have been used to evaluate user simu-
lators in the literature (Georgila et al., 2005; Eshky
et al., 2012; Pietquin and Hastie, 2013). We com-
pute the per-utterance probability of held-out data,
instead of the per-dialog probability, since the lat-
ter was deemed incompatible across dialogs of dif-
ferent lengths by Pietquin and Hastie (2013). Per-
plexity is 2−log2(d) where d is the probability of the
instance in question. We evaluate using leave-one-
out validation, which estimates the model from all
but one dialog, then evaluates the probability of
that dialog. We repeat this process until all dialogs
have been evaluated as the unseen dialog.
Because we evaluate on held-out dialogs, we
need to be able to assign probabilities to pre-
viously unseen instances. We therefore smooth
our models (at training time) by learning a back-
ground model which we estimate from all the
training data. This results in high variance in the
distribution over features and a flat overall dis-
tribution. Where no model can be estimated for
a particular semantic unit, we use that semantic
unit’s smoothed prior probability combined with
the background model for its likelihood.
We first consider the suitability of the differ-
ent feature sets for predicting utterances. Fig-
ure 4 shows the mean per-utterance probability our
model assigns to held-out data when using differ-
ent sets. The more predictable the model finds the
data, the higher the probability. Note that the tar-
get metric here is not 1, as there is no single cor-
rect answer. It can be seen that the most success-
ful features in order of predictiveness are: Abso-
lute, then Polynomial, then Landmark, and finally
Edge. The combination of all buys us further im-
provement. Perplexity is shown in Table 2.
Secondly, we consider two baselines inspired by
similar approaches of comparison in the literature
(Eckert et al., 1997; Levin and Pieraccini, 2000;
Georgila et al., 2005). Both are variants of our
model that lack the spatial component, i.e. they are
not goal-based. Although the baselines are weak,
they allow us to measure the reduction in uncer-
tainty brought by the introduction of the spatial
componenet to our model, which is the purpose
of this comparison. Baseline 1 is p(g, u) while
Baseline 2 is (g, u f). The first tells us how pre-
dictable giver utterances are (in the held-out data),
based only on the normalised frequencies. The
</bodyText>
<figure confidence="0.937395">
0.00 0.08 0.16 0.24 0.32
A P L E A.P A.L A.E All
</figure>
<figureCaption confidence="0.994742666666667">
Figure 3: Mean per-utterance probability, as-
signed to held-out data by our model, when de-
fined over the four feature sets and their combina-
tions, estimated through leave-one-out validation.
A=Absolute, P=Polynomial, L=Landmark, and
E=Edge. Error bars are standard deviations.
</figureCaption>
<table confidence="0.999610833333333">
Feature Set Perplexity
Absolute (A) 7.26 f 4.08
Polynomial (P) 12.86 f 8.39
Landmark (L) 15.16 f 6.27
Edge (E) 17.92 f 8.47
All 4.66 f 2.22
</table>
<tableCaption confidence="0.990346">
Table 2: Perplexity scores (and standard devia-
</tableCaption>
<bodyText confidence="0.999491538461538">
tions) of our model, computed over the four fea-
ture sets and their combination, estimated through
leave-one-out validation. (A) outperforms all indi-
vidual sets, while the combination performs best.
second tells us how predictable they become when
we condition on the previous follower act. Details
of the baselines are similar to Section 3.1.
Figure 4 shows the mean per-utterance prob-
ability our model assigns to held-out data when
compared to the two baselines. Baseline 2 slightly
improves our predictions over Baseline 1, al-
though not reliably so, when considering the small
increase in perplexity in Table 3. SSM demon-
strates a much larger relative improvement across
both metrics. The results demonstrate that our
spatial component enables substantial reduction in
uncertainty, brought by the transfer of information
from the maps to the utterances.
Intrinsic metrics, such as the probability of
held-out data and perplexity, provide us with an el-
egant way of evaluating probabilistic models in a
setting where there is no single correct answer, but
a range of plausible answers, because they exploit
the model’s inherit ability to assign probability to
behaviour. However, the metrics can be hard to in-
terpret in an absolute sense, providing much better
</bodyText>
<page confidence="0.991875">
631
</page>
<figure confidence="0.559928">
Baseline1 Baseline2 SSM
</figure>
<figureCaption confidence="0.9538182">
Figure 4: Mean per-utterance probability, assigned
to held-out data by our model (SSM), compared to
two baselines which lack the spatial componenet,
estimated through leave-one-out validation. Error
bars are standard deviations.
</figureCaption>
<table confidence="0.9991265">
Model Perplexity
Baseline1 24.95 f 4.05
Baseline2 25.06 f 12.02
SSM 4.66 f 2.22
</table>
<tableCaption confidence="0.99838">
Table 3: Perplexity scores of our model (SSM),
</tableCaption>
<bodyText confidence="0.994899142857143">
compared to the two baselines, estimated through
leave-one-out validation. SSM finds the held-out
data to be least surprising.
information about the relative strengths of differ-
ent models rather than their absolute utility. In the
next section, we explore methods for determining
the utility of the models when applied to tasks.
</bodyText>
<sectionHeader confidence="0.990432" genericHeader="method">
6 Extrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.999949823529412">
In this section, we undertake a task-based evalu-
ation of model output. We train on 22 of the di-
alogs, holding out 3 at random for testing. The
task is to then generate, for each sub-route in the
test dialogs, the most probable unit to describe it6.
Figure 5 shows some examples of sub-routes taken
from the test dialogs, and shows the the most prob-
able unit to describe each under our model, SSM.
We first explore a naive notion of accuracy:
the percentage of model-generated units matching
Real Giver units observed in the test dialogs. We
compute the same for Baseline 1 from Section 5
as a lower bound. A quick glance at the results in
Table 4 might suggest that both models have lit-
tle utility: SSM is “correct” only 33% of the time.
However, the extent to which this conclusion fol-
lows depends on the suitability of accuracy as a
</bodyText>
<footnote confidence="0.996865">
6The models can generate 1 of the 87 units observed in
the training set, but are made to output the most probable in
this experiment.
</footnote>
<table confidence="0.8266585">
Baseline SSM
Match to Real Giver 7.69% 33.08%
</table>
<tableCaption confidence="0.960758">
Table 4: Percentage of model-generated units that
</tableCaption>
<bodyText confidence="0.82449675">
match Real Giver units in the test set. The models
output the most probable unit to describe a given
sub-route. We argue that this metric is unsuitable
as it assumes one correct answer.
</bodyText>
<table confidence="0.9846075">
Mismatch Baseline SSM Real Giver
1.45 3.04 5.27 5.11
</table>
<tableCaption confidence="0.988225">
Table 5: Average scores assigned by human judges
</tableCaption>
<bodyText confidence="0.975606717948718">
to model-generated units on a 7-level Likert scale.
Mismatch is judged to be the worst, followed by
Baseline. SSM and Real Giver are scored well,
and are judged to be of similar quality.
means of evaluating dialog. In most situations,
there is not a single correct description and a host
of incorrect ones, but rather a gradient of descrip-
tions from the highly informative and appropriate
to the nonsensical and confusing. Such subtleties
are not captured by an accuracy test (or the closely
related recall and precision). In demonstration of
this point, we next conduct qualitative evaluation
of model output.
We ask humans to rate, on a Likert scale of 7,
the degree to which a given unit provides a suitable
description of a given sub-route. Sub-routes are
taken from the test dialogs, and are marked simi-
larly to Figure 5 but on the complete map. Units
are generated from SSM, Baseline, Real Giver,
and a control condition: a deliberate Mismatch
to the sub-route. The Mismatch is generated au-
tomatically by taking the least probable unit under
SSM, of the form MOVE(TOWARD, x) where x is
one of the four compass directions. We collect 5
judgements for each sub-route-unit combinations
on Mechanical Turk, and randomise so that no
judge sees the same order of pairs. Test dialogs
contained 94 distinct sub-routes.
We analyse the results with a two-way ANOVA,
with the first factor being model, and the second
being the sub-route, for a 4×94 design. The means
of the “model” factor are shown in Table 5. It
can be seen that Mismatch and Baseline are scored
sensibly poorly, while SSM and Real Giver are
scored reasonably well, and are judged to be of a
similar quality. We thus proceed with a more rig-
orous analysis. The ANOVA summary is shown
in Table 6. A significant effect of the model fac-
0.00 0.08 0.16 0.24 0.32
</bodyText>
<page confidence="0.69283">
632
</page>
<figure confidence="0.999871333333333">
(a) (b)
(c) (d)
(e)
</figure>
<figureCaption confidence="0.974893333333334">
Figure 5: Given a sub-route marked with start-point o and end-point x (in red), SSM generates the
following u: (a) MOVE(TOWARDS, ABS NORTHEAST) (b) TURN(ABS WEST) (c) MOVE(FOLLOW-
BOUNDARY, LM) (d) MOVE(AROUND, LM) (e) MOVE(TOWARDS, ABS SOUTHWEST)
</figureCaption>
<table confidence="0.9993372">
Factor S Sq Df F Pr(&gt;F)
Model 4845.3 3 783.93 &lt;0.001
Sub-route 1140.0 93 5.95 &lt;0.001
M:S 2208.7 279 3.84 &lt;0.001
Residuals 3263.5 1584
</table>
<tableCaption confidence="0.944323">
Table 6: Two way ANOVA with factors model (4
</tableCaption>
<bodyText confidence="0.8253772">
possibilities), and sub-route (94 possibilities). Re-
sults show a model effect accounting for most of
the variance. Meaning that the scores assigned to
the units by human judges are significantly influ-
enced by the model used to generate the units.
</bodyText>
<table confidence="0.999461428571429">
Model Comparison tvalue Pr(&gt;|t|)
Mismatch: Baseline -16.974 &lt;0.001
SSM: Baseline 23.882 &lt;0.001
Real Giver: Baseline 23.192 &lt;0.001
SSM: Mismatch 40.857 &lt;0.001
Real Giver: Mismatch 40.507 &lt;0.001
SSM: Real Giver 1.171 0.646
</table>
<tableCaption confidence="0.991241">
Table 7: Tukey HSD shows that all models are
</tableCaption>
<bodyText confidence="0.993150833333333">
assigned significantly different scores by judges,
apart from SSM and Real Giver. This asserts that,
although only 33% of SSM units match Real Giver
units (as shown in Table 4), the quality of the units
are not judged to be significantly different.
tor is present, meaning that the scores assigned by
human judges to the units are significantly influ-
enced by which model was used to generate the
units. Additionally, a significant effect for the sub-
route factor can be seen, which is due to some sub-
routes being harder to describe than others. An in-
teraction effect is also present, which is expected
given such a large number of examples. Note how
the model factor accounts for the largest amount
of variance of all the factors.
Having confirmed the presence of a model ef-
fect, we conduct a post-hoc analysis of the model
factors. Table 7 shows a Tukey HSD test, demon-
strating that all models are significantly different
from one another, except Real Giver and SSM. Re-
sults show that, despite the large number of judge-
ments collected, we are unable to separate the
quality of our model’s unit from that in the origi-
nal data, against which accuracy was being judged
in Table 4. This demonstrates that when many an-
swers are feasible, scoring correctness against the
original human units is unsuitable. It also firmly
demonstrates the suitability of our spatial repre-
sentation, and the strength of the generative model
we have induced for the task.
</bodyText>
<sectionHeader confidence="0.986048" genericHeader="conclusions">
7 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999995">
We have shown how to represent spatial goals in a
navigational domain, and have validated our rep-
resentation by inducing (fully from data) a gen-
erative model of the Giver’s semantic utterances
conditioned on the spatial goal and the previous
Follower act. Intrinsic and extrinsic evaluation
demonstrate the strength of our model.
A direct application of this work is robot guid-
ance, by using the Giver’s simulator to induce an
optimal Follower: an MDP-based dialog manager
that interprets and follows navigational instruc-
tions. Another variation would be to learn a gen-
erative model of the Follower, by extracting fea-
tures from Follower maps (labelled with routes
drawn by real Followers). Finally, this work has
broader applications beyond simulation, in partic-
ular for systems that describe routes to users (spa-
tial goal representation and model dependencies
would hold). Decisions about which part of the
route to describe next is one extension to that end.
</bodyText>
<sectionHeader confidence="0.996437" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.955573166666667">
We thank Ioannis Konstas, Johanna Moore, Robin
Hill, S. M. Ali Eslami, and the anonymous review-
ers for valuable feedback. This work is funded
by King Saud University. Mark Steedman is sup-
ported by EC-PF7-270273 Xperience and ERC
Advanced Fellowship 249520 GRAMPLUS.
</bodyText>
<page confidence="0.99899">
633
</page>
<sectionHeader confidence="0.98898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998334644859813">
Hua Ai and Diane J. Litman. 2007. Knowledge con-
sistent user simulations for dialog systems. In Inter-
Speech 2007, pages 2697–2700.
Anne H. Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry S. Thompson,
and Regina Weinert. 1991. The hcrc map task cor-
pus. Language and Speech, 34(4):351–366.
Heriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
ASRU 2005, pages 290–295.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system
evaluation. In Proceedings of IEEE Workshop on
Automatic Speech Recognition and Understanding.
Aciel Eshky, Ben Allison, and Mark Steedman. 2012.
Generative goal-driven user simulation for dialog
management. In EMNLP-CoNLL 2012, pages 71–
81, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005. Learning user simulations for in-
formation state update dialogue systems. In Inter-
Speech 2005.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2006. User Simulation for Spoken Dia-
logue Systems: Learning and Evaluation. In Inter-
Speech 2006.
James Henderson and Oliver Lemon. 2008. Mixture
model pomdps for efficient handling of uncertainty
in dialogue management. In ACL, HLT-Short ’08,
pages 73–76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Srinivasan Janarthanam, Oliver Lemon, Phil Bartie,
Tiphaine Dalmas, Anna Dickinson, Xingkun Liu,
William Mackaness, and Bonnie Webber. 2013.
Evaluating a city exploration dialogue system with
integrated question-answering and pedestrian navi-
gation. In ACL.
Esther Levin and Roberto Pieraccini. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. In IEEE Transactions on Speech
and Audio Processing.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1998. Using markov decision process for learning
dialogue strategies. In Proc. ICASSP, pages 201–
204.
M. Levit and D. Roy. 2007. Interpretation of spatial
language in a map navigation task. IEEE Trans-
actions on Systems, Man, and Cybernetics, Part A,
37(3):667–679.
Pierre Lison. 2013. Model-based bayesian reinforce-
ment learning for dialogue management. In Inter-
speech 2013.
Olivier Pietquin and Thierry Dutoit. 2006. A prob-
abilistic framework for dialog simulation and opti-
mal strategy learning. Audio, Speech, and Language
Processing, IEEE Transactions on, 14(2):589–599,
march.
Olivier Pietquin and Helen Hastie. 2013. A survey
on metrics for the evaluation of user simulations.
Knowledge Eng. Review, 28(1):59–73.
Olivier Pietquin. 2004. A Framework for Unsuper-
vised Learning of Dialogue Strategies. Ph.D. thesis,
Facult´e Polytechnique de Mons, TCTS Lab (Bel-
gique), apr.
Olivier Pietquin. 2006. Consistent goal-directed user
model for realisitc man-machine task-oriented spo-
ken dialogue simulation. In Multimedia and Expo,
2006 IEEE International Conference on, pages 425–
428. IEEE.
David Reitter and Johanna D. Moore. 2007. Predicting
success in dialogue. In ACL.
Verena Rieser and Oliver Lemon. 2006. Cluster-
based user simulations for learning dialogue strate-
gies. In INTERSPEECH 2006 - ICSLP, Ninth Inter-
national Conference on Spoken Language Process-
ing, September.
St´ephane Rossignol, Olivier Pietquin, and Michel Ian-
otto. 2010. Simulation of the grounding process in
spoken dialog systems with bayesian networks. In
IWSDS, pages 110–121.
St´ephane Rossignol, Olivier Pietquin, and Michel Ian-
otto. 2011. Training a bn-based user model for di-
alogue simulation with missing data. In IJCNLP,
pages 598–604.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’00, pages 93–100, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jost Schatzmann and Steve Young. 2009. The hid-
den agenda user simulation model. Audio, Speech,
and Language Processing, IEEE Transactions on,
17(4):733–747.
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
HLT 2002.
Satinder Singh, Diane J. Litman, Michael Kearns, and
Marilyn A. Walker. 2002. Optimizing dialogue
management with reinforcement learning: Experi-
ments with the njfun system. Journal of Artificial
Intelligence Research, 16:105–133.
</reference>
<page confidence="0.985991">
634
</page>
<reference confidence="0.999806185185185">
Henry S. Thompson, Anne Anderson, Ellen G. Bard,
Gwyneth D. Sneddon, Alison Newlands, and Cathy
Sotillo. 1993. The HCRC Map Task corpus: natu-
ral dialogue for speech recognition. In Proceedings
of the workshop on Human Language Technology,
HLT ’93, pages 25–30, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In ACL 2010, Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, July 11-16,
2010, Uppsala, Sweden, pages 806–814. The Asso-
ciation for Computer Linguistics.
Jason D. Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393–422.
Jason D. Williams. 2007. Applying pomdps to dia-
log systems in the troubleshooting domain. In Pro-
ceedings of the Workshop on Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technolo-
gies, NAACL-HLT ’07, pages 1–8, Morristown, NJ,
USA. Association for Computational Linguistics.
Steve Young, Milica Gasic, Blaise Thomson, and Ja-
son D. Williams. 2013. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160–1179.
</reference>
<page confidence="0.998786">
635
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.548485">
<title confidence="0.999738">A Generative Model for User Simulation in a Spatial Navigation Domain</title>
<author confidence="0.999994">Ben Subramanian</author>
<affiliation confidence="0.7935265">of Informatics, University of Edinburgh, Analytics Ltd., Edinburgh,</affiliation>
<email confidence="0.999913">ballison@actualanalytics.com</email>
<abstract confidence="0.996482">We propose the use of a generative model to simulate user behaviour in a novel taskoriented dialog domain, where user goals are spatial routes across artificial landscapes. We show how to derive an efficient feature-based representation of spatial goals, admitting exact inference and generalising to new routes. The use of a generative model allows us to capture a range of plausible behaviour given the same underlying goal. We evaluate intrinsically using held-out probability and perplexity, and find a substantial reduction in uncertainty brought by our spatial representation. We evaluate extrinsically in a human judgement task and find that our model’s behaviour does not differ significantly from the behaviour of real users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hua Ai</author>
<author>Diane J Litman</author>
</authors>
<title>Knowledge consistent user simulations for dialog systems. In InterSpeech</title>
<date>2007</date>
<pages>2697--2700</pages>
<contexts>
<context position="9180" citStr="Ai and Litman, 2007" startWordPosition="1457" endWordPosition="1460">e to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s utterances in response to the Follower</context>
</contexts>
<marker>Ai, Litman, 2007</marker>
<rawString>Hua Ai and Diane J. Litman. 2007. Knowledge consistent user simulations for dialog systems. In InterSpeech 2007, pages 2697–2700.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne H Anderson</author>
<author>Miles Bader</author>
<author>Ellen Gurman Bard</author>
<author>Elizabeth Boyle</author>
<author>Gwyneth Doherty</author>
<author>Simon Garrod</author>
<author>Stephen Isard</author>
<author>Jacqueline Kowtko</author>
<author>Jan McAllister</author>
<author>Jim Miller</author>
<author>Catherine Sotillo</author>
<author>Henry S Thompson</author>
<author>Regina Weinert</author>
</authors>
<title>The hcrc map task corpus.</title>
<date>1991</date>
<journal>Language and Speech,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1988" citStr="Anderson et al., 1991" startWordPosition="300" endWordPosition="303">oung et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigational domains present an interesting challenge, due to the disparity between the spatial goals and their grounding as utterances. This disparity renders much of the statistical management literature inapplicable. In this paper, we address this deficiency. We focus on the task of simulating user behaviour, both because of the important role simulators plays </context>
<context position="17715" citStr="Anderson et al., 1991" startWordPosition="2969" endWordPosition="2972">t in W 3.5 The Complete Generative Model Our complete generative model of the Giver is a distribution over Giver act g and semantic unit u, given the preceding Follower act f and the spatial feature vector v. Vector v is the result of applying the feature extraction function ψ over W and M, where W is the ordered point-set describing the sub-route aligned with u, and M is the pointset describing landmark locations and map edge information. We rewrite Equation (2) as: p(g, u|f, v) = p(u) p(g|u) p(f|g) p(v|u) 4 Corpus Statistics and State Space We conduct our experiments on the Map Task corpus (Anderson et al., 1991), a collection of cooperative human-human dialogs arising from the task explained in Figure 1 and Table 1. The original corpus was labelled with dialog acts, such as Acknowledge and Instruct. The semantic units can be obtained through a semantic parse of the natural language utterances, while the spatial information can be obtained through vision processing of the maps. We use an existing extension of the corpus by Levit and Roy (2007), which is semantically and spatially annotated. The spatial annotation are x, y pixel coordinates of landmark locations and evenly spaces points on the routes. </context>
</contexts>
<marker>Anderson, Bader, Bard, Boyle, Doherty, Garrod, Isard, Kowtko, McAllister, Miller, Sotillo, Thompson, Weinert, 1991</marker>
<rawString>Anne H. Anderson, Miles Bader, Ellen Gurman Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller, Catherine Sotillo, Henry S. Thompson, and Regina Weinert. 1991. The hcrc map task corpus. Language and Speech, 34(4):351–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heriberto Cuay´ahuitl</author>
<author>Steve Renals</author>
<author>Oliver Lemon</author>
<author>Hiroshi Shimodaira</author>
</authors>
<title>Human-computer dialogue simulation using hidden markov models. In ASRU</title>
<date>2005</date>
<pages>290--295</pages>
<marker>Cuay´ahuitl, Renals, Lemon, Shimodaira, 2005</marker>
<rawString>Heriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and Hiroshi Shimodaira. 2005. Human-computer dialogue simulation using hidden markov models. In ASRU 2005, pages 290–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wieland Eckert</author>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>User modeling for spoken dialogue system evaluation.</title>
<date>1997</date>
<booktitle>In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="8726" citStr="Eckert et al., 1997" startWordPosition="1388" endWordPosition="1391">ty to a particular route. 2 Related Work 2.1 Related Work on the Map Task To our knowledge, there are no attempts to model instruction Givers as users in the Map Task domain. Two studies model the Follower, in the context of understanding natural language instructions and interpreting them by drawing a route (Levit and Roy, 2007; Vogel and Jurafsky, 2010). Both studies exclude dialog from their modelling. Although their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by exten</context>
<context position="21688" citStr="Eckert et al., 1997" startWordPosition="3648" endWordPosition="3651">ances. Figure 4 shows the mean per-utterance probability our model assigns to held-out data when using different sets. The more predictable the model finds the data, the higher the probability. Note that the target metric here is not 1, as there is no single correct answer. It can be seen that the most successful features in order of predictiveness are: Absolute, then Polynomial, then Landmark, and finally Edge. The combination of all buys us further improvement. Perplexity is shown in Table 2. Secondly, we consider two baselines inspired by similar approaches of comparison in the literature (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005). Both are variants of our model that lack the spatial component, i.e. they are not goal-based. Although the baselines are weak, they allow us to measure the reduction in uncertainty brought by the introduction of the spatial componenet to our model, which is the purpose of this comparison. Baseline 1 is p(g, u) while Baseline 2 is (g, u f). The first tells us how predictable giver utterances are (in the held-out data), based only on the normalised frequencies. The 0.00 0.08 0.16 0.24 0.32 A P L E A.P A.L A.E All Figure 3: Mean per-utterance </context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>Wieland Eckert, Esther Levin, and Roberto Pieraccini. 1997. User modeling for spoken dialogue system evaluation. In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aciel Eshky</author>
<author>Ben Allison</author>
<author>Mark Steedman</author>
</authors>
<title>Generative goal-driven user simulation for dialog management.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL 2012,</booktitle>
<pages>71--81</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="9695" citStr="Eshky et al., 2012" startWordPosition="1539" endWordPosition="1542">goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s utterances in response to the Follower’s, at the semantic level. A Giver’s utterance takes the form: g = Instruct, u = MOVE (UNDER, LM) consisting of a dialog act g and a semantic unit u2. Aligned with u, is an ordered set of waypoints W, corresponding only to part of the route u describes. Figure 2(a) shows an example of such a sub-route. The point-set W can be seen as the Giver’s current goal on which they base their behaviour. Because the routes are drawn on the Giver’s maps, we treat W as observed. To model some of the interaction between the </context>
<context position="19931" citStr="Eshky et al., 2012" startWordPosition="3358" endWordPosition="3361"> theoretic one, based on notion that better models find new instances of data (not used to train them) to be more predictable. One such metric is the probability a model assigns to the data, (higher is better). A 520×2 for TURN and ORIENTATION, + 208×2 for MOVE and POSITION. Eg/u/ p(u&apos;) p(g&apos;|u&apos;) p(f|g&apos;) p(v|u&apos;) (7) We call our model the Spatio-Semantic Model, SSM, and depict it in Figure 2(c). 630 second metric is perplexity, which computes how surprising a model finds the data (lower is better). Both metrics have been used to evaluate user simulators in the literature (Georgila et al., 2005; Eshky et al., 2012; Pietquin and Hastie, 2013). We compute the per-utterance probability of held-out data, instead of the per-dialog probability, since the latter was deemed incompatible across dialogs of different lengths by Pietquin and Hastie (2013). Perplexity is 2−log2(d) where d is the probability of the instance in question. We evaluate using leave-oneout validation, which estimates the model from all but one dialog, then evaluates the probability of that dialog. We repeat this process until all dialogs have been evaluated as the unseen dialog. Because we evaluate on held-out dialogs, we need to be able </context>
</contexts>
<marker>Eshky, Allison, Steedman, 2012</marker>
<rawString>Aciel Eshky, Ben Allison, and Mark Steedman. 2012. Generative goal-driven user simulation for dialog management. In EMNLP-CoNLL 2012, pages 71– 81, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning user simulations for information state update dialogue systems.</title>
<date>2005</date>
<journal>In InterSpeech</journal>
<contexts>
<context position="8777" citStr="Georgila et al., 2005" startWordPosition="1396" endWordPosition="1399">ted Work on the Map Task To our knowledge, there are no attempts to model instruction Givers as users in the Map Task domain. Two studies model the Follower, in the context of understanding natural language instructions and interpreting them by drawing a route (Levit and Roy, 2007; Vogel and Jurafsky, 2010). Both studies exclude dialog from their modelling. Although their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to </context>
<context position="19911" citStr="Georgila et al., 2005" startWordPosition="3354" endWordPosition="3357">etric is an information theoretic one, based on notion that better models find new instances of data (not used to train them) to be more predictable. One such metric is the probability a model assigns to the data, (higher is better). A 520×2 for TURN and ORIENTATION, + 208×2 for MOVE and POSITION. Eg/u/ p(u&apos;) p(g&apos;|u&apos;) p(f|g&apos;) p(v|u&apos;) (7) We call our model the Spatio-Semantic Model, SSM, and depict it in Figure 2(c). 630 second metric is perplexity, which computes how surprising a model finds the data (lower is better). Both metrics have been used to evaluate user simulators in the literature (Georgila et al., 2005; Eshky et al., 2012; Pietquin and Hastie, 2013). We compute the per-utterance probability of held-out data, instead of the per-dialog probability, since the latter was deemed incompatible across dialogs of different lengths by Pietquin and Hastie (2013). Perplexity is 2−log2(d) where d is the probability of the instance in question. We evaluate using leave-oneout validation, which estimates the model from all but one dialog, then evaluates the probability of that dialog. We repeat this process until all dialogs have been evaluated as the unseen dialog. Because we evaluate on held-out dialogs,</context>
<context position="21740" citStr="Georgila et al., 2005" startWordPosition="3656" endWordPosition="3659">ability our model assigns to held-out data when using different sets. The more predictable the model finds the data, the higher the probability. Note that the target metric here is not 1, as there is no single correct answer. It can be seen that the most successful features in order of predictiveness are: Absolute, then Polynomial, then Landmark, and finally Edge. The combination of all buys us further improvement. Perplexity is shown in Table 2. Secondly, we consider two baselines inspired by similar approaches of comparison in the literature (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005). Both are variants of our model that lack the spatial component, i.e. they are not goal-based. Although the baselines are weak, they allow us to measure the reduction in uncertainty brought by the introduction of the spatial componenet to our model, which is the purpose of this comparison. Baseline 1 is p(g, u) while Baseline 2 is (g, u f). The first tells us how predictable giver utterances are (in the held-out data), based only on the normalised frequencies. The 0.00 0.08 0.16 0.24 0.32 A P L E A.P A.L A.E All Figure 3: Mean per-utterance probability, assigned to held-out data by our model,</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2005</marker>
<rawString>Kallirroi Georgila, James Henderson, and Oliver Lemon. 2005. Learning user simulations for information state update dialogue systems. In InterSpeech 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>User Simulation for Spoken Dialogue Systems: Learning and Evaluation. In InterSpeech</title>
<date>2006</date>
<contexts>
<context position="8801" citStr="Georgila et al., 2006" startWordPosition="1400" endWordPosition="1403">k To our knowledge, there are no attempts to model instruction Givers as users in the Map Task domain. Two studies model the Follower, in the context of understanding natural language instructions and interpreting them by drawing a route (Levit and Roy, 2007; Vogel and Jurafsky, 2010). Both studies exclude dialog from their modelling. Although their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting </context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2006</marker>
<rawString>Kallirroi Georgila, James Henderson, and Oliver Lemon. 2006. User Simulation for Spoken Dialogue Systems: Learning and Evaluation. In InterSpeech 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>Mixture model pomdps for efficient handling of uncertainty in dialogue management.</title>
<date>2008</date>
<booktitle>In ACL, HLT-Short ’08,</booktitle>
<pages>73--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1683" citStr="Henderson and Lemon, 2008" startWordPosition="254" endWordPosition="257">ction Automated dialog management is an area of research that has undergone rapid advancement in the last decade. The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigational domains present an interesting challenge, due to</context>
</contexts>
<marker>Henderson, Lemon, 2008</marker>
<rawString>James Henderson and Oliver Lemon. 2008. Mixture model pomdps for efficient handling of uncertainty in dialogue management. In ACL, HLT-Short ’08, pages 73–76, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
<author>Phil Bartie</author>
</authors>
<title>Tiphaine Dalmas,</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<location>Anna Dickinson, Xingkun Liu, William</location>
<contexts>
<context position="2220" citStr="Janarthanam et al., 2013" startWordPosition="334" endWordPosition="338">y et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigational domains present an interesting challenge, due to the disparity between the spatial goals and their grounding as utterances. This disparity renders much of the statistical management literature inapplicable. In this paper, we address this deficiency. We focus on the task of simulating user behaviour, both because of the important role simulators plays in the induction of dialog managers, and because it provides a self-contained means of developing the domain representations which facilitate dialog reasoning. We show how a generative model of user behaviour can be induced from dat</context>
</contexts>
<marker>Janarthanam, Lemon, Bartie, 2013</marker>
<rawString>Srinivasan Janarthanam, Oliver Lemon, Phil Bartie, Tiphaine Dalmas, Anna Dickinson, Xingkun Liu, William Mackaness, and Bonnie Webber. 2013. Evaluating a city exploration dialogue system with integrated question-answering and pedestrian navigation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<booktitle>In IEEE Transactions on Speech and Audio Processing.</booktitle>
<contexts>
<context position="8754" citStr="Levin and Pieraccini, 2000" startWordPosition="1392" endWordPosition="1395">ute. 2 Related Work 2.1 Related Work on the Map Task To our knowledge, there are no attempts to model instruction Givers as users in the Map Task domain. Two studies model the Follower, in the context of understanding natural language instructions and interpreting them by drawing a route (Levit and Roy, 2007; Vogel and Jurafsky, 2010). Both studies exclude dialog from their modelling. Although their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of</context>
<context position="21716" citStr="Levin and Pieraccini, 2000" startWordPosition="3652" endWordPosition="3655"> the mean per-utterance probability our model assigns to held-out data when using different sets. The more predictable the model finds the data, the higher the probability. Note that the target metric here is not 1, as there is no single correct answer. It can be seen that the most successful features in order of predictiveness are: Absolute, then Polynomial, then Landmark, and finally Edge. The combination of all buys us further improvement. Perplexity is shown in Table 2. Secondly, we consider two baselines inspired by similar approaches of comparison in the literature (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005). Both are variants of our model that lack the spatial component, i.e. they are not goal-based. Although the baselines are weak, they allow us to measure the reduction in uncertainty brought by the introduction of the spatial componenet to our model, which is the purpose of this comparison. Baseline 1 is p(g, u) while Baseline 2 is (g, u f). The first tells us how predictable giver utterances are (in the held-out data), based only on the normalised frequencies. The 0.00 0.08 0.16 0.24 0.32 A P L E A.P A.L A.E All Figure 3: Mean per-utterance probability, assigned to hel</context>
</contexts>
<marker>Levin, Pieraccini, 2000</marker>
<rawString>Esther Levin and Roberto Pieraccini. 2000. A stochastic model of human-machine interaction for learning dialog strategies. In IEEE Transactions on Speech and Audio Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
<author>Wieland Eckert</author>
</authors>
<title>Using markov decision process for learning dialogue strategies.</title>
<date>1998</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>201--204</pages>
<contexts>
<context position="1591" citStr="Levin et al., 1998" startWordPosition="238" endWordPosition="241"> behaviour does not differ significantly from the behaviour of real users. 1 Introduction Automated dialog management is an area of research that has undergone rapid advancement in the last decade. The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand craft</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 1998</marker>
<rawString>Esther Levin, Roberto Pieraccini, and Wieland Eckert. 1998. Using markov decision process for learning dialogue strategies. In Proc. ICASSP, pages 201– 204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Levit</author>
<author>D Roy</author>
</authors>
<title>Interpretation of spatial language in a map navigation task.</title>
<date>2007</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics, Part A,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="8437" citStr="Levit and Roy, 2007" startWordPosition="1340" endWordPosition="1343">luate extrinsically by generating utterances from our model, and comparing them to held-out utterance of real humans in the test data. We also utilise human judgements for the task, where the judges score the output of the different models and the human utterances based on their suitability to a particular route. 2 Related Work 2.1 Related Work on the Map Task To our knowledge, there are no attempts to model instruction Givers as users in the Map Task domain. Two studies model the Follower, in the context of understanding natural language instructions and interpreting them by drawing a route (Levit and Roy, 2007; Vogel and Jurafsky, 2010). Both studies exclude dialog from their modelling. Although their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency us</context>
<context position="18154" citStr="Levit and Roy (2007)" startWordPosition="3044" endWordPosition="3047">n. We rewrite Equation (2) as: p(g, u|f, v) = p(u) p(g|u) p(f|g) p(v|u) 4 Corpus Statistics and State Space We conduct our experiments on the Map Task corpus (Anderson et al., 1991), a collection of cooperative human-human dialogs arising from the task explained in Figure 1 and Table 1. The original corpus was labelled with dialog acts, such as Acknowledge and Instruct. The semantic units can be obtained through a semantic parse of the natural language utterances, while the spatial information can be obtained through vision processing of the maps. We use an existing extension of the corpus by Levit and Roy (2007), which is semantically and spatially annotated. The spatial annotation are x, y pixel coordinates of landmark locations and evenly spaces points on the routes. All 15 maps were annotated. The semantic units take the predicates MOVE, TURN, POSITION, or ORIENTATION, and two arguments: a route descriptor and a referent. The semantic annotations were restricted to the Giver’s Instruct, Clarify, and Explain acts. Out of the original 128 dialogs, 25 were semantically annotated. For our experiments, we use all 15 pairs of maps, and all 25 semantically annotated dialogs. A dialog on average contain 5</context>
</contexts>
<marker>Levit, Roy, 2007</marker>
<rawString>M. Levit and D. Roy. 2007. Interpretation of spatial language in a map navigation task. IEEE Transactions on Systems, Man, and Cybernetics, Part A, 37(3):667–679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Lison</author>
</authors>
<title>Model-based bayesian reinforcement learning for dialogue management.</title>
<date>2013</date>
<booktitle>In Interspeech</booktitle>
<contexts>
<context position="1860" citStr="Lison, 2013" startWordPosition="285" endWordPosition="286">adigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigational domains present an interesting challenge, due to the disparity between the spatial goals and their grounding as utterances. This disparity renders much of the statistical management literature inapplicable. In this paper, we </context>
</contexts>
<marker>Lison, 2013</marker>
<rawString>Pierre Lison. 2013. Model-based bayesian reinforcement learning for dialogue management. In Interspeech 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
<author>Thierry Dutoit</author>
</authors>
<title>A probabilistic framework for dialog simulation and optimal strategy learning. Audio, Speech, and Language Processing,</title>
<date>2006</date>
<journal>IEEE Transactions on,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="9626" citStr="Pietquin and Dutoit, 2006" startWordPosition="1527" endWordPosition="1530"> mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s utterances in response to the Follower’s, at the semantic level. A Giver’s utterance takes the form: g = Instruct, u = MOVE (UNDER, LM) consisting of a dialog act g and a semantic unit u2. Aligned with u, is an ordered set of waypoints W, corresponding only to part of the route u describes. Figure 2(a) shows an example of such a sub-route. The point-set W can be seen as the Giver’s current goal on which they base their behaviour. Because the routes are drawn on the Giver’s maps, </context>
</contexts>
<marker>Pietquin, Dutoit, 2006</marker>
<rawString>Olivier Pietquin and Thierry Dutoit. 2006. A probabilistic framework for dialog simulation and optimal strategy learning. Audio, Speech, and Language Processing, IEEE Transactions on, 14(2):589–599, march.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
<author>Helen Hastie</author>
</authors>
<title>A survey on metrics for the evaluation of user simulations.</title>
<date>2013</date>
<journal>Knowledge Eng. Review,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="19959" citStr="Pietquin and Hastie, 2013" startWordPosition="3362" endWordPosition="3365">d on notion that better models find new instances of data (not used to train them) to be more predictable. One such metric is the probability a model assigns to the data, (higher is better). A 520×2 for TURN and ORIENTATION, + 208×2 for MOVE and POSITION. Eg/u/ p(u&apos;) p(g&apos;|u&apos;) p(f|g&apos;) p(v|u&apos;) (7) We call our model the Spatio-Semantic Model, SSM, and depict it in Figure 2(c). 630 second metric is perplexity, which computes how surprising a model finds the data (lower is better). Both metrics have been used to evaluate user simulators in the literature (Georgila et al., 2005; Eshky et al., 2012; Pietquin and Hastie, 2013). We compute the per-utterance probability of held-out data, instead of the per-dialog probability, since the latter was deemed incompatible across dialogs of different lengths by Pietquin and Hastie (2013). Perplexity is 2−log2(d) where d is the probability of the instance in question. We evaluate using leave-oneout validation, which estimates the model from all but one dialog, then evaluates the probability of that dialog. We repeat this process until all dialogs have been evaluated as the unseen dialog. Because we evaluate on held-out dialogs, we need to be able to assign probabilities to p</context>
</contexts>
<marker>Pietquin, Hastie, 2013</marker>
<rawString>Olivier Pietquin and Helen Hastie. 2013. A survey on metrics for the evaluation of user simulations. Knowledge Eng. Review, 28(1):59–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
</authors>
<title>A Framework for Unsupervised Learning of Dialogue Strategies.</title>
<date>2004</date>
<booktitle>Ph.D. thesis, Facult´e Polytechnique de Mons, TCTS Lab (Belgique),</booktitle>
<contexts>
<context position="9573" citStr="Pietquin, 2004" startWordPosition="1521" endWordPosition="1522">istic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s utterances in response to the Follower’s, at the semantic level. A Giver’s utterance takes the form: g = Instruct, u = MOVE (UNDER, LM) consisting of a dialog act g and a semantic unit u2. Aligned with u, is an ordered set of waypoints W, corresponding only to part of the route u describes. Figure 2(a) shows an example of such a sub-route. The point-set W can be seen as the Giver’s current goal on which they base their behaviou</context>
</contexts>
<marker>Pietquin, 2004</marker>
<rawString>Olivier Pietquin. 2004. A Framework for Unsupervised Learning of Dialogue Strategies. Ph.D. thesis, Facult´e Polytechnique de Mons, TCTS Lab (Belgique), apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
</authors>
<title>Consistent goal-directed user model for realisitc man-machine task-oriented spoken dialogue simulation. In Multimedia and Expo,</title>
<date>2006</date>
<booktitle>IEEE International Conference on,</booktitle>
<pages>425--428</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="9159" citStr="Pietquin, 2006" startWordPosition="1455" endWordPosition="1456">rectly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s utterances in res</context>
</contexts>
<marker>Pietquin, 2006</marker>
<rawString>Olivier Pietquin. 2006. Consistent goal-directed user model for realisitc man-machine task-oriented spoken dialogue simulation. In Multimedia and Expo, 2006 IEEE International Conference on, pages 425– 428. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Reitter</author>
<author>Johanna D Moore</author>
</authors>
<title>Predicting success in dialogue.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2037" citStr="Reitter and Moore, 2007" startWordPosition="308" endWordPosition="311">ent treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigational domains present an interesting challenge, due to the disparity between the spatial goals and their grounding as utterances. This disparity renders much of the statistical management literature inapplicable. In this paper, we address this deficiency. We focus on the task of simulating user behaviour, both because of the important role simulators plays in the induction of dialog managers, and because </context>
</contexts>
<marker>Reitter, Moore, 2007</marker>
<rawString>David Reitter and Johanna D. Moore. 2007. Predicting success in dialogue. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Clusterbased user simulations for learning dialogue strategies.</title>
<date>2006</date>
<booktitle>In INTERSPEECH 2006 - ICSLP, Ninth International Conference on Spoken Language Processing,</booktitle>
<contexts>
<context position="9143" citStr="Rieser and Lemon, 2006" startWordPosition="1451" endWordPosition="1454">ugh their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s u</context>
</contexts>
<marker>Rieser, Lemon, 2006</marker>
<rawString>Verena Rieser and Oliver Lemon. 2006. Clusterbased user simulations for learning dialogue strategies. In INTERSPEECH 2006 - ICSLP, Ninth International Conference on Spoken Language Processing, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Rossignol</author>
<author>Olivier Pietquin</author>
<author>Michel Ianotto</author>
</authors>
<title>Simulation of the grounding process in spoken dialog systems with bayesian networks.</title>
<date>2010</date>
<booktitle>In IWSDS,</booktitle>
<pages>110--121</pages>
<contexts>
<context position="9650" citStr="Rossignol et al., 2010" startWordPosition="1531" endWordPosition="1534">stency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s utterances in response to the Follower’s, at the semantic level. A Giver’s utterance takes the form: g = Instruct, u = MOVE (UNDER, LM) consisting of a dialog act g and a semantic unit u2. Aligned with u, is an ordered set of waypoints W, corresponding only to part of the route u describes. Figure 2(a) shows an example of such a sub-route. The point-set W can be seen as the Giver’s current goal on which they base their behaviour. Because the routes are drawn on the Giver’s maps, we treat W as observed. </context>
</contexts>
<marker>Rossignol, Pietquin, Ianotto, 2010</marker>
<rawString>St´ephane Rossignol, Olivier Pietquin, and Michel Ianotto. 2010. Simulation of the grounding process in spoken dialog systems with bayesian networks. In IWSDS, pages 110–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Rossignol</author>
<author>Olivier Pietquin</author>
<author>Michel Ianotto</author>
</authors>
<title>Training a bn-based user model for dialogue simulation with missing data. In</title>
<date>2011</date>
<booktitle>IJCNLP,</booktitle>
<pages>598--604</pages>
<contexts>
<context position="9674" citStr="Rossignol et al., 2011" startWordPosition="1535" endWordPosition="1538">njunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s utterances in response to the Follower’s, at the semantic level. A Giver’s utterance takes the form: g = Instruct, u = MOVE (UNDER, LM) consisting of a dialog act g and a semantic unit u2. Aligned with u, is an ordered set of waypoints W, corresponding only to part of the route u describes. Figure 2(a) shows an example of such a sub-route. The point-set W can be seen as the Giver’s current goal on which they base their behaviour. Because the routes are drawn on the Giver’s maps, we treat W as observed. To model some of the int</context>
</contexts>
<marker>Rossignol, Pietquin, Ianotto, 2011</marker>
<rawString>St´ephane Rossignol, Olivier Pietquin, and Michel Ianotto. 2011. Training a bn-based user model for dialogue simulation with missing data. In IJCNLP, pages 598–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Roy</author>
<author>Joelle Pineau</author>
<author>Sebastian Thrun</author>
</authors>
<title>Spoken dialogue management using probabilistic reasoning.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00,</booktitle>
<pages>93--100</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1609" citStr="Roy et al., 2000" startWordPosition="242" endWordPosition="245">differ significantly from the behaviour of real users. 1 Introduction Automated dialog management is an area of research that has undergone rapid advancement in the last decade. The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et</context>
</contexts>
<marker>Roy, Pineau, Thrun, 2000</marker>
<rawString>Nicholas Roy, Joelle Pineau, and Sebastian Thrun. 2000. Spoken dialogue management using probabilistic reasoning. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 93–100, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Steve Young</author>
</authors>
<title>The hidden agenda user simulation model.</title>
<date>2009</date>
<journal>Audio, Speech, and Language Processing, IEEE Transactions on,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="9209" citStr="Schatzmann and Young, 2009" startWordPosition="1461" endWordPosition="1464">de a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task is to model the Giver’s utterances in response to the Follower’s, at the semantic level. A </context>
</contexts>
<marker>Schatzmann, Young, 2009</marker>
<rawString>Jost Schatzmann and Steve Young. 2009. The hidden agenda user simulation model. Audio, Speech, and Language Processing, IEEE Transactions on, 17(4):733–747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konrad Scheffler</author>
<author>Steve Young</author>
</authors>
<title>Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT</booktitle>
<contexts>
<context position="9119" citStr="Scheffler and Young, 2002" startWordPosition="1446" endWordPosition="1450">from their modelling. Although their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas (Scheffler and Young, 2002; Rieser and Lemon, 2006; Pietquin, 2006; Ai and Litman, 2007; Schatzmann and Young, 2009). However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states. An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them (Pietquin, 2004; Cuay´ahuitl et al., 2005; Pietquin and Dutoit, 2006; Rossignol et al., 2010; Rossignol et al., 2011; Eshky et al., 2012). 3 The Model Our task i</context>
</contexts>
<marker>Scheffler, Young, 2002</marker>
<rawString>Konrad Scheffler and Steve Young. 2002. Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. In Proceedings of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder Singh</author>
<author>Diane J Litman</author>
<author>Michael Kearns</author>
<author>Marilyn A Walker</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the njfun system.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>16--105</pages>
<contexts>
<context position="1629" citStr="Singh et al., 2002" startWordPosition="246" endWordPosition="249">ly from the behaviour of real users. 1 Introduction Automated dialog management is an area of research that has undergone rapid advancement in the last decade. The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigat</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>Satinder Singh, Diane J. Litman, Michael Kearns, and Marilyn A. Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the njfun system. Journal of Artificial Intelligence Research, 16:105–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
<author>Anne Anderson</author>
<author>Ellen G Bard</author>
<author>Gwyneth D Sneddon</author>
<author>Alison Newlands</author>
<author>Cathy Sotillo</author>
</authors>
<title>The HCRC Map Task corpus: natural dialogue for speech recognition.</title>
<date>1993</date>
<booktitle>In Proceedings of the workshop on Human Language Technology, HLT ’93,</booktitle>
<pages>25--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2011" citStr="Thompson et al., 1993" startWordPosition="304" endWordPosition="307">tistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigational domains present an interesting challenge, due to the disparity between the spatial goals and their grounding as utterances. This disparity renders much of the statistical management literature inapplicable. In this paper, we address this deficiency. We focus on the task of simulating user behaviour, both because of the important role simulators plays in the induction of dia</context>
</contexts>
<marker>Thompson, Anderson, Bard, Sneddon, Newlands, Sotillo, 1993</marker>
<rawString>Henry S. Thompson, Anne Anderson, Ellen G. Bard, Gwyneth D. Sneddon, Alison Newlands, and Cathy Sotillo. 1993. The HCRC Map Task corpus: natural dialogue for speech recognition. In Proceedings of the workshop on Human Language Technology, HLT ’93, pages 25–30, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>806--814</pages>
<publisher>The Association</publisher>
<institution>for Computer Linguistics.</institution>
<location>Uppsala,</location>
<contexts>
<context position="8464" citStr="Vogel and Jurafsky, 2010" startWordPosition="1344" endWordPosition="1347">y generating utterances from our model, and comparing them to held-out utterance of real humans in the test data. We also utilise human judgements for the task, where the judges score the output of the different models and the human utterances based on their suitability to a particular route. 2 Related Work 2.1 Related Work on the Map Task To our knowledge, there are no attempts to model instruction Givers as users in the Map Task domain. Two studies model the Follower, in the context of understanding natural language instructions and interpreting them by drawing a route (Levit and Roy, 2007; Vogel and Jurafsky, 2010). Both studies exclude dialog from their modelling. Although their work is not directly comparable to ours, they provide a corpus suitable for our task. 2.2 Related Work on User Simulation Early user simulation techniques are based on Ngrams (Eckert et al., 1997; Levin and Pieraccini, 2000; Georgila et al., 2005; Georgila et al., 2006), ensuring that simulator responses to a machine utterance are sensible locally. However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction wi</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Daniel Jurafsky. 2010. Learning to follow navigational directions. In ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden, pages 806–814. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1655" citStr="Williams and Young, 2007" startWordPosition="250" endWordPosition="253">r of real users. 1 Introduction Automated dialog management is an area of research that has undergone rapid advancement in the last decade. The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigational domains present an i</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D. Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
</authors>
<title>Applying pomdps to dialog systems in the troubleshooting domain.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies, NAACL-HLT ’07,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1823" citStr="Williams, 2007" startWordPosition="279" endWordPosition="280">has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1991; Thompson et al., 1993; Reitter and Moore, 2007), they have not been the subject of statistical dialog management research, and existing systems addressing navigational domains remain largely hand crafted (Janarthanam et al., 2013). Navigational domains present an interesting challenge, due to the disparity between the spatial goals and their grounding as utterances. This disparity renders much of the statistical management litera</context>
</contexts>
<marker>Williams, 2007</marker>
<rawString>Jason D. Williams. 2007. Applying pomdps to dialog systems in the troubleshooting domain. In Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies, NAACL-HLT ’07, pages 1–8, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gasic</author>
<author>Blaise Thomson</author>
<author>Jason D Williams</author>
</authors>
<title>Pomdp-based statistical spoken dialog systems: A review.</title>
<date>2013</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>101</volume>
<issue>5</issue>
<contexts>
<context position="1385" citStr="Young et al., 2013" startWordPosition="206" endWordPosition="209">using held-out probability and perplexity, and find a substantial reduction in uncertainty brought by our spatial representation. We evaluate extrinsically in a human judgement task and find that our model’s behaviour does not differ significantly from the behaviour of real users. 1 Introduction Automated dialog management is an area of research that has undergone rapid advancement in the last decade. The driving force of this innovation has been the rise of the statistical paradigm for monitoring dialog state, reasoning about the effects of possible dialog moves, and planning future actions (Young et al., 2013). Statistical dialog management treats conversations as Markov Decision Processes, where dialog moves are associated with a utility, estimated online by interacting with a simulated user (Levin et al., 1998; Roy et al., 2000; Singh et al., 2002; Williams and Young, 2007; Henderson and Lemon, 2008). Slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains (Williams, 2007) and relational domains (Lison, 2013). Although navigational dialogs have received much attention in studies of human conversational behaviour (Anderson et al., 1</context>
</contexts>
<marker>Young, Gasic, Thomson, Williams, 2013</marker>
<rawString>Steve Young, Milica Gasic, Blaise Thomson, and Jason D. Williams. 2013. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160–1179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>