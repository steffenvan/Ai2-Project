<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.2354905">
Dual Decomposition
for Natural Language Processing
Alexander M. Rush and Michael Collins
Decoding complexity
focus: decoding problem for natural language tasks
y∗ = arg max f (y)
Y
motivation:
</figure>
<listItem confidence="0.979851">
• richer model structure often leads to improved accuracy
• exact decoding for complex models tends to be intractable
</listItem>
<subsectionHeader confidence="0.471456">
Decoding tasks
</subsectionHeader>
<bodyText confidence="0.7219145">
many common problems are intractable to decode exactly
high complexity
</bodyText>
<listItem confidence="0.998391538461538">
• combined parsing and part-of-speech tagging (Rush et al.,
2010)
• “loopy” HMM part-of-speech tagging
• syntactic machine translation (Rush and Collins, 2011)
NP-Hard
• symmetric HMM alignment (DeNero and Macherey, 2011)
• phrase-based translation
• higher-order non-projective dependency parsing (Koo et al.,
2010)
in practice:
• approximate decoding methods (coarse-to-fine, beam search,
cube pruning, gibbs sampling, belief propagation)
• approximate models (mean field, variational models)
</listItem>
<sectionHeader confidence="0.279361" genericHeader="abstract">
Motivation
</sectionHeader>
<bodyText confidence="0.760517333333333">
cannot hope to find exact algorithms (particularly when NP-Hard)
aim: develop decoding algorithms with formal guarantees
method:
</bodyText>
<listItem confidence="0.9895696">
• derive fast algorithms that provide certificates of optimality
• show that for practical instances, these algorithms often yield
exact solutions
• provide strategies for improving solutions or finding
approximate solutions when no certificate is found
</listItem>
<bodyText confidence="0.803586">
dual decomposition helps us develop algorithms of this form
</bodyText>
<note confidence="0.765146">
Dual Decomposition (Komodakis et al., 2010; Lemar´echal, 2001)
goal: solve complicated optimization problem
y∗ = arg max f (y)
Y
</note>
<bodyText confidence="0.904904">
method: decompose into subproblems, solve iteratively
</bodyText>
<listItem confidence="0.844876866666667">
benefit: can choose decomposition to provide “easy” subproblems
aim for simple and efficient combinatorial algorithms
• dynamic programming
• minimum spanning tree
• shortest path
• min-cut
• bipartite match
• etc.
Related work
there are related methods used NLP with similar motivation
related methods:
• belief propagation (particularly max-product) (Smith and
Eisner, 2008)
• factored A* search (Klein and Manning, 2003)
• exact coarse-to-fine (Raphael, 2001)
</listItem>
<note confidence="0.671758">
aim to find exact solutions without exploring the full search space
Tutorial outline
focus:
</note>
<listItem confidence="0.986349375">
• developing dual decomposition algorithms for new NLP tasks
• understanding formal guarantees of the algorithms
• extensions to improve exactness and select solutions
outline:
1. worked algorithm for combined parsing and tagging
2. important theorems and formal derivation
3. more examples from parsing, sequence labeling, MT
4. practical considerations for implementing dual decomposition
5. relationship to linear programming relaxations
6. further variations and advanced examples
1. Worked example
aim: walk through a dual decomposition algorithm for combined
parsing and part-of-speech tagging
• introduce formal notation for parsing and tagging
• give assumptions necessary for decoding
• step through a run of the dual decomposition algorithm
</listItem>
<figure confidence="0.841943689655173">
Combined parsing and part-of-speech tagging
S
NP VP
N
United
V NP
flies D A N
some large jet
goal: find parse tree that optimizes
score(S → NP VP) + score(VP → V NP) +
... + score(Unitedl, N) + score(V, N) + ...
Constituency parsing
notation:
• Y is set of constituency parses for input
• y E Y is a valid parse
• f (y) scores a parse tree
goal:
arg max f (y)
YGY
example: a context-free grammar for constituency parsing
S
NP VP
N
United
V NP
flies D A N
some large jet
Part-of-speech tagging
notation:
</figure>
<listItem confidence="0.987253333333333">
• Z is set of tag sequences for input
• z ∈ Z is a valid tag sequence
• g(z) scores of a tag sequence
</listItem>
<equation confidence="0.523898333333333">
goal:
arg maxg(z)
zCZ
example: an HMM for part-of speech tagging
N V D A N
United1 flies2 some3 large4 jets
</equation>
<bodyText confidence="0.483262">
Identifying tags
notation: identify the tag labels selected by each model
</bodyText>
<listItem confidence="0.949392333333333">
• y(i, t) = 1 when parse y selects tag t at position i
• z(i, t) = 1 when tag sequence z selects tag t at position i
example: a parse and tagging with y(4, A) = 1 and z(4, A) = 1
</listItem>
<figure confidence="0.921710777777778">
S
some
large
jet
N V D A N
United1 flies2 some3 large4 jets
z
y
NP
N
United
flies
V
VP
D
NP
N
A
Combined optimization
goal: arg max f (y) + g(z)
YGY,ZGZ
such that for all i = 1... n, t E T ,
y(i, t) = z(i, t)
i.e. find the best parse and tagging pair that agree on tag labels
equivalent formulation:
arg max f (y) + g(l(y))
YGY
</figure>
<bodyText confidence="0.763588">
where l : Y —+ i extracts the tag sequence from a parse tree
</bodyText>
<subsectionHeader confidence="0.334574">
Dynamic programming intersection
</subsectionHeader>
<bodyText confidence="0.602635">
can solve by solving the product of the two models
example:
</bodyText>
<listItem confidence="0.999089333333333">
• parsing model is a context-free grammar
• tagging model is a first-order HMM
• can solve as CFG and finite-state automata intersection
</listItem>
<figure confidence="0.906071277777778">
S
replace S —+ NP VP with
SN,N —+ NPN,V VPV,N
NP
N
United
VP
V NP
flies D A N
some large jet
Parsing assumption
the structure of Y is open (could be CFG, TAG, etc.)
assumption: optimization with u can be solved efficiently
arg max X u(i, t)y(i, t)
y∈Y f (y) +
i,t
generally benign since u can be incorporated into the structure of f
example: CFG with rule scoring function h
</figure>
<equation confidence="0.932497261904762">
f (y) = X h(X � Y Z) + X h(X -+ wi)
X→Y Z∈y (i,X)∈y
where
arg maxy∈Y f (y) + X u(i, t)y(i, t) =
i,t
arg maxy∈Y X h(X � Y Z) + X (h(X —� wi) + u(i, X))
X→Y Z∈y (i,X)∈y
Tagging assumption
we make a similar assumption for the set Z
assumption: optimization with u can be solved efficiently
X u(i, t)z(i, t)
arg maxg(z) −
z∈Z
i,t
example: HMM with scores for transitions T and observations O
g(z) = X T(t � t0) + X O(t -+ wi)
t→t&apos;∈z (i,t)∈z
where
Xarg maxz∈Z g(z) − u(i, t)z(i, t) =
i,t
Xarg maxz∈Z T(t � t0) + X (O(t -+ wi) − u(i, t))
t→t&apos;∈z (i,t)∈z
Dual decomposition algorithm
Set u(1)(i, t) = 0 for all i, t ∈ T
For k = 1 to K
y(k) ← arg max �
yGY f (y) +
i,t
�
z(k) ← arg maxg(z) −
zGZ
i,t
u(k)(i, t)y(i, t) [Parsing]
u(k)(i, t)z(i, t) [Tagging]
If y(k)(i, t) = z(k)(i, t) for all i, t Return (y(k), z(k))
Else u(k+1)(i, t) ← u(k)(i, t) − αk(y(k)(i, t) − z(k)(i, t))
Algorithm step-by-step
[Animation]
Main theorem
theorem: if at any iteration, for all i, t ∈ T
y(k)(i, t) = z(k)(i, t)
then (y(k), z(k)) is the global optimum
</equation>
<listItem confidence="0.901155">
proof: focus of the next section
2. Formal properties
aim: formal derivation of the algorithm given in the previous
section
• derive Lagrangian dual
• prove three properties
► upper bound
► convergence
► optimality
• describe subgradient method
</listItem>
<equation confidence="0.947905617647059">
Lagrangian
goal: arg max f (y) + g(z) such that y(i, t) = z(i, t)
yGY,zGZ
Lagrangian:
L(u, y, z) = f (y) + g(z) + E u(i, t) (y(i, t) − z(i, t))
i,t
redistribute terms
L(u, y, z) = I f (y) + E u(i, t)y(i, t) I + I g(z) − E u(i, t)z(i, t)
i,t / i,t
Lagrangian dual
Lagrangian:
⎛ ⎞ ⎛
E E
L(u, y, z) = ⎝f (y) + u(i, t)y(i, t) ⎠ + ⎝g(z) − u(i, t)z(i, t)
i,t i,t
Lagrangian dual:
L(u) = max L(u, y, z)
yGY,zGZ
⎛ ⎞
E
⎝f (y) + u(i, t)y(i, t) ⎠+
i,t
= max
yGY
⎛
⎝g(z)E
− u(i, t)z(i, t)
max
zGZ
i,t
Theorem 1. Upper bound
define:
• y∗, z∗ is the optimal combined parsing and tagging solution
with y∗(i, t) = z∗(i, t) for all i, t
</equation>
<figure confidence="0.895942714285714">
theorem: for any value of u
L(u) &gt; f (y∗) + g(z∗)
L(u) provides an upper bound on the score of the optimal solution
note: upper bound may be useful as input to branch and bound or
A* search
Theorem 1. Upper bound (proof)
theorem: for any value of u, L(u) &gt; f (y∗) + g(z∗)
proof:
L(u) = max L(u, y, z) (1)
y∈Y,z∈Z
&gt; max L(u, y, z) (2)
y∈Y,z∈Z:y=z f (y) + g(z) (3)
= max
y∈Y,z∈Z:y=z
</figure>
<equation confidence="0.8371790625">
= f (y∗) + g(z∗) (4)
Formal algorithm (reminder)
Set u(1)(i, t) = 0 for all i, t E T
For k = 1 to K
y(k) +- arg max �
yEY f (y) +
i,t
�
z(k) +- arg maxg(z) −
zEZ
i,t
u(k)(i, t)y(i, t) [Parsing]
u(k)(i, t)z(i, t) [Tagging]
If y(k)(i, t) = z(k)(i, t) for all i, t Return (y(k), z(k))
Else u(k+1)(i, t) +- u(k)(i, t) − αk(y(k)(i, t) − z(k)(i, t))
Theorem 2. Convergence
</equation>
<bodyText confidence="0.315198">
notation:
</bodyText>
<listItem confidence="0.9397005">
• u(k+1)(i, t) +- u(k)(i, t) + αk(y(k)(i, t) − z(k)(i, t)) is update
• u(k) is the penalty vector at iteration k
• αk is the update rate at iteration k
theorem: for any sequence α1, α2, α3, ... such that
</listItem>
<figure confidence="0.9601564">
lim αt = 0 and oo αt = oo,
t-+oo E
t=1
we have lim L(ut) = min L(u)
t-+oo u
i.e. the algorithm converges to the tightest possible upper bound
proof: by subgradient convergence (next section)
Dual solutions
define: ⎛
• for any value of u E
yu = arg max ⎝f (y) + u(i, t)y(i, t)
yCY i,t
and
⎛
⎝g(z)E
</figure>
<equation confidence="0.706949">
− u(i, t)z(i, t)
i,t
zu = arg max
zCZ
</equation>
<listItem confidence="0.779018">
• yu and zu are the dual solutions for a given u
</listItem>
<equation confidence="0.920746024390244">
Theorem 3. Optimality
theorem: if there exists u such that
yu(i, t) = zu(i, t)
for all i, t then
f (yu) + g(zu) = f (y∗) + g(z∗)
i.e. if the dual solutions agree, we have an optimal solution
(yu, zu)
Theorem 3. Optimality (proof)
theorem: if u such that yu(i, t) = zu(i, t) for all i, t then
f (yu) + g(zu) = f (y*) + g(z*)
proof: by the definitions of yu and zu
L(u) = f (yu) + g(zu) + E u(i, t)(yu(i, t) − zu(i, t))
i,t
= f (yu) + g(zu)
since L(u) &gt; f (y*) + g(z*) for all values of u
f (yu) + g(zu) &gt; f (y*) + g(z*)
but y* and z* are optimal
f (yu) + g(zu) ≤ f (y*) + g(z*)
Dual optimization
Lagrangian dual: L(u, y, z)
L(u) = max
yEY,zEZ
⎛ ⎞
E
⎝f (y) + u(i, t)y(i, t) ⎠+
i,t
= max
yEY
max ⎛
zEZ ⎝g(z)E
− u(i, t)z(i, t)
i,t
goal: dual problem is to find the tightest upper bound
min
u
L(u)
Dual subgradient
L(u) = max
y∈Y
⎛ ⎞ ⎛
f (y) + 1: u(i, t)y(i, t) + m �⎝g(z) − 1: u(i, t)z(i, t)
</equation>
<figure confidence="0.613755344827586">
i,ti,t
(because of max operator)
handle non-differentiability
by using subgradient descent
define: a subgradient of L(u) at u is a vector gu such that for all v
L(v) ≥ L(u) + gu · (v − u)
Subgradient algorithm
L(u) = max f
y∈Y ) + 1: u(i, t)
(i, t) + m� g(
)
1: u(i, t)z(i
⎛ ⎞ ⎛
y
y
z
−
, t)
i,t i,j
recall, yu and zu are the argmax’s of the two terms
subgradient:
properties:
• L(u) is convex in u (no local minima)
• L(u) is not differentiable
gu(i, t) = yu(i, t) − zu(i, t)
subgradient descent: move along the subgradient
u�(i, t) = u(i, t) − α (yu(i, t) − zu(i, t))
guaranteed to find a minimum with conditions given earlier for α
3. More examples
</figure>
<bodyText confidence="0.2136835">
aim: demonstrate similar algorithms that can be applied to other
decoding applications
</bodyText>
<listItem confidence="0.785679">
• context-free parsing combined with dependency parsing
• corpus-level part-of-speech tagging
• combined translation alignment
Combined constituency and dependency parsing
setup: assume separate models trained for constituency and
dependency parsing
problem: find constituency parse that maximizes the sum of the
two models
example:
• combine lexicalized CFG with second-order dependency parser
Lexicalized constituency parsing
notation:
• Y is set of lexicalized constituency parses for input
• y E Y is a valid parse
• f (y) scores a parse tree
</listItem>
<figure confidence="0.91624725">
goal:
arg max f (y)
YGY
example: a lexicalized context-free grammar
S(flies)
NP(United)
N
United
some large jet
Dependency parsing
define:
• Z is set of dependency parses for input
• z E Z is a valid dependency parse
• g(z) scores a dependency parse
example:
*0 United1 flies2 some3 large4 jets
VP(flies)
NP(jet)
V
flies
N
A
D
Identifying dependencies
</figure>
<listItem confidence="0.847251">
notation: identify the dependencies selected by each model
• y(i, j) = 1 when constituency parse y selects word i as a
modifier of word j
• z(i, j) = 1 when dependency parse z selects word i as a
modifier of word j
example: a constituency and dependency parse with y(3,5) = 1
and z(3,5) = 1
</listItem>
<figure confidence="0.983733210526316">
S(flies)
NP(United) VP(flies)
N V
United
*0 United1 flies2 some3 large4 jets
NP(jet)
flies
D N
A
some large jet z
y
Combined optimization
goal: arg max f (y) + g(z)
YEY,ZEZ
such that for all i = 1... n, j = 0 ... n,
y(i, j) = z(i,j)
Algorithm step-by-step
[Animation]
Corpus-level tagging
</figure>
<bodyText confidence="0.5722325">
setup: given a corpus of sentences and a trained sentence-level
tagging model
problem: find best tagging for each sentence, while at the same
time enforcing inter-sentence soft constraints
</bodyText>
<listItem confidence="0.833799666666667">
example:
• test-time decoding with a trigram tagger
• constraint that each word type prefer a single POS tag
</listItem>
<bodyText confidence="0.82669925">
Corpus-level tagging
full model for corpus-level tagging
Man is the best measure
Sentence-level decoding
</bodyText>
<listItem confidence="0.8992244">
notation:
• Y; is set of tag sequences for input sentence i
• Y = Y1 x ... x Ym is set of tag sequences for the input corpus
• Y E Y is a valid tag sequence for the corpus
•
</listItem>
<figure confidence="0.926719375">
f (Y;) is the score for tagging the whole corpus
goal:
arg max F(Y )
YGY
example: decode each sentence with a trigram tagger
The smart man stood outside
He saw an American man
N
an
N
man
D
He
saw
A
The
smart
P
V
D
man
American
R
stood
F(Y ) =
;
outside
N
A
V
Inter-sentence constraints
notation:
</figure>
<listItem confidence="0.99426225">
• Z is set of possible assignments of tags to word types
• z ∈ Z is a valid tag assignment
• g(z) is a scoring function for assignments to word types
(e.g. a hard constraint - all word types only have one tag)
</listItem>
<bodyText confidence="0.58469">
example: an MRF model that encourages words of the same type
to choose the same tag
</bodyText>
<equation confidence="0.948761076923077">
z1
man man man
N
N
N
N
z2
man man man
N
N
N
A
g(z1) &gt; g(z2)
</equation>
<bodyText confidence="0.7337075">
Identifying word tags
notation: identify the tag labels selected by each model
</bodyText>
<listItem confidence="0.786378833333333">
• Y,(i, t) = 1 when the tagger for sentence s at position i
selects tag t
• z(s, i, t) = 1 when the constraint assigns at sentence s
position i the tag t
example: a parse and tagging with Y1(5, N) = 1 and
z(1, 5, N) = 1
</listItem>
<figure confidence="0.745632136363636">
He saw an American man
The smart man stood outside
Y
man man man
z
Combined optimization
goal: arg max F(Y ) + g(z)
Y EY,ZEZ
such that for all s = 1... m, i = 1... n, t ∈ T ,
YS(i, t) = z(s, i, t)
Algorithm step-by-step
[Animation]
Combined alignment (DeNero and Macherey, 2011)
setup: assume separate models trained for English-to-French and
French-to-English alignment
problem: find an alignment that maximizes the score of both
models with soft agreement
example:
• HMM models for both directional alignments (assume correct
alignment is one-to-one for simplicity)
English-to-French alignment
define:
</figure>
<listItem confidence="0.968706333333333">
• Y is set of all possible English-to-French alignments
• y ∈ Y is a valid alignment
• f (y) scores of the alignment
</listItem>
<equation confidence="0.505406">
example: HMM alignment
1 3 2 4 6 5
The1 u9ly2 do93 has4 reds fur6
French-to-English alignment
define:
</equation>
<listItem confidence="0.984441">
• Z is set of all possible French-to-English alignments
• z ∈ Z is a valid alignment
• g(z) scores of an alignment
</listItem>
<equation confidence="0.838266666666667">
example: HMM alignment
1 2 3 4 6 5
Le1 chien2 laid3 a4 fourrure5 rouge6
</equation>
<bodyText confidence="0.691036">
Identifying word alignments
notation: identify the tag labels selected by each model
</bodyText>
<listItem confidence="0.98539075">
• y(i, j) = 1 when e-to-f alignment y selects French word i to
align with English word j
• z(i, j) = 1 when f-to-e alignment z selects French word i to
align with English word j
</listItem>
<equation confidence="0.973684285714286">
example: two HMM alignment models with y(6,5) = 1 and
z(6,5) = 1
1 3 2 4 6 5
The1 u9ly2 do93 has4 red5 fur6
y
1 2 3 4 6 5
Le1 chien2 laid3 a4 fourrure5 rouge6
</equation>
<figure confidence="0.745250888888889">
z
Combined optimization
goal: arg max f (y) + g(z)
YEY,ZEZ
such that for all i = 1... n, j = 1... n,
y(i, j) = z(i, j)
Algorithm step-by-step
[Animation]
4. Practical issues
</figure>
<listItem confidence="0.877045692307692">
aim: overview of practical dual decomposition techniques
• tracking the progress of the algorithm
• extracting solutions if algorithm does not converge
• lazy update of dual solutions
Tracking progress
at each stage of the algorithm there are several useful values
track:
• y(k), z(k) are current dual solutions
• L(u(k)) is the current dual value
• y(k), l(y(k)) is a potential primal feasible solution
• f (y(k)) + g(l(y(k))) is the potential primal value
useful signals:
• L(u(k)) − L(u(k−1)) is the dual change (may be positive)
</listItem>
<figure confidence="0.784123666666667">
• min L(u(k)) is the best dual value (tightest upper bound)
k
• max f (y(k)) + g(l(y(k))) is the best primal value
k
the optimal value must be between the best dual and primal values
Approximate solution
</figure>
<bodyText confidence="0.949393">
upon agreement the solution is exact, but this may not occur
otherwise, there is an easy way to find an approximate solution
choose: the structure y(kf) where
</bodyText>
<equation confidence="0.8656785">
k&apos; = arg max f (y(k)) + g(l(y(k)))
k
</equation>
<bodyText confidence="0.56754">
is the iteration with the best primal score
guarantee: the solution yk, is non-optimal by at most
</bodyText>
<equation confidence="0.8638535">
(min L(ut)) − (f (y(k&apos;)) + g(l(y(k&apos;))))
t
</equation>
<bodyText confidence="0.854267166666667">
there are other methods to estimate solutions, for instance by
averaging solutions (see Nedi´c and Ozdaglar (2009))
Lazy decoding
idea: don’t recompute y(k) or z(k) from scratch each iteration
lazy decoding: if subgradient u(k) is sparse, then y(k) may be
very easy to compute from y(k−1)
</bodyText>
<listItem confidence="0.84319755">
use:
• very helpful if y or z factors naturally into several parts
• decompositions with this property are very fast in practice
example:
• in corpus-level tagging, only need to recompute sentences
with a word type that received an update
5. Linear programming
aim: explore the connections between dual decomposition and
linear programming
• basic optimization over the simplex
• formal properties of linear programming
• full example with fractional optimal solutions
• tightening linear program relaxations
Simplex
define:
• Dy is the simplex over Y where α E Dy implies
�αy ≥ 0 and αy = 1
y
• Dz is the simplex over i
• δy : Y → Dy maps elements to the simplex
</listItem>
<figure confidence="0.8073335625">
example:
Y = {y1, y2, y3}
vertices
• δy(y1) = (1, 0, 0)
• δy(y2) = (0, 1, 0)
• δy(y3) = (0,0,1)
δy(y1)
Dy
δy(y2) δy(y3)
Linear programming
optimize over the simplices Oy and Oz instead of the discrete sets
Y and Z
goal: optimize linear program
Emax E βzg(z)
αEOy,QEOz y αyf (y) +
z
</figure>
<bodyText confidence="0.596029">
such that for all i, t
</bodyText>
<equation confidence="0.974517055555556">
E αyy(i, t) = E βzz(i, t)
y z
Lagrangian
Lagrangian:
XM(u, α, β) = X X u(i, t) X X !βzz(i, t)
y αyf (y) + βzg(z) + y αyy(i, t) −
z i,t z
X= X X !αyy(i, t) +
y αyf (y) + u(i, t)
i,t y
X βzg(z) − X
z i,t
Lagrangian dual:
X
u(i, t)
z
!βzz(i, t)
M(u) = max M(u, α, β)
</equation>
<figure confidence="0.962182693877551">
αEOy,QEOz
Strong duality
define:
• α*„ Q* is the optimal assignment to α„ Q in the linear program
theorem:
min � � Q*Zg(z)
U M(u) = α*Yf (y) +
Y Z
proof: by linear programming duality
Dual relationship
theorem: for any value of u,
M(u) = L(u)
note: solving the original Lagrangian dual also solves dual of the
linear program
Primal relationship
define:
• 2 C Oy x Oz corresponds to feasible solutions of the original
problem
2 = {(δy(y), δz(z)): y E Y, z E Z,
y(i, t) = z(i, t) for all (i, t)1
• 20 C Oy x Oz is the set of feasible solutions to the LP
20 = {(α,β): α E OY,β E OZ,
Ey αyy(i, t) = Ez βzz(i, t) for all (i, t)1
• 2C 20
solutions:
max h(q) &lt; max h(q) for any h
q∈Q q∈Q�
Concrete example
• Y = {y1, y2, y31
• Z = {z1, z2, z31
• Oy C R3, Oz C R3
y1 y2 y3
x
a a
He is
z1
z2
x
c c
He is
z3
x
b
is
b
He
b
He
a
is
c
He
c
is
a
He
b
is
Y
Z
Simple solution
y1 y2 y3
Y
Z
x
a a
He is
z1
z2
z3
a
He
b
is
x
b
is
b
He
b
He
a
is
x
c
c
He
is
c
He
c
is
choose:
• α(1) = (0, 0,1) E Oy is representation of y3
• β(1) = (0, 0,1) E Oz is representation of z3
confirm:
� α(y1)y(i , t) = β(1)
y z z z(i, t)
α(1) and β(1) satisfy agreement constraint
Fractional solution
y1 y2 y3
Y
Z
x
a a
He is
z1
z2
x
c c
He is
z3
a
He
b
is
x
b
is
b
He
b
He
a
is
c
He
c
is
choose:
• α(2) = (0.5, 0.5, 0) E Oy is combination of y1 and y2
• β(2) = (0.5, 0.5, 0) E Oz is combination of z1 and z2
confirm:
� α(y2)y(i,t) = β(2)
y z z z(i, t)
α(2) and β(2) satisfy agreement constraint, but not integral
Optimal solution
weights:
• the choice of f and g determines the optimal solution
• if (f , g) favors (α(2),β(2)), the optimal solution is fractional
example: f = [1 1 2] and g = [1 1 − 2]
• f · α(1) + g · β(1) = 0 vs f · α(2) + g · β(2) = 2
• α(2), β(2) is optimal, even though it is fractional
Algorithm run
[Animation]
Tightening (Sherali and Adams, 1994; Sontag et al., 2008)
modify:
</figure>
<listItem confidence="0.989238">
• extend Y, i to identify bigrams of part-of-speech tags
• y(i, t1, t2) = 1 y(i, t1) = 1 and y(i + 1, t2) = 1
• z(i, t1, t2) = 1 z(i, t1) = 1 and z(i + 1, t2) = 1
all bigram constraints: valid to add for all i, t1, t2 E T
</listItem>
<equation confidence="0.8525985">
X αyy(i, t1, t2) = X βzz(i, t1, t2)
y z
</equation>
<bodyText confidence="0.858895">
however this would make decoding expensive
single bigram constraint: cheaper to implement
</bodyText>
<figure confidence="0.761709">
X αyy(1, a, b) = X βzz(1, a, b)
y z
the solution α(1),β(1) trivially passes this constraint, while
α(2), β(2) violates it
Dual decomposition with tightening
tightened decomposition includes an additional Lagrange multiplier
yu,v = arg max X
yEY f (y) +
i,t
X
zu,v = arg maxg(z) −
zEZ
i,t
</figure>
<construct confidence="0.5188985">
u(i, t)y(i, t) + v(1, a, b)y(1, a, b)
u(i, t)z(i, t) − v(1, a, b)z(1, a, b)
</construct>
<listItem confidence="0.6403846">
in general, this term can make the decoding problem more difficult
example:
• for small examples, these penalties are easy to compute
• for CFG parsing, need to include extra states that maintain
tag bigrams (still faster than full intersection)
</listItem>
<bodyText confidence="0.441546">
Tightening step-by-step
[Animation]
</bodyText>
<sectionHeader confidence="0.598004" genericHeader="keywords">
6. Advanced examples
</sectionHeader>
<bodyText confidence="0.725316">
aim: demonstrate some different relaxation techniques
</bodyText>
<listItem confidence="0.994893">
• higher-order non-projective dependency parsing
• syntactic machine translation
</listItem>
<bodyText confidence="0.866603666666667">
Higher-order non-projective dependency parsing
setup: given a model for higher-order non-projective dependency
parsing (sibling features)
problem: find non-projective dependency parse that maximizes the
score of this model
difficulty:
</bodyText>
<listItem confidence="0.971525666666667">
• model is NP-hard to decode
• complexity of the model comes from enforcing combinatorial
constraints
</listItem>
<bodyText confidence="0.82792475">
strategy: design a decomposition that separates combinatorial
constraints from direct implementation of the scoring function
Non-projective dependency parsing
structure:
</bodyText>
<listItem confidence="0.9937275">
• starts at the root symbol *
• each word has a exactly one parent word
• produces a tree structure (no cycles)
• dependencies can cross
</listItem>
<figure confidence="0.6559358">
example:
*0 John, saw2 a3 movie4 today5 that6 he7 liked$
*0 John, saw2 a3 movie4 today5 that6 he7 liked$
Arc-Factored
*0 John, saw2 a3 movie4 today5 that6 he7 liked$
</figure>
<equation confidence="0.948746269230769">
f (y) = score(head =*0, mod =saw2) +score(saw2, John,)
+score(saw2, movie4) +score(saw2, today5)
+score(movie4, a3) + ...
e.g. score(*0, saw2) = log p(saw2|*0) (generative model)
or score(*0, saw2) = w · φ(saw2, *0) (CRF/perceptron model)
y* = arg max f (y) -�-- Minimum Spanning Tree Algorithm
y
Sibling models
*0 John, saw2 a3 movie4 today5 that6 he7 liked$
f (y) = score(head = *0, prev = NULL, mod = saw2)
+score(saw2, NULL, John,)+score(saw2, NULL, movie4)
+score(saw2,movie4, today5) + ...
e.g. score(saw2, movie4, today5) = log p(today5|saw2, movie4)
or score(saw2, movie4, today5) = w · φ(saw2, movie4, today5)
y* = arg max
y
f (y) -�-- NP-Hard
Thought experiment: individual decoding
*0 John1 saw2 a3 movie4 today5 that6 he7 liked$
score(saw2, NULL, John1) + score(saw2, NULL, movie4)
+score(saw2, movie4, today5)
score(saw2, NULL, John1) + score(saw2, NULL, that6)
score(saw2, NULL, a3) + score(saw2, a3, he7)
under sibling model, can solve for each word with Viterbi decoding.
Thought experiment continued
*0 John1 saw2 a3 movie4 today5 that6 he7 liked$
</equation>
<bodyText confidence="0.9948425">
idea: do individual decoding for each head word using dynamic
programming
if we’re lucky, we’ll end up with a valid final tree
but we might violate some constraints
</bodyText>
<figure confidence="0.947618583333333">
2n−1
possibilities
Dual decomposition structure
goal: y∗ = arg max f (y)
rewrite: yEY
arg max f (y) + g(z)
yE Y zE Z,
such that for all i, j
y(i, j) = z(i, j)
Algorithm step-by-step
[Animation]
Syntactic translation decoding
</figure>
<bodyText confidence="0.78914875">
setup: assume a trained model for syntactic machine translation
problem: find best derivation that maximizes the score of this
model
difficulty:
</bodyText>
<listItem confidence="0.975918333333333">
• need to incorporate language model in decoding
• empirically, relaxation is often not tight, so dual
decomposition does not always converge
strategy:
• use a different relaxation to handle language model
• incrementally add constraints to find exact solution
</listItem>
<bodyText confidence="0.6069725">
Syntactic translation example
[Animation]
</bodyText>
<sectionHeader confidence="0.729644" genericHeader="introduction">
Summary
</sectionHeader>
<bodyText confidence="0.935236">
presented dual decomposition as a method for decoding in NLP
formal guarantees
</bodyText>
<listItem confidence="0.869605">
• gives certificate or approximate solution
• can improve approximate solutions by tightening relaxation
efficient algorithms
• uses fast combinatorial algorithms
• can improve speed with lazy decoding
widely applicable
• demonstrated algorithms for a wide range of NLP tasks
(parsing, tagging, alignment, mt decoding)
</listItem>
<sectionHeader confidence="0.993563" genericHeader="method">
References I
</sectionHeader>
<reference confidence="0.999486333333333">
J. DeNero and K. Macherey. Model-Based Aligner Combination
Using Dual Decomposition. In Proc. ACL, 2011.
D. Klein and C.D. Manning. Factored A* Search for Models over
Sequences and Trees. In Proc IJCAI, volume 18, pages
1246–1251. Citeseer, 2003.
N. Komodakis, N. Paragios, and G. Tziritas. Mrf energy
minimization and beyond via dual decomposition. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2010. ISSN 0162-8828.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola,
and David Sontag. Dual decomposition for parsing with
non-projective head automata. In EMNLP, 2010. URL
http://www.aclweb.org/anthology/D10-1125.
B.H. Korte and J. Vygen. Combinatorial Optimization: Theory and
Algorithms. Springer Verlag, 2008.
</reference>
<sectionHeader confidence="0.549714" genericHeader="method">
References II
</sectionHeader>
<reference confidence="0.9997015">
C. Lemar´echal. Lagrangian Relaxation. In Computational
Combinatorial Optimization, Optimal or Provably Near-Optimal
Solutions [based on a Spring School], pages 112–156, London,
UK, 2001. Springer-Verlag. ISBN 3-540-42877-1.
Angelia Nedi´c and Asuman Ozdaglar. Approximate primal
solutions and rate analysis for dual subgradient methods. SIAM
Journal on Optimization, 19(4):1757–1780, 2009.
Christopher Raphael. Coarse-to-fine dynamic programming. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 23:
1379–1390, 2001.
A.M. Rush and M. Collins. Exact Decoding of Syntactic
Translation Models through Lagrangian Relaxation. In Proc.
ACL, 2011.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. On Dual
Decomposition and Linear Programming Relaxations for Natural
Language Processing. In Proc. EMNLP, 2010.
</reference>
<sectionHeader confidence="0.755938" genericHeader="method">
References III
</sectionHeader>
<reference confidence="0.8419223">
Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations
and convex hull characterizations for mixed-integer zero–one
programming problems. Discrete Applied Mathematics, 52(1):83
– 106, 1994.
D.A. Smith and J. Eisner. Dependency Parsing by Belief
Propagation. In Proc. EMNLP, pages 145–156, 2008. URL
http://www.aclweb.org/anthology/D08-1016.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss.
Tightening LP relaxations for MAP using message passing. In
Proc. UAI, 2008.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9918905">Dual for Natural Language Processing</title>
<author confidence="0.997737">Alexander M Rush</author>
<author confidence="0.997737">Michael Collins</author>
<abstract confidence="0.990471913043478">Decoding complexity problem for natural language tasks arg max Y motivation: • richer model structure often leads to improved accuracy • exact decoding for complex models tends to be intractable Decoding tasks many common problems are intractable to decode exactly high complexity • combined parsing and part-of-speech tagging (Rush et al., 2010) • “loopy” HMM part-of-speech tagging • syntactic machine translation (Rush and Collins, 2011) NP-Hard • symmetric HMM alignment (DeNero and Macherey, 2011) • phrase-based translation • higher-order non-projective dependency parsing (Koo et al., 2010) in practice: • approximate decoding methods (coarse-to-fine, beam search, cube pruning, gibbs sampling, belief propagation) • approximate models (mean field, variational models) Motivation cannot hope to find exact algorithms (particularly when NP-Hard) decoding algorithms with formal guarantees method: • derive fast algorithms that provide certificates of optimality • show that for practical instances, these algorithms often yield exact solutions • provide strategies for improving solutions or finding approximate solutions when no certificate is found dual decomposition helps us develop algorithms of this form Decomposition et al., 2010; Lemar´echal, 2001) complicated optimization problem arg max Y into subproblems, solve iteratively can choose decomposition to provide “easy” subproblems aim for simple and efficient combinatorial algorithms • dynamic programming • minimum spanning tree • shortest path • min-cut • bipartite match • etc.</abstract>
<note confidence="0.720991857142857">Related work there are related methods used NLP with similar motivation related methods: • belief propagation (particularly max-product) (Smith and Eisner, 2008) • factored A* search (Klein and Manning, 2003) • exact coarse-to-fine (Raphael, 2001)</note>
<abstract confidence="0.958404848484848">aim to find exact solutions without exploring the full search space Tutorial outline focus: • developing dual decomposition algorithms for new NLP tasks • understanding formal guarantees of the algorithms • extensions to improve exactness and select solutions outline: worked algorithmfor combined parsing and tagging important theorems andformal derivation 3. moreexamplesfrom parsing, sequence labeling, MT practical considerationsfor implementing dual decomposition relationship tolinear programmingrelaxations further variations andadvanced examples 1. Worked example through a dual decomposition algorithm for combined parsing and part-of-speech tagging • introduce formal notation for parsing and tagging • give assumptions necessary for decoding • step through a run of the dual decomposition algorithm Combined parsing and part-of-speech tagging S United VP V NP flies D A N some large jet parse tree that optimizes + + + + Constituency parsing notation: Y set of constituency parses for input y Y a valid parse f scores a parse tree goal: arg max context-free grammar for constituency parsing S N VP United V NP flies D A N some large jet Part-of-speech tagging notation: Z set of tag sequences for input z Z a valid tag sequence • scores of a tag sequence goal: zCZ HMM for part-of speech tagging N V D A N Identifying tags the tag labels selected by each model • = 1 when parse tag position • = 1 when tag sequence tag position parse and tagging with = 1 and = 1 S some large jet V DAN z y United flies V</abstract>
<title confidence="0.922371333333333">VP D NP N A Combined optimization</title>
<abstract confidence="0.972377207951071">goal: arg max + that for all T = i.e. find the best parse and tagging pair that agree on tag labels equivalent formulation: arg max + —+ i the tag sequence from a parse tree Dynamic programming intersection can solve by solving the product of the two models example: • parsing model is a context-free grammar • tagging model is a first-order HMM • can solve as CFG and finite-state automata intersection S VP N United VP D A some large jet Parsing assumption structure of open (could be CFG, TAG, etc.) with be solved efficiently arg max X + i,t benign since be incorporated into the structure of with rule scoring function = X + X where f + X= i,t X + X + Tagging assumption make a similar assumption for the set with be solved efficiently X with scores for transitions observations = X + X where = i,t + X Dual decomposition algorithm = 0 for all T 1 max � + i,t � i,t [Parsing] [Tagging] = for all Algorithm step-by-step [Animation] Main theorem at any iteration, for all i, t T = is the global optimum of the next section 2. Formal properties derivation of the algorithm given in the previous section • derive Lagrangian dual • prove three properties ► upper bound ► convergence ► optimality • describe subgradient method Lagrangian goal: arg max (y) + g(z) such that = Lagrangian: = f (y) + g(z) + E i,t redistribute terms = (y) + Lagrangian dual Lagrangian: ⎛ ⎞ ⎛ E = (y) + i,t i,t Lagrangian dual: L(u) = max ⎛ ⎞ E (y) + i,t = max ⎛ max i,t Theorem 1. Upper bound define: • is the optimal combined parsing and tagging solution = for all any value of + provides an upper bound on the score of the optimal solution bound may be useful as input to branch and bound or A* search Theorem 1. Upper bound (proof) any value of + proof: = max (1) y∈Y,z∈Z (2) y∈Y,z∈Z:y=z + (3) = max y∈Y,z∈Z:y=z + (4) Formal algorithm (reminder) = 0 for all T 1 max � + i,t � i,t [Parsing] [Tagging] = for all Theorem 2. Convergence notation: • + is update • the penalty vector at iteration • the update rate at iteration any sequence ... that lim 0 and oo E t=1 we have lim = min u i.e. the algorithm converges to the tightest possible upper bound subgradient convergence (next section) Dual solutions define: ⎛ • for any value of u E = arg max (y) + yCY i,t and ⎛ i,t = arg max zCZ • and are the dual solutions for a given u Theorem 3. Optimality there exists u such that = all then + = f + i.e. if the dual solutions agree, we have an optimal solution Theorem 3. Optimality (proof) u such that = for all then + = f + the definitions of and = f + + E i,t f + L(u) + for all values of u + + and are optimal + + Dual optimization Lagrangian dual: L(u) = max yEY,zEZ ⎛ ⎞ E (y) + i,t = max yEY max ⎛ zEZ i,t problem is to find the tightest upper bound min u L(u) Dual subgradient L(u) = max y∈Y ⎛ ⎞ ⎛ (y) + + m i,ti,t (because of max operator) handle non-differentiability by using subgradient descent subgradient of L(u) at u is a vector such that for all v + · Subgradient algorithm L(u) = max f y∈Y + t) + m� g( ) 1: t)z(i ⎛ ⎞ ⎛ and are the argmax’s of the two terms subgradient: properties: • L(u) is convex in u (no local minima) • L(u) is not differentiable = descent: along the subgradient = to find a minimum with conditions given earlier for 3. More examples similar algorithms that can be applied to other decoding applications • context-free parsing combined with dependency parsing • corpus-level part-of-speech tagging • combined translation alignment Combined constituency and dependency parsing separate models trained for constituency and dependency parsing constituency parse that maximizes the sum of the two models example: • combine lexicalized CFG with second-order dependency parser Lexicalized constituency parsing notation: Y set of lexicalized constituency parses for input y Y a valid parse • f (y) scores a parse tree goal: arg max f (y) lexicalized context-free grammar S(flies) NP(United) N United some large jet Dependency parsing define: Z set of dependency parses for input z Z a valid dependency parse • g(z) scores a dependency parse example: VP(flies) NP(jet) V flies N A D Identifying dependencies the dependencies selected by each model • = 1 when constituency parse y selects word i as a modifier of word j • = 1 when dependency parse z selects word i as a modifier of word j constituency and dependency parse with = 1 = 1 S(flies) NP(United) VP(flies) N United NP(jet) flies D N A large jet y Combined optimization goal: arg max f (y) + g(z) YEY,ZEZ that for all i = j = 0 = Algorithm step-by-step [Animation] Corpus-level tagging a corpus of sentences and a trained sentence-level tagging model best tagging for each sentence, while at the same time enforcing inter-sentence soft constraints example: • test-time decoding with a trigram tagger • constraint that each word type prefer a single POS tag Corpus-level tagging full model for corpus-level tagging Man is the best measure Sentence-level decoding notation: • set of tag sequences for input sentence i Y is set of tag sequences for the input corpus Y Y a valid tag sequence for the corpus • is the score for tagging the whole corpus goal: arg max F(Y ) YGY each sentence with a trigram tagger The smart man stood outside He saw an American man N an N man D He saw A The smart P V D man American R stood F(Y ) = ; outside N A V Inter-sentence constraints notation: Z set of possible assignments of tags to word types z Z a valid tag assignment • g(z) is a scoring function for assignments to word types (e.g. a hard constraint all word types only have one tag) MRF model that encourages words of the same type to choose the same tag man man man</abstract>
<title confidence="0.9130635">N N N N</title>
<author confidence="0.988901">man man man</author>
<title confidence="0.883118">N N N A</title>
<abstract confidence="0.994561494680851">Identifying word tags the tag labels selected by each model • = 1 when the tagger for sentence s at position i selects tag t • = 1 when the constraint assigns at sentence s position i the tag t parse and tagging with = 1 and = 1 He saw an American man The smart man stood outside Y man man man z Combined optimization goal: arg max + Y EY,ZEZ that for all T = Algorithm step-by-step [Animation] alignment and Macherey, 2011) separate models trained for English-to-French and French-to-English alignment an alignment that maximizes the score of both models with soft agreement example: • HMM models for both directional alignments (assume correct alignment is one-to-one for simplicity) English-to-French alignment define: Y set of all possible English-to-French alignments y Y a valid alignment • f (y) scores of the alignment alignment 1 3 2 4 6 5 French-to-English alignment define: Z set of all possible French-to-English alignments z Z a valid alignment • g(z) scores of an alignment alignment 1 2 3 4 6 5 Identifying word alignments the tag labels selected by each model • = 1 when e-to-f alignment y selects French word i to align with English word j • = 1 when f-to-e alignment z selects French word i to align with English word j HMM alignment models with = 1 and = 1 3 2 4 6 5 y 2 3 4 65 z Combined optimization goal: arg max f (y) + g(z) YEY,ZEZ that for all i = j = = Algorithm step-by-step [Animation] 4. Practical issues of practical dual decomposition techniques • tracking the progress of the algorithm • extracting solutions if algorithm does not converge • lazy update of dual solutions Tracking progress at each stage of the algorithm there are several useful values track: • are current dual solutions • is the current dual value • is a potential primal feasible solution f + is the potential primal value useful signals: • is the dual change (may be positive) • min k is the best dual value (tightest upper bound) • max k + is the best primal value the optimal value must be between the best dual and primal values Approximate solution upon agreement the solution is exact, but this may not occur otherwise, there is an easy way to find an approximate solution structure arg max + k is the iteration with the best primal score solution non-optimal by at most (min + t there are other methods to estimate solutions, for instance by averaging solutions (see Nedi´c and Ozdaglar (2009)) Lazy decoding recompute scratch each iteration decoding: subgradient sparse, then be easy to compute from use: very helpful if naturally into several parts • decompositions with this property are very fast in practice example: • in corpus-level tagging, only need to recompute sentences with a word type that received an update 5. Linear programming the connections between dual decomposition and linear programming • basic optimization over the simplex • formal properties of linear programming • full example with fractional optimal solutions • tightening linear program relaxations Simplex define: • is the simplex over implies and = 1 y • is the simplex over • : → maps elements to the simplex example: vertices • = 0) • = 0) • = Linear programming over the simplices of the discrete sets linear program E + z that for all = y z Lagrangian Lagrangian: α, = X X y X y + + z z i,t y X X + + y i,t z i,t Lagrangian dual: X z = max α, Strong duality define: • the optimal assignment to Q the linear program theorem: min � � U = + Y Z linear programming duality Dual relationship any value of = the original Lagrangian dual also solves dual of the linear program Primal relationship define: 2 C to feasible solutions of the original problem = for all • the set of feasible solutions to the LP = for all 2C solutions: max for any q∈Q Concrete example Y Z • x a a He is x c c He is x b is He b He a is c He c is a He b is Y Z Simple solution Y Z x a a He is a He b is x b is He b He a is x c He is c He c is choose: • is representation of • is representation of confirm: � = y z andsatisfy agreement constraint Fractional solution Y Z x a a He is x c c He is a He b is x b is He b He a is c He c is choose: • is combination of • is combination of confirm: � = y z andsatisfy agreement constraint, but not integral Optimal solution weights: • the choice of f and g determines the optimal solution if (f favors the optimal solution is fractional = [1 1 2] and g = [1 1 f + g = 0 vs f + g = 2 • is optimal, even though it is fractional Algorithm run [Animation] and Adams, 1994; Sontag et al., 2008) modify: extend identify bigrams of part-of-speech tags • = 1 = 1 and = 1 • = 1 = 1 and = bigram constraints: to add for all T X= X y z however this would make decoding expensive bigram constraint: to implement X= X y z solution trivially passes this constraint, while violates it Dual decomposition with tightening tightened decomposition includes an additional Lagrange multiplier arg max X yEY + X arg zEZ + in general, this term can make the decoding problem more difficult example: • for small examples, these penalties are easy to compute • for CFG parsing, need to include extra states that maintain tag bigrams (still faster than full intersection) Tightening step-by-step [Animation] 6. Advanced examples some different relaxation techniques • higher-order non-projective dependency parsing • syntactic machine translation Higher-order non-projective dependency parsing a model for higher-order non-projective dependency parsing (sibling features) non-projective dependency parse that maximizes the score of this model difficulty: • model is NP-hard to decode • complexity of the model comes from enforcing combinatorial constraints a decomposition that separates combinatorial constraints from direct implementation of the scoring function Non-projective dependency parsing structure: • starts at the root symbol * • each word has a exactly one parent word • produces a tree structure (no cycles) • dependencies can cross example: Arc-Factored = + = log (generative = (CRF/perceptron model) arg max -�--Minimum Spanning Tree Algorithm y Sibling models = + = log = arg max y -�--NP-Hard Thought experiment: individual decoding + + + sibling model, can solve for each word withViterbi decoding. Thought experiment continued individual decoding for each head word using dynamic programming if we’re lucky, we’ll end up with a valid final tree we mightviolatesome constraints possibilities Dual decomposition structure goal: arg max f (y) rewrite: yEY max (y) that for all = Algorithm step-by-step [Animation] Syntactic translation decoding a trained model for syntactic machine translation best derivation that maximizes the score of this model difficulty: • need to incorporate language model in decoding • empirically, relaxation is often not tight, so dual decomposition does not always converge strategy: • use a different relaxation to handle language model • incrementally add constraints to find exact solution Syntactic translation example [Animation] Summary presented dual decomposition as a method for decoding in NLP formal guarantees • gives certificate or approximate solution • can improve approximate solutions by tightening relaxation efficient algorithms • uses fast combinatorial algorithms • can improve speed with lazy decoding widely applicable • demonstrated algorithms for a wide range of NLP tasks (parsing, tagging, alignment, mt decoding)</abstract>
<title confidence="0.7300475">References I J. DeNero and K. Macherey. Model-Based Aligner Combination</title>
<author confidence="0.452258">D Klein</author>
<author confidence="0.452258">C D Manning Factored A Search for Models over</author>
<abstract confidence="0.8732525">and Trees. In volume 18, pages 1246–1251. Citeseer, 2003. N. Komodakis, N. Paragios, and G. Tziritas. Mrf energy and beyond via dual decomposition. on Pattern Analysis and Machine 2010. ISSN 0162-8828.</abstract>
<author confidence="0.5658985">Dual decomposition for parsing with</author>
<note confidence="0.828486">head automata. In 2010. URL Korte and J. Vygen. Optimization: Theory and Springer Verlag, 2008.</note>
<title confidence="0.499726">References II</title>
<author confidence="0.661953">Lagrangian Relaxation In</author>
<affiliation confidence="0.229843">Combinatorial Optimization, Optimal or Provably Near-Optimal [based on a Spring pages 112–156, London,</affiliation>
<address confidence="0.385625">UK, 2001. Springer-Verlag. ISBN 3-540-42877-1.</address>
<abstract confidence="0.7549575">Angelia Nedi´c and Asuman Ozdaglar. Approximate primal and rate analysis for dual subgradient methods. on 19(4):1757–1780, 2009. Raphael. Coarse-to-fine dynamic programming.</abstract>
<note confidence="0.720162444444445">on Pattern Analysis and Machine 23: 1379–1390, 2001. A.M. Rush and M. Collins. Exact Decoding of Syntactic Models through Lagrangian Relaxation. In 2011. A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. On Dual Decomposition and Linear Programming Relaxations for Natural Processing. In 2010. References III Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations and convex hull characterizations for mixed-integer zero–one problems. Applied 52(1):83 – 106, 1994. D.A. Smith and J. Eisner. Dependency Parsing by Belief In pages 145–156, 2008. URL D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations for MAP using message passing. In 2008.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>K Macherey</author>
</authors>
<title>Model-Based Aligner Combination Using Dual Decomposition.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<contexts>
<context position="12943" citStr="DeNero and Macherey, 2011" startWordPosition="2464" endWordPosition="2467">man N N N N z2 man man man N N N A g(z1) &gt; g(z2) Identifying word tags notation: identify the tag labels selected by each model • Y,(i, t) = 1 when the tagger for sentence s at position i selects tag t • z(s, i, t) = 1 when the constraint assigns at sentence s position i the tag t example: a parse and tagging with Y1(5, N) = 1 and z(1, 5, N) = 1 He saw an American man The smart man stood outside Y man man man z Combined optimization goal: arg max F(Y ) + g(z) Y EY,ZEZ such that for all s = 1... m, i = 1... n, t ∈ T , YS(i, t) = z(s, i, t) Algorithm step-by-step [Animation] Combined alignment (DeNero and Macherey, 2011) setup: assume separate models trained for English-to-French and French-to-English alignment problem: find an alignment that maximizes the score of both models with soft agreement example: • HMM models for both directional alignments (assume correct alignment is one-to-one for simplicity) English-to-French alignment define: • Y is set of all possible English-to-French alignments • y ∈ Y is a valid alignment • f (y) scores of the alignment example: HMM alignment 1 3 2 4 6 5 The1 u9ly2 do93 has4 reds fur6 French-to-English alignment define: • Z is set of all possible French-to-English alignments</context>
</contexts>
<marker>DeNero, Macherey, 2011</marker>
<rawString>J. DeNero and K. Macherey. Model-Based Aligner Combination Using Dual Decomposition. In Proc. ACL, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Factored A* Search for Models over Sequences and Trees.</title>
<date>2003</date>
<booktitle>In Proc IJCAI,</booktitle>
<volume>18</volume>
<pages>1246--1251</pages>
<publisher>Citeseer,</publisher>
<contexts>
<context position="1952" citStr="Klein and Manning, 2003" startWordPosition="273" endWordPosition="276">thms of this form Dual Decomposition (Komodakis et al., 2010; Lemar´echal, 2001) goal: solve complicated optimization problem y∗ = arg max f (y) Y method: decompose into subproblems, solve iteratively benefit: can choose decomposition to provide “easy” subproblems aim for simple and efficient combinatorial algorithms • dynamic programming • minimum spanning tree • shortest path • min-cut • bipartite match • etc. Related work there are related methods used NLP with similar motivation related methods: • belief propagation (particularly max-product) (Smith and Eisner, 2008) • factored A* search (Klein and Manning, 2003) • exact coarse-to-fine (Raphael, 2001) aim to find exact solutions without exploring the full search space Tutorial outline focus: • developing dual decomposition algorithms for new NLP tasks • understanding formal guarantees of the algorithms • extensions to improve exactness and select solutions outline: 1. worked algorithm for combined parsing and tagging 2. important theorems and formal derivation 3. more examples from parsing, sequence labeling, MT 4. practical considerations for implementing dual decomposition 5. relationship to linear programming relaxations 6. further variations and a</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. Factored A* Search for Models over Sequences and Trees. In Proc IJCAI, volume 18, pages 1246–1251. Citeseer, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Komodakis</author>
<author>N Paragios</author>
<author>G Tziritas</author>
</authors>
<title>Mrf energy minimization and beyond via dual decomposition.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>0162--8828</pages>
<contexts>
<context position="1388" citStr="Komodakis et al., 2010" startWordPosition="191" endWordPosition="194">to-fine, beam search, cube pruning, gibbs sampling, belief propagation) • approximate models (mean field, variational models) Motivation cannot hope to find exact algorithms (particularly when NP-Hard) aim: develop decoding algorithms with formal guarantees method: • derive fast algorithms that provide certificates of optimality • show that for practical instances, these algorithms often yield exact solutions • provide strategies for improving solutions or finding approximate solutions when no certificate is found dual decomposition helps us develop algorithms of this form Dual Decomposition (Komodakis et al., 2010; Lemar´echal, 2001) goal: solve complicated optimization problem y∗ = arg max f (y) Y method: decompose into subproblems, solve iteratively benefit: can choose decomposition to provide “easy” subproblems aim for simple and efficient combinatorial algorithms • dynamic programming • minimum spanning tree • shortest path • min-cut • bipartite match • etc. Related work there are related methods used NLP with similar motivation related methods: • belief propagation (particularly max-product) (Smith and Eisner, 2008) • factored A* search (Klein and Manning, 2003) • exact coarse-to-fine (Raphael, 20</context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2010</marker>
<rawString>N. Komodakis, N. Paragios, and G. Tziritas. Mrf energy minimization and beyond via dual decomposition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010. ISSN 0162-8828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<note>URL http://www.aclweb.org/anthology/D10-1125.</note>
<contexts>
<context position="713" citStr="Koo et al., 2010" startWordPosition="100" endWordPosition="103">g complexity focus: decoding problem for natural language tasks y∗ = arg max f (y) Y motivation: • richer model structure often leads to improved accuracy • exact decoding for complex models tends to be intractable Decoding tasks many common problems are intractable to decode exactly high complexity • combined parsing and part-of-speech tagging (Rush et al., 2010) • “loopy” HMM part-of-speech tagging • syntactic machine translation (Rush and Collins, 2011) NP-Hard • symmetric HMM alignment (DeNero and Macherey, 2011) • phrase-based translation • higher-order non-projective dependency parsing (Koo et al., 2010) in practice: • approximate decoding methods (coarse-to-fine, beam search, cube pruning, gibbs sampling, belief propagation) • approximate models (mean field, variational models) Motivation cannot hope to find exact algorithms (particularly when NP-Hard) aim: develop decoding algorithms with formal guarantees method: • derive fast algorithms that provide certificates of optimality • show that for practical instances, these algorithms often yield exact solutions • provide strategies for improving solutions or finding approximate solutions when no certificate is found dual decomposition helps us</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. Dual decomposition for parsing with non-projective head automata. In EMNLP, 2010. URL http://www.aclweb.org/anthology/D10-1125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Korte</author>
<author>J Vygen</author>
</authors>
<title>Combinatorial Optimization: Theory and Algorithms.</title>
<date>2008</date>
<publisher>Springer Verlag,</publisher>
<marker>Korte, Vygen, 2008</marker>
<rawString>B.H. Korte and J. Vygen. Combinatorial Optimization: Theory and Algorithms. Springer Verlag, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lemar´echal</author>
</authors>
<title>Lagrangian Relaxation. In Computational Combinatorial Optimization, Optimal or Provably Near-Optimal Solutions [based on a Spring School],</title>
<date>2001</date>
<pages>112--156</pages>
<publisher>Springer-Verlag. ISBN</publisher>
<location>London, UK,</location>
<marker>Lemar´echal, 2001</marker>
<rawString>C. Lemar´echal. Lagrangian Relaxation. In Computational Combinatorial Optimization, Optimal or Provably Near-Optimal Solutions [based on a Spring School], pages 112–156, London, UK, 2001. Springer-Verlag. ISBN 3-540-42877-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelia Nedi´c</author>
<author>Asuman Ozdaglar</author>
</authors>
<title>Approximate primal solutions and rate analysis for dual subgradient methods.</title>
<date>2009</date>
<journal>SIAM Journal on Optimization,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Nedi´c, Ozdaglar, 2009</marker>
<rawString>Angelia Nedi´c and Asuman Ozdaglar. Approximate primal solutions and rate analysis for dual subgradient methods. SIAM Journal on Optimization, 19(4):1757–1780, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Raphael</author>
</authors>
<title>Coarse-to-fine dynamic programming.</title>
<date>2001</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>23</volume>
<pages>1379--1390</pages>
<contexts>
<context position="1991" citStr="Raphael, 2001" startWordPosition="280" endWordPosition="281">t al., 2010; Lemar´echal, 2001) goal: solve complicated optimization problem y∗ = arg max f (y) Y method: decompose into subproblems, solve iteratively benefit: can choose decomposition to provide “easy” subproblems aim for simple and efficient combinatorial algorithms • dynamic programming • minimum spanning tree • shortest path • min-cut • bipartite match • etc. Related work there are related methods used NLP with similar motivation related methods: • belief propagation (particularly max-product) (Smith and Eisner, 2008) • factored A* search (Klein and Manning, 2003) • exact coarse-to-fine (Raphael, 2001) aim to find exact solutions without exploring the full search space Tutorial outline focus: • developing dual decomposition algorithms for new NLP tasks • understanding formal guarantees of the algorithms • extensions to improve exactness and select solutions outline: 1. worked algorithm for combined parsing and tagging 2. important theorems and formal derivation 3. more examples from parsing, sequence labeling, MT 4. practical considerations for implementing dual decomposition 5. relationship to linear programming relaxations 6. further variations and advanced examples 1. Worked example aim:</context>
</contexts>
<marker>Raphael, 2001</marker>
<rawString>Christopher Raphael. Coarse-to-fine dynamic programming. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23: 1379–1390, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>M Collins</author>
</authors>
<title>Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation.</title>
<date>2011</date>
<booktitle>In Proc. ACL,</booktitle>
<marker>Rush, Collins, 2011</marker>
<rawString>A.M. Rush and M. Collins. Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation. In Proc. ACL, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>D Sontag</author>
<author>M Collins</author>
<author>T Jaakkola</author>
</authors>
<title>On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP,</booktitle>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing. In Proc. EMNLP, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanif D Sherali</author>
<author>Warren P Adams</author>
</authors>
<title>A hierarchy of relaxations and convex hull characterizations for mixed-integer zero–one programming problems.</title>
<date>1994</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>52</volume>
<issue>1</issue>
<contexts>
<context position="18695" citStr="Sherali and Adams, 1994" startWordPosition="3632" endWordPosition="3635"> He b is x b is b He b He a is c He c is choose: • α(2) = (0.5, 0.5, 0) E Oy is combination of y1 and y2 • β(2) = (0.5, 0.5, 0) E Oz is combination of z1 and z2 confirm: � α(y2)y(i,t) = β(2) y z z z(i, t) α(2) and β(2) satisfy agreement constraint, but not integral Optimal solution weights: • the choice of f and g determines the optimal solution • if (f , g) favors (α(2),β(2)), the optimal solution is fractional example: f = [1 1 2] and g = [1 1 − 2] • f · α(1) + g · β(1) = 0 vs f · α(2) + g · β(2) = 2 • α(2), β(2) is optimal, even though it is fractional Algorithm run [Animation] Tightening (Sherali and Adams, 1994; Sontag et al., 2008) modify: • extend Y, i to identify bigrams of part-of-speech tags • y(i, t1, t2) = 1 y(i, t1) = 1 and y(i + 1, t2) = 1 • z(i, t1, t2) = 1 z(i, t1) = 1 and z(i + 1, t2) = 1 all bigram constraints: valid to add for all i, t1, t2 E T X αyy(i, t1, t2) = X βzz(i, t1, t2) y z however this would make decoding expensive single bigram constraint: cheaper to implement X αyy(1, a, b) = X βzz(1, a, b) y z the solution α(1),β(1) trivially passes this constraint, while α(2), β(2) violates it Dual decomposition with tightening tightened decomposition includes an additional Lagrange mult</context>
</contexts>
<marker>Sherali, Adams, 1994</marker>
<rawString>Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations and convex hull characterizations for mixed-integer zero–one programming problems. Discrete Applied Mathematics, 52(1):83 – 106, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency Parsing by Belief Propagation.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>145--156</pages>
<contexts>
<context position="1905" citStr="Smith and Eisner, 2008" startWordPosition="265" endWordPosition="268">und dual decomposition helps us develop algorithms of this form Dual Decomposition (Komodakis et al., 2010; Lemar´echal, 2001) goal: solve complicated optimization problem y∗ = arg max f (y) Y method: decompose into subproblems, solve iteratively benefit: can choose decomposition to provide “easy” subproblems aim for simple and efficient combinatorial algorithms • dynamic programming • minimum spanning tree • shortest path • min-cut • bipartite match • etc. Related work there are related methods used NLP with similar motivation related methods: • belief propagation (particularly max-product) (Smith and Eisner, 2008) • factored A* search (Klein and Manning, 2003) • exact coarse-to-fine (Raphael, 2001) aim to find exact solutions without exploring the full search space Tutorial outline focus: • developing dual decomposition algorithms for new NLP tasks • understanding formal guarantees of the algorithms • extensions to improve exactness and select solutions outline: 1. worked algorithm for combined parsing and tagging 2. important theorems and formal derivation 3. more examples from parsing, sequence labeling, MT 4. practical considerations for implementing dual decomposition 5. relationship to linear prog</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D.A. Smith and J. Eisner. Dependency Parsing by Belief Propagation. In Proc. EMNLP, pages 145–156, 2008. URL http://www.aclweb.org/anthology/D08-1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sontag</author>
<author>T Meltzer</author>
<author>A Globerson</author>
<author>T Jaakkola</author>
<author>Y Weiss</author>
</authors>
<title>Tightening LP relaxations for MAP using message passing.</title>
<date>2008</date>
<booktitle>In Proc. UAI,</booktitle>
<contexts>
<context position="18717" citStr="Sontag et al., 2008" startWordPosition="3636" endWordPosition="3639"> a is c He c is choose: • α(2) = (0.5, 0.5, 0) E Oy is combination of y1 and y2 • β(2) = (0.5, 0.5, 0) E Oz is combination of z1 and z2 confirm: � α(y2)y(i,t) = β(2) y z z z(i, t) α(2) and β(2) satisfy agreement constraint, but not integral Optimal solution weights: • the choice of f and g determines the optimal solution • if (f , g) favors (α(2),β(2)), the optimal solution is fractional example: f = [1 1 2] and g = [1 1 − 2] • f · α(1) + g · β(1) = 0 vs f · α(2) + g · β(2) = 2 • α(2), β(2) is optimal, even though it is fractional Algorithm run [Animation] Tightening (Sherali and Adams, 1994; Sontag et al., 2008) modify: • extend Y, i to identify bigrams of part-of-speech tags • y(i, t1, t2) = 1 y(i, t1) = 1 and y(i + 1, t2) = 1 • z(i, t1, t2) = 1 z(i, t1) = 1 and z(i + 1, t2) = 1 all bigram constraints: valid to add for all i, t1, t2 E T X αyy(i, t1, t2) = X βzz(i, t1, t2) y z however this would make decoding expensive single bigram constraint: cheaper to implement X αyy(1, a, b) = X βzz(1, a, b) y z the solution α(1),β(1) trivially passes this constraint, while α(2), β(2) violates it Dual decomposition with tightening tightened decomposition includes an additional Lagrange multiplier yu,v = arg max </context>
</contexts>
<marker>Sontag, Meltzer, Globerson, Jaakkola, Weiss, 2008</marker>
<rawString>D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations for MAP using message passing. In Proc. UAI, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>