<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023385">
<title confidence="0.997534">
A New Entity Salience Task with Millions of Training Examples
</title>
<author confidence="0.997812">
Jesse Dunietz
</author>
<affiliation confidence="0.934499">
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.997266">
jdunietz@cs.cmu.edu
</email>
<author confidence="0.95441">
Dan Gillick
</author>
<affiliation confidence="0.890458">
Google Research
</affiliation>
<address confidence="0.7100125">
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
</address>
<email confidence="0.998547">
dgillick@google.com
</email>
<sectionHeader confidence="0.993876" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999667647058824">
Although many NLP systems are moving
toward entity-based processing, most still
identify important phrases using classi-
cal keyword-based approaches. To bridge
this gap, we introduce the task of entity
salience: assigning a relevance score to
each entity in a document. We demon-
strate how a labeled corpus for the task
can be automatically generated from a cor-
pus of documents and accompanying ab-
stracts. We then show how a classifier
with features derived from a standard NLP
pipeline outperforms a strong baseline by
34%. Finally, we outline initial experi-
ments on further improving accuracy by
leveraging background knowledge about
the relationships between entities.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999614083333333">
Information retrieval, summarization, and online
advertising rely on identifying the most important
words and phrases in web documents. While tradi-
tional techniques treat documents as collections of
keywords, many NLP systems are shifting toward
understanding documents in terms of entities. Ac-
cordingly, we need new algorithms to determine
the prominence – the salience – of each entity in
the document.
Toward this end, we describe three primary con-
tributions. First, we show how a labeled cor-
pus for this task can be automatically constructed
from a corpus of documents with accompanying
abstracts. We also demonstrate the validity of
the corpus with a manual annotation study. Sec-
ond, we train an entity salience model using fea-
tures derived from a coreference resolution sys-
tem. This model significantly outperforms a base-
line model based on sentence position. Third, we
suggest how our model can be improved by lever-
aging background information about the entities
and their relationships – information not specifi-
cally provided in the document in question.
Our notion of salience is similar to that of Bogu-
raev and Kenney (1997): “discourse objects with
high salience are the focus of attention”, inspired
by earlier work on Centering Theory (Walker et
al., 1998). Here we take a more empirical ap-
proach: salient entities are those that human read-
ers deem most relevant to the document.
The entity salience task in particular is briefly
alluded to by Cornolti et al. (2013), and addressed
in the context of Twitter messages by Meij et. al
(2012). It is also similar in spirit to the much more
common keyword extraction task (Tomokiyo and
Hurst, 2003; Hulth, 2003).
</bodyText>
<sectionHeader confidence="0.536361" genericHeader="method">
2 Generating an entity salience corpus
</sectionHeader>
<bodyText confidence="0.999713035714286">
Rather than manually annotating a corpus, we au-
tomatically generate salience labels for an existing
corpus of document/abstract pairs. We derive the
labels using the assumption that the salient entities
will be mentioned in the abstract, so we identify
and align the entities in each text.
Given a document and abstract, we run a stan-
dard NLP pipeline on both. This includes a POS
tagger and dependency parser, comparable in ac-
curacy to the current Stanford dependency parser
(Klein and Manning, 2003); an NP extractor that
uses POS tags and dependency edges to identify
a set of entity mentions; a coreference resolver,
comparable to that of Haghighi and Klein, (2009)
for clustering mentions; and an entity resolver that
links entities to Freebase profiles. The entity re-
solver is described in detail by Lao, et al. (2012).
We then apply a simple heuristic to align the
entities in the abstract and document: Let ME be
the set of mentions of an entity E that are proper
names. An entity EA from the abstract aligns to an
entity ED from the document if the syntactic head
token of some mention in MEA matches the head
token of some mention in MED. If EA aligns with
more than one document entity, we align it with
the document entity that appears earliest.
In general, aligning an abstract to its source doc-
ument is difficult (Daum´e III and Marcu, 2005).
</bodyText>
<page confidence="0.983805">
205
</page>
<note confidence="0.688272">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 205–209,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999979923076923">
We avoid most of this complexity by aligning only
entities with at least one proper-name mention, for
which there is little ambiguity. Generic mentions
like CEO or state are often more ambiguous, so re-
solving them would be closer to the difficult prob-
lem of word sense disambiguation.
Once we have entity alignments, we assume
that a document entity is salient only if it has
been aligned to some abstract entity. Ideally, we
would like to induce a salience ranking over enti-
ties. Given the limitations of short abstracts, how-
ever, we settle for binary classification, which still
captures enough salience information to be useful.
</bodyText>
<subsectionHeader confidence="0.994038">
2.1 The New York Times corpus
</subsectionHeader>
<bodyText confidence="0.999974">
Our corpus of document/abstract pairs is the anno-
tated New York Times corpus (Sandhaus, 2008).
It includes 1.8 million articles published between
January 1987 and June 2007; some 650,000 in-
clude a summary written by one of the newspa-
per’s library scientists. We selected a subset of the
summarized articles from 2003-2007 by filtering
out articles and summaries that were very short or
very long, as well as several special article types
(e.g., corrections and letters to the editor).
Our full labeled dataset includes 110,639 docu-
ments with 2,229,728 labeled entities; about 14%
are marked as salient. For comparison, the average
summary is about 6% of the length (in tokens) of
the associated article. We use the 9,719 documents
from 2007 as test data and the rest as training.
</bodyText>
<subsectionHeader confidence="0.999113">
2.2 Validating salience via manual evaluation
</subsectionHeader>
<bodyText confidence="0.999940055555556">
To validate our alignment method for inferring en-
tity salience, we conducted a manual evaluation.
Two expert linguists discussed the task and gen-
erated a rubric, giving them a chance to calibrate
their scores. They then independently annotated
all detected entities in 50 random documents from
our corpus (a total of 744 entities), without read-
ing the accompanying abstracts. Each entity was
assigned a salience score in {1, 2, 3, 4}, where 1 is
most salient. We then thresholded the annotators’
scores as salient/non-salient for comparison to the
binary NYT labels.
Table 1 summarizes the agreement results, mea-
sured by Cohen’s kappa. The experts’ agreement
is probably best described as moderate,1 indicat-
ing that this is a difficult, subjective task, though
deciding on the most salient entities (with score 1)
is easier. Even without calibrating to the induced
</bodyText>
<footnote confidence="0.967649">
1For comparison, word sense disambiguation tasks have
reported agreement as low as κ = 0.3 (Yong and Foo, 1999).
</footnote>
<bodyText confidence="0.997217">
NYT salience scores, the expert vs. NYT agree-
ment is close enough to the inter-expert agreement
to convince us that our induced labels are a rea-
sonable if somewhat noisy proxy for the experts’
definition of salience.
</bodyText>
<table confidence="0.9990984">
Comparison κ{1,2} κ{1}
A1 vs. A2 0.56 0.69
A1 vs. NYT 0.36 0.48
A2 vs. NYT 0.39 0.35
A1 &amp; A2 vs. NYT 0.43 0.38
</table>
<tableCaption confidence="0.998686">
Table 1: Annotator agreement for entity salience
</tableCaption>
<bodyText confidence="0.9753642">
as a binary classification. A1 and A2 are expert an-
notators; NYT represents the induced labels. The
first n column assumes annotator scores {1, 2} are
salient and {3, 4} are non-salient, while the second
n column assumes only scores of 1 are salient.
</bodyText>
<sectionHeader confidence="0.985261" genericHeader="method">
3 Salience classification
</sectionHeader>
<bodyText confidence="0.999988">
We built a regularized binary logistic regression
model to predict the probability that an entity is
salient. To simplify feature selection and to add
some further regularization, we used feature hash-
ing (Ganchev and Dredze, 2008) to randomly map
each feature string to an integer in [1,100000];
larger alphabet sizes yielded no improvement. The
model was trained with L-BGFS.
</bodyText>
<subsectionHeader confidence="0.998957">
3.1 Positional baseline
</subsectionHeader>
<bodyText confidence="0.9999305">
For news documents, it is well known that sen-
tence position is a very strong indicator for rele-
vance. Thus, our baseline is a system that identi-
fies an entity as salient if it is mentioned in the first
sentence of the document. (Including the next few
sentences did not significantly change the score.)
</bodyText>
<subsectionHeader confidence="0.999426">
3.2 Model features
</subsectionHeader>
<bodyText confidence="0.99999">
Table 2 describes our feature classes; each indi-
vidual feature in the model is a binary indicator.
Count features are bucketed by applying the func-
tion f(x) = round(log(k(x + 1))), where k can
be used to control the number of buckets. We sim-
ply set k = 10 in all cases.
</bodyText>
<subsectionHeader confidence="0.992824">
3.3 Experimental results
</subsectionHeader>
<bodyText confidence="0.999751571428571">
Table 3 shows experimental results on our test set.
Each experiment uses a classification threshold of
0.3 to determine salience, which in each case is
very close to the threshold that maximizes F1. For
comparison, a classifier that always predicts the
majority class, non-salient, has F1 = 23.9 (for the
salient class).
</bodyText>
<page confidence="0.993963">
206
</page>
<bodyText confidence="0.979147823529412">
Feature name Description
1st-loc Index of the sentence in
which the first mention of the
entity appears.
head-count Number of times the head
word of the entity’s first men-
tion appears.
mentions Conjuction of the numbers
of named (Barack Obama),
nominal (president), pronom-
inal (he), and total mentions
of the entity.
headline POS tag of each word that ap-
pears in at least one mention
and also in the headline.
head-lex Lowercased head word of the
first mention.
</bodyText>
<tableCaption confidence="0.982797">
Table 2: The feature classes used by the classifier.
</tableCaption>
<bodyText confidence="0.999900666666667">
Lines 2 and 3 serve as a comparison between
traditional keyword counts and the mention counts
derived from our coreference resolution system.
Named, nominal, and pronominal mention counts
clearly add significant information despite coref-
erence errors. Lines 4-8 show results when our
model features are incrementally added. Each fea-
ture raises accuracy, and together our simple set of
features improves on the baseline by 34%.
</bodyText>
<sectionHeader confidence="0.991546" genericHeader="method">
4 Entity centrality
</sectionHeader>
<bodyText confidence="0.999898545454545">
All the features described above use only infor-
mation available within the document. But arti-
cles are written with the assumption that the reader
knows something about at least some of the enti-
ties involved. Inspired by results using Wikipedia
to improve keyword extraction tasks (Mihalcea
and Csomai, 2007; Xu et al., 2010), we experi-
mented with a simple method for including back-
ground knowledge about each entity: an adapta-
tion of PageRank (Page et al., 1999) to a graph
of connected entities, in the spirit of Erkan and
Radev’s work (2004) on summarization.
Consider, for example, an article about a recent
congressional budget debate. Although House
Speaker John Boehner may be mentioned just
once, we know he is likely salient because he is
closely related to other entities in the article, such
as Congress, the Republican Party, and Barack
Obama. On the other hand, the Federal Emer-
gency Management Agency may be mentioned re-
peatedly because it happened to host a major pres-
idential speech, but it is less related to the story’s
</bodyText>
<table confidence="0.9904086">
# Description P R F1
1 Positional baseline 59.5 37.8 46.2
2 head-count 37.3 54.7 44.4
3 mentions 57.2 51.3 54.1
4 1st-loc 46.1 60.2 52.2
5 + head-count 52.6 63.4 57.5
6 +mentions 59.3 61.3 60.3
7 +headline 59.1 61.9 60.5
8 + head-lex 59.7 63.6 61.6
9 + centrality 60.5 63.5 62.0
</table>
<tableCaption confidence="0.86757675">
Table 3: Test set (P)recision, (R)ecall, and (F)
measure of the salient class for some com-
binations of features listed in Table 2. The
centrality feature is discussed in Section 4.
</tableCaption>
<bodyText confidence="0.999079">
key figures and less central to the article’s point.
Our intuition about these relationships, mostly
not explicit in the document, can be formalized in
a local PageRank computation on the entity graph.
</bodyText>
<subsectionHeader confidence="0.916831">
4.1 PageRank for computing centrality
</subsectionHeader>
<bodyText confidence="0.999791566666667">
In the weighted version of the PageRank algorithm
(Xing and Ghorbani, 2004), a web link is con-
sidered a weighted vote by the containing page
for the landing page – a directed edge in a graph
where each node is a webpage. In place of the web
graph, we consider the graph of Freebase entities
that appear in the document. The nodes are the
entities, and a directed edge from E1 to E2 repre-
sents P(E2|E1), the probability of observing E2
in a document given that we have observed E1.
We estimate P(E2|E1) by counting the number of
training documents in which E1 and E2 co-occur
and normalizing by the number of training docu-
ments in which E1 occurs.
The nodes’ initial PageRank values act as a
prior, where the uniform distribution, used in the
classic PageRank algorithm, indicates a lack of
prior knowledge. Since we have some prior sig-
nal about salience, we initialize the node values to
the normalized mention counts of the entities in
the document. We use a damping factor d, allow-
ing random jumps between nodes with probability
1 − d, with the standard value d = 0.85.
We implemented the iterative version of
weighted PageRank, which tends to converge in
under 10 iterations. The centrality features
in Table 3 are indicators for the rank orders of the
converged entity scores. The improvement from
adding centrality features is small but statistically
significant at p ≤ 0.001.
</bodyText>
<page confidence="0.981132">
207
</page>
<figure confidence="0.9993541">
Republican
Party
Boehner
Obama
FEMA
Republican
Party
Boehner
Obama
FEMA
</figure>
<figureCaption confidence="0.67753475">
Figure 1: A graphical representation of the centrality computation on a toy example. Circle size and
arrow thickness represent node value and edge weight, respectively. The initial node values, based on
mention count, are shown on the left. The final node values are on the right; dotted circles show the
initial sizes for comparison. Edge weights remain constant.
</figureCaption>
<subsectionHeader confidence="0.968159">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999988275862069">
We experimented with a number of variations on
this algorithm, but none gave much meaningful
improvement. In particular, we tried to include
the neighbors of all entities to increase the size
of the graph, with the values of neighbor enti-
ties not in the document initialized to some small
value k. We set a minimum co-occurrence count
for an edge to be included, varying it from 1
to 100 (where 1 results in very large graphs).
We also tried using Freebase relations between
entities (rather than raw co-occurrence counts)
to determine the set of neighbors. Finally, we
experimented with undirected graphs using un-
normalized co-occurrence counts.
While the ranked centrality scores look reason-
able for most documents, the addition of these fea-
tures does not produce a substantial improvement.
One potential problem is our reliance on the entity
resolver. Because the PageRank computation links
all of a document’s entities, a single resolver error
can significantly alter all the centrality scores. Per-
haps more importantly, the resolver is incomplete:
many tail entities are not included in Freebase.
Still, it seems likely that even with perfect reso-
lution, entity centrality would not significantly im-
prove the accuracy of our model. The mentions
features are sufficiently powerful that entity cen-
trality seems to add little information to the model
beyond what these features already provide.
</bodyText>
<sectionHeader confidence="0.999484" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.997743714285714">
We have demonstrated how a simple alignment
of entities in documents with entities in their ac-
companying abstracts provides salience labels that
roughly agree with manual salience annotations.
This allows us to create a large corpus – over
100,000 labeled documents with over 2 million la-
beled entities – that we use to train a classifier for
predicting entity salience.
Our experiments show that features derived
from a coreference system are more robust than
simple word count features typical of a keyword
extraction system. These features combine nicely
with positional features (and a few others) to give
a large improvement over a first-sentence baseline.
There is likely significant room for improve-
ment, especially by leveraging background infor-
mation about the entities, and we have presented
some initial experiments in that direction. Perhaps
features more directly linked to Wikipedia, as in
related work on keyword extraction, can provide
more focused background information.
We believe entity salience is an important task
with many applications. To facilitate further re-
search, our automatically generated salience an-
notations, along with resolved entity ids, for the
subset of the NYT corpus discussed in this paper
are available here:
https://code.google.com/p/nyt-salience/
</bodyText>
<page confidence="0.99747">
208
</page>
<sectionHeader confidence="0.989896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999155116883117">
Branimir Boguraev and Christopher Kennedy. 1997.
Salience-based content characterisation of text doc-
uments. In Proceedings of the ACL, volume 97,
pages 2–9.
Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmarking
entity-annotation systems. In Proceedings of the
22nd international conference on World Wide Web,
pages 249–260.
Hal Daum´e III and Daniel Marcu. 2005. Induction
of word and phrase alignments for automatic doc-
ument summarization. Computational Linguistics,
31(4):505–530.
G¨unes Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research (JAIR), 22(1):457–479.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In Pro-
ceedings of the ACL08 HLT Workshop on Mobile
Language Processing, pages 19–20.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1152–1161. Asso-
ciation for Computational Linguistics.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 216–223.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1017–1026. Association for Computational Linguis-
tics.
Edgar Meij, Wouter Weerkamp, and Maarten de Ri-
jke. 2012. Adding semantics to microblog posts.
In Proceedings of the fifth ACM international con-
ference on Web search and data mining, pages 563–
572. ACM.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233–242.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The PageRank citation
ranking: Bringing order to the web. Technical Re-
port 1999-66, Stanford InfoLab.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.
Takashi Tomokiyo and Matthew Hurst. 2003. A
language model approach to keyphrase extraction.
In Proceedings of the ACL 2003 workshop on
Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 33–40.
Marilyn A Walker, Aravind Krishna Joshi, and
Ellen Friedman Prince. 1998. Centering theory in
discourse. Oxford University Press.
Wenpu Xing and Ali Ghorbani. 2004. Weighted page-
rank algorithm. In Communication Networks and
Services Research, pages 305–314. IEEE.
Songhua Xu, Shaohui Yang, and Francis Chi-Moon
Lau. 2010. Keyword extraction and headline gener-
ation using novel word features. In AAAI.
Chung Yong and Shou King Foo. 1999. A case study
on inter-annotator agreement for word sense disam-
biguation.
</reference>
<page confidence="0.998944">
209
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.831182">
<title confidence="0.999921">A New Entity Salience Task with Millions of Training Examples</title>
<author confidence="0.999894">Jesse Dunietz</author>
<affiliation confidence="0.999947">Computer Science Department Carnegie Mellon University</affiliation>
<address confidence="0.9986">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999818">jdunietz@cs.cmu.edu</email>
<author confidence="0.950512">Dan</author>
<affiliation confidence="0.876953">Google</affiliation>
<address confidence="0.9905875">1600 Amphitheatre Mountain View, CA 94043,</address>
<email confidence="0.999776">dgillick@google.com</email>
<abstract confidence="0.999676222222222">Although many NLP systems are moving toward entity-based processing, most still identify important phrases using classical keyword-based approaches. To bridge gap, we introduce the task of assigning a relevance score to each entity in a document. We demonstrate how a labeled corpus for the task can be automatically generated from a corpus of documents and accompanying abstracts. We then show how a classifier with features derived from a standard NLP pipeline outperforms a strong baseline by 34%. Finally, we outline initial experiments on further improving accuracy by leveraging background knowledge about the relationships between entities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Branimir Boguraev</author>
<author>Christopher Kennedy</author>
</authors>
<title>Salience-based content characterisation of text documents.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<volume>97</volume>
<pages>2--9</pages>
<marker>Boguraev, Kennedy, 1997</marker>
<rawString>Branimir Boguraev and Christopher Kennedy. 1997. Salience-based content characterisation of text documents. In Proceedings of the ACL, volume 97, pages 2–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Cornolti</author>
<author>Paolo Ferragina</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>A framework for benchmarking entity-annotation systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web,</booktitle>
<pages>249--260</pages>
<contexts>
<context position="2453" citStr="Cornolti et al. (2013)" startWordPosition="376" endWordPosition="379">osition. Third, we suggest how our model can be improved by leveraging background information about the entities and their relationships – information not specifically provided in the document in question. Our notion of salience is similar to that of Boguraev and Kenney (1997): “discourse objects with high salience are the focus of attention”, inspired by earlier work on Centering Theory (Walker et al., 1998). Here we take a more empirical approach: salient entities are those that human readers deem most relevant to the document. The entity salience task in particular is briefly alluded to by Cornolti et al. (2013), and addressed in the context of Twitter messages by Meij et. al (2012). It is also similar in spirit to the much more common keyword extraction task (Tomokiyo and Hurst, 2003; Hulth, 2003). 2 Generating an entity salience corpus Rather than manually annotating a corpus, we automatically generate salience labels for an existing corpus of document/abstract pairs. We derive the labels using the assumption that the salient entities will be mentioned in the abstract, so we identify and align the entities in each text. Given a document and abstract, we run a standard NLP pipeline on both. This inc</context>
</contexts>
<marker>Cornolti, Ferragina, Ciaramita, 2013</marker>
<rawString>Marco Cornolti, Paolo Ferragina, and Massimiliano Ciaramita. 2013. A framework for benchmarking entity-annotation systems. In Proceedings of the 22nd international conference on World Wide Web, pages 249–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Induction of word and phrase alignments for automatic document summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. Induction of word and phrase alignments for automatic document summarization. Computational Linguistics, 31(4):505–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Mark Dredze</author>
</authors>
<title>Small statistical models by random feature mixing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL08 HLT Workshop on Mobile Language Processing,</booktitle>
<pages>pages</pages>
<contexts>
<context position="7606" citStr="Ganchev and Dredze, 2008" startWordPosition="1233" endWordPosition="1236"> A1 vs. NYT 0.36 0.48 A2 vs. NYT 0.39 0.35 A1 &amp; A2 vs. NYT 0.43 0.38 Table 1: Annotator agreement for entity salience as a binary classification. A1 and A2 are expert annotators; NYT represents the induced labels. The first n column assumes annotator scores {1, 2} are salient and {3, 4} are non-salient, while the second n column assumes only scores of 1 are salient. 3 Salience classification We built a regularized binary logistic regression model to predict the probability that an entity is salient. To simplify feature selection and to add some further regularization, we used feature hashing (Ganchev and Dredze, 2008) to randomly map each feature string to an integer in [1,100000]; larger alphabet sizes yielded no improvement. The model was trained with L-BGFS. 3.1 Positional baseline For news documents, it is well known that sentence position is a very strong indicator for relevance. Thus, our baseline is a system that identifies an entity as salient if it is mentioned in the first sentence of the document. (Including the next few sentences did not significantly change the score.) 3.2 Model features Table 2 describes our feature classes; each individual feature in the model is a binary indicator. Count fe</context>
</contexts>
<marker>Ganchev, Dredze, 2008</marker>
<rawString>Kuzman Ganchev and Mark Dredze. 2008. Small statistical models by random feature mixing. In Proceedings of the ACL08 HLT Workshop on Mobile Language Processing, pages 19–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1152--1161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3353" citStr="Haghighi and Klein, (2009)" startWordPosition="525" endWordPosition="528">e automatically generate salience labels for an existing corpus of document/abstract pairs. We derive the labels using the assumption that the salient entities will be mentioned in the abstract, so we identify and align the entities in each text. Given a document and abstract, we run a standard NLP pipeline on both. This includes a POS tagger and dependency parser, comparable in accuracy to the current Stanford dependency parser (Klein and Manning, 2003); an NP extractor that uses POS tags and dependency edges to identify a set of entity mentions; a coreference resolver, comparable to that of Haghighi and Klein, (2009) for clustering mentions; and an entity resolver that links entities to Freebase profiles. The entity resolver is described in detail by Lao, et al. (2012). We then apply a simple heuristic to align the entities in the abstract and document: Let ME be the set of mentions of an entity E that are proper names. An entity EA from the abstract aligns to an entity ED from the document if the syntactic head token of some mention in MEA matches the head token of some mention in MED. If EA aligns with more than one document entity, we align it with the document entity that appears earliest. In general,</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1152–1161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="2643" citStr="Hulth, 2003" startWordPosition="411" endWordPosition="412">tion. Our notion of salience is similar to that of Boguraev and Kenney (1997): “discourse objects with high salience are the focus of attention”, inspired by earlier work on Centering Theory (Walker et al., 1998). Here we take a more empirical approach: salient entities are those that human readers deem most relevant to the document. The entity salience task in particular is briefly alluded to by Cornolti et al. (2013), and addressed in the context of Twitter messages by Meij et. al (2012). It is also similar in spirit to the much more common keyword extraction task (Tomokiyo and Hurst, 2003; Hulth, 2003). 2 Generating an entity salience corpus Rather than manually annotating a corpus, we automatically generate salience labels for an existing corpus of document/abstract pairs. We derive the labels using the assumption that the salient entities will be mentioned in the abstract, so we identify and align the entities in each text. Given a document and abstract, we run a standard NLP pipeline on both. This includes a POS tagger and dependency parser, comparable in accuracy to the current Stanford dependency parser (Klein and Manning, 2003); an NP extractor that uses POS tags and dependency edges </context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="3185" citStr="Klein and Manning, 2003" startWordPosition="497" endWordPosition="500">e much more common keyword extraction task (Tomokiyo and Hurst, 2003; Hulth, 2003). 2 Generating an entity salience corpus Rather than manually annotating a corpus, we automatically generate salience labels for an existing corpus of document/abstract pairs. We derive the labels using the assumption that the salient entities will be mentioned in the abstract, so we identify and align the entities in each text. Given a document and abstract, we run a standard NLP pipeline on both. This includes a POS tagger and dependency parser, comparable in accuracy to the current Stanford dependency parser (Klein and Manning, 2003); an NP extractor that uses POS tags and dependency edges to identify a set of entity mentions; a coreference resolver, comparable to that of Haghighi and Klein, (2009) for clustering mentions; and an entity resolver that links entities to Freebase profiles. The entity resolver is described in detail by Lao, et al. (2012). We then apply a simple heuristic to align the entities in the abstract and document: Let ME be the set of mentions of an entity E that are proper names. An entity EA from the abstract aligns to an entity ED from the document if the syntactic head token of some mention in MEA</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>William W Cohen</author>
</authors>
<title>Reading the web with learned syntactic-semantic inference rules.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1017--1026</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3508" citStr="Lao, et al. (2012)" startWordPosition="551" endWordPosition="554">l be mentioned in the abstract, so we identify and align the entities in each text. Given a document and abstract, we run a standard NLP pipeline on both. This includes a POS tagger and dependency parser, comparable in accuracy to the current Stanford dependency parser (Klein and Manning, 2003); an NP extractor that uses POS tags and dependency edges to identify a set of entity mentions; a coreference resolver, comparable to that of Haghighi and Klein, (2009) for clustering mentions; and an entity resolver that links entities to Freebase profiles. The entity resolver is described in detail by Lao, et al. (2012). We then apply a simple heuristic to align the entities in the abstract and document: Let ME be the set of mentions of an entity E that are proper names. An entity EA from the abstract aligns to an entity ED from the document if the syntactic head token of some mention in MEA matches the head token of some mention in MED. If EA aligns with more than one document entity, we align it with the document entity that appears earliest. In general, aligning an abstract to its source document is difficult (Daum´e III and Marcu, 2005). 205 Proceedings of the 14th Conference of the European Chapter of t</context>
</contexts>
<marker>Lao, Subramanya, Pereira, Cohen, 2012</marker>
<rawString>Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1017–1026. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Meij</author>
<author>Wouter Weerkamp</author>
<author>Maarten de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining,</booktitle>
<pages>563--572</pages>
<publisher>ACM.</publisher>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 563– 572. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>233--242</pages>
<contexts>
<context position="9987" citStr="Mihalcea and Csomai, 2007" startWordPosition="1628" endWordPosition="1631">ution system. Named, nominal, and pronominal mention counts clearly add significant information despite coreference errors. Lines 4-8 show results when our model features are incrementally added. Each feature raises accuracy, and together our simple set of features improves on the baseline by 34%. 4 Entity centrality All the features described above use only information available within the document. But articles are written with the assumption that the reader knows something about at least some of the entities involved. Inspired by results using Wikipedia to improve keyword extraction tasks (Mihalcea and Csomai, 2007; Xu et al., 2010), we experimented with a simple method for including background knowledge about each entity: an adaptation of PageRank (Page et al., 1999) to a graph of connected entities, in the spirit of Erkan and Radev’s work (2004) on summarization. Consider, for example, an article about a recent congressional budget debate. Although House Speaker John Boehner may be mentioned just once, we know he is likely salient because he is closely related to other entities in the article, such as Congress, the Republican Party, and Barack Obama. On the other hand, the Federal Emergency Management</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 233–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The PageRank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<tech>Technical Report 1999-66,</tech>
<institution>Stanford InfoLab.</institution>
<contexts>
<context position="10143" citStr="Page et al., 1999" startWordPosition="1656" endWordPosition="1659">eatures are incrementally added. Each feature raises accuracy, and together our simple set of features improves on the baseline by 34%. 4 Entity centrality All the features described above use only information available within the document. But articles are written with the assumption that the reader knows something about at least some of the entities involved. Inspired by results using Wikipedia to improve keyword extraction tasks (Mihalcea and Csomai, 2007; Xu et al., 2010), we experimented with a simple method for including background knowledge about each entity: an adaptation of PageRank (Page et al., 1999) to a graph of connected entities, in the spirit of Erkan and Radev’s work (2004) on summarization. Consider, for example, an article about a recent congressional budget debate. Although House Speaker John Boehner may be mentioned just once, we know he is likely salient because he is closely related to other entities in the article, such as Congress, the Republican Party, and Barack Obama. On the other hand, the Federal Emergency Management Agency may be mentioned repeatedly because it happened to host a major presidential speech, but it is less related to the story’s # Description P R F1 1 Po</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times annotated corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia, 6(12):e26752.</location>
<contexts>
<context position="5011" citStr="Sandhaus, 2008" startWordPosition="806" endWordPosition="807">ic mentions like CEO or state are often more ambiguous, so resolving them would be closer to the difficult problem of word sense disambiguation. Once we have entity alignments, we assume that a document entity is salient only if it has been aligned to some abstract entity. Ideally, we would like to induce a salience ranking over entities. Given the limitations of short abstracts, however, we settle for binary classification, which still captures enough salience information to be useful. 2.1 The New York Times corpus Our corpus of document/abstract pairs is the annotated New York Times corpus (Sandhaus, 2008). It includes 1.8 million articles published between January 1987 and June 2007; some 650,000 include a summary written by one of the newspaper’s library scientists. We selected a subset of the summarized articles from 2003-2007 by filtering out articles and summaries that were very short or very long, as well as several special article types (e.g., corrections and letters to the editor). Our full labeled dataset includes 110,639 documents with 2,229,728 labeled entities; about 14% are marked as salient. For comparison, the average summary is about 6% of the length (in tokens) of the associate</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Tomokiyo</author>
<author>Matthew Hurst</author>
</authors>
<title>A language model approach to keyphrase extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2629" citStr="Tomokiyo and Hurst, 2003" startWordPosition="407" endWordPosition="410">ed in the document in question. Our notion of salience is similar to that of Boguraev and Kenney (1997): “discourse objects with high salience are the focus of attention”, inspired by earlier work on Centering Theory (Walker et al., 1998). Here we take a more empirical approach: salient entities are those that human readers deem most relevant to the document. The entity salience task in particular is briefly alluded to by Cornolti et al. (2013), and addressed in the context of Twitter messages by Meij et. al (2012). It is also similar in spirit to the much more common keyword extraction task (Tomokiyo and Hurst, 2003; Hulth, 2003). 2 Generating an entity salience corpus Rather than manually annotating a corpus, we automatically generate salience labels for an existing corpus of document/abstract pairs. We derive the labels using the assumption that the salient entities will be mentioned in the abstract, so we identify and align the entities in each text. Given a document and abstract, we run a standard NLP pipeline on both. This includes a POS tagger and dependency parser, comparable in accuracy to the current Stanford dependency parser (Klein and Manning, 2003); an NP extractor that uses POS tags and dep</context>
</contexts>
<marker>Tomokiyo, Hurst, 2003</marker>
<rawString>Takashi Tomokiyo and Matthew Hurst. 2003. A language model approach to keyphrase extraction. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment-Volume 18, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Aravind Krishna Joshi</author>
<author>Ellen Friedman Prince</author>
</authors>
<title>Centering theory in discourse.</title>
<date>1998</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="2243" citStr="Walker et al., 1998" startWordPosition="339" endWordPosition="342">with a manual annotation study. Second, we train an entity salience model using features derived from a coreference resolution system. This model significantly outperforms a baseline model based on sentence position. Third, we suggest how our model can be improved by leveraging background information about the entities and their relationships – information not specifically provided in the document in question. Our notion of salience is similar to that of Boguraev and Kenney (1997): “discourse objects with high salience are the focus of attention”, inspired by earlier work on Centering Theory (Walker et al., 1998). Here we take a more empirical approach: salient entities are those that human readers deem most relevant to the document. The entity salience task in particular is briefly alluded to by Cornolti et al. (2013), and addressed in the context of Twitter messages by Meij et. al (2012). It is also similar in spirit to the much more common keyword extraction task (Tomokiyo and Hurst, 2003; Hulth, 2003). 2 Generating an entity salience corpus Rather than manually annotating a corpus, we automatically generate salience labels for an existing corpus of document/abstract pairs. We derive the labels usi</context>
</contexts>
<marker>Walker, Joshi, Prince, 1998</marker>
<rawString>Marilyn A Walker, Aravind Krishna Joshi, and Ellen Friedman Prince. 1998. Centering theory in discourse. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenpu Xing</author>
<author>Ali Ghorbani</author>
</authors>
<title>Weighted pagerank algorithm.</title>
<date>2004</date>
<booktitle>In Communication Networks and Services Research,</booktitle>
<pages>305--314</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="11493" citStr="Xing and Ghorbani, 2004" startWordPosition="1887" endWordPosition="1890"> 52.6 63.4 57.5 6 +mentions 59.3 61.3 60.3 7 +headline 59.1 61.9 60.5 8 + head-lex 59.7 63.6 61.6 9 + centrality 60.5 63.5 62.0 Table 3: Test set (P)recision, (R)ecall, and (F) measure of the salient class for some combinations of features listed in Table 2. The centrality feature is discussed in Section 4. key figures and less central to the article’s point. Our intuition about these relationships, mostly not explicit in the document, can be formalized in a local PageRank computation on the entity graph. 4.1 PageRank for computing centrality In the weighted version of the PageRank algorithm (Xing and Ghorbani, 2004), a web link is considered a weighted vote by the containing page for the landing page – a directed edge in a graph where each node is a webpage. In place of the web graph, we consider the graph of Freebase entities that appear in the document. The nodes are the entities, and a directed edge from E1 to E2 represents P(E2|E1), the probability of observing E2 in a document given that we have observed E1. We estimate P(E2|E1) by counting the number of training documents in which E1 and E2 co-occur and normalizing by the number of training documents in which E1 occurs. The nodes’ initial PageRank </context>
</contexts>
<marker>Xing, Ghorbani, 2004</marker>
<rawString>Wenpu Xing and Ali Ghorbani. 2004. Weighted pagerank algorithm. In Communication Networks and Services Research, pages 305–314. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songhua Xu</author>
<author>Shaohui Yang</author>
<author>Francis Chi-Moon Lau</author>
</authors>
<title>Keyword extraction and headline generation using novel word features.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="10005" citStr="Xu et al., 2010" startWordPosition="1632" endWordPosition="1635">l, and pronominal mention counts clearly add significant information despite coreference errors. Lines 4-8 show results when our model features are incrementally added. Each feature raises accuracy, and together our simple set of features improves on the baseline by 34%. 4 Entity centrality All the features described above use only information available within the document. But articles are written with the assumption that the reader knows something about at least some of the entities involved. Inspired by results using Wikipedia to improve keyword extraction tasks (Mihalcea and Csomai, 2007; Xu et al., 2010), we experimented with a simple method for including background knowledge about each entity: an adaptation of PageRank (Page et al., 1999) to a graph of connected entities, in the spirit of Erkan and Radev’s work (2004) on summarization. Consider, for example, an article about a recent congressional budget debate. Although House Speaker John Boehner may be mentioned just once, we know he is likely salient because he is closely related to other entities in the article, such as Congress, the Republican Party, and Barack Obama. On the other hand, the Federal Emergency Management Agency may be men</context>
</contexts>
<marker>Xu, Yang, Lau, 2010</marker>
<rawString>Songhua Xu, Shaohui Yang, and Francis Chi-Moon Lau. 2010. Keyword extraction and headline generation using novel word features. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung Yong</author>
<author>Shou King Foo</author>
</authors>
<title>A case study on inter-annotator agreement for word sense disambiguation.</title>
<date>1999</date>
<contexts>
<context position="6720" citStr="Yong and Foo, 1999" startWordPosition="1081" endWordPosition="1084">panying abstracts. Each entity was assigned a salience score in {1, 2, 3, 4}, where 1 is most salient. We then thresholded the annotators’ scores as salient/non-salient for comparison to the binary NYT labels. Table 1 summarizes the agreement results, measured by Cohen’s kappa. The experts’ agreement is probably best described as moderate,1 indicating that this is a difficult, subjective task, though deciding on the most salient entities (with score 1) is easier. Even without calibrating to the induced 1For comparison, word sense disambiguation tasks have reported agreement as low as κ = 0.3 (Yong and Foo, 1999). NYT salience scores, the expert vs. NYT agreement is close enough to the inter-expert agreement to convince us that our induced labels are a reasonable if somewhat noisy proxy for the experts’ definition of salience. Comparison κ{1,2} κ{1} A1 vs. A2 0.56 0.69 A1 vs. NYT 0.36 0.48 A2 vs. NYT 0.39 0.35 A1 &amp; A2 vs. NYT 0.43 0.38 Table 1: Annotator agreement for entity salience as a binary classification. A1 and A2 are expert annotators; NYT represents the induced labels. The first n column assumes annotator scores {1, 2} are salient and {3, 4} are non-salient, while the second n column assumes </context>
</contexts>
<marker>Yong, Foo, 1999</marker>
<rawString>Chung Yong and Shou King Foo. 1999. A case study on inter-annotator agreement for word sense disambiguation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>