<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000483">
<title confidence="0.9324735">
Combining Lexical and Grammatical Features to Improve Readability
Measures for First and Second Language Texts
</title>
<author confidence="0.5526705">
Michael J. Heilman Kevyn Collins- Jamie Callan Maxine Eskenazi
Thompson
</author>
<affiliation confidence="0.949607">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.8998125">
4502 Newell Simon Hall
Pittsburgh, PA 15213-8213
</address>
<email confidence="0.999702">
{mheilman,kct,callan,max}@cs.cmu.edu
</email>
<sectionHeader confidence="0.996674" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988444444444">
This work evaluates a system that uses in-
terpolated predictions of reading difficulty
that are based on both vocabulary and
grammatical features. The combined ap-
proach is compared to individual gram-
mar- and language modeling-based
approaches. While the vocabulary-based
language modeling approach outper-
formed the grammar-based approach,
grammar-based predictions can be com-
bined using confidence scores with the
vocabulary-based predictions to produce
more accurate predictions of reading dif-
ficulty for both first and second language
texts. The results also indicate that gram-
matical features may play a more impor-
tant role in second language readability
than in first language readability.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947714285714">
The REAP tutoring system (Heilman, et al. 2006),
aims to provide authentic reading materials of the
appropriate difficulty level, in terms of both vo-
cabulary and grammar, for English as a Second
Language students. An automatic measure of read-
ability that incorporated both lexical and gram-
matical features was thus needed.
For first language (L1) learners (i.e., children
learning their native tongue), reading level has
been predicted using a variety of techniques, based
on models of a student’s lexicon, grammatical sur-
face features such as sentence length (Flesch,
1948), or combinations of such features (Schwarm
and Ostendorf, 2005). It was shown by Collins-
Thompson and Callan (2004) that a vocabulary-
based language modeling approach was effective at
predicting the readability of grades 1 to 12 of Web
documents of varying length, even with high levels
of noise.
Prior work on first language readability by
Schwarm and Ostendorf (2005) incorporated
grammatical surface features such as parse tree
depth and average number of verb phrases. This
work combining grammatical and lexical features
was promising, but it was not clear to what extent
the grammatical features improved predictions.
Also, discussions with L2 instructors suggest
that a more detailed grammatical analysis of texts
that examines features such as passive voice and
various verb tenses can provide better features with
which to predict reading difficulty. One goal of
this work is to show that the use of pedagogically
motivated grammatical features (e.g., passive
voice, rather than the number of words per sen-
tence) can improve readability measures based on
lexical features alone.
One of the differences between L1 and L2 read-
ability is the timeline and processes by which first
and second languages are acquired. First language
acquisition begins at infancy, and the primary
grammatical structures of the target language are
acquired by age four in typically developing chil-
</bodyText>
<page confidence="0.989081">
460
</page>
<note confidence="0.797501">
Proceedings of NAACL HLT 2007, pages 460–467,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999536229166667">
dren (Bates, 2003). That is, most grammar is ac-
quired prior to the beginning of a child’s formal
education. Therefore, most grammatical features
seen at high reading levels such as high school are
present with similar frequencies at low reading
levels such as grades 1-3 that correspond to ele-
mentary school-age children. It should be noted
that sentence length is one grammar-related differ-
ence that can be observed as L1 reading level in-
creases. Sentences are kept short in texts for low
L1 reading levels in order to reduce the cognitive
load on child readers. The average sentence length
of texts increases with the age and reading level of
the intended audience. This phenomenon has been
utilized in early readability measures (Flesch,
1948). Vocabulary change, however, continues
even into adulthood, and has been shown to be a
more effective predictor of L1 readability than
simpler measures such as sentence length (Collins-
Thompson and Callan, 2005).
Second language learners, unlike their L1 coun-
terparts, are still very much in the process of ac-
quiring the grammar of their target language. In
fact, even intermediate and advanced students of
second languages, who correspond to higher L2
reading levels, often struggle with the grammatical
structures of their target language. This phenome-
non suggests that grammatical features may play a
more important role in predicting and measuring
L2 readability. That is not to say, however, that
vocabulary cannot be used to predict L2 reading
levels. Second language learners are learning both
vocabulary and grammar concurrently, and reading
materials for this population are chosen or au-
thored according to both lexical and grammatical
complexity. Therefore, the authors predict that a
readability measure for texts intended for second
language learners that incorporates both grammati-
cal and lexical features could clearly outperform a
measure based on only one of these two types of
features.
This paper begins with descriptions of the lan-
guage modeling and grammar-based prediction
systems. A description of the experiments follows
that covers both the evaluation metrics and corpora
used. Experimental results are presented, followed
by a discussion of these results, and a summary of
the conclusions of this work.
</bodyText>
<sectionHeader confidence="0.985231" genericHeader="method">
2 Language Model Readability Prediction
</sectionHeader>
<subsectionHeader confidence="0.838174">
for First Language Texts
</subsectionHeader>
<bodyText confidence="0.999988553191489">
Statistical language modeling exploits patterns of
use in language. To build a statistical model of
text, training examples are used to collect statistics
such as word frequency and order. Each training
example has a label that tells the model the ‘true’
category of the example. In this approach, one
statistical model is built for each grade level to be
predicted.
The statistical language modeling approach has
several advantages over traditional readability
formulas, which are usually based on linear regres-
sion with two or three variables. First, a language
modeling approach generally gives much better
accuracy for Web documents and short passages
(Collins-Thompson and Callan, 2004). Second,
language modeling provides a probability distribu-
tion across all grade models, not just a single pre-
diction. Third, language modeling provides more
data on the relative difficulty of each word in the
document. This might allow an application, for
example, to provide more accurate vocabulary as-
sistance.
The statistical model used for this study is
based on a variation of the multinomial Naïve
Bayes classifier. For a given text passage T, the
semantic difficulty of T relative to a specific grade
level G; is predicted by calculating the likelihood
that the words of T were generated from a repre-
sentative language model of G;. This likelihood is
calculated for each of a number of language mod-
els, corresponding to reading difficulty levels. The
reading difficulty of the passage is then estimated
as the grade level of the language model most
likely to have generated the passage T.
The language models employed in this work are
simple: they are based on unigrams and assume
that the probability of a token is independent of the
surrounding tokens. A unigram language model is
simply defined by a list of types (words) and their
individual probabilities. Although this is a weak
model, it can be effectively trained from less la-
beled data than more complex models, such as bi-
gram or trigram models. Additionally, higher
order n-gram models might capture grammatical as
well as lexical differences. The relative contribu-
tions of grammatical and lexical features were thus
better distinguished by using unigram language
</bodyText>
<page confidence="0.999086">
461
</page>
<bodyText confidence="0.999897">
models that more exclusively focus on lexical dif-
ferences.
In this language modeling approach, a genera-
tive model is assumed for a passage T, in which a
hypothetical author generates the tokens of T by:
</bodyText>
<listItem confidence="0.996731142857143">
1. Choosing a grade language model, Gi,
from the set G = {Gi} of 12 unigram language
models, according to a prior probability distri-
bution P(Gi).
2. Choosing a passage length |T |in tokens ac-
cording to a probability distribution P(|T|).
3. Sampling |T |tokens from Gi’s multinomial
</listItem>
<bodyText confidence="0.804491222222222">
word distribution according to the ‘naïve’ as-
sumption that each token is independent of all
other tokens in the passage, given the language
model Gi.
These assumptions lead to the following expres-
sion for the probability of T being generated by
language model Gi according to a multinomial dis-
tribution:
Next, according to Bayes’ Theorem:
</bodyText>
<equation confidence="0.992423">
P(Gi  |T) = P(Gi)P(T  |Gi)
P(T)
</equation>
<bodyText confidence="0.86826">
Substituting (1) into (2), taking logarithms, and
simplifying produces:
</bodyText>
<equation confidence="0.990769625">
T) = E C w P
( ) log
w V
�
log ( ) ! log log
C w + R + S
w V
�
</equation>
<bodyText confidence="0.879816">
where V is the list of all types in the passage T, w is
a type in V, and C(w) is the number of tokens with
type w in T. For simplicity, the factor R represents
the contribution of the prior P(Gi), and S represents
the contribution of the passage length |T|, given the
grade level.
Two further assumptions are made to simplify
the illustration:
</bodyText>
<listItem confidence="0.969253">
1. That all grades are equally likely a priori.
</listItem>
<equation confidence="0.8487276">
P G
( ) = where NG is the number
1
i
NG
</equation>
<bodyText confidence="0.998614066666667">
of grade levels. For example, if there are 12
grade levels, then NG = 12. This allows log R to
be ignored.
2. That all passage lengths (up to a maximum
length M) are equally likely. This allows log S
to be ignored.
These may be poor assumptions in a real appli-
cation, but they can be easily included or excluded
in the model as desired. The log C(w)! term can
also be ignored because it is constant across levels.
Under these conditions, an extremely simple form
for the grade likelihood remains. In order to find
which model Gi maximizes Equation (3), the
model which Gi that maximizes the following
equation must be found:
</bodyText>
<equation confidence="0.9860195">
L T Gi �
(  |) = C(w) log P(w  |Gi
w V
�
</equation>
<bodyText confidence="0.999987222222222">
This is straightforward to compute: for each token
in the passage T, the log probability of the token
according to the language model of Gi is calcu-
lated. Summing the log probabilities of all tokens
produces the overall likelihood of the passage,
given the grade. The grade level with the maxi-
mum likelihood is then chosen as the final read-
ability level prediction.
This study employs a slightly more sophisti-
cated extension of this model, in which a sliding
window is moved across the text, with a grade pre-
diction being made for each window. This results
in a distribution of grade predictions. The grade
level corresponding to a given percentile of this
distribution is chosen as the prediction for the en-
tire document. The values used in these experi-
ments for the percentile thresholds for L1 and L2
were chosen by accuracy on held-out data.
</bodyText>
<sectionHeader confidence="0.98296" genericHeader="method">
3 Grammatical Construction Readability
</sectionHeader>
<subsectionHeader confidence="0.850493">
Prediction for Second Language Texts
</subsectionHeader>
<bodyText confidence="0.9999415">
The following sections describe the approach to
predicting readability based on grammatical fea-
tures. As with any classifier, two components are
required to classify texts by their reading level:
first, a definition for and method of identifying
features; second, an algorithm for using these fea-
tures to classify a given text. A third component,
training data, is also necessary in this classification
</bodyText>
<figure confidence="0.856829588235294">
P(T
)=P(|T|)|T|!
|
i
C w
( ) !
(
P
w
 |Gi
( )
) C w
ri
w V
�
.
|
</figure>
<equation confidence="0.895593444444445">
log (
P G i
−
)
,
(  |i
w G
That is,
)
</equation>
<page confidence="0.993716">
462
</page>
<bodyText confidence="0.9930425">
task. The corpus of materials used for training and
testing is discussed in a subsequent section.
matching sentence is, “The student was reading a
book,” shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.79953">
3.1 Features for Grammar-based Prediction
</subsectionHeader>
<bodyText confidence="0.997205295454545">
L2 learners usually learn grammatical patterns ex-
plicitly from grammar explanations in L2 text-
books, unlike their L1 counterparts who learn them
implicitly through natural interactions. Grammati-
cal features would therefore seem to be an essential
component of an automatic readability measure for
L2 learners, who must actively acquire both the
lexicon and grammar of their target language.
The grammar-based readability measure relies
on being able to automatically identify grammati-
cal constructions in text. Doing so is a multi-step
process that begins by syntactically parsing the
document. The Stanford Parser (Klein and Man-
ning, 2002) was used to produce constituent struc-
ture trees. The choice of parser is not essential to
the approach, although the accuracy of parsing
does play a role in successful identification of cer-
tain grammatical patterns. PCFG scores from the
parser were also used to filter out some of the ill-
formed text present in the test corpora. The default
training set of Penn Treebank (Marcus et al. 1993)
was used for the parser because the domain and
style of those texts actually matches fairly well
with the domain and style of the texts on which a
reading level predictor for second language learn-
ers might be used.
Once a document is parsed, the predictor uses
Tgrep2 (Rohde, 2005), a tree structure searching
tool, to identify instances of the target patterns. A
Tgrep2 pattern defines dominance, sisterhood,
precedence, and other relationships between nodes
in the parse tree for a sentence. A pattern can also
place constraints on the terminal symbols (e.g.,
words and punctuation), such that a pattern might
require a form of the copula “be” to exist in a cer-
tain position in the construction. An example of a
TGrep2 search pattern for the progressive verb
tense is the following:
“VP &lt; /^VB/ &lt; (VP &lt; VBG)”
Searching for this pattern returns sentences in
which a verb phrase (VP) dominates an auxiliary
verb (whose symbol begins with VB) as well as
another verb phrase, which in turn dominates a
verb in gerund form (VBG). An example of a
</bodyText>
<figure confidence="0.984342">
S
NP
The student
NP
was
reading a book
</figure>
<figureCaption confidence="0.960125">
Figure 2: The parse tree for an example sentence
that matches a pattern for progressive verb tense.
</figureCaption>
<bodyText confidence="0.999926571428571">
A set of 22 relevant grammatical constructions
were identified from grammar textbooks for three
different ESL levels (Fuchs et al., 2005). These
grammar textbooks had different authors and pub-
lishers than the ones used in the evaluation corpora
in order to minimize the chance of experimental
results not generalizing beyond the specific materi-
als employed in this study. The ESL levels corre-
spond to the low-intermediate (hereafter, level 3),
high-intermediate (level 4), and advanced (level 5)
courses at the University of Pittsburgh’s English
Language Institute. The constructions identified in
these grammar textbooks were then implemented
in the form of Tgrep2 patterns.
</bodyText>
<table confidence="0.99973625">
Feature Lowest Level Highest Level
Passive Voice 0.11 0.71
Past Participle 0.28 1.63
Perfect Tense 0.01 0.33
Relative Clause 0.54 0.60
Continuous 0.19 0.27
Tense
Modal 0.80 1.44
</table>
<tableCaption confidence="0.90704975">
Table 1: The rates of occurrence per 100 words of
a few of the features used by the grammar-based
predictor. Rates are shown for the lowest (2) and
highest (5) levels in the L2 corpus.
</tableCaption>
<bodyText confidence="0.7652075">
The rate of occurrence of constructions was
calculated on a per word basis. A per-word rather
</bodyText>
<figure confidence="0.711392333333333">
VP
VBD VP
VBG
</figure>
<page confidence="0.998759">
463
</page>
<bodyText confidence="0.999879714285714">
than a per-sentence measure was chosen because a
per-sentence measure would depend too greatly on
sentence length, which also varies by level. It was
also desirable to avoid having sentence length con-
founded with other features. Table 1 shows that
the rates of occurrence of certain constructions be-
come more frequent as level increases. This sys-
tematic variation across levels is the basis for the
grammar-based readability predictions.
A second feature set was defined that consisted
of 12 grammatical features that could easily be
identified without computationally intensive syn-
tactic parsing. These features included sentence
length, the various verb forms in English, includ-
ing the present, progressive, past, perfect, continu-
ous tenses, as well as part of speech labels for
words. The goal of using a second feature set was
to examine how dependent prediction quality was
on a specific set of features, as well as to test the
extent to which the output of syntactic parsing
might improve prediction accuracy.
</bodyText>
<subsectionHeader confidence="0.9335885">
3.2 Algorithm for Grammatical Feature-
based Classification
</subsectionHeader>
<bodyText confidence="0.999845">
A k-Nearest Neighbor (kNN) algorithm is used for
classification based on the grammatical features
described above. The kNN algorithm is an in-
stance-based learning technique originally devel-
oped by Cover and Hart (1967) by which a test
instance is classified according to the classifica-
tions of a given number (k) of training instances
closest to it. Distance is defined in this work as the
Euclidean distance of feature vectors. Mitchell
(1997) provides more details on the kNN algo-
rithm. This algorithm was chosen because it has
been shown to be effective in text classification
tasks when compared to other popular methods
(Yang 1999). A k value of 12 was chosen because
it provided the best performance on held-out data.
Additionally, it is straightforward to calculate
a confidence measure with which kNN predictions
can be combined with predictions from other clas-
sifiers—in this case with predictions from the uni-
gram language modeling-based approach described
above. A confidence measure was important in
this task because it provided a means with which to
combine the grammar-based predictions with the
predictions from the language modeling-based
predictor while maintaining separate models for
each type of feature. These separate models were
maintained to better determine the relative contri-
butions of grammatical and lexical features.
A static linear interpolation of predictions us-
ing the two approaches led to only minimal reduc-
tions of prediction error, likely because predictions
from the poorer performing grammar-based classi-
fier were always given the same weight. However,
with the confidence measures, predictions from the
grammar-based classifier could be given more
weight when the confidence measure was high, and
less weight when the measure was low and the
predictions were likely to be inaccurate. The case-
dependent interpolation of prediction values al-
lowed for the effective combination of language
modeling- and grammar-based predictions.
The confidence measure employed is the pro-
portion of the k most similar training examples, or
nearest neighbors, that agree with the final label
chosen for a given test document. For example, if
seven of ten neighbors have the same label, then
the confidence score will be 0.6. The interpolated
readability prediction value is calculated as fol-
lows:
</bodyText>
<equation confidence="0.888038">
Lr = LLm + CkNN * LcR,
</equation>
<bodyText confidence="0.999808714285714">
where LLm is the language model-based prediction,
LcR is the grammar-based prediction from the kNN
algorithm, and CkNN is the confidence value for the
kNN prediction. The language modeling approach
is treated as a black box, but it would likely be
beneficial to have confidence measures for it as
well.
</bodyText>
<sectionHeader confidence="0.93239" genericHeader="method">
4 Descriptions of Experiments
</sectionHeader>
<bodyText confidence="0.999993142857143">
This section describes the experiments used to test
the hypothesis that grammar-based features can
improve readability measures for English, espe-
cially for second language texts. The measures
and cross-validation setup are described. A de-
scription of the evaluation corpora of labeled first
and second language texts follows.
</bodyText>
<subsectionHeader confidence="0.966611">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999617333333333">
Two measurements were used in evaluating the
effectiveness of the reading level predictions.
First, the correlation coefficient evaluated whether
the trends of prediction values matched the trends
for human-labeled texts. Second, the mean
squared error of prediction values provided a
</bodyText>
<page confidence="0.999012">
464
</page>
<bodyText confidence="0.9999938">
measure of how correct each of the predictors was
on average, penalizing more severe errors more
heavily. Mean square error was used rather than
simple accuracy (i.e., number correct divided by
sample size) because the task of readability predic-
tion is more akin to regression than classification.
Evaluation measures such as accuracy, precision,
and recall are thus less meaningful for readability
prediction tasks because they do not capture the
fact that an error of 4 levels is more costly than an
error of a single level.
A nine-fold cross-validation was employed.
The data was first split into ten sets. One set was
used as held-out data for selecting the parameter k
for the kNN algorithm and the percentile value for
the language modeling predictor, and then the re-
maining nine were used to evaluate the quality of
predictions. Each of these nine was in turn se-
lected as the test set, and the other eight were used
as training data.
</bodyText>
<subsectionHeader confidence="0.999306">
4.2 Corpora of Labeled Texts
</subsectionHeader>
<bodyText confidence="0.999991851851852">
Two corpora of labeled texts were used in the
evaluation. The first corpus was from a set of texts
gathered from the Web for a prior evaluation of the
language modeling approach. The 362 texts had
been assigned L1 levels (1-12) by grade school
teachers, and consisted of approximately 250,000
words. For more details on the L1 corpus, see
(Collins-Thompson and Callan, 2005).
The second corpora consisted of textbook mate-
rials (Adelson-Goldstein and Howard, 2004, for
level 2; Ediger and Pavlik, 2000, for levels 3 and 4;
Silberstein, 2002, for level 5) from a series of Eng-
lish as a Second Language reading courses at the
English Language Institute at the University of
Pittsburgh. The four reading practice textbooks
that constitute this corpus were from separate au-
thors and publishers than the grammar textbooks
used to select and define grammatical features.
The reading textbooks in the corpus are used in
courses intended for beginning (level 2) through
advanced (level 5) students. The textbooks were
scanned into electronic format, and divided into
fifty roughly equally sized files. This second lan-
guage corpus consisted of approximately 200,000
words.
Although the sources and formats of the two
corpora were different, they share a number of
characteristics. Their size was roughly equal. The
documents in both were also fairly but not per-
fectly evenly distributed across the levels. Both
corpora also contained a significant amount of
noise which made accurate prediction of reading
level more challenging. The L1 corpus was from
the Web, and therefore contained navigation
menus, links, and the like. The texts in the L2 cor-
pus also contained significant levels of noise due to
the inclusion of directions preceding readings, ex-
ercises and questions following readings, as well as
labels on figures and charts. The scanned files
were not hand-corrected in this study, in part to test
that the measures are robust to noise, which is pre-
sent in the Web documents for which the readabil-
ity measures are employed in the REAP tutoring
system.
The grammar-based prediction seems to be
more significantly negatively affected by the noise
in the two corpora because the features rely more
on dependencies between different words in the
text. For example, if a word happened to be part of
an image caption rather than a well-formed sen-
tence, the unigram language modeling approach
would only be affected for that word, but the
grammar-based approach might be affected for
features spanning an entire clause or sentence.
</bodyText>
<sectionHeader confidence="0.993095" genericHeader="method">
5 Results of Experiments
</sectionHeader>
<bodyText confidence="0.999993380952381">
The results show that for both the first and sec-
ond language corpora, the language modeling
(LM) approach alone produced more accurate pre-
dictions than the grammar-based approach alone.
The mean squared error values (Table 2) were
lower, and the correlation coefficients (Table 3)
were higher for the LM predictor than the gram-
mar-based predictor.
The results also indicate that while grammar-
based predictions are not as accurate as the vo-
cabulary-based scores, they can be combined with
vocabulary-based scores to produce more accurate
interpolated scores. The interpolated predictions
combined by using the kNN confidence measure
were slightly and in most tests significantly more
accurate in terms of mean squared error than the
predictions from either single measure. Interpola-
tion using the first set of grammatical features led
to 7% and 22% reductions in mean squared error
on the L1 and L2 corpora, respectively. These re-
sults were verified using a one-tailed paired t-test
</bodyText>
<page confidence="0.998834">
465
</page>
<bodyText confidence="0.9547015">
of the squared error values of the predictions, and
significance levels are indicated in Table 2.
</bodyText>
<table confidence="0.999356285714286">
Mean Squared Error Values
Test Set (Num. Levels) L1(12) L2(4)
Language Modeling 5.02 0.51
Grammar 10.27 1.08
Interpolation 4.65* 0.40**
Grammar2 (feature set #2) 12.77 1.26
Interp2. (feature set #2) 4.73 0.43*
</table>
<tableCaption confidence="0.998708">
Table 2. Comparison of Mean Squared Error of
</tableCaption>
<bodyText confidence="0.80176975">
predictions compared to human labels for different
methods. Interpolated values are significantly bet-
ter compared to language modeling predictions
where indicated (* = p&lt;0.05, ** = p&lt;0.01).
</bodyText>
<table confidence="0.998312714285714">
Correlation Coefficients
Test Set (Num. Levels) L1(12) L2(4)
Language Modeling 0.71 0.80
Grammar 0.46 0.55
Interpolation 0.72 0.83
Grammar2 (feature set #2) 0.34 0.48
Interp2. (feature set #2) 0.72 0.81
</table>
<tableCaption confidence="0.985108">
Table 3. Comparison of Correlation Coefficients
</tableCaption>
<bodyText confidence="0.996866296296296">
of prediction values to human labels for different
prediction methods.
The trends were similar for both sets of gram-
matical features. However, the first set of features
that included complex syntactic constructs led to
better performance than the second set, which in-
cluded only verb tenses, part of speech labels, and
sentence length. Therefore, when syntactic parsing
is not feasible because of corpora size, it seems
that grammatical features requiring only part-of-
speech tagging and word counts may still improve
readability predictions. This is practically impor-
tant because parsing can be too computationally
intensive for large corpora.
All prediction methods performed better, in
terms of correlations, on the L2 corpus than on the
L1 corpus. The L2 corpus is somewhat smaller in
size and should, if only on the basis of training ma-
terial available to the prediction algorithms, actu-
ally be more difficult to predict than the L1 corpus.
To ensure that the range of levels was not causing
the four-level L2 corpus to have higher predictions
than the twelve-level L1 corpus, the L1 corpus was
also divided into four bins (grades 1-3, 4-6, 7-9,
10-12). The accuracy of predictions for the binned
version of the L1 corpus was not substantially dif-
ferent than for the 12-level version.
</bodyText>
<sectionHeader confidence="0.999363" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999990318181818">
In the experimental tests, the LM approach was
more effective for measuring both L1 and L2 read-
ability. There are several potential causes of this
effect. First, the language modeling approach can
utilize all the words as they appear in the text as
features, while the grammatical features were cho-
sen and defined manually. As a result, the LM
approach can make measurements on a text for as
many features as there are words in its lexicon.
Additionally, the noise present in the corpora likely
affected the grammar-based approach dispropor-
tionately more because that method relies on accu-
rate parsing of relationships between words.
Additionally, English is a morphologically im-
poverished language compared to most languages.
Text classification, information retrieval, and many
other human language technology tasks can be ac-
complished for English without accounting for
grammatical features such as morphological inflec-
tions. For example, an information retrieval sys-
tem can perform reasonably well in English
without performing stemming, which does not
greatly increase performance except when queries
and documents are short (Krovetz, 1993).
However, most languages have a rich morphol-
ogy by which a single root form may have thou-
sands or perhaps millions of inflected or derived
forms. Language technologies must account for
morphological features in such languages or the
vocabulary grows so large that it becomes unman-
ageable. Lee (2004), for example, showed that
morphological analysis can improve the quality of
statistical machine translation for Arabic. Thus it
seems that grammatical features could contribute
even more to measures of readability for texts in
other languages.
That said, the use of grammatical features ap-
pears to play a more important role in readability
measures for L2 than for L1. When interpolated
with grammar-based scores, the reduction of mean
squared error over the language modeling approach
for L1 was only 7%, while for L2 the reduction or
squared error was 22%. An evaluation on corpora
with less noise would likely bring out these differ-
</bodyText>
<page confidence="0.998611">
466
</page>
<bodyText confidence="0.999965">
ences further and show grammar to be an even
more important factor in second language readabil-
ity. This result is consistent with the fact that sec-
ond language learners are still in the process of
acquiring the basic grammatical constructs of their
target language.
</bodyText>
<sectionHeader confidence="0.998837" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999959533333334">
The results of this work suggest that grammatical
features can play a role in predicting reading diffi-
culty levels for both first and second language texts
in English. Although a vocabulary-based language
modeling approach outperformed the grammar-
based predictor, an interpolated measure using
confidence scores for the grammar-based predic-
tions showed improvement over both individual
measures. Also, grammar appears to play a more
important role in second language readability than
in first language readability. Ongoing work aims
to improve grammar-based readability by reducing
noise in training data, automatically creating larger
grammar feature sets, and applying more sophisti-
cated modeling techniques.
</bodyText>
<sectionHeader confidence="0.99829" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999879">
We would like to acknowledge Lori Levin for use-
ful advice regarding grammatical constructions, as
well as the anonymous reviewers for their sugges-
tions.
This material is based on work supported by
NSF grant IIS-0096139 and Dept. of Education
grant R305G03123. Any opinions, findings, con-
clusions or recommendations expressed in this ma-
terial are the authors&apos;, and do not necessarily reflect
those of the sponsors.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999802018867924">
J. Adelson-Goldstein and L. Howard. 2004. Read and
Reflect 1. Oxford University Press, USA.
E. Bates. 2003. On the nature and nurture of language.
In R. Levi-Montalcini, D. Baltimore, R. Dulbecco, F.
Jacob, E. Bizzi, P. Calissano, &amp; V. Volterra (Eds.),
Frontiers of biology: The brain of Homo sapiens (pp.
241–265). Rome: Istituto della Enciclopedia Italiana
fondata da Giovanni Trecanni.
M. Fuchs, M. Bonner, M. Westheimer. 2005. Focus on
Grammar, 3rd Edition. Pearson ESL.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty.
Proceedings of the HLT/NAACL Annual Conference.
T. Cover and P. Hart. 1967. Nearest neighbor pattern
classification. IEEE Transactions on Information
Theory, 13, 21-27.
A. Ediger and C. Pavlik. 2000. Reading Connections
Intermediate. Oxford University Press, USA.
A. Ediger and C. Pavlik. 2000. Reading Connections
High Intermediate. Oxford University Press, USA.
M. Heilman, K. Collins-Thompson, J. Callan &amp; M. Es-
kenazi. 2006. Classroom success of an Intelligent Tu-
toring System for lexical practice and reading
comprehension. Proceedings of the Ninth Interna-
tional Conference on Spoken Language Processing.
D. Klein and C. D. Manning. 2002. Fast Exact Inference
with a Factored Model for Natural Language Parsing.
Advances in Neural Information Processing Systems
15 (NIPS 2002), December 2002.
R. Krovetz. 1993. Viewing morphology as an inference
process. SIGIR-93, 191–202.
Y. Lee. 2004. Morphological Analysis for Statistical
Machine Translation. Proceedings of the
HLT/NAACL Annual Conference.
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993.
&amp;quot;Building a large annotated corpus of English: the
Penn Treebank.&amp;quot; Computational Linguistics, 19(2).
T. Mitchell. 1997. Machine Learning. The McGraw-
Hill Companies, Inc. pp. 231-236.
D. Rohde. 2005. Tgrep2 User Manual.
http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf.
S. Schwarm, and M. Ostendorf. 2005. Reading Level
Assessment Using Support Vector Machines and Sta-
tistical Language Models. Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
S. Silberstein, B. K. Dobson, and M. A. Clarke. 2002.
Reader&apos;s Choice, 4th edition. University of Michigan
Press/ESL.
Y. Yang. 1999. A re-examination of text categorization
methods. Proceedings of ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR&apos;99, pp 42--49).
</reference>
<page confidence="0.999289">
467
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823478">
<title confidence="0.999548">Combining Lexical and Grammatical Features to Improve Measures for First and Second Language Texts</title>
<author confidence="0.999982">Michael J Heilman Kevyn Collins- Jamie Callan Maxine Eskenazi</author>
<affiliation confidence="0.95806025">Thompson Language Technologies School of Computer Carnegie Mellon</affiliation>
<address confidence="0.9948545">4502 Newell Simon Pittsburgh, PA 15213-8213</address>
<email confidence="0.999113">mheilman@cs.cmu.edu</email>
<email confidence="0.999113">kct@cs.cmu.edu</email>
<email confidence="0.999113">callan@cs.cmu.edu</email>
<email confidence="0.999113">max@cs.cmu.edu</email>
<abstract confidence="0.999001368421052">This work evaluates a system that uses interpolated predictions of reading difficulty that are based on both vocabulary and grammatical features. The combined approach is compared to individual grammarand language modeling-based approaches. While the vocabulary-based language modeling approach outperformed the grammar-based approach, grammar-based predictions can be combined using confidence scores with the vocabulary-based predictions to produce more accurate predictions of reading difficulty for both first and second language texts. The results also indicate that grammatical features may play a more important role in second language readability than in first language readability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Adelson-Goldstein</author>
<author>L Howard</author>
</authors>
<title>Read and Reflect 1.</title>
<date>2004</date>
<publisher>Oxford University Press, USA.</publisher>
<contexts>
<context position="20635" citStr="Adelson-Goldstein and Howard, 2004" startWordPosition="3342" endWordPosition="3345"> evaluate the quality of predictions. Each of these nine was in turn selected as the test set, and the other eight were used as training data. 4.2 Corpora of Labeled Texts Two corpora of labeled texts were used in the evaluation. The first corpus was from a set of texts gathered from the Web for a prior evaluation of the language modeling approach. The 362 texts had been assigned L1 levels (1-12) by grade school teachers, and consisted of approximately 250,000 words. For more details on the L1 corpus, see (Collins-Thompson and Callan, 2005). The second corpora consisted of textbook materials (Adelson-Goldstein and Howard, 2004, for level 2; Ediger and Pavlik, 2000, for levels 3 and 4; Silberstein, 2002, for level 5) from a series of English as a Second Language reading courses at the English Language Institute at the University of Pittsburgh. The four reading practice textbooks that constitute this corpus were from separate authors and publishers than the grammar textbooks used to select and define grammatical features. The reading textbooks in the corpus are used in courses intended for beginning (level 2) through advanced (level 5) students. The textbooks were scanned into electronic format, and divided into fift</context>
</contexts>
<marker>Adelson-Goldstein, Howard, 2004</marker>
<rawString>J. Adelson-Goldstein and L. Howard. 2004. Read and Reflect 1. Oxford University Press, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bates</author>
</authors>
<title>On the nature and nurture of language. In</title>
<date>2003</date>
<contexts>
<context position="3171" citStr="Bates, 2003" startWordPosition="468" endWordPosition="469">ogically motivated grammatical features (e.g., passive voice, rather than the number of words per sentence) can improve readability measures based on lexical features alone. One of the differences between L1 and L2 readability is the timeline and processes by which first and second languages are acquired. First language acquisition begins at infancy, and the primary grammatical structures of the target language are acquired by age four in typically developing chil460 Proceedings of NAACL HLT 2007, pages 460–467, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics dren (Bates, 2003). That is, most grammar is acquired prior to the beginning of a child’s formal education. Therefore, most grammatical features seen at high reading levels such as high school are present with similar frequencies at low reading levels such as grades 1-3 that correspond to elementary school-age children. It should be noted that sentence length is one grammar-related difference that can be observed as L1 reading level increases. Sentences are kept short in texts for low L1 reading levels in order to reduce the cognitive load on child readers. The average sentence length of texts increases with th</context>
</contexts>
<marker>Bates, 2003</marker>
<rawString>E. Bates. 2003. On the nature and nurture of language. In R. Levi-Montalcini, D. Baltimore, R. Dulbecco, F. Jacob, E. Bizzi, P. Calissano, &amp; V. Volterra (Eds.), Frontiers of biology: The brain of Homo sapiens (pp. 241–265). Rome: Istituto della Enciclopedia Italiana fondata da Giovanni Trecanni.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fuchs</author>
<author>M Bonner</author>
<author>M Westheimer</author>
</authors>
<date>2005</date>
<booktitle>Focus on Grammar, 3rd Edition. Pearson ESL.</booktitle>
<contexts>
<context position="13802" citStr="Fuchs et al., 2005" startWordPosition="2260" endWordPosition="2263"> example of a TGrep2 search pattern for the progressive verb tense is the following: “VP &lt; /^VB/ &lt; (VP &lt; VBG)” Searching for this pattern returns sentences in which a verb phrase (VP) dominates an auxiliary verb (whose symbol begins with VB) as well as another verb phrase, which in turn dominates a verb in gerund form (VBG). An example of a S NP The student NP was reading a book Figure 2: The parse tree for an example sentence that matches a pattern for progressive verb tense. A set of 22 relevant grammatical constructions were identified from grammar textbooks for three different ESL levels (Fuchs et al., 2005). These grammar textbooks had different authors and publishers than the ones used in the evaluation corpora in order to minimize the chance of experimental results not generalizing beyond the specific materials employed in this study. The ESL levels correspond to the low-intermediate (hereafter, level 3), high-intermediate (level 4), and advanced (level 5) courses at the University of Pittsburgh’s English Language Institute. The constructions identified in these grammar textbooks were then implemented in the form of Tgrep2 patterns. Feature Lowest Level Highest Level Passive Voice 0.11 0.71 Pa</context>
</contexts>
<marker>Fuchs, Bonner, Westheimer, 2005</marker>
<rawString>M. Fuchs, M. Bonner, M. Westheimer. 2005. Focus on Grammar, 3rd Edition. Pearson ESL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>Proceedings of the HLT/NAACL Annual Conference.</booktitle>
<contexts>
<context position="6183" citStr="Collins-Thompson and Callan, 2004" startWordPosition="933" endWordPosition="936"> language. To build a statistical model of text, training examples are used to collect statistics such as word frequency and order. Each training example has a label that tells the model the ‘true’ category of the example. In this approach, one statistical model is built for each grade level to be predicted. The statistical language modeling approach has several advantages over traditional readability formulas, which are usually based on linear regression with two or three variables. First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004). Second, language modeling provides a probability distribution across all grade models, not just a single prediction. Third, language modeling provides more data on the relative difficulty of each word in the document. This might allow an application, for example, to provide more accurate vocabulary assistance. The statistical model used for this study is based on a variation of the multinomial Naïve Bayes classifier. For a given text passage T, the semantic difficulty of T relative to a specific grade level G; is predicted by calculating the likelihood that the words of T were generated from</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>K. Collins-Thompson and J. Callan. 2004. A language modeling approach to predicting reading difficulty. Proceedings of the HLT/NAACL Annual Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>P Hart</author>
</authors>
<title>Nearest neighbor pattern classification.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<pages>21--27</pages>
<contexts>
<context position="16108" citStr="Cover and Hart (1967)" startWordPosition="2625" endWordPosition="2628">h, including the present, progressive, past, perfect, continuous tenses, as well as part of speech labels for words. The goal of using a second feature set was to examine how dependent prediction quality was on a specific set of features, as well as to test the extent to which the output of syntactic parsing might improve prediction accuracy. 3.2 Algorithm for Grammatical Featurebased Classification A k-Nearest Neighbor (kNN) algorithm is used for classification based on the grammatical features described above. The kNN algorithm is an instance-based learning technique originally developed by Cover and Hart (1967) by which a test instance is classified according to the classifications of a given number (k) of training instances closest to it. Distance is defined in this work as the Euclidean distance of feature vectors. Mitchell (1997) provides more details on the kNN algorithm. This algorithm was chosen because it has been shown to be effective in text classification tasks when compared to other popular methods (Yang 1999). A k value of 12 was chosen because it provided the best performance on held-out data. Additionally, it is straightforward to calculate a confidence measure with which kNN predictio</context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>T. Cover and P. Hart. 1967. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13, 21-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ediger</author>
<author>C Pavlik</author>
</authors>
<title>Reading Connections Intermediate.</title>
<date>2000</date>
<publisher>Oxford University Press, USA.</publisher>
<contexts>
<context position="20673" citStr="Ediger and Pavlik, 2000" startWordPosition="3349" endWordPosition="3352">se nine was in turn selected as the test set, and the other eight were used as training data. 4.2 Corpora of Labeled Texts Two corpora of labeled texts were used in the evaluation. The first corpus was from a set of texts gathered from the Web for a prior evaluation of the language modeling approach. The 362 texts had been assigned L1 levels (1-12) by grade school teachers, and consisted of approximately 250,000 words. For more details on the L1 corpus, see (Collins-Thompson and Callan, 2005). The second corpora consisted of textbook materials (Adelson-Goldstein and Howard, 2004, for level 2; Ediger and Pavlik, 2000, for levels 3 and 4; Silberstein, 2002, for level 5) from a series of English as a Second Language reading courses at the English Language Institute at the University of Pittsburgh. The four reading practice textbooks that constitute this corpus were from separate authors and publishers than the grammar textbooks used to select and define grammatical features. The reading textbooks in the corpus are used in courses intended for beginning (level 2) through advanced (level 5) students. The textbooks were scanned into electronic format, and divided into fifty roughly equally sized files. This se</context>
</contexts>
<marker>Ediger, Pavlik, 2000</marker>
<rawString>A. Ediger and C. Pavlik. 2000. Reading Connections Intermediate. Oxford University Press, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ediger</author>
<author>C Pavlik</author>
</authors>
<title>Reading Connections High Intermediate.</title>
<date>2000</date>
<publisher>Oxford University Press, USA.</publisher>
<contexts>
<context position="20673" citStr="Ediger and Pavlik, 2000" startWordPosition="3349" endWordPosition="3352">se nine was in turn selected as the test set, and the other eight were used as training data. 4.2 Corpora of Labeled Texts Two corpora of labeled texts were used in the evaluation. The first corpus was from a set of texts gathered from the Web for a prior evaluation of the language modeling approach. The 362 texts had been assigned L1 levels (1-12) by grade school teachers, and consisted of approximately 250,000 words. For more details on the L1 corpus, see (Collins-Thompson and Callan, 2005). The second corpora consisted of textbook materials (Adelson-Goldstein and Howard, 2004, for level 2; Ediger and Pavlik, 2000, for levels 3 and 4; Silberstein, 2002, for level 5) from a series of English as a Second Language reading courses at the English Language Institute at the University of Pittsburgh. The four reading practice textbooks that constitute this corpus were from separate authors and publishers than the grammar textbooks used to select and define grammatical features. The reading textbooks in the corpus are used in courses intended for beginning (level 2) through advanced (level 5) students. The textbooks were scanned into electronic format, and divided into fifty roughly equally sized files. This se</context>
</contexts>
<marker>Ediger, Pavlik, 2000</marker>
<rawString>A. Ediger and C. Pavlik. 2000. Reading Connections High Intermediate. Oxford University Press, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>M Eskenazi</author>
</authors>
<title>Classroom success of an Intelligent Tutoring System for lexical practice and reading comprehension.</title>
<date>2006</date>
<booktitle>Proceedings of the Ninth International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="1119" citStr="Heilman, et al. 2006" startWordPosition="150" endWordPosition="153">l features. The combined approach is compared to individual grammar- and language modeling-based approaches. While the vocabulary-based language modeling approach outperformed the grammar-based approach, grammar-based predictions can be combined using confidence scores with the vocabulary-based predictions to produce more accurate predictions of reading difficulty for both first and second language texts. The results also indicate that grammatical features may play a more important role in second language readability than in first language readability. 1 Introduction The REAP tutoring system (Heilman, et al. 2006), aims to provide authentic reading materials of the appropriate difficulty level, in terms of both vocabulary and grammar, for English as a Second Language students. An automatic measure of readability that incorporated both lexical and grammatical features was thus needed. For first language (L1) learners (i.e., children learning their native tongue), reading level has been predicted using a variety of techniques, based on models of a student’s lexicon, grammatical surface features such as sentence length (Flesch, 1948), or combinations of such features (Schwarm and Ostendorf, 2005). It was </context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2006</marker>
<rawString>M. Heilman, K. Collins-Thompson, J. Callan &amp; M. Eskenazi. 2006. Classroom success of an Intelligent Tutoring System for lexical practice and reading comprehension. Proceedings of the Ninth International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 15 (NIPS</booktitle>
<contexts>
<context position="12090" citStr="Klein and Manning, 2002" startWordPosition="1964" endWordPosition="1968">lly learn grammatical patterns explicitly from grammar explanations in L2 textbooks, unlike their L1 counterparts who learn them implicitly through natural interactions. Grammatical features would therefore seem to be an essential component of an automatic readability measure for L2 learners, who must actively acquire both the lexicon and grammar of their target language. The grammar-based readability measure relies on being able to automatically identify grammatical constructions in text. Doing so is a multi-step process that begins by syntactically parsing the document. The Stanford Parser (Klein and Manning, 2002) was used to produce constituent structure trees. The choice of parser is not essential to the approach, although the accuracy of parsing does play a role in successful identification of certain grammatical patterns. PCFG scores from the parser were also used to filter out some of the illformed text present in the test corpora. The default training set of Penn Treebank (Marcus et al. 1993) was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might b</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. Fast Exact Inference with a Factored Model for Natural Language Parsing. Advances in Neural Information Processing Systems 15 (NIPS 2002), December 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
</authors>
<title>Viewing morphology as an inference process.</title>
<date>1993</date>
<booktitle>SIGIR-93,</booktitle>
<pages>191--202</pages>
<contexts>
<context position="26950" citStr="Krovetz, 1993" startWordPosition="4342" endWordPosition="4343">disproportionately more because that method relies on accurate parsing of relationships between words. Additionally, English is a morphologically impoverished language compared to most languages. Text classification, information retrieval, and many other human language technology tasks can be accomplished for English without accounting for grammatical features such as morphological inflections. For example, an information retrieval system can perform reasonably well in English without performing stemming, which does not greatly increase performance except when queries and documents are short (Krovetz, 1993). However, most languages have a rich morphology by which a single root form may have thousands or perhaps millions of inflected or derived forms. Language technologies must account for morphological features in such languages or the vocabulary grows so large that it becomes unmanageable. Lee (2004), for example, showed that morphological analysis can improve the quality of statistical machine translation for Arabic. Thus it seems that grammatical features could contribute even more to measures of readability for texts in other languages. That said, the use of grammatical features appears to p</context>
</contexts>
<marker>Krovetz, 1993</marker>
<rawString>R. Krovetz. 1993. Viewing morphology as an inference process. SIGIR-93, 191–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lee</author>
</authors>
<title>Morphological Analysis for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>Proceedings of the HLT/NAACL Annual Conference.</booktitle>
<contexts>
<context position="27250" citStr="Lee (2004)" startWordPosition="4391" endWordPosition="4392">ed for English without accounting for grammatical features such as morphological inflections. For example, an information retrieval system can perform reasonably well in English without performing stemming, which does not greatly increase performance except when queries and documents are short (Krovetz, 1993). However, most languages have a rich morphology by which a single root form may have thousands or perhaps millions of inflected or derived forms. Language technologies must account for morphological features in such languages or the vocabulary grows so large that it becomes unmanageable. Lee (2004), for example, showed that morphological analysis can improve the quality of statistical machine translation for Arabic. Thus it seems that grammatical features could contribute even more to measures of readability for texts in other languages. That said, the use of grammatical features appears to play a more important role in readability measures for L2 than for L1. When interpolated with grammar-based scores, the reduction of mean squared error over the language modeling approach for L1 was only 7%, while for L2 the reduction or squared error was 22%. An evaluation on corpora with less noise</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Y. Lee. 2004. Morphological Analysis for Statistical Machine Translation. Proceedings of the HLT/NAACL Annual Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12482" citStr="Marcus et al. 1993" startWordPosition="2033" endWordPosition="2036">ability measure relies on being able to automatically identify grammatical constructions in text. Doing so is a multi-step process that begins by syntactically parsing the document. The Stanford Parser (Klein and Manning, 2002) was used to produce constituent structure trees. The choice of parser is not essential to the approach, although the accuracy of parsing does play a role in successful identification of certain grammatical patterns. PCFG scores from the parser were also used to filter out some of the illformed text present in the test corpora. The default training set of Penn Treebank (Marcus et al. 1993) was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might be used. Once a document is parsed, the predictor uses Tgrep2 (Rohde, 2005), a tree structure searching tool, to identify instances of the target patterns. A Tgrep2 pattern defines dominance, sisterhood, precedence, and other relationships between nodes in the parse tree for a sentence. A pattern can also place constraints on the terminal symbols (e.g., words and punctuation), such that a p</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. &amp;quot;Building a large annotated corpus of English: the Penn Treebank.&amp;quot; Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning. The McGrawHill Companies, Inc.</booktitle>
<pages>231--236</pages>
<contexts>
<context position="16334" citStr="Mitchell (1997)" startWordPosition="2665" endWordPosition="2666">tures, as well as to test the extent to which the output of syntactic parsing might improve prediction accuracy. 3.2 Algorithm for Grammatical Featurebased Classification A k-Nearest Neighbor (kNN) algorithm is used for classification based on the grammatical features described above. The kNN algorithm is an instance-based learning technique originally developed by Cover and Hart (1967) by which a test instance is classified according to the classifications of a given number (k) of training instances closest to it. Distance is defined in this work as the Euclidean distance of feature vectors. Mitchell (1997) provides more details on the kNN algorithm. This algorithm was chosen because it has been shown to be effective in text classification tasks when compared to other popular methods (Yang 1999). A k value of 12 was chosen because it provided the best performance on held-out data. Additionally, it is straightforward to calculate a confidence measure with which kNN predictions can be combined with predictions from other classifiers—in this case with predictions from the unigram language modeling-based approach described above. A confidence measure was important in this task because it provided a </context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>T. Mitchell. 1997. Machine Learning. The McGrawHill Companies, Inc. pp. 231-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rohde</author>
</authors>
<date>2005</date>
<note>Tgrep2 User Manual. http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf.</note>
<contexts>
<context position="12764" citStr="Rohde, 2005" startWordPosition="2085" endWordPosition="2086">f parser is not essential to the approach, although the accuracy of parsing does play a role in successful identification of certain grammatical patterns. PCFG scores from the parser were also used to filter out some of the illformed text present in the test corpora. The default training set of Penn Treebank (Marcus et al. 1993) was used for the parser because the domain and style of those texts actually matches fairly well with the domain and style of the texts on which a reading level predictor for second language learners might be used. Once a document is parsed, the predictor uses Tgrep2 (Rohde, 2005), a tree structure searching tool, to identify instances of the target patterns. A Tgrep2 pattern defines dominance, sisterhood, precedence, and other relationships between nodes in the parse tree for a sentence. A pattern can also place constraints on the terminal symbols (e.g., words and punctuation), such that a pattern might require a form of the copula “be” to exist in a certain position in the construction. An example of a TGrep2 search pattern for the progressive verb tense is the following: “VP &lt; /^VB/ &lt; (VP &lt; VBG)” Searching for this pattern returns sentences in which a verb phrase (V</context>
</contexts>
<marker>Rohde, 2005</marker>
<rawString>D. Rohde. 2005. Tgrep2 User Manual. http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Reading Level Assessment Using Support Vector Machines and Statistical Language Models.</title>
<date>2005</date>
<booktitle>Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1710" citStr="Schwarm and Ostendorf, 2005" startWordPosition="240" endWordPosition="243">utoring system (Heilman, et al. 2006), aims to provide authentic reading materials of the appropriate difficulty level, in terms of both vocabulary and grammar, for English as a Second Language students. An automatic measure of readability that incorporated both lexical and grammatical features was thus needed. For first language (L1) learners (i.e., children learning their native tongue), reading level has been predicted using a variety of techniques, based on models of a student’s lexicon, grammatical surface features such as sentence length (Flesch, 1948), or combinations of such features (Schwarm and Ostendorf, 2005). It was shown by CollinsThompson and Callan (2004) that a vocabularybased language modeling approach was effective at predicting the readability of grades 1 to 12 of Web documents of varying length, even with high levels of noise. Prior work on first language readability by Schwarm and Ostendorf (2005) incorporated grammatical surface features such as parse tree depth and average number of verb phrases. This work combining grammatical and lexical features was promising, but it was not clear to what extent the grammatical features improved predictions. Also, discussions with L2 instructors sug</context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>S. Schwarm, and M. Ostendorf. 2005. Reading Level Assessment Using Support Vector Machines and Statistical Language Models. Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Silberstein</author>
<author>B K Dobson</author>
<author>M A Clarke</author>
</authors>
<title>Reader&apos;s Choice, 4th edition.</title>
<date>2002</date>
<institution>University of Michigan Press/ESL.</institution>
<marker>Silberstein, Dobson, Clarke, 2002</marker>
<rawString>S. Silberstein, B. K. Dobson, and M. A. Clarke. 2002. Reader&apos;s Choice, 4th edition. University of Michigan Press/ESL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;99,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="16526" citStr="Yang 1999" startWordPosition="2697" endWordPosition="2698"> algorithm is used for classification based on the grammatical features described above. The kNN algorithm is an instance-based learning technique originally developed by Cover and Hart (1967) by which a test instance is classified according to the classifications of a given number (k) of training instances closest to it. Distance is defined in this work as the Euclidean distance of feature vectors. Mitchell (1997) provides more details on the kNN algorithm. This algorithm was chosen because it has been shown to be effective in text classification tasks when compared to other popular methods (Yang 1999). A k value of 12 was chosen because it provided the best performance on held-out data. Additionally, it is straightforward to calculate a confidence measure with which kNN predictions can be combined with predictions from other classifiers—in this case with predictions from the unigram language modeling-based approach described above. A confidence measure was important in this task because it provided a means with which to combine the grammar-based predictions with the predictions from the language modeling-based predictor while maintaining separate models for each type of feature. These sepa</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Y. Yang. 1999. A re-examination of text categorization methods. Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;99, pp 42--49).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>