<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996362">
What lies beneath: Semantic and syntactic analysis
of manually reconstructed spontaneous speech
</title>
<author confidence="0.986308">
Erin Fitzgerald
</author>
<affiliation confidence="0.966139">
Johns Hopkins University
</affiliation>
<address confidence="0.952831">
Baltimore, MD, USA
</address>
<email confidence="0.999557">
erinf@jhu.edu
</email>
<author confidence="0.988089">
Frederick Jelinek
</author>
<affiliation confidence="0.966727">
Johns Hopkins University
</affiliation>
<address confidence="0.953098">
Baltimore, MD, USA
</address>
<email confidence="0.999478">
jelinek@jhu.edu
</email>
<author confidence="0.988361">
Robert Frank
</author>
<affiliation confidence="0.985955">
Yale University
</affiliation>
<address confidence="0.888392">
New Haven, CT, USA
</address>
<email confidence="0.999481">
bob.frank@yale.edu
</email>
<sectionHeader confidence="0.993913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847857142857">
Spontaneously produced speech text often
includes disfluencies which make it diffi-
cult to analyze underlying structure. Suc-
cessful reconstruction of this text would
transform these errorful utterances into
fluent strings and offer an alternate mech-
anism for analysis.
Our investigation of naturally-occurring
spontaneous speaker errors aligned to
corrected text with manual semantico-
syntactic analysis yields new insight into
the syntactic and structural semantic
differences between spoken and recon-
structed language.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999796764705882">
In recent years, natural language processing tasks
such as machine translation, information extrac-
tion, and question answering have been steadily
improving, but relatively little of these systems
besides transcription have been applied to the
most natural form of language input: spontaneous
speech. Moreover, there has historically been lit-
tle consideration of how to analyze the underlying
semantico-syntactic structure of speech.
A system would accomplish reconstruction of
its spontaneous speech input if its output were
to represent, in flawless, fluent, and content-
preserved English, the message that the speaker
intended to convey (Fitzgerald and Jelinek, 2008;
Fitzgerald et al., 2009). Examples of such recon-
structions are seen in the following sentence-like
units (SUs).
</bodyText>
<footnote confidence="0.58963825">
EX1: that’s uh that’s a relief
becomes
that’s a relief
EX2: how can you do that without + it’s a catch-22
</footnote>
<page confidence="0.4693">
becomes
</page>
<bodyText confidence="0.871219416666667">
how can you do that without &lt;ARG&gt;
it’s a catch-22
EX3: they like video games some kids do
becomes
some kids like video games
In EX1, reconstruction requires only the dele-
tion of a simple filled pause and speaker repetition
(or reparandum (Shriberg, 1994)). The second ex-
ample shows a restart fragment, where an utter-
ance is aborted by the speaker and then restarted
with a new train of thought. Reconstruction here
requires
</bodyText>
<listItem confidence="0.8654839">
1. Detection of an interruption point (denoted
+ in the example) between the abandoned
thought and its replacement,
2. Determination that the abandoned portion
contains unique and preservable content and
should be made a new sentence rather than be
deleted (which would alter meaning)
3. Analysis showing that a required argument
must be inserted in order to complete the sen-
tence.
</listItem>
<bodyText confidence="0.885543333333333">
Finally, in the third example EX3, in order to pro-
duce one of the reconstructions given, a system
must
</bodyText>
<listItem confidence="0.9461214">
1. Detect the anaphoric relationship between
“they” and “some kids”
2. Detect the referral of “do” to “like video games”
3. Make the necessary word reorderings and
deletion of the less informative lexemes.
</listItem>
<bodyText confidence="0.99960325">
These examples show varying degrees of diffi-
culty for the task of automatic reconstruction. In
each case, we also see that semantic analysis of the
reconstruction is more straightforward than of the
</bodyText>
<page confidence="0.973438">
746
</page>
<note confidence="0.9996325">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 746–754,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.9820005">
 |{z }
some kids
ARG0 predicate
video games
 |{z }
ARG1
|{z}
like
</figure>
<bodyText confidence="0.991044916666667">
original string directly. Such analysis not only in-
forms us of what the speaker intended to commu-
nicate, but also reveals insights into the types of er-
rors speakers make when speaking spontaneously
and where these errors occur. The semantic la-
beling of reconstructed sentences, when combined
with the reconstruction alignments, may yield new
quantifiable insights into the structure of disfluent
natural speech text.
In this paper, we will investigate this relation-
ship further. Generally, we seek to answer two
questions:
</bodyText>
<listItem confidence="0.8377395">
• What generalizations about the underlying
structure of errorful and reconstructed speech
utterances are possible?
• Are these generalizations sufficiently robust
as to be incorporated into statistical models
in automatic systems?
</listItem>
<bodyText confidence="0.999895857142857">
We begin by reviewing previous work in the au-
tomatic labeling of structural semantics and moti-
vating the analysis not only in terms of discovery
but also regarding its potential application to auto-
matic speech reconstruction research. In Section 2
we describe the Spontaneous Speech Reconstruc-
tion (SSR) corpus and the manual semantic role
labeling it includes. Section 3 analyzes structural
differences between verbatim and reconstructed
text in the SSR as evaluated by a combination of
manual and automatically generated phrasal con-
stituent parses, while Section 4 combines syntactic
structure and semantic label annotations to deter-
mine the consistency of patterns and their compar-
ison to similar patterns in the Wall Street Journal
(WSJ)-based Proposition Bank (PropBank) corpus
(Palmer et al., 2005). We conclude by offering a
high level analysis of discoveries made and sug-
gesting areas for continued analysis in the future.
Expanded analysis of these results is described in
(Fitzgerald, 2009).
</bodyText>
<subsectionHeader confidence="0.999459">
1.1 Semantic role labeling
</subsectionHeader>
<bodyText confidence="0.9903189">
Every verb can be associated with a set of core
and optional argument roles, sometimes called a
roleset. For example, the verb “say” must have a
sayer and an utterance which is said, along with
an optionally defined hearer and any number of
locative, temporal, manner, etc. adjunctival argu-
ments.
The task of predicate-argument labeling (some-
times called semantic role labeling or SRL) as-
signs a simple who did what to whom when, where,
</bodyText>
<figureCaption confidence="0.742424">
Figure 1: Semantic role labeling for the sentence
</figureCaption>
<bodyText confidence="0.994196533333333">
“some kids like video games”. According to Prop-
Bank specifications, core arguments for each pred-
icate are assigned a corresponding label ARG0-
ARG5 (where ARG0 is a proto-agent, ARG1 is a
proto-patient, etc. (Palmer et al., 2005)).
why, how, etc. structure to sentences (see Figure
1), often for downstream processes such as infor-
mation extraction and question answering. Reli-
ably identifying and assigning these roles to gram-
matical text is an active area of research (Gildea
and Jurafsky, 2002; Pradhan et al., 2004; Prad-
han et al., 2008), using training resources like the
Linguistic Data Consortium’s Proposition Bank
(PropBank) (Palmer et al., 2005), a 300k-word
corpus with semantic role relations labeled for
verbs in the WSJsection of the Penn Treebank.
A common approach for automatic semantic
role labeling is to separate the process into two
steps: argument identification and argument label-
ing. For each task, standard cue features in au-
tomatic systems include verb identification, anal-
ysis of the syntactic path between that verb and
the prospective argument, and the direction (to the
left or to the right) in which the candidate argu-
ment falls in respect to its predicate. In (Gildea
and Palmer, 2002), the effect of parser accuracy
on semantic role labeling is quantified, and con-
sistent quality parses were found to be essential
when automatically identifying semantic roles on
WSJ text.
</bodyText>
<subsectionHeader confidence="0.994964">
1.2 Potential benefit of semantic analysis to
speech reconstruction
</subsectionHeader>
<bodyText confidence="0.999902">
With an adequate amount of appropriately anno-
tated conversational text, methods such as those
referred to in Section 1.1 may be adapted for
transcriptions of spontaneous speech in future re-
search. Furthermore, given a set of semantic
role labels on an ungrammatical string, and armed
with the knowledge of a set of core semantico-
syntactic principles which constrain the set of pos-
sible grammatical sentences, we hope to discover
and take advantage of new cues for construction
errors in the field of automatic spontaneous speech
reconstruction.
</bodyText>
<page confidence="0.998214">
747
</page>
<sectionHeader confidence="0.991742" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999397166666667">
We conducted our experiments on the Spon-
taneous Speech Reconstruction (SSR) corpus
(Fitzgerald and Jelinek, 2008), a 6,000 SU set of
reconstruction annotations atop a subset of Fisher
conversational telephone speech data (Cieri et al.,
2004), including
</bodyText>
<listItem confidence="0.993581142857143">
• manual word alignments between corre-
sponding original and cleaned sentence-like
units (SUs) which are labeled with transfor-
mation types (Section 2.1), and
• annotated semantic role labels on predicates
and their arguments for all grammatical re-
constructions (Section 2.2).
</listItem>
<bodyText confidence="0.999955714285714">
The fully reconstructed portion of the SSR cor-
pus consists of 6,116 SUs and 82,000 words to-
tal. While far smaller than the 300,000-word Prop-
Bank corpus, we believe that this data will be ad-
equate for an initial investigation to characterize
semantic structure of verbatim and reconstructed
speech.
</bodyText>
<subsectionHeader confidence="0.994021">
2.1 Alignments and alteration labels
</subsectionHeader>
<bodyText confidence="0.987277111111111">
In the SSR corpus, words in each reconstructed
utterance were deleted, inserted, substituted, or
moved as required to make the SU as grammatical
as possible without altering the original meaning
and without the benefit of extrasentential context.
Alignments between the original words and their
reconstructed “source” words (i.e. in the noisy
channel paradigm) are explicitly defined, and for
each alteration a corresponding alteration label
has been chosen from the following.
- DELETE words: fillers, repetitions/revisions,
false starts, co-reference, leading conjuga-
tion, and extraneous phrases
- INSERT neutral elements, such as function
words like “the”, auxiliary verbs like “is”, or
undefined argument placeholders, as in “he
wants &lt;ARG&gt;”
- SUBSTITUTE words to change tense or num-
ber, correct transcriber errors, and replace
colloquial phrases (such as: “he was like...” �
“he said...”)
- REORDER words (within sentence bound-
aries) and label as adjuncts, arguments, or
other structural reorderings
Unchanged original words are aligned to the cor-
responding word in the reconstruction with an arc
marked BASIC.
</bodyText>
<subsectionHeader confidence="0.996636">
2.2 Semantic role labeling in the SSR corpus
</subsectionHeader>
<bodyText confidence="0.99999075">
One goal of speech reconstruction is to develop
machinery to automatically reduce an utterance to
its underlying meaning and then generate clean
text. To do this, we would like to understand
how semantic structure in spontaneous speech text
varies from that of written text. Here, we can take
advantage of the semantic role labeling included
in the SSR annotation effort.
Rather than attempt to label incomplete ut-
terances or errorful phrases, SSR annotators as-
signed semantic annotation only to those utter-
ances which were well-formed and grammatical
post-reconstruction. Therefore, only these utter-
ances (about 72% of the annotated SSR data) can
be given a semantic analysis in the following sec-
tions. For each well-formed and grammatical sen-
tence, all (non-auxiliary and non-modal) verbs
were identified by annotators and the correspond-
ing predicate-argument structure was labeled ac-
cording to the role-sets defined in the PropBank
annotation effort1.
We believe the transitive bridge between the
aligned original and reconstructed sentences and
the predicate-argument labels for those recon-
structions (described further in Section 4) may
yield insight into the structure of speech errors and
how to extract these verb-argument relationships
in verbatim and errorful speech text.
</bodyText>
<sectionHeader confidence="0.644441" genericHeader="method">
3 Syntactic variation between original
and reconstructed strings
</sectionHeader>
<bodyText confidence="0.996105375">
As we begin our analysis, we first aim to under-
stand the types of syntactic changes which occur
during the course of spontaneous speech recon-
struction. These observations are made empiri-
cally given automatic analysis of the SSR corpus
annotations. Syntactic evaluation of speech and
reconstructed structure is based on the following
resources:
</bodyText>
<listItem confidence="0.99919875">
1. the manual parse P,,. for each verbatim orig-
inal SU (from SSR)
2. the automatic parse P„a of each verbatim
original SU
</listItem>
<footnote confidence="0.999622">
1PropBank roleset definitions for given verbs can be re-
viewed at http://www.cs.rochester.edu/∼gildea/Verbs/.
</footnote>
<page confidence="0.992657">
748
</page>
<bodyText confidence="0.993444333333333">
3. the automatic parse Pra of each reconstructed
SU
We note that automatic parses (using the state
of the art (Charniak, 1999) parser) of verbatim,
unreconstructed strings are likely to contain many
errors due to the inconsistent structure of ver-
batim spontaneous speech (Harper et al., 2005).
While this limits the reliability of syntactic obser-
vations, it represents the current state of the art for
syntactic analysis of unreconstructed spontaneous
speech text.
On the other hand, automatically obtained
parses for cleaned reconstructed text are more
likely to be accurate given the simplified and more
predictable structure of these SUs. This obser-
vation is unfortunately not evaluable without first
manually parsing all reconstructions in the SSR
corpus, but is assumed in the course of the follow-
ing syntax-dependent analysis.
In reconstructing from errorful and disfluent
text to clean text, a system makes not only surface
changes but also changes in underlying constituent
dependencies and parser interpretation. We can
quantify these changes in part by comparing the
internal context-free structure between the two
sets of parses.
We compare the internal syntactic structure be-
tween sets Pva and Pra of the SSR check set.
Statistics are compiled in Table 1 and analyzed be-
low.
</bodyText>
<listItem confidence="0.999297117647059">
• 64.2% of expansion rules in parses Pva
also occur in reconstruction parses Pra, and
92.4% (86.8%) of reconstruction parse Pra
expansions come directly from the verbatim
parses Pva (from columns one and two of Ta-
ble 1).
• Column three of Table 1 shows the rule types
most often dropped from the verbatim string
parses Pva in the transformation to recon-
struction. The Pva parses select full clause
non-terminals (NTs) for the verbatim parses
which are not in turn selected for automatic
parses of the reconstruction (e.g. [SBAR —*
S] or [S —* VP]). This suggests that these
rules may be used to handle errorful struc-
tures not seen by the trained grammar.
• Rule types in column four of Table 1 are the
</listItem>
<bodyText confidence="0.925651148148149">
most often “generated” in Pra (as they are
unseen in the automatic parse Pva). Since
rules like [S —* NP VP], [PP —* IN NP],
and [SBAR —* IN S] appear in a recon-
struction parse but not corresponding verba-
tim parse at similar frequencies regardless of
whether Pvm or Pva are being compared, we
are more confident that these patterns are ef-
fects of the verbatim-reconstruction compar-
ison and not the specific parser used in anal-
ysis. The fact that these patterns occur in-
dicates that it is these common rules which
are most often confounded by spontaneous
speaker errors.
• Given a Levenshtein alignment between al-
tered rules, the most common changes within
a given NT phrase are detailed in column five
of Table 1. We see that the most com-
mon aligned rule changes capture the most
basic of errors: a leading coordinator (#1
and 2) and rules proceeded by unnecessary
filler words (#3 and 5). Complementary rules
#7 and 9 (e.g. VP —* [rule]/[rule SBAR] and
VP —* [rule SBAR]/[rule]) show that comple-
menting clauses are both added and removed,
possibly in the same SU (i.e. a phrase shift),
during reconstruction.
</bodyText>
<sectionHeader confidence="0.517239" genericHeader="method">
4 Analysis of semantics for speech
</sectionHeader>
<figureCaption confidence="0.933911">
Figure 2: Manual semantic role labeling for the
</figureCaption>
<bodyText confidence="0.920246785714286">
sentence “some kids like video games” and SRL
mapped onto its verbatim source string “they like
video games and stuff some kids do”
To analyze the semantic and syntactic patterns
found in speech data and its corresponding recon-
structions, we project semantic role labels from
strings into automatic parses, and moreover from
their post-reconstruction source to the verbatim
original speech strings via the SSR manual word
alignments, as shown in Figures 2.
The automatic SRL mapping procedure from
the reconstructed string Wr to related parses Pra
and Pva and the verbatim original string Wv is as
follows.
</bodyText>
<page confidence="0.986837">
749
</page>
<note confidence="0.426916">
P„a rules P,a rules P„a rules most P,a rules most Levenshtein-aligned expansion
</note>
<listItem confidence="0.907480269230769">
in P,a in P„a frequently dropped frequently added changes (P„a/P,a)
1. NP → PRP 1. S → NP VP 1. S → [ CC rule] / [rule]
2. ROOT → S 2. PP → IN NP 2. S → [ CC NP VP] / [ NP VP]
3. S → NP VP 3. ROOT → S 3. S → [ INTJ rule] / [rule]
4. INTJ → UH 4. ADVP → RB 4. S → [ NP rule] / [rule]
64.2% 92.4% 5. PP → IN NP 5. S → NP ADVP VP 5. S → [ INTJ NP VP] / [ NP VP]
6. ADVP → RB 6. SBAR → IN S 6. S → [ NP NP VP] / [ NP VP]
7. SBAR → S 7. SBAR → S 7. VP → [rule] / [rule SBAR]
8. NP → DT NN 8. S → ADVP NP VP 8. S → [ RB rule] / [rule]
9. S → VP 9. S → VP 9. VP → [rule SBAR] / [rule]
10. PRN → S 10. NP → NP SBAR 10. S → [rule] / [ ADVP rule]
Table 1: Internal syntactic structure removed and gained during reconstruction. This table compares
the rule expansions for each verbatim string automatically parsed P„a and the automatic parse of the
corresponding reconstruction in the SSR corpus (P,a).
1. Tag each reconstruction word w, E string
W, with the annotated SRL tag t,,,r.
(a) Tag each verbatim word w„ E string W„
aligned to w, via a BASIC, REORDER,
or SUBSTITUTE alteration label with the
SRL tag t,,,r as well.
(b) Tag each verbatim word w„ aligned
to w, via a DELETE REPETITION
or DELETE CO-REFERENCE alignment
with a shadow of that SRL tag t,,,r (see
the lower tags in Figure 2 for an exam-
ple)
</listItem>
<bodyText confidence="0.9926982">
Any verbatim original word w„ with any
other alignment label is ignored in this se-
mantic analysis as SRL labels for the aligned
reconstruction word w, do not directly trans-
late to them.
</bodyText>
<listItem confidence="0.7946965">
2. Overlay tagged words of string W„ and W,
with the automatic (or manual) parse of the
same string.
3. Propagate labels. For each constituent in
</listItem>
<bodyText confidence="0.8974426">
the parse, if all children within a syntactic
constituent expansion (or all but EDITED or
INTJ) has a given SRL tag for a given pred-
icate, we instead tag that NT (and not chil-
dren) with the semantic label information.
</bodyText>
<subsectionHeader confidence="0.999195">
4.1 Labeled verbs and their arguments
</subsectionHeader>
<bodyText confidence="0.99982065625">
In the 3,626 well-formed and grammatical SUs la-
beled with semantic roles in the SSR, 895 distinct
verb types were labeled with core and adjunct ar-
guments as defined in Section 1.1. The most fre-
quent of these verbs was the orthographic form “’s”
which was labeled 623 times, or in roughly 5%
of analyzed sentences. Other forms of the verb
“to be”, including “is”, “was”, “be”, “are”, “re”, “’m”,
and “being”, were labeled over 1,500 times, or at
a rate of nearly one in half of all well-formed re-
constructed sentences. The verb type frequencies
roughly follow a Zipfian distribution (Zipf, 1949),
where most verb words appear only once (49.9%)
or twice (16.0%).
On average, 1.86 core arguments (ARG[0-4])
are labeled per verb, but the specific argument
types and typical argument numbers per predicate
are verb-specific. For example, the ditransitive
verb “give” has an average of 2.61 core arguments
for its 18 occurrences, while the verb “divorced”
(whose core arguments “initiator of end of mar-
riage” and “ex-spouse” are often combined, as in
“we divorced two years ago”) was labeled 11 times
with an average of 1.00 core arguments per occur-
rence.
In the larger PropBank corpus, annotated atop
WSJ news text, the most frequently reported verb
root is “say”, with over ten thousand labeled ap-
pearances in various tenses (this is primarily ex-
plained by the genre difference between WSJ and
telephone speech)2; again, most verbs occur two
or fewer times.
</bodyText>
<subsectionHeader confidence="0.674774">
4.2 Structural semantic statistics in cleaned
speech
</subsectionHeader>
<bodyText confidence="0.997306">
A reconstruction of a verbatim spoken utterance
can be considered an underlying form, analogous
</bodyText>
<footnote confidence="0.996115">
2The reported PropBank analysis ignores past and present
participle (passive) usage; we do not do this in our analysis.
</footnote>
<page confidence="0.997319">
750
</page>
<bodyText confidence="0.994635714285714">
to that of Chomskian theory or Harris’s concep-
tion of transformation (Harris, 1957). In this view,
the original verbatim string is the surface form of
the sentence, and as in linguistic theory should be
constrained in some manner similar to constraints
between Logical Form (LF) and Surface Structure
(SS).
</bodyText>
<table confidence="0.9997028">
Data SRL Total Most common syntactic
categories, with rel. frequency
Pva 10110 NP (50%) PP (6%)
Pra ARG1 8341 NP (58%) SBAR (9%)
PB05 Obj-NP (52%) S (22%)
Pva 4319 NP (90%) WHNP (3%)
Pra ARG0 4518 NP (93%) WHNP (3%)
PB05 Subj-NP (97%) NP (2%)
Pva 3836 NP (28%) PP (13%)
Pra ARG2 3179 NP (29%) PP (18%)
PB05 NP (36%) Obj-NP (29%)
Pva 931 ADVP (25%) NP (20%)
Pra TMP 872 ADVP (27%) PP (18%)
PB05 ADVP (26%) PP-in (16%)
Pva 562 MD (58%) TO (18%)
Pra MOD 642 MD (57%) TO (19%)
PB05 MD (99%) ADVP (1%)
Pva 505 PP (47%) ADVP (16%)
Pra LOC 489 PP (54%) ADVP (17%)
PB05 PP-in (59%) PP-on (10.0%)
</table>
<tableCaption confidence="0.68970725">
Table 2: Most frequent phrasal categories for com-
mon arguments in the SSR (mapping SRLs onto
P„a parses). PB05 refers to the PropBank data de-
scribed in (Palmer et al., 2005).
</tableCaption>
<table confidence="0.999891434782609">
Data NT Total Most common argument
labels, with rel. frequency
Pva 10541 ARG1 (48%) ARG0 (37%)
Pra NP 10218 ARG1 (47%) ARG0 (41%)
PB05 ARG2 (34%) ARG1 (24%)
PB05 Subj-NP ARG0 (79%) ARG1 (17%)
PB05 Obj-NP ARG1 (84%) ARG2 (10%)
Pva PP 1714 ARG1 (34%) ARG2 (30%)
Pra 1777 ARG1 (31%) ARG2 (30%)
PB05 PP-in LOC (48%) TMP (35%)
PB05 PP-at EXT (36%) LOC (27%)
Pva 1519 ARG2 (21%) ARG1 (19%)
Pra ADVP 1444 ARG2 (22%) ADV (20%)
PB05 TMP (30%) MNR (22%)
Pva 930 ARG1 (61%) ARG2 (14%)
Pra SBAR 1241 ARG1 (62%) ARG2 (12%)
PB05 ADV (36%) TMP (30%)
Pva 523 ARG1 (70%) ARG2 (16%)
Pra S 526 ARG1 (72%) ARG2 (17%)
PB05 ARG1 (76%) ADV (9%)
Pva 449 MOD (73%) ARG1 (18%)
Pra MD 427 MOD (86%) ARG1 (11%)
PB05 MOD (97%)Adjuncts (3%)
</table>
<tableCaption confidence="0.922086333333333">
Table 3: Most frequent argument categories for
common syntactic phrases in the SSR (mapping
SRLs onto P„a parses).
</tableCaption>
<bodyText confidence="0.955231">
In this section, we identify additional trends
which may help us to better understand these con-
straints, such as the most common phrasal cate-
gory for common arguments in common contexts
– listed in Table 2 – and the most frequent seman-
tic argument type for NTs in the SSR – listed in
Table 3.
</bodyText>
<subsectionHeader confidence="0.97736">
4.3 Structural semantic differences between
</subsectionHeader>
<bodyText confidence="0.936612375">
verbatim speech and reconstructed
speech
We now compare the placement of semantic role
labels with reconstruction-type labels assigned in
the SSR annotations.
These analyses were conducted on Pra parses of
reconstructed strings, the strings upon which se-
mantic labels were directly assigned.
</bodyText>
<subsectionHeader confidence="0.775946">
Reconstructive deletions
</subsectionHeader>
<bodyText confidence="0.999977419354839">
Q: Is there a relationship between speaker er-
ror types requiring deletions and the argument
shadows contained within? Only two deletion
types – repetitions/revisions and co-references –
have direct alignments between deleted text and
preserved text and thus can have argument shad-
ows from the reconstruction marked onto the ver-
batim text.
Of 9,082 propagated deleted repetition/ revision
phrase nodes from P„a, we found that 31.0% of ar-
guments within were ARG1, 22.7% of arguments
were ARG0, 8.6% of nodes were predicates la-
beled with semantic roles of their own, and 8.4%
of argument nodes were ARG2. Just 8.4% of
“delete repetition/revision” nodes were modifier
(vs. core) arguments, with TMP and CAU labels
being the most common.
Far fewer (353) nodes from P„a represented
deleted co-reference words. Of these, 57.2% of ar-
gument nodes were ARG1, 26.6% were ARG0 and
13.9% were ARG2. 7.6% of “argument” nodes
here were SRL-labeled predicates, and 10.2%
were in modifier rather than core arguments, the
most prevalent were TMP and LOC.
These observations indicate to us that redun-
dant co-references are far most likely to occur for
ARG1 roles (most often objects, though also sub-
jects for copular verbs (i.e. “to be”) and others) and
appear more likely than random to occur in core
argument regions of an utterance rather than in op-
tional modifying material.
</bodyText>
<subsectionHeader confidence="0.328724">
Reconstructive insertions
</subsectionHeader>
<page confidence="0.988646">
751
</page>
<bodyText confidence="0.999888606060606">
Q: When null arguments are inserted into re-
constructions of errorful speech, what seman-
tic role do they typically fill? Three types of
insertions were made by annotators during the re-
construction of the SSR corpus. Inserted function
words, the most common, were also the most var-
ied. Analyzing the automatic parses of the recon-
structions Pra, we find that the most commonly
assigned parts-of-speech (POS) for these elements
was fittingly IN (21.5%, preposition), DT (16.7%,
determiner) and CC (14.3%, conjunction). Inter-
estingly, we found that the next most common
POS assignments were noun labels, which may in-
dicate errors in SSR labeling.
Other inserted word types were auxiliary or oth-
erwise neutral verbs, and, as expected, most POS
labels assigned by the parses were verb types,
mostly VBP (non-third person present singular).
About half of these were labeled as predicates with
corresponding semantic roles; the rest were unla-
beled which makes sense as true auxiliary verbs
were not labeled in the process.
Finally, around 147 insertion types made were
neutral arguments (given the orthographic form
&lt;ARG&gt;). 32.7% were common nouns and 18.4%
of these were labeled personal pronouns PRP. An-
other 11.6% were adjectives JJ. We found that 22
(40.7%) of 54 neutral argument nodes directly as-
signed as semantic roles were ARG1, and another
33.3% were ARG0. Nearly a quarter of inserted
arguments became part of a larger phrase serv-
ing as a modifier argument, the most common of
which were CAU and LOC.
</bodyText>
<subsectionHeader confidence="0.916856">
Reconstructive substitutions
</subsectionHeader>
<bodyText confidence="0.999760785714286">
Q: How often do substitutions occur in the an-
alyzed data, and is there any semantic con-
sistency in the types of words changed? 230
phrase tense substitutions occurred in the SSR cor-
pus. Only 13 of these were directly labeled as
predicate arguments (as opposed to being part of
a larger argument), 8 of which were ARG1. Mor-
phology changes generally affect verb tense rather
than subject number, and with no real impact on
semantic structure.
Colloquial substitutions of verbs, such as “he
was like...” “he said...”, yield more unusual seman-
tic analysis on the unreconstructed side as non-
verbs were analyzed as verbs.
</bodyText>
<subsectionHeader confidence="0.603319">
Reconstructive word re-orderings
</subsectionHeader>
<bodyText confidence="0.999901488372093">
Q: How is the predicate-argument labeling af-
fected? If reorderings occur as a phrase, what
type of phrase? Word reorderings labeled as
argument movements occurred 136 times in the
3,626 semantics-annotated SUs in the SSR corpus.
Of these, 81% were directly labeled as arguments
to some sentence-internal predicate. 52% of those
arguments were ARG1, 17% were ARG0, and 13%
were predicates. 11% were labeled as modifying
arguments rather than core arguments, which may
indicate confusion on the part of the annotators
and possibly necessary cleanup.
More commonly labeled than argument move-
ment was adjunct movement, assigned to 206
phrases. 54% of these reordered adjuncts were not
directly labeled as predicate arguments but were
within other labeled arguments. The most com-
monly labeled adjunct types were TMP (19% of all
arguments), ADV (13%), and LOC (11%).
Syntactically, 25% of reordered adjuncts were
assigned ADVP by the automatic parser, 19% were
assigned NP, 18% were labeled PP, and remaining
common NT assignments included IN, RB, and
SBAR.
Finally, 239 phrases were labeled as being re-
ordered for the general reason of fixing the gram-
mar, the default change assignment given by the
annotation tool when a word was moved. This
category was meant to encompass all movements
not included in the previous two categories (argu-
ments and adjuncts), including moving “I guess”
from the middle or end of a sentence to the be-
ginning, determiner movement, etc. Semantically,
63% of nodes were directly labeled as predicates
or predicate arguments. 34% of these were PRED,
28% were ARG1, 27% were ARG0, 8% were
ARG2, and 8% were roughly evenly distributed
across the adjunct argument types.
Syntactically, 31% of these changes were NPs,
16% were ADVPs, and 14% were VBPs (24% were
verbs in general). The remaining 30% of changes
were divided amongst 19 syntactic categories from
CC to DT to PP.
</bodyText>
<subsectionHeader confidence="0.832306">
4.4 Testing the generalizations required for
automatic SRL for speech
</subsectionHeader>
<bodyText confidence="0.9999362">
The results described in (Gildea and Palmer, 2002)
show that parsing dramatically helps during the
course of automatic SRL. We hypothesize that
the current state-of-art for parsing speech is ade-
quate to generally identify semantic roles in spon-
</bodyText>
<page confidence="0.991801">
752
</page>
<bodyText confidence="0.999980578947369">
taneously produced speech text. For this to be true,
features for which SRL is currently dependent on
such as consistent predicate-to-parse paths within
automatic constituent parses must be found to ex-
ist in data such as the SSR corpus.
The predicate-argument path is defined as the
number of steps up and down a parse tree (and
through which NTs) which are taken to traverse
the tree from the predicate (verb) to its argument.
For example, the path from predicate VBP —* “like”
to the argument ARG0 (NP —* “some kids”) might
be [VBP T VP T S 1 NP]. As trees grow more
complex, as well as more errorful (as expected
for the automatic parses of verbatim speech text),
the paths seen are more sparsely observed (i.e. the
probability density is less concentrated at the most
common paths than similar paths seen in the Prop-
Bank annotations). We thus consider two path
simplifications as well:
</bodyText>
<listItem confidence="0.976179">
• compressed: only the source, target, and root
nodes are preserved in the path (so the path
above becomes [VBP T S 1 NP])
• POS class clusters: rather than distinguish,
</listItem>
<bodyText confidence="0.997721230769231">
for example, between different tenses of
verbs in a path, we consider only the first let-
ter of each NT. Thus, clustering compressed
output, the new path from predicate to ARG0
becomes [V T S 1 N].
The top paths were similarly consistent regardless
of whether paths are extracted from P,.a, P,,., or
P„a (P„a results shown in Table 4), but we see that
the distributions of paths are much flatter (i.e. a
greater number and total relative frequency of path
types) going from manual to automatic parses and
from parses of verbatim to parses of reconstructed
strings.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999481181818182">
In this work, we sought to find generalizations
about the underlying structure of errorful and re-
constructed speech utterances, in the hopes of de-
termining semantic-based features which can be
incorporated into automatic systems identifying
semantic roles in speech text as well as statisti-
cal models for reconstruction itself. We analyzed
syntactic and semantic variation between original
and reconstructed utterances according to manu-
ally and automatically generated parses and man-
ually labeled semantic roles.
</bodyText>
<table confidence="0.9997693125">
Argument Path from Predicate Freq
VBP ↑ VP ↑ S ↓ NP 4.9%
Predicate- VB ↑ VP ↑ VP ↑ S ↓ NP 3.9%
Argument VB ↑ VP ↓ NP 3.8%
Paths VBD ↑ VP ↑ S ↓ NP 2.8%
944 more path types 84.7%
VB ↑ S ↓ NP 7.3%
VB ↑ VP ↓ NP 5.8%
Compressed VBP ↑ S ↓ NP 5.3%
VBD ↑ S ↓ NP 3.5%
333 more path types 77.1%
V ↑ S ↓ N 25.8%
V ↑ V ↓ N 17.5%
POS class+ V ↑ V ↓ A 8.2%
compressed V ↑ V ↓ V 7.7%
60 more path types 40.8%
</table>
<tableCaption confidence="0.99945">
Table 4: Frequent P„a predicate-argument paths
</tableCaption>
<bodyText confidence="0.999779">
Syntactic paths from predicates to arguments
were similar to those presented for WSJ data
(Palmer et al., 2005), though these patterns de-
graded when considered for automatically parsed
verbatim and errorful data. We believe that auto-
matic models may be trained, but if entirely depen-
dent on automatic parses of verbatim strings, an
SRL-labeled resource much bigger than the SSR
and perhaps even PropBank may be required.
</bodyText>
<sectionHeader confidence="0.997393" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999989529411765">
This work is an initial proof of concept that au-
tomatic semantic role labeling (SRL) of verbatim
speech text may be produced in the future. This is
supported by the similarity of common predicate-
argument paths between this data and the Prop-
Bank WSJ annotations (Palmer et al., 2005) and
the consistency of other features currently empha-
sized in automatic SRL work on clean text data.
To automatically semantically label speech tran-
scripts, however, is expected to require additional
annotated data beyond the 3k utterances annotated
for SRL included in the SSR corpus, though it may
be adequate for initial adaptation studies.
This new ground work using available corpora
to model speaker errors may lead to new intelli-
gent feature design for automatic systems for shal-
low semantic labeling and speech reconstruction.
</bodyText>
<sectionHeader confidence="0.998845" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999207833333334">
Support for this work was provided by NSF PIRE
Grant No. OISE-0530118. Any opinions, find-
ings, conclusions, or recommendations expressed
in this material are those of the authors and do
not necessarily reflect the views of the supporting
agency.
</bodyText>
<page confidence="0.979362">
753
</page>
<bodyText confidence="0.4220015">
George K. Zipf. 1949. Human Behavior and the Prin-
ciple of Least-Effort. Addison-Wesley.
</bodyText>
<sectionHeader confidence="0.561463" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.9336125">
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. In Proceedings of the Annual Meet-
ing of the North American Association for Compu-
tational Linguistics.
</bodyText>
<reference confidence="0.999758145833333">
Christopher Cieri, Stephanie Strassel, Mohamed
Maamouri, Shudong Huang, James Fiumara, David
Graff, Kevin Walker, and Mark Liberman. 2004.
Linguistic resource creation and distribution for
EARS. In Rich Transcription Fall Workshop.
Erin Fitzgerald and Frederick Jelinek. 2008. Linguis-
tic resources for reconstructing spontaneous speech
text. In Proceedings of the Language Resources and
Evaluation Conference.
Erin Fitzgerald, Keith Hall, and Frederick Jelinek.
2009. Reconstructing false start errors in sponta-
neous speech text. In Proceedings of the Annual
Meeting of the European Association for Computa-
tional Linguistics.
Erin Fitzgerald. 2009. Reconstructing Spontaneous
Speech. Ph.D. thesis, The Johns Hopkins University.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Mary Harper, Bonnie Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. Structural metadata and parsing
speech. Technical report, JHU Language Engineer-
ing Workshop.
Zellig S. Harris. 1957. Co-occurrence and transforma-
tion in linguistic structure. Language, 33:283–340.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106, March.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of the Human Language Technology Confer-
ence/North American chapter of the Association of
Computational Linguistics (HLT/NAACL), Boston,
MA.
Sameer Pradhan, James Martin, and Wayne Ward.
2008. Towards robust semantic role labeling. Com-
putational Linguistics, 34(2):289–310.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
</reference>
<page confidence="0.998883">
754
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900237">
<title confidence="0.9959995">What lies beneath: Semantic and syntactic analysis of manually reconstructed spontaneous speech</title>
<author confidence="0.999874">Erin Fitzgerald</author>
<affiliation confidence="0.999773">Johns Hopkins University</affiliation>
<address confidence="0.999747">Baltimore, MD, USA</address>
<email confidence="0.999876">erinf@jhu.edu</email>
<author confidence="0.999912">Frederick Jelinek</author>
<affiliation confidence="0.999866">Johns Hopkins University</affiliation>
<address confidence="0.9998">Baltimore, MD, USA</address>
<email confidence="0.999851">jelinek@jhu.edu</email>
<author confidence="0.999953">Robert Frank</author>
<affiliation confidence="0.990329">Yale University</affiliation>
<address confidence="0.999054">New Haven, CT, USA</address>
<email confidence="0.999827">bob.frank@yale.edu</email>
<abstract confidence="0.994474">Spontaneously produced speech text often includes disfluencies which make it difficult to analyze underlying structure. Successful reconstruction of this text would transform these errorful utterances into fluent strings and offer an alternate mechanism for analysis. Our investigation of naturally-occurring spontaneous speaker errors aligned to corrected text with manual semanticosyntactic analysis yields new insight into the syntactic and structural semantic differences between spoken and reconstructed language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>Stephanie Strassel</author>
<author>Mohamed Maamouri</author>
<author>Shudong Huang</author>
<author>James Fiumara</author>
<author>David Graff</author>
<author>Kevin Walker</author>
<author>Mark Liberman</author>
</authors>
<title>Linguistic resource creation and distribution for EARS. In Rich Transcription Fall Workshop.</title>
<date>2004</date>
<contexts>
<context position="7831" citStr="Cieri et al., 2004" startWordPosition="1210" endWordPosition="1213">n future research. Furthermore, given a set of semantic role labels on an ungrammatical string, and armed with the knowledge of a set of core semanticosyntactic principles which constrain the set of possible grammatical sentences, we hope to discover and take advantage of new cues for construction errors in the field of automatic spontaneous speech reconstruction. 747 2 Data We conducted our experiments on the Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a 6,000 SU set of reconstruction annotations atop a subset of Fisher conversational telephone speech data (Cieri et al., 2004), including • manual word alignments between corresponding original and cleaned sentence-like units (SUs) which are labeled with transformation types (Section 2.1), and • annotated semantic role labels on predicates and their arguments for all grammatical reconstructions (Section 2.2). The fully reconstructed portion of the SSR corpus consists of 6,116 SUs and 82,000 words total. While far smaller than the 300,000-word PropBank corpus, we believe that this data will be adequate for an initial investigation to characterize semantic structure of verbatim and reconstructed speech. 2.1 Alignments </context>
</contexts>
<marker>Cieri, Strassel, Maamouri, Huang, Fiumara, Graff, Walker, Liberman, 2004</marker>
<rawString>Christopher Cieri, Stephanie Strassel, Mohamed Maamouri, Shudong Huang, James Fiumara, David Graff, Kevin Walker, and Mark Liberman. 2004. Linguistic resource creation and distribution for EARS. In Rich Transcription Fall Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erin Fitzgerald</author>
<author>Frederick Jelinek</author>
</authors>
<title>Linguistic resources for reconstructing spontaneous speech text.</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="1525" citStr="Fitzgerald and Jelinek, 2008" startWordPosition="203" endWordPosition="206">ssing tasks such as machine translation, information extraction, and question answering have been steadily improving, but relatively little of these systems besides transcription have been applied to the most natural form of language input: spontaneous speech. Moreover, there has historically been little consideration of how to analyze the underlying semantico-syntactic structure of speech. A system would accomplish reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and contentpreserved English, the message that the speaker intended to convey (Fitzgerald and Jelinek, 2008; Fitzgerald et al., 2009). Examples of such reconstructions are seen in the following sentence-like units (SUs). EX1: that’s uh that’s a relief becomes that’s a relief EX2: how can you do that without + it’s a catch-22 becomes how can you do that without &lt;ARG&gt; it’s a catch-22 EX3: they like video games some kids do becomes some kids like video games In EX1, reconstruction requires only the deletion of a simple filled pause and speaker repetition (or reparandum (Shriberg, 1994)). The second example shows a restart fragment, where an utterance is aborted by the speaker and then restarted with a</context>
<context position="7703" citStr="Fitzgerald and Jelinek, 2008" startWordPosition="1190" endWordPosition="1193"> annotated conversational text, methods such as those referred to in Section 1.1 may be adapted for transcriptions of spontaneous speech in future research. Furthermore, given a set of semantic role labels on an ungrammatical string, and armed with the knowledge of a set of core semanticosyntactic principles which constrain the set of possible grammatical sentences, we hope to discover and take advantage of new cues for construction errors in the field of automatic spontaneous speech reconstruction. 747 2 Data We conducted our experiments on the Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a 6,000 SU set of reconstruction annotations atop a subset of Fisher conversational telephone speech data (Cieri et al., 2004), including • manual word alignments between corresponding original and cleaned sentence-like units (SUs) which are labeled with transformation types (Section 2.1), and • annotated semantic role labels on predicates and their arguments for all grammatical reconstructions (Section 2.2). The fully reconstructed portion of the SSR corpus consists of 6,116 SUs and 82,000 words total. While far smaller than the 300,000-word PropBank corpus, we believe that this data will b</context>
</contexts>
<marker>Fitzgerald, Jelinek, 2008</marker>
<rawString>Erin Fitzgerald and Frederick Jelinek. 2008. Linguistic resources for reconstructing spontaneous speech text. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erin Fitzgerald</author>
<author>Keith Hall</author>
<author>Frederick Jelinek</author>
</authors>
<title>Reconstructing false start errors in spontaneous speech text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the European Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1551" citStr="Fitzgerald et al., 2009" startWordPosition="207" endWordPosition="210">anslation, information extraction, and question answering have been steadily improving, but relatively little of these systems besides transcription have been applied to the most natural form of language input: spontaneous speech. Moreover, there has historically been little consideration of how to analyze the underlying semantico-syntactic structure of speech. A system would accomplish reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and contentpreserved English, the message that the speaker intended to convey (Fitzgerald and Jelinek, 2008; Fitzgerald et al., 2009). Examples of such reconstructions are seen in the following sentence-like units (SUs). EX1: that’s uh that’s a relief becomes that’s a relief EX2: how can you do that without + it’s a catch-22 becomes how can you do that without &lt;ARG&gt; it’s a catch-22 EX3: they like video games some kids do becomes some kids like video games In EX1, reconstruction requires only the deletion of a simple filled pause and speaker repetition (or reparandum (Shriberg, 1994)). The second example shows a restart fragment, where an utterance is aborted by the speaker and then restarted with a new train of thought. Rec</context>
</contexts>
<marker>Fitzgerald, Hall, Jelinek, 2009</marker>
<rawString>Erin Fitzgerald, Keith Hall, and Frederick Jelinek. 2009. Reconstructing false start errors in spontaneous speech text. In Proceedings of the Annual Meeting of the European Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erin Fitzgerald</author>
</authors>
<title>Reconstructing Spontaneous Speech.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>The Johns Hopkins University.</institution>
<contexts>
<context position="5045" citStr="Fitzgerald, 2009" startWordPosition="768" endWordPosition="769">ructural differences between verbatim and reconstructed text in the SSR as evaluated by a combination of manual and automatically generated phrasal constituent parses, while Section 4 combines syntactic structure and semantic label annotations to determine the consistency of patterns and their comparison to similar patterns in the Wall Street Journal (WSJ)-based Proposition Bank (PropBank) corpus (Palmer et al., 2005). We conclude by offering a high level analysis of discoveries made and suggesting areas for continued analysis in the future. Expanded analysis of these results is described in (Fitzgerald, 2009). 1.1 Semantic role labeling Every verb can be associated with a set of core and optional argument roles, sometimes called a roleset. For example, the verb “say” must have a sayer and an utterance which is said, along with an optionally defined hearer and any number of locative, temporal, manner, etc. adjunctival arguments. The task of predicate-argument labeling (sometimes called semantic role labeling or SRL) assigns a simple who did what to whom when, where, Figure 1: Semantic role labeling for the sentence “some kids like video games”. According to PropBank specifications, core arguments f</context>
</contexts>
<marker>Fitzgerald, 2009</marker>
<rawString>Erin Fitzgerald. 2009. Reconstructing Spontaneous Speech. Ph.D. thesis, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="6056" citStr="Gildea and Jurafsky, 2002" startWordPosition="931" endWordPosition="934">mantic role labeling or SRL) assigns a simple who did what to whom when, where, Figure 1: Semantic role labeling for the sentence “some kids like video games”. According to PropBank specifications, core arguments for each predicate are assigned a corresponding label ARG0- ARG5 (where ARG0 is a proto-agent, ARG1 is a proto-patient, etc. (Palmer et al., 2005)). why, how, etc. structure to sentences (see Figure 1), often for downstream processes such as information extraction and question answering. Reliably identifying and assigning these roles to grammatical text is an active area of research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Pradhan et al., 2008), using training resources like the Linguistic Data Consortium’s Proposition Bank (PropBank) (Palmer et al., 2005), a 300k-word corpus with semantic role relations labeled for verbs in the WSJsection of the Penn Treebank. A common approach for automatic semantic role labeling is to separate the process into two steps: argument identification and argument labeling. For each task, standard cue features in automatic systems include verb identification, analysis of the syntactic path between that verb and the prospective argument, and the direction (to </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6777" citStr="Gildea and Palmer, 2002" startWordPosition="1047" endWordPosition="1050">Consortium’s Proposition Bank (PropBank) (Palmer et al., 2005), a 300k-word corpus with semantic role relations labeled for verbs in the WSJsection of the Penn Treebank. A common approach for automatic semantic role labeling is to separate the process into two steps: argument identification and argument labeling. For each task, standard cue features in automatic systems include verb identification, analysis of the syntactic path between that verb and the prospective argument, and the direction (to the left or to the right) in which the candidate argument falls in respect to its predicate. In (Gildea and Palmer, 2002), the effect of parser accuracy on semantic role labeling is quantified, and consistent quality parses were found to be essential when automatically identifying semantic roles on WSJ text. 1.2 Potential benefit of semantic analysis to speech reconstruction With an adequate amount of appropriately annotated conversational text, methods such as those referred to in Section 1.1 may be adapted for transcriptions of spontaneous speech in future research. Furthermore, given a set of semantic role labels on an ungrammatical string, and armed with the knowledge of a set of core semanticosyntactic prin</context>
<context position="27174" citStr="Gildea and Palmer, 2002" startWordPosition="4469" endWordPosition="4472">iddle or end of a sentence to the beginning, determiner movement, etc. Semantically, 63% of nodes were directly labeled as predicates or predicate arguments. 34% of these were PRED, 28% were ARG1, 27% were ARG0, 8% were ARG2, and 8% were roughly evenly distributed across the adjunct argument types. Syntactically, 31% of these changes were NPs, 16% were ADVPs, and 14% were VBPs (24% were verbs in general). The remaining 30% of changes were divided amongst 19 syntactic categories from CC to DT to PP. 4.4 Testing the generalizations required for automatic SRL for speech The results described in (Gildea and Palmer, 2002) show that parsing dramatically helps during the course of automatic SRL. We hypothesize that the current state-of-art for parsing speech is adequate to generally identify semantic roles in spon752 taneously produced speech text. For this to be true, features for which SRL is currently dependent on such as consistent predicate-to-parse paths within automatic constituent parses must be found to exist in data such as the SSR corpus. The predicate-argument path is defined as the number of steps up and down a parse tree (and through which NTs) which are taken to traverse the tree from the predicat</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Harper</author>
<author>Bonnie Dorr</author>
<author>John Hale</author>
<author>Brian Roark</author>
<author>Izhak Shafran</author>
<author>Matthew Lease</author>
</authors>
<title>Structural metadata and parsing speech.</title>
<date>2005</date>
<tech>Technical report, JHU Language Engineering Workshop.</tech>
<location>Yang Liu, Matthew Snover, Lisa Yung, Anna Krasnyanskaya, and</location>
<contexts>
<context position="11833" citStr="Harper et al., 2005" startWordPosition="1815" endWordPosition="1818">tactic evaluation of speech and reconstructed structure is based on the following resources: 1. the manual parse P,,. for each verbatim original SU (from SSR) 2. the automatic parse P„a of each verbatim original SU 1PropBank roleset definitions for given verbs can be reviewed at http://www.cs.rochester.edu/∼gildea/Verbs/. 748 3. the automatic parse Pra of each reconstructed SU We note that automatic parses (using the state of the art (Charniak, 1999) parser) of verbatim, unreconstructed strings are likely to contain many errors due to the inconsistent structure of verbatim spontaneous speech (Harper et al., 2005). While this limits the reliability of syntactic observations, it represents the current state of the art for syntactic analysis of unreconstructed spontaneous speech text. On the other hand, automatically obtained parses for cleaned reconstructed text are more likely to be accurate given the simplified and more predictable structure of these SUs. This observation is unfortunately not evaluable without first manually parsing all reconstructions in the SSR corpus, but is assumed in the course of the following syntax-dependent analysis. In reconstructing from errorful and disfluent text to clean</context>
</contexts>
<marker>Harper, Dorr, Hale, Roark, Shafran, Lease, 2005</marker>
<rawString>Mary Harper, Bonnie Dorr, John Hale, Brian Roark, Izhak Shafran, Matthew Lease, Yang Liu, Matthew Snover, Lisa Yung, Anna Krasnyanskaya, and Robin Stewart. 2005. Structural metadata and parsing speech. Technical report, JHU Language Engineering Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Co-occurrence and transformation in linguistic structure.</title>
<date>1957</date>
<journal>Language,</journal>
<pages>33--283</pages>
<contexts>
<context position="19121" citStr="Harris, 1957" startWordPosition="3121" endWordPosition="3122">SJ news text, the most frequently reported verb root is “say”, with over ten thousand labeled appearances in various tenses (this is primarily explained by the genre difference between WSJ and telephone speech)2; again, most verbs occur two or fewer times. 4.2 Structural semantic statistics in cleaned speech A reconstruction of a verbatim spoken utterance can be considered an underlying form, analogous 2The reported PropBank analysis ignores past and present participle (passive) usage; we do not do this in our analysis. 750 to that of Chomskian theory or Harris’s conception of transformation (Harris, 1957). In this view, the original verbatim string is the surface form of the sentence, and as in linguistic theory should be constrained in some manner similar to constraints between Logical Form (LF) and Surface Structure (SS). Data SRL Total Most common syntactic categories, with rel. frequency Pva 10110 NP (50%) PP (6%) Pra ARG1 8341 NP (58%) SBAR (9%) PB05 Obj-NP (52%) S (22%) Pva 4319 NP (90%) WHNP (3%) Pra ARG0 4518 NP (93%) WHNP (3%) PB05 Subj-NP (97%) NP (2%) Pva 3836 NP (28%) PP (13%) Pra ARG2 3179 NP (29%) PP (18%) PB05 NP (36%) Obj-NP (29%) Pva 931 ADVP (25%) NP (20%) Pra TMP 872 ADVP (2</context>
</contexts>
<marker>Harris, 1957</marker>
<rawString>Zellig S. Harris. 1957. Co-occurrence and transformation in linguistic structure. Language, 33:283–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Paul Kingsbury</author>
<author>Daniel Gildea</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="4849" citStr="Palmer et al., 2005" startWordPosition="735" endWordPosition="738">ation to automatic speech reconstruction research. In Section 2 we describe the Spontaneous Speech Reconstruction (SSR) corpus and the manual semantic role labeling it includes. Section 3 analyzes structural differences between verbatim and reconstructed text in the SSR as evaluated by a combination of manual and automatically generated phrasal constituent parses, while Section 4 combines syntactic structure and semantic label annotations to determine the consistency of patterns and their comparison to similar patterns in the Wall Street Journal (WSJ)-based Proposition Bank (PropBank) corpus (Palmer et al., 2005). We conclude by offering a high level analysis of discoveries made and suggesting areas for continued analysis in the future. Expanded analysis of these results is described in (Fitzgerald, 2009). 1.1 Semantic role labeling Every verb can be associated with a set of core and optional argument roles, sometimes called a roleset. For example, the verb “say” must have a sayer and an utterance which is said, along with an optionally defined hearer and any number of locative, temporal, manner, etc. adjunctival arguments. The task of predicate-argument labeling (sometimes called semantic role labeli</context>
<context position="6215" citStr="Palmer et al., 2005" startWordPosition="955" endWordPosition="958">rding to PropBank specifications, core arguments for each predicate are assigned a corresponding label ARG0- ARG5 (where ARG0 is a proto-agent, ARG1 is a proto-patient, etc. (Palmer et al., 2005)). why, how, etc. structure to sentences (see Figure 1), often for downstream processes such as information extraction and question answering. Reliably identifying and assigning these roles to grammatical text is an active area of research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Pradhan et al., 2008), using training resources like the Linguistic Data Consortium’s Proposition Bank (PropBank) (Palmer et al., 2005), a 300k-word corpus with semantic role relations labeled for verbs in the WSJsection of the Penn Treebank. A common approach for automatic semantic role labeling is to separate the process into two steps: argument identification and argument labeling. For each task, standard cue features in automatic systems include verb identification, analysis of the syntactic path between that verb and the prospective argument, and the direction (to the left or to the right) in which the candidate argument falls in respect to its predicate. In (Gildea and Palmer, 2002), the effect of parser accuracy on sem</context>
<context position="20106" citStr="Palmer et al., 2005" startWordPosition="3304" endWordPosition="3307">(52%) S (22%) Pva 4319 NP (90%) WHNP (3%) Pra ARG0 4518 NP (93%) WHNP (3%) PB05 Subj-NP (97%) NP (2%) Pva 3836 NP (28%) PP (13%) Pra ARG2 3179 NP (29%) PP (18%) PB05 NP (36%) Obj-NP (29%) Pva 931 ADVP (25%) NP (20%) Pra TMP 872 ADVP (27%) PP (18%) PB05 ADVP (26%) PP-in (16%) Pva 562 MD (58%) TO (18%) Pra MOD 642 MD (57%) TO (19%) PB05 MD (99%) ADVP (1%) Pva 505 PP (47%) ADVP (16%) Pra LOC 489 PP (54%) ADVP (17%) PB05 PP-in (59%) PP-on (10.0%) Table 2: Most frequent phrasal categories for common arguments in the SSR (mapping SRLs onto P„a parses). PB05 refers to the PropBank data described in (Palmer et al., 2005). Data NT Total Most common argument labels, with rel. frequency Pva 10541 ARG1 (48%) ARG0 (37%) Pra NP 10218 ARG1 (47%) ARG0 (41%) PB05 ARG2 (34%) ARG1 (24%) PB05 Subj-NP ARG0 (79%) ARG1 (17%) PB05 Obj-NP ARG1 (84%) ARG2 (10%) Pva PP 1714 ARG1 (34%) ARG2 (30%) Pra 1777 ARG1 (31%) ARG2 (30%) PB05 PP-in LOC (48%) TMP (35%) PB05 PP-at EXT (36%) LOC (27%) Pva 1519 ARG2 (21%) ARG1 (19%) Pra ADVP 1444 ARG2 (22%) ADV (20%) PB05 TMP (30%) MNR (22%) Pva 930 ARG1 (61%) ARG2 (14%) Pra SBAR 1241 ARG1 (62%) ARG2 (12%) PB05 ADV (36%) TMP (30%) Pva 523 ARG1 (70%) ARG2 (16%) Pra S 526 ARG1 (72%) ARG2 (17%) P</context>
<context position="30074" citStr="Palmer et al., 2005" startWordPosition="4991" endWordPosition="4994">automatically generated parses and manually labeled semantic roles. Argument Path from Predicate Freq VBP ↑ VP ↑ S ↓ NP 4.9% Predicate- VB ↑ VP ↑ VP ↑ S ↓ NP 3.9% Argument VB ↑ VP ↓ NP 3.8% Paths VBD ↑ VP ↑ S ↓ NP 2.8% 944 more path types 84.7% VB ↑ S ↓ NP 7.3% VB ↑ VP ↓ NP 5.8% Compressed VBP ↑ S ↓ NP 5.3% VBD ↑ S ↓ NP 3.5% 333 more path types 77.1% V ↑ S ↓ N 25.8% V ↑ V ↓ N 17.5% POS class+ V ↑ V ↓ A 8.2% compressed V ↑ V ↓ V 7.7% 60 more path types 40.8% Table 4: Frequent P„a predicate-argument paths Syntactic paths from predicates to arguments were similar to those presented for WSJ data (Palmer et al., 2005), though these patterns degraded when considered for automatically parsed verbatim and errorful data. We believe that automatic models may be trained, but if entirely dependent on automatic parses of verbatim strings, an SRL-labeled resource much bigger than the SSR and perhaps even PropBank may be required. 6 Conclusions and future work This work is an initial proof of concept that automatic semantic role labeling (SRL) of verbatim speech text may be produced in the future. This is supported by the similarity of common predicateargument paths between this data and the PropBank WSJ annotations</context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American chapter of the Association of Computational Linguistics (HLT/NAACL),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="6078" citStr="Pradhan et al., 2004" startWordPosition="935" endWordPosition="938">) assigns a simple who did what to whom when, where, Figure 1: Semantic role labeling for the sentence “some kids like video games”. According to PropBank specifications, core arguments for each predicate are assigned a corresponding label ARG0- ARG5 (where ARG0 is a proto-agent, ARG1 is a proto-patient, etc. (Palmer et al., 2005)). why, how, etc. structure to sentences (see Figure 1), often for downstream processes such as information extraction and question answering. Reliably identifying and assigning these roles to grammatical text is an active area of research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Pradhan et al., 2008), using training resources like the Linguistic Data Consortium’s Proposition Bank (PropBank) (Palmer et al., 2005), a 300k-word corpus with semantic role relations labeled for verbs in the WSJsection of the Penn Treebank. A common approach for automatic semantic role labeling is to separate the process into two steps: argument identification and argument labeling. For each task, standard cue features in automatic systems include verb identification, analysis of the syntactic path between that verb and the prospective argument, and the direction (to the left or to the rig</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of the Human Language Technology Conference/North American chapter of the Association of Computational Linguistics (HLT/NAACL), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>James Martin</author>
<author>Wayne Ward</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="6101" citStr="Pradhan et al., 2008" startWordPosition="939" endWordPosition="943"> did what to whom when, where, Figure 1: Semantic role labeling for the sentence “some kids like video games”. According to PropBank specifications, core arguments for each predicate are assigned a corresponding label ARG0- ARG5 (where ARG0 is a proto-agent, ARG1 is a proto-patient, etc. (Palmer et al., 2005)). why, how, etc. structure to sentences (see Figure 1), often for downstream processes such as information extraction and question answering. Reliably identifying and assigning these roles to grammatical text is an active area of research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Pradhan et al., 2008), using training resources like the Linguistic Data Consortium’s Proposition Bank (PropBank) (Palmer et al., 2005), a 300k-word corpus with semantic role relations labeled for verbs in the WSJsection of the Penn Treebank. A common approach for automatic semantic role labeling is to separate the process into two steps: argument identification and argument labeling. For each task, standard cue features in automatic systems include verb identification, analysis of the syntactic path between that verb and the prospective argument, and the direction (to the left or to the right) in which the candid</context>
</contexts>
<marker>Pradhan, Martin, Ward, 2008</marker>
<rawString>Sameer Pradhan, James Martin, and Wayne Ward. 2008. Towards robust semantic role labeling. Computational Linguistics, 34(2):289–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="2007" citStr="Shriberg, 1994" startWordPosition="288" endWordPosition="289">ent, in flawless, fluent, and contentpreserved English, the message that the speaker intended to convey (Fitzgerald and Jelinek, 2008; Fitzgerald et al., 2009). Examples of such reconstructions are seen in the following sentence-like units (SUs). EX1: that’s uh that’s a relief becomes that’s a relief EX2: how can you do that without + it’s a catch-22 becomes how can you do that without &lt;ARG&gt; it’s a catch-22 EX3: they like video games some kids do becomes some kids like video games In EX1, reconstruction requires only the deletion of a simple filled pause and speaker repetition (or reparandum (Shriberg, 1994)). The second example shows a restart fragment, where an utterance is aborted by the speaker and then restarted with a new train of thought. Reconstruction here requires 1. Detection of an interruption point (denoted + in the example) between the abandoned thought and its replacement, 2. Determination that the abandoned portion contains unique and preservable content and should be made a new sentence rather than be deleted (which would alter meaning) 3. Analysis showing that a required argument must be inserted in order to complete the sentence. Finally, in the third example EX3, in order to p</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disfluencies. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>