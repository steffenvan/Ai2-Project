<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.982996">
Labeled LDA: A supervised topic model for credit attribution in
multi-labeled corpora
</title>
<author confidence="0.999243">
Daniel Ramage, David Hall, Ramesh Nallapati and Christopher D. Manning
</author>
<affiliation confidence="0.987772">
Computer Science Department
Stanford University
</affiliation>
<email confidence="0.998502">
{dramage,dlwh,nmramesh,manning}@cs.stanford.edu
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996784">
A significant portion of the world’s text
is tagged by readers on social bookmark-
ing websites. Credit attribution is an in-
herent problem in these corpora because
most pages have multiple tags, but the tags
do not always apply with equal specificity
across the whole document. Solving the
credit attribution problem requires associ-
ating each word in a document with the
most appropriate tags and vice versa. This
paper introduces Labeled LDA, a topic
model that constrains Latent Dirichlet Al-
location by defining a one-to-one corre-
spondence between LDA’s latent topics
and user tags. This allows Labeled LDA to
directly learn word-tag correspondences.
We demonstrate Labeled LDA’s improved
expressiveness over traditional LDA with
visualizations of a corpus of tagged web
pages from del.icio.us. Labeled LDA out-
performs SVMs by more than 3 to 1 when
extracting tag-specific document snippets.
As a multi-label text classifier, our model
is competitive with a discriminative base-
line on a variety of datasets.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99970941509434">
From news sources such as Reuters to modern
community web portals like del.icio.us, a signif-
icant proportion of the world’s textual data is la-
beled with multiple human-provided tags. These
collections reflect the fact that documents are often
about more than one thing—for example, a news
story about a highway transportation bill might
naturally be filed under both transportation and
politics, with neither category acting as a clear
subset of the other. Similarly, a single web page
in del.icio.us might well be annotated with tags as
diverse as arts, physics, alaska, and beauty.
However, not all tags apply with equal speci-
ficity across the whole document, opening up new
opportunities for information retrieval and cor-
pus analysis on tagged corpora. For instance,
users who browse for documents with a particu-
lar tag might prefer to see summaries that focus
on the portion of the document most relevant to
the tag, a task we call tag-specific snippet extrac-
tion. And when a user browses to a particular
document, a tag-augmented user interface might
provide overview visualization cues highlighting
which portions of the document are more or less
relevant to the tag, helping the user quickly access
the information they seek.
One simple approach to these challenges can
be found in models that explicitly address the
credit attribution problem by associating individ-
ual words in a document with their most appropri-
ate labels. For instance, in our news story about
the transportation bill, if the model knew that the
word “highway” went with transportation and that
the word “politicians” went with politics, more
relevant passages could be extracted for either la-
bel. We seek an approach that can automatically
learn the posterior distribution of each word in a
document conditioned on the document’s label set.
One promising approach to the credit attribution
problem lies in the machinery of Latent Dirich-
let Allocation (LDA) (Blei et al., 2003), a recent
model that has gained popularity among theoreti-
cians and practitioners alike as a tool for automatic
corpus summarization and visualization. LDA is
a completely unsupervised algorithm that models
each document as a mixture of topics. The model
generates automatic summaries of topics in terms
of a discrete probability distribution over words
for each topic, and further infers per-document
discrete distributions over topics. Most impor-
tantly, LDA makes the explicit assumption that
each word is generated from one underlying topic.
Although LDA is expressive enough to model
</bodyText>
<page confidence="0.961495">
248
</page>
<note confidence="0.9966045">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 248–256,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99819665">
multiple topics per document, it is not appropriate
for multi-labeled corpora because, as an unsuper-
vised model, it offers no obvious way of incorpo-
rating a supervised label set into its learning proce-
dure. In particular, LDA often learns some topics
that are hard to interpret, and the model provides
no tools for tuning the generated topics to suit an
end-use application, even when time and resources
exist to provide some document labels.
Several modifications of LDA to incorporate
supervision have been proposed in the literature.
Two such models, Supervised LDA (Blei and
McAuliffe, 2007) and DiscLDA (Lacoste-Julien
et al., 2008) are inappropriate for multiply labeled
corpora because they limit a document to being as-
sociated with only a single label. Supervised LDA
posits that a label is generated from each docu-
ment’s empirical topic mixture distribution. Dis-
cLDA associates a single categorical label variable
with each document and associates a topic mixture
with each label. A third model, MM-LDA (Ram-
age et al., 2009), is not constrained to one label
per document because it models each document as
a bag of words with a bag of labels, with topics for
each observation drawn from a shared topic dis-
tribution. But, like the other models, MM-LDA’s
learned topics do not correspond directly with the
label set. Consequently, these models fall short as
a solution to the credit attribution problem. Be-
cause labels have meaning to the people that as-
signed them, a simple solution to the credit attri-
bution problem is to assign a document’s words to
its labels rather than to a latent and possibly less
interpretable semantic space.
This paper presents Labeled LDA (L-LDA), a
generative model for multiply labeled corpora that
marries the multi-label supervision common to
modern text datasets with the word-assignment
ambiguity resolution of the LDA family of mod-
els. In contrast to standard LDA and its existing
supervised variants, our model associates each la-
bel with one topic in direct correspondence. In the
following section, L-LDA is shown to be a natu-
ral extension of both LDA (by incorporating su-
pervision) and Multinomial Naive Bayes (by in-
corporating a mixture model). We demonstrate
that L-LDA can go a long way toward solving the
credit attribution problem in multiply labeled doc-
uments with improved interpretability over LDA
(Section 4). We show that L-LDA’s credit attribu-
tion ability enables it to greatly outperform sup-
Figure 1: Graphical model of Labeled LDA: un-
like standard LDA, both the label set Λ as well as
the topic prior α influence the topic mixture e.
port vector machines on a tag-driven snippet ex-
traction task on web pages from del.icio.us (Sec-
tion 6). And despite its generative semantics,
we show that Labeled LDA is competitive with
a strong baseline discriminative classifier on two
multi-label text classification tasks (Section 7).
</bodyText>
<sectionHeader confidence="0.987806" genericHeader="introduction">
2 Labeled LDA
</sectionHeader>
<bodyText confidence="0.99952078125">
Labeled LDA is a probabilistic graphical model
that describes a process for generating a labeled
document collection. Like Latent Dirichlet Allo-
cation, Labeled LDA models each document as a
mixture of underlying topics and generates each
word from one topic. Unlike LDA, L-LDA in-
corporates supervision by simply constraining the
topic model to use only those topics that corre-
spond to a document’s (observed) label set. The
model description that follows assumes the reader
is familiar with the basic LDA model (Blei et al.,
2003).
Let each document d be represented by a tu-
ple consisting of a list of word indices w(d) =
(w1, ... , wNd) and a list of binary topic pres-
ence/absence indicators Λ(d) = (l1,...,lK)
where each wz E {1, ... , V } and each lk E {0,1}.
Here Nd is the document length, V is the vocabu-
lary size and K the total number of unique labels
in the corpus.
We set the number of topics in Labeled LDA to
be the number of unique labels K in the corpus.
The generative process for the algorithm is found
in Table 1. Steps 1 and 2—drawing the multi-
nomial topic distributions over vocabulary )3k for
each topic k, from a Dirichlet prior 77—remain
the same as for traditional LDA (see (Blei et al.,
2003), page 4). The traditional LDA model then
draws a multinomial mixture distribution e(d) over
all K topics, for each document d, from a Dirichlet
prior α. However, we would like to restrict e(d) to
be defined only over the topics that correspond to
</bodyText>
<equation confidence="0.704211636363636">
Λ
η
D
Φ
β
α
e
zw w
N
K
249
1 For each topic k ∈ {1, ... , K}:
2 Generate βk = (βk,1,..., βk,V )T ∼ Dir(·|η)
3 For each document d:
4 For each topic k ∈ {1, ... ,K}
5 Generate A(d)
k ∈ {0, 1} ∼ Bernoulli(·|Φk)
6 Generate α(d) = L(d) × α
7 Generate θ(d) = (θl.,..., θlMd)T ∼ Dir(·|α(d))
8 For each i in {1, ... , Nd}:
9 Generate zi ∈ {λ(d)
1 , ... , λ(d)Md} ∼ Mult(·|θ(d) )
</equation>
<tableCaption confidence="0.7009675">
10 Generate wi ∈ {1, ... , V } ∼ Mult(·|βzi )
Table 1: Generative process for Labeled LDA:
</tableCaption>
<bodyText confidence="0.980842409090909">
,C3k is a vector consisting of the parameters of the
multinomial distribution corresponding to the kth
topic, α are the parameters of the Dirichlet topic
prior and 77 are the parameters of the word prior,
while Φk is the label prior for topic k. For the
meaning of the projection matrix L(d), please re-
fer to Eq 1.
its labels A(d). Since the word-topic assignments
zi (see step 9 in Table 1) are drawn from this dis-
tribution, this restriction ensures that all the topic
assignments are limited to the document’s labels.
Towards this objective, we first generate the
document’s labels A(d) using a Bernoulli coin toss
for each topic k, with a labeling prior probability
Φk, as shown in step 5. Next, we define the vector
of document’s labels to be X(d) = {k|Λ(d)
k = 1}.
This allows us to define a document-specific la-
bel projection matrix L(d) of size Md × K for
each document d, where Md = |X(d)|, as fol-
lows: For each row i ∈ {1, ... , Md} and column
j ∈ {1,...,K} :
</bodyText>
<equation confidence="0.958403666666667">
� (1)
L(d) = 1 if Ai = j
ij0 otherwise.
</equation>
<bodyText confidence="0.934156375">
In other words, the ith row of L(d) has an entry of
1 in column j if and only if the ith document label
A(d)
i is equal to the topic j, and zero otherwise.
As the name indicates, we use the L(d) matrix to
project the parameter vector of the Dirichlet topic
prior α = (α1, ... , αK)T to a lower dimensional
vector α(d) as follows:
</bodyText>
<equation confidence="0.956807">
α(d) = L(d) × α = (αλ(d)
1 , ... , αλ(d) )T (2)
Md
</equation>
<bodyText confidence="0.9999608">
Clearly, the dimensions of the projected vector
correspond to the topics represented by the labels
of the document. For example, suppose K = 4
and that a document d has labels given by A(d) =
{0,1,1, 0} which implies X(d) = {2, 3}, then L(d)
</bodyText>
<equation confidence="0.87506475">
would be:
� 0 1 0 0
.
0 0 1 0
</equation>
<bodyText confidence="0.9988100625">
Then, 0(d) is drawn from a Dirichlet distribution
with parameters α(d) = L(d) × α = (α2, α3)T
(i.e., with the Dirichlet restricted to the topics 2
and 3).
This fulfills our requirement that the docu-
ment’s topics are restricted to its own labels. The
projection step constitutes the deterministic step
6 in Table 1. The remaining part of the model
from steps 7 through 10 are the same as for reg-
ular LDA.
The dependency of 0 on both α and A is in-
dicated by directed edges from A and α to 0 in
the plate notation in Figure 1. This is the only ad-
ditional dependency we introduce in LDA’s repre-
sentation (please compare with Figure 1 in (Blei et
al., 2003)).
</bodyText>
<subsectionHeader confidence="0.996519">
2.1 Learning and inference
</subsectionHeader>
<bodyText confidence="0.997598">
In most applications discussed in this paper, we
will assume that the documents are multiply
tagged with human labels, both at learning and in-
ference time.
When the labels A(d) of the document are ob-
served, the labeling prior Φ is d-separated from
the rest of the model given A(d). Hence the model
is same as traditional LDA, except the constraint
that the topic prior α(d) is now restricted to the
set of labeled topics X(d). Therefore, we can use
collapsed Gibbs sampling (Griffiths and Steyvers,
2004) for training where the sampling probability
for a topic for position i in a document d in La-
beled LDA is given by:
</bodyText>
<equation confidence="0.9968022">
n−i,j + αj
(d)
n−i,· + αT 1
(d)
(3)
</equation>
<bodyText confidence="0.997998142857143">
where nwi
−i,j is the count of word wi in topic j, that
does not include the current assignment zi, a miss-
ing subscript or superscript (e.g. n(·)
−i,j)) indicates
a summation over that dimension, and 1 is a vector
of 1’s of appropriate dimension.
Although the equation above looks exactly the
same as that of LDA, we have an important dis-
tinction in that, the target topic j is restricted to
belong to the set of labels, i.e., j ∈ X(d).
Once the topic multinomials ,C3 are learned from
the training set, one can perform inference on any
new labeled test document using Gibbs sampling
</bodyText>
<figure confidence="0.5719198">
P(zi = j|Z−i) ∝ nwi+
−i,j ηwi
×
n−i,j + 77T1
(·)
</figure>
<page confidence="0.975608">
250
</page>
<bodyText confidence="0.999969193548387">
restricted to its tags, to determine its per-word la-
bel assignments z. In addition, one can also com-
pute its posterior distribution θ over topics by ap-
propriately normalizing the topic assignments z.
It should now be apparent to the reader how
the new model addresses some of the problems in
multi-labeled corpora that we highlighted in Sec-
tion 1. For example, since there is a one-to-one
correspondence between the labels and topics, the
model can display automatic topical summaries
for each label k in terms of the topic-specific dis-
tribution βk. Similarly, since the model assigns a
label zz to each word wz in the document d au-
tomatically, we can now extract portions of the
document relevant to each label k (it would be all
words wz E w(d) such that zz = k). In addition,
we can use the topic distribution θ(d) to rank the
user specified labels in the order of their relevance
to the document, thereby also eliminating spurious
ones if necessary.
Finally, we note that other less restrictive vari-
ants of the proposed L-LDA model are possible.
For example, one could consider a version that
allows topics that do not correspond to the label
set of a given document with a small probability,
or one that allows a common background topic in
all documents. We did implement these variants
in our preliminary experiments, but they did not
yield better performance than L-LDA in the tasks
we considered. Hence we do not report them in
this paper.
</bodyText>
<subsectionHeader confidence="0.991629">
2.2 Relationship to Naive Bayes
</subsectionHeader>
<bodyText confidence="0.999987487179487">
The derivation of the algorithm so far has fo-
cused on its relationship to LDA. However, La-
beled LDA can also be seen as an extension of
the event model of a traditional Multinomial Naive
Bayes classifier (McCallum and Nigam, 1998) by
the introduction of a mixture model. In this sec-
tion, we develop the analogy as another way to
understand L-LDA from a supervised perspective.
Consider the case where no document in the
collection is assigned two or more labels. Now
for a particular document d with label ld, Labeled
LDA draws each word’s topic variable zz from a
multinomial constrained to the document’s label
set, i.e. zz = ld for each word position i in the doc-
ument. During learning, the Gibbs sampler will
assign each zz to ld while incrementing Old(wz),
effectively counting the occurences of each word
type in documents labeled with ld. Thus in the
singly labeled document case, the probability of
each document under Labeled LDA is equal to
the probability of the document under the Multi-
nomial Naive Bayes event model trained on those
same document instances. Unlike the Multino-
mial Naive Bayes classifier, Labeled LDA does
not encode a decision boundary for unlabeled doc-
uments by comparing P(w(d)|ld) to P(w(d)|¬ld),
although we discuss using Labeled LDA for multi-
label classification in Section 7.
Labeled LDA’s similarity to Naive Bayes ends
with the introduction of a second label to any doc-
ument. In a traditional one-versus-rest Multino-
mial Naive Bayes model, a separate classifier for
each label would be trained on all documents with
that label, so each word can contribute a count
of 1 to every observed label’s word distribution.
By contrast, Labeled LDA assumes that each doc-
ument is a mixture of underlying topics, so the
count mass of single word instance must instead be
distributed over the document’s observed labels.
</bodyText>
<sectionHeader confidence="0.9971515" genericHeader="method">
3 Credit attribution within tagged
documents
</sectionHeader>
<bodyText confidence="0.999871962962963">
Social bookmarking websites contain millions of
tags describing many of the web’s most popu-
lar and useful pages. However, not all tags are
uniformly appropriate at all places within a doc-
ument. In the sections that follow, we examine
mechanisms by which Labeled LDA’s credit as-
signment mechanism can be utilized to help sup-
port browsing and summarizing tagged document
collections.
To create a consistent dataset for experimenting
with our model, we selected 20 tags of medium
to high frequency from a collection of documents
dataset crawled from del.icio.us, a popular so-
cial bookmarking website (Heymann et al., 2008).
From that larger dataset, we selected uniformly at
random four thousand documents that contained
at least one of the 20 tags, and then filtered each
document’s tag set by removing tags not present
in our tag set. After filtering, the resulting cor-
pus averaged 781 non-stop words per document,
with each document having 4 distinct tags on aver-
age. In contrast to many existing text datasets, our
tagged corpus is highly multiply labeled: almost
90% of of the documents have more than one tag.
(For comparison, less than one third of the news
documents in the popular RCV1-v2 collection of
newswire are multiply labeled). We will refer to
</bodyText>
<page confidence="0.988056">
251
</page>
<bodyText confidence="0.778782">
this collection of data as the del.icio.us tag dataset.
</bodyText>
<sectionHeader confidence="0.992278" genericHeader="method">
4 Topic Visualization
</sectionHeader>
<bodyText confidence="0.980216825">
A first question we ask of Labeled LDA is how its
topics compare with those learned by traditional
LDA on the same collection of documents. We ran
our implementations of Labeled LDA and LDA
on the del.icio.us corpus described above. Both
are based on the standard collapsed Gibbs sam-
pler, with the constraints for Labeled LDA imple-
mented as in Section 2.
Figure 2: Comparison of some of the 20 topics
learned on del.icio.us by Labeled LDA (left) and
traditional LDA (right), with representative words
for each topic shown in the boxes. Labeled LDA’s
topics are named by their associated tag. Arrows
from right-to-left show the mapping of LDA topics
to the closest Labeled LDA topic by cosine simi-
larity. Tags not shown are: design, education, en-
glish, grammar, history, internet, language, phi-
losophy, politics, programming, reference, style,
writing.
Figure 2 shows the top words associated with
20 topics learned by Labeled LDA and 20 topics
learned by unsupervised LDA on the del.icio.us
document collection. Labeled LDA’s topics are
directly named with the tag that corresponds to
each topic, an improvement over standard prac-
tice of inferring the topic name by inspection (Mei
et al., 2007). The topics learned by the unsu-
pervised variant were matched to a Labeled LDA
topic highest cosine similarity.
The topics selected are representative: com-
pared to Labeled LDA, unmodified LDA allocates
many topics for describing the largest parts of the
The Elements of Style, William Strunk, Jr.
Asserting that one must first know the rules to break them, this
classic reference book is a must-have for any student and
conscientious writer. Intended for use in which the practice of
composition is combined with the study of literature,it gives in
brief space the principal requirements of plain English style and
concentratesattention on the rules of usage and principles of
composition most commonly violated.
</bodyText>
<figureCaption confidence="0.88183">
Figure 3: Example document with important
words annotated with four of the page’s tags as
learned by Labeled LDA. Red (single underline)
is style, green (dashed underline) grammar, blue
(double underline) reference, and black (jagged
underline) education.
</figureCaption>
<bodyText confidence="0.9997645">
corpus and under-represents tags that are less un-
common: of the 20 topics learned, LDA learned
multiple topics mapping to each of five tags (web,
culture, and computer, reference, and politics, all
of which were common in the dataset) and learned
no topics that aligned with six tags (books, english,
science, history, grammar, java, and philosophy,
which were rarer).
</bodyText>
<sectionHeader confidence="0.963411" genericHeader="method">
5 Tagged document visualization
</sectionHeader>
<bodyText confidence="0.96125498">
In addition to providing automatic summaries of
the words best associated with each tag in the cor-
pus, Labeled LDA’s credit attribution mechanism
can be used to augment the view of a single doc-
ument with rich contextual information about the
document’s tags.
Figure 3 shows one web document from the col-
lection, a page describing a guide to writing En-
glish prose. The 10 most common tags for that
document are writing, reference, english, gram-
mar, style, language, books, book, strunk, and ed-
ucation, the first eight of which were included in
our set of 20 tags. In the figure, each word that has
high posterior probability from one tag has been
annotated with that tag. The red words come from
the style tag, green from the grammar tag, blue
from the reference tag, and black from the educa-
tion tag. In this case, the model does very well at
assigning individual words to the tags that, subjec-
tively, seem to strongly imply the presence of that
tag on this page. A more polished rendering could
add subtle visual cues about which parts of a page
are most appropriate for a particular set of tags.
Tag (Labeled LDA) (LDA) Topic ID
book image pdf review library
posted read copyright books title
works water map human life work
science time world years sleep
windows file version linux comp-
uterfree system software mac
comment god jesus people gospel
bible reply lord religion written
applications spring open web java
pattern eclipse development ajax
people day link posted time com-
ments back music jane permalink
comments read nice post great
april blog march june wordpress
news information service web on-
line project site free search home
web images design content java
css website articles page learning
jun quote pro views added check
anonymous card core power ghz
life written jesus words made man
called mark john person fact name
house light radio media photo-
graphy news music travel cover
game review street public art
health food city history science
</bodyText>
<figure confidence="0.958706117647059">
8
13
19
4
3
2
12
web search site blog css content
google list page posted great work
web
books
science
comp
uter
religion
java
culture
</figure>
<page confidence="0.925284">
252
</page>
<bodyText confidence="0.954857090909091">
books
L-LDA this classic reference book is a must-have for any
student and conscientious writer. Intended for
SVM the rules of usage and principles of composition
most commonly violated. Search: CONTENTS Bibli-
ographic
language
L-LDA the beginning of a sentence must refer to the gram-
matical subject 8. Divide words at
SVM combined with the study of literature, it gives in brief
space the principal requirements of
</bodyText>
<figure confidence="0.334933">
grammar
L-LDA requirements of plain English style and concen-
</figure>
<figureCaption confidence="0.85923">
trates attention on the rules of usage and principles of
SVM them, this classic reference book is a must-have for
any student and conscientious writer.
Figure 4: Representative snippets extracted by
L-LDA and tag-specific SVMs for the web page
shown in Figure 3.
</figureCaption>
<sectionHeader confidence="0.984819" genericHeader="method">
6 Snippet Extraction
</sectionHeader>
<bodyText confidence="0.999977714285714">
Another natural application of Labeled LDA’s
credit assignment mechanism is as a means of se-
lecting snippets of a document that best describe
its contents from the perspective of a particular
tag. Consider again the document in Figure 3. In-
tuitively, if this document were shown to a user
interested in the tag grammar, the most appropri-
ate snippet of words might prefer to contain the
phrase “rules of usage,” whereas a user interested
in the term style might prefer the title “Elements
of Style.”
To quantitatively evaluate Labeled LDA’s per-
formance at this task, we constructed a set of 29
recently tagged documents from del.icio.us that
were labeled with two or more tags from the 20 tag
subset, resulting in a total of 149 (document,tag)
pairs. For each pair, we extracted a 15-word win-
dow with the highest tag-specific score from the
document. Two systems were used to score each
window: Labeled LDA and a collection of one-
vs-rest SVMs trained for each tag in the system.
L-LDA scored each window as the expected prob-
ability that the tag had generated each word. For
SVMs, each window was taken as its own doc-
ument and scored using the tag-specific SVM’s
un-thresholded scoring function, taking the win-
dow with the most positive score. While a com-
plete solution to the tag-specific snippet extraction
</bodyText>
<table confidence="0.711196">
Model Best Snippet Unanimous
L-LDA 72 / 149 24 / 51
SVM 21 / 149 2 / 51
</table>
<tableCaption confidence="0.83412">
Table 2: Human judgments of tag-specific snippet
</tableCaption>
<bodyText confidence="0.999674904761905">
quality as extracted by L-LDA and SVM. The cen-
ter column is the number of document-tag pairs for
which a system’s snippet was judged superior. The
right column is the number of snippets for which
all three annotators were in complete agreement
(numerator) in the subset of document scored by
all three annotators (denominator).
problem might be more informed by better lin-
guistic features (such as phrase boundaries), this
experimental setup suffices to evaluate both kinds
of models for their ability to appropriately assign
words to underlying labels.
Figure 3 shows some example snippets output
by our system for this document. Note that while
SVMs did manage to select snippets that were
vaguely on topic, Labeled LDA’s outputs are gen-
erally of superior subjective quality. To quantify
this intuition, three human annotators rated each
pair of snippets. The outputs were randomly la-
beled as “System A” or “System B,” and the anno-
tators were asked to judge which system generated
a better tag-specific document subset. The judges
were also allowed to select neither system if there
was no clear winner. The results are summarized
in Table 2.
L-LDA was judged superior by a wide margin:
of the 149 judgments, L-LDA’s output was se-
lected as preferable in 72 cases, whereas SVM’s
was selected in only 21. The difference between
these scores was highly significant (p &lt; .001) by
the sign test. To quantify the reliability of the judg-
ments, 51 of the 149 document-tag pairs were la-
beled by all three annotators. In this group, the
judgments were in substantial agreement,1 with
Fleiss’ Kappa at .63.
Further analysis of the triply-annotated sub-
set yields further evidence of L-LDA’s advantage
over SVM’s: 33 of the 51 were tag-page pairs
where L-LDA’s output was picked by at least one
annotator as a better snippet (although L-LDA
might not have been picked by the other annota-
tors). And of those, 24 were unanimous in that
</bodyText>
<footnote confidence="0.99890025">
1Of the 15 judgments that were in contention, only two
conflicted on which system was superior (L-LDA versus
SVM); the remaining disagreements were about whether or
not one of the systems was a clear winner.
</footnote>
<page confidence="0.998298">
253
</page>
<bodyText confidence="0.9996248">
all three judges selected L-LDA’s output. By con-
trast, only 10 of the 51 were tag-page pairs where
SVMs’ output was picked by at least one anno-
tator, and of those, only 2 were selected unani-
mously.
</bodyText>
<sectionHeader confidence="0.949591" genericHeader="method">
7 Multilabeled Text Classification
</sectionHeader>
<bodyText confidence="0.999985074074074">
In the preceding section we demonstrated how La-
beled LDA’s credit attribution mechanism enabled
effective modeling within documents. In this sec-
tion, we consider whether L-LDA can be adapted
as an effective multi-label classifier for documents
as a whole. To answer that question, we applied
a modified variant of L-LDA to a multi-label doc-
ument classification problem: given a training set
consisting of documents with multiple labels, pre-
dict the set of labels appropriate for each docu-
ment in a test set.
Multi-label classification is a well researched
problem. Many modern approaches incorporate
label correlations (e.g., Kazawa et al. (2004), Ji
et al. (2008)). Others, like our algorithm are
based on mixture models (such as Ueda and Saito
(2003)). However, we are aware of no methods
that trade off label-specific word distributions with
document-specific label distributions in quite the
same way.
In Section 2, we discussed learning and infer-
ence when labels are observed. In the task of mul-
tilabel classification, labels are available at train-
ing time, so the learning part remains the same as
discussed before. However, inferring the best set
of labels for an unlabeled document at test time is
more complex: it involves assessing all label as-
signments and returning the assignment that has
the highest posterior probability. However, this
is not straight-forward, since there are 2K possi-
ble label assignments. To make matters worse, the
support of α(A(d)) is different for different label
assignments. Although we are in the process of
developing an efficient sampling algorithm for this
inference, for the purposes of this paper we make
the simplifying assumption that the model reduces
to standard LDA at inference, where the document
is free to sample from any of the K topics. This
is a reasonable assumption because allowing the
model to explore the whole topic space for each
document is similar to exploring all possible label
assignments. The document’s most likely labels
can then be inferred by suitably thresholding its
posterior probability over topics.
As a baseline, we use a set of multiple one-vs-
rest SVM classifiers which is a popular and ex-
tremely competitive baseline used by most previ-
ous papers (see (Kazawa et al., 2004; Ueda and
Saito, 2003) for instance). We scored each model
based on Micro-F1 and Macro-F1 as our evalua-
tion measures (Lewis et al., 2004). While the for-
mer allows larger classes to dominate its results,
the latter assigns an equal weight to all classes,
providing us complementary information.
</bodyText>
<subsectionHeader confidence="0.964539">
7.1 Yahoo
</subsectionHeader>
<bodyText confidence="0.999985787878788">
We ran experiments on a corpus from the Yahoo
directory, modeling our experimental conditions
on the ones described in (Ji et al., 2008).2 We
considered documents drawn from 8 top level cat-
egories in the Yahoo directory, where each doc-
ument can be placed in any number of subcate-
gories. The results were mixed, with SVMs ahead
on one measure: Labeled LDA beat SVMs on five
out of eight datasets on MacroF1, but didn’t win
on any datasets on MicroF1. Results are presented
in Table 3.
Because only a processed form of the docu-
ments was released, the Yahoo dataset does not
lend itself well to error analysis. However, only
33% of the documents in each top-level category
were applied to more than one sub-category, so the
credit assignment machinery of L-LDA was un-
used for the majority of documents. We there-
fore ran an artificial second set of experiments
considering only those documents that had been
given more than one label in the training data. On
these documents, the results were again mixed, but
Labeled LDA comes out ahead. For MacroF1,
L-LDA beat SVMs on four datasets, SVMs beat
L-LDA on one dataset, and three were a statistical
tie.3 On MicroF1, L-LDA did much better than on
the larger subset, outperforming on four datasets
with the other four a statistical tie.
It is worth noting that the Yahoo datasets are
skewed by construction to contain many docu-
ments with highly overlapping content: because
each collection is within the same super-class such
as “Arts”, “Business”, etc., each sub-categories’
</bodyText>
<footnote confidence="0.534072875">
2We did not carefully tune per-class thresholds of each of
the one vs. rest classifiers in each model, but instead tuned
only one threshold for all classifiers in each model via cross-
validation on the Arts subsets. As such, our numbers were on
an average 3-4% less than those reported in (Ji et al., 2008),
but the methods were comparably tuned.
3The difference between means of multiple runs were not
significantly different by two-tailed paired t-test.
</footnote>
<page confidence="0.991041">
254
</page>
<table confidence="0.9999302">
Dataset %MacroF1 %MicroF1
L-LDA SVM L-LDA SVM
Arts 30.70(1.62) 23.23 (0.67) 39.81(1.85) 48.42 (0.45)
Business 30.81(0.75) 22.82 (1.60) 67.00(1.29) 72.15 (0.62)
Computers 27.55(1.98) 18.29 (1.53) 48.95(0.76) 61.97 (0.54)
Education 33.78(1.70) 36.03 (1.30) 41.19(1.48) 59.45 (0.56)
Entertainment 39.42(1.38) 43.22 (0.49) 47.71(0.61) 62.89 (0.50)
Health 45.36(2.00) 47.86 (1.72) 58.13(0.43) 72.21 (0.26)
Recreation 37.63(1.00) 33.77 (1.17) 43.71(0.31) 59.15 (0.71)
Society 27.32(1.24) 23.89 (0.74) 42.98(0.28) 52.29 (0.67)
</table>
<tableCaption confidence="0.999675">
Table 3: Averaged performance across ten runs of multi-label text classification for predicting subsets
</tableCaption>
<bodyText confidence="0.901835333333333">
of the named Yahoo directory categories. Numbers in parentheses are standard deviations across runs.
L-LDA outperforms SVMs on 5 subsets with MacroF1, but on no subsets with MicroF1.
vocabularies will naturally overlap a great deal.
L-LDA’s credit attribution mechanism is most ef-
fective at partitioning semantically distinct words
into their respective label vocabularies, so we ex-
pect that Labeled-LDA’s performance as a text
classifier would improve on collections with more
semantically diverse labels.
</bodyText>
<subsectionHeader confidence="0.993058">
7.2 Tagged Web Pages
</subsectionHeader>
<bodyText confidence="0.999392666666667">
We also applied our method to text classification
on the del.icio.us dataset, where the documents are
naturally multiply labeled (more than 89%) and
where the tags are less inherently similar than in
the Yahoo subcategories. Therefore we expect La-
beled LDA to do better credit assignment on this
subset and consequently to show improved perfor-
mance as a classifier, and indeed this is the case.
We evaluated L-LDA and multiple one-vs-rest
SVMs on 4000 documents with the 20 tag sub-
set described in Section 3. L-LDA and multiple
one-vs-rest SVMs were trained on the first 80% of
documents and evaluated on the remaining 20%,
with results averaged across 10 random permuta-
tions of the dataset. The results are shown in Ta-
ble 4. We tuned the SVMs’ shared cost parameter
C(= 10.0) and selected raw term frequency over
tf-idf weighting based on 4-fold cross-validation
on 3,000 documents drawn from an independent
permutation of the data. For L-LDA, we tuned the
shared parameters of threshold and proportional-
ity constants in word and topic priors. L-LDA and
SVM have very similar performance on MacroF1,
while L-LDA substantially outperforms on Mi-
croF1. In both cases, L-LDA’s improvement is
statistically significantly by a 2-tailed paired t-test
at 95% confidence.
</bodyText>
<table confidence="0.979486333333333">
Model %MacroF1 %MicroF1
L-LDA 39.85 (.989) 52.12 (.434)
SVM 39.00 (.423) 39.33 (.574)
</table>
<tableCaption confidence="0.986491">
Table 4: Mean performance across ten runs of
</tableCaption>
<bodyText confidence="0.9890335">
multi-label text classification for predicting 20
tags on del.icio.us data. L-LDA outperforms
SVMs significantly on both metrics by a 2-tailed,
paired t-test at 95% confidence.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="method">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999797375">
One of the main advantages of L-LDA on mul-
tiply labeled documents comes from the model’s
document-specific topic mixture θ. By explicitly
modeling the importance of each label in the doc-
ument, Labeled LDA can effective perform some
contextual word sense disambiguation, which sug-
gests why L-LDA can outperform SVMs on the
del.icio.us dataset.
As a concrete example, consider the excerpt
of text from the del.icio.us dataset in Figure 5.
The document itself has several tags, including
design and programming. Initially, many of the
likelihood probabilities p(wllabel) for the (con-
tent) words in this excerpt are higher for the label
programming than design, including “content”,
“client”, “CMS” and even “designed”, while de-
sign has higher likelihoods for just “website” and
“happy”. However, after performing inference on
this document using L-LDA, the inferred docu-
ment probability for design (p(design)) is much
higher than it is for programming. In fact, the
higher probability for the tag more than makes up
the difference in the likelihood for all the words
except “CMS” (Content Management System), so
</bodyText>
<page confidence="0.961358">
255
</page>
<figure confidence="0.20292">
Before Inference The website is designed, CMS works, content has been added and the client is happy.
After Inference The website is designed, CMS works, content has been added and the client is happy.
</figure>
<figureCaption confidence="0.800929">
Figure 5: The effect of tag mixture proportions for credit assignment in a web document. Blue (single
</figureCaption>
<bodyText confidence="0.996343857142857">
underline) words are generated from the design tag; red (dashed underline) from the programming tag.
By themselves, most words used here have a higher probability in programming than in design. But
because the document as a whole is more about design than programming(incorporating words not shown
here), inferring the document’s topic-mixture θ enables L-LDA to correctly re-assign most words.
that L-LDA correctly infers that most of the words
in this passage have more to do with design than
programming.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999982538461538">
This paper has introduced Labeled LDA, a novel
model of multi-labeled corpora that directly ad-
dresses the credit assignment problem. The new
model improves upon LDA for labeled corpora
by gracefully incorporating user supervision in the
form of a one-to-one mapping between topics and
labels. We demonstrate the model’s effectiveness
on tasks related to credit attribution within docu-
ments, including document visualizations and tag-
specific snippet extraction. An approximation to
Labeled LDA is also shown to be competitive with
a strong baseline (multiple one vs-rest SVMs) for
multi-label classification.
Because Labeled LDA is a graphical model
in the LDA family, it enables a range of natu-
ral extensions for future investigation. For exam-
ple, the current model does not capture correla-
tions between labels, but such correlations might
be introduced by composing Labeled LDA with
newer state of the art topic models like the Cor-
related Topic Model (Blei and Lafferty, 2006) or
the Pachinko Allocation Model (Li and McCal-
lum, 2006). And with improved inference for un-
supervised Λ, Labeled LDA lends itself naturally
to modeling semi-supervised corpora where labels
are observed for only some documents.
</bodyText>
<sectionHeader confidence="0.998762" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.914502333333333">
This project was supported in part by the Presi-
dent of Stanford University through the IRiSS Ini-
tiatives Assessment project.
</bodyText>
<sectionHeader confidence="0.995743" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889">
D. M. Blei and J. Lafferty. 2006. Correlated Topic
Models. NIPS, 18:147.
D. Blei and J McAuliffe. 2007. Supervised Topic
Models. In NIPS, volume 21.
D. M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet allocation. JMLR.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. PNAS, 1:5228–35.
P. Heymann, G. Koutrika, and H. Garcia-Molina. 2008.
Can social bookmarking improve web search. In
WSDM.
S. Ji, L. Tang, S. Yu, and J. Ye. 2008. Extracting
shared subspace for multi-label classification. In
KDD, pages 381–389, New York, NY, USA. ACM.
H. Kazawa, H. Taira T. Izumitani, and E. Maeda. 2004.
Maximal margin labeling for multi-topic text catego-
rization. In NIPS.
S. Lacoste-Julien, F. Sha, and M. I. Jordan. 2008. Dis-
cLDA: Discriminative learning for dimensionality
reduction and classification. In NIPS, volume 22.
D. D. Lewis, Y. Yang, T. G. Rose, G. Dietterich, F. Li,
and F. Li. 2004. RCV1: A new benchmark collec-
tion for text categorization research. JMLR, 5:361–
397.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In International conference on Machine
learning, pages 577–584.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
AAAI-98 workshop on learning for text categoriza-
tion, volume 7.
Q. Mei, X. Shen, and C Zhai. 2007. Automatic label-
ing of multinomial topic models. In KDD.
D. Ramage, P. Heymann, C. D. Manning, and
H. Garcia-Molina. 2009. Clustering the tagged web.
In WSDM.
N. Ueda and K. Saito. 2003. Parametric mixture mod-
els for multi-labeled text includes models that can be
seen to fit within a dimensionality reduction frame-
work. In NIPS.
</reference>
<page confidence="0.998448">
256
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.434527">
<title confidence="0.931191">Labeled LDA: A supervised topic model for credit attribution multi-labeled corpora</title>
<author confidence="0.994533">Daniel Ramage</author>
<author confidence="0.994533">David Hall</author>
<author confidence="0.994533">Ramesh Nallapati</author>
<author confidence="0.994533">D Christopher</author>
<affiliation confidence="0.749023">Computer Science Stanford</affiliation>
<abstract confidence="0.999069346153846">A significant portion of the world’s text is tagged by readers on social bookmarkwebsites. attribution an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web from Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J Lafferty</author>
</authors>
<title>Correlated Topic Models.</title>
<date>2006</date>
<tech>NIPS,</tech>
<pages>18--147</pages>
<marker>Blei, Lafferty, 2006</marker>
<rawString>D. M. Blei and J. Lafferty. 2006. Correlated Topic Models. NIPS, 18:147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J McAuliffe</author>
</authors>
<title>Supervised Topic Models.</title>
<date>2007</date>
<booktitle>In NIPS,</booktitle>
<volume>21</volume>
<contexts>
<context position="4578" citStr="Blei and McAuliffe, 2007" startWordPosition="706" endWordPosition="709">st 2009. c�2009 ACL and AFNLP multiple topics per document, it is not appropriate for multi-labeled corpora because, as an unsupervised model, it offers no obvious way of incorporating a supervised label set into its learning procedure. In particular, LDA often learns some topics that are hard to interpret, and the model provides no tools for tuning the generated topics to suit an end-use application, even when time and resources exist to provide some document labels. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007) and DiscLDA (Lacoste-Julien et al., 2008) are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label. Supervised LDA posits that a label is generated from each document’s empirical topic mixture distribution. DiscLDA associates a single categorical label variable with each document and associates a topic mixture with each label. A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>D. Blei and J McAuliffe. 2007. Supervised Topic Models. In NIPS, volume 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="3239" citStr="Blei et al., 2003" startWordPosition="500" endWordPosition="503">bution problem by associating individual words in a document with their most appropriate labels. For instance, in our news story about the transportation bill, if the model knew that the word “highway” went with transportation and that the word “politicians” went with politics, more relevant passages could be extracted for either label. We seek an approach that can automatically learn the posterior distribution of each word in a document conditioned on the document’s label set. One promising approach to the credit attribution problem lies in the machinery of Latent Dirichlet Allocation (LDA) (Blei et al., 2003), a recent model that has gained popularity among theoreticians and practitioners alike as a tool for automatic corpus summarization and visualization. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. The model generates automatic summaries of topics in terms of a discrete probability distribution over words for each topic, and further infers per-document discrete distributions over topics. Most importantly, LDA makes the explicit assumption that each word is generated from one underlying topic. Although LDA is expressive enough to model 248 Proceedi</context>
<context position="7409" citStr="Blei et al., 2003" startWordPosition="1169" endWordPosition="1172">tive classifier on two multi-label text classification tasks (Section 7). 2 Labeled LDA Labeled LDA is a probabilistic graphical model that describes a process for generating a labeled document collection. Like Latent Dirichlet Allocation, Labeled LDA models each document as a mixture of underlying topics and generates each word from one topic. Unlike LDA, L-LDA incorporates supervision by simply constraining the topic model to use only those topics that correspond to a document’s (observed) label set. The model description that follows assumes the reader is familiar with the basic LDA model (Blei et al., 2003). Let each document d be represented by a tuple consisting of a list of word indices w(d) = (w1, ... , wNd) and a list of binary topic presence/absence indicators Λ(d) = (l1,...,lK) where each wz E {1, ... , V } and each lk E {0,1}. Here Nd is the document length, V is the vocabulary size and K the total number of unique labels in the corpus. We set the number of topics in Labeled LDA to be the number of unique labels K in the corpus. The generative process for the algorithm is found in Table 1. Steps 1 and 2—drawing the multinomial topic distributions over vocabulary )3k for each topic k, fro</context>
<context position="11120" citStr="Blei et al., 2003" startWordPosition="1914" endWordPosition="1917">et distribution with parameters α(d) = L(d) × α = (α2, α3)T (i.e., with the Dirichlet restricted to the topics 2 and 3). This fulfills our requirement that the document’s topics are restricted to its own labels. The projection step constitutes the deterministic step 6 in Table 1. The remaining part of the model from steps 7 through 10 are the same as for regular LDA. The dependency of 0 on both α and A is indicated by directed edges from A and α to 0 in the plate notation in Figure 1. This is the only additional dependency we introduce in LDA’s representation (please compare with Figure 1 in (Blei et al., 2003)). 2.1 Learning and inference In most applications discussed in this paper, we will assume that the documents are multiply tagged with human labels, both at learning and inference time. When the labels A(d) of the document are observed, the labeling prior Φ is d-separated from the rest of the model given A(d). Hence the model is same as traditional LDA, except the constraint that the topic prior α(d) is now restricted to the set of labeled topics X(d). Therefore, we can use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) for training where the sampling probability for a topic for posit</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Dirichlet allocation. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<pages>1--5228</pages>
<publisher>PNAS,</publisher>
<contexts>
<context position="11654" citStr="Griffiths and Steyvers, 2004" startWordPosition="2005" endWordPosition="2008">dency we introduce in LDA’s representation (please compare with Figure 1 in (Blei et al., 2003)). 2.1 Learning and inference In most applications discussed in this paper, we will assume that the documents are multiply tagged with human labels, both at learning and inference time. When the labels A(d) of the document are observed, the labeling prior Φ is d-separated from the rest of the model given A(d). Hence the model is same as traditional LDA, except the constraint that the topic prior α(d) is now restricted to the set of labeled topics X(d). Therefore, we can use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) for training where the sampling probability for a topic for position i in a document d in Labeled LDA is given by: n−i,j + αj (d) n−i,· + αT 1 (d) (3) where nwi −i,j is the count of word wi in topic j, that does not include the current assignment zi, a missing subscript or superscript (e.g. n(·) −i,j)) indicates a summation over that dimension, and 1 is a vector of 1’s of appropriate dimension. Although the equation above looks exactly the same as that of LDA, we have an important distinction in that, the target topic j is restricted to belong to the set of labels, i.e., j ∈ X(d). Once the to</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. PNAS, 1:5228–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heymann</author>
<author>G Koutrika</author>
<author>H Garcia-Molina</author>
</authors>
<title>Can social bookmarking improve web search.</title>
<date>2008</date>
<booktitle>In WSDM.</booktitle>
<contexts>
<context position="16433" citStr="Heymann et al., 2008" startWordPosition="2829" endWordPosition="2832">s Social bookmarking websites contain millions of tags describing many of the web’s most popular and useful pages. However, not all tags are uniformly appropriate at all places within a document. In the sections that follow, we examine mechanisms by which Labeled LDA’s credit assignment mechanism can be utilized to help support browsing and summarizing tagged document collections. To create a consistent dataset for experimenting with our model, we selected 20 tags of medium to high frequency from a collection of documents dataset crawled from del.icio.us, a popular social bookmarking website (Heymann et al., 2008). From that larger dataset, we selected uniformly at random four thousand documents that contained at least one of the 20 tags, and then filtered each document’s tag set by removing tags not present in our tag set. After filtering, the resulting corpus averaged 781 non-stop words per document, with each document having 4 distinct tags on average. In contrast to many existing text datasets, our tagged corpus is highly multiply labeled: almost 90% of of the documents have more than one tag. (For comparison, less than one third of the news documents in the popular RCV1-v2 collection of newswire a</context>
</contexts>
<marker>Heymann, Koutrika, Garcia-Molina, 2008</marker>
<rawString>P. Heymann, G. Koutrika, and H. Garcia-Molina. 2008. Can social bookmarking improve web search. In WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ji</author>
<author>L Tang</author>
<author>S Yu</author>
<author>J Ye</author>
</authors>
<title>Extracting shared subspace for multi-label classification. In</title>
<date>2008</date>
<booktitle>KDD,</booktitle>
<pages>381--389</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="27038" citStr="Ji et al. (2008)" startWordPosition="4595" endWordPosition="4598">eled LDA’s credit attribution mechanism enabled effective modeling within documents. In this section, we consider whether L-LDA can be adapted as an effective multi-label classifier for documents as a whole. To answer that question, we applied a modified variant of L-LDA to a multi-label document classification problem: given a training set consisting of documents with multiple labels, predict the set of labels appropriate for each document in a test set. Multi-label classification is a well researched problem. Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al. (2008)). Others, like our algorithm are based on mixture models (such as Ueda and Saito (2003)). However, we are aware of no methods that trade off label-specific word distributions with document-specific label distributions in quite the same way. In Section 2, we discussed learning and inference when labels are observed. In the task of multilabel classification, labels are available at training time, so the learning part remains the same as discussed before. However, inferring the best set of labels for an unlabeled document at test time is more complex: it involves assessing all label assignments </context>
<context position="29069" citStr="Ji et al., 2008" startWordPosition="4927" endWordPosition="4930">As a baseline, we use a set of multiple one-vsrest SVM classifiers which is a popular and extremely competitive baseline used by most previous papers (see (Kazawa et al., 2004; Ueda and Saito, 2003) for instance). We scored each model based on Micro-F1 and Macro-F1 as our evaluation measures (Lewis et al., 2004). While the former allows larger classes to dominate its results, the latter assigns an equal weight to all classes, providing us complementary information. 7.1 Yahoo We ran experiments on a corpus from the Yahoo directory, modeling our experimental conditions on the ones described in (Ji et al., 2008).2 We considered documents drawn from 8 top level categories in the Yahoo directory, where each document can be placed in any number of subcategories. The results were mixed, with SVMs ahead on one measure: Labeled LDA beat SVMs on five out of eight datasets on MacroF1, but didn’t win on any datasets on MicroF1. Results are presented in Table 3. Because only a processed form of the documents was released, the Yahoo dataset does not lend itself well to error analysis. However, only 33% of the documents in each top-level category were applied to more than one sub-category, so the credit assignme</context>
<context position="30757" citStr="Ji et al., 2008" startWordPosition="5217" endWordPosition="5220">, outperforming on four datasets with the other four a statistical tie. It is worth noting that the Yahoo datasets are skewed by construction to contain many documents with highly overlapping content: because each collection is within the same super-class such as “Arts”, “Business”, etc., each sub-categories’ 2We did not carefully tune per-class thresholds of each of the one vs. rest classifiers in each model, but instead tuned only one threshold for all classifiers in each model via crossvalidation on the Arts subsets. As such, our numbers were on an average 3-4% less than those reported in (Ji et al., 2008), but the methods were comparably tuned. 3The difference between means of multiple runs were not significantly different by two-tailed paired t-test. 254 Dataset %MacroF1 %MicroF1 L-LDA SVM L-LDA SVM Arts 30.70(1.62) 23.23 (0.67) 39.81(1.85) 48.42 (0.45) Business 30.81(0.75) 22.82 (1.60) 67.00(1.29) 72.15 (0.62) Computers 27.55(1.98) 18.29 (1.53) 48.95(0.76) 61.97 (0.54) Education 33.78(1.70) 36.03 (1.30) 41.19(1.48) 59.45 (0.56) Entertainment 39.42(1.38) 43.22 (0.49) 47.71(0.61) 62.89 (0.50) Health 45.36(2.00) 47.86 (1.72) 58.13(0.43) 72.21 (0.26) Recreation 37.63(1.00) 33.77 (1.17) 43.71(0.3</context>
</contexts>
<marker>Ji, Tang, Yu, Ye, 2008</marker>
<rawString>S. Ji, L. Tang, S. Yu, and J. Ye. 2008. Extracting shared subspace for multi-label classification. In KDD, pages 381–389, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kazawa</author>
<author>H Taira T Izumitani</author>
<author>E Maeda</author>
</authors>
<title>Maximal margin labeling for multi-topic text categorization.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="27020" citStr="Kazawa et al. (2004)" startWordPosition="4591" endWordPosition="4594">e demonstrated how Labeled LDA’s credit attribution mechanism enabled effective modeling within documents. In this section, we consider whether L-LDA can be adapted as an effective multi-label classifier for documents as a whole. To answer that question, we applied a modified variant of L-LDA to a multi-label document classification problem: given a training set consisting of documents with multiple labels, predict the set of labels appropriate for each document in a test set. Multi-label classification is a well researched problem. Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al. (2008)). Others, like our algorithm are based on mixture models (such as Ueda and Saito (2003)). However, we are aware of no methods that trade off label-specific word distributions with document-specific label distributions in quite the same way. In Section 2, we discussed learning and inference when labels are observed. In the task of multilabel classification, labels are available at training time, so the learning part remains the same as discussed before. However, inferring the best set of labels for an unlabeled document at test time is more complex: it involves assessing all </context>
<context position="28628" citStr="Kazawa et al., 2004" startWordPosition="4854" endWordPosition="4857">s paper we make the simplifying assumption that the model reduces to standard LDA at inference, where the document is free to sample from any of the K topics. This is a reasonable assumption because allowing the model to explore the whole topic space for each document is similar to exploring all possible label assignments. The document’s most likely labels can then be inferred by suitably thresholding its posterior probability over topics. As a baseline, we use a set of multiple one-vsrest SVM classifiers which is a popular and extremely competitive baseline used by most previous papers (see (Kazawa et al., 2004; Ueda and Saito, 2003) for instance). We scored each model based on Micro-F1 and Macro-F1 as our evaluation measures (Lewis et al., 2004). While the former allows larger classes to dominate its results, the latter assigns an equal weight to all classes, providing us complementary information. 7.1 Yahoo We ran experiments on a corpus from the Yahoo directory, modeling our experimental conditions on the ones described in (Ji et al., 2008).2 We considered documents drawn from 8 top level categories in the Yahoo directory, where each document can be placed in any number of subcategories. The resu</context>
</contexts>
<marker>Kazawa, Izumitani, Maeda, 2004</marker>
<rawString>H. Kazawa, H. Taira T. Izumitani, and E. Maeda. 2004. Maximal margin labeling for multi-topic text categorization. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Julien</author>
<author>F Sha</author>
<author>M I Jordan</author>
</authors>
<title>DiscLDA: Discriminative learning for dimensionality reduction and classification.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<volume>22</volume>
<contexts>
<context position="4620" citStr="Lacoste-Julien et al., 2008" startWordPosition="712" endWordPosition="715">topics per document, it is not appropriate for multi-labeled corpora because, as an unsupervised model, it offers no obvious way of incorporating a supervised label set into its learning procedure. In particular, LDA often learns some topics that are hard to interpret, and the model provides no tools for tuning the generated topics to suit an end-use application, even when time and resources exist to provide some document labels. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007) and DiscLDA (Lacoste-Julien et al., 2008) are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label. Supervised LDA posits that a label is generated from each document’s empirical topic mixture distribution. DiscLDA associates a single categorical label variable with each document and associates a topic mixture with each label. A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution. But, like th</context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>S. Lacoste-Julien, F. Sha, and M. I. Jordan. 2008. DiscLDA: Discriminative learning for dimensionality reduction and classification. In NIPS, volume 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T G Rose</author>
<author>G Dietterich</author>
<author>F Li</author>
<author>F Li</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>JMLR,</journal>
<volume>5</volume>
<pages>397</pages>
<contexts>
<context position="28766" citStr="Lewis et al., 2004" startWordPosition="4878" endWordPosition="4881">any of the K topics. This is a reasonable assumption because allowing the model to explore the whole topic space for each document is similar to exploring all possible label assignments. The document’s most likely labels can then be inferred by suitably thresholding its posterior probability over topics. As a baseline, we use a set of multiple one-vsrest SVM classifiers which is a popular and extremely competitive baseline used by most previous papers (see (Kazawa et al., 2004; Ueda and Saito, 2003) for instance). We scored each model based on Micro-F1 and Macro-F1 as our evaluation measures (Lewis et al., 2004). While the former allows larger classes to dominate its results, the latter assigns an equal weight to all classes, providing us complementary information. 7.1 Yahoo We ran experiments on a corpus from the Yahoo directory, modeling our experimental conditions on the ones described in (Ji et al., 2008).2 We considered documents drawn from 8 top level categories in the Yahoo directory, where each document can be placed in any number of subcategories. The results were mixed, with SVMs ahead on one measure: Labeled LDA beat SVMs on five out of eight datasets on MacroF1, but didn’t win on any data</context>
</contexts>
<marker>Lewis, Yang, Rose, Dietterich, Li, Li, 2004</marker>
<rawString>D. D. Lewis, Y. Yang, T. G. Rose, G. Dietterich, F. Li, and F. Li. 2004. RCV1: A new benchmark collection for text categorization research. JMLR, 5:361– 397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: Dag-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In International conference on Machine learning,</booktitle>
<pages>577--584</pages>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: Dag-structured mixture models of topic correlations. In International conference on Machine learning, pages 577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification.</title>
<date>1998</date>
<booktitle>In AAAI-98 workshop on learning for text categorization,</booktitle>
<volume>7</volume>
<contexts>
<context position="14153" citStr="McCallum and Nigam, 1998" startWordPosition="2455" endWordPosition="2458">ersion that allows topics that do not correspond to the label set of a given document with a small probability, or one that allows a common background topic in all documents. We did implement these variants in our preliminary experiments, but they did not yield better performance than L-LDA in the tasks we considered. Hence we do not report them in this paper. 2.2 Relationship to Naive Bayes The derivation of the algorithm so far has focused on its relationship to LDA. However, Labeled LDA can also be seen as an extension of the event model of a traditional Multinomial Naive Bayes classifier (McCallum and Nigam, 1998) by the introduction of a mixture model. In this section, we develop the analogy as another way to understand L-LDA from a supervised perspective. Consider the case where no document in the collection is assigned two or more labels. Now for a particular document d with label ld, Labeled LDA draws each word’s topic variable zz from a multinomial constrained to the document’s label set, i.e. zz = ld for each word position i in the document. During learning, the Gibbs sampler will assign each zz to ld while incrementing Old(wz), effectively counting the occurences of each word type in documents l</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. A comparison of event models for naive bayes text classification. In AAAI-98 workshop on learning for text categorization, volume 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>X Shen</author>
<author>C Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="18349" citStr="Mei et al., 2007" startWordPosition="3146" endWordPosition="3149">sociated tag. Arrows from right-to-left show the mapping of LDA topics to the closest Labeled LDA topic by cosine similarity. Tags not shown are: design, education, english, grammar, history, internet, language, philosophy, politics, programming, reference, style, writing. Figure 2 shows the top words associated with 20 topics learned by Labeled LDA and 20 topics learned by unsupervised LDA on the del.icio.us document collection. Labeled LDA’s topics are directly named with the tag that corresponds to each topic, an improvement over standard practice of inferring the topic name by inspection (Mei et al., 2007). The topics learned by the unsupervised variant were matched to a Labeled LDA topic highest cosine similarity. The topics selected are representative: compared to Labeled LDA, unmodified LDA allocates many topics for describing the largest parts of the The Elements of Style, William Strunk, Jr. Asserting that one must first know the rules to break them, this classic reference book is a must-have for any student and conscientious writer. Intended for use in which the practice of composition is combined with the study of literature,it gives in brief space the principal requirements of plain Eng</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Q. Mei, X. Shen, and C Zhai. 2007. Automatic labeling of multinomial topic models. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>P Heymann</author>
<author>C D Manning</author>
<author>H Garcia-Molina</author>
</authors>
<title>Clustering the tagged web. In WSDM.</title>
<date>2009</date>
<contexts>
<context position="5016" citStr="Ramage et al., 2009" startWordPosition="775" endWordPosition="779">provide some document labels. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007) and DiscLDA (Lacoste-Julien et al., 2008) are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label. Supervised LDA posits that a label is generated from each document’s empirical topic mixture distribution. DiscLDA associates a single categorical label variable with each document and associates a topic mixture with each label. A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution. But, like the other models, MM-LDA’s learned topics do not correspond directly with the label set. Consequently, these models fall short as a solution to the credit attribution problem. Because labels have meaning to the people that assigned them, a simple solution to the credit attribution problem is to assign a document’s words to its labels rather than to a latent and possibly less interpretable semant</context>
</contexts>
<marker>Ramage, Heymann, Manning, Garcia-Molina, 2009</marker>
<rawString>D. Ramage, P. Heymann, C. D. Manning, and H. Garcia-Molina. 2009. Clustering the tagged web. In WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueda</author>
<author>K Saito</author>
</authors>
<title>Parametric mixture models for multi-labeled text includes models that can be seen to fit within a dimensionality reduction framework.</title>
<date>2003</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="27126" citStr="Ueda and Saito (2003)" startWordPosition="4610" endWordPosition="4613">. In this section, we consider whether L-LDA can be adapted as an effective multi-label classifier for documents as a whole. To answer that question, we applied a modified variant of L-LDA to a multi-label document classification problem: given a training set consisting of documents with multiple labels, predict the set of labels appropriate for each document in a test set. Multi-label classification is a well researched problem. Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al. (2008)). Others, like our algorithm are based on mixture models (such as Ueda and Saito (2003)). However, we are aware of no methods that trade off label-specific word distributions with document-specific label distributions in quite the same way. In Section 2, we discussed learning and inference when labels are observed. In the task of multilabel classification, labels are available at training time, so the learning part remains the same as discussed before. However, inferring the best set of labels for an unlabeled document at test time is more complex: it involves assessing all label assignments and returning the assignment that has the highest posterior probability. However, this i</context>
<context position="28651" citStr="Ueda and Saito, 2003" startWordPosition="4858" endWordPosition="4861">implifying assumption that the model reduces to standard LDA at inference, where the document is free to sample from any of the K topics. This is a reasonable assumption because allowing the model to explore the whole topic space for each document is similar to exploring all possible label assignments. The document’s most likely labels can then be inferred by suitably thresholding its posterior probability over topics. As a baseline, we use a set of multiple one-vsrest SVM classifiers which is a popular and extremely competitive baseline used by most previous papers (see (Kazawa et al., 2004; Ueda and Saito, 2003) for instance). We scored each model based on Micro-F1 and Macro-F1 as our evaluation measures (Lewis et al., 2004). While the former allows larger classes to dominate its results, the latter assigns an equal weight to all classes, providing us complementary information. 7.1 Yahoo We ran experiments on a corpus from the Yahoo directory, modeling our experimental conditions on the ones described in (Ji et al., 2008).2 We considered documents drawn from 8 top level categories in the Yahoo directory, where each document can be placed in any number of subcategories. The results were mixed, with SV</context>
</contexts>
<marker>Ueda, Saito, 2003</marker>
<rawString>N. Ueda and K. Saito. 2003. Parametric mixture models for multi-labeled text includes models that can be seen to fit within a dimensionality reduction framework. In NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>