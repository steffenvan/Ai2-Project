<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.160552">
<title confidence="0.956999">
Real-Time Correction of Closed-Captions
</title>
<author confidence="0.950629">
P. Cardinal, G. Boulianne, M. Comeau, M. Boisvert
</author>
<affiliation confidence="0.7361045">
Centre de recherche Informatique de Montreal (CRIM)
Montreal, Canada
</affiliation>
<email confidence="0.976904">
patrick.cardinal@crim.ca
</email>
<sectionHeader confidence="0.995044" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999369615384615">
Live closed-captions for deaf and hard of
hearing audiences are currently produced
by stenographers, or by voice writers us-
ing speech recognition. Both techniques can
produce captions with errors. We are cur-
rently developing a correction module that
allows a user to intercept the real-time cap-
tion stream and correct it before it is broad-
cast. We report results of preliminary ex-
periments on correction rate and actual user
performance using a prototype correction
module connected to the output of a speech
recognition captioning system.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941368421053">
CRIM’s automatic speech recognition system has
been applied to live closed-captioning of french-
canadian television programs (Boulianne et al.,
2006). The low error rate of our approach depends
notably on the integration of the re-speak method
(Imai et al., 2002) for a controlled acoustic environ-
ment, automatic speaker adaptation and dynamic up-
dates of language models and vocabularies, and was
deemed acceptable by several Canadian broadcast-
ers (RDS,CPAC,GTVA and TQS) who have adopted
it over the past few years for captioning sports, pub-
lic affairs and newscasts.
However, for sensitive applications where error
rates must practically be zero, or other situations
where speech recognition error rates are too high,
we are currently developing a real-time correction
interface. In essence, this interface allows a user to
correct the word stream from speech recognition be-
fore it arrives at the closed-caption encoder.
</bodyText>
<sectionHeader confidence="0.982113" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.998636533333333">
Real-time correction must be done within difficult
constraints : with typical captioning rates of 130
words per minute, and 5 to 10% word error rate,
the user must correct between 6 and 13 errors per
minute. In addition, the process should not introduce
more than a few seconds of additional delay over the
3 seconds already needed by speech recognition.
In a previous work, (Wald et al., 2006) ex-
plored how different input modalities, such as
mouse/keyboard combination, keyboard only or
function keys to select words for editing, could re-
duce the amount of time required for correction. In
(Bateman et al., 2000), the correction interface con-
sisted in a scrolling window which can be edited by
the user using a text editor style interface. They
introduced the idea of a controllable delay during
which the text can be edited.
Our approach combines characteristics of the two
previous systems. We use a delay parameter, which
can be modified online, for controlling the output
rate. We also use the standard mouse/keyboard com-
bination for selecting and editing words. However
we added, for each word, a list of alternate words
that can be selected by a simple mouse click; this
simplifies the edition process and speeds up the cor-
rection time. However, manual word edition is still
available.
Another distinctive feature of our approach is
the fixed word position. When a word appears on
screen, it will remain in its position until it is sent
</bodyText>
<page confidence="0.988603">
113
</page>
<bodyText confidence="0.8674318">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 113–116,
Prague, June 2007. c�2007 Association for Computational Linguistics
out. This allows the user to focus on the words
and not be distracted by word-scrolling or any other
word movement.
</bodyText>
<sectionHeader confidence="0.961281" genericHeader="method">
3 Correction Software
</sectionHeader>
<bodyText confidence="0.999979245283019">
The correction software allows edition of the closed-
captions by intercepting them while they are being
sent to the encoder. Both assisted and manual cor-
rections can be applied to the word stream.
Assisted correction reduces the number of opera-
tions by presenting a list of alternate words, so that
a correction can be done with a simple mouse click.
Manual correction requires editing the word to be
changed and is more expensive in terms of delay.
As a consequence, the number of these operations
should be reduced to a strict minimum.
The user interface shown in figure 1 has been de-
signed with this consideration in mind. The princi-
pal characteristic of the interface is that there is no
scrolling. Words never move; instead the matrix is
filled from left to right, top to bottom, with words
coming from the speech recognition, in synchroni-
sation with the audio. When the bottom right of
the matrix is reached, filling in starts from the upper
left corner again. Words appear in blue while they
are editable, and in red once they have been sent to
the caption encoder. Thus a blue ”window”, cor-
responding to the interval during which words can
be edited, moves across the word matrix, while the
words themselves remain fixed.
For assisted correction, the list of available alter-
natives is presented in a list box under each word.
These lists are always present, instead of being pre-
sented only upon selection of a word. In this way
the user has the opportunity of scanning the lists in
advance whenever his time budget allows.
The selected word can also be deleted with a sin-
gle click. Different shortcut corrections, as sug-
gested in (Wald et al., 2006) can also be applied
depending on the mouse button used to select the
word: a left button click changes the gender (mas-
culin or feminin) of the word while a right button
click changes the plurality (singular or plural) of the
word. These available choices are in principle ex-
cluded from the list box choices.
To apply a manual correction, the user simply
clicks the word with the middle button to make it
editable; modifications are done using the keyboard.
Two users can run two correction interfaces in
parallel, on alternating sentences. This configuration
avoids the accumulation of delays. This functional-
ity may prove useful if the word rate is so high that it
becomes too difficult to keep track of the word flow.
In this mode, the second user can begin the correc-
tion of a new sentence even if the first has not yet
completed the correction of his/her sentence. Only
one out of two sentences is editable by each user.
The synchronisation is on a sentence basis.
</bodyText>
<subsectionHeader confidence="0.996948">
3.1 Alternate word lists
</subsectionHeader>
<bodyText confidence="0.999975944444445">
As described in the previous section, the gen-
der/plurality forms of the word are implicitly in-
cluded and accessible through a simple left/right
mouse click. Other available forms explicitly appear
in a list box. This approach has two major benefits.
First, when a gender/plurality error is detected by the
user, no delay is incurred from scanning the choices
in the list box. Second, since the gender/plurality
forms are not included in the list box, their place be-
comes available for additional alternate words.
The main problem is to establish word lists short
enough to reduce scanning time, but long enough to
contain the correct form. For a given word output by
the speech recognition system, the alternate words
should be those that are most likely to be confused
by the recognizer.
We experimented with two pre-computed sources
of alternate word lists:
</bodyText>
<listItem confidence="0.994986714285714">
1. A list of frequently confused words was com-
puted from all the available closed-captions of
our speech recognition system for which corre-
sponding exact transcriptions exist. The train-
ing and development sets were made up of
1.37M words and 0.17M words, respectively.
2. A phoneme based confusion matrix was used
</listItem>
<bodyText confidence="0.9652575">
for scoring the alignment of each word of the
vocabulary with every other word of the same
vocabulary. The alignment program was an im-
plementation of the standard dynamic program-
ming technique for string alignment (Cormen
et al., 2001).
Each of these techniques yields a list of alternate
words with probabilities based on substitution like-
</bodyText>
<page confidence="0.998477">
114
</page>
<figureCaption confidence="0.998517">
Figure 1: Real-time corrector software.
</figureCaption>
<table confidence="0.99783575">
Source of alternates coverage (%)
Word confusion matrix 52%
Phoneme confusion matrix 37%
Combined 60%
</table>
<tableCaption confidence="0.999749">
Table 1: Coverage of substitutions (dev set).
</tableCaption>
<bodyText confidence="0.999924">
lihoods. Table 1 shows how many times substitu-
tions in the development set could be corrected with
a word in the list, for each list and their combination.
To combine both lists, we take this coverage into
consideration and the fact that 48% of the words
were common to both lists. On this basis, we have
constructed an alternate list of 10 words comprised
of the most likely 7 words of case 1; the remaining 3
words are the most probable substitutions from the
remaining words of both lists.
</bodyText>
<subsectionHeader confidence="0.999083">
3.2 Real-time List Update
</subsectionHeader>
<bodyText confidence="0.999987947368421">
The previous technique can only handle simple sub-
stitutions: a word that is replaced by another one.
Another frequent error in speech recognition is the
replacement of a single word by several smaller
ones. In this case, the sequence of errors contains
one substitution and one or more insertions. From
the interface point of view, the user must delete some
words before editing the last word in the sequence.
To assist the user in this case, we have imple-
mented the following procedure. When a word is
deleted by the user, the phonemes of this word are
concatenated with those of the following words. The
resulting sequence of phonemes is used to search the
dictionary for the most likely words according to the
pronunciation. These words are dynamically added
to the list appearing under the preceding word. The
search technique used is the same alignment proce-
dure implemented for computing the confusion ma-
trix based on phoneme confusion.
</bodyText>
<sectionHeader confidence="0.999886" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9998865">
In this section we present the results of two prelim-
inary experiments. In the first one, we simulated
a perfect correction, as if the user had an infinite
amount of time, to determine the best possible re-
sults that can be expected from the alternate word
lists. In the second experiment, we submitted a pro-
totype to users and collected performance measure-
ments.
</bodyText>
<subsectionHeader confidence="0.99961">
4.1 Simulation Results
</subsectionHeader>
<bodyText confidence="0.999956">
The simulation is applied to a test set consisting
of a 30 minute hockey game description for which
closed-captions and exact transcripts are available.
We aligned the produced closed-captions with their
corrected transcripts and replaced any incorrect
word by its correct counterpart if it appeared in the
alternate list. In addition, all insertion errors were
deleted. Table 2 shows the word error rate (WER)
</bodyText>
<page confidence="0.996301">
115
</page>
<table confidence="0.9985306">
Source of alternates WER
Original closed-captions 5.8%
Phoneme confusion matrix 4.4%
Word confusion matrix 3.1%
Combined 2.9%
</table>
<tableCaption confidence="0.949074">
Table 2: Error rate for perfect correction.
</tableCaption>
<table confidence="0.991847125">
Delay
2 seconds 15 seconds
test duration 30 minutes 8 minutes
# of words 4631 1303
# of editions 21 28
WER before 6.8% 6.2%
WER after 6.1% 2.5%
Gain (relative %) 8.1% 58.7%
</table>
<tableCaption confidence="0.999649">
Table 3: Error rate after user correction.
</tableCaption>
<bodyText confidence="0.999299142857143">
obtained for different alternate word lists.
The word confusion matrix captures most of the
substitutions. This behavior was expected since the
matrix has been trained explicitely for that purpose.
The performance should increase in the future as the
amount of training data grows. In comparison, the
contribution of words from the phoneme confusion
matrix is clearly limited.
The corrected word was the first in the list 35%
of the time, while it was in the first three 59% of
the time. We also simulated the effect of collaps-
ing words in insertion-substitution sequences to al-
low corrections of insertions : the increase in perfor-
mance was less than 0.5%.
</bodyText>
<subsectionHeader confidence="0.973307">
4.2 User Tests
</subsectionHeader>
<bodyText confidence="0.99998925">
Experiments were performed by 3 unacquainted
users of the system on hockey game descriptions.
In one case, we allowed a delay of 15 seconds; the
second case allowed a 2 second delay to give a pre-
liminary assessment of user behavior in the case
of minimum-delay real-time closed-captioning. Ta-
ble 3 shows the error rate before and after correction.
The results show that a significant WER decrease
is achieved by correcting using a delay of 15 sec-
onds. The reduction with a 2 second delay is minor;
with appropriate training, however, we can expect
the users to outperform these preliminary results.
</bodyText>
<sectionHeader confidence="0.976861" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999995809523809">
We are currently developing a user interface for cor-
recting live closed-captions in real-time. The inter-
face presents a list of alternatives for each automati-
cally generated word. The theoretical results that as-
sumes the user always chooses the correct suggested
word shows the potential for large error reductions,
with a minimum of interaction. When larger delays
are allowed, manual edition of words for which there
is no acceptable suggested alternative can yield fur-
ther improvements.
We tested the application for real-time text cor-
rection produced in a real-world application. With
users having no prior experience and with only a 15
second delay, the WER dropped from 6.1% to 2.5%.
In the future, users will be trained on the system
and we expect an important improvement in both
accuracy and required delay. We will also experi-
ment the effect of running 2 corrections in parallel
for more difficult tasks. Future work also includes
the integration of an automatic correction tool for
improving or highlighting the alternate word list.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99962332">
A. Bateman, J. Hewitt, A. Ariyaeeinia, P. Sivakumaran,
and A. Lambourne. 2000. The Quest for The Last
5%: Interfaces for Correcting Real-Time Speech-
Generated Subtitles Proceedings of the 2000 Confer-
ence on Human Factors in Computing Systems (CHI
2000), April 1-6, The Hague, Netherlands.
T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein
2001. Introduction to Algorithms second edition, MIT
Press, Cambridge, MA.
G. Boulianne, J.-F. Beaumont, M. Boisvert, J. Brousseau,
P. Cardinal, C. Chapdelaine, M.Comeau, P. Ouellet,
and F. Osterrath. 2006. Computer-assisted closed-
captioning of live TV broadcasts in French Proceed-
ings of the 2006 Interspeech - ICSLP, September 17-
21, Pittsburg, US.
T. Imai, A. Matsui, S. Homma, T. Kobayakawa, O.
Kazuo, S. Sato, and A. Ando 2002. Speech Recogni-
tion with a respeak methodfor subtiling live broadcast
Proceedings of the 2002 ICSLP, September 16-20, Or-
lando, US.
Wald, M. 2006 Creating Accessible Educational Multi-
media through Editing Automatic Speech Recognition
Captioning in Real Time. International Journal of In-
teractive Technology and Smart Education : Smarter
Use of Technology in Education 3(2) pp. 131-142
</reference>
<page confidence="0.999024">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978079">
<title confidence="0.999943">Real-Time Correction of Closed-Captions</title>
<author confidence="0.999925">P Cardinal</author>
<author confidence="0.999925">G Boulianne</author>
<author confidence="0.999925">M Comeau</author>
<author confidence="0.999925">M Boisvert</author>
<affiliation confidence="0.999262">Centre de recherche Informatique de Montreal (CRIM)</affiliation>
<address confidence="0.985574">Montreal, Canada</address>
<email confidence="0.997481">patrick.cardinal@crim.ca</email>
<abstract confidence="0.999692571428571">Live closed-captions for deaf and hard of hearing audiences are currently produced by stenographers, or by voice writers using speech recognition. Both techniques can produce captions with errors. We are currently developing a correction module that allows a user to intercept the real-time caption stream and correct it before it is broadcast. We report results of preliminary experiments on correction rate and actual user performance using a prototype correction module connected to the output of a speech recognition captioning system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bateman</author>
<author>J Hewitt</author>
<author>A Ariyaeeinia</author>
<author>P Sivakumaran</author>
<author>A Lambourne</author>
</authors>
<title>The Quest for The Last 5%: Interfaces for Correcting Real-Time SpeechGenerated</title>
<date>2000</date>
<booktitle>Subtitles Proceedings of the 2000 Conference on Human Factors in Computing Systems (CHI</booktitle>
<pages>1--6</pages>
<location>The Hague, Netherlands.</location>
<contexts>
<context position="2297" citStr="Bateman et al., 2000" startWordPosition="351" endWordPosition="354">ground Real-time correction must be done within difficult constraints : with typical captioning rates of 130 words per minute, and 5 to 10% word error rate, the user must correct between 6 and 13 errors per minute. In addition, the process should not introduce more than a few seconds of additional delay over the 3 seconds already needed by speech recognition. In a previous work, (Wald et al., 2006) explored how different input modalities, such as mouse/keyboard combination, keyboard only or function keys to select words for editing, could reduce the amount of time required for correction. In (Bateman et al., 2000), the correction interface consisted in a scrolling window which can be edited by the user using a text editor style interface. They introduced the idea of a controllable delay during which the text can be edited. Our approach combines characteristics of the two previous systems. We use a delay parameter, which can be modified online, for controlling the output rate. We also use the standard mouse/keyboard combination for selecting and editing words. However we added, for each word, a list of alternate words that can be selected by a simple mouse click; this simplifies the edition process and </context>
</contexts>
<marker>Bateman, Hewitt, Ariyaeeinia, Sivakumaran, Lambourne, 2000</marker>
<rawString>A. Bateman, J. Hewitt, A. Ariyaeeinia, P. Sivakumaran, and A. Lambourne. 2000. The Quest for The Last 5%: Interfaces for Correcting Real-Time SpeechGenerated Subtitles Proceedings of the 2000 Conference on Human Factors in Computing Systems (CHI 2000), April 1-6, The Hague, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
<author>C Stein</author>
</authors>
<date>2001</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<note>Introduction to Algorithms second edition,</note>
<contexts>
<context position="7467" citStr="Cormen et al., 2001" startWordPosition="1229" endWordPosition="1232">We experimented with two pre-computed sources of alternate word lists: 1. A list of frequently confused words was computed from all the available closed-captions of our speech recognition system for which corresponding exact transcriptions exist. The training and development sets were made up of 1.37M words and 0.17M words, respectively. 2. A phoneme based confusion matrix was used for scoring the alignment of each word of the vocabulary with every other word of the same vocabulary. The alignment program was an implementation of the standard dynamic programming technique for string alignment (Cormen et al., 2001). Each of these techniques yields a list of alternate words with probabilities based on substitution like114 Figure 1: Real-time corrector software. Source of alternates coverage (%) Word confusion matrix 52% Phoneme confusion matrix 37% Combined 60% Table 1: Coverage of substitutions (dev set). lihoods. Table 1 shows how many times substitutions in the development set could be corrected with a word in the list, for each list and their combination. To combine both lists, we take this coverage into consideration and the fact that 48% of the words were common to both lists. On this basis, we hav</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein 2001. Introduction to Algorithms second edition, MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Boulianne</author>
<author>J-F Beaumont</author>
<author>M Boisvert</author>
<author>J Brousseau</author>
<author>P Cardinal</author>
<author>C Chapdelaine</author>
<author>P Ouellet M Comeau</author>
<author>F Osterrath</author>
</authors>
<date>2006</date>
<booktitle>Computer-assisted closedcaptioning of live TV broadcasts in French Proceedings of the 2006 Interspeech - ICSLP,</booktitle>
<location>Pittsburg, US.</location>
<contexts>
<context position="896" citStr="Boulianne et al., 2006" startWordPosition="126" endWordPosition="129">ed by stenographers, or by voice writers using speech recognition. Both techniques can produce captions with errors. We are currently developing a correction module that allows a user to intercept the real-time caption stream and correct it before it is broadcast. We report results of preliminary experiments on correction rate and actual user performance using a prototype correction module connected to the output of a speech recognition captioning system. 1 Introduction CRIM’s automatic speech recognition system has been applied to live closed-captioning of frenchcanadian television programs (Boulianne et al., 2006). The low error rate of our approach depends notably on the integration of the re-speak method (Imai et al., 2002) for a controlled acoustic environment, automatic speaker adaptation and dynamic updates of language models and vocabularies, and was deemed acceptable by several Canadian broadcasters (RDS,CPAC,GTVA and TQS) who have adopted it over the past few years for captioning sports, public affairs and newscasts. However, for sensitive applications where error rates must practically be zero, or other situations where speech recognition error rates are too high, we are currently developing a</context>
</contexts>
<marker>Boulianne, Beaumont, Boisvert, Brousseau, Cardinal, Chapdelaine, Comeau, Osterrath, 2006</marker>
<rawString>G. Boulianne, J.-F. Beaumont, M. Boisvert, J. Brousseau, P. Cardinal, C. Chapdelaine, M.Comeau, P. Ouellet, and F. Osterrath. 2006. Computer-assisted closedcaptioning of live TV broadcasts in French Proceedings of the 2006 Interspeech - ICSLP, September 17-21, Pittsburg, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Imai</author>
<author>A Matsui</author>
<author>S Homma</author>
<author>T Kobayakawa</author>
<author>O Kazuo</author>
<author>S Sato</author>
<author>A Ando</author>
</authors>
<title>Speech Recognition with a respeak methodfor subtiling live broadcast</title>
<date>2002</date>
<booktitle>Proceedings of the 2002 ICSLP,</booktitle>
<location>Orlando, US.</location>
<contexts>
<context position="1010" citStr="Imai et al., 2002" startWordPosition="146" endWordPosition="149"> are currently developing a correction module that allows a user to intercept the real-time caption stream and correct it before it is broadcast. We report results of preliminary experiments on correction rate and actual user performance using a prototype correction module connected to the output of a speech recognition captioning system. 1 Introduction CRIM’s automatic speech recognition system has been applied to live closed-captioning of frenchcanadian television programs (Boulianne et al., 2006). The low error rate of our approach depends notably on the integration of the re-speak method (Imai et al., 2002) for a controlled acoustic environment, automatic speaker adaptation and dynamic updates of language models and vocabularies, and was deemed acceptable by several Canadian broadcasters (RDS,CPAC,GTVA and TQS) who have adopted it over the past few years for captioning sports, public affairs and newscasts. However, for sensitive applications where error rates must practically be zero, or other situations where speech recognition error rates are too high, we are currently developing a real-time correction interface. In essence, this interface allows a user to correct the word stream from speech r</context>
</contexts>
<marker>Imai, Matsui, Homma, Kobayakawa, Kazuo, Sato, Ando, 2002</marker>
<rawString>T. Imai, A. Matsui, S. Homma, T. Kobayakawa, O. Kazuo, S. Sato, and A. Ando 2002. Speech Recognition with a respeak methodfor subtiling live broadcast Proceedings of the 2002 ICSLP, September 16-20, Orlando, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wald</author>
</authors>
<title>Creating Accessible Educational Multimedia through Editing Automatic Speech Recognition Captioning in Real Time.</title>
<date>2006</date>
<booktitle>International Journal of Interactive Technology and Smart Education : Smarter Use of Technology in Education</booktitle>
<volume>3</volume>
<issue>2</issue>
<pages>131--142</pages>
<marker>Wald, 2006</marker>
<rawString>Wald, M. 2006 Creating Accessible Educational Multimedia through Editing Automatic Speech Recognition Captioning in Real Time. International Journal of Interactive Technology and Smart Education : Smarter Use of Technology in Education 3(2) pp. 131-142</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>