<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.999255">
A Hierarchical Bayesian Model for
Unsupervised Induction of Script Knowledge
</title>
<author confidence="0.959015">
Lea Frermann1 Ivan Titov&apos; Manfred Pinkal3
</author>
<email confidence="0.780007">
l.frermann@ed.ac.uk titov@uva.nl pinkal@coli.uni-sb.de
</email>
<affiliation confidence="0.764466">
1 ILCC, School of Informatics, University of Edinburgh, United Kingdom
&apos; ILLC, University of Amsterdam, Netherlands
3 Department of Computational Linguistics, Saarland University, Germany
</affiliation>
<sectionHeader confidence="0.969153" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995317">
Scripts representing common sense
knowledge about stereotyped sequences
of events have been shown to be a valu-
able resource for NLP applications. We
present a hierarchical Bayesian model for
unsupervised learning of script knowledge
from crowdsourced descriptions of human
activities. Events and constraints on event
ordering are induced jointly in one unified
framework. We use a statistical model
over permutations which captures event
ordering constraints in a more flexible
way than previous approaches. In order
to alleviate the sparsity problem caused
by using relatively small datasets, we
incorporate in our hierarchical model an
informed prior on word distributions. The
resulting model substantially outperforms
a state-of-the-art method on the event
ordering task.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921">
A script is a “predetermined, stereotyped se-
quence of actions that define a well-known sit-
uation” (Schank and Abelson, 1975). While
humans acquire such common-sense knowledge
over their lifetime, it constitutes a bottleneck for
many NLP systems. Effective question answer-
ing and summarization are impossible without a
form of story understanding, which in turn has
been shown to benefit from access to databases of
script knowledge (Mueller, 2004; Miikkulainen,
1995). Knowledge about the typical ordering of
events can further help assessing document co-
herence and generating coherent text. Here, we
present a general method for acquiring data bases
of script knowledge.
Our work may be regarded as complementary to
existing work on learning script knowledge from
natural text (cf. (Chambers and Jurafsky, 2008)),
as not all types of scripts are elaborated in natural
text – being left implicit because of assumed read-
ers’ world knowledge. Our model, operating on
data obtained in a cheap way by crowdsourcing,
is applicable to any kind of script and can fill this
gap. We follow work in inducing script knowl-
edge from explicit instantiations of scripts, so-
called event sequence descriptions (ESDs) (Reg-
neri et al., 2010). Our data consists of sets of
ESDs, each set describing a well-known situation
we will call scenario (e.g., “washing laundry”).
An ESD consists of a sequence of events, each
describing an action defining part of the scenario
(e.g., “place the laundry in the washing machine”).
We refer to descriptions of the same event across
ESDs as event types. We refer to entities involved
in a scenario as participants (e.g., a “washing ma-
chine” or a “detergent”), and to sets of participant
descriptions describing the same entity as partici-
pant types.
For each type of scenario, our model clusters
descriptions which refer to the same type of event,
and infers constraints on the temporal order in
which the events types occur in a particular sce-
nario. Common characteristics of ESDs such as
event optionality and varying degrees of temporal
flexibility of event types make this task nontrivial.
We propose a model which, in contrast to previ-
ous approaches, explicitly targets these character-
istics. We develop a Bayesian formulation of the
script learning problem, and present a generative
model for joint learning of event types and order-
ing constraints, arguing that the temporal position
of an event in an ESD provides a strong cue for its
type, and vice versa. Our model is unsupervised
in that no event- or participant labels are required
for training.
We model constraints on the order of event
types using a statistical model over permutations,
the Generalized Mallows Model (GMM; Fligner
</bodyText>
<page confidence="0.99454">
49
</page>
<note confidence="0.99295">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 49–57,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999949083333333">
and Verducci (1986)). With the GMM we can flex-
ibly model apparent characteristics of scripts, such
as event type-specific temporal flexibility. Assum-
ing that types of participants provide a strong cue
for the type of event they are observed in, we use
participant types as a latent variable in our model.
Finally, by modeling event type occurrence using
Binomial distributions, we can model event op-
tionality, a characteristic of scripts that previous
approaches did not capture.
We evaluate our model on a data set of ESDs
collected via web experiments from non-expert
annotators by Regneri et al. (2010) and compare
our model against their approach. Our model
achieves an absolute average improvement of 7%
over the model of Regneri et al. on the task of
event ordering.
For our unsupervised Bayesian model the lim-
ited size of this training set constitutes an ad-
ditional challenge. In order to alleviate this
problem, we use an informed prior on the word
distributions. Instead of using Dirichlet priors
which do not encode a-priori correlations between
words, we incorporate a logistic normal distri-
bution with the covariance matrix derived from
WordNet. While we will show that prior knowl-
edge as defined above enables the application of
our model to small data sets, we emphasize that
the model is generally widely applicable for two
reasons. First, the data, collected using crowd-
sourcing, is comparatively easy and cheap to ex-
tend. Secondly, our model is domain independent
and can be applied to scenario descriptions from
any domain without any modification. Note that
parameters were tuned on held-out scenarios, and
no scenario-specific tuning was performed.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999710625">
In the 1970s, scripts were introduced as a way to
equip AI systems with world knowledge (Schank
and Abelson, 1975; Barr and Feigenbaum, 1986).
Task-specific script databases were developed
manually. FrameNet (Baker et al., 1998) follows a
similar idea, in defining verb frames together with
argument types that can fill the verbs’ argument
slots. Frames can then be combined into “scenario
frames”. Manual composition of such databases,
is arguably expensive and does not scale well.
This paper follows a series of more recent work
which aims to infer script knowledge automati-
cally from data. Chambers and Jurafsky (2008)
present a system which learns narrative chains
from newswire texts. Relevant phrases are iden-
tified based on shared protagonists. The phrases
are clustered into equivalence classes and tempo-
rally ordered using a pipeline of methods. We
work with explicit event sequence descriptions of
a specific scenario, arguing that large-scale com-
mon sense knowledge is hard to acquire from nat-
ural text, since it is often left implicit. Regneri
et al. (2010) induce script knowledge from ex-
plicit ESDs using a graph-based method. Event
types and ordering constraints are induced by
aligning descriptions of equivalent events using
WordNet-based semantic similarity. On this basis
an abstract graph-representation (Temporal Script
Graph; TSG) of the scenario is computed, us-
ing Multiple Sequence Alignment (MSA). Our
work follows the work of Regneri et al. (2010),
in that we use the same data and aim to focus on
the same task. However, the two approaches de-
scribed above employ a pipeline architecture and
treat event learning and learning ordering con-
straints as separate problems. In contrast, we pro-
pose to learn both tasks jointly. We incorporate
both tasks in a hierarchical Bayesian model, thus
using one unified framework.
A related task, unsupervised frame induction,
has also been considered in the past (Titov and
Klementiev, 2011; Modi et al., 2012; O’Connor,
2012); the frame representations encode events
and participants but ignore the temporal aspect of
script knowledge.
We model temporal constraints on event type
orderings with the Generalized Mallows Model
(GMM; Mallows (1957); Fligner and Verducci
(1986); Klementiev et al. (2008)), a statistical
model over permutations. The GMM is a flexi-
ble model which can specify item-specific sensi-
tivity to perturbation from the item’s position in
the canonical permutation. With the GMM we are
thus able to model event type-specific temporal
flexibility – a feature of scripts that MSA cannot
capture.
The GMM has been successfully applied to
modeling ordering constraints in NLP tasks. Chen
et al. (2009) augment classical topic models with
a GMM, under the assumption that topics in struc-
tured domains (e.g., biographies in Wikipedia)
tend to follow an underlying canonical ordering,
an assumption which matches well our data (the
annotators were asked to follow the temporal or-
</bodyText>
<page confidence="0.992393">
50
</page>
<bodyText confidence="0.99995114893617">
der of events in their descriptions (Regneri et al.,
2010)). Chen et al. show that for these domains
their approach significantly outperforms Marko-
vian modeling of topics. This is expected as
Markov models (MMs) are not very appropriate
for representing linear structure with potentially
missing topics (e.g., they cannot encode that ev-
ery topic is assigned to at most one continuous
fragment of text). Also GMMs are preferable for
smaller collections such as ours, as the parameter
number is linear in the number of topics (i.e., for
us, event types) rather than quadratic as in Markov
models. We are not aware of previous work on
modeling events with GMMs. Conversely, MMs
were considered in the very recent work of Che-
ung et al. (2013) in the context of script induction
from news corpora where the Markovian assump-
tion is much more natural.
There exists a body of work for learning par-
ticipant types involved in scripts. Regneri et al.
(2011) extend their work by inducing participant
types on the basis of the TSG, using structural in-
formation about participant mentions in the TSG
as well as WordNet similarity, which they then
combine into an Integer Linear Program. Simi-
larly, Chambers and Jurafsky (2009) extend their
work on narrative chains, presenting a system with
which they jointly learn event types and semantic
roles of the participants involved, but do not con-
sider event orderings. We include participant types
as a latent feature in our model, assuming that par-
ticipant mentions in an event description are a pre-
dictive feature for the corresponding event type.
One way of alleviating the problem of small
data sets is incorporating informed prior knowl-
edge. Raina et al. (2006) encode word correlations
in a variance-covariance matrix of a multivariate
normal distribution (MVN), and sample prior pa-
rameter vectors from it, thus introducing depen-
dencies among the parameters. They induce the
covariances from supervised learning tasks in the
transfer learning set-up. We use the same idea, but
obtain word covariances from WordNet relations.
In a slightly different setting, covariance matrices
of MVNs have been used in topic models to induce
correlation between topics in documents (Blei and
Lafferty, 2006).
</bodyText>
<sectionHeader confidence="0.980156" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999987916666667">
Our input consists of a corpus of scenario-specific
ESDs, and our goal is to label each event descrip-
tion in an ESD with one event type e. We specify
the number of possible event types E a priori as a
number exceeding the number of event types in all
the scripts considered. The model will select an
effective subset of those types.
Assume a scenario-specific corpus c, consist-
ing of D ESDs, c = {d1, ..., dD}. Each
ESD di consists of Nd event descriptions di =
{di,1, ..., di,Ni}. Boundaries between descriptions
of single events are marked in the data. For each
event description di,n a bag of participant descrip-
tions is extracted. Each participant description
corresponds to one noun phrase as identified au-
tomatically by a dependency parser (cf. Regneri
et al. (2011)). We also associate participant types
with participant descriptions, these types are latent
and induced at the inference stage.
Given such a corpus of ESDs, our model assigns
each event description di,n in an ESD di one event
type zdi,n = e, where e E {1, ..., E}. Assuming
that all ESDs are generated from the same under-
lying set of event types, our objective is to assign
the same event type to equivalent event descrip-
tions across all ESDs in the corpus.
We furthermore assume that there exists a
canonical temporal ordering of event types for
each scenario type, and that events in observed
scenarios tend to follow this ordering, but allowing
for some flexibility. The event labeling sequence
zdi of an entire ESD should reflect this canonical
ordering. This allows us to use global structural
patterns of ESDs in the event type assignments,
and thus introducing dependence between event
types through their position in the sequence.
</bodyText>
<sectionHeader confidence="0.997871" genericHeader="method">
4 The Model
</sectionHeader>
<bodyText confidence="0.99995025">
Before we describe our model, we briefly explain
the Generalized Mallows Model (GMM) which
we use to encode a preference for linear ordering
of events in a script.
</bodyText>
<subsectionHeader confidence="0.995804">
4.1 The (Generalized) Mallows Model
</subsectionHeader>
<bodyText confidence="0.999976222222222">
The Mallows Model (MM) is a statistical model
over orderings (Mallows, 1957). It takes two pa-
rameters o,, the canonical ordering, and p &gt; 0,
a dispersion parameter. The dispersion parame-
ter is a penalty for the divergence d(7r, o,) of an
observed ordering 7r from the canonical ordering
o,. The divergence can be any distance metric but
Kendall’s tau distance (“bubble-sort” distance), a
number of swaps needed to bring 7r in the order o,,
</bodyText>
<page confidence="0.991374">
51
</page>
<bodyText confidence="0.999848">
is arguably the most common choice. The proba-
bility of an observed ordering 7r is defined as
</bodyText>
<equation confidence="0.991133">
e
P(7r|ρ, Q) =
</equation>
<bodyText confidence="0.9995715">
where 0(ρ) is a normalization factor. The distri-
bution is centered around the canonical ordering
(as d(Q, Q) = 0), and the probability decreases
exponentially with an increasing distance. For our
purposes, without loss of generality, we can as-
sume that Q is the identity permutation, that is
Q = [1, ... , n], where n is the number of items.
The Mallows model has been generalized to
take as a parameter a vector of item-specific
dispersion parameters p (Fligner and Verducci,
1986). In order to introduce this extension, we
first need to reformulate Kendall’s tau in a way
that captures item-specific distance. An ordering
7r of n items can be equivalently represented by
a vector of inversion counts v of length n − 1,
where each component vi equals the number of
items j &gt; i that occur before item i in 7r. For
example, for an observed ordering 7r = [2,1,0] the
inversion vector v = (2, 1).1 Then the generalized
Mallows model (GMM) is defined as
</bodyText>
<equation confidence="0.9833065">
GMM(7r|p) oC � e−ρi vi.
i
</equation>
<bodyText confidence="0.9985975">
The GMM can be factorized into item-specific
components, which allows for efficient inference:
</bodyText>
<equation confidence="0.884884">
GMMi(vi|ρi) oC e−ρi vi. (1)
</equation>
<bodyText confidence="0.9999928">
Intuitively, we will be able to induce event type-
specific penalty parameters, and will thus be able
to model individual degrees of temporal flexibility
among the event types.
Since the GMM is member of the exponential
family, a conjugate prior can be defined, which
allows for efficient learning of the parameters p
(Fligner and Verducci, 1990). Like the GMM, its
prior distribution GMM0 can be factorized into
independent components for each item i:
</bodyText>
<equation confidence="0.988529">
GMM0(ρi|vi,0,ν0) oC e−ρivi,0−log(ψi(ρi))ν0. (2)
</equation>
<bodyText confidence="0.8606144">
The parameters vi,0 and ν0 represent our prior
beliefs about flexibility for each item i, and the
strength of these beliefs, respectively.
1Trivially, the inversion count for the last element in the
canonical ordering is always 0.
</bodyText>
<subsectionHeader confidence="0.969425">
4.2 The Generative Story
</subsectionHeader>
<bodyText confidence="0.999987790697675">
Our model encodes two fundamental assumptions,
based on characteristics observed in the data: (1)
We assume that each event type can occur at most
once per ESD; (2) Each participant type is as-
sumed to occur at most once per event type.
The formalized generative story is given in Fig-
ure 1. For each document (ESD) d, we decide in-
dependently for each event type e whether to re-
alize it or not by drawing from Binomial(θe).2
We obtain a binary event vector t where te = 1 if
event type e is realized and te = 0 otherwise. We
draw an event ordering 7r from GMM(p), repre-
sented as a vector of inversion counts.
Now, we pass event types in the order defined
by 7r. For each realized event type i (i.e., i :
ti = 1), we first generate a word (normally a
predicate) from the corresponding language model
Mult(Vi). Then we independently decide for each
participant type p whether to realize it or not with
the probability Binomial(coip). If realized, the
participant word (its syntactic head) is generated
from the participant language model Mult(Wp).
Note that though the distribution controlling
frequency of participant generation (coij) is event
type-specific, the language model associated with
the participant (Mult(Wj)) is shared across
events, thus, ensuring that participant types are de-
fined across events.
The learnt binary realization parameters 0 and
We should ensure that an appropriate number of
events and participants is generated (e.g. the real-
ization probability for obligatory events, observed
in almost every ESD for a particular scenario,
should be close to 1).
Priors We draw the parameters for the binomial
distributions from the Beta distribution, which al-
lows us to model a global preference for using
only few event types and only few participant
types for each event type. We draw the parame-
ters of the multinomials from the Dirichlet distri-
bution, and can thus model a preference towards
sparsity. The GMM parameter vector p is drawn
from GMM0 (c.f. Equation (2)).
</bodyText>
<subsectionHeader confidence="0.999396">
4.3 Adding Prior Knowledge
</subsectionHeader>
<bodyText confidence="0.9988575">
Since we are faced with a limited amount of train-
ing data, we augment the model described above
</bodyText>
<footnote confidence="0.9800535">
2We slightly abuse the notation by dropping the super-
script d for ESD-specific variables.
</footnote>
<equation confidence="0.810454">
−ρ d(π,σ)
0(ρ) ,
</equation>
<page confidence="0.945046">
52
</page>
<table confidence="0.586754">
Generation of parameters
for event type e = 1, ... , E do
θe — Beta(α+, α−) [ freq of event ]
Ve — Dirichlet(ry) [event lang mod]
for participant type p = 1, ... , P do
cpep — Beta(β+, β−) [ freq of ptcpt ]
for participant type p = 1, ... , P do
wp — Dirichlet(S) [ ptcpt lang mod ]
for event type e = 1, ... , E — 1 do
ρe — GMM0(p0, v0) [ ordering params]
Generation of ESD d
</table>
<bodyText confidence="0.569157">
for event type e = 1, ... , E do
</bodyText>
<equation confidence="0.948751125">
te — Binomial(θe) [ realized events ]
7r — GMM(p, v) [ event ordering ]
for event i from 7r s.th. ti=1 do
wi — Mult(Vi) [ event lexical unit ]
for participant type p = 1, ... , P do
up — Binomial(cpep) [ realized ptcpts ]
if up = 1 then
wp — Mult(wp) [ ptcpt lexical unit]
</equation>
<figureCaption confidence="0.999579">
Figure 1: The generative story of the basic model.
</figureCaption>
<bodyText confidence="0.9995136">
to encode correlations between semantically simi-
lar words in the priors for language models. We
describe our approach by first introducing the
model extension allowing for injecting prior cor-
relations between words, and then explaining how
the word correlations are derived from WordNet
(Fellbaum, 1998). Since the event vocabulary
and the participant vocabulary are separate in our
model, the following procedure is carried out sep-
arately, but equivalently, for the two vocabularies.
</bodyText>
<subsectionHeader confidence="0.953557">
4.3.1 Modeling Word Correlation
</subsectionHeader>
<bodyText confidence="0.998002875">
Dirichlet distributions do not provide a way to en-
code correlations between words. To tackle this
problem we add another level in the model hier-
archy: instead of specifying priors Dirichlet(ry)
and Dirichlet(S) directly, we generate them for
each event type e and participant type p using mul-
tivariate normal distributions.
The modification for the generative story is
shown in Figure 2. In this extension, each event
type e and participant type p has a different associ-
ated (nonsymmetric) Dirichlet prior, rye and Sp, re-
spectively. The generative story for choosing rye is
the following: A vector 77e is drawn from the zero-
mean normal distribution N(Eη, 0), where Eη is
Generation of parameters Ve and wp
for event type e = 1, ... , E do
</bodyText>
<equation confidence="0.968686">
77e — N(Eη, 0)
for all words w do
γew =exp(ηew)/�w,exp(ηew,) [Dir prior]
Ve — Dirichlet(rye) [event lang mod]
</equation>
<bodyText confidence="0.833008">
for participant type p = 1, ... , P do
</bodyText>
<equation confidence="0.961474">
�p — N(Eξ, 0)
for all words w /do
δpw=exp(ξpw)/Ew, exp(ξpw,) [ Dir prior]
wp — Dirichlet(Sp) [ ptcpt lang mod ]
</equation>
<figureCaption confidence="0.8521855">
Figure 2: The modified parameter generation pro-
cedure for Ve and wp to encode word correlations.
</figureCaption>
<bodyText confidence="0.991963428571429">
the covariance matrix encoding the semantic relat-
edness of words (see Section 4.3.2). The vector’s
dimensionality corresponds to size of the vocab-
ulary of event words. Then, the vector is expo-
nentiated and normalized to yield rye.3 The same
procedure is used to choose Sp as shown in Figure
2.
</bodyText>
<subsectionHeader confidence="0.941366">
4.3.2 Defining Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.999988">
We use WordNet to obtain semantic similarity
scores for each pair of words in our vocabulary.
Since we work on limited domains, we define a
subset of WordNet as all synsets that any word in
our vocabulary is a member of, plus the hypernym
sets of all these synsets. We then create a feature
vector for each word f(wi) as follows:
</bodyText>
<equation confidence="0.472866">
�
1 any sense of wi E synset n
0 otherwise
</equation>
<bodyText confidence="0.999811375">
The similarity of two words wi and wj is de-
fined as the dot product f(wi)·f(wj). We use this
similarity to define the covariance matrices Eη and
Eξ. Each component (i, j) stores the similarity
between words wi and wj as defined above. Note
that the matrices are guaranteed to be valid covari-
ance matrices, as they are positive semidefinite by
construction.
</bodyText>
<sectionHeader confidence="0.999744" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.9953535">
Our goal is to infer the set of labelings z of our
corpus of ESDs. A labeling z consists of event
</bodyText>
<footnote confidence="0.4917638">
3In fact, Dirichlet concentration parameters do not need
to sum to one. We experimented with normalizing them to
yield a different constant, thus regulating the influence of the
prior, but have not observed much of improvement from this
extension.
</footnote>
<equation confidence="0.507747">
f(wi)n =
</equation>
<page confidence="0.991711">
53
</page>
<bodyText confidence="0.999987357142857">
types t, participant types u and event ordering π.
Additionally, we induce parameters of our model:
ordering dispersion parameters (ρ) and the lan-
guage model parameters η and ξ. We induce these
variables conditioned on all the observable words
in the data set w. Since direct joint sampling from
the posterior distributions is intractable, we use
Gibbs sampling for approximate inference. Since
we chose conjugate prior distributions over the pa-
rameter distributions, we can “collapse” the Gibbs
sampler by integrating out all parameters (Grif-
fiths and Steyvers, 2004), except for the ones listed
above. The unnormalized posterior can be written
as the following product of terms:
</bodyText>
<equation confidence="0.993689333333333">
YP (z, ρ, η, ξ|w) ∝ DCMe Y DCMp
e p
Y BBMe Y
e p
Y GMMe MNe Y MNp.
e p
</equation>
<bodyText confidence="0.999516333333333">
The terms DCMe and DCMp are Dirichlet com-
pound multinomials associated with event-specific
and participant-specific language models:
</bodyText>
<equation confidence="0.872207285714286">
Γ(P v γe v) Y Γ(Ne v + γe v)
DCMe =
Γ(Pv Ne v + γev) Γ(γe v)
v
Γ(Pv δp v) Y Γ(Np v + δp v)
DCMp = Γ(Pv Np v + δp v) Γ(δpv) ,
v
</equation>
<bodyText confidence="0.999983857142857">
where Nev and Npv is the number of times word
type v is assigned to event e and participant p,
respectively. The terms BBMe and BBMep are
the Beta-Binomial distributions associated with
generating event types and generating participant
types for each event type (i.e. encoding optionality
of events and participants):
</bodyText>
<equation confidence="0.998764">
Γ(N+ e + α+)Γ(N− e + α−)
BBMe ∝
Γ(N+e + N−e + α+ + α−)
Γ(N+ep + β+)Γ(N− + β−)
ep
Γ(N+ep + N−ep + β+ + β−) ,
</equation>
<bodyText confidence="0.997028">
where N+e and N−e is the number of ESDs where
event type is generated and the number of ESD
where it is not generated, respectively. N+ep and
N−ep are analogously defined for participant types
(for each event type e). The term GMMe is as-
sociated with the inversion count distribution for
event type e and has the form
</bodyText>
<equation confidence="0.6250045">
GMMe ∝ GMM0(ρe; Pd N + ve,0ν0 , N + ν0),
0
</equation>
<bodyText confidence="0.999985558139535">
where GMM0 is defined in expression (2) and vde
is the inversion count for event e in ESD d. N is
the cumulative number of event occurrences in the
data set.
Finally, MNe and MNp correspond to the
probability of drawing ηe and ξp from the cor-
responding normal distributions, as discussed in
Section 4.3.1.
Though, at each step of Gibbs sampling, com-
ponents of z could potentially be sampled by
considering the full unnormalized posterior, this
clearly can be made much more efficient by ob-
serving that only a fraction of terms affect the cor-
responding conditional probability. For example,
when sampling an event type for a given event
in a ESD d, only the terms DCMe, BBMep and
BBMe for all e and p are affected. For DCMs it
can be simplified further as only a few word types
are affected. Due to space constraints, we cannot
describe the entire sampling algorithms but it natu-
rally follows from the above equations and is sim-
ilar to the one described in Chen et al. (2009).
For sampling the other parameters of our model,
ranking dispersion parameters ρ and the language
model parameters η and ξ, we use slice sampling
(MacKay, 2002). For each event type e we draw
its dispersion parameter ρe independently from the
slice sampler.
After every nth iteration we resample η and
ξ for all language models to capture the corre-
lations. However, to improve mixing time, we
also resample components ηki and ηli when word
i has changed event membership from type k to
type l. In addition we define classes of closely
related words (heuristically based on the covari-
ance matrix) by classifying words as related when
their similarity exceeds an empirically determined
threshold. We also resample all components ηkj
and ηl j for each word j that related to word i. We
re-normalize ηm and ηn after resampling to up-
date the Dirichlet concentration parameters. The
same procedure is used for participant language
models (parameters ξ).
</bodyText>
<sectionHeader confidence="0.996643" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999951">
In our evaluation, we evaluate the quality of the
event clusters induced by the model and the ex-
tent to which the clusters capture the global event
ordering underlying the script, as well as the bene-
fit of the GMM and the informed prior knowledge.
We start by describing data and evaluation metrics.
</bodyText>
<figure confidence="0.64993">
BBMep
YBBMep ∝ Y
e p
</figure>
<page confidence="0.98895">
54
</page>
<table confidence="0.999584307692308">
Scenario Name OESDs Avg len
OMICS corpus
Cook in microwave 59 5.03
Answer the telephone 55 4.47
Buy from vending machine 32 4.53
Make coffee 38 5.00
R10 corpus
Iron clothes 19 8.79
Make scrambled eggs 20 10.3
Eat in fast food restaurant 15 8.93
Return food (in a restaurant) 15 5.93
Take a shower 21 11.29
Take the bus 19 8.53
</table>
<tableCaption confidence="0.998572">
Table 1: Test scenarios used in experiments (left),
</tableCaption>
<bodyText confidence="0.7877555">
the size of the corresponding corpus (middle), and
the average length of an ESD in events (right).
</bodyText>
<subsectionHeader confidence="0.996248">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.9999969375">
We use the data sets presented in Regneri et al.
(2010) (henceforth R10) for development and test-
ing. The data is comprised of ESDs from two cor-
pora. R10 collected a corpus, consisting of sets of
ESDs for a variety of scenarios, via a web exper-
iment from non-expert annotators. In addition we
use ESDs from the OMICS corpus4 (Kochender-
fer and Gupta, 2003), which consists of instantia-
tions of descriptions of several ‘stories’, but is re-
stricted to indoor activities. The details of our data
are displayed in Table 1. For each event descrip-
tion we extract all noun phrases, as automatically
identified by Regneri et al. (2011), separating par-
ticipant descriptions from action descriptions. We
remove articles and pronouns, and reduce NPs to
their head words.
</bodyText>
<subsectionHeader confidence="0.999936">
6.2 Gold Standard and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9998852">
We follow R10 in evaluating induced event types
and orderings in a binary classification setting.
R10 collected a gold standard by classifying pairs
of event descriptions w.r.t. whether or not they are
paraphrases. Our model classifies two event de-
scriptions as equivalent whenever ze1 = ze2.
Equivalently, R10 classify ordered pairs of
event descriptions as to whether they are presented
in their natural order. Assuming the identity order-
ing as canonical ordering in the Generalized Mal-
lows Model, event types tending to occur earlier
in the script should be assigned lower cluster IDs
than event types occurring later. Thus, whenever
ze1 &lt; ze2, our the model predicts that two event
descriptions occur in their natural order.
</bodyText>
<footnote confidence="0.966701">
4http://csc.media.mit.edu/
</footnote>
<table confidence="0.999974285714286">
Event Paraphrase Evt. Ordering
P R F P R F
Ret. Food 0.92 0.52 0.67 0.87 0.72 0.79
-GMM 0.70 0.30 0.42 0.46 0.44 0.45
-COVAR 0.92 0.52 0.67 0.77 0.67 0.71
Vending 0.76 0.78 0.77 0.90 0.74 0.81
-GMM 0.74 0.39 0.51 0.64 0.47 0.54
-COVAR 0.74 0.87 0.80 0.85 0.73 0.78
Shower 0.68 0.67 0.67 0.85 0.84 0.85
-GMM 0.36 0.17 0.23 0.42 0.38 0.40
-COVAR 0.64 0.44 0.52 0.77 0.73 0.75
Microwave 0.85 0.80 0.82 0.91 0.74 0.82
-GMM 0.88 0.30 0.45 0.67 0.62 0.64
-COVAR 0.89 0.81 0.85 0.92 0.82 0.87
</table>
<tableCaption confidence="0.997935">
Table 2: Comparison of model variants: For each
</tableCaption>
<bodyText confidence="0.979603777777778">
scenario: The full model (top), a version without
the GMM (-GMM), and a version with a uniform
Dirichlet prior over language models (-COVAR).
We evaluate the output of our model against the
described gold standard, using Precision, Recall
and F1 as evaluation metrics, so that our results are
directly comparable to R10. We tune our parame-
ters on a development set of 5 scenarios which are
not used in testing.
</bodyText>
<subsectionHeader confidence="0.714013">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.93649348">
Table 3 presents the results of our two evaluation
tasks. While on the event paraphrase task the R10
system performs slightly better, our model out-
performs the R10 system on the event ordering
task by a substantial margin of 7 points average
F-score. While both systems perform similarly on
the task of event type induction, we induce a joint
model for both objectives. The results show that,
despite the limited amount of data, and the more
complex learning objective, our model succeeds in
inducing event types and ordering constraints.
In order to demonstrate the benefit of the GMM,
we compare the performance of our model to a
variant which excludes this component (-GMM),
cf. Table 2. The results confirm our expectation
that biasing the model towards encouraging a lin-
ear ordering on the event types provides a strong
cue for event cluster inference.
As an example of a clustering learnt by our
model, consider the following event chain:
{get} → {open,take} → {put,place} →
{close} → {set,select,enter,turn} → {start}
→ {wait} → {remove,take,open} →
{push,press,turn}
We display the most frequent words in the clusters
</bodyText>
<page confidence="0.99665">
55
</page>
<table confidence="0.999831571428571">
Scenario Event Paraphrase Task Event Ordering Task
Precision Recall F1 Precision Recall F1
R10 BS R10 BS R10 BS R10 BS R10 BS R10 BS
Coffee 0.50 0.47 0.94 0.58 0.65 0.52 0.70 0.68 0.78 0.57 0.74 0.62
Telephone 0.93 0.92 0.85 0.72 0.89 0.81 0.83 0.92 0.86 0.87 0.84 0.89
Bus 0.65 0.52 0.87 0.43 0.74 0.47 0.80 0.76 0.80 0.76 0.80 0.76
Iron 0.52 0.65 0.94 0.56 0.67 0.60 0.78 0.87 0.72 0.69 0.75 0.77
Scr. Eggs 0.58 0.92 0.86 0.65 0.69 0.76 0.67 0.77 0.64 0.59 0.66 0.67
Vending 0.59 0.76 0.83 0.78 0.69 0.77 0.84 0.90 0.85 0.74 0.84 0.81
Microwave• 0.75 0.85 0.75 0.80 0.75 0.82 0.47 0.91 0.83 0.74 0.60 0.82
Shower• 0.70 0.68 0.88 0.67 0.78 0.67 0.48 0.85 0.82 0.84 0.61 0.85
Fastfood• 0.50 0.74 0.73 0.87 0.59 0.80 0.53 0.97 0.81 0.65 0.64 0.78
Ret. Food• 0.73 0.92 0.68 0.52 0.71 0.67 0.48 0.87 0.75 0.72 0.58 0.79
Average 0.645 0.743 0.833 0.658 0.716 0.689 0.658 0.850 0.786 0.717 0.706 0.776
</table>
<tableCaption confidence="0.998869">
Table 3: Results of our model for the event paraphrase task (left) and event type ordering task (right).
</tableCaption>
<bodyText confidence="0.992802478260869">
Our system (BS) is compared to the system in Regneri et al. (2010) (R10). We were able to obtain the
R10 system from the authors and evaluate on additional scenarios for which no results are reported in
the paper. These additional scenarios are marked with a dot (•).
inferred for the “Microwave” scenario. Clusters
are sorted by event type ID. Note that the word
‘open’ is assigned to two event types in the se-
quence, which is intuitively reasonable. This illus-
trates why assuming a deterministic mapping from
predicates to events (as in Chambers and Jurafsky
(2008)) is limiting for our dataset.
We finally examined the influence of the in-
formed prior component, comparing to a model
variant which uses uniform Dirichlet parameters
(-COVAR; see Table 2). As expected, using an in-
formed prior component leads to improved perfor-
mance on scenario types with fewer training ESDs
available (‘Take a shower’ and ‘Return food’; cf.
Table 1). For scenarios with a larger set of training
documents no reliable benefit from the informed
prior is observable. We did not optimize this com-
ponent, e.g. by testing more sophisticated meth-
ods for construction of the covariance matrix, but
expect to be able to improve its reliability.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999976769230769">
The evaluation shows that our model is able to
create meaningful event type clusters, which re-
semble the underlying event ordering imposed by
the scenario. We achieve an absolute average im-
provement of 7% over a state-of-the-art model. In
contrast to previous approaches to script induc-
tion, our model does not include specifically cus-
tomized components, and is thus flexibly applica-
ble without additional engineering effort.
Our model provides a clean, statistical formula-
tion of the problem of jointly inducing event types
and their ordering. Using a Bayesian model al-
lows for flexible enhancement of the model. One
straightforward next step would be to explore the
influence of participants, and try to jointly infer
them with our current set of latent variables.
Statistical models highly rely on a sufficient
amount of training data in order to be able to
induce latent structures. The limited amount of
training data in our case is a bottleneck for the per-
formance. The model performs best on the two
scenarios with the most training data (‘Telephone’
and ‘Microwave’), which supports this assump-
tion. We showed, however, that our model can be
applied to small data sets through incorporation of
informed prior knowledge without supervision.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999992222222222">
We presented a hierarchical Bayesian model for
joint induction of event clusters and constraints on
their orderings from sets of ESDs. We incorporate
the Generalized Mallows Model over orderings.
The evaluation shows that our model successfully
induces event clusters and ordering constraints.
We compare our joint, statistical model to a
pipeline based model using MSA for event clus-
tering. Our system outperforms the system on the
task of event ordering induction by a substantial
margin, while achieving comparable results in the
event induction task. We could further explicitly
show the benefit of modeling global ESD struc-
ture, using the GMM.
In future work we plan to apply our model to
larger data sets, and to examine the role of par-
ticipants in our model, exploring the potential of
inferring them jointly with our current objectives.
</bodyText>
<page confidence="0.995403">
56
</page>
<sectionHeader confidence="0.998958" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999829666666667">
We thank Michaela Regneri for substantial support
with the script data, and Mirella Lapata for helpful
comments.
</bodyText>
<sectionHeader confidence="0.994321" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903866666667">
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project.
In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on Compu-
tational Linguistics, pages 86–90.
A. Barr and E.A. Feigenbaum. 1986. The hand-
book of artificial intelligence. 1 (1981). The
Handbook of Artificial Intelligence. Addison-
Wesley.
David Blei and John Lafferty. 2006. Correlated
topic models. In Advances in Neural Informa-
tion Processing Systems 18, pages 147–154.
Nathanael Chambers and Dan Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of ACL-08: HLT, pages 789–797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and
their participants. In Proceedings of the 47th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 602–610.
H. Chen, S. R. K. Branavan, R. Barzilay, and D. R.
Karger. 2009. Content modeling using latent
permutations. Journal of Artificial Intelligence
Research, 36(1):129–163.
Christiane Fellbaum, editor. 1998. WordNet: an
electronic lexical database. MIT Press.
M. Fligner and J. Verducci. 1986. Distance based
ranking models. Journal of the Royal Statistical
Society, Series B, 48:359–369.
M. Fligner and J. Verducci. 1990. Posterior prob-
abilities for a consensus ordering. Psychome-
trika, 55:53–63.
T. L. Griffiths and M. Steyvers. 2004. Finding
scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228–
5235.
Alexandre Klementiev, Dan Roth, and Kevin
Small. 2008. Unsupervised rank aggregation
with distance-based models. In Proceedings of
the 25th International Conference on Machine
Learning, pages 472–479.
Mykel J. Kochenderfer and Rakesh Gupta. 2003.
Common sense data acquisition for indoor mo-
bile robots. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence
(AAAI-04), pages 605–610.
D. J. C. MacKay. 2002. Information Theory, Infer-
ence &amp; Learning Algorithms. Cambridge Uni-
versity Press, New York, NY, USA.
C. L. Mallows. 1957. Non-null ranking models.
Biometrika, 44:114–130.
Risto Miikkulainen. 1995. Script-based inference
and memory retrieval in subsymbolic story pro-
cessing. Applied Intelligence, pages 137–163.
Ashutosh Modi, Ivan Titov, and Alexandre Kle-
mentiev. 2012. Unsupervised induction of
frame-semantic representations. In Proceedings
of the NAACL-HLT Workshop on the Induction
of Linguistic Structure, pages 1–7.
Erik T. Mueller. 2004. Understanding script-based
stories using commonsense reasoning. Cogni-
tive Systems Research, 5(4):307–340.
Brendan O’Connor. 2012. Bayesian unsupervised
frame learning from text. Technical report,
Carnegie Mellon University.
Rajat Raina, Andrew Y. Ng, and Daphne Koller.
2006. Constructing informative priors using
transfer learning. In Proceedings of the 23rd In-
ternational Conference on Machine Learning,
pages 713–720.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with
web experiments. In Proceedings of the 48th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 979–988.
Michaela Regneri, Alexander Koller, Josef Rup-
penhofer, and Manfred Pinkal. 2011. Learning
script participants from unlabeled data. In Pro-
ceedings of RANLP 2011, pages 463–470.
Roger C. Schank and Robert P. Abelson. 1975.
Scripts, plans, and knowledge. In Proceedings
of the 4th International Joint Conference on Ar-
tificial Intelligence, IJCAI’75, pages 151–157.
Ivan Titov and Alexandre Klementiev. 2011. A
bayesian model for unsupervised semantic pars-
ing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies, pages
1445–1455.
</reference>
<page confidence="0.999144">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.339147">
<title confidence="0.9438385">A Hierarchical Bayesian Model Unsupervised Induction of Script Knowledge</title>
<author confidence="0.55102">l frermanned ac uk titovuva nl pinkalcoli uni-sb de</author>
<affiliation confidence="0.860722666666667">School of Informatics, University of Edinburgh, United University of Amsterdam, Netherlands of Computational Linguistics, Saarland University, Germany</affiliation>
<abstract confidence="0.999140571428571">Scripts representing common sense knowledge about stereotyped sequences of events have been shown to be a valuable resource for NLP applications. We present a hierarchical Bayesian model for unsupervised learning of script knowledge from crowdsourced descriptions of human activities. Events and constraints on event ordering are induced jointly in one unified framework. We use a statistical model over permutations which captures event ordering constraints in a more flexible way than previous approaches. In order to alleviate the sparsity problem caused by using relatively small datasets, we incorporate in our hierarchical model an informed prior on word distributions. The resulting model substantially outperforms a state-of-the-art method on the event ordering task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="5997" citStr="Baker et al., 1998" startWordPosition="924" endWordPosition="927"> model is generally widely applicable for two reasons. First, the data, collected using crowdsourcing, is comparatively easy and cheap to extend. Secondly, our model is domain independent and can be applied to scenario descriptions from any domain without any modification. Note that parameters were tuned on held-out scenarios, and no scenario-specific tuning was performed. 2 Related Work In the 1970s, scripts were introduced as a way to equip AI systems with world knowledge (Schank and Abelson, 1975; Barr and Feigenbaum, 1986). Task-specific script databases were developed manually. FrameNet (Baker et al., 1998) follows a similar idea, in defining verb frames together with argument types that can fill the verbs’ argument slots. Frames can then be combined into “scenario frames”. Manual composition of such databases, is arguably expensive and does not scale well. This paper follows a series of more recent work which aims to infer script knowledge automatically from data. Chambers and Jurafsky (2008) present a system which learns narrative chains from newswire texts. Relevant phrases are identified based on shared protagonists. The phrases are clustered into equivalence classes and temporally ordered u</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Barr</author>
<author>E A Feigenbaum</author>
</authors>
<title>The handbook of artificial intelligence.</title>
<date>1986</date>
<booktitle>The Handbook of Artificial Intelligence.</booktitle>
<volume>1</volume>
<publisher>AddisonWesley.</publisher>
<contexts>
<context position="5910" citStr="Barr and Feigenbaum, 1986" startWordPosition="913" endWordPosition="916">s defined above enables the application of our model to small data sets, we emphasize that the model is generally widely applicable for two reasons. First, the data, collected using crowdsourcing, is comparatively easy and cheap to extend. Secondly, our model is domain independent and can be applied to scenario descriptions from any domain without any modification. Note that parameters were tuned on held-out scenarios, and no scenario-specific tuning was performed. 2 Related Work In the 1970s, scripts were introduced as a way to equip AI systems with world knowledge (Schank and Abelson, 1975; Barr and Feigenbaum, 1986). Task-specific script databases were developed manually. FrameNet (Baker et al., 1998) follows a similar idea, in defining verb frames together with argument types that can fill the verbs’ argument slots. Frames can then be combined into “scenario frames”. Manual composition of such databases, is arguably expensive and does not scale well. This paper follows a series of more recent work which aims to infer script knowledge automatically from data. Chambers and Jurafsky (2008) present a system which learns narrative chains from newswire texts. Relevant phrases are identified based on shared pr</context>
</contexts>
<marker>Barr, Feigenbaum, 1986</marker>
<rawString>A. Barr and E.A. Feigenbaum. 1986. The handbook of artificial intelligence. 1 (1981). The Handbook of Artificial Intelligence. AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18,</booktitle>
<pages>147--154</pages>
<contexts>
<context position="10957" citStr="Blei and Lafferty, 2006" startWordPosition="1714" endWordPosition="1717">he problem of small data sets is incorporating informed prior knowledge. Raina et al. (2006) encode word correlations in a variance-covariance matrix of a multivariate normal distribution (MVN), and sample prior parameter vectors from it, thus introducing dependencies among the parameters. They induce the covariances from supervised learning tasks in the transfer learning set-up. We use the same idea, but obtain word covariances from WordNet relations. In a slightly different setting, covariance matrices of MVNs have been used in topic models to induce correlation between topics in documents (Blei and Lafferty, 2006). 3 Problem Formulation Our input consists of a corpus of scenario-specific ESDs, and our goal is to label each event description in an ESD with one event type e. We specify the number of possible event types E a priori as a number exceeding the number of event types in all the scripts considered. The model will select an effective subset of those types. Assume a scenario-specific corpus c, consisting of D ESDs, c = {d1, ..., dD}. Each ESD di consists of Nd event descriptions di = {di,1, ..., di,Ni}. Boundaries between descriptions of single events are marked in the data. For each event descri</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David Blei and John Lafferty. 2006. Correlated topic models. In Advances in Neural Information Processing Systems 18, pages 147–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>789--797</pages>
<contexts>
<context position="1974" citStr="Chambers and Jurafsky, 2008" startWordPosition="276" endWordPosition="279">r lifetime, it constitutes a bottleneck for many NLP systems. Effective question answering and summarization are impossible without a form of story understanding, which in turn has been shown to benefit from access to databases of script knowledge (Mueller, 2004; Miikkulainen, 1995). Knowledge about the typical ordering of events can further help assessing document coherence and generating coherent text. Here, we present a general method for acquiring data bases of script knowledge. Our work may be regarded as complementary to existing work on learning script knowledge from natural text (cf. (Chambers and Jurafsky, 2008)), as not all types of scripts are elaborated in natural text – being left implicit because of assumed readers’ world knowledge. Our model, operating on data obtained in a cheap way by crowdsourcing, is applicable to any kind of script and can fill this gap. We follow work in inducing script knowledge from explicit instantiations of scripts, socalled event sequence descriptions (ESDs) (Regneri et al., 2010). Our data consists of sets of ESDs, each set describing a well-known situation we will call scenario (e.g., “washing laundry”). An ESD consists of a sequence of events, each describing an a</context>
<context position="6391" citStr="Chambers and Jurafsky (2008)" startWordPosition="987" endWordPosition="990"> Work In the 1970s, scripts were introduced as a way to equip AI systems with world knowledge (Schank and Abelson, 1975; Barr and Feigenbaum, 1986). Task-specific script databases were developed manually. FrameNet (Baker et al., 1998) follows a similar idea, in defining verb frames together with argument types that can fill the verbs’ argument slots. Frames can then be combined into “scenario frames”. Manual composition of such databases, is arguably expensive and does not scale well. This paper follows a series of more recent work which aims to infer script knowledge automatically from data. Chambers and Jurafsky (2008) present a system which learns narrative chains from newswire texts. Relevant phrases are identified based on shared protagonists. The phrases are clustered into equivalence classes and temporally ordered using a pipeline of methods. We work with explicit event sequence descriptions of a specific scenario, arguing that large-scale common sense knowledge is hard to acquire from natural text, since it is often left implicit. Regneri et al. (2010) induce script knowledge from explicit ESDs using a graph-based method. Event types and ordering constraints are induced by aligning descriptions of equ</context>
<context position="31063" citStr="Chambers and Jurafsky (2008)" startWordPosition="5235" endWordPosition="5238">paraphrase task (left) and event type ordering task (right). Our system (BS) is compared to the system in Regneri et al. (2010) (R10). We were able to obtain the R10 system from the authors and evaluate on additional scenarios for which no results are reported in the paper. These additional scenarios are marked with a dot (•). inferred for the “Microwave” scenario. Clusters are sorted by event type ID. Note that the word ‘open’ is assigned to two event types in the sequence, which is intuitively reasonable. This illustrates why assuming a deterministic mapping from predicates to events (as in Chambers and Jurafsky (2008)) is limiting for our dataset. We finally examined the influence of the informed prior component, comparing to a model variant which uses uniform Dirichlet parameters (-COVAR; see Table 2). As expected, using an informed prior component leads to improved performance on scenario types with fewer training ESDs available (‘Take a shower’ and ‘Return food’; cf. Table 1). For scenarios with a larger set of training documents no reliable benefit from the informed prior is observable. We did not optimize this component, e.g. by testing more sophisticated methods for construction of the covariance mat</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of ACL-08: HLT, pages 789–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>602--610</pages>
<contexts>
<context position="9940" citStr="Chambers and Jurafsky (2009)" startWordPosition="1555" endWordPosition="1558">v models. We are not aware of previous work on modeling events with GMMs. Conversely, MMs were considered in the very recent work of Cheung et al. (2013) in the context of script induction from news corpora where the Markovian assumption is much more natural. There exists a body of work for learning participant types involved in scripts. Regneri et al. (2011) extend their work by inducing participant types on the basis of the TSG, using structural information about participant mentions in the TSG as well as WordNet similarity, which they then combine into an Integer Linear Program. Similarly, Chambers and Jurafsky (2009) extend their work on narrative chains, presenting a system with which they jointly learn event types and semantic roles of the participants involved, but do not consider event orderings. We include participant types as a latent feature in our model, assuming that participant mentions in an event description are a predictive feature for the corresponding event type. One way of alleviating the problem of small data sets is incorporating informed prior knowledge. Raina et al. (2006) encode word correlations in a variance-covariance matrix of a multivariate normal distribution (MVN), and sample p</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics, pages 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
<author>S R K Branavan</author>
<author>R Barzilay</author>
<author>D R Karger</author>
</authors>
<title>Content modeling using latent permutations.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="8449" citStr="Chen et al. (2009)" startWordPosition="1308" endWordPosition="1311"> temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts that MSA cannot capture. The GMM has been successfully applied to modeling ordering constraints in NLP tasks. Chen et al. (2009) augment classical topic models with a GMM, under the assumption that topics in structured domains (e.g., biographies in Wikipedia) tend to follow an underlying canonical ordering, an assumption which matches well our data (the annotators were asked to follow the temporal or50 der of events in their descriptions (Regneri et al., 2010)). Chen et al. show that for these domains their approach significantly outperforms Markovian modeling of topics. This is expected as Markov models (MMs) are not very appropriate for representing linear structure with potentially missing topics (e.g., they cannot </context>
<context position="24079" citStr="Chen et al. (2009)" startWordPosition="4031" endWordPosition="4034"> of z could potentially be sampled by considering the full unnormalized posterior, this clearly can be made much more efficient by observing that only a fraction of terms affect the corresponding conditional probability. For example, when sampling an event type for a given event in a ESD d, only the terms DCMe, BBMep and BBMe for all e and p are affected. For DCMs it can be simplified further as only a few word types are affected. Due to space constraints, we cannot describe the entire sampling algorithms but it naturally follows from the above equations and is similar to the one described in Chen et al. (2009). For sampling the other parameters of our model, ranking dispersion parameters ρ and the language model parameters η and ξ, we use slice sampling (MacKay, 2002). For each event type e we draw its dispersion parameter ρe independently from the slice sampler. After every nth iteration we resample η and ξ for all language models to capture the correlations. However, to improve mixing time, we also resample components ηki and ηli when word i has changed event membership from type k to type l. In addition we define classes of closely related words (heuristically based on the covariance matrix) by </context>
</contexts>
<marker>Chen, Branavan, Barzilay, Karger, 2009</marker>
<rawString>H. Chen, S. R. K. Branavan, R. Barzilay, and D. R. Karger. 2009. Content modeling using latent permutations. Journal of Artificial Intelligence Research, 36(1):129–163.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fligner</author>
<author>J Verducci</author>
</authors>
<title>Distance based ranking models.</title>
<date>1986</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>48--359</pages>
<contexts>
<context position="8007" citStr="Fligner and Verducci (1986)" startWordPosition="1237" endWordPosition="1240">ure and treat event learning and learning ordering constraints as separate problems. In contrast, we propose to learn both tasks jointly. We incorporate both tasks in a hierarchical Bayesian model, thus using one unified framework. A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts that MSA cannot capture. The GMM has been successfully applied to modeling ordering constraints in NLP tasks. Chen et al. (2009) augment classical topic models with a GMM, under the assumption that topics in structured domains (e.g., biographies in Wikipedia) tend to follow an underlyi</context>
<context position="13938" citStr="Fligner and Verducci, 1986" startWordPosition="2225" endWordPosition="2228">eded to bring 7r in the order o,, 51 is arguably the most common choice. The probability of an observed ordering 7r is defined as e P(7r|ρ, Q) = where 0(ρ) is a normalization factor. The distribution is centered around the canonical ordering (as d(Q, Q) = 0), and the probability decreases exponentially with an increasing distance. For our purposes, without loss of generality, we can assume that Q is the identity permutation, that is Q = [1, ... , n], where n is the number of items. The Mallows model has been generalized to take as a parameter a vector of item-specific dispersion parameters p (Fligner and Verducci, 1986). In order to introduce this extension, we first need to reformulate Kendall’s tau in a way that captures item-specific distance. An ordering 7r of n items can be equivalently represented by a vector of inversion counts v of length n − 1, where each component vi equals the number of items j &gt; i that occur before item i in 7r. For example, for an observed ordering 7r = [2,1,0] the inversion vector v = (2, 1).1 Then the generalized Mallows model (GMM) is defined as GMM(7r|p) oC � e−ρi vi. i The GMM can be factorized into item-specific components, which allows for efficient inference: GMMi(vi|ρi)</context>
</contexts>
<marker>Fligner, Verducci, 1986</marker>
<rawString>M. Fligner and J. Verducci. 1986. Distance based ranking models. Journal of the Royal Statistical Society, Series B, 48:359–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fligner</author>
<author>J Verducci</author>
</authors>
<title>Posterior probabilities for a consensus ordering.</title>
<date>1990</date>
<tech>Psychometrika,</tech>
<pages>55--53</pages>
<contexts>
<context position="14899" citStr="Fligner and Verducci, 1990" startWordPosition="2391" endWordPosition="2394">ample, for an observed ordering 7r = [2,1,0] the inversion vector v = (2, 1).1 Then the generalized Mallows model (GMM) is defined as GMM(7r|p) oC � e−ρi vi. i The GMM can be factorized into item-specific components, which allows for efficient inference: GMMi(vi|ρi) oC e−ρi vi. (1) Intuitively, we will be able to induce event typespecific penalty parameters, and will thus be able to model individual degrees of temporal flexibility among the event types. Since the GMM is member of the exponential family, a conjugate prior can be defined, which allows for efficient learning of the parameters p (Fligner and Verducci, 1990). Like the GMM, its prior distribution GMM0 can be factorized into independent components for each item i: GMM0(ρi|vi,0,ν0) oC e−ρivi,0−log(ψi(ρi))ν0. (2) The parameters vi,0 and ν0 represent our prior beliefs about flexibility for each item i, and the strength of these beliefs, respectively. 1Trivially, the inversion count for the last element in the canonical ordering is always 0. 4.2 The Generative Story Our model encodes two fundamental assumptions, based on characteristics observed in the data: (1) We assume that each event type can occur at most once per ESD; (2) Each participant type is</context>
</contexts>
<marker>Fligner, Verducci, 1990</marker>
<rawString>M. Fligner and J. Verducci. 1990. Posterior probabilities for a consensus ordering. Psychometrika, 55:53–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>5235</pages>
<contexts>
<context position="21874" citStr="Griffiths and Steyvers, 2004" startWordPosition="3606" endWordPosition="3610">bserved much of improvement from this extension. f(wi)n = 53 types t, participant types u and event ordering π. Additionally, we induce parameters of our model: ordering dispersion parameters (ρ) and the language model parameters η and ξ. We induce these variables conditioned on all the observable words in the data set w. Since direct joint sampling from the posterior distributions is intractable, we use Gibbs sampling for approximate inference. Since we chose conjugate prior distributions over the parameter distributions, we can “collapse” the Gibbs sampler by integrating out all parameters (Griffiths and Steyvers, 2004), except for the ones listed above. The unnormalized posterior can be written as the following product of terms: YP (z, ρ, η, ξ|w) ∝ DCMe Y DCMp e p Y BBMe Y e p Y GMMe MNe Y MNp. e p The terms DCMe and DCMp are Dirichlet compound multinomials associated with event-specific and participant-specific language models: Γ(P v γe v) Y Γ(Ne v + γe v) DCMe = Γ(Pv Ne v + γev) Γ(γe v) v Γ(Pv δp v) Y Γ(Np v + δp v) DCMp = Γ(Pv Np v + δp v) Γ(δpv) , v where Nev and Npv is the number of times word type v is assigned to event e and participant p, respectively. The terms BBMe and BBMep are the Beta-Binomial </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228– 5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
<author>Kevin Small</author>
</authors>
<title>Unsupervised rank aggregation with distance-based models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>472--479</pages>
<contexts>
<context position="8033" citStr="Klementiev et al. (2008)" startWordPosition="1241" endWordPosition="1244">and learning ordering constraints as separate problems. In contrast, we propose to learn both tasks jointly. We incorporate both tasks in a hierarchical Bayesian model, thus using one unified framework. A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts that MSA cannot capture. The GMM has been successfully applied to modeling ordering constraints in NLP tasks. Chen et al. (2009) augment classical topic models with a GMM, under the assumption that topics in structured domains (e.g., biographies in Wikipedia) tend to follow an underlying canonical ordering, an </context>
</contexts>
<marker>Klementiev, Roth, Small, 2008</marker>
<rawString>Alexandre Klementiev, Dan Roth, and Kevin Small. 2008. Unsupervised rank aggregation with distance-based models. In Proceedings of the 25th International Conference on Machine Learning, pages 472–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mykel J Kochenderfer</author>
<author>Rakesh Gupta</author>
</authors>
<title>Common sense data acquisition for indoor mobile robots.</title>
<date>2003</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04),</booktitle>
<pages>605--610</pages>
<contexts>
<context position="26207" citStr="Kochenderfer and Gupta, 2003" startWordPosition="4402" endWordPosition="4406">in fast food restaurant 15 8.93 Return food (in a restaurant) 15 5.93 Take a shower 21 11.29 Take the bus 19 8.53 Table 1: Test scenarios used in experiments (left), the size of the corresponding corpus (middle), and the average length of an ESD in events (right). 6.1 Data We use the data sets presented in Regneri et al. (2010) (henceforth R10) for development and testing. The data is comprised of ESDs from two corpora. R10 collected a corpus, consisting of sets of ESDs for a variety of scenarios, via a web experiment from non-expert annotators. In addition we use ESDs from the OMICS corpus4 (Kochenderfer and Gupta, 2003), which consists of instantiations of descriptions of several ‘stories’, but is restricted to indoor activities. The details of our data are displayed in Table 1. For each event description we extract all noun phrases, as automatically identified by Regneri et al. (2011), separating participant descriptions from action descriptions. We remove articles and pronouns, and reduce NPs to their head words. 6.2 Gold Standard and Evaluation Metrics We follow R10 in evaluating induced event types and orderings in a binary classification setting. R10 collected a gold standard by classifying pairs of eve</context>
</contexts>
<marker>Kochenderfer, Gupta, 2003</marker>
<rawString>Mykel J. Kochenderfer and Rakesh Gupta. 2003. Common sense data acquisition for indoor mobile robots. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04), pages 605–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J C MacKay</author>
</authors>
<title>Information Theory, Inference &amp; Learning Algorithms.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="24240" citStr="MacKay, 2002" startWordPosition="4059" endWordPosition="4060">erms affect the corresponding conditional probability. For example, when sampling an event type for a given event in a ESD d, only the terms DCMe, BBMep and BBMe for all e and p are affected. For DCMs it can be simplified further as only a few word types are affected. Due to space constraints, we cannot describe the entire sampling algorithms but it naturally follows from the above equations and is similar to the one described in Chen et al. (2009). For sampling the other parameters of our model, ranking dispersion parameters ρ and the language model parameters η and ξ, we use slice sampling (MacKay, 2002). For each event type e we draw its dispersion parameter ρe independently from the slice sampler. After every nth iteration we resample η and ξ for all language models to capture the correlations. However, to improve mixing time, we also resample components ηki and ηli when word i has changed event membership from type k to type l. In addition we define classes of closely related words (heuristically based on the covariance matrix) by classifying words as related when their similarity exceeds an empirically determined threshold. We also resample all components ηkj and ηl j for each word j that</context>
</contexts>
<marker>MacKay, 2002</marker>
<rawString>D. J. C. MacKay. 2002. Information Theory, Inference &amp; Learning Algorithms. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Mallows</author>
</authors>
<title>Non-null ranking models.</title>
<date>1957</date>
<journal>Biometrika,</journal>
<pages>44--114</pages>
<contexts>
<context position="7978" citStr="Mallows (1957)" startWordPosition="1235" endWordPosition="1236">peline architecture and treat event learning and learning ordering constraints as separate problems. In contrast, we propose to learn both tasks jointly. We incorporate both tasks in a hierarchical Bayesian model, thus using one unified framework. A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts that MSA cannot capture. The GMM has been successfully applied to modeling ordering constraints in NLP tasks. Chen et al. (2009) augment classical topic models with a GMM, under the assumption that topics in structured domains (e.g., biographies in Wikipedi</context>
<context position="12981" citStr="Mallows, 1957" startWordPosition="2059" endWordPosition="2060">follow this ordering, but allowing for some flexibility. The event labeling sequence zdi of an entire ESD should reflect this canonical ordering. This allows us to use global structural patterns of ESDs in the event type assignments, and thus introducing dependence between event types through their position in the sequence. 4 The Model Before we describe our model, we briefly explain the Generalized Mallows Model (GMM) which we use to encode a preference for linear ordering of events in a script. 4.1 The (Generalized) Mallows Model The Mallows Model (MM) is a statistical model over orderings (Mallows, 1957). It takes two parameters o,, the canonical ordering, and p &gt; 0, a dispersion parameter. The dispersion parameter is a penalty for the divergence d(7r, o,) of an observed ordering 7r from the canonical ordering o,. The divergence can be any distance metric but Kendall’s tau distance (“bubble-sort” distance), a number of swaps needed to bring 7r in the order o,, 51 is arguably the most common choice. The probability of an observed ordering 7r is defined as e P(7r|ρ, Q) = where 0(ρ) is a normalization factor. The distribution is centered around the canonical ordering (as d(Q, Q) = 0), and the pr</context>
</contexts>
<marker>Mallows, 1957</marker>
<rawString>C. L. Mallows. 1957. Non-null ranking models. Biometrika, 44:114–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risto Miikkulainen</author>
</authors>
<title>Script-based inference and memory retrieval in subsymbolic story processing. Applied Intelligence,</title>
<date>1995</date>
<pages>137--163</pages>
<contexts>
<context position="1629" citStr="Miikkulainen, 1995" startWordPosition="225" endWordPosition="226">nformed prior on word distributions. The resulting model substantially outperforms a state-of-the-art method on the event ordering task. 1 Introduction A script is a “predetermined, stereotyped sequence of actions that define a well-known situation” (Schank and Abelson, 1975). While humans acquire such common-sense knowledge over their lifetime, it constitutes a bottleneck for many NLP systems. Effective question answering and summarization are impossible without a form of story understanding, which in turn has been shown to benefit from access to databases of script knowledge (Mueller, 2004; Miikkulainen, 1995). Knowledge about the typical ordering of events can further help assessing document coherence and generating coherent text. Here, we present a general method for acquiring data bases of script knowledge. Our work may be regarded as complementary to existing work on learning script knowledge from natural text (cf. (Chambers and Jurafsky, 2008)), as not all types of scripts are elaborated in natural text – being left implicit because of assumed readers’ world knowledge. Our model, operating on data obtained in a cheap way by crowdsourcing, is applicable to any kind of script and can fill this g</context>
</contexts>
<marker>Miikkulainen, 1995</marker>
<rawString>Risto Miikkulainen. 1995. Script-based inference and memory retrieval in subsymbolic story processing. Applied Intelligence, pages 137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashutosh Modi</author>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Unsupervised induction of frame-semantic representations.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="7741" citStr="Modi et al., 2012" startWordPosition="1200" endWordPosition="1203">f the scenario is computed, using Multiple Sequence Alignment (MSA). Our work follows the work of Regneri et al. (2010), in that we use the same data and aim to focus on the same task. However, the two approaches described above employ a pipeline architecture and treat event learning and learning ordering constraints as separate problems. In contrast, we propose to learn both tasks jointly. We incorporate both tasks in a hierarchical Bayesian model, thus using one unified framework. A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts that MSA cannot capt</context>
</contexts>
<marker>Modi, Titov, Klementiev, 2012</marker>
<rawString>Ashutosh Modi, Ivan Titov, and Alexandre Klementiev. 2012. Unsupervised induction of frame-semantic representations. In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik T Mueller</author>
</authors>
<title>Understanding script-based stories using commonsense reasoning.</title>
<date>2004</date>
<journal>Cognitive Systems Research,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="1608" citStr="Mueller, 2004" startWordPosition="223" endWordPosition="224">ical model an informed prior on word distributions. The resulting model substantially outperforms a state-of-the-art method on the event ordering task. 1 Introduction A script is a “predetermined, stereotyped sequence of actions that define a well-known situation” (Schank and Abelson, 1975). While humans acquire such common-sense knowledge over their lifetime, it constitutes a bottleneck for many NLP systems. Effective question answering and summarization are impossible without a form of story understanding, which in turn has been shown to benefit from access to databases of script knowledge (Mueller, 2004; Miikkulainen, 1995). Knowledge about the typical ordering of events can further help assessing document coherence and generating coherent text. Here, we present a general method for acquiring data bases of script knowledge. Our work may be regarded as complementary to existing work on learning script knowledge from natural text (cf. (Chambers and Jurafsky, 2008)), as not all types of scripts are elaborated in natural text – being left implicit because of assumed readers’ world knowledge. Our model, operating on data obtained in a cheap way by crowdsourcing, is applicable to any kind of scrip</context>
</contexts>
<marker>Mueller, 2004</marker>
<rawString>Erik T. Mueller. 2004. Understanding script-based stories using commonsense reasoning. Cognitive Systems Research, 5(4):307–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
</authors>
<title>Bayesian unsupervised frame learning from text.</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<marker>O’Connor, 2012</marker>
<rawString>Brendan O’Connor. 2012. Bayesian unsupervised frame learning from text. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Daphne Koller</author>
</authors>
<title>Constructing informative priors using transfer learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning,</booktitle>
<pages>713--720</pages>
<contexts>
<context position="10425" citStr="Raina et al. (2006)" startWordPosition="1635" endWordPosition="1638">the TSG as well as WordNet similarity, which they then combine into an Integer Linear Program. Similarly, Chambers and Jurafsky (2009) extend their work on narrative chains, presenting a system with which they jointly learn event types and semantic roles of the participants involved, but do not consider event orderings. We include participant types as a latent feature in our model, assuming that participant mentions in an event description are a predictive feature for the corresponding event type. One way of alleviating the problem of small data sets is incorporating informed prior knowledge. Raina et al. (2006) encode word correlations in a variance-covariance matrix of a multivariate normal distribution (MVN), and sample prior parameter vectors from it, thus introducing dependencies among the parameters. They induce the covariances from supervised learning tasks in the transfer learning set-up. We use the same idea, but obtain word covariances from WordNet relations. In a slightly different setting, covariance matrices of MVNs have been used in topic models to induce correlation between topics in documents (Blei and Lafferty, 2006). 3 Problem Formulation Our input consists of a corpus of scenario-s</context>
</contexts>
<marker>Raina, Ng, Koller, 2006</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Daphne Koller. 2006. Constructing informative priors using transfer learning. In Proceedings of the 23rd International Conference on Machine Learning, pages 713–720.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Alexander Koller</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning script knowledge with web experiments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>979--988</pages>
<contexts>
<context position="2384" citStr="Regneri et al., 2010" startWordPosition="345" endWordPosition="349">present a general method for acquiring data bases of script knowledge. Our work may be regarded as complementary to existing work on learning script knowledge from natural text (cf. (Chambers and Jurafsky, 2008)), as not all types of scripts are elaborated in natural text – being left implicit because of assumed readers’ world knowledge. Our model, operating on data obtained in a cheap way by crowdsourcing, is applicable to any kind of script and can fill this gap. We follow work in inducing script knowledge from explicit instantiations of scripts, socalled event sequence descriptions (ESDs) (Regneri et al., 2010). Our data consists of sets of ESDs, each set describing a well-known situation we will call scenario (e.g., “washing laundry”). An ESD consists of a sequence of events, each describing an action defining part of the scenario (e.g., “place the laundry in the washing machine”). We refer to descriptions of the same event across ESDs as event types. We refer to entities involved in a scenario as participants (e.g., a “washing machine” or a “detergent”), and to sets of participant descriptions describing the same entity as participant types. For each type of scenario, our model clusters descriptio</context>
<context position="4690" citStr="Regneri et al. (2010)" startWordPosition="716" endWordPosition="719">nal Linguistics and Verducci (1986)). With the GMM we can flexibly model apparent characteristics of scripts, such as event type-specific temporal flexibility. Assuming that types of participants provide a strong cue for the type of event they are observed in, we use participant types as a latent variable in our model. Finally, by modeling event type occurrence using Binomial distributions, we can model event optionality, a characteristic of scripts that previous approaches did not capture. We evaluate our model on a data set of ESDs collected via web experiments from non-expert annotators by Regneri et al. (2010) and compare our model against their approach. Our model achieves an absolute average improvement of 7% over the model of Regneri et al. on the task of event ordering. For our unsupervised Bayesian model the limited size of this training set constitutes an additional challenge. In order to alleviate this problem, we use an informed prior on the word distributions. Instead of using Dirichlet priors which do not encode a-priori correlations between words, we incorporate a logistic normal distribution with the covariance matrix derived from WordNet. While we will show that prior knowledge as defi</context>
<context position="6839" citStr="Regneri et al. (2010)" startWordPosition="1058" endWordPosition="1061">expensive and does not scale well. This paper follows a series of more recent work which aims to infer script knowledge automatically from data. Chambers and Jurafsky (2008) present a system which learns narrative chains from newswire texts. Relevant phrases are identified based on shared protagonists. The phrases are clustered into equivalence classes and temporally ordered using a pipeline of methods. We work with explicit event sequence descriptions of a specific scenario, arguing that large-scale common sense knowledge is hard to acquire from natural text, since it is often left implicit. Regneri et al. (2010) induce script knowledge from explicit ESDs using a graph-based method. Event types and ordering constraints are induced by aligning descriptions of equivalent events using WordNet-based semantic similarity. On this basis an abstract graph-representation (Temporal Script Graph; TSG) of the scenario is computed, using Multiple Sequence Alignment (MSA). Our work follows the work of Regneri et al. (2010), in that we use the same data and aim to focus on the same task. However, the two approaches described above employ a pipeline architecture and treat event learning and learning ordering constrai</context>
<context position="8785" citStr="Regneri et al., 2010" startWordPosition="1362" endWordPosition="1365">rom the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts that MSA cannot capture. The GMM has been successfully applied to modeling ordering constraints in NLP tasks. Chen et al. (2009) augment classical topic models with a GMM, under the assumption that topics in structured domains (e.g., biographies in Wikipedia) tend to follow an underlying canonical ordering, an assumption which matches well our data (the annotators were asked to follow the temporal or50 der of events in their descriptions (Regneri et al., 2010)). Chen et al. show that for these domains their approach significantly outperforms Markovian modeling of topics. This is expected as Markov models (MMs) are not very appropriate for representing linear structure with potentially missing topics (e.g., they cannot encode that every topic is assigned to at most one continuous fragment of text). Also GMMs are preferable for smaller collections such as ours, as the parameter number is linear in the number of topics (i.e., for us, event types) rather than quadratic as in Markov models. We are not aware of previous work on modeling events with GMMs.</context>
<context position="25907" citStr="Regneri et al. (2010)" startWordPosition="4350" endWordPosition="4353">e. We start by describing data and evaluation metrics. BBMep YBBMep ∝ Y e p 54 Scenario Name OESDs Avg len OMICS corpus Cook in microwave 59 5.03 Answer the telephone 55 4.47 Buy from vending machine 32 4.53 Make coffee 38 5.00 R10 corpus Iron clothes 19 8.79 Make scrambled eggs 20 10.3 Eat in fast food restaurant 15 8.93 Return food (in a restaurant) 15 5.93 Take a shower 21 11.29 Take the bus 19 8.53 Table 1: Test scenarios used in experiments (left), the size of the corresponding corpus (middle), and the average length of an ESD in events (right). 6.1 Data We use the data sets presented in Regneri et al. (2010) (henceforth R10) for development and testing. The data is comprised of ESDs from two corpora. R10 collected a corpus, consisting of sets of ESDs for a variety of scenarios, via a web experiment from non-expert annotators. In addition we use ESDs from the OMICS corpus4 (Kochenderfer and Gupta, 2003), which consists of instantiations of descriptions of several ‘stories’, but is restricted to indoor activities. The details of our data are displayed in Table 1. For each event description we extract all noun phrases, as automatically identified by Regneri et al. (2011), separating participant desc</context>
<context position="30562" citStr="Regneri et al. (2010)" startWordPosition="5151" endWordPosition="5154">7 0.64 0.59 0.66 0.67 Vending 0.59 0.76 0.83 0.78 0.69 0.77 0.84 0.90 0.85 0.74 0.84 0.81 Microwave• 0.75 0.85 0.75 0.80 0.75 0.82 0.47 0.91 0.83 0.74 0.60 0.82 Shower• 0.70 0.68 0.88 0.67 0.78 0.67 0.48 0.85 0.82 0.84 0.61 0.85 Fastfood• 0.50 0.74 0.73 0.87 0.59 0.80 0.53 0.97 0.81 0.65 0.64 0.78 Ret. Food• 0.73 0.92 0.68 0.52 0.71 0.67 0.48 0.87 0.75 0.72 0.58 0.79 Average 0.645 0.743 0.833 0.658 0.716 0.689 0.658 0.850 0.786 0.717 0.706 0.776 Table 3: Results of our model for the event paraphrase task (left) and event type ordering task (right). Our system (BS) is compared to the system in Regneri et al. (2010) (R10). We were able to obtain the R10 system from the authors and evaluate on additional scenarios for which no results are reported in the paper. These additional scenarios are marked with a dot (•). inferred for the “Microwave” scenario. Clusters are sorted by event type ID. Note that the word ‘open’ is assigned to two event types in the sequence, which is intuitively reasonable. This illustrates why assuming a deterministic mapping from predicates to events (as in Chambers and Jurafsky (2008)) is limiting for our dataset. We finally examined the influence of the informed prior component, c</context>
</contexts>
<marker>Regneri, Koller, Pinkal, 2010</marker>
<rawString>Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning script knowledge with web experiments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 979–988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Alexander Koller</author>
<author>Josef Ruppenhofer</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning script participants from unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP 2011,</booktitle>
<pages>463--470</pages>
<contexts>
<context position="9673" citStr="Regneri et al. (2011)" startWordPosition="1512" endWordPosition="1515">ode that every topic is assigned to at most one continuous fragment of text). Also GMMs are preferable for smaller collections such as ours, as the parameter number is linear in the number of topics (i.e., for us, event types) rather than quadratic as in Markov models. We are not aware of previous work on modeling events with GMMs. Conversely, MMs were considered in the very recent work of Cheung et al. (2013) in the context of script induction from news corpora where the Markovian assumption is much more natural. There exists a body of work for learning participant types involved in scripts. Regneri et al. (2011) extend their work by inducing participant types on the basis of the TSG, using structural information about participant mentions in the TSG as well as WordNet similarity, which they then combine into an Integer Linear Program. Similarly, Chambers and Jurafsky (2009) extend their work on narrative chains, presenting a system with which they jointly learn event types and semantic roles of the participants involved, but do not consider event orderings. We include participant types as a latent feature in our model, assuming that participant mentions in an event description are a predictive featur</context>
<context position="11753" citStr="Regneri et al. (2011)" startWordPosition="1852" endWordPosition="1855">he number of possible event types E a priori as a number exceeding the number of event types in all the scripts considered. The model will select an effective subset of those types. Assume a scenario-specific corpus c, consisting of D ESDs, c = {d1, ..., dD}. Each ESD di consists of Nd event descriptions di = {di,1, ..., di,Ni}. Boundaries between descriptions of single events are marked in the data. For each event description di,n a bag of participant descriptions is extracted. Each participant description corresponds to one noun phrase as identified automatically by a dependency parser (cf. Regneri et al. (2011)). We also associate participant types with participant descriptions, these types are latent and induced at the inference stage. Given such a corpus of ESDs, our model assigns each event description di,n in an ESD di one event type zdi,n = e, where e E {1, ..., E}. Assuming that all ESDs are generated from the same underlying set of event types, our objective is to assign the same event type to equivalent event descriptions across all ESDs in the corpus. We furthermore assume that there exists a canonical temporal ordering of event types for each scenario type, and that events in observed scen</context>
<context position="26478" citStr="Regneri et al. (2011)" startWordPosition="4448" endWordPosition="4451">e the data sets presented in Regneri et al. (2010) (henceforth R10) for development and testing. The data is comprised of ESDs from two corpora. R10 collected a corpus, consisting of sets of ESDs for a variety of scenarios, via a web experiment from non-expert annotators. In addition we use ESDs from the OMICS corpus4 (Kochenderfer and Gupta, 2003), which consists of instantiations of descriptions of several ‘stories’, but is restricted to indoor activities. The details of our data are displayed in Table 1. For each event description we extract all noun phrases, as automatically identified by Regneri et al. (2011), separating participant descriptions from action descriptions. We remove articles and pronouns, and reduce NPs to their head words. 6.2 Gold Standard and Evaluation Metrics We follow R10 in evaluating induced event types and orderings in a binary classification setting. R10 collected a gold standard by classifying pairs of event descriptions w.r.t. whether or not they are paraphrases. Our model classifies two event descriptions as equivalent whenever ze1 = ze2. Equivalently, R10 classify ordered pairs of event descriptions as to whether they are presented in their natural order. Assuming the </context>
</contexts>
<marker>Regneri, Koller, Ruppenhofer, Pinkal, 2011</marker>
<rawString>Michaela Regneri, Alexander Koller, Josef Ruppenhofer, and Manfred Pinkal. 2011. Learning script participants from unlabeled data. In Proceedings of RANLP 2011, pages 463–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, plans, and knowledge.</title>
<date>1975</date>
<booktitle>In Proceedings of the 4th International Joint Conference on Artificial Intelligence, IJCAI’75,</booktitle>
<pages>151--157</pages>
<contexts>
<context position="1286" citStr="Schank and Abelson, 1975" startWordPosition="173" endWordPosition="176">s and constraints on event ordering are induced jointly in one unified framework. We use a statistical model over permutations which captures event ordering constraints in a more flexible way than previous approaches. In order to alleviate the sparsity problem caused by using relatively small datasets, we incorporate in our hierarchical model an informed prior on word distributions. The resulting model substantially outperforms a state-of-the-art method on the event ordering task. 1 Introduction A script is a “predetermined, stereotyped sequence of actions that define a well-known situation” (Schank and Abelson, 1975). While humans acquire such common-sense knowledge over their lifetime, it constitutes a bottleneck for many NLP systems. Effective question answering and summarization are impossible without a form of story understanding, which in turn has been shown to benefit from access to databases of script knowledge (Mueller, 2004; Miikkulainen, 1995). Knowledge about the typical ordering of events can further help assessing document coherence and generating coherent text. Here, we present a general method for acquiring data bases of script knowledge. Our work may be regarded as complementary to existin</context>
<context position="5882" citStr="Schank and Abelson, 1975" startWordPosition="909" endWordPosition="912">how that prior knowledge as defined above enables the application of our model to small data sets, we emphasize that the model is generally widely applicable for two reasons. First, the data, collected using crowdsourcing, is comparatively easy and cheap to extend. Secondly, our model is domain independent and can be applied to scenario descriptions from any domain without any modification. Note that parameters were tuned on held-out scenarios, and no scenario-specific tuning was performed. 2 Related Work In the 1970s, scripts were introduced as a way to equip AI systems with world knowledge (Schank and Abelson, 1975; Barr and Feigenbaum, 1986). Task-specific script databases were developed manually. FrameNet (Baker et al., 1998) follows a similar idea, in defining verb frames together with argument types that can fill the verbs’ argument slots. Frames can then be combined into “scenario frames”. Manual composition of such databases, is arguably expensive and does not scale well. This paper follows a series of more recent work which aims to infer script knowledge automatically from data. Chambers and Jurafsky (2008) present a system which learns narrative chains from newswire texts. Relevant phrases are i</context>
</contexts>
<marker>Schank, Abelson, 1975</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1975. Scripts, plans, and knowledge. In Proceedings of the 4th International Joint Conference on Artificial Intelligence, IJCAI’75, pages 151–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A bayesian model for unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1445--1455</pages>
<contexts>
<context position="7722" citStr="Titov and Klementiev, 2011" startWordPosition="1196" endWordPosition="1199">emporal Script Graph; TSG) of the scenario is computed, using Multiple Sequence Alignment (MSA). Our work follows the work of Regneri et al. (2010), in that we use the same data and aim to focus on the same task. However, the two approaches described above employ a pipeline architecture and treat event learning and learning ordering constraints as separate problems. In contrast, we propose to learn both tasks jointly. We incorporate both tasks in a hierarchical Bayesian model, thus using one unified framework. A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts t</context>
</contexts>
<marker>Titov, Klementiev, 2011</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2011. A bayesian model for unsupervised semantic parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1445–1455.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>