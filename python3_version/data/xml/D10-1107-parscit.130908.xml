<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000099">
<title confidence="0.995093">
Constraints based Taxonomic Relation Classification
</title>
<author confidence="0.999519">
Quang Xuan Do Dan Roth
</author>
<affiliation confidence="0.846167">
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.998684">
{quangdo2,danr}@illinois.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999730178571428">
Determining whether two terms in text have
an ancestor relation (e.g. Toyota and car) or
a sibling relation (e.g. Toyota and Honda) is
an essential component of textual inference in
NLP applications such as Question Answer-
ing, Summarization, and Recognizing Textual
Entailment. Significant work has been done
on developing stationary knowledge sources
that could potentially support these tasks, but
these resources often suffer from low cover-
age, noise, and are inflexible when needed to
support terms that are not identical to those
placed in them, making their use as general
purpose background knowledge resources dif-
ficult. In this paper, rather than building a sta-
tionary hierarchical structure of terms and re-
lations, we describe a system that, given two
terms, determines the taxonomic relation be-
tween them using a machine learning-based
approach that makes use of existing resources.
Moreover, we develop a global constraint opti-
mization inference process and use it to lever-
age an existing knowledge base also to enforce
relational constraints among terms and thus
improve the classifier predictions. Our exper-
imental evaluation shows that our approach
significantly outperforms other systems built
upon existing well-known knowledge sources.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999028564102564">
Taxonomic relations that are read off of structured
ontological knowledge bases have been shown to
play important roles in many computational linguis-
tics tasks, such as document clustering (Hotho et
al., 2003), navigating text databases (Chakrabarti et
al., 1997), Question Answering (QA) (Saxena et al.,
2007) and summarization (Vikas et al., 2008). It
is clear that the recognition of taxonomic relation
between terms in sentences is essential to support
textual inference tasks such as Recognizing Textual
Entailment (RTE) (Dagan et al., 2006). For exam-
ple, it may be important to know that a blue Toy-
ota is neither a red Toyota nor a blue Honda, but
that all are cars, and even Japanese cars. Work in
Textual Entailment has argued quite convincingly
(MacCartney and Manning, 2008; MacCartney and
Manning, 2009) that many such textual inferences
are largely compositional and depend on the ability
to recognize some basic taxonomic relations such
as the ancestor or sibling relations between terms.
To date, these taxonomic relations can be read off
manually generated ontologies such as Wordnet that
explicitly represent these, and there has also been
some work trying to extend the manually built re-
sources using automatic acquisition methods result-
ing in structured knowledge bases such as the Ex-
tended WordNet (Snow et al., 2006) and the YAGO
ontology (Suchanek et al., 2007).
However, identifying when these relations hold
using fixed stationary hierarchical structures may
be impaired by noise in the resource and by uncer-
tainty in mapping targeted terms to concepts in the
structures. In addition, for knowledge sources de-
rived using bootstrapping algorithms and distribu-
tional semantic models such as (Pantel and Pen-
nacchiotti, 2006; Kozareva et al., 2008; Baroni and
Lenci, 2010), there is typically a trade-off between
precision and recall, resulting either in a relatively
accurate resource with low coverage or a noisy re-
</bodyText>
<page confidence="0.984088">
1099
</page>
<note confidence="0.817456">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099–1109,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.990497309090909">
source with broader coverage. In the current work,
we take a different approach, identifying directly
whether a pair of terms hold a taxonomic relation.
Fixed resources, as we observe, are inflexible
when dealing with targeted terms not being cov-
ered. This often happens when targeted terms have
the same meaning, but different surface forms, than
the terms used in the resources (e.g. Toyota Camry
and Camry). We argue that it is essential to have a
classifier that, given two terms, can build a semantic
representation of the terms and determines the tax-
onomic relations between them. This classifier will
make use of existing knowledge bases in multiple
ways, but will provide significantly larger coverage
and more precise results. We make use of a dynamic
resource such as Wikipedia to guarantee increased
coverage without changing our model and also per-
form normalization-to-Wikipedia to find appropri-
ate Wikipedia replacements for outside-Wikipedia
terms. Moreover, stationary resources are usually
brittle because of the way most of them are built:
using local relational patterns (e.g. (Hearst, 1992;
Snow et al., 2005)). Infrequent terms are less likely
to be covered, and some relations may not be sup-
ported well by these methods because their cor-
responding terms rarely appear in close proximity
(e.g., an Israeli tennis player Dudi Sela and Roger
Federrer). Our approach uses search techniques to
gather relevant Wikipedia pages of input terms and
performs a learning-based classification w.r.t. to the
features extracted from these pages as a way to get
around this brittleness.
Motivated by the needs of NLP applications such
as RTE, QA, Summarization, and the composition-
ality argument alluded to above, we focus on identi-
fying two fundamental types of taxonomic relations
- ancestor and sibling. An ancestor relation and its
directionality can help us infer that a statement with
respect to the child (e.g. cannabis) holds for an
ancestor (e.g. drugs) as in the following example,
taken from a textual entailment challenge dataset:
T: Nigeria’s NDLEA has seized 80 metric
tonnes of cannabis in one of its largest ever
hauls, officials say.
H: Nigeria seizes 80 tonnes of drugs.
Similarly, it is important to know of a sibling re-
lation to infer that a statement about Taiwan may
(without additional information) contradict a simi-
lar statement with respect to Japan since these are
different countries, as in the following:
T: A strong earthquake struck off the southern
tip of Taiwan at 12:26 UTC, triggering a warn-
ing from Japan’s Meteorological Agency that
a 3.3 foot tsunami could be heading towards
Basco, in the Philippines.
</bodyText>
<subsectionHeader confidence="0.661049">
H: An earthquake strikes Japan.
</subsectionHeader>
<bodyText confidence="0.999818875">
Several recent TE studies (Abad et al., 2010; Sam-
mons et al., 2010) suggest to isolate TE phenomena,
such as recognizing taxonomic relations, and study
them separately; they discuss some of characteristics
of phenomena such as contradiction from a similar
perspective to ours, but do not provide a solution.
In this paper, we present TAxonomic RElation
Classifier (TAREC), a system that classifies taxo-
nomic relations between a given pair of terms us-
ing a machine learning based classifier. An inte-
gral part of TAREC is also our inference model that
makes use of relational constraints to enforce co-
herency among several related predictions. TAREC
does not aim at building or extracting a hierarchi-
cal structure of concepts and relations, but rather to
directly recognize taxonomic relations given a pair
of terms. Target terms are represented using vector
of features that are extracted from retrieved corre-
sponding Wikipedia pages. In addition, we make
use of existing stationary ontologies to find related
terms to the target terms, and classify those too. This
allows us to make use of a constraint-based infer-
ence model (following (Roth and Yih, 2004; Roth
and Yih, 2007) that enforces coherency of decisions
across related pairs (e.g., if x is-a y and y is-a z, it
cannot be that x is a sibling of z).
In the rest of the paper, after discussing re-
lated work in Section 2, we present an overview of
TAREC in Section 3. The learning component and
the inference model of TAREC are described in Sec-
tions 4 and 5. We experimentally evaluate TAREC
in Section 6 and conclude our paper in Section 7.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9997565">
There are several works that aim at building tax-
onomies and ontologies which organize concepts
and their taxonomic relations into hierarchical struc-
tures. (Snow et al., 2005; Snow et al., 2006) con-
</bodyText>
<page confidence="0.984837">
1100
</page>
<bodyText confidence="0.999983298245614">
structed classifiers to identify hypernym relation-
ship between terms from dependency trees of large
corpora. Terms with recognized hypernym rela-
tion are extracted and incorporated into a man-made
lexical database, WordNet (Fellbaum, 1998), re-
sulting in the extended WordNet, which has been
augmented with over 400, 000 synsets. (Ponzetto
and Strube, 2007) and (Suchanek et al., 2007) both
mined Wikipedia to construct hierarchical structures
of concepts and relations. While the former ex-
ploited Wikipedia category system as a conceptual
network and extracted a taxonomy consisting of sub-
sumption relations, the latter presented the YAGO
ontology, which was automatically constructed by
mining and combining Wikipedia and WordNet. A
natural way to use these hierarchical structures to
support taxonomic relation classification is to map
targeted terms onto the hierarchies and check if
they subsume each other or share a common sub-
sumer. However, this approach is limited because
constructed hierarchies may suffer from noise and
require exact mapping (Section 6). TAREC over-
comes these limitations by searching and selecting
the top relevant articles in Wikipedia for each input
term; taxonomic relations are then recognized based
on the features extracted from these articles.
On the other hand, information extraction boot-
strapping algorithms, such as (Pantel and Pennac-
chiotti, 2006; Kozareva et al., 2008), automatically
harvest related terms on large corpora by starting
with a few seeds of pre-specified relations (e.g. is-
a, part-of). Bootstrapping algorithms rely on some
scoring function to assess the quality of terms and
additional patterns extracted during bootstrapping it-
erations. Similarly, but with a different focus, Open
IE, (Banko and Etzioni, 2008; Davidov and Rap-
poport, 2008), deals with a large number of relations
which are not pre-specified. Either way, the out-
put of these algorithms is usually limited to a small
number of high-quality terms while sacrificing cov-
erage (or vice versa). Moreover, an Open IE sys-
tem cannot control the extracted relations and this is
essential when identifying taxonomic relations. Re-
cently, (Baroni and Lenci, 2010) described a gen-
eral framework of distributional semantic models
that extracts significant contexts of given terms from
large corpora. Consequently, a term can be repre-
sented by a vector of contexts in which it frequently
appears. Any vector space model could then use the
terms’ vectors to cluster terms into categories. Sib-
ling terms (e.g. Honda, Toyota), therefore, have very
high chance to be clustered together. Nevertheless,
this approach cannot recognize ancestor relations.
In this paper, we compare TAREC with this frame-
work only on recognizing sibling vs. no relation, in
a strict experimental setting which pre-specifies the
categories to which the terms belong.
</bodyText>
<sectionHeader confidence="0.858564" genericHeader="method">
3 An Overview of the TAREC Algorithm
</sectionHeader>
<subsectionHeader confidence="0.993176">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.999869885714286">
In the TAREC algorithm, a term refers to any men-
tion in text, such as mountain, George W. Bush, bat-
tle of Normandy. TAREC does not aim at extracting
terms and building a stationary hierarchical structure
of terms, but rather recognize the taxonomic relation
between any two given terms. TAREC focuses on
classifying two fundamental types of taxonomic re-
lations: ancestor and sibling. Determining whether
two terms hold a taxonomic relation depends on a
pragmatic decision of how far one wants to climb up
a taxonomy to find a common subsumer. For exam-
ple, George W. Bush is a child of Presidents of the
United States as well as people, even more, that term
could also be considered as a child of mammals or
organisms w.r.t. the Wikipedia category system; in
that sense, George W. Bush may be considered as a
sibling of oak because they have organisms as a least
common subsumer. TAREC makes use of a hierar-
chical structure as background knowledge and con-
siders two terms to hold a taxonomic relation only
if the relation can be recognized from information
acquired by climbing up at most K levels from the
representation of the target terms in the structure. It
is also possible that the sibling relation can be rec-
ognized by clustering terms together by using vector
space models. If so, two terms are siblings if they
belong to the same cluster.
To cast the problem of identifying taxonomic rela-
tions between two terms x and y in a machine learn-
ing perspective, we model it as a multi-class classi-
fication problem. Table 1 defines four relations with
some examples in our experiment data sets.
This paper focuses on studying a fundamental
problem of recognizing taxonomic relations (given
well-segmented terms) and leaves the orthogonal is-
</bodyText>
<page confidence="0.970064">
1101
</page>
<table confidence="0.9981709">
Relation Meaning Examples
Term x Term y
x y x is an ancestor actor Mel Gibson
of y food rice
x y x is a child Makalu mountain
of y Monopoly game
x $ y x and y are Paris London
siblings copper oxygen
x�W+ y x and y have Roja C++
no relation egg Vega
</table>
<tableCaption confidence="0.9813885">
Table 1: Taxonomic relations and some examples in our
data sets.
</tableCaption>
<bodyText confidence="0.992864">
sues of how to take contexts into account and how it
should be used in applications to a future work.
</bodyText>
<subsectionHeader confidence="0.999167">
3.2 The Overview of TAREC
</subsectionHeader>
<bodyText confidence="0.99728675">
Assume that we already have a learned local clas-
sifier that can classify taxonomic relations between
any two terms. Given two terms, TAREC uses
Wikipedia and the local classifier in an inference
model to make a final prediction on the taxonomic
relation between these two. To motivate the need for
an inference model, beyond the local classifier itself,
we observe that the presence of other terms in addi-
tion to the two input terms, can provide some natural
constraints on the possible taxonomic relations and
thus can be used to make the final prediction (which
we also refer as global prediction) more coherent. In
practice, we first train a local classifier (Section 4),
then incorporate it into an inference model (Section
5) to classify taxonomic relations between terms.
The TAREC algorithm consists of three steps and
is summarized in Figure 1 and explained below.
1. Normalizing input terms to Wikipedia: Al-
though most commonly used terms have corre-
sponding Wikipedia articles, there are still a lot of
terms with no corresponding Wikipedia articles. For
a non-Wikipedia term, we make an attempt to find
a replacement by using Web search. We wish to
find a replacement such that the taxonomic relation
is unchanged. For example, for input pair (Lojze
Kovaxcixc, Rudi xSeligo), there is no English Wikipedia
page for Lojze Kovaxcixc, but if we can find Marjan
Roxzanc and use it as a replacement of Lojze Kovaxcixc
(two terms are siblings and refer to two writers), we
can continue classifying the taxonomic relation of
the pair (Marjan Roxzanc, Rudi xSeligo). This part
of the algorithm was motivated by (Sarmento et al.,
</bodyText>
<figure confidence="0.478791444444444">
TAxonomic RElation Classifier (TAREC)
INPUT: A pair of terms (x, y)
A learned local classifier R (Sec. 4)
Wikipedia W
OUTPUT: Taxonomic relation r* between x and y
1. (x, y) NormalizeToWikipedia(x, y, W)
2. Z GetAddionalTerms(x, y) (Sec. 5.2)
3. r* = ClassifyAndInference(x, y, Z, R, W) (Sec. 5.1)
RETURN: r*;
</figure>
<figureCaption confidence="0.999873">
Figure 1: The TAREC algorithm.
</figureCaption>
<bodyText confidence="0.958014666666667">
2007). We first make a query with the two input
terms (e.g. “Lojze Kovaxcixc” AND “Rudi xSeligo”)
to search for list-structure snippets in Web docu-
ments1 such as “... (delimiter) ca (delimiter) cb
(delimiter) cc (delimiter) ...” (the two input terms
should be among ca, cb, cc, ...). The delimiter could
be commas, periods, or asterisks2. For snippets that
contain the patterns of interest, we extract ca, cb, cc
etc. as replacement candidates. To reduce noise,
we empirically constrain the list to contain at least
4 terms that are no longer than 20 characters each.
The candidates are ranked based on their occurrence
frequency. The top candidate with Wikipedia pages
is used as a replacement.
2. Getting additional terms (Section 5.2): TAREC
leverage an existing knowledge base to extract addi-
tional terms related to the input terms, to be used in
the inference model in step 3.
</bodyText>
<listItem confidence="0.987234428571429">
3. Making global prediction with relational con-
straints (Section 5.1): TAREC performs several lo-
cal predictions using the local classifier R (Section
4) on the two input terms and these terms with the
additional ones. The global prediction is then in-
ferred by enforcing relational constraints among the
terms’ relations.
</listItem>
<sectionHeader confidence="0.890932" genericHeader="method">
4 Learning Taxonomic Relations
</sectionHeader>
<bodyText confidence="0.999910714285714">
The local classifier of TAREC is trained on the
pairs of terms with correct taxonomic relation labels
(some examples are showed in Table 1). The trained
classifier when applied on a new input pair of terms
will return a real valued number which can be inter-
preted as the probability of the predicted label. In
this section, we describe the learning features used
</bodyText>
<footnote confidence="0.999618">
1We use http://developer.yahoo.com/search/web/
2Periods and asterisks capture enumerations.
</footnote>
<page confidence="0.989039">
1102
</page>
<table confidence="0.9693606">
Title/Term Text Categories
President of The President of the United States is the head of state and head of government of the United States and is the Presidents of the United States, Presidency of
the United highest political official in the United States by influence and recognition. The President leads the executive the United States
States branch of the federal government and is one of only two elected members of the executive branch...
George W. George Walker Bush; born July 6, 1946) served as the 43rd President of the United States from 2001 to 2009. Children of Presidents of the United States, Gov-
Bush He was the 46th Governor of Texas from 1995 to 2000 before being sworn in as President on January 20, 2001... ernors of Texas, Presidents of the United States,
Texas Republicans...
Gerald Ford Gerald Rudolff Ford (born Leslie Lynch King, Jr.) (July 14, 1913 December 26, 2006) was the 38th President Presidents of the United States, Vice Presidents
of the United States, serving from 1974 to 1977, and the 40th Vice President of the United States serving from of the United States, Republican Party (United
1973 to 1974. States) presidential nominees...
</table>
<tableCaption confidence="0.999444">
Table 2: Examples of texts and categories of Wikipedia articles.
</tableCaption>
<bodyText confidence="0.998766710526316">
by our local taxonomic relation classifier.
Given two input terms, we first build a semantic
representation for each term by using a local search
engine3 to retrieve a list of top articles in Wikipedia
that are relevant to the term. To do this, we use the
following procedure: (1) Using both terms to make a
query (e.g. “George W. Bush” AND “Bill Clinton”)
to search in Wikipedia ; (2) Extracting important
keywords in the titles and categories of the retrieved
articles using TF-IDF (e.g. president, politician); (3)
Combining each input term with the extracted key-
words (e.g. “George W. Bush” AND “president”
AND “politician”) to create a final query used to
search for the term’s relevant articles in Wikipedia.
This is motivated by the assumption that the real
world applications calling TAREC typically does so
with two terms that are related in some sense, so our
procedure is designed to exploit that. For example,
it’s more likely that term Ford in the pair (George
W. Bush, Ford) refers to the former president of the
United States, Gerald Ford, than the founder of Ford
Motor Company, Henry Ford.
Once we have a semantic representation of each
term, in the form of the extracted articles, we extract
from it features that we use as the representation of
the two input terms in our learning algorithm. It is
worth noting that a Wikipedia page usually consists
of a title (i.e. the term), a body text, and a list of
categories to which the page belongs. Table 2 shows
some Wikipedia articles. From now on, we use the
titles of x, the texts of x, and the categories of x to
refer to the titles, texts, and categories of the asso-
ciated articles in the representation of x. Below are
the learning features extracted for input pair (x,y).
Bags-of-words Similarity: We use cosine simi-
larity metric to measure the degree of similarity be-
tween bags of words. We define four bags-of-words
features as the degree of similarity between the texts
</bodyText>
<equation confidence="0.624196166666667">
3E.g. http://lucene.apache.org/
Degree of similarity
texts(x) vs. categories(y)
categories(x) vs. texts(y)
texts(x) vs. texts(y)
categories(x) vs. categories(y)
</equation>
<tableCaption confidence="0.97503925">
Table 3: Bag-of-word features of the pair of terms (x,y);
texts(.) and categories(.) are two functions that extract
associated texts and categories from the semantic repre-
sentation of x and y.
</tableCaption>
<bodyText confidence="0.9999154">
and categories associated with two input terms x and
y in Table 3. To collect categories of a term, we take
the categories of its associated articles and go up K
levels in the Wikipedia category system. In our ex-
periments, we use abstracts of Wikipedia articles in-
stead of whole texts.
Association Information: This features repre-
sents a measure of association between the terms
by considering their information overlap. We cap-
ture this feature by the pointwise mutual informa-
tion (pmi) which quantifies the discrepancy between
the probability of two terms appearing together ver-
sus the probability of each term appearing indepen-
dently4. The pmi of two terms x and y is estimated
as follows:
</bodyText>
<equation confidence="0.9957245">
pmi(x, y) = log p(x, y)
p(x)p(y)
</equation>
<bodyText confidence="0.999622222222222">
where N is the total number of Wikipedia articles,
and f(.) is the function which counts the number of
appearances of its argument.
Overlap Ratios: The overlap ratio features cap-
ture the fact that the titles of a term usually overlap
with the categories of its descendants. We measure
this overlap as the ratio of the number of common
phrases used in the titles of one term and the cate-
gories of the other term. In our context, a phrase is
</bodyText>
<footnote confidence="0.525787666666667">
4pmi is different than mutual information. The former ap-
plies to specific outcomes, while the latter is to measure the
mutual dependence of two random variables.
</footnote>
<equation confidence="0.975721">
Nf(x, y)
= logf(x)f(y),
</equation>
<page confidence="0.862617">
1103
</page>
<bodyText confidence="0.999844263157895">
considered to be a common phrase if it appears in the
titles of one term and the categories of the other term
and it is also of the following types: (1) the whole
string of a category, or (2) the head in the root form
of a category, or (3) the post-modifier of a category.
We use the Noun Group Parser from (Suchanek et
al., 2007) to extract the head and post-modifier from
a category. For example, one of the categories of an
article about Chicago is Cities in Illinois. This cate-
gory can be parsed into a head in its root form City,
and a post-modifier Illinois. Given term pair (City,
Chicago), we observe that City matches the head of
the category Cities in Illinois of term Chicago. This
is a strong indication that Chicago is a child of City.
We also use a feature that captures the overlap
ratio of common phrases between the categories of
two input terms. For this feature, we do not use the
post-modifier of the categories. We use Jaccard sim-
ilarity coefficient to measure these overlaps ratios.
</bodyText>
<sectionHeader confidence="0.996969" genericHeader="method">
5 Inference with Relational Constraints
</sectionHeader>
<bodyText confidence="0.983035222222223">
Once we have a local multi-class classifier that maps
a given pair of terms to one of the four possible rela-
tions, we use a constraint-based optimization algo-
rithm to improve this prediction. The key insight
behind the way we model the inference model is
that if we consider more than two terms, there are
logical constraints that restrict the possible relations
among them. For instance, George W. Bush can-
not be an ancestor or sibling of president if we are
confident that president is an ancestor of Bill Clin-
ton, and Bill Clinton is a sibling of George W. Bush.
We call the combination of terms and their relations
a term network. Figure 2 shows some n-term net-
works consisting of two input terms (x, y), and ad-
ditional terms z, w, v.
The aforementioned observations show that if we
can obtain additional terms that are related to the
two target terms, we can enforce such coherency
relational constraints and make a global prediction
that would improve the prediction of the taxonomic
relation between the two given terms. Our infer-
ence model follows constraint-based formulations
that were introduced in the NLP community and
were shown to be very effective in exploiting declar-
ative background knowledge (Roth and Yih, 2004;
Denis and Baldridge, 2007; Punyakanok et al., 2008;
Chang et al., 2008).
</bodyText>
<figureCaption confidence="0.98548175">
Figure 2: Examples of n-term networks with two input
term x and y. (a) and (c) show valid combinations of
edges, whereas (b) and (d) are two relational constraints.
For simplicity, we do not draw no relation edges in (d).
</figureCaption>
<subsectionHeader confidence="0.99206">
5.1 Enforcing Coherency through Inference
</subsectionHeader>
<bodyText confidence="0.9916995">
Let x, y be two input terms, and Z =
{z1, z2,..., z,,,} be a set of additional terms. For a
subset Z E Z, we construct a set of term networks
whose nodes are x, y and all elements in Z, and the
edge, e, between every two nodes is one of four tax-
onomic relations whose weight, w(e), is given by
a local classifier (Section 4). If l = JZJ, there are
n = 2 + l nodes in each network, and 4[12 n(n−1)]
term networks can be constructed. In our experi-
ments we only use 3-term networks (i.e. l = 1).
For example, for the input pair (red, green) and
Z = {blue, yellow}, we can construct 64 networks
for the triple (red, green, Z = {blue}) and 64 net-
works for (red, green, Z = {yellow}) by trying all
possible relations between the terms.
A relational constraint is defined as a term net-
work consisting of only its “illegitimate” edge set-
tings, those that belongs to a pre-defined list of in-
valid edge combinations. For example, Figure 2b
shows an invalid network where red is a sibling of
both green and blue, and green is an ancestor of blue.
In Figure 2d, Celcius and meter cannot be siblings
because they are children of two sibling terms tem-
perature and length. The relational constraints used
in our experiments are manually constructed.
Let C be a list of relational constraints. Equation
(1) defines the network scoring function, which is a
linear combination of the edge weights, w(e), and
the penalties, Pk, of term networks matching con-
straint Ck E C.
</bodyText>
<equation confidence="0.9138295">
�score(t) =
eEt
</equation>
<bodyText confidence="0.9966865">
function dCk(t) indicates if t matches Ck. In our
work, we use relational constraints as hard con-
</bodyText>
<figure confidence="0.9988265">
car physical
Bill Clinton Blue manufacturer BMW quantities
x ,
z
x ,
z
z w
x ,
temperature
z w
x ,
v
length
meter
Red Green
President
Honda Toyota
(c)
George W.
Bush
Celcius
(d)
(a) (b)
w(e) − |S |PkdCk(t) (1)
L
k=1
</figure>
<page confidence="0.924287">
1104
</page>
<figureCaption confidence="0.9608">
Figure 3: Our YAGO query patterns used to obtain related
terms for “x”.
</figureCaption>
<bodyText confidence="0.999944739130435">
straints and set their penalty Pk to oc. For a set of
term networks formed by (x, y, Z) and all possible
relations between the terms, we select the best net-
work, t* = argmaxtscore(t).
After picking the best term network t* for every
Z E Z, we make the final decision on the taxonomic
relation between x and y. Let r denote the relation
between x and y in a particular t* (e.g. r = x H y.)
The set of all t* is divided into 4 groups with respect
to r (e.g. a group of all t* having r = x H y, a
group of all t* having r = x +— y.) We denote a
group with term networks holding r as the relation
between x and y by Tr. To choose the best taxo-
nomic relation, r*, of x and y, we solve the objective
function defined in Equation 2.
where At is the weight of term network t, defined
as the occurrence probability of t (regarding only its
edges’ setting) in the training data, which is aug-
mented with additional terms. Equation (2) finds the
best taxonomic relation of two input terms by com-
puting the average score of every group of the best
term networks representing a particular relation of
two input terms.
</bodyText>
<subsectionHeader confidence="0.99853">
5.2 Extracting Related Terms
</subsectionHeader>
<bodyText confidence="0.999900777777778">
In the inference model, we need to obtain other
terms that are related to the two input terms. Here-
after, we refer to additional terms as related terms.
The related term space is a space of direct ancestors,
siblings and children in a particular resource.
We propose an approach that uses the YAGO on-
tology (Suchanek et al., 2007) to provide related
terms. It is worth noting that YAGO is chosen over
the Wikipedia category system used in our work be-
cause YAGO is a clean ontology built by carefully
combining Wikipedia and WordNet.5
In YAGO model, all objects (e.g. cities, people,
etc.) are represented as entities. To map our input
terms to entities in YAGO, we use the MEANS re-
lation defined in the YAGO ontology. Furthermore,
similar entities are grouped into classes. This allows
us to obtain direct ancestors of an entity by using
the TYPE relation which gives the entity’s classes.
Furthermore, we can get ancestors of a class with
the SUBCLASSOF relation6. By using three relations
MEANS, TYPE and SUBCLASSOF in YAGO model,
we can obtain Proposals for direct ancestors, sib-
lings, and children, if any, for any input term. We
then evaluate our classifier on all pairs, and run the
inference to improve the prediction using the co-
herency constraints. Figure 3 presents three patterns
that we used to query related terms from YAGO.
</bodyText>
<sectionHeader confidence="0.994824" genericHeader="evaluation">
6 Experimental Study
</sectionHeader>
<bodyText confidence="0.999913">
In this section, we evaluate TAREC against several
systems built upon existing well-known knowledge
sources. The resources are either hierarchical struc-
tures or extracted by using distributional semantic
models. We also perform several experimental anal-
yses to understand TAREC’s behavior in details.
</bodyText>
<subsectionHeader confidence="0.998169">
6.1 Comparison to Hierarchical Structures
</subsectionHeader>
<bodyText confidence="0.999969866666667">
We create and use two main data sets in our ex-
periments. Dataset-I is generated from 40 seman-
tic classes of about 11,000 instances. The orig-
inal semantic classes and instances were manu-
ally constructed with a limited amount of manual
post-filtering and were used to evaluate informa-
tion extraction tasks in (Pas¸ca, 2007; Pas¸ca and
Van Durme, 2008) (we refer to this original data as
OrgData-I). This dataset contains both terms with
Wikipedia pages (e.g. George W. Bush) and non-
Wikipedia terms (e.g. hindu mysticism). Pairs of
terms are generated by randomly pairing seman-
tic class names and instances. We generate dis-
joint training and test sets of 8,000 and 12,000 pairs
of terms, respectively. We call the test set of this
</bodyText>
<footnote confidence="0.740219333333333">
5However, YAGO by itself is weaker than our approach in
identifying taxonomic relations (see Section 6.)
6These relations are defined in the YAGO ontology.
</footnote>
<figure confidence="0.8925074375">
Pattern 1
Pattern 3
Pattern 2
YAGO Query Patterns
INPUT: term “x”
OUTPUT: lists of ancestors, siblings, and children of “x”
“x” MEANS ?A
“x” MEANS ?A
“x” MEANS ?D
?A SUBCLASSOF ?B
?C SUBCLASSOF ?B
?A TYPE ?B
?C TYPE ?B
?E TYPE ?D
RETURN: ?B, ?C, ?E as
lists of ancestors, siblings, and children, respectively.
</figure>
<equation confidence="0.85214">
r* = argmaxr |7 |1: At.score(t*) (2)
r
t*ETr
</equation>
<page confidence="0.96892">
1105
</page>
<bodyText confidence="0.999945372093023">
dataset Test-I. Dataset-II is generated from 44 se-
mantic classes of more than 10,000 instances used
in (Vyas and Pantel, 2009)7. The original semantic
classes and instances were extracted from Wikipedia
lists. This data, therefore, only contains terms with
corresponding Wikipedia pages. We also generate
disjoint training and test sets of 8,000 and 12,000
pairs of terms, respectively, and call the test set of
this dataset Test-II.8
Several semantic class names in the original data
are written in short forms (e.g. chemicalelem,
proglanguage). We expand these names to some
meaningful names which are used by all systems in
our experiments. For example, terroristgroup is ex-
panded to terrorist group, terrorism. Table 1 shows
some pairs of terms which are generated. Four types
of taxonomic relations are covered with balanced
numbers of examples in all data sets. To evaluate our
systems, we use a snapshot of Wikipedia from July,
2008. After cleaning and removing articles without
categories (except redirect pages), 5,503,763 articles
remain. We index these articles using Lucene9. As
a learning algorithm, we use a regularized averaged
Perceptron (Freund and Schapire, 1999).
We compare TAREC with three systems that we
built using recently developed large-scale hierarchi-
cal structures. Strube07 is built on the latest ver-
sion of a taxonomy, TStrube, which was derived from
Wikipedia (Ponzetto and Strube, 2007). It is worth
noting that the structure of TStrube is similar to the
page structure of Wikipedia. For a fair comparison,
we first generate a semantic representation for each
input term by following the same procedure used in
TAREC described in Section 4. The titles and cat-
egories of the articles in the representation of each
input term are then extracted. Only titles and their
corresponding categories that are in TStrube are con-
sidered. A term is an ancestor of the other if at
least one of its titles is in the categories of the other
term. If two terms share a common category, they
are considered siblings; and no relation, otherwise.
The ancestor relation is checked first, then sibling,
and finally no relation. Snow06 uses the extended
</bodyText>
<footnote confidence="0.9995856">
7There were 50 semantic classes in the original dataset. We
grouped some semantically similar classes for the purpose of
classifying taxonomic relations.
8Published at http://cogcomp.cs.illinois.edu/page/software
9http://lucene.apache.org, version 2.3.2
</footnote>
<table confidence="0.992475666666667">
Test-I Test-II
Strube07 24.32 25.63
Snow06 41.97 36.26
Yago07 65.93 70.63
TAREC (local) 81.89 84.7
TAREC 85.34 86.98
</table>
<tableCaption confidence="0.989497">
Table 4: Evaluating and comparing performances, in ac-
curacy, of the systems on Test-I and Test-II. TAREC (lo-
cal) uses only our local classifier to identify taxonomic re-
lations by choosing the relation with highest confidence.
</tableCaption>
<bodyText confidence="0.999473638888889">
WordNet (Snow et al., 2006). Words in the extended
WordNet can be common nouns or proper nouns.
Given two input terms, we first map them onto the
hierarchical structure of the extended WordNet by
exact string matching. A term is an ancestor of the
other if it can be found as an hypernym after going
up K levels in the hierarchy from the other term. If
two terms share a common subsumer within some
levels, then they are considered as siblings. Oth-
erwise, there is no relation between the two input
terms. Similar to Strube07, we first check ancestor,
then sibling, and finally no relation. Yago07 uses
the YAGO ontology (Suchanek et al., 2007) as its
main source of background knowledge. Because the
YAGO ontology is a combination of Wikipedia and
WordNet, this system is expected to perform well at
recognizing taxonomic relations. To access a term’s
ancestors and siblings, we use patterns 1 and 2 in
Figure 3 to map a term to the ontology and move up
on the ontology. The relation identification process
is then similar to those of Snow06 and Strube07. If
an input term is not recognized by these systems,
they return no relation.
Our overall algorithm, TAREC, is described in
Figure 1. We manually construct a pre-defined list
of 35 relational constraints to use in the inference
model. We also evaluate our local classifier (Section
4), which is referred as TAREC (local). To make
classification decision with TAREC (local), for a
pair of terms, we choose the predicted relation with
highest confidence returned by the classifier.
In all systems compared, we vary the value of K10
from 1 to 4. The best result of each system is re-
ported. Table 4 shows the comparison of all sys-
tems evaluated on both Test-I and Test-II. Our sys-
tems, as shown, significantly outperform the other
</bodyText>
<footnote confidence="0.718589">
10See Section 3.1 for the meaning of K.
</footnote>
<page confidence="0.993371">
1106
</page>
<bodyText confidence="0.995659580645161">
systems. In Table 4, the improvement of TAREC
over TAREC (local) on Test-I shows the contribu-
tion of both the normalization procedure (that is, go-
ing outside Wikipedia terms) and the global infer-
ence model to the classification decisions, whereas
the improvement on Test-II shows only the contribu-
tion of the inference model, because Test-II contains
only terms with corresponding Wikipedia articles.
Observing the results we see that our algorithms
is doing significantly better that fixed taxonomies
based algorithms. This is true both for TAREC (lo-
cal) and for TAREC. We believe that our machine
learning based classifier is very flexible in extract-
ing features of the two input terms and thus in pre-
dicting their taxonomic Relation. On the other hand,
other system rely heavily on string matching tech-
niques to map input terms to their respective ontolo-
gies, and these are very inflexible and brittle. This
clearly shows one limitation of using existing struc-
tured resources to classify taxonomic relations.
We do not use special tactics to handle polyse-
mous terms. However, our procedure of building se-
mantic representations for input terms described in
Section 4 ties the senses of the two input terms and
thus, implicitly, may get some sense information.
We do not use this procedure in Snow06 because
WordNet and Wikipedia are two different knowl-
edge bases. We also do not use this procedure in
Yago07 because in YAGO, a term is mapped onto the
ontology by using the MEANS operator (in Pattern 1,
Figure 3). This cannot follow our procedure.
</bodyText>
<subsectionHeader confidence="0.999745">
6.2 Comparison to Harvested Knowledge
</subsectionHeader>
<bodyText confidence="0.999997547169812">
As we discussed in Section 2, the output of
bootstrapping-based algorithms is usually limited to
a small number of high-quality terms while sacri-
ficing coverage (or vice versa). For example, the
full Espresso algorithm in (Pantel and Pennacchiotti,
2006) extracted 69,156 instances of is-a relation
with 36.2% precision. Similarly, (Kozareva et al.,
2008) evaluated only a small number (a few hun-
dreds) of harvested instances. Recently, (Baroni
and Lenci, 2010) proposed a general framework to
extract properties of input terms. Their TypeDM
model harvested 5,000 significant properties for
each term out of 20,410 noun terms. For exam-
ple, the properties of marine include (own, bomb),
(use, gun). Using vector space models we could
measure the similarity between terms using their
property vectors. However, since the information
available in TypeDM does not support predicting the
ancestor relation between terms, we only evaluate
TypeDM in classifying sibling vs. no relation. We
do this by giving a list of semantic classes using the
following procedure: (1) For each semantic class,
use some seeds to compute a centroid vector from
the seeds’ vectors in TypeDM, (2) each term in an
input pair is classified into its best semantic class
based on the cosine similarity between its vector and
the centroid vector of the category, (3) two terms are
siblings if they are classified into the same category;
and have no relation, otherwise. Out of 20,410 noun
terms in TypeDM, there are only 345 terms overlap-
ping with the instances in OrgData-I and belonging
to 10 significant semantic classes. For each seman-
tic class, we randomly pick 5 instances as its seeds to
make a centroid vector. The rest of the overlapping
instances are randomly paired to make a dataset of
4,000 pairs of terms balanced in the number of sib-
ling and no relation pairs. On this dataset, TypeDM
achieves the accuracy of 79.75%. TAREC (local),
with the local classifier trained on the training set
(with 4 relation classes) of Dataset-I, gives 78.35%
of accuracy. The full TAREC system with relational
constraints achieves 82.65%. We also re-train and
evaluate the local classifier of TAREC on the same
training set but without ancestor relation pairs. This
local classifier has an accuracy of 81.08%.
These results show that although the full TAREC
system gives better performance, TypeDM is very
competitive in recognizing sibling vs. no relation.
However, TypeDM can only work in a limited set-
ting where semantic classes are given in advance,
which is not practical in real-world applications; and
of course, TypeDM does not help to recognize an-
cestor relations between two terms.
</bodyText>
<subsectionHeader confidence="0.99429">
6.3 Experimental Analysis
</subsectionHeader>
<bodyText confidence="0.9993805">
In this section, we discuss some experimental anal-
yses to better understand our systems.
Precision and Recall: We want to study TAREC
on individual taxonomic relations using Precision
and Recall. Table 5 shows that TAREC performs
very well on ancestor relation. Sibling and no rela-
tion are the most difficult relations to classify. In
the same experimental setting on Test-I, Yago07
</bodyText>
<page confidence="0.998417">
1107
</page>
<tableCaption confidence="0.9937665">
Table 5: Performance of TAREC on individual taxo-
nomic relation.
</tableCaption>
<table confidence="0.9998065">
Wiki WordNet non-Wiki
Strube07 24.59 24.13 21.18
Snow06 41.23 46.91 34.46
Yago07 69.95 70.42 34.26
TAREC (local) 89.37 89.72 31.22
TAREC 91.03 91.2 45.21
</table>
<tableCaption confidence="0.880994666666667">
Table 6: Performance of the systems on special data sets,
in accuracy. On the non-Wikipedia test set, TAREC (lo-
cal) simply returns sibling relation.
</tableCaption>
<bodyText confidence="0.99863051724138">
achieves 79.34% and 66.03% of average Precision
and Recall, respectively. These numbers on Test-II
are 81.33% and 70.44%.
Special Data Sets: We evaluate all systems that
use hierarchical structures as background knowl-
edge on three special data sets derived from Test-I.
From 12,000 pairs in Test-I, we created a test set,
Wiki, consisting of 10, 456 pairs with all terms in
Wikipedia. We use the rest of 1, 544 pairs with at
least one non-Wikipedia term to build a non-Wiki
test set. The third dataset, WordNet, contains 8, 625
pairs with all terms in WordNet and Wikipedia. Ta-
ble 6 shows the performance of the systems on these
data sets. Unsurprisingly, Yago07 gets better results
on Wiki than on Test-I. Snow06, as expected, gives
better performance on the WordNet test set. TAREC
still significantly outperforms these systems. The
improvement of TAREC over TAREC (local) on the
Wiki and WordNet test sets shows the contribution
of the inference model, whereas the improvement on
the non-Wikipedia test set shows the contribution of
normalizing input terms to Wikipedia.
Contribution of Related Terms in Inference:
We evaluate TAREC when the inference procedure
is fed by related terms that are generated using a
“gold standard” source instead of YAGO. To do this,
we use the original data which was used to generate
Test-I. For each term in the examples of Test-I, we
get its ancestors, siblings, and children, if any, from
</bodyText>
<table confidence="0.985388333333333">
K=1 K=2 K=3 K=4
TAREC 82.93 85.34 85.23 83.95
TAREC (Gold Infer.) 83.46 86.18 85.9 84.93
</table>
<tableCaption confidence="0.9856495">
Table 7: Evaluating TAREC with different sources pro-
viding related terms to do inference.
</tableCaption>
<bodyText confidence="0.99991275">
the original data and use them as related terms in the
inference model. This system is referred as TAREC
(Gold Infer.). Table 7 shows the results of the two
systems on different K as the number of levels to
go up on the Wikipedia category system. We see
that TAREC gets better results when doing inference
with better related terms. In this experiment, the two
systems use the same number of related terms.
</bodyText>
<sectionHeader confidence="0.999574" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999991">
We studied an important component of many com-
putational linguistics tasks: given two target terms,
determine that taxonomic relation between them.
We have argued that static structured knowledge
bases cannot support this task well enough, and pro-
vided empirical support for this claim. We have de-
veloped TAREC, a novel algorithm that leverages in-
formation from existing knowledge sources and uses
machine learning and a constraint-based inference
model to mitigate the noise and the level of uncer-
tainty inherent in these resources. Our evaluations
show that TAREC significantly outperforms other
systems built upon existing well-known knowledge
sources. Our approach generalizes and handles non-
Wikipedia term well across semantic classes. Our
future work will include an evaluation of TAREC in
the context of textual inference applications.
</bodyText>
<sectionHeader confidence="0.997493" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.96805275">
The authors thank Mark Sammons, Vivek Srikumar, James
Clarke and the anonymous reviewers for their insightful com-
ments and suggestions. University of Illinois at Urbana-
Champaign gratefully acknowledges the support of Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract No. FA8750-09-C-0181. The first author also
thanks the Vietnam Education Foundation (VEF) for its spon-
sorship. Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the view of the VEF, DARPA, AFRL,
or the US government.
</bodyText>
<figure confidence="0.995606375">
TAREC
x y
x ! y
x $ y
x �W+ y
Average
Test-I
Test-II
Prec
Rec
Prec
Rec
95.82
88.01
96.46
88.48
94.61
89.29
96.15
88.86
79.23
84.01
83.15
81.87
73.94
79.9
75.54
88.27
85.9
85.3
87.83
86.87
</figure>
<page confidence="0.98362">
1108
</page>
<sectionHeader confidence="0.994478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882261363636">
A. Abad, L. Bentivogli, I. Dagan, D. Giampiccolo,
S. Mirkin, E. Pianta, and A. Stern. 2010. A resource
for investigating the impact of anaphora and corefer-
ence on inference. In LREC.
M. Banko and O. Etzioni. 2008. The tradeoffs between
open and traditional relation extraction. In ACL-HLT.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36.
S. Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan.
1997. Using taxonomy, discriminants, and signatures
for navigating in text databases. In VLDB.
M. Chang, L. Ratinov, and D. Roth. 2008. Constraints as
prior knowledge. In ICML Workshop on Prior Knowl-
edge for Text and Language Processing.
D. Davidov and A. Rappoport. 2008. Unsupervised dis-
covery of generic relationships using pattern clusters
and its evaluation by automatically generated sat anal-
ogy questions. In ACL.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In NAACL.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning.
M. A. Hearst. 1992. Acquisition of hyponyms from large
text corpora. In COLING.
A. Hotho, S. Staab, and G. Stumme. 2003. Ontologies
improve text document clustering. In ICDM.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pattern
linkage graphs. In ACL-HLT.
B. MacCartney and C. D. Manning. 2008. Modeling se-
mantic containment and exclusion in natural language
inference. In COLING.
B. MacCartney and C. D. Manning. 2009. An extended
model of natural logic. In IWCS-8.
M. Pas¸ca and B. Van Durme. 2008. Weakly-supervised
acquisition of open-domain classes and class attributes
from web documents and query logs. In ACL-HLT.
M. Pas¸ca. 2007. Organizing and searching the world
wide web of facts step two: Harnessing the wisdom
of the crowds. In WWW.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In ACL, pages 113–120.
S. P. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. AAAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
CoNLL.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
M. Sammons, V.G. Vydiswaran, and D. Roth. 2010. Ask
not what textual entailment can do for you... In ACL.
L. Sarmento, V. Jijkuon, M. de Rijke, and E. Oliveira.
2007. ”more like these”: growing entity classes from
seeds. In CIKM.
A. K. Saxena, G. V. Sambhu, S. Kaushik, and L. V. Sub-
ramaniam. 2007. Iitd-ibmirl system for question an-
swering using pattern matching, semantic type and se-
mantic category recognition. In TREC.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
ACL.
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: A Core of Semantic Knowledge. In WWW.
O. Vikas, A. K. Meshram, G. Meena, and A. Gupta.
2008. Multiple document summarization using princi-
pal component analysis incorporating semantic vector
space model. In Computational Linguistics and Chi-
nese Language Processing.
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In NAACL-HLT.
D. Yarowsky. 1995. Unsupervised woed sense disam-
biguation rivaling supervied methods. In Proceedings
of ACL-95.
</reference>
<page confidence="0.997697">
1109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923531">
<title confidence="0.999462">Constraints based Taxonomic Relation Classification</title>
<author confidence="0.999937">Quang Xuan Do Dan Roth</author>
<affiliation confidence="0.9983605">Department of Computer University of Illinois at</affiliation>
<address confidence="0.965219">Urbana, IL 61801,</address>
<abstract confidence="0.998246586206897">Determining whether two terms in text have ancestor relation (e.g. or sibling relation (e.g. is an essential component of textual inference in NLP applications such as Question Answering, Summarization, and Recognizing Textual Entailment. Significant work has been done on developing stationary knowledge sources that could potentially support these tasks, but these resources often suffer from low coverage, noise, and are inflexible when needed to support terms that are not identical to those placed in them, making their use as general purpose background knowledge resources difficult. In this paper, rather than building a stationary hierarchical structure of terms and relations, we describe a system that, given two terms, determines the taxonomic relation between them using a machine learning-based approach that makes use of existing resources. Moreover, we develop a global constraint optimization inference process and use it to leverage an existing knowledge base also to enforce relational constraints among terms and thus improve the classifier predictions. Our experimental evaluation shows that our approach significantly outperforms other systems built upon existing well-known knowledge sources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abad</author>
<author>L Bentivogli</author>
<author>I Dagan</author>
<author>D Giampiccolo</author>
<author>S Mirkin</author>
<author>E Pianta</author>
<author>A Stern</author>
</authors>
<title>A resource for investigating the impact of anaphora and coreference on inference.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="6339" citStr="Abad et al., 2010" startWordPosition="980" endWordPosition="983">is in one of its largest ever hauls, officials say. H: Nigeria seizes 80 tonnes of drugs. Similarly, it is important to know of a sibling relation to infer that a statement about Taiwan may (without additional information) contradict a similar statement with respect to Japan since these are different countries, as in the following: T: A strong earthquake struck off the southern tip of Taiwan at 12:26 UTC, triggering a warning from Japan’s Meteorological Agency that a 3.3 foot tsunami could be heading towards Basco, in the Philippines. H: An earthquake strikes Japan. Several recent TE studies (Abad et al., 2010; Sammons et al., 2010) suggest to isolate TE phenomena, such as recognizing taxonomic relations, and study them separately; they discuss some of characteristics of phenomena such as contradiction from a similar perspective to ours, but do not provide a solution. In this paper, we present TAxonomic RElation Classifier (TAREC), a system that classifies taxonomic relations between a given pair of terms using a machine learning based classifier. An integral part of TAREC is also our inference model that makes use of relational constraints to enforce coherency among several related predictions. TA</context>
</contexts>
<marker>Abad, Bentivogli, Dagan, Giampiccolo, Mirkin, Pianta, Stern, 2010</marker>
<rawString>A. Abad, L. Bentivogli, I. Dagan, D. Giampiccolo, S. Mirkin, E. Pianta, and A. Stern. 2010. A resource for investigating the impact of anaphora and coreference on inference. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>O Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="9877" citStr="Banko and Etzioni, 2008" startWordPosition="1542" endWordPosition="1545">rticles in Wikipedia for each input term; taxonomic relations are then recognized based on the features extracted from these articles. On the other hand, information extraction bootstrapping algorithms, such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008), automatically harvest related terms on large corpora by starting with a few seeds of pre-specified relations (e.g. isa, part-of). Bootstrapping algorithms rely on some scoring function to assess the quality of terms and additional patterns extracted during bootstrapping iterations. Similarly, but with a different focus, Open IE, (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), deals with a large number of relations which are not pre-specified. Either way, the output of these algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa). Moreover, an Open IE system cannot control the extracted relations and this is essential when identifying taxonomic relations. Recently, (Baroni and Lenci, 2010) described a general framework of distributional semantic models that extracts significant contexts of given terms from large corpora. Consequently, a term can be represented by a vector of con</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>M. Banko and O. Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<contexts>
<context position="3269" citStr="Baroni and Lenci, 2010" startWordPosition="494" endWordPosition="497">ying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007). However, identifying when these relations hold using fixed stationary hierarchical structures may be impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures. In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008; Baroni and Lenci, 2010), there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re1099 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099–1109, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics source with broader coverage. In the current work, we take a different approach, identifying directly whether a pair of terms hold a taxonomic relation. Fixed resources, as we observe, are inflexible when dealing with targeted terms not being covered. </context>
<context position="10284" citStr="Baroni and Lenci, 2010" startWordPosition="1608" endWordPosition="1611">ping algorithms rely on some scoring function to assess the quality of terms and additional patterns extracted during bootstrapping iterations. Similarly, but with a different focus, Open IE, (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), deals with a large number of relations which are not pre-specified. Either way, the output of these algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa). Moreover, an Open IE system cannot control the extracted relations and this is essential when identifying taxonomic relations. Recently, (Baroni and Lenci, 2010) described a general framework of distributional semantic models that extracts significant contexts of given terms from large corpora. Consequently, a term can be represented by a vector of contexts in which it frequently appears. Any vector space model could then use the terms’ vectors to cluster terms into categories. Sibling terms (e.g. Honda, Toyota), therefore, have very high chance to be clustered together. Nevertheless, this approach cannot recognize ancestor relations. In this paper, we compare TAREC with this framework only on recognizing sibling vs. no relation, in a strict experimen</context>
<context position="36949" citStr="Baroni and Lenci, 2010" startWordPosition="6139" endWordPosition="6142"> a term is mapped onto the ontology by using the MEANS operator (in Pattern 1, Figure 3). This cannot follow our procedure. 6.2 Comparison to Harvested Knowledge As we discussed in Section 2, the output of bootstrapping-based algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa). For example, the full Espresso algorithm in (Pantel and Pennacchiotti, 2006) extracted 69,156 instances of is-a relation with 36.2% precision. Similarly, (Kozareva et al., 2008) evaluated only a small number (a few hundreds) of harvested instances. Recently, (Baroni and Lenci, 2010) proposed a general framework to extract properties of input terms. Their TypeDM model harvested 5,000 significant properties for each term out of 20,410 noun terms. For example, the properties of marine include (own, bomb), (use, gun). Using vector space models we could measure the similarity between terms using their property vectors. However, since the information available in TypeDM does not support predicting the ancestor relation between terms, we only evaluate TypeDM in classifying sibling vs. no relation. We do this by giving a list of semantic classes using the following procedure: (1</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chakrabarti</author>
<author>B Dom</author>
<author>R Agrawal</author>
<author>P Raghavan</author>
</authors>
<title>Using taxonomy, discriminants, and signatures for navigating in text databases.</title>
<date>1997</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="1741" citStr="Chakrabarti et al., 1997" startWordPosition="251" endWordPosition="254">develop a global constraint optimization inference process and use it to leverage an existing knowledge base also to enforce relational constraints among terms and thus improve the classifier predictions. Our experimental evaluation shows that our approach significantly outperforms other systems built upon existing well-known knowledge sources. 1 Introduction Taxonomic relations that are read off of structured ontological knowledge bases have been shown to play important roles in many computational linguistics tasks, such as document clustering (Hotho et al., 2003), navigating text databases (Chakrabarti et al., 1997), Question Answering (QA) (Saxena et al., 2007) and summarization (Vikas et al., 2008). It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2006). For example, it may be important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars. Work in Textual Entailment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely co</context>
</contexts>
<marker>Chakrabarti, Dom, Agrawal, Raghavan, 1997</marker>
<rawString>S. Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan. 1997. Using taxonomy, discriminants, and signatures for navigating in text databases. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Constraints as prior knowledge.</title>
<date>2008</date>
<booktitle>In ICML Workshop on Prior Knowledge for Text and Language Processing.</booktitle>
<contexts>
<context position="24104" citStr="Chang et al., 2008" startWordPosition="3947" endWordPosition="3950"> two input terms (x, y), and additional terms z, w, v. The aforementioned observations show that if we can obtain additional terms that are related to the two target terms, we can enforce such coherency relational constraints and make a global prediction that would improve the prediction of the taxonomic relation between the two given terms. Our inference model follows constraint-based formulations that were introduced in the NLP community and were shown to be very effective in exploiting declarative background knowledge (Roth and Yih, 2004; Denis and Baldridge, 2007; Punyakanok et al., 2008; Chang et al., 2008). Figure 2: Examples of n-term networks with two input term x and y. (a) and (c) show valid combinations of edges, whereas (b) and (d) are two relational constraints. For simplicity, we do not draw no relation edges in (d). 5.1 Enforcing Coherency through Inference Let x, y be two input terms, and Z = {z1, z2,..., z,,,} be a set of additional terms. For a subset Z E Z, we construct a set of term networks whose nodes are x, y and all elements in Z, and the edge, e, between every two nodes is one of four taxonomic relations whose weight, w(e), is given by a local classifier (Section 4). If l = J</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2008</marker>
<rawString>M. Chang, L. Ratinov, and D. Roth. 2008. Constraints as prior knowledge. In ICML Workshop on Prior Knowledge for Text and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>A Rappoport</author>
</authors>
<title>Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated sat analogy questions.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9907" citStr="Davidov and Rappoport, 2008" startWordPosition="1546" endWordPosition="1550">each input term; taxonomic relations are then recognized based on the features extracted from these articles. On the other hand, information extraction bootstrapping algorithms, such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008), automatically harvest related terms on large corpora by starting with a few seeds of pre-specified relations (e.g. isa, part-of). Bootstrapping algorithms rely on some scoring function to assess the quality of terms and additional patterns extracted during bootstrapping iterations. Similarly, but with a different focus, Open IE, (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), deals with a large number of relations which are not pre-specified. Either way, the output of these algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa). Moreover, an Open IE system cannot control the extracted relations and this is essential when identifying taxonomic relations. Recently, (Baroni and Lenci, 2010) described a general framework of distributional semantic models that extracts significant contexts of given terms from large corpora. Consequently, a term can be represented by a vector of contexts in which it frequently a</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>D. Davidov and A. Rappoport. 2008. Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated sat analogy questions. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="24058" citStr="Denis and Baldridge, 2007" startWordPosition="3939" endWordPosition="3942">k. Figure 2 shows some n-term networks consisting of two input terms (x, y), and additional terms z, w, v. The aforementioned observations show that if we can obtain additional terms that are related to the two target terms, we can enforce such coherency relational constraints and make a global prediction that would improve the prediction of the taxonomic relation between the two given terms. Our inference model follows constraint-based formulations that were introduced in the NLP community and were shown to be very effective in exploiting declarative background knowledge (Roth and Yih, 2004; Denis and Baldridge, 2007; Punyakanok et al., 2008; Chang et al., 2008). Figure 2: Examples of n-term networks with two input term x and y. (a) and (c) show valid combinations of edges, whereas (b) and (d) are two relational constraints. For simplicity, we do not draw no relation edges in (d). 5.1 Enforcing Coherency through Inference Let x, y be two input terms, and Z = {z1, z2,..., z,,,} be a set of additional terms. For a subset Z E Z, we construct a set of term networks whose nodes are x, y and all elements in Z, and the edge, e, between every two nodes is one of four taxonomic relations whose weight, w(e), is giv</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8348" citStr="Fellbaum, 1998" startWordPosition="1314" endWordPosition="1315">omponent and the inference model of TAREC are described in Sections 4 and 5. We experimentally evaluate TAREC in Section 6 and conclude our paper in Section 7. 2 Related Work There are several works that aim at building taxonomies and ontologies which organize concepts and their taxonomic relations into hierarchical structures. (Snow et al., 2005; Snow et al., 2006) con1100 structed classifiers to identify hypernym relationship between terms from dependency trees of large corpora. Terms with recognized hypernym relation are extracted and incorporated into a man-made lexical database, WordNet (Fellbaum, 1998), resulting in the extended WordNet, which has been augmented with over 400, 000 synsets. (Ponzetto and Strube, 2007) and (Suchanek et al., 2007) both mined Wikipedia to construct hierarchical structures of concepts and relations. While the former exploited Wikipedia category system as a conceptual network and extracted a taxonomy consisting of subsumption relations, the latter presented the YAGO ontology, which was automatically constructed by mining and combining Wikipedia and WordNet. A natural way to use these hierarchical structures to support taxonomic relation classification is to map t</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="31490" citStr="Freund and Schapire, 1999" startWordPosition="5243" endWordPosition="5246">d these names to some meaningful names which are used by all systems in our experiments. For example, terroristgroup is expanded to terrorist group, terrorism. Table 1 shows some pairs of terms which are generated. Four types of taxonomic relations are covered with balanced numbers of examples in all data sets. To evaluate our systems, we use a snapshot of Wikipedia from July, 2008. After cleaning and removing articles without categories (except redirect pages), 5,503,763 articles remain. We index these articles using Lucene9. As a learning algorithm, we use a regularized averaged Perceptron (Freund and Schapire, 1999). We compare TAREC with three systems that we built using recently developed large-scale hierarchical structures. Strube07 is built on the latest version of a taxonomy, TStrube, which was derived from Wikipedia (Ponzetto and Strube, 2007). It is worth noting that the structure of TStrube is similar to the page structure of Wikipedia. For a fair comparison, we first generate a semantic representation for each input term by following the same procedure used in TAREC described in Section 4. The titles and categories of the articles in the representation of each input term are then extracted. Only</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="4725" citStr="Hearst, 1992" startWordPosition="717" endWordPosition="718"> semantic representation of the terms and determines the taxonomic relations between them. This classifier will make use of existing knowledge bases in multiple ways, but will provide significantly larger coverage and more precise results. We make use of a dynamic resource such as Wikipedia to guarantee increased coverage without changing our model and also perform normalization-to-Wikipedia to find appropriate Wikipedia replacements for outside-Wikipedia terms. Moreover, stationary resources are usually brittle because of the way most of them are built: using local relational patterns (e.g. (Hearst, 1992; Snow et al., 2005)). Infrequent terms are less likely to be covered, and some relations may not be supported well by these methods because their corresponding terms rarely appear in close proximity (e.g., an Israeli tennis player Dudi Sela and Roger Federrer). Our approach uses search techniques to gather relevant Wikipedia pages of input terms and performs a learning-based classification w.r.t. to the features extracted from these pages as a way to get around this brittleness. Motivated by the needs of NLP applications such as RTE, QA, Summarization, and the compositionality argument allude</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. A. Hearst. 1992. Acquisition of hyponyms from large text corpora. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hotho</author>
<author>S Staab</author>
<author>G Stumme</author>
</authors>
<title>Ontologies improve text document clustering.</title>
<date>2003</date>
<booktitle>In ICDM.</booktitle>
<contexts>
<context position="1687" citStr="Hotho et al., 2003" startWordPosition="244" endWordPosition="247">t makes use of existing resources. Moreover, we develop a global constraint optimization inference process and use it to leverage an existing knowledge base also to enforce relational constraints among terms and thus improve the classifier predictions. Our experimental evaluation shows that our approach significantly outperforms other systems built upon existing well-known knowledge sources. 1 Introduction Taxonomic relations that are read off of structured ontological knowledge bases have been shown to play important roles in many computational linguistics tasks, such as document clustering (Hotho et al., 2003), navigating text databases (Chakrabarti et al., 1997), Question Answering (QA) (Saxena et al., 2007) and summarization (Vikas et al., 2008). It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2006). For example, it may be important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars. Work in Textual Entailment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, </context>
</contexts>
<marker>Hotho, Staab, Stumme, 2003</marker>
<rawString>A. Hotho, S. Staab, and G. Stumme. 2003. Ontologies improve text document clustering. In ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="3244" citStr="Kozareva et al., 2008" startWordPosition="490" endWordPosition="493"> also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007). However, identifying when these relations hold using fixed stationary hierarchical structures may be impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures. In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008; Baroni and Lenci, 2010), there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re1099 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099–1109, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics source with broader coverage. In the current work, we take a different approach, identifying directly whether a pair of terms hold a taxonomic relation. Fixed resources, as we observe, are inflexible when dealing with targeted </context>
<context position="9520" citStr="Kozareva et al., 2008" startWordPosition="1489" endWordPosition="1492">port taxonomic relation classification is to map targeted terms onto the hierarchies and check if they subsume each other or share a common subsumer. However, this approach is limited because constructed hierarchies may suffer from noise and require exact mapping (Section 6). TAREC overcomes these limitations by searching and selecting the top relevant articles in Wikipedia for each input term; taxonomic relations are then recognized based on the features extracted from these articles. On the other hand, information extraction bootstrapping algorithms, such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008), automatically harvest related terms on large corpora by starting with a few seeds of pre-specified relations (e.g. isa, part-of). Bootstrapping algorithms rely on some scoring function to assess the quality of terms and additional patterns extracted during bootstrapping iterations. Similarly, but with a different focus, Open IE, (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), deals with a large number of relations which are not pre-specified. Either way, the output of these algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa</context>
<context position="36843" citStr="Kozareva et al., 2008" startWordPosition="6122" endWordPosition="6125">Wikipedia are two different knowledge bases. We also do not use this procedure in Yago07 because in YAGO, a term is mapped onto the ontology by using the MEANS operator (in Pattern 1, Figure 3). This cannot follow our procedure. 6.2 Comparison to Harvested Knowledge As we discussed in Section 2, the output of bootstrapping-based algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa). For example, the full Espresso algorithm in (Pantel and Pennacchiotti, 2006) extracted 69,156 instances of is-a relation with 36.2% precision. Similarly, (Kozareva et al., 2008) evaluated only a small number (a few hundreds) of harvested instances. Recently, (Baroni and Lenci, 2010) proposed a general framework to extract properties of input terms. Their TypeDM model harvested 5,000 significant properties for each term out of 20,410 noun terms. For example, the properties of marine include (own, bomb), (use, gun). Using vector space models we could measure the similarity between terms using their property vectors. However, since the information available in TypeDM does not support predicting the ancestor relation between terms, we only evaluate TypeDM in classifying </context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2261" citStr="MacCartney and Manning, 2008" startWordPosition="338" endWordPosition="341">asks, such as document clustering (Hotho et al., 2003), navigating text databases (Chakrabarti et al., 1997), Question Answering (QA) (Saxena et al., 2007) and summarization (Vikas et al., 2008). It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2006). For example, it may be important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars. Work in Textual Entailment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such as the ancestor or sibling relations between terms. To date, these taxonomic relations can be read off manually generated ontologies such as Wordnet that explicitly represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007).</context>
</contexts>
<marker>MacCartney, Manning, 2008</marker>
<rawString>B. MacCartney and C. D. Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>An extended model of natural logic.</title>
<date>2009</date>
<booktitle>In IWCS-8.</booktitle>
<contexts>
<context position="2292" citStr="MacCartney and Manning, 2009" startWordPosition="342" endWordPosition="345">ing (Hotho et al., 2003), navigating text databases (Chakrabarti et al., 1997), Question Answering (QA) (Saxena et al., 2007) and summarization (Vikas et al., 2008). It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2006). For example, it may be important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars. Work in Textual Entailment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such as the ancestor or sibling relations between terms. To date, these taxonomic relations can be read off manually generated ontologies such as Wordnet that explicitly represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007). However, identifying when thes</context>
</contexts>
<marker>MacCartney, Manning, 2009</marker>
<rawString>B. MacCartney and C. D. Manning. 2009. An extended model of natural logic. In IWCS-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>B Van Durme</author>
</authors>
<title>Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<marker>Pas¸ca, Van Durme, 2008</marker>
<rawString>M. Pas¸ca and B. Van Durme. 2008. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>Organizing and searching the world wide web of facts step two: Harnessing the wisdom of the crowds.</title>
<date>2007</date>
<booktitle>In WWW.</booktitle>
<marker>Pas¸ca, 2007</marker>
<rawString>M. Pas¸ca. 2007. Organizing and searching the world wide web of facts step two: Harnessing the wisdom of the crowds. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="3221" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="485" endWordPosition="489">y represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007). However, identifying when these relations hold using fixed stationary hierarchical structures may be impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures. In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008; Baroni and Lenci, 2010), there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re1099 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099–1109, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics source with broader coverage. In the current work, we take a different approach, identifying directly whether a pair of terms hold a taxonomic relation. Fixed resources, as we observe, are inflexible when</context>
<context position="9496" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="1484" endWordPosition="1488">e hierarchical structures to support taxonomic relation classification is to map targeted terms onto the hierarchies and check if they subsume each other or share a common subsumer. However, this approach is limited because constructed hierarchies may suffer from noise and require exact mapping (Section 6). TAREC overcomes these limitations by searching and selecting the top relevant articles in Wikipedia for each input term; taxonomic relations are then recognized based on the features extracted from these articles. On the other hand, information extraction bootstrapping algorithms, such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008), automatically harvest related terms on large corpora by starting with a few seeds of pre-specified relations (e.g. isa, part-of). Bootstrapping algorithms rely on some scoring function to assess the quality of terms and additional patterns extracted during bootstrapping iterations. Similarly, but with a different focus, Open IE, (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), deals with a large number of relations which are not pre-specified. Either way, the output of these algorithms is usually limited to a small number of high-quality terms while sacrificing</context>
<context position="36742" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="6108" endWordPosition="6111"> thus, implicitly, may get some sense information. We do not use this procedure in Snow06 because WordNet and Wikipedia are two different knowledge bases. We also do not use this procedure in Yago07 because in YAGO, a term is mapped onto the ontology by using the MEANS operator (in Pattern 1, Figure 3). This cannot follow our procedure. 6.2 Comparison to Harvested Knowledge As we discussed in Section 2, the output of bootstrapping-based algorithms is usually limited to a small number of high-quality terms while sacrificing coverage (or vice versa). For example, the full Espresso algorithm in (Pantel and Pennacchiotti, 2006) extracted 69,156 instances of is-a relation with 36.2% precision. Similarly, (Kozareva et al., 2008) evaluated only a small number (a few hundreds) of harvested instances. Recently, (Baroni and Lenci, 2010) proposed a general framework to extract properties of input terms. Their TypeDM model harvested 5,000 significant properties for each term out of 20,410 noun terms. For example, the properties of marine include (own, bomb), (use, gun). Using vector space models we could measure the similarity between terms using their property vectors. However, since the information available in TypeDM doe</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In ACL, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Deriving a large scale taxonomy from wikipedia.</title>
<date>2007</date>
<publisher>AAAI.</publisher>
<contexts>
<context position="8465" citStr="Ponzetto and Strube, 2007" startWordPosition="1331" endWordPosition="1334">REC in Section 6 and conclude our paper in Section 7. 2 Related Work There are several works that aim at building taxonomies and ontologies which organize concepts and their taxonomic relations into hierarchical structures. (Snow et al., 2005; Snow et al., 2006) con1100 structed classifiers to identify hypernym relationship between terms from dependency trees of large corpora. Terms with recognized hypernym relation are extracted and incorporated into a man-made lexical database, WordNet (Fellbaum, 1998), resulting in the extended WordNet, which has been augmented with over 400, 000 synsets. (Ponzetto and Strube, 2007) and (Suchanek et al., 2007) both mined Wikipedia to construct hierarchical structures of concepts and relations. While the former exploited Wikipedia category system as a conceptual network and extracted a taxonomy consisting of subsumption relations, the latter presented the YAGO ontology, which was automatically constructed by mining and combining Wikipedia and WordNet. A natural way to use these hierarchical structures to support taxonomic relation classification is to map targeted terms onto the hierarchies and check if they subsume each other or share a common subsumer. However, this app</context>
<context position="31728" citStr="Ponzetto and Strube, 2007" startWordPosition="5280" endWordPosition="5283"> relations are covered with balanced numbers of examples in all data sets. To evaluate our systems, we use a snapshot of Wikipedia from July, 2008. After cleaning and removing articles without categories (except redirect pages), 5,503,763 articles remain. We index these articles using Lucene9. As a learning algorithm, we use a regularized averaged Perceptron (Freund and Schapire, 1999). We compare TAREC with three systems that we built using recently developed large-scale hierarchical structures. Strube07 is built on the latest version of a taxonomy, TStrube, which was derived from Wikipedia (Ponzetto and Strube, 2007). It is worth noting that the structure of TStrube is similar to the page structure of Wikipedia. For a fair comparison, we first generate a semantic representation for each input term by following the same procedure used in TAREC described in Section 4. The titles and categories of the articles in the representation of each input term are then extracted. Only titles and their corresponding categories that are in TStrube are considered. A term is an ancestor of the other if at least one of its titles is in the categories of the other term. If two terms share a common category, they are conside</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S. P. Ponzetto and M. Strube. 2007. Deriving a large scale taxonomy from wikipedia. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="24083" citStr="Punyakanok et al., 2008" startWordPosition="3943" endWordPosition="3946">rm networks consisting of two input terms (x, y), and additional terms z, w, v. The aforementioned observations show that if we can obtain additional terms that are related to the two target terms, we can enforce such coherency relational constraints and make a global prediction that would improve the prediction of the taxonomic relation between the two given terms. Our inference model follows constraint-based formulations that were introduced in the NLP community and were shown to be very effective in exploiting declarative background knowledge (Roth and Yih, 2004; Denis and Baldridge, 2007; Punyakanok et al., 2008; Chang et al., 2008). Figure 2: Examples of n-term networks with two input term x and y. (a) and (c) show valid combinations of edges, whereas (b) and (d) are two relational constraints. For simplicity, we do not draw no relation edges in (d). 5.1 Enforcing Coherency through Inference Let x, y be two input terms, and Z = {z1, z2,..., z,,,} be a set of additional terms. For a subset Z E Z, we construct a set of term networks whose nodes are x, y and all elements in Z, and the edge, e, between every two nodes is one of four taxonomic relations whose weight, w(e), is given by a local classifier </context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="7450" citStr="Roth and Yih, 2004" startWordPosition="1158" endWordPosition="1161">e model that makes use of relational constraints to enforce coherency among several related predictions. TAREC does not aim at building or extracting a hierarchical structure of concepts and relations, but rather to directly recognize taxonomic relations given a pair of terms. Target terms are represented using vector of features that are extracted from retrieved corresponding Wikipedia pages. In addition, we make use of existing stationary ontologies to find related terms to the target terms, and classify those too. This allows us to make use of a constraint-based inference model (following (Roth and Yih, 2004; Roth and Yih, 2007) that enforces coherency of decisions across related pairs (e.g., if x is-a y and y is-a z, it cannot be that x is a sibling of z). In the rest of the paper, after discussing related work in Section 2, we present an overview of TAREC in Section 3. The learning component and the inference model of TAREC are described in Sections 4 and 5. We experimentally evaluate TAREC in Section 6 and conclude our paper in Section 7. 2 Related Work There are several works that aim at building taxonomies and ontologies which organize concepts and their taxonomic relations into hierarchical</context>
<context position="24031" citStr="Roth and Yih, 2004" startWordPosition="3935" endWordPosition="3938">ations a term network. Figure 2 shows some n-term networks consisting of two input terms (x, y), and additional terms z, w, v. The aforementioned observations show that if we can obtain additional terms that are related to the two target terms, we can enforce such coherency relational constraints and make a global prediction that would improve the prediction of the taxonomic relation between the two given terms. Our inference model follows constraint-based formulations that were introduced in the NLP community and were shown to be very effective in exploiting declarative background knowledge (Roth and Yih, 2004; Denis and Baldridge, 2007; Punyakanok et al., 2008; Chang et al., 2008). Figure 2: Examples of n-term networks with two input term x and y. (a) and (c) show valid combinations of edges, whereas (b) and (d) are two relational constraints. For simplicity, we do not draw no relation edges in (d). 5.1 Enforcing Coherency through Inference Let x, y be two input terms, and Z = {z1, z2,..., z,,,} be a set of additional terms. For a subset Z E Z, we construct a set of term networks whose nodes are x, y and all elements in Z, and the edge, e, between every two nodes is one of four taxonomic relations</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Global inference for entity and relation identification via a linear programming formulation.</title>
<date>2007</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7471" citStr="Roth and Yih, 2007" startWordPosition="1162" endWordPosition="1165">se of relational constraints to enforce coherency among several related predictions. TAREC does not aim at building or extracting a hierarchical structure of concepts and relations, but rather to directly recognize taxonomic relations given a pair of terms. Target terms are represented using vector of features that are extracted from retrieved corresponding Wikipedia pages. In addition, we make use of existing stationary ontologies to find related terms to the target terms, and classify those too. This allows us to make use of a constraint-based inference model (following (Roth and Yih, 2004; Roth and Yih, 2007) that enforces coherency of decisions across related pairs (e.g., if x is-a y and y is-a z, it cannot be that x is a sibling of z). In the rest of the paper, after discussing related work in Section 2, we present an overview of TAREC in Section 3. The learning component and the inference model of TAREC are described in Sections 4 and 5. We experimentally evaluate TAREC in Section 6 and conclude our paper in Section 7. 2 Related Work There are several works that aim at building taxonomies and ontologies which organize concepts and their taxonomic relations into hierarchical structures. (Snow et</context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>D. Roth and W. Yih. 2007. Global inference for entity and relation identification via a linear programming formulation. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sammons</author>
<author>V G Vydiswaran</author>
<author>D Roth</author>
</authors>
<title>Ask not what textual entailment can do for you...</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6362" citStr="Sammons et al., 2010" startWordPosition="984" endWordPosition="988">rgest ever hauls, officials say. H: Nigeria seizes 80 tonnes of drugs. Similarly, it is important to know of a sibling relation to infer that a statement about Taiwan may (without additional information) contradict a similar statement with respect to Japan since these are different countries, as in the following: T: A strong earthquake struck off the southern tip of Taiwan at 12:26 UTC, triggering a warning from Japan’s Meteorological Agency that a 3.3 foot tsunami could be heading towards Basco, in the Philippines. H: An earthquake strikes Japan. Several recent TE studies (Abad et al., 2010; Sammons et al., 2010) suggest to isolate TE phenomena, such as recognizing taxonomic relations, and study them separately; they discuss some of characteristics of phenomena such as contradiction from a similar perspective to ours, but do not provide a solution. In this paper, we present TAxonomic RElation Classifier (TAREC), a system that classifies taxonomic relations between a given pair of terms using a machine learning based classifier. An integral part of TAREC is also our inference model that makes use of relational constraints to enforce coherency among several related predictions. TAREC does not aim at bui</context>
</contexts>
<marker>Sammons, Vydiswaran, Roth, 2010</marker>
<rawString>M. Sammons, V.G. Vydiswaran, and D. Roth. 2010. Ask not what textual entailment can do for you... In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sarmento</author>
<author>V Jijkuon</author>
<author>M de Rijke</author>
<author>E Oliveira</author>
</authors>
<title>more like these”: growing entity classes from seeds.</title>
<date>2007</date>
<booktitle>In CIKM.</booktitle>
<marker>Sarmento, Jijkuon, de Rijke, Oliveira, 2007</marker>
<rawString>L. Sarmento, V. Jijkuon, M. de Rijke, and E. Oliveira. 2007. ”more like these”: growing entity classes from seeds. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Saxena</author>
<author>G V Sambhu</author>
<author>S Kaushik</author>
<author>L V Subramaniam</author>
</authors>
<title>Iitd-ibmirl system for question answering using pattern matching, semantic type and semantic category recognition.</title>
<date>2007</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="1788" citStr="Saxena et al., 2007" startWordPosition="258" endWordPosition="261">rocess and use it to leverage an existing knowledge base also to enforce relational constraints among terms and thus improve the classifier predictions. Our experimental evaluation shows that our approach significantly outperforms other systems built upon existing well-known knowledge sources. 1 Introduction Taxonomic relations that are read off of structured ontological knowledge bases have been shown to play important roles in many computational linguistics tasks, such as document clustering (Hotho et al., 2003), navigating text databases (Chakrabarti et al., 1997), Question Answering (QA) (Saxena et al., 2007) and summarization (Vikas et al., 2008). It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2006). For example, it may be important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars. Work in Textual Entailment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recogn</context>
</contexts>
<marker>Saxena, Sambhu, Kaushik, Subramaniam, 2007</marker>
<rawString>A. K. Saxena, G. V. Sambhu, S. Kaushik, and L. V. Subramaniam. 2007. Iitd-ibmirl system for question answering using pattern matching, semantic type and semantic category recognition. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="4745" citStr="Snow et al., 2005" startWordPosition="719" endWordPosition="722">esentation of the terms and determines the taxonomic relations between them. This classifier will make use of existing knowledge bases in multiple ways, but will provide significantly larger coverage and more precise results. We make use of a dynamic resource such as Wikipedia to guarantee increased coverage without changing our model and also perform normalization-to-Wikipedia to find appropriate Wikipedia replacements for outside-Wikipedia terms. Moreover, stationary resources are usually brittle because of the way most of them are built: using local relational patterns (e.g. (Hearst, 1992; Snow et al., 2005)). Infrequent terms are less likely to be covered, and some relations may not be supported well by these methods because their corresponding terms rarely appear in close proximity (e.g., an Israeli tennis player Dudi Sela and Roger Federrer). Our approach uses search techniques to gather relevant Wikipedia pages of input terms and performs a learning-based classification w.r.t. to the features extracted from these pages as a way to get around this brittleness. Motivated by the needs of NLP applications such as RTE, QA, Summarization, and the compositionality argument alluded to above, we focus</context>
<context position="8081" citStr="Snow et al., 2005" startWordPosition="1273" endWordPosition="1276">, 2007) that enforces coherency of decisions across related pairs (e.g., if x is-a y and y is-a z, it cannot be that x is a sibling of z). In the rest of the paper, after discussing related work in Section 2, we present an overview of TAREC in Section 3. The learning component and the inference model of TAREC are described in Sections 4 and 5. We experimentally evaluate TAREC in Section 6 and conclude our paper in Section 7. 2 Related Work There are several works that aim at building taxonomies and ontologies which organize concepts and their taxonomic relations into hierarchical structures. (Snow et al., 2005; Snow et al., 2006) con1100 structed classifiers to identify hypernym relationship between terms from dependency trees of large corpora. Terms with recognized hypernym relation are extracted and incorporated into a man-made lexical database, WordNet (Fellbaum, 1998), resulting in the extended WordNet, which has been augmented with over 400, 000 synsets. (Ponzetto and Strube, 2007) and (Suchanek et al., 2007) both mined Wikipedia to construct hierarchical structures of concepts and relations. While the former exploited Wikipedia category system as a conceptual network and extracted a taxonomy </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2814" citStr="Snow et al., 2006" startWordPosition="424" endWordPosition="427">lment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such as the ancestor or sibling relations between terms. To date, these taxonomic relations can be read off manually generated ontologies such as Wordnet that explicitly represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007). However, identifying when these relations hold using fixed stationary hierarchical structures may be impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures. In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008; Baroni and Lenci, 2010), there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re</context>
<context position="8101" citStr="Snow et al., 2006" startWordPosition="1277" endWordPosition="1280">es coherency of decisions across related pairs (e.g., if x is-a y and y is-a z, it cannot be that x is a sibling of z). In the rest of the paper, after discussing related work in Section 2, we present an overview of TAREC in Section 3. The learning component and the inference model of TAREC are described in Sections 4 and 5. We experimentally evaluate TAREC in Section 6 and conclude our paper in Section 7. 2 Related Work There are several works that aim at building taxonomies and ontologies which organize concepts and their taxonomic relations into hierarchical structures. (Snow et al., 2005; Snow et al., 2006) con1100 structed classifiers to identify hypernym relationship between terms from dependency trees of large corpora. Terms with recognized hypernym relation are extracted and incorporated into a man-made lexical database, WordNet (Fellbaum, 1998), resulting in the extended WordNet, which has been augmented with over 400, 000 synsets. (Ponzetto and Strube, 2007) and (Suchanek et al., 2007) both mined Wikipedia to construct hierarchical structures of concepts and relations. While the former exploited Wikipedia category system as a conceptual network and extracted a taxonomy consisting of subsum</context>
<context position="33098" citStr="Snow et al., 2006" startWordPosition="5495" endWordPosition="5498"> were 50 semantic classes in the original dataset. We grouped some semantically similar classes for the purpose of classifying taxonomic relations. 8Published at http://cogcomp.cs.illinois.edu/page/software 9http://lucene.apache.org, version 2.3.2 Test-I Test-II Strube07 24.32 25.63 Snow06 41.97 36.26 Yago07 65.93 70.63 TAREC (local) 81.89 84.7 TAREC 85.34 86.98 Table 4: Evaluating and comparing performances, in accuracy, of the systems on Test-I and Test-II. TAREC (local) uses only our local classifier to identify taxonomic relations by choosing the relation with highest confidence. WordNet (Snow et al., 2006). Words in the extended WordNet can be common nouns or proper nouns. Given two input terms, we first map them onto the hierarchical structure of the extended WordNet by exact string matching. A term is an ancestor of the other if it can be found as an hypernym after going up K levels in the hierarchy from the other term. If two terms share a common subsumer within some levels, then they are considered as siblings. Otherwise, there is no relation between the two input terms. Similar to Strube07, we first check ancestor, then sibling, and finally no relation. Yago07 uses the YAGO ontology (Sucha</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Suchanek</author>
<author>G Kasneci</author>
<author>G Weikum</author>
</authors>
<title>Yago: A Core of Semantic Knowledge.</title>
<date>2007</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="2860" citStr="Suchanek et al., 2007" startWordPosition="432" endWordPosition="435">rtney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such as the ancestor or sibling relations between terms. To date, these taxonomic relations can be read off manually generated ontologies such as Wordnet that explicitly represent these, and there has also been some work trying to extend the manually built resources using automatic acquisition methods resulting in structured knowledge bases such as the Extended WordNet (Snow et al., 2006) and the YAGO ontology (Suchanek et al., 2007). However, identifying when these relations hold using fixed stationary hierarchical structures may be impaired by noise in the resource and by uncertainty in mapping targeted terms to concepts in the structures. In addition, for knowledge sources derived using bootstrapping algorithms and distributional semantic models such as (Pantel and Pennacchiotti, 2006; Kozareva et al., 2008; Baroni and Lenci, 2010), there is typically a trade-off between precision and recall, resulting either in a relatively accurate resource with low coverage or a noisy re1099 Proceedings of the 2010 Conference on Emp</context>
<context position="8493" citStr="Suchanek et al., 2007" startWordPosition="1336" endWordPosition="1339">r paper in Section 7. 2 Related Work There are several works that aim at building taxonomies and ontologies which organize concepts and their taxonomic relations into hierarchical structures. (Snow et al., 2005; Snow et al., 2006) con1100 structed classifiers to identify hypernym relationship between terms from dependency trees of large corpora. Terms with recognized hypernym relation are extracted and incorporated into a man-made lexical database, WordNet (Fellbaum, 1998), resulting in the extended WordNet, which has been augmented with over 400, 000 synsets. (Ponzetto and Strube, 2007) and (Suchanek et al., 2007) both mined Wikipedia to construct hierarchical structures of concepts and relations. While the former exploited Wikipedia category system as a conceptual network and extracted a taxonomy consisting of subsumption relations, the latter presented the YAGO ontology, which was automatically constructed by mining and combining Wikipedia and WordNet. A natural way to use these hierarchical structures to support taxonomic relation classification is to map targeted terms onto the hierarchies and check if they subsume each other or share a common subsumer. However, this approach is limited because con</context>
<context position="22086" citStr="Suchanek et al., 2007" startWordPosition="3602" endWordPosition="3605">ses used in the titles of one term and the categories of the other term. In our context, a phrase is 4pmi is different than mutual information. The former applies to specific outcomes, while the latter is to measure the mutual dependence of two random variables. Nf(x, y) = logf(x)f(y), 1103 considered to be a common phrase if it appears in the titles of one term and the categories of the other term and it is also of the following types: (1) the whole string of a category, or (2) the head in the root form of a category, or (3) the post-modifier of a category. We use the Noun Group Parser from (Suchanek et al., 2007) to extract the head and post-modifier from a category. For example, one of the categories of an article about Chicago is Cities in Illinois. This category can be parsed into a head in its root form City, and a post-modifier Illinois. Given term pair (City, Chicago), we observe that City matches the head of the category Cities in Illinois of term Chicago. This is a strong indication that Chicago is a child of City. We also use a feature that captures the overlap ratio of common phrases between the categories of two input terms. For this feature, we do not use the post-modifier of the categorie</context>
<context position="27693" citStr="Suchanek et al., 2007" startWordPosition="4627" endWordPosition="4630">ting) in the training data, which is augmented with additional terms. Equation (2) finds the best taxonomic relation of two input terms by computing the average score of every group of the best term networks representing a particular relation of two input terms. 5.2 Extracting Related Terms In the inference model, we need to obtain other terms that are related to the two input terms. Hereafter, we refer to additional terms as related terms. The related term space is a space of direct ancestors, siblings and children in a particular resource. We propose an approach that uses the YAGO ontology (Suchanek et al., 2007) to provide related terms. It is worth noting that YAGO is chosen over the Wikipedia category system used in our work because YAGO is a clean ontology built by carefully combining Wikipedia and WordNet.5 In YAGO model, all objects (e.g. cities, people, etc.) are represented as entities. To map our input terms to entities in YAGO, we use the MEANS relation defined in the YAGO ontology. Furthermore, similar entities are grouped into classes. This allows us to obtain direct ancestors of an entity by using the TYPE relation which gives the entity’s classes. Furthermore, we can get ancestors of a c</context>
<context position="33715" citStr="Suchanek et al., 2007" startWordPosition="5604" endWordPosition="5607">2006). Words in the extended WordNet can be common nouns or proper nouns. Given two input terms, we first map them onto the hierarchical structure of the extended WordNet by exact string matching. A term is an ancestor of the other if it can be found as an hypernym after going up K levels in the hierarchy from the other term. If two terms share a common subsumer within some levels, then they are considered as siblings. Otherwise, there is no relation between the two input terms. Similar to Strube07, we first check ancestor, then sibling, and finally no relation. Yago07 uses the YAGO ontology (Suchanek et al., 2007) as its main source of background knowledge. Because the YAGO ontology is a combination of Wikipedia and WordNet, this system is expected to perform well at recognizing taxonomic relations. To access a term’s ancestors and siblings, we use patterns 1 and 2 in Figure 3 to map a term to the ontology and move up on the ontology. The relation identification process is then similar to those of Snow06 and Strube07. If an input term is not recognized by these systems, they return no relation. Our overall algorithm, TAREC, is described in Figure 1. We manually construct a pre-defined list of 35 relati</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>F. M. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago: A Core of Semantic Knowledge. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Vikas</author>
<author>A K Meshram</author>
<author>G Meena</author>
<author>A Gupta</author>
</authors>
<title>Multiple document summarization using principal component analysis incorporating semantic vector space model.</title>
<date>2008</date>
<booktitle>In Computational Linguistics and Chinese Language Processing.</booktitle>
<contexts>
<context position="1827" citStr="Vikas et al., 2008" startWordPosition="264" endWordPosition="267">g knowledge base also to enforce relational constraints among terms and thus improve the classifier predictions. Our experimental evaluation shows that our approach significantly outperforms other systems built upon existing well-known knowledge sources. 1 Introduction Taxonomic relations that are read off of structured ontological knowledge bases have been shown to play important roles in many computational linguistics tasks, such as document clustering (Hotho et al., 2003), navigating text databases (Chakrabarti et al., 1997), Question Answering (QA) (Saxena et al., 2007) and summarization (Vikas et al., 2008). It is clear that the recognition of taxonomic relation between terms in sentences is essential to support textual inference tasks such as Recognizing Textual Entailment (RTE) (Dagan et al., 2006). For example, it may be important to know that a blue Toyota is neither a red Toyota nor a blue Honda, but that all are cars, and even Japanese cars. Work in Textual Entailment has argued quite convincingly (MacCartney and Manning, 2008; MacCartney and Manning, 2009) that many such textual inferences are largely compositional and depend on the ability to recognize some basic taxonomic relations such</context>
</contexts>
<marker>Vikas, Meshram, Meena, Gupta, 2008</marker>
<rawString>O. Vikas, A. K. Meshram, G. Meena, and A. Gupta. 2008. Multiple document summarization using principal component analysis incorporating semantic vector space model. In Computational Linguistics and Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vyas</author>
<author>P Pantel</author>
</authors>
<title>Semi-automatic entity set refinement.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="30435" citStr="Vyas and Pantel, 2009" startWordPosition="5082" endWordPosition="5085">self is weaker than our approach in identifying taxonomic relations (see Section 6.) 6These relations are defined in the YAGO ontology. Pattern 1 Pattern 3 Pattern 2 YAGO Query Patterns INPUT: term “x” OUTPUT: lists of ancestors, siblings, and children of “x” “x” MEANS ?A “x” MEANS ?A “x” MEANS ?D ?A SUBCLASSOF ?B ?C SUBCLASSOF ?B ?A TYPE ?B ?C TYPE ?B ?E TYPE ?D RETURN: ?B, ?C, ?E as lists of ancestors, siblings, and children, respectively. r* = argmaxr |7 |1: At.score(t*) (2) r t*ETr 1105 dataset Test-I. Dataset-II is generated from 44 semantic classes of more than 10,000 instances used in (Vyas and Pantel, 2009)7. The original semantic classes and instances were extracted from Wikipedia lists. This data, therefore, only contains terms with corresponding Wikipedia pages. We also generate disjoint training and test sets of 8,000 and 12,000 pairs of terms, respectively, and call the test set of this dataset Test-II.8 Several semantic class names in the original data are written in short forms (e.g. chemicalelem, proglanguage). We expand these names to some meaningful names which are used by all systems in our experiments. For example, terroristgroup is expanded to terrorist group, terrorism. Table 1 sho</context>
</contexts>
<marker>Vyas, Pantel, 2009</marker>
<rawString>V. Vyas and P. Pantel. 2009. Semi-automatic entity set refinement. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised woed sense disambiguation rivaling supervied methods.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL-95.</booktitle>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised woed sense disambiguation rivaling supervied methods. In Proceedings of ACL-95.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>