<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000734">
<title confidence="0.9976">
A Bidirectional Grammar-Based Medical Speech Translator
</title>
<author confidence="0.8558745">
Pierrette Bouillon&apos;, Glenn Flores2, Marianne Starlander&apos;, Nikos Chatzichrisafis&apos;
Marianne Santaholma&apos;, Nikos Tsourakis&apos;, Manny Rayner&apos;,3, Beth Ann Hockey4
</author>
<affiliation confidence="0.987081">
&apos; University of Geneva, TIM/ISSCO, 40 bvd du Pont-d’Arve, CH-1211 Geneva 4, Switzerland
</affiliation>
<email confidence="0.802389333333333">
Pierrette.Bouillon@issco.unige.ch
Marianne.Starlander@eti.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Nikolaos.Tsourakis@issco.unige.ch
</email>
<affiliation confidence="0.587049">
2 Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226
</affiliation>
<email confidence="0.929961">
gflores@mcw.edu
</email>
<affiliation confidence="0.43499">
3 Powerset, Inc., 475 Brannan Street, San Francisco, CA 94107
</affiliation>
<email confidence="0.936373">
manny@powerset.com
</email>
<note confidence="0.427662">
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035–1000
</note>
<email confidence="0.993577">
bahockey@ucsc.edu
</email>
<sectionHeader confidence="0.995518" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998205">
We describe a bidirectional version of the
grammar-based MedSLT medical speech
system. The system supports simple medi-
cal examination dialogues about throat pain
between an English-speaking physician and
a Spanish-speaking patient. The physician’s
side of the dialogue is assumed to consist
mostly of WH-questions, and the patient’s of
elliptical answers. The paper focusses on the
grammar-based speech processing architec-
ture, the ellipsis resolution mechanism, and
the online help system.
</bodyText>
<sectionHeader confidence="0.99054" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999941162790698">
There is an urgent need for medical speech trans-
lation systems. The world’s current population
of 6.6 billion speaks more than 6,000 languages
(Graddol, 2004). Language barriers are associated
with a wide variety of deleterious consequences in
healthcare, including impaired health status, a lower
likelihood of having a regular physician, lower rates
of mammograms, pap smears, and other preven-
tive services, non-adherence with medications, a
greater likelihood of a diagnosis of more severe psy-
chopathology and leaving the hospital against med-
ical advice among psychiatric patients, a lower like-
lihood of being given a follow-up appointment af-
ter an emergency department visit, an increased risk
of intubation among children with asthma, a greater
risk of hospital admissions among adults, an in-
creased risk of drug complications, longer medical
visits, higher resource utilization for diagnostic test-
ing, lower patient satisfaction, impaired patient un-
derstanding of diagnoses, medications, and follow-
up, and medical errors and injuries (Flores, 2005;
Flores, 2006). Nevertheless, many patients who
need medical interpreters do not get them. For ex-
ample, in the United States, where 52 million peo-
ple speak a language other than English at home
and 23 million people have limited English profi-
ciency (LEP) (Census, 2007), one study found that
about half of LEP patients presenting to an emer-
gency department were not provided with a medical
interpreter (Baker et al., 1996). There is thus a sub-
stantial gap between the need for and availability of
language services in health care, a gap that could be
bridged through effective medical speech translation
systems.
An ideal system would be able to interpret ac-
curately and flexibly between patients and health
care professionals, using unrestricted language and
a large vocabulary. A system of this kind is, un-
fortunately, beyond the current state of the art.
It is, however, possible, using today’s technol-
ogy, to build speech translation systems for specific
scenarios and language-pairs, which can achieve
acceptable levels of reliability within the bounds
</bodyText>
<page confidence="0.99444">
41
</page>
<note confidence="0.58583">
Proceedings of SPEECHGRAM 2007, pages 41–48,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999784083333334">
of a well-defined controlled language. MedSLT
(Bouillon et al., 2005) is an Open Source system
of this type, which has been under construction at
Geneva University since 2003. The system is built
on top of Regulus (Rayner et al., 2006), an Open
Source platform which supports development of
grammar-based speech-enabled applications. Regu-
lus has also been used to build several other systems,
including NASA’s Clarissa (Rayner et al., 2005b).
The most common architecture for speech trans-
lation today uses statistical methods to perform both
speech recognition and translation, so it is worth
clarifying why we have chosen to use grammar-
based methods. Even though statistical architec-
tures exhibit many desirable properties (purely data-
driven, domain independent), this is not necessar-
ily the best alternative in safety-critical medical ap-
plications. Anecdotally, many physicians express
reluctance to trust a translation device whose out-
put is not readily predictable, and most of the
speech translation systems which have reached the
stage of field testing rely on various types of
grammar-based recognition and rule-based transla-
tion (Phraselator, 2007; Fluential, 2007).
Statistical speech recognisers can achieve impres-
sive levels of accuracy when trained on enough data,
but it is a daunting task to collect training mate-
rial in the requisite quantities (usually, tens of thou-
sands of high-quality utterances) when trying to
build practical systems. Considering that the medi-
cal speech translation applications we are interested
in constructing here need to work for multiple lan-
guages and subdomains, the problem becomes even
more challenging. Our experience is that grammar-
based systems which also incorporate probabilistic
context-free grammar tuning deliver better results
than purely statistical ones when training data are
sparse (Rayner et al., 2005a).
Another common criticism of grammar-based
systems is that out-of-coverage utterances will
neither be recognized nor translated, an objec-
tion that critics have sometimes painted as de-
cisive. It is by no means obvious, however,
that restricted coverage is such a serious prob-
lem. In text processing, work on several gener-
ations of controlled language systems has devel-
oped a range of techniques for keeping users within
the bounds of system coverage (Kittredge, 2003;
Mitamura, 1999), and variants of these methods can
also be adapted for spoken language applications.
Our experiments with MedSLT show that even a
quite simple help system is enough to guide users
quickly towards the intended coverage of a medium-
vocabulary grammar-based speech translation appli-
cation, with most users appearing confident after just
an hour or two of exposure (Starlander et al., 2005;
Chatzichrisafis et al., 2006).
Until recently, the MedSLT system only sup-
ported unidirectional processing in the physician
to patient direction. The assumption was that the
physician would mostly ask yes/no questions, to
which the patient would respond non-verbally, for
example by nodding or shaking their head. A uni-
directional architecture is easier to make habitable
than a bidirectional one. It is reasonable to as-
sume that the physician will use the system regu-
larly enough to learn the coverage, but most patients
will not have used the system before, and it is less
clear that they will be able to acclimatize within the
narrow window at their disposal. These consider-
ations must however be balanced against the fact
that a unidirectional system does not allow for a
patient-centered interaction characterized by mean-
ingful patient-clinician communication or shared de-
cision making. Multiple studies in the medical lit-
erature document that patient-centeredness, effec-
tive patient-clinician communication, and shared de-
cision making are associated with significant im-
provements in patient health outcomes, including
reduced anxiety levels, improved functional sta-
tus, reduced pain, better control of diabetes melli-
tus, blood pressure reduction among hypertensives,
improved adherence, increased patient satisfaction,
and symptom reduction for a variety of conditions
(Stewart, 1995; Michie et al., 2003). Abidirectional
system is considered close to essential from a health-
care perspective, since it appropriately addresses the
key issues of patient centeredness and shared de-
cision making. For these reasons, we have over
the last few months developed a bidirectional ver-
sion of MedSLT, initially focussing on a throat pain
scenario with an English-speaking physician and a
Spanish-speaking patient. The physician uses full
sentences, while the patient answers with short re-
sponses.
One of the strengths of the Regulus approach is
</bodyText>
<page confidence="0.994911">
42
</page>
<bodyText confidence="0.999537666666667">
that it is very easy to construct parallel versions of
a grammar; generally, all that is required is to vary
the training corpus. (We will have more to say about
this soon). We have exploited these properties of
the platform to create two different configurations
of the bidirectional system, so that we can compare
competing approaches to the problem of accommo-
dating patients unfamiliar with speech technology.
In Version 1 (less restricted), the patient is allowed
to answer using both elliptical utterances and short
sentences, while in Version 2 (more restricted) they
are only permitted to use elliptical utterances. Thus,
for example, if the physician asks the question “How
long have you had a sore throat?”, Version 1 allows
the patient to respond both “Desde algunos dias”
(“For several days”) and “Me ha dolido la garganta
desde algunos dias” (“I have had a sore throat for
several days”), while Version 2 only allows the first
of these. Both the short and the long versions are
translated uniformly, with the short version resolved
using the context from the preceding question.
In both versions, if the patient finds it too chal-
lenging to use the system to answer WH-questions
directly, it is possible to back off to the earlier di-
alogue architecture in which the physician uses Y-
N questions and the patient responds with simple
yes/no answers, or nonverbally. Continuing the ex-
ample, if the patient is unable to find an appro-
priate way to answer the physician’s question, the
physician could ask “Have you had a sore throat for
more than three days?”; if the patient responds nega-
tively, they could continue with the follow-on ques-
tion “More than a week?”, and so on.
In the rest of the paper, we first describe the
system top-level (Section 2), the way in which
grammar-based processing is used (Section 3), the
ellipsis processing mechanism (Section 4), and the
help system (Section 5). Section 6 presents an ini-
tial evaluation, and the final section concludes.
</bodyText>
<sectionHeader confidence="0.969501" genericHeader="introduction">
2 Top-level architecture
</sectionHeader>
<bodyText confidence="0.999990608695652">
The system is operated through the graphical user
interface (GUI) shown in Figures 1 and 2. In
accordance with the basic principles of patient-
centeredness and shared decision-making outlined
in Section 1, the patient and the physician each have
their own headset, use their own mouse, and share
the same view of the screen. This is in sharp contrast
to the majority of the medical speech translation sys-
tems described in the literature (Somers, 2006).
As shown in the screenshots, the main GUI win-
dow is separated into two tabbed panes, marked
“Doctor” and “Patient”. Initially, the “Doctor” view
(the one shown in Figure 1) is active. The physician
presses the “Push to talk” button, and speaks into
the headset microphone. If recognition is success-
ful, the GUI displays four separate results, listed on
the right side of the screen. At the top, immediately
under the heading “Question”, we can see the actual
words returned by speech recognition. Here, these
words are “Have you had rapid strep test”. Below,
we have the help pane: this displays similar ques-
tions taken from the help corpus, which are known to
be within system coverage. The pane marked “Sys-
tem understood” shows a back-translation, produced
by first translating the recognition result into inter-
lingua, and then translating it back into English. In
the present example, this corrects the minor mistake
the recogniser has made, missing the indefinite ar-
ticle “a”, and confirms that the system has obtained
a correct grammatical analysis and interpretation at
the level of interlingua. At the bottom, we see the
target language translation. The left-hand side of the
screen logs the history of the conversation to date, so
that both sides can refer back to it.
If the physician decides that the system has cor-
rectly understood what they said, they can now press
the “Play” button. This results in the system produc-
ing a spoken output, using the Vocalizer TTS engine.
Simultaneously with speaking, the GUI shifts to the
“Patient” configuration shown in Figure 2. This dif-
fers from the “Doctor” configuration in two respects:
all text is in the patient language, and the help pane
presents its suggestions immediately, based on the
preceding physician question. The various process-
ing components used to support these functionalities
are described in the following sections.
</bodyText>
<sectionHeader confidence="0.997799" genericHeader="method">
3 Grammar-based processing
</sectionHeader>
<bodyText confidence="0.9998316">
Grammar-based processing is used for source-
language speech recognition and target-side genera-
tion. (Source-language analysis is part of the recog-
nition process, since grammar-based recognition in-
cludes creating a parse). All of these functionalities
</bodyText>
<page confidence="0.999687">
43
</page>
<figureCaption confidence="0.95281625">
Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressed
the “Play” button. The help pane shows similar queries known to be within coverage.
Figure 2: Screenshot showing the state of the GUI after the physician has pressed the “Play” button. The
help pane shows known valid responses to similar questions.
</figureCaption>
<page confidence="0.995084">
44
</page>
<bodyText confidence="0.999960147727274">
are implemented using the Regulus platform, with
the task-specific grammars compiled out of general
feature grammar resources by the Regulus tools. For
both recognition and generation, the first step is
to extract a domain-specific feature grammar from
the general one, using a version of the Explanation
Based Learning (EBL) algorithm.
The extraction process is driven by a corpus of ex-
amples and a set of “operationality criteria”, which
define how the rules in the original resource gram-
mar are recombined into domain-specific ones. It is
important to realise that the domain-specific gram-
mar is not merely a subset of the resource grammar;
a typical domain-specific grammar rule is created by
merging two to five resource grammar rules into a
single “flatter” rule. The result is a feature gram-
mar which is less general than the original one, but
more efficient. For recognition, the grammar is then
processed further into a CFG language model, using
an algorithm which alternates expansion of feature
values and filtering of the partially expanded gram-
mar to remove irrelevant rules. Detailed descrip-
tions of the EBL learning and feature grammar —*
CFG compilation algorithms can be found in Chap-
ters 8 and 10 of (Rayner et al., 2006). Regulus fea-
ture grammars can also be compiled into generators
using a version of the Semantic Head Driven algo-
rithm (Shieber et al., 1990).
The English (physician) side recogniser is com-
piled from the large English resource grammar de-
scribed in Chapter 9 of (Rayner et al., 2006), and
was constructed in the same way as the one de-
scribed in (Rayner et al., 2005a), which was used for
a headache examination task. The operationality cri-
teria are the same, and the only changes are a differ-
ent training corpus and the addition of new entries
to the lexicon. The same resources, with a differ-
ent training corpus, were used to build the English
language generator. It is worth pointing out that, al-
though a uniform method was used to build these
various grammars, the results were all very differ-
ent. For example, the recognition grammar from
(Rayner et al., 2005a) is specialised to cover only
second-person questions (“Do you get headaches
in the mornings?”), while the generator grammar
used in the present application covers only first-
person declarative statements (“I visited the doctor
last Monday.”). In terms of structure, each gram-
mar contains several important constructions that the
other lacks. For example, subordinate clauses are
central in the headache domain (“Do the headaches
occur when you are stressed?”) but are not present
in the sore throat domain; this is because the stan-
dard headache examination questions mostly focus
on generic conditions, while the sore throat exami-
nation questions only relate to concrete ones. Con-
versely, relative clauses are important in the sore
throat domain (“I have recently been in contact with
someone who has strep throat”), but are not suffi-
ciently important in the headache domain to be cov-
ered there.
On the Spanish (patient) side, there are four
grammars involved. For recognition, we have
two different grammars, corresponding to the two
versions of the system; the grammar for Ver-
sion 2 is essentially a subset of that for Version
1. For generation, there are two separate and
quite different grammars: one is used for trans-
lating the physician’s questions, while the other
produces back-translations of the patient’s ques-
tions. All of these grammars are extracted from
a general shared resource grammar for Romance
languages, which currently combines rules for
French, Spanish and Catalan (Bouillon et al., 2006;
Bouillon et al., to appear 2007b).
One interesting consequence of our methodology
is related to the fact that Spanish is a prodrop lan-
guage, which implies that many sentences are sys-
tematically ambiguous between declarative and Y-N
question readings. For example, “He consultado un
m´edico” could in principle mean either “I visited a
doctor” or “Did I visit a doctor?”. When training the
specialised Spanish grammars, it is thus necessary to
specify which readings of the training sentences are
to be used. Continuing the example, if the sentence
occurred in training material for the answer gram-
mar, we would specify that the declarative reading
was the intended one1.
</bodyText>
<sectionHeader confidence="0.967091" genericHeader="method">
4 Ellipsis processing and contextual
interpretation
</sectionHeader>
<bodyText confidence="0.9947665">
In Version 1 of the system, the patient is per-
mitted to answer using elliptical phrases; in Ver-
</bodyText>
<footnote confidence="0.913183">
1The specification can be formulated as a preference that
applies uniformly to all the training examples in a given group.
</footnote>
<page confidence="0.99948">
45
</page>
<bodyText confidence="0.999652235294118">
sion 2, she is obliged to do so. Ability to pro-
cess elliptical responses makes it easier to guide the
patient towards the intended coverage of the sys-
tem, without degrading the quality of recognition
(Bouillon et al., to appear 2007a). The downside is
that ellipses are also harder to translate than full sen-
tences. Even in a limited domain like ours, and in a
closely related language-pair, ellipsis can generally
not be translated word for word, and it is necessary
to look at the preceding context if the rules are to
be applied correctly. In examples 1 and 2 below,
the locative phrase “In your stomach” in the English
source becomes the subject in the Spanish transla-
tion. This implies that the translation of the ellipsis
in the second physician utterance needs to change
syntactic category: “In your head” (PP) becomes
“La cabeza” (NP) .
</bodyText>
<listItem confidence="0.99995775">
(1) Doctor: Do you have a pain in your
stomach?
(Trans): Le duele el estomago?
(2) Doctor: In your head?
</listItem>
<bodyText confidence="0.98663608">
(Trans): *En la cabeza?
Since examples like this are frequent, our sys-
tem implements a solution in which the patient’s
replies are translated in the context of the preced-
ing utterance. If the patient-side recogniser’s output
is classified as an ellipsis (this can done fairly reli-
ably thanks to use of suitably specialised grammars;
cf. Section 3), we expand the incomplete phrase
into a full sentence structure by adding appropriate
structural elements from the preceding physician-
side question; the expanded semantic structure is the
one which is then translated into interlingual form,
and thence back to the physician-side language.
Since all linguistic representations, including
those of elliptical phrases and their contexts, are rep-
resented as flat attribute-value lists, we are able to
implement the resolution algorithm very simply in
terms of list manipulation. In YN-questions, where
the elliptical answer intuitively adds information to
the question (“Did you visit the doctor?”; “El lunes”
--+ “I visited the doctor on Monday”), the repre-
sentations are organised so that resolution mainly
amounts to concatenation of the two lists2. In WH-
questions, where the answer intuitively substitutes
the elliptical answer for the WH-phrase (“What is
</bodyText>
<footnote confidence="0.9938865">
2It is also necessary to replace second-person pronouns with
first-person counterparts.
</footnote>
<bodyText confidence="0.999785947368421">
your temperature?”; “Cuarenta grados” —* “My tem-
perature is forty degrees”), resolution substitutes the
representation of the elliptical phrase for that of a
semantically similar element in the question.
The least trivial aspect of this process is provid-
ing a suitable definition of “semantically similar”.
This is done using a simple example-based method,
in which the grammar developer writes a set of dec-
larations, each of which lists a set of semantically
similar NPs. At compile-time, the grammar is used
to parse each NP, and extract a generalised skele-
ton, in which specific lexical information is stripped
away; at run-time, two NPs are held to be semanti-
cally similar if they can each be unified with skele-
tons in the same equivalence class. This ensures that
the definition of the semantic similarity relation is
stable across most changes to the grammar and lex-
icon. The issues are described in greater detail in
(Bouillon et al., to appear 2007a).
</bodyText>
<sectionHeader confidence="0.928959" genericHeader="method">
5 Help system
</sectionHeader>
<bodyText confidence="0.9995765">
Since the performance of grammar-based speech un-
derstanding is only reliable on in-coverage mate-
rial, systems based on this type of architecture must
necessarily use a controlled language approach, in
which it is assumed that the user is able to learn the
relevant coverage. As previously noted, the Med-
SLT system addresses this problem by incorporat-
ing an online help system (Starlander et al., 2005;
Chatzichrisafis et al., 2006).
On the physician side, the help system offers, af-
ter each recognition event, a list of related ques-
tions; similarly, on the patient side, it provides ex-
amples of known valid answers to the current ques-
tion. In both cases, the help examples are extracted
from a precompiled corpus of question-answer pairs,
which have been judged for correctness by system
developers. The process of selecting the examples
is slightly different on the two sides. For questions
(physician side), the system performs a second par-
allel recognition of the input speech, using a sta-
tistical recogniser. It then compares the recogni-
tion result, using an N-gram based metric, against
the set of known correct in-coverage questions from
the question-answer corpus, to extract the most sim-
ilar ones. For answers (patient side), the help sys-
tem searches the question-answer corpus to find the
</bodyText>
<page confidence="0.99845">
46
</page>
<bodyText confidence="0.9970336">
questions most similar to the current one, and shows
the list of corresponding valid answers, using the
whole list in the case of Version 1 of the system, and
only the subset consisting of elliptical phrases in the
case of Version 2.
</bodyText>
<sectionHeader confidence="0.994196" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9999728125">
In previous studies, we have evaluated speech
recognition and speech understanding per-
formance for physician-side questions in
English (Bouillon et al., 2005) and Spanish
(Bouillon et al., to appear 2007b), and investi-
gated the impact on performance of the help system
(Rayner et al., 2005a; Starlander et al., 2005). We
have also carried out recent evaluations designed to
contrast recognition performance on elliptical and
full versions of the same utterance; here, our results
suggest that elliptical forms of (French-language)
MedSLT utterances are slightly easier to recognise
in terms of semantic error rate than full sentential
forms (Bouillon et al., to appear 2007a). Our initial
evaluation studies on the bidirectional system have
focussed on a specific question which has particular
relevance to this new version of MedSLT. Since
we are assuming that the patient will respond
using elliptical utterances, and that these utterances
will be translated in the context of the preceding
physician-side question, how confident can we
be that this context-dependent translation will be
correct?
In order to investigate these issues, we performed
a small data-collection using Version 2 of the sys-
tem, whose results we summarise here. One of the
authors of the paper played the role of an English-
speaking physician, in a simulated medical exam-
ination scenario where the goal was to determine
whether or not the “patient” was suffering from a
viral throat infection. The six subjects playing the
role of the patient were all native speakers of Span-
ish, and had had no previous exposure to the system,
or indeed any kind of speech technology. They were
given cards describing the symptoms they were sup-
posed to be displaying, on which they were asked
to based their answers. From a total of 92 cor-
rectly recognised patient responses, we obtained 50
yes/no answers and 42 examples of real elliptical ut-
terances. Out of these, 36 were judged to have been
translated completely correctly, and a further 3 were
judged correct in terms of meaning, but less than flu-
ent. Only 3 examples were badly translated: of these
two were caused by problems in a translation rule,
and one by incorrect treatment of ellipsis resolution.
We show representative exchanges below; the last of
these is the one in which ellipsis processing failed to
work correctly.
</bodyText>
<listItem confidence="0.991616">
(3) Doctor: For how long have you
</listItem>
<bodyText confidence="0.8860484">
had your sore throat?
Patient: Desde hace m´as de
una semana
(Trans): I have had a sore
throat for more than one week
</bodyText>
<listItem confidence="0.925909333333333">
(4) Doctor: What were the results?
Patient: Negativo
(Trans): The results were negative
(5) Doctor: Have you seen a doctor
for your sore throat?
Patient: Siel lunes
</listItem>
<bodyText confidence="0.958667">
(Trans): I visited the doctor
for my sore throat monday
</bodyText>
<listItem confidence="0.991243666666667">
(6) Doctor: Have you been with anyone
recently who has a strep throat?
Patient: Si m´as de dos semanas
</listItem>
<bodyText confidence="0.998880666666667">
(Trans): I was in contact with someone
more than two weeks recently
who had strep throat
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999884461538462">
We have presented a bidirectional grammar-based
English H Spanish medical speech translation sys-
tem built using a linguistically motivated archi-
tecture, where all linguistic information is ulti-
mately derived from two resource grammars, one
for each language. We have shown how this en-
ables us to derive the multiple grammars needed,
which differ both with respect to function (recog-
nition/generation) and to domain (physician ques-
tions/patient answers). The system is currently un-
dergoing initial lab testing; we hope to advance to
initial trials on real patients some time towards the
end of the year.
</bodyText>
<sectionHeader confidence="0.994363" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.697568">
[Baker et al.1996] D.W. Baker, R.M. Parker, M.V.
Williams, W.C. Coates, and Kathryn Pitkin. 1996.
</reference>
<page confidence="0.997566">
47
</page>
<reference confidence="0.98621536">
Use and effectiveness of interpreters in an emer-
gency department. Journal of the American Medical
Association, 275:783–8.
[Bouillon et al.2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50–58, Budapest, Hungary.
[Bouillon et al.2006] P. Bouillon, M. Rayner, B. Novel-
las Vall, Y. Nakao, M. Santaholma, M. Starlander, and
N. Chatzichrisafis. 2006. Une grammaire multilingue
partag´ee pour la traduction automatique de la parole.
In Proceedings of TALN 2006, Leuwen, Belgium.
[Bouillon et al.to appear 2007a] P. Bouillon, M. Rayner,
M. Santaholma, and M. Starlander. to appear 2007a.
Les ellipses dans un syst`eme de traduction automa-
tique de la parole. In Proceedings of TALN 2006,
Toulouse, France.
[Bouillon et al.to appear 2007b] P. Bouillon, M. Rayner,
B. Novellas Vall, Y. Nakao, M. Santaholma, M. Star-
lander, and N. Chatzichrisafis. to appear 2007b. Une
grammaire partag´ee multi-tˆache pour le traitement de
la parole : application aux langues romanes. Traite-
ment Automatique des Langues.
[Census2007] U.S. Census, 2007. Selected Social Char-
acteristics in the United States: 2005. Data Set: 2005
American Community Survey. Available here.
[Chatzichrisafis et al.2006] N. Chatzichrisafis, P. Bouil-
lon, M. Rayner, M. Santaholma, M. Starlander, and
B.A. Hockey. 2006. Evaluating task performance for
a unidirectional controlled language medical speech
translation system. In Proceedings of the HLT-NAACL
International Workshop on Medical Speech Transla-
tion, pages 9–16, New York.
[Flores2005] G. Flores. 2005. The impact of medical in-
terpreter services on the quality of health care: A sys-
tematic review. Medical Care Research and Review,
62:255–299.
[Flores2006] G. Flores. 2006. Language barriers to
health care in the united states. New England Journal
ofMedicine, 355:229–231.
[Fluential2007] Fluential, 2007.
http://www.fluentialinc.com. As of 24 March
2007.
[Graddol2004] D. Graddol. 2004. The future of lan-
guage. Science, 303:1329–1331.
[Kittredge2003] R. I. Kittredge. 2003. Sublanguages and
comtrolled languages. In R. Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430–447. Oxford University Press.
[Michie et al.2003] S. Michie, J. Miles, and J. Weinman.
2003. Patient-centeredness in chronic illness: what is
it and does it matter? Patient Education and Counsel-
ing, 51:197–206.
[Mitamura1999] T. Mitamura. 1999. Controlled lan-
guage for multilingual machine translation. In Pro-
ceedings of Machine Translation Summit VII, Singa-
pore.
[Phraselator2007] Phraselator, 2007.
http://www.voxtec.com/. As of 24 March 2007.
[Rayner et al.2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. A methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103–1107, Lisboa, Portugal.
[Rayner et al.2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the International
Space Station. In Proceedings of the 43rd Annual
Meeting ofthe Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
[Rayner et al.2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
[Shieber et al.1990] S. Shieber, G. van Noord, F.C.N.
Pereira, and R.C. Moore. 1990. Semantic-head-driven
generation. Computational Linguistics, 16(1).
[Somers2006] H. Somers. 2006. Language engineering
and the path to healthcare: a user-oriented view. In
Proceedings of the HLT-NAACL International Work-
shop on Medical Speech Translation, pages 32–39,
New York.
[Starlander et al.2005] M. Starlander, P. Bouillon,
N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A.
Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005.
Practising controlled language through a help system
integrated into the medical speech translation system
(MedSLT). In Proceedings of MT Summit X, Phuket,
Thailand.
[Stewart1995] M.A. Stewart. 1995. Effective physician-
patient communication and health outcomes: a review.
Canadian Medical Association Journal, 152:1423–
1433.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000064">
<title confidence="0.8477615">A Bidirectional Grammar-Based Medical Speech Translator Glenn Marianne Nikos</title>
<author confidence="0.999735">Nikos Manny Beth Ann</author>
<affiliation confidence="0.87825">of Geneva, TIM/ISSCO, 40 bvd du Pont-d’Arve, CH-1211 Geneva 4,</affiliation>
<address confidence="0.961934">2Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI</address>
<email confidence="0.996507">gflores@mcw.edu</email>
<address confidence="0.648873">3Powerset, Inc., 475 Brannan Street, San Francisco, CA</address>
<email confidence="0.990237">manny@powerset.com</email>
<address confidence="0.890792">4Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035–1000</address>
<email confidence="0.999004">bahockey@ucsc.edu</email>
<abstract confidence="0.998723258911821">We describe a bidirectional version of the grammar-based MedSLT medical speech system. The system supports simple medical examination dialogues about throat pain between an English-speaking physician and a Spanish-speaking patient. The physician’s side of the dialogue is assumed to consist mostly of WH-questions, and the patient’s of elliptical answers. The paper focusses on the grammar-based speech processing architecture, the ellipsis resolution mechanism, and the online help system. 1 Background There is an urgent need for medical speech translation systems. The world’s current population of 6.6 billion speaks more than 6,000 languages (Graddol, 2004). Language barriers are associated with a wide variety of deleterious consequences in healthcare, including impaired health status, a lower likelihood of having a regular physician, lower rates of mammograms, pap smears, and other preventive services, non-adherence with medications, a greater likelihood of a diagnosis of more severe psychopathology and leaving the hospital against medical advice among psychiatric patients, a lower likelihood of being given a follow-up appointment after an emergency department visit, an increased risk of intubation among children with asthma, a greater risk of hospital admissions among adults, an increased risk of drug complications, longer medical visits, higher resource utilization for diagnostic testing, lower patient satisfaction, impaired patient understanding of diagnoses, medications, and followup, and medical errors and injuries (Flores, 2005; Flores, 2006). Nevertheless, many patients who need medical interpreters do not get them. For example, in the United States, where 52 million people speak a language other than English at home and 23 million people have limited English proficiency (LEP) (Census, 2007), one study found that about half of LEP patients presenting to an emergency department were not provided with a medical interpreter (Baker et al., 1996). There is thus a substantial gap between the need for and availability of language services in health care, a gap that could be bridged through effective medical speech translation systems. An ideal system would be able to interpret accurately and flexibly between patients and health care professionals, using unrestricted language and a large vocabulary. A system of this kind is, unfortunately, beyond the current state of the art. It is, however, possible, using today’s technology, to build speech translation systems for specific scenarios and language-pairs, which can achieve acceptable levels of reliability within the bounds 41 of SPEECHGRAM pages 41–48, Czech Republic, June 2007. Association for Computational Linguistics of a well-defined controlled language. MedSLT (Bouillon et al., 2005) is an Open Source system of this type, which has been under construction at Geneva University since 2003. The system is built on top of Regulus (Rayner et al., 2006), an Open Source platform which supports development of grammar-based speech-enabled applications. Regulus has also been used to build several other systems, including NASA’s Clarissa (Rayner et al., 2005b). The most common architecture for speech translation today uses statistical methods to perform both speech recognition and translation, so it is worth clarifying why we have chosen to use grammarbased methods. Even though statistical architectures exhibit many desirable properties (purely datadriven, domain independent), this is not necessarily the best alternative in safety-critical medical applications. Anecdotally, many physicians express reluctance to trust a translation device whose output is not readily predictable, and most of the speech translation systems which have reached the stage of field testing rely on various types of grammar-based recognition and rule-based translation (Phraselator, 2007; Fluential, 2007). Statistical speech recognisers can achieve impressive levels of accuracy when trained on enough data, but it is a daunting task to collect training material in the requisite quantities (usually, tens of thousands of high-quality utterances) when trying to build practical systems. Considering that the medical speech translation applications we are interested in constructing here need to work for multiple languages and subdomains, the problem becomes even more challenging. Our experience is that grammarbased systems which also incorporate probabilistic context-free grammar tuning deliver better results than purely statistical ones when training data are sparse (Rayner et al., 2005a). Another common criticism of grammar-based systems is that out-of-coverage utterances will neither be recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled language systems has developed a range of techniques for keeping users within the bounds of system coverage (Kittredge, 2003; Mitamura, 1999), and variants of these methods can also be adapted for spoken language applications. Our experiments with MedSLT show that even a quite simple help system is enough to guide users quickly towards the intended coverage of a mediumvocabulary grammar-based speech translation application, with most users appearing confident after just an hour or two of exposure (Starlander et al., 2005; Chatzichrisafis et al., 2006). Until recently, the MedSLT system only supported unidirectional processing in the physician to patient direction. The assumption was that the physician would mostly ask yes/no questions, to which the patient would respond non-verbally, for example by nodding or shaking their head. A unidirectional architecture is easier to make habitable than a bidirectional one. It is reasonable to assume that the physician will use the system regularly enough to learn the coverage, but most patients will not have used the system before, and it is less clear that they will be able to acclimatize within the narrow window at their disposal. These considerations must however be balanced against the fact that a unidirectional system does not allow for a patient-centered interaction characterized by meaningful patient-clinician communication or shared decision making. Multiple studies in the medical literature document that patient-centeredness, effective patient-clinician communication, and shared decision making are associated with significant improvements in patient health outcomes, including reduced anxiety levels, improved functional status, reduced pain, better control of diabetes mellitus, blood pressure reduction among hypertensives, improved adherence, increased patient satisfaction, and symptom reduction for a variety of conditions (Stewart, 1995; Michie et al., 2003). Abidirectional system is considered close to essential from a healthcare perspective, since it appropriately addresses the key issues of patient centeredness and shared decision making. For these reasons, we have over the last few months developed a bidirectional version of MedSLT, initially focussing on a throat pain scenario with an English-speaking physician and a Spanish-speaking patient. The physician uses full sentences, while the patient answers with short responses. One of the strengths of the Regulus approach is 42 that it is very easy to construct parallel versions of a grammar; generally, all that is required is to vary the training corpus. (We will have more to say about this soon). We have exploited these properties of the platform to create two different configurations of the bidirectional system, so that we can compare competing approaches to the problem of accommodating patients unfamiliar with speech technology. In Version 1 (less restricted), the patient is allowed to answer using both elliptical utterances and short sentences, while in Version 2 (more restricted) they are only permitted to use elliptical utterances. Thus, for example, if the physician asks the question “How long have you had a sore throat?”, Version 1 allows the patient to respond both “Desde algunos dias” (“For several days”) and “Me ha dolido la garganta desde algunos dias” (“I have had a sore throat for several days”), while Version 2 only allows the first of these. Both the short and the long versions are translated uniformly, with the short version resolved using the context from the preceding question. In both versions, if the patient finds it too challenging to use the system to answer WH-questions directly, it is possible to back off to the earlier dialogue architecture in which the physician uses Y- N questions and the patient responds with simple yes/no answers, or nonverbally. Continuing the example, if the patient is unable to find an appropriate way to answer the physician’s question, the physician could ask “Have you had a sore throat for more than three days?”; if the patient responds negatively, they could continue with the follow-on question “More than a week?”, and so on. In the rest of the paper, we first describe the system top-level (Section 2), the way in which grammar-based processing is used (Section 3), the ellipsis processing mechanism (Section 4), and the help system (Section 5). Section 6 presents an initial evaluation, and the final section concludes. 2 Top-level architecture The system is operated through the graphical user interface (GUI) shown in Figures 1 and 2. In accordance with the basic principles of patientcenteredness and shared decision-making outlined in Section 1, the patient and the physician each have their own headset, use their own mouse, and share the same view of the screen. This is in sharp contrast to the majority of the medical speech translation systems described in the literature (Somers, 2006). As shown in the screenshots, the main GUI window is separated into two tabbed panes, marked “Doctor” and “Patient”. Initially, the “Doctor” view (the one shown in Figure 1) is active. The physician presses the “Push to talk” button, and speaks into the headset microphone. If recognition is successful, the GUI displays four separate results, listed on the right side of the screen. At the top, immediately under the heading “Question”, we can see the actual words returned by speech recognition. Here, these words are “Have you had rapid strep test”. Below, we have the help pane: this displays similar questions taken from the help corpus, which are known to be within system coverage. The pane marked “System understood” shows a back-translation, produced by first translating the recognition result into interlingua, and then translating it back into English. In the present example, this corrects the minor mistake the recogniser has made, missing the indefinite article “a”, and confirms that the system has obtained a correct grammatical analysis and interpretation at the level of interlingua. At the bottom, we see the target language translation. The left-hand side of the screen logs the history of the conversation to date, so that both sides can refer back to it. If the physician decides that the system has correctly understood what they said, they can now press the “Play” button. This results in the system producing a spoken output, using the Vocalizer TTS engine. Simultaneously with speaking, the GUI shifts to the “Patient” configuration shown in Figure 2. This differs from the “Doctor” configuration in two respects: all text is in the patient language, and the help pane presents its suggestions immediately, based on the preceding physician question. The various processing components used to support these functionalities are described in the following sections. 3 Grammar-based processing Grammar-based processing is used for sourcelanguage speech recognition and target-side generation. (Source-language analysis is part of the recognition process, since grammar-based recognition includes creating a parse). All of these functionalities 43 Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressed the “Play” button. The help pane shows similar queries known to be within coverage. Figure 2: Screenshot showing the state of the GUI after the physician has pressed the “Play” button. The help pane shows known valid responses to similar questions. 44 are implemented using the Regulus platform, with the task-specific grammars compiled out of general feature grammar resources by the Regulus tools. For both recognition and generation, the first step is to extract a domain-specific feature grammar from the general one, using a version of the Explanation Based Learning (EBL) algorithm. The extraction process is driven by a corpus of examples and a set of “operationality criteria”, which define how the rules in the original resource grammar are recombined into domain-specific ones. It is important to realise that the domain-specific gramis a subset of the resource grammar; a typical domain-specific grammar rule is created by merging two to five resource grammar rules into a single “flatter” rule. The result is a feature grammar which is less general than the original one, but more efficient. For recognition, the grammar is then processed further into a CFG language model, using an algorithm which alternates expansion of feature values and filtering of the partially expanded grammar to remove irrelevant rules. Detailed descripof the EBL learning and feature grammar CFG compilation algorithms can be found in Chapters 8 and 10 of (Rayner et al., 2006). Regulus feature grammars can also be compiled into generators using a version of the Semantic Head Driven algorithm (Shieber et al., 1990). The English (physician) side recogniser is compiled from the large English resource grammar described in Chapter 9 of (Rayner et al., 2006), and was constructed in the same way as the one described in (Rayner et al., 2005a), which was used for a headache examination task. The operationality criteria are the same, and the only changes are a different training corpus and the addition of new entries to the lexicon. The same resources, with a different training corpus, were used to build the English language generator. It is worth pointing out that, although a uniform method was used to build these various grammars, the results were all very different. For example, the recognition grammar from (Rayner et al., 2005a) is specialised to cover only second-person questions (“Do you get headaches in the mornings?”), while the generator grammar used in the present application covers only firstperson declarative statements (“I visited the doctor last Monday.”). In terms of structure, each grammar contains several important constructions that the other lacks. For example, subordinate clauses are central in the headache domain (“Do the headaches occur when you are stressed?”) but are not present in the sore throat domain; this is because the standard headache examination questions mostly focus on generic conditions, while the sore throat examination questions only relate to concrete ones. Conversely, relative clauses are important in the sore throat domain (“I have recently been in contact with someone who has strep throat”), but are not sufficiently important in the headache domain to be covered there. On the Spanish (patient) side, there are four grammars involved. For recognition, we have two different grammars, corresponding to the two versions of the system; the grammar for Version 2 is essentially a subset of that for Version 1. For generation, there are two separate and quite different grammars: one is used for translating the physician’s questions, while the other produces back-translations of the patient’s questions. All of these grammars are extracted from a general shared resource grammar for Romance languages, which currently combines rules for French, Spanish and Catalan (Bouillon et al., 2006; Bouillon et al., to appear 2007b). One interesting consequence of our methodology is related to the fact that Spanish is a prodrop language, which implies that many sentences are systematically ambiguous between declarative and Y-N question readings. For example, “He consultado un m´edico” could in principle mean either “I visited a doctor” or “Did I visit a doctor?”. When training the specialised Spanish grammars, it is thus necessary to specify which readings of the training sentences are to be used. Continuing the example, if the sentence occurred in training material for the answer grammar, we would specify that the declarative reading the intended 4 Ellipsis processing and contextual interpretation In Version 1 of the system, the patient is perto answer using elliptical phrases; in Verspecification can be formulated as a preference that applies uniformly to all the training examples in a given group. 45 sion 2, she is obliged to do so. Ability to process elliptical responses makes it easier to guide the patient towards the intended coverage of the system, without degrading the quality of recognition (Bouillon et al., to appear 2007a). The downside is that ellipses are also harder to translate than full sentences. Even in a limited domain like ours, and in a closely related language-pair, ellipsis can generally not be translated word for word, and it is necessary to look at the preceding context if the rules are to be applied correctly. In examples 1 and 2 below, the locative phrase “In your stomach” in the English source becomes the subject in the Spanish translation. This implies that the translation of the ellipsis in the second physician utterance needs to change syntactic category: “In your head” (PP) becomes “La cabeza” (NP) . (1) Doctor: Do you have a pain in your stomach? (Trans): Le duele el estomago? (2) Doctor: In your head? (Trans): *En la cabeza? Since examples like this are frequent, our system implements a solution in which the patient’s replies are translated in the context of the preceding utterance. If the patient-side recogniser’s output is classified as an ellipsis (this can done fairly reliably thanks to use of suitably specialised grammars; cf. Section 3), we expand the incomplete phrase into a full sentence structure by adding appropriate structural elements from the preceding physicianside question; the expanded semantic structure is the one which is then translated into interlingual form, and thence back to the physician-side language. Since all linguistic representations, including those of elliptical phrases and their contexts, are represented as flat attribute-value lists, we are able to implement the resolution algorithm very simply in terms of list manipulation. In YN-questions, where the elliptical answer intuitively adds information to the question (“Did you visit the doctor?”; “El lunes” visited the doctor on Monday”), the representations are organised so that resolution mainly to concatenation of the two In WHquestions, where the answer intuitively substitutes the elliptical answer for the WH-phrase (“What is is also necessary to replace second-person pronouns with first-person counterparts. temperature?”; “Cuarenta grados” temperature is forty degrees”), resolution substitutes the representation of the elliptical phrase for that of a semantically similar element in the question. The least trivial aspect of this process is providing a suitable definition of “semantically similar”. This is done using a simple example-based method, in which the grammar developer writes a set of declarations, each of which lists a set of semantically similar NPs. At compile-time, the grammar is used to parse each NP, and extract a generalised skeleton, in which specific lexical information is stripped away; at run-time, two NPs are held to be semantically similar if they can each be unified with skeletons in the same equivalence class. This ensures that the definition of the semantic similarity relation is stable across most changes to the grammar and lexicon. The issues are described in greater detail in (Bouillon et al., to appear 2007a). 5 Help system Since the performance of grammar-based speech understanding is only reliable on in-coverage material, systems based on this type of architecture must necessarily use a controlled language approach, in which it is assumed that the user is able to learn the relevant coverage. As previously noted, the Med- SLT system addresses this problem by incorporating an online help system (Starlander et al., 2005; Chatzichrisafis et al., 2006). On the physician side, the help system offers, after each recognition event, a list of related questions; similarly, on the patient side, it provides examples of known valid answers to the current question. In both cases, the help examples are extracted from a precompiled corpus of question-answer pairs, which have been judged for correctness by system developers. The process of selecting the examples is slightly different on the two sides. For questions (physician side), the system performs a second parallel recognition of the input speech, using a statistical recogniser. It then compares the recognition result, using an N-gram based metric, against the set of known correct in-coverage questions from the question-answer corpus, to extract the most similar ones. For answers (patient side), the help system searches the question-answer corpus to find the 46 questions most similar to the current one, and shows the list of corresponding valid answers, using the whole list in the case of Version 1 of the system, and only the subset consisting of elliptical phrases in the case of Version 2. 6 Evaluation In previous studies, we have evaluated speech recognition and speech understanding performance for physician-side questions in English (Bouillon et al., 2005) and Spanish et al., to appear 2007b), and gated the impact on performance of the help system (Rayner et al., 2005a; Starlander et al., 2005). We have also carried out recent evaluations designed to contrast recognition performance on elliptical and full versions of the same utterance; here, our results suggest that elliptical forms of (French-language) MedSLT utterances are slightly easier to recognise in terms of semantic error rate than full sentential forms (Bouillon et al., to appear 2007a). Our initial evaluation studies on the bidirectional system have focussed on a specific question which has particular relevance to this new version of MedSLT. Since we are assuming that the patient will respond using elliptical utterances, and that these utterances will be translated in the context of the preceding physician-side question, how confident can we be that this context-dependent translation will be correct? In order to investigate these issues, we performed a small data-collection using Version 2 of the system, whose results we summarise here. One of the authors of the paper played the role of an Englishspeaking physician, in a simulated medical examination scenario where the goal was to determine whether or not the “patient” was suffering from a viral throat infection. The six subjects playing the role of the patient were all native speakers of Spanish, and had had no previous exposure to the system, or indeed any kind of speech technology. They were given cards describing the symptoms they were supposed to be displaying, on which they were asked to based their answers. From a total of 92 correctly recognised patient responses, we obtained 50 yes/no answers and 42 examples of real elliptical utterances. Out of these, 36 were judged to have been translated completely correctly, and a further 3 were judged correct in terms of meaning, but less than fluent. Only 3 examples were badly translated: of these two were caused by problems in a translation rule, and one by incorrect treatment of ellipsis resolution. We show representative exchanges below; the last of these is the one in which ellipsis processing failed to work correctly. (3) Doctor: For how long have you had your sore throat? Patient: Desde hace m´as de una semana (Trans): I have had a sore throat for more than one week (4) Doctor: What were the results? Patient: Negativo (Trans): The results were negative (5) Doctor: Have you seen a doctor for your sore throat? Patient: Siel lunes (Trans): I visited the doctor for my sore throat monday (6) Doctor: Have you been with anyone recently who has a strep throat? Patient: Si m´as de dos semanas (Trans): I was in contact with someone more than two weeks recently who had strep throat 7 Conclusions We have presented a bidirectional grammar-based medical speech translation system built using a linguistically motivated architecture, where all linguistic information is ultimately derived from two resource grammars, one for each language. We have shown how this enables us to derive the multiple grammars needed, which differ both with respect to function (recognition/generation) and to domain (physician questions/patient answers). The system is currently undergoing initial lab testing; we hope to advance to initial trials on real patients some time towards the end of the year.</abstract>
<note confidence="0.952683434782609">References [Baker et al.1996] D.W. Baker, R.M. Parker, M.V. Williams, W.C. Coates, and Kathryn Pitkin. 1996. 47 Use and effectiveness of interpreters in an emerdepartment. of the American Medical 275:783–8. [Bouillon et al.2005] P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara. 2005. A generic multi-lingual open source platform for limited-domain medical speech translation. In Proceedings of the 10th Conference of the European for Machine Translation pages 50–58, Budapest, Hungary. [Bouillon et al.2006] P. Bouillon, M. Rayner, B. Novellas Vall, Y. Nakao, M. Santaholma, M. Starlander, and N. Chatzichrisafis. 2006. Une grammaire multilingue partag´ee pour la traduction automatique de la parole. of TALN Leuwen, Belgium. [Bouillon et al.to appear 2007a] P. Bouillon, M. Rayner, M. Santaholma, and M. Starlander. to appear 2007a. Les ellipses dans un syst`eme de traduction automa-</note>
<affiliation confidence="0.900209">de la parole. In of TALN</affiliation>
<address confidence="0.915719">Toulouse, France.</address>
<email confidence="0.42018">lander,andN.Chatzichrisafis.toappear2007b.Une</email>
<abstract confidence="0.5078483125">grammaire partag´ee multi-tˆache pour le traitement de parole : application aux langues romanes. Traite- Automatique des U.S. Census, 2007. Social Characteristics in the United States: 2005. Data Set: 2005 Community Available here. [Chatzichrisafis et al.2006] N. Chatzichrisafis, P. Bouillon, M. Rayner, M. Santaholma, M. Starlander, and B.A. Hockey. 2006. Evaluating task performance for a unidirectional controlled language medical speech system. In of the HLT-NAACL International Workshop on Medical Speech Translapages 9–16, New York. [Flores2005] G. Flores. 2005. The impact of medical interpreter services on the quality of health care: A sysreview. Care Research and</abstract>
<note confidence="0.951058538461538">62:255–299. [Flores2006] G. Flores. 2006. Language barriers to care in the united states. England Journal 355:229–231. [Fluential2007] Fluential, 2007. http://www.fluentialinc.com. As of 24 March 2007. [Graddol2004] D. Graddol. 2004. The future of lan- 303:1329–1331. [Kittredge2003] R. I. Kittredge. 2003. Sublanguages and languages. In R. Mitkov, editor, Ox- Handbook of Computational pages 430–447. Oxford University Press. [Michie et al.2003] S. Michie, J. Miles, and J. Weinman. 2003. Patient-centeredness in chronic illness: what is and does it matter? Education and Counsel- 51:197–206. [Mitamura1999] T. Mitamura. 1999. Controlled lanfor multilingual machine translation. In Proof Machine Translation Summit Singapore. [Phraselator2007] Phraselator, 2007. http://www.voxtec.com/. As of 24 March 2007. [Rayner et al.2005a] M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey, M. Santaholma, M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao. 2005a. A methodology for comparing grammar-based and robust approaches to speech understanding. In Proceedings of the 9th International Conference Spoken Language Processing pages 1103–1107, Lisboa, Portugal. [Rayner et al.2005b] M. Rayner, B.A. Hockey, J.M. Renders, N. Chatzichrisafis, and K. Farrell. 2005b. A voice enabled procedure browser for the International Station. In of the 43rd Annual Meeting ofthe Association for Computational Linguis- (interactive poster and demo Ann Arbor, [Rayner et al.2006] M. Rayner, B.A. Hockey, and Bouillon. 2006. Linguistics into Speech The Regulus Grammar CSLI Press, Chicago. [Shieber et al.1990] S. Shieber, G. van Noord, F.C.N. Pereira, and R.C. Moore. 1990. Semantic-head-driven 16(1). [Somers2006] H. Somers. 2006. Language engineering and the path to healthcare: a user-oriented view. In Proceedings of the HLT-NAACL International Workon Medical Speech pages 32–39, New York. [Starlander et al.2005] M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A. Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005.</note>
<title confidence="0.524991">Practising controlled language through a help system integrated into the medical speech translation system</title>
<affiliation confidence="0.720362">In of MT Summit Phuket,</affiliation>
<address confidence="0.605177">Thailand.</address>
<note confidence="0.8056466">[Stewart1995] M.A. Stewart. 1995. Effective physicianpatient communication and health outcomes: a review. Medical Association 152:1423– 1433. 48</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Baker</author>
<author>R M Parker</author>
<author>M V Williams</author>
<author>W C Coates</author>
<author>Kathryn Pitkin</author>
</authors>
<title>Use and effectiveness of interpreters in an emergency department.</title>
<date>1996</date>
<journal>Journal of the American Medical Association,</journal>
<pages>275--783</pages>
<marker>[Baker et al.1996]</marker>
<rawString>D.W. Baker, R.M. Parker, M.V. Williams, W.C. Coates, and Kathryn Pitkin. 1996. Use and effectiveness of interpreters in an emergency department. Journal of the American Medical Association, 275:783–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bouillon</author>
<author>M Rayner</author>
<author>N Chatzichrisafis</author>
<author>B A Hockey</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>Y Nakao</author>
<author>K Kanzaki</author>
<author>H Isahara</author>
</authors>
<title>A generic multi-lingual open source platform for limited-domain medical speech translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>50--58</pages>
<location>Budapest, Hungary.</location>
<marker>[Bouillon et al.2005]</marker>
<rawString>P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara. 2005. A generic multi-lingual open source platform for limited-domain medical speech translation. In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT), pages 50–58, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bouillon</author>
<author>M Rayner</author>
<author>B Novellas Vall</author>
<author>Y Nakao</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>N Chatzichrisafis</author>
</authors>
<title>Une grammaire multilingue partag´ee pour la traduction automatique de la parole.</title>
<date>2006</date>
<booktitle>In Proceedings of TALN</booktitle>
<location>Leuwen, Belgium.</location>
<marker>[Bouillon et al.2006]</marker>
<rawString>P. Bouillon, M. Rayner, B. Novellas Vall, Y. Nakao, M. Santaholma, M. Starlander, and N. Chatzichrisafis. 2006. Une grammaire multilingue partag´ee pour la traduction automatique de la parole. In Proceedings of TALN 2006, Leuwen, Belgium.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Bouillon</author>
<author>M Rayner</author>
<author>M Santaholma</author>
<author>M Starlander</author>
</authors>
<title>to appear 2007a. Les ellipses dans un syst`eme de traduction automatique de la parole.</title>
<booktitle>In Proceedings of TALN 2006,</booktitle>
<location>Toulouse, France.</location>
<marker>[Bouillon et al.to appear 2007a]</marker>
<rawString>P. Bouillon, M. Rayner, M. Santaholma, and M. Starlander. to appear 2007a. Les ellipses dans un syst`eme de traduction automatique de la parole. In Proceedings of TALN 2006, Toulouse, France.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Bouillon</author>
<author>M Rayner</author>
<author>B Novellas Vall</author>
<author>Y Nakao</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>N Chatzichrisafis</author>
</authors>
<title>to appear 2007b. Une grammaire partag´ee multi-tˆache pour le traitement de la parole : application aux langues romanes. Traitement Automatique des Langues.</title>
<marker>[Bouillon et al.to appear 2007b]</marker>
<rawString>P. Bouillon, M. Rayner, B. Novellas Vall, Y. Nakao, M. Santaholma, M. Starlander, and N. Chatzichrisafis. to appear 2007b. Une grammaire partag´ee multi-tˆache pour le traitement de la parole : application aux langues romanes. Traitement Automatique des Langues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S Census</author>
</authors>
<title>Selected Social Characteristics in the United States:</title>
<date>2007</date>
<marker>[Census2007]</marker>
<rawString>U.S. Census, 2007. Selected Social Characteristics in the United States: 2005. Data Set: 2005 American Community Survey. Available here.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chatzichrisafis</author>
<author>P Bouillon</author>
<author>M Rayner</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>B A Hockey</author>
</authors>
<title>Evaluating task performance for a unidirectional controlled language medical speech translation system.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL International Workshop on Medical Speech Translation,</booktitle>
<pages>9--16</pages>
<location>New York.</location>
<marker>[Chatzichrisafis et al.2006]</marker>
<rawString>N. Chatzichrisafis, P. Bouillon, M. Rayner, M. Santaholma, M. Starlander, and B.A. Hockey. 2006. Evaluating task performance for a unidirectional controlled language medical speech translation system. In Proceedings of the HLT-NAACL International Workshop on Medical Speech Translation, pages 9–16, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Flores</author>
</authors>
<title>The impact of medical interpreter services on the quality of health care: A systematic review. Medical Care Research and Review,</title>
<date>2005</date>
<pages>62--255</pages>
<marker>[Flores2005]</marker>
<rawString>G. Flores. 2005. The impact of medical interpreter services on the quality of health care: A systematic review. Medical Care Research and Review, 62:255–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Flores</author>
</authors>
<title>Language barriers to health care in the united states. New England Journal ofMedicine,</title>
<date>2006</date>
<pages>355--229</pages>
<marker>[Flores2006]</marker>
<rawString>G. Flores. 2006. Language barriers to health care in the united states. New England Journal ofMedicine, 355:229–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fluential</author>
</authors>
<title>http://www.fluentialinc.com.</title>
<date>2007</date>
<journal>As of</journal>
<volume>24</volume>
<marker>[Fluential2007]</marker>
<rawString>Fluential, 2007. http://www.fluentialinc.com. As of 24 March 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graddol</author>
</authors>
<title>The future of language.</title>
<date>2004</date>
<journal>Science,</journal>
<pages>303--1329</pages>
<marker>[Graddol2004]</marker>
<rawString>D. Graddol. 2004. The future of language. Science, 303:1329–1331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R I Kittredge</author>
</authors>
<title>Sublanguages and comtrolled languages. In</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics,</booktitle>
<pages>430--447</pages>
<editor>R. Mitkov, editor,</editor>
<publisher>Oxford University Press.</publisher>
<marker>[Kittredge2003]</marker>
<rawString>R. I. Kittredge. 2003. Sublanguages and comtrolled languages. In R. Mitkov, editor, The Oxford Handbook of Computational Linguistics, pages 430–447. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Michie</author>
<author>J Miles</author>
<author>J Weinman</author>
</authors>
<title>Patient-centeredness in chronic illness: what is it and does it matter? Patient Education and Counseling,</title>
<date>2003</date>
<pages>51--197</pages>
<marker>[Michie et al.2003]</marker>
<rawString>S. Michie, J. Miles, and J. Weinman. 2003. Patient-centeredness in chronic illness: what is it and does it matter? Patient Education and Counseling, 51:197–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitamura</author>
</authors>
<title>Controlled language for multilingual machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of Machine Translation Summit VII,</booktitle>
<marker>[Mitamura1999]</marker>
<rawString>T. Mitamura. 1999. Controlled language for multilingual machine translation. In Proceedings of Machine Translation Summit VII, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phraselator</author>
</authors>
<title>http://www.voxtec.com/.</title>
<date>2007</date>
<journal>As of</journal>
<volume>24</volume>
<marker>[Phraselator2007]</marker>
<rawString>Phraselator, 2007. http://www.voxtec.com/. As of 24 March 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>P Bouillon</author>
<author>N Chatzichrisafis</author>
<author>B A Hockey</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>Y Nakao</author>
</authors>
<title>A methodology for comparing grammar-based and robust approaches to speech understanding.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>1103--1107</pages>
<location>Lisboa, Portugal.</location>
<marker>[Rayner et al.2005a]</marker>
<rawString>M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey, M. Santaholma, M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao. 2005a. A methodology for comparing grammar-based and robust approaches to speech understanding. In Proceedings of the 9th International Conference on Spoken Language Processing (ICSLP), pages 1103–1107, Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>J M Renders</author>
<author>N Chatzichrisafis</author>
<author>K Farrell</author>
</authors>
<title>A voice enabled procedure browser for the International Space Station.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics</booktitle>
<location>Ann Arbor,</location>
<marker>[Rayner et al.2005b]</marker>
<rawString>M. Rayner, B.A. Hockey, J.M. Renders, N. Chatzichrisafis, and K. Farrell. 2005b. A voice enabled procedure browser for the International Space Station. In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (interactive poster and demo track), Ann Arbor,</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>P Bouillon</author>
</authors>
<title>Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler.</title>
<date>2006</date>
<publisher>CSLI Press,</publisher>
<location>Chicago.</location>
<marker>[Rayner et al.2006]</marker>
<rawString>M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler. CSLI Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>G van Noord</author>
<author>F C N Pereira</author>
<author>R C Moore</author>
</authors>
<title>Semantic-head-driven generation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>[Shieber et al.1990]</marker>
<rawString>S. Shieber, G. van Noord, F.C.N. Pereira, and R.C. Moore. 1990. Semantic-head-driven generation. Computational Linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Somers</author>
</authors>
<title>Language engineering and the path to healthcare: a user-oriented view.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL International Workshop on Medical Speech Translation,</booktitle>
<pages>32--39</pages>
<location>New York.</location>
<marker>[Somers2006]</marker>
<rawString>H. Somers. 2006. Language engineering and the path to healthcare: a user-oriented view. In Proceedings of the HLT-NAACL International Workshop on Medical Speech Translation, pages 32–39, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Starlander</author>
<author>P Bouillon</author>
<author>N Chatzichrisafis</author>
<author>M Santaholma</author>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>Y Nakao</author>
</authors>
<title>Practising controlled language through a help system integrated into the medical speech translation system (MedSLT).</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit X,</booktitle>
<location>Phuket, Thailand.</location>
<marker>[Starlander et al.2005]</marker>
<rawString>M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A. Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005. Practising controlled language through a help system integrated into the medical speech translation system (MedSLT). In Proceedings of MT Summit X, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Stewart</author>
</authors>
<title>Effective physicianpatient communication and health outcomes: a review.</title>
<date>1995</date>
<journal>Canadian Medical Association Journal,</journal>
<volume>152</volume>
<pages>1433</pages>
<marker>[Stewart1995]</marker>
<rawString>M.A. Stewart. 1995. Effective physicianpatient communication and health outcomes: a review. Canadian Medical Association Journal, 152:1423– 1433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>