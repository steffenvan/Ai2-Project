<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023486">
<title confidence="0.995952">
Balancing Data-driven and Rule-based Approaches in the Context of a
Multimodal Conversational System
</title>
<author confidence="0.700588">
Srinivas Bangalore
</author>
<affiliation confidence="0.605905">
AT&amp;T Labs-Research
</affiliation>
<address confidence="0.9233215">
180 Park Avenue
Florham Park, NJ 07932
</address>
<email confidence="0.996555">
srini@research.att.com
</email>
<author confidence="0.700882">
Michael Johnston
</author>
<affiliation confidence="0.601771">
AT&amp;T Labs-Research
</affiliation>
<address confidence="0.9448795">
180 Park Avenue
Florham Park, NJ 07932
</address>
<email confidence="0.999102">
johnston@research.att.com
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899190476191">
Moderate-sized rule-based spoken language
models for recognition and understanding are
easy to develop and provide the ability to
rapidly prototype conversational applications.
However, scalability of such systems is a bot-
tleneck due to the heavy cost of authoring and
maintenance of rule sets and inevitable brittle-
ness due to lack of coverage in the rule sets.
In contrast, data-driven approaches are robust
and the procedure for model building is usu-
ally simple. However, the lack of data in a par-
ticular application domain limits the ability to
build data-driven models. In this paper, we ad-
dress the issue of combining data-driven and
grammar-based models for rapid prototyping
of robust speech recognition and understanding
models for a multimodal conversational sys-
tem. We also present methods that reuse data
from different domains and investigate the lim-
its of such models in the context of a particular
application domain.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975894736842">
In the past four decades of speech and natural language
processing, both data-driven approaches and rule-based
approaches have been prominent at different periods in
time. In the recent past, rule-based approaches have
fallen into disfavor due to their brittleness and the sig-
nificant cost of authoring and maintaining complex rule
sets. Data-driven approaches are robust and provide a
simple process of developing applications given the data
from the application domain. However, the reliance on
domain-specific data is also one of the significant bottle-
necks of data-driven approaches. Development of a con-
versational system using data-driven approaches cannot
proceed until data pertaining to the application domain is
available. The collection and annotation of such data is
extremely time-consuming and tedious, which is aggra-
vated by the presence of multiple modalities in the user’s
input, as in our case. Also, extending an existing applica-
tion to support an additional feature requires adding ad-
ditional data sets with that feature.
In this paper, we explore various methods for combin-
ing rule-based and in-domain data for rapid prototyping
of speech recognition and understanding models that are
robust to ill-formed or unexpected input in the context
of a multimodal conversational system. We also investi-
gate approaches to reuse out-of-domain data and compare
their performance against the performance of in-domain
data-driven models.
We investigate these issues in the context of a multi-
modal application designed to provide an interactive city
guide: MATCH. In Section 2, we present the MATCH
application, the architecture of the system and the appa-
ratus for multimodal understanding. In Section 3, we dis-
cuss various approaches to rapid prototyping of the lan-
guage model for the speech recognizer and in Section 4
we present two approaches to robust multimodal under-
standing. Section 5 presents the results for speech recog-
nition and multimodal understanding using the different
approaches we consider.
</bodyText>
<sectionHeader confidence="0.947169" genericHeader="introduction">
2 The MATCH application
</sectionHeader>
<bodyText confidence="0.997839772727273">
MATCH (Multimodal Access To City Help) is a work-
ing city guide and navigation system that enables mo-
bile users to access restaurant and subway information
for New York City (NYC) (Johnston et al., 2002b; John-
ston et al., 2002a). The user interacts with a graphical in-
terface displaying restaurant listings and a dynamic map
showing locations and street information. The inputs can
be speech, drawing on the display with a stylus, or syn-
chronous multimodal combinations of the two modes.
The user can ask for the review, cuisine, phone number,
address, or other information about restaurants and sub-
way directions to locations. The system responds with
graphical callouts on the display, synchronized with syn-
thetic speech output. For example, if the user says phone
numbers for these two restaurants and circles two restau-
rants as in Figure 1 [a], the system will draw a callout
with the restaurant name and number and say, for exam-
ple Time Cafe can be reached at 212-533-7000, for each
restaurant in turn (Figure 1 [b]). If the immediate en-
vironment is too noisy or public, the same command can
be given completely in pen by circling the restaurants and
writing phone.
</bodyText>
<figureCaption confidence="0.995165">
Figure 1: Two area gestures
</figureCaption>
<subsectionHeader confidence="0.813411">
2.1 MATCH Multimodal Architecture
</subsectionHeader>
<bodyText confidence="0.999985">
The underlying architecture that supports MATCH con-
sists of a series of re-usable components which commu-
nicate over sockets through a facilitator (MCUBE) (Fig-
ure 2). Users interact with the system through a Multi-
modal User Interface Client (MUI). Their speech and ink
are processed by speech recognition (Sharp et al., 1997)
(ASR) and handwriting/gesture recognition (GESTURE,
HW RECO) components respectively. These recognition
processes result in lattices of potential words and ges-
tures. These are then combined and assigned a mean-
ing representation using a multimodal finite-state device
(MMFST) (Johnston and Bangalore, 2000; Johnston et
al., 2002b). This provides as output a lattice encoding all
of the potential meaning representations assigned to the
user inputs. This lattice is flattened to an N-best list and
passed to a multimodal dialog manager (MDM) (John-
ston et al., 2002b), which re-ranks them in accordance
with the current dialogue state. If additional informa-
tion or confirmation is required, the MDM enters into a
short information gathering dialogue with the user. Once
a command or query is complete, it is passed to the mul-
timodal generation component (MMGEN), which builds
a multimodal score indicating a coordinated sequence of
graphical actions and TTS prompts. This score is passed
back to the Multimodal UI (MUI). The Multimodal UI
coordinates presentation of graphical content with syn-
thetic speech output using the AT&amp;T Natural Voices TTS
engine (Beutnagel et al., 1999). The subway route con-
straint solver (SUBWAY) identifies the best route be-
tween any two points in New York City.
</bodyText>
<figureCaption confidence="0.982059">
Figure 2: Multimodal Architecture
</figureCaption>
<subsectionHeader confidence="0.998429">
2.2 Multimodal Integration and Understanding
</subsectionHeader>
<bodyText confidence="0.999926">
Our approach to integrating and interpreting multimodal
inputs (Johnston et al., 2002b; Johnston et al., 2002a) is
an extension of the finite-state approach previously pro-
posed (Bangalore and Johnston, 2000; Johnston and Ban-
galore, 2000). In this approach, a declarative multimodal
grammar captures both the structure and the interpreta-
tion of multimodal and unimodal commands. The gram-
mar consists of a set of context-free rules. The multi-
modal aspects of the grammar become apparent in the
terminals, each of which is a triple W:G:M, consisting
of speech (words, W), gesture (gesture symbols, G), and
meaning (meaning symbols, M). The multimodal gram-
mar encodes not just multimodal integration patterns but
also the syntax of speech and gesture, and the assignment
of meaning. The meaning is represented in XML, facil-
itating parsing and logging by other system components.
The symbol SEM is used to abstract over specific content
such as the set of points delimiting an area or the identi-
fiers of selected objects. In Figure 3, we present a small
simplified fragment from the MATCH application capa-
ble of handling information seeking commands such as
phonefor these three restaurants. The epsilon symbol ( )
indicates that a stream is empty in a given terminal.
</bodyText>
<figureCaption confidence="0.996258">
Figure 4: Multimodal Example
</figureCaption>
<bodyText confidence="0.9999232">
In the example above where the user says phone for
these two restaurants while circling two restaurants (Fig-
ure 1 [a]), assume the speech recognizer returns the lat-
tice in Figure 4 (Speech). The gesture recognition com-
ponent also returns a lattice (Figure 4, Gesture) indicat-
ing that the user’s ink is either a selection of two restau-
rants or a geographical area. The multimodal grammar
(Figure 3) expresses the relationship between what the
user said, what they drew with the pen, and their com-
bined meaning, in this case Figure 4 (Meaning). The
meaning is generated by concatenating the meaning sym-
bols and replacing SEMwith the appropriate specific con-
tent: cmd info type phone /type obj
rest [r12,r15] /rest /obj /info /cmd .
For the purpose of evaluation of concept accuracy, we
developed an approach similar to (Boros et al., 1996)
in which computing concept accuracy is reduced to com-
paring strings representing core contentful concepts. We
extract a sorted flat list of attribute value pairs that repre-
sents the core contentful concepts of each command from
</bodyText>
<figure confidence="0.979270925925926">
phone for
Speech:
these two
restaurants
ten
G area loc
sel
2 SEM(r12,r15)
SEM(points...)
Gesture:
&lt;cmd&gt; &lt;info&gt;
&lt;rest&gt;
r12,r15 &lt;/rest&gt; &lt;/obj&gt; &lt;/info&gt; &lt;/cmd&gt;
&lt;rest&gt;
&lt;type&gt;
phone
&lt;/type&gt;
&lt;obj&gt;
Meaning:
CMD : : cmd INFO : : /cmd
INFO : : type TYPE :: /type
for:: :: obj DEICNP :: /obj
TYPE phone: :phone review: :review
DEICNP DDETPL :area: :sel: NUM BEADPL
DDETPL these:G: those:G:
BEADPL restaurants:rest: rest SEM:SEM: :: /rest
NUM two:2: three:3: ... ten:10:
</figure>
<figureCaption confidence="0.999298">
Figure 3: Multimodal grammar fragment
</figureCaption>
<bodyText confidence="0.999928071428571">
the XML output. The example above yields the following
meaning representation for concept accuracy.
The multimodal grammar can be used to create lan-
guage models for ASR, align the speech and gesture re-
sults from the respective recognizers and transform the
multimodal utterance to a meaning representation. All
these operations are achieved using finite-state transducer
operations (See (Bangalore and Johnston, 2000; John-
ston and Bangalore, 2000) for details). However, this ap-
proach to recognition needs to be more robust to extra-
grammaticality and language variation in user’s utter-
ances and the interpretation needs to be more robust to
speech recognition errors. We address these issues in the
rest of the paper.
</bodyText>
<sectionHeader confidence="0.9648405" genericHeader="method">
3 Bootstrapping Corpora for Language
Models
</sectionHeader>
<bodyText confidence="0.999969928571429">
The problem of speech recognition can be succinctly rep-
resented as a search for the most likely word sequence
( ) through the network created by the composition of a
language of acoustic observations ( ), an acoustic model
which is a transduction from acoustic observations to
phone sequences ( ), a pronounciation model which is
a transduction from phone sequences to word sequences
( ), and a language model acceptor ( ) (Pereira and Ri-
ley, 1997). The language model acceptor encodes the
(weighted) word sequences permitted in an application.
Typically, is built using either a hand-crafted gram-
mar or using a statistical language model derived from a
corpus of sentences from the application domain. While
a grammar could be written so as to be easily portable
across applications, it suffers from being too prescrip-
tive and has no metric for relative likelihood of users’
utterances. In contrast, in the data-driven approach a
weighted grammar is automatically induced from a cor-
pus and the weights can be interpreted as a measure for
relative likelihood of users’ utterances. However, the re-
liance on a domain-specific corpus is one of the signif-
icant bottlenecks of data-driven approaches, since col-
lecting a corpus specific to a domain is an expensive and
time-consuming task.
In this section, we investigate a range of techniques
for producing a domain-specific corpus using resources
such as a domain-specific grammar as well as an out-of-
domain corpus. We refer to the corpus resulting from
such techniques as a domain-specific derived corpus in
contrast to a domain-specific collected corpus. The idea
is that the derived domain-specific corpus would obvi-
ate the need for in-domain corpus collection. In partic-
ular, we are interested in techniques that would result
in corpora such that the performance of language mod-
els trained on these corpora would rival the performance
of models trained on corpora collected specifically for a
specific domain. We investigate these techniques in the
context of MATCH.
We use the notation for the corpus, for the lan-
guage model built using the corpus , and for the
language model acceptor representation of the model ,
which can be used in Equation 2 above.
</bodyText>
<subsectionHeader confidence="0.99133">
3.1 Language Model using in-domain corpus
</subsectionHeader>
<bodyText confidence="0.999939736842105">
In order to evaluate the MATCH system, we collected a
corpus of multimodal utterances for the MATCH domain
in a laboratory setting from a set of sixteen first time
users (8 male, 8 female). We use this corpus to estab-
lish a point of reference to compare the models trained on
derived corpora against models trained on an in-domain
corpus. A total of 833 user interactions (218 multimodal
/ 491 speech-only / 124 pen-only) resulting from six sam-
ple task scenarios involving finding restaurants of various
types and getting their names, phones, addresses, or re-
views, and getting subway directions between locations
were collected and annotated. The data collected was
conversational speech where the users gestured and spoke
freely. We built a class-based trigram language model
( ) using the 709 multimodal and speech-only
utterances as the corpus ( ). The performance
of this model serves as the point of reference to compare
the performance of language models trained on derived
corpora.
</bodyText>
<subsectionHeader confidence="0.998516">
3.2 Grammar as Language Model
</subsectionHeader>
<bodyText confidence="0.999996">
The multimodal CFG (a fragment is presented in Sec-
tion 2) encodes the repertoire of language and ges-
ture commands allowed by the system and their com-
bined interpretations. The CFG can be approximated by
an FSM with arcs labeled with language, gesture and
meaning symbols, using well-known compilation tech-
niques (Nederhof, 1997). The resulting FSM can be pro-
jected on the language component and can be used as
the language model acceptor ( ) for speech recog-
nition. Note that the resulting language model acceptor
is unweighted if the grammar is unweighted and suffers
from not being robust to language variations in user’s in-
put. However, due to the tight coupling of the grammar
used for recognition and interpretion, every recognized
string can be assigned an interpretation (though it may
not necessarily be the intended interpretation).
</bodyText>
<subsectionHeader confidence="0.998546">
3.3 Grammar-based N-gram Language Model
</subsectionHeader>
<bodyText confidence="0.9990328">
As mentioned earlier, a hand-crafted grammar typically
suffers from the problem of being too restrictive and in-
adequate to cover the variations and extra-grammaticality
of user’s input. In contrast, an N-gram language model
derives its robustness by permitting all strings over an al-
phabet, albeit with different likelihoods. In an attempt
to provide robustness to the grammar-based model, we
created a corpus ( ) of sentences by randomly
sampling the set of paths of the grammar ( ) and
built a class-based N-gram language model( ) us-
ing this corpus. Although this corpus might not represent
the true distribution of sentences in the MATCH domain,
we are able to derive some of the benefits of N-gram lan-
guage modeling techniques. This technique is similar to
Galescu et.al (1998).
</bodyText>
<subsectionHeader confidence="0.9996">
3.4 Combining Grammar and Corpus
</subsectionHeader>
<bodyText confidence="0.970676090909091">
A straightforward extension of the idea of sampling the
grammar in order to create a corpus is to select those
sentences out of the grammar which make the result-
ing corpus “similar” to the corpus collected in the pi-
lot studies. In order to create this corpus, we choose
the most likely sentences as determined by a language
model ( ) built using the collected corpus. A
mixture model ( ) with mixture weight ( ) is built by
interpolating the model trained on the corpus of extracted
sentences ( ) and the model trained on the collected
corpus ( ).
</bodyText>
<subsectionHeader confidence="0.881991">
3.5 Class-based Out-of-domain Language Model
</subsectionHeader>
<bodyText confidence="0.9999951">
An alternative to using in-domain corpora for building
language models is to “migrate” a corpus of a different
domain to the MATCH domain. The process of migrat-
ing a corpus involves suitably generalizing the corpus to
remove information specific only to the out-of-domain
and instantiating the generalized corpus to the MATCH
domain. Although there are a number of ways of gener-
alizing the out-of-domain corpus, the generalization we
have investigated involved identifying linguistic units,
such as noun and verb chunks in the out-of-domain cor-
pus and treating them as classes. These classes are then
instantiated to the corresponding linguistic units from the
MATCH domain. The identification of the linguistic units
in the out-of-domain corpus is done automatically using
a supertagger (Bangalore and Joshi, 1999). We use a cor-
pus collected in the context of a software helpdesk ap-
plication as an example out-of-domain corpus. In cases
where the out-of-domain corpus is closely related to the
domain at hand, a more semantically driven generaliza-
tion might be more suitable.
</bodyText>
<subsectionHeader confidence="0.998212">
3.6 Adapting the SwitchBoard Language Model
</subsectionHeader>
<bodyText confidence="0.998756">
We investigate the performance of a large vocabulary
conversational speech recognition system when applied
to a specific domain such as MATCH. We used the
Switchboard corpus ( ) as an example of a large vo-
cabulary conversational speech corpus. We built a tri-
gram model ( ) using the 5.4 million word corpus
and investigated the effect of adapting the Switchboard
language model given in-domain untranscribed speech
utterances ( ). The adaptation is done by first rec-
ognizing the in-domain speech utterances and then build-
ing a language model ( ) from the corpus of recog-
nized text ( ). This bootstrapping mechanism can
be used to derive an domain-specific corpus and language
model without any transcriptions. Similar techniques for
unsupervised language model adaptation are presented
in (Bacchiani and Roark, 2003; Souvignier and Kellner,
1998).
</bodyText>
<subsectionHeader confidence="0.997384">
3.7 Adapting a wide-coverage grammar
</subsectionHeader>
<bodyText confidence="0.99999068">
There have been a number of computational implemen-
tations of wide-coverage, domain-independent, syntac-
tic grammars for English in various formalisms (XTAG,
2001; Clark and Hockenmaier, 2002; Flickinger et al.,
2000). Here, we describe a method that exploits one
such grammar implementation in the Lexicalized Tree-
Adjoining Grammar (LTAG) formalism, for deriving
domain-specific corpora. An LTAG consists of a set of
elementary trees (Supertags) (Bangalore and Joshi, 1999)
each associated with a lexical item. The set of sentences
generated by an LTAG can be obtained by combining su-
pertags using substitution and adjunction operations. In
related work (Rambow et al., 2002), it has been shown
that for a restricted version of LTAG, the combinations
of a set of supertags can be represented as an FSM. This
FSM compactly encodes the set of sentences generated
by an LTAG grammar.
We derive a domain-specific corpus by constructing
a lexicon consisting of pairings of words with their su-
pertags that are relevant to that domain. We then com-
pile the grammar to build an FSM of all sentences upto a
given length. We sample this FSM and build a language
model as discussed in Section 3.3. Given untranscribed
utterances from a specific domain, we can also adapt the
language model as discussed in Section 3.6.
</bodyText>
<sectionHeader confidence="0.993631" genericHeader="method">
4 Robust Multimodal Understanding
</sectionHeader>
<bodyText confidence="0.999942625">
The grammar-based interpreter uses composition oper-
ation on FSTs to transduce multimodal strings (ges-
ture,speech) to an interpretation. The set of speech strings
that can be assigned an interpretation are exactly those
that are represented in the grammar. It is to be expected
that the accuracy of meaning representation will be rea-
sonable, if the user’s input matches one of the multimodal
strings encoded in the grammar. But for those user inputs
that are not encoded in the grammar, the system will not
return a meaning representation. In order to improve the
usability of the system, we expect it to produce a (partial)
meaning representation, irrespective of the grammatical-
ity of the user’s input and the coverage limitations of the
grammar. It is this aspect that we refer to as robustness in
understanding. We present below two approaches to ro-
bust multimodal understanding that we have developed.
</bodyText>
<subsectionHeader confidence="0.96737">
4.1 Pattern Matching Approach
</subsectionHeader>
<bodyText confidence="0.999712277777778">
In order to overcome the possible mismatch between
the user’s input and the language encoded in the multi-
modal grammar ( ), we use an edit-distance based pat-
tern matching algorithm to coerce the set of strings ( )
encoded in the lattice resulting from ASR ( ) to match
one of the strings that can be assigned an interpretation.
The edit operations (insertion, deletion, substitution) can
either be word-based or phone-based and are associated
with a cost. These costs can be tuned based on the
word/phone confusions present in the domain. The edit
operations are encoded as an transducer ( ) as shown
in Figure 5 and can apply to both one-best and lattice out-
put of the recognizer. We are interested in the string with
the least number of edits ( ) that can be assigned
an interpretation by the grammar. This can be achieved
by composition ( ) of transducers followed by a search
for the least cost path through a weighted transducer as
shown below.
</bodyText>
<figure confidence="0.76948">
(6)
</figure>
<figureCaption confidence="0.998213">
Figure 5: Edit transducer with insertion, deletion, sub-
</figureCaption>
<bodyText confidence="0.999344428571429">
stitution and identity arcs. and could be words or
phones. The costs on the arcs are setup such that scost
&lt; icost + dcost.
This approach is akin to example-based techniques
used in other areas of NLP such as machine translation.
In our case, the set of examples (encoded by the gram-
mar) is represented as a finite-state machine.
</bodyText>
<subsectionHeader confidence="0.967333">
4.2 Classification-based Approach
</subsectionHeader>
<bodyText confidence="0.999660769230769">
A second approach is to view robust multimodal under-
standing as a sequence of classification problems in or-
der to determine the predicate and arguments of an ut-
terance. The meaning representation shown in (1) con-
sists of an predicate (the command attribute) and a se-
quence of one or more argument attributes which are the
parameters for the successful interpretation of the user’s
intent. For example, in (1), is the predicate
and is the set of ar-
guments to the predicate.
We determine the predicate ( ) for a token multi-
modal utterance ( ) by maximizing the posterior prob-
ability as shown in Equation 7.
</bodyText>
<equation confidence="0.872054">
(7)
</equation>
<bodyText confidence="0.993459625">
We view the problem of identifying and extracting ar-
guments from a multimodal input as a problem of asso-
ciating each token of the input with a specific tag that
encodes the label of the argument and the span of the ar-
gument. These tags are drawn from a tagset which is con-
structed by extending each argument label by three addi-
tional symbols , following (Ramshaw and Mar-
cus, 1995). These symbols correspond to cases when a
token is inside ( ) an argument span, outside ( ) an ar-
gument span or at the boundary of two argument spans
( ) (See Table 1).
Given this encoding, the problem of extracting the ar-
guments is a search for the most likely sequence of tags
( ) given the input multimodal utterance as shown
in Equation (8). We approximate the posterior proba-
bility using independence assumptions as
</bodyText>
<table confidence="0.997547375">
User cheap thai upper west side
Utterance
Argument price cheap /price cuisine
Annotation thai /cuisine place upper west
side /place
IOB cheap price B thai cuisine B
Encoding upper place I west place I
side place I
</table>
<tableCaption confidence="0.947346">
Table 1: The I,O,B encoding for argument extraction.
shown in Equation (9).
</tableCaption>
<bodyText confidence="0.994026571428572">
Owing to the large set of features that are used for
predicate identification and argument extraction, we es-
timate the probabilities using a classification model. In
particular, we use the Adaboost classifier (Freund and
Schapire, 1996) wherein a highly accurate classifier is
build by combining many “weak” or “simple” base classi-
fiers , each of which may only be moderately accurate.
The selection of the weak classifiers proceeds iteratively
picking the weak classifier that correctly classifies the ex-
amples that are misclassified by the previously selected
weak classifiers. Each weak classifier is associated with
a weight ( ) that reflects its contribution towards mini-
mizing the classification error. The posterior probability
of is computed as in Equation 10.
</bodyText>
<equation confidence="0.872474">
(10)
</equation>
<bodyText confidence="0.99984525">
It should be noted that the data for training the clas-
sifiers can be collected from the domain or derived from
an in-domain grammar using techniques similar to those
presented in Section 3.
</bodyText>
<sectionHeader confidence="0.996659" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999970222222222">
We describe a set of experiments to evaluate the perfor-
mance of the speech recognizer and the concept accu-
racy of speech only and speech and gesture exchanges in
our MATCH multimodal system. We use word accuracy
and string accuracy for evaluating ASR output. All re-
sults presented in this section are based on 10-fold cross-
validation experiments run on the 709 spoken and multi-
modal exchanges collected from the pilot study described
in Section 3.1.
</bodyText>
<subsectionHeader confidence="0.890027">
5.1 Language Model
</subsectionHeader>
<bodyText confidence="0.996406625">
Table 2 presents the performance results for ASR word
and sentence accuracy using language models trained on
collected in-domain corpus as well as on corpora derived
using the different methods discussed in Section 3. For
the class-based models mentioned in the table, we defined
different classes based on areas of interest (eg. riverside
park, turtle pond), points of interest (eg. Ellis Island,
United Nations Building), type of cuisine (eg. Afghani,
</bodyText>
<table confidence="0.9936903">
w; :wj /scost
wi :6 /dcost 6 : wi /icost
w; :w; /0
Scenario ASR Word Accuracy Sentence Accuracy
Grammar Based Grammar as Language Model 41.6 38.0
Class-based N-gram Language Model 60.6 42.9
In-domain Data Class-based N-gram Model 73.8 57.1
Grammar+In-domain Data Class-based N-gram Model 75.0 59.5
Out-of-domain N-gram Model 17.6 17.5
Class-based N-gram Model 58.4 38.8
Class-based N-gram Model 64.0 45.4
with Grammar-based N-gram
Language Model
SwitchBoard N-gram Model 43.5 25.0
Language model trained on 55.7 36.3
recognized in-domain data
Wide-coverage N-gram Model 43.7 24.8
Grammar
Language model trained on 55.8 36.2
recognized in-domain data
</table>
<tableCaption confidence="0.9875225">
Table 2: Performance results for ASR Word and Sentence accuracy using models trained on data derived from different
methods of bootstrapping domain-specific data.
</tableCaption>
<bodyText confidence="0.999847607843138">
Indonesian), price categories (eg. moderately priced, ex-
pensive), and neighborhoods (eg. Upper East Side, Chi-
natown).
It is immediately apparent that the hand-crafted gram-
mar as language model performs poorly and a language
model trained on the collected domain-specific corpus
performs significantly better than models trained on de-
rived data. However, it is encouraging to note that a
model trained on a derived corpus (obtained from com-
bining migrated out-of-domain corpus and a corpus cre-
ated by sampling in-domain grammar) is within 10%
word accuracy as compared to the model trained on the
collected corpus. There are several other noteworthy ob-
servations from these experiments.
The performance of the language model trained on data
sampled from the grammar is dramatically better as com-
pared to the performance of the hand-crafted grammar.
This technique provides a promising direction for author-
ing portable grammars that can be sampled subsequently
to build robust language models when no in-domain cor-
pora are available. Furthermore, combining grammar and
in-domain data as described in Section 3.4, outperforms
all other models significantly.
For the experiment on migration of out-of-domain cor-
pus, we used a corpus from a software helpdesk appli-
cation. Table 2 shows that the migration of data using
linguistic units as described in Section 3.5 significantly
outperforms a model trained only on the out-of-domain
corpus. Also, combining the grammar sampled corpus
with the migrated corpus provides a further improvement.
The performance of the SwitchBoard model on the
MATCH domain is presented in Table 2. We built a tri-
gram model using a 5.4 million word SwitchBoard cor-
pus and investigated the effect of adapting the resulting
language model on in-domain untranscribed speech ut-
terances. The adaptation is done by first recognizing the
training partition of the in-domain speech utterances and
then building a language model from the recognized text.
We observe that although the performance of the Switch-
Board language model on the MATCH domain is poorer
than the performance of a model obtained by migrating
data from a related domain, the performance can be sig-
nificantly improved using the adaptation technique.
The last row of Table 2 shows the results of using
the MATCH specific lexicon to generate a corpus us-
ing a wide-coverage grammar, training a language model
and adapting the resulting model using in-domain untran-
scribed speech utterances as was done for the Switch-
Board model. The class-based trigram model was built
using 500,000 randomly sampled paths from the network
constructed by the procedure described in Section 3.7.
</bodyText>
<subsectionHeader confidence="0.999397">
5.2 Multimodal Understanding
</subsectionHeader>
<bodyText confidence="0.999914043478261">
In this section, we present results on multimodal under-
standing using the two techniques presented in Section 4.
We use concept token accuracy and concept string accu-
racy as evaluation metrics for the entire meaning repre-
sentation in these experiments. These metrics correspond
to the word accuracy and string accuracy metrics used for
ASR evaluation. In order to provide a finer-grained eval-
uation, we breakdown the concept accuracy in terms of
the accuracy of identifying the predicates and arguments.
Again, we use string accuracy metrics to evaluate pred-
icate and argument accuracy. We use the output of the
ASR with the language model trained on the collected
data (word accuracy of 73.8%) as the input to the under-
standing component.
The grammar-based multimodal understanding system
composes the input multimodal string with the multi-
modal grammar represented as an FST to produce an in-
terpretation. Thus an interpretation can be assigned to
only those multimodal strings that are encoded in the
grammar. However, the result of ASR and gesture recog-
nition may not be one of the strings encoded in the gram-
mar, and such strings are not assigned an interpretation.
This fact is reflected in the low concept string accuracy
</bodyText>
<table confidence="0.9987535">
Predicate String Argument String Concept Token Concept String
Accuracy(%) Accuracy(%) Accuracy(%) Accuracy(%)
Baseline 65.2 52.1 53.5 45.2
Word-based Pattern-Matching 73.7 62.4 68.1 59.0
Phone-based Pattern-Matching 73.7 63.8 67.8 61.3
Classification-based 84.1 59.1 73.5 56.4
</table>
<tableCaption confidence="0.999663">
Table 3: Performance results of robust multimodal understanding
</tableCaption>
<bodyText confidence="0.999847368421053">
for the baseline as shown in Table 3.
The pattern-matching based robust understanding ap-
proach mediates the mismatch between the strings that
are output by ASR and the strings that can be assigned an
interpretation. We experimented with word based pattern
matching as well as phone based pattern matching on the
one-best output of the recognizer. As shown in Table 3,
the pattern-matching robust understanding approach im-
proves the concept accuracy over the baseline signifi-
cantly. Furthermore, the phone-based matching method
has a similar performace to the word-based matching
method.
For the classification-based approach to robust under-
standing we used a total of 10 predicates such as help, as-
sert, inforequest, and 20 argument types such as cuisine,
price, location. We use unigrams, bigrams and trigrams
appearing in the multimodal utterance as weak classifiers
for the purpose of predicate classification. In order to
predict the tag of a word for argument extraction, we use
the left and right trigram context and the tags for the pre-
ceding two tokens as weak classifiers. The results are
presented in Table 3.
Both the approaches to robust understanding outper-
form the baseline model significantly. However, it is in-
teresting to note that while the pattern-matching based
approach has a better argument extraction accuracy, the
classification based approach has a better predicate iden-
tification accuracy. Two possible reasons for this are:
first, argument extraction requires more non-local infor-
mation that is available in the pattern-matching based ap-
proach while the classification-based approach relies on
local information and is more conducive for identifying
the simple predicates in MATCH. Second, the pattern-
matching approach uses the entire grammar as a model
for matching while the classification approach is trained
on the training data which is significantly smaller when
compared to the number of examples encoded in the
grammar.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999866333333333">
Although we are not aware of any attempts to address
the issue of robust understanding in the context of multi-
modal systems, this issue has been of great interest in the
context of speech-only conversational systems (Dowd-
ing et al., 1993; Seneff, 1992; Allen et al., 2000; Lavie,
1996). The output of the recognizer in these systems usu-
ally is parsed using a handcrafted grammar that assigns
a meaning representation suited for the downstream dia-
log component. The coverage problems of the grammar
and parsing of extra-grammatical utterances is typically
addressed by retrieving fragments from the parse chart
and incorporating operations that combine fragments to
derive a meaning of the recognized utterance. We have
presented an approach that achieves robust multimodal
utterance understanding using the edit-distance automa-
ton in a finite-state-based interpreter without the need for
combining fragments from a parser.
The issue of combining rule-based and data-driven ap-
proaches has received less attention, with the exception
of a few (Wang et al., 2000; Rayner and Hockey, 2003;
Wang and Acero, 2003). In a recent paper (Rayner and
Hockey, 2003), the authors address this issue by em-
ploying a decision-list-based speech understanding sys-
tem as a means of progressing from rule-based models
to data-driven models when data becomes available. The
decision-list-based understanding system also provides a
method for robust understanding. In contrast, the ap-
proach presented in this paper can be used on lattices of
speech and gestures to produce a lattice of meaning rep-
resentations.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999972352941176">
In this paper, we have addressed how to rapidly proto-
type multimodal conversational systems without relying
on the collection of domain-specific corpora. We have
presented several techniques that exploit domain-specific
grammars, reuse out-of-domain corpora and adapt large
conversational corpora and wide-coverage grammars to
derive a domain-specific corpus. We have demonstrated
that a language model trained on a derived corpus per-
forms within 10% word accuracy of a language model
trained on collected domain-specific corpus, suggest-
ing a method of building an initial language model
without having to collect domain-specific corpora. We
have also presented and evaluated pattern-matching and
classification-based approaches to improve the robust-
ness of multimodal understanding. We have presented re-
sults for these approaches in the context of a multimodal
city guide application (MATCH).
</bodyText>
<sectionHeader confidence="0.998704" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.977090333333333">
We thank Patrick Ehlen, Amanda Stent, Helen Hastie,
Candy Kamm, Marilyn Walker, and Steve Whittaker for
their contributions to the MATCH system. We also thank
Allen Gorin, Mazin Rahim, Giuseppe Riccardi, and Juer-
gen Schroeter for their comments on earlier versions of
this paper.
</bodyText>
<sectionHeader confidence="0.993221" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999839193877551">
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. JNLE, 6(3).
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In In Proc. Int. Conf. Acous-
tic,Speech,Signal Processing.
S. Bangalore and M. Johnston. 2000. Tight-coupling of
multimodal language processing with speech recogni-
tion. In Proceedings ofICSLP, Beijing, China.
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
approach to almost parsing. Computational Linguis-
tics, 25(2).
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, and
A. Syrdal. 1999. The AT&amp;T next-generation TTS. In
In Joint Meeting ofASA; EAA and DAGA.
M. Boros, W. Eckert, F. Gallwitz, G. G˘orz, G. Hanrieder,
and H. Niemann. 1996. Towards Understanding Spon-
taneous Speech: Word Accuracy vs. Concept Accu-
racy. In Proceedings ofICSLP, Philadelphia.
Stephen Clark and Julia Hockenmaier. 2002. Evaluating
a wide-coverage CCG parser. In Proceedings of the
LREC 2002 Beyond Parseval Workshop, Las Palmas,
Spain.
J. Dowding, J. M. Gawron, D. E. Appelt, J. Bear,
L. Cherny, R. Moore, and D. B. Moran. 1993. GEM-
INI: A natural language system for spoken-language
understanding. In Proceedings ofACL, pages 54–61.
D. Flickinger, A. Copestake, and I. Sag. 2000. Hpsg
analysis of english. In W. Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translation, pages
254–263. Springer–Verlag, Berlin, Heidelberg, New
York.
Y. Freund and R. E. Schapire. 1996. Experiments with
a new boosting alogrithm. In Machine Learning: Pro-
ceedings of the Thirteenth International Conference,
pages 148–156.
L. Galescu, E. K. Ringger, and J. F. Allen. 1998. Rapid
language model development for new task domains. In
Proceedings of the ELRA First International Confer-
ence on Language Resources and Evaluation (LREC),
Granada, Spain.
M. Johnston and S. Bangalore. 2000. Finite-state mul-
timodal parsing and understanding. In Proceedings of
COLING, Saarbr¨ucken, Germany.
M. Johnston, S. Bangalore, A. Stent, G. Vasireddy, and
P. Ehlen. 2002a. Multimodal language processing for
mobile information access. In In Proceedings of IC-
SLP, Denver, CO.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Maloor.
2002b. MATCH: An architecture for multimodal di-
alog systems. In Proceedings ofACL, Philadelphia.
A. Lavie. 1996. GLR*: A Robust Grammar-Focused
Parser for Spontaneously Spoken Language. Ph.D.
thesis, Carnegie Mellon University.
M-J. Nederhof. 1997. Regular approximations of CFLs:
A grammatical view. In Proceedings of the Interna-
tional Workshop on Parsing Technology, Boston.
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech recognition by composition of weighted finite
automata. In E. Roche and Schabes Y., editors, Finite
State Devices for Natural Language Processing, pages
431–456. MIT Press, Cambridge, Massachusetts.
Owen Rambow, Srinivas Bangalore, Tahir Butt, Alexis
Nasr, and Richard Sproat. 2002. Creating a finite-
state parser with application semantics. In In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics (COLING 2002), Taipei, Taiwan.
Lance Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Pro-
ceedings of the Third Workshop on Very Large Cor-
pora, MIT, Cambridge, Boston.
M. Rayner and B. A. Hockey. 2003. Transparent com-
bination of rule-based and data-driven approaches in
speech understanding. In In Proceedings of the EACL
2003.
S. Seneff. 1992. A relaxation method for understand-
ing spontaneous speech utterances. In Proceedings,
Speech and Natural Language Workshop, San Mateo,
CA.
R.D. Sharp, E. Bocchieri, C. Castillo, S. Parthasarathy,
C. Rath, M. Riley, and J.Rowland. 1997. The Wat-
son speech recognition engine. In In Proceedings of
ICASSP, pages 4065–4068.
B. Souvignier and A. Kellner. 1998. Online adaptation
for language models in spoken dialogue systems. In
Int. Conference on Spoken Language Processing (IC-
SLP).
Y. Wang and A. Acero. 2003. Combination of cfg and
n-gram modeling in semantic grammar learning. In
In Proceedings ofthe Eurospeech Conference, Geneva,
Switzerland.
Y.Y. Wang, M. Mahajan, and X. Huang. 2000. Unified
Context-Free Grammar and N-Gram Model for Spo-
ken Language Processing. In Proceedings ofICASSP.
XTAG. 2001. A lexicalized tree-adjoining grammar for
english. Technical report, University of Pennsylvania,
http://www.cis.upenn.edu/ xtag/gramrelease.html.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.744127">
<title confidence="0.9996575">Balancing Data-driven and Rule-based Approaches in the Context of Multimodal Conversational System</title>
<author confidence="0.84603">Srinivas</author>
<affiliation confidence="0.988467">AT&amp;T</affiliation>
<address confidence="0.968982">180 Park Florham Park, NJ</address>
<email confidence="0.999401">srini@research.att.com</email>
<author confidence="0.993246">Michael</author>
<affiliation confidence="0.997944">AT&amp;T</affiliation>
<address confidence="0.9790345">180 Park Florham Park, NJ</address>
<email confidence="0.999605">johnston@research.att.com</email>
<abstract confidence="0.999385363636364">Moderate-sized rule-based spoken language models for recognition and understanding are easy to develop and provide the ability to rapidly prototype conversational applications. However, scalability of such systems is a bottleneck due to the heavy cost of authoring and maintenance of rule sets and inevitable brittleness due to lack of coverage in the rule sets. In contrast, data-driven approaches are robust and the procedure for model building is usually simple. However, the lack of data in a particular application domain limits the ability to build data-driven models. In this paper, we address the issue of combining data-driven and grammar-based models for rapid prototyping of robust speech recognition and understanding models for a multimodal conversational system. We also present methods that reuse data from different domains and investigate the limits of such models in the context of a particular application domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>D Byron</author>
<author>M Dzikovska</author>
<author>G Ferguson</author>
<author>L Galescu</author>
<author>A Stent</author>
</authors>
<title>An architecture for a generic dialogue shell.</title>
<date>2000</date>
<journal>JNLE,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="31980" citStr="Allen et al., 2000" startWordPosition="5120" endWordPosition="5123">local information and is more conducive for identifying the simple predicates in MATCH. Second, the patternmatching approach uses the entire grammar as a model for matching while the classification approach is trained on the training data which is significantly smaller when compared to the number of examples encoded in the grammar. 6 Discussion Although we are not aware of any attempts to address the issue of robust understanding in the context of multimodal systems, this issue has been of great interest in the context of speech-only conversational systems (Dowding et al., 1993; Seneff, 1992; Allen et al., 2000; Lavie, 1996). The output of the recognizer in these systems usually is parsed using a handcrafted grammar that assigns a meaning representation suited for the downstream dialog component. The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a finite-state-based interpreter with</context>
</contexts>
<marker>Allen, Byron, Dzikovska, Ferguson, Galescu, Stent, 2000</marker>
<rawString>J. Allen, D. Byron, M. Dzikovska, G. Ferguson, L. Galescu, and A. Stent. 2000. An architecture for a generic dialogue shell. JNLE, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bacchiani</author>
<author>B Roark</author>
</authors>
<title>Unsupervised language model adaptation. In</title>
<date>2003</date>
<booktitle>In Proc. Int. Conf. Acoustic,Speech,Signal Processing.</booktitle>
<contexts>
<context position="17339" citStr="Bacchiani and Roark, 2003" startWordPosition="2764" endWordPosition="2767">e of a large vocabulary conversational speech corpus. We built a trigram model ( ) using the 5.4 million word corpus and investigated the effect of adapting the Switchboard language model given in-domain untranscribed speech utterances ( ). The adaptation is done by first recognizing the in-domain speech utterances and then building a language model ( ) from the corpus of recognized text ( ). This bootstrapping mechanism can be used to derive an domain-specific corpus and language model without any transcriptions. Similar techniques for unsupervised language model adaptation are presented in (Bacchiani and Roark, 2003; Souvignier and Kellner, 1998). 3.7 Adapting a wide-coverage grammar There have been a number of computational implementations of wide-coverage, domain-independent, syntactic grammars for English in various formalisms (XTAG, 2001; Clark and Hockenmaier, 2002; Flickinger et al., 2000). Here, we describe a method that exploits one such grammar implementation in the Lexicalized TreeAdjoining Grammar (LTAG) formalism, for deriving domain-specific corpora. An LTAG consists of a set of elementary trees (Supertags) (Bangalore and Joshi, 1999) each associated with a lexical item. The set of sentences</context>
</contexts>
<marker>Bacchiani, Roark, 2003</marker>
<rawString>M. Bacchiani and B. Roark. 2003. Unsupervised language model adaptation. In In Proc. Int. Conf. Acoustic,Speech,Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>M Johnston</author>
</authors>
<title>Tight-coupling of multimodal language processing with speech recognition.</title>
<date>2000</date>
<booktitle>In Proceedings ofICSLP,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="6432" citStr="Bangalore and Johnston, 2000" startWordPosition="991" endWordPosition="994"> and TTS prompts. This score is passed back to the Multimodal UI (MUI). The Multimodal UI coordinates presentation of graphical content with synthetic speech output using the AT&amp;T Natural Voices TTS engine (Beutnagel et al., 1999). The subway route constraint solver (SUBWAY) identifies the best route between any two points in New York City. Figure 2: Multimodal Architecture 2.2 Multimodal Integration and Understanding Our approach to integrating and interpreting multimodal inputs (Johnston et al., 2002b; Johnston et al., 2002a) is an extension of the finite-state approach previously proposed (Bangalore and Johnston, 2000; Johnston and Bangalore, 2000). In this approach, a declarative multimodal grammar captures both the structure and the interpretation of multimodal and unimodal commands. The grammar consists of a set of context-free rules. The multimodal aspects of the grammar become apparent in the terminals, each of which is a triple W:G:M, consisting of speech (words, W), gesture (gesture symbols, G), and meaning (meaning symbols, M). The multimodal grammar encodes not just multimodal integration patterns but also the syntax of speech and gesture, and the assignment of meaning. The meaning is represented </context>
<context position="9498" citStr="Bangalore and Johnston, 2000" startWordPosition="1487" endWordPosition="1490"> phone: :phone review: :review DEICNP DDETPL :area: :sel: NUM BEADPL DDETPL these:G: those:G: BEADPL restaurants:rest: rest SEM:SEM: :: /rest NUM two:2: three:3: ... ten:10: Figure 3: Multimodal grammar fragment the XML output. The example above yields the following meaning representation for concept accuracy. The multimodal grammar can be used to create language models for ASR, align the speech and gesture results from the respective recognizers and transform the multimodal utterance to a meaning representation. All these operations are achieved using finite-state transducer operations (See (Bangalore and Johnston, 2000; Johnston and Bangalore, 2000) for details). However, this approach to recognition needs to be more robust to extragrammaticality and language variation in user’s utterances and the interpretation needs to be more robust to speech recognition errors. We address these issues in the rest of the paper. 3 Bootstrapping Corpora for Language Models The problem of speech recognition can be succinctly represented as a search for the most likely word sequence ( ) through the network created by the composition of a language of acoustic observations ( ), an acoustic model which is a transduction from ac</context>
</contexts>
<marker>Bangalore, Johnston, 2000</marker>
<rawString>S. Bangalore and M. Johnston. 2000. Tight-coupling of multimodal language processing with speech recognition. In Proceedings ofICSLP, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="16218" citStr="Bangalore and Joshi, 1999" startWordPosition="2583" endWordPosition="2586">y generalizing the corpus to remove information specific only to the out-of-domain and instantiating the generalized corpus to the MATCH domain. Although there are a number of ways of generalizing the out-of-domain corpus, the generalization we have investigated involved identifying linguistic units, such as noun and verb chunks in the out-of-domain corpus and treating them as classes. These classes are then instantiated to the corresponding linguistic units from the MATCH domain. The identification of the linguistic units in the out-of-domain corpus is done automatically using a supertagger (Bangalore and Joshi, 1999). We use a corpus collected in the context of a software helpdesk application as an example out-of-domain corpus. In cases where the out-of-domain corpus is closely related to the domain at hand, a more semantically driven generalization might be more suitable. 3.6 Adapting the SwitchBoard Language Model We investigate the performance of a large vocabulary conversational speech recognition system when applied to a specific domain such as MATCH. We used the Switchboard corpus ( ) as an example of a large vocabulary conversational speech corpus. We built a trigram model ( ) using the 5.4 million</context>
<context position="17881" citStr="Bangalore and Joshi, 1999" startWordPosition="2840" endWordPosition="2843">r unsupervised language model adaptation are presented in (Bacchiani and Roark, 2003; Souvignier and Kellner, 1998). 3.7 Adapting a wide-coverage grammar There have been a number of computational implementations of wide-coverage, domain-independent, syntactic grammars for English in various formalisms (XTAG, 2001; Clark and Hockenmaier, 2002; Flickinger et al., 2000). Here, we describe a method that exploits one such grammar implementation in the Lexicalized TreeAdjoining Grammar (LTAG) formalism, for deriving domain-specific corpora. An LTAG consists of a set of elementary trees (Supertags) (Bangalore and Joshi, 1999) each associated with a lexical item. The set of sentences generated by an LTAG can be obtained by combining supertags using substitution and adjunction operations. In related work (Rambow et al., 2002), it has been shown that for a restricted version of LTAG, the combinations of a set of supertags can be represented as an FSM. This FSM compactly encodes the set of sentences generated by an LTAG grammar. We derive a domain-specific corpus by constructing a lexicon consisting of pairings of words with their supertags that are relevant to that domain. We then compile the grammar to build an FSM </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>S. Bangalore and A. K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Beutnagel</author>
<author>A Conkie</author>
<author>J Schroeter</author>
<author>Y Stylianou</author>
<author>A Syrdal</author>
</authors>
<date>1999</date>
<booktitle>The AT&amp;T next-generation TTS. In In Joint Meeting ofASA; EAA and DAGA.</booktitle>
<contexts>
<context position="6034" citStr="Beutnagel et al., 1999" startWordPosition="932" endWordPosition="935">al., 2002b), which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM enters into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating a coordinated sequence of graphical actions and TTS prompts. This score is passed back to the Multimodal UI (MUI). The Multimodal UI coordinates presentation of graphical content with synthetic speech output using the AT&amp;T Natural Voices TTS engine (Beutnagel et al., 1999). The subway route constraint solver (SUBWAY) identifies the best route between any two points in New York City. Figure 2: Multimodal Architecture 2.2 Multimodal Integration and Understanding Our approach to integrating and interpreting multimodal inputs (Johnston et al., 2002b; Johnston et al., 2002a) is an extension of the finite-state approach previously proposed (Bangalore and Johnston, 2000; Johnston and Bangalore, 2000). In this approach, a declarative multimodal grammar captures both the structure and the interpretation of multimodal and unimodal commands. The grammar consists of a set </context>
</contexts>
<marker>Beutnagel, Conkie, Schroeter, Stylianou, Syrdal, 1999</marker>
<rawString>M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, and A. Syrdal. 1999. The AT&amp;T next-generation TTS. In In Joint Meeting ofASA; EAA and DAGA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Boros</author>
<author>W Eckert</author>
<author>F Gallwitz</author>
<author>G G˘orz</author>
<author>G Hanrieder</author>
<author>H Niemann</author>
</authors>
<title>Towards Understanding Spontaneous Speech: Word Accuracy vs. Concept Accuracy.</title>
<date>1996</date>
<booktitle>In Proceedings ofICSLP,</booktitle>
<location>Philadelphia.</location>
<marker>Boros, Eckert, Gallwitz, G˘orz, Hanrieder, Niemann, 1996</marker>
<rawString>M. Boros, W. Eckert, F. Gallwitz, G. G˘orz, G. Hanrieder, and H. Niemann. 1996. Towards Understanding Spontaneous Speech: Word Accuracy vs. Concept Accuracy. In Proceedings ofICSLP, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Evaluating a wide-coverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC 2002 Beyond Parseval Workshop,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="17598" citStr="Clark and Hockenmaier, 2002" startWordPosition="2799" endWordPosition="2802">s done by first recognizing the in-domain speech utterances and then building a language model ( ) from the corpus of recognized text ( ). This bootstrapping mechanism can be used to derive an domain-specific corpus and language model without any transcriptions. Similar techniques for unsupervised language model adaptation are presented in (Bacchiani and Roark, 2003; Souvignier and Kellner, 1998). 3.7 Adapting a wide-coverage grammar There have been a number of computational implementations of wide-coverage, domain-independent, syntactic grammars for English in various formalisms (XTAG, 2001; Clark and Hockenmaier, 2002; Flickinger et al., 2000). Here, we describe a method that exploits one such grammar implementation in the Lexicalized TreeAdjoining Grammar (LTAG) formalism, for deriving domain-specific corpora. An LTAG consists of a set of elementary trees (Supertags) (Bangalore and Joshi, 1999) each associated with a lexical item. The set of sentences generated by an LTAG can be obtained by combining supertags using substitution and adjunction operations. In related work (Rambow et al., 2002), it has been shown that for a restricted version of LTAG, the combinations of a set of supertags can be represente</context>
</contexts>
<marker>Clark, Hockenmaier, 2002</marker>
<rawString>Stephen Clark and Julia Hockenmaier. 2002. Evaluating a wide-coverage CCG parser. In Proceedings of the LREC 2002 Beyond Parseval Workshop, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>J M Gawron</author>
<author>D E Appelt</author>
<author>J Bear</author>
<author>L Cherny</author>
<author>R Moore</author>
<author>D B Moran</author>
</authors>
<title>GEMINI: A natural language system for spoken-language understanding.</title>
<date>1993</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>54--61</pages>
<contexts>
<context position="31946" citStr="Dowding et al., 1993" startWordPosition="5113" endWordPosition="5117">sification-based approach relies on local information and is more conducive for identifying the simple predicates in MATCH. Second, the patternmatching approach uses the entire grammar as a model for matching while the classification approach is trained on the training data which is significantly smaller when compared to the number of examples encoded in the grammar. 6 Discussion Although we are not aware of any attempts to address the issue of robust understanding in the context of multimodal systems, this issue has been of great interest in the context of speech-only conversational systems (Dowding et al., 1993; Seneff, 1992; Allen et al., 2000; Lavie, 1996). The output of the recognizer in these systems usually is parsed using a handcrafted grammar that assigns a meaning representation suited for the downstream dialog component. The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a f</context>
</contexts>
<marker>Dowding, Gawron, Appelt, Bear, Cherny, Moore, Moran, 1993</marker>
<rawString>J. Dowding, J. M. Gawron, D. E. Appelt, J. Bear, L. Cherny, R. Moore, and D. B. Moran. 1993. GEMINI: A natural language system for spoken-language understanding. In Proceedings ofACL, pages 54–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>A Copestake</author>
<author>I Sag</author>
</authors>
<title>Hpsg analysis of english.</title>
<date>2000</date>
<booktitle>Verbmobil: Foundations of Speech-to-Speech Translation,</booktitle>
<pages>254--263</pages>
<editor>In W. Wahlster, editor,</editor>
<publisher>Springer–Verlag,</publisher>
<location>Berlin, Heidelberg, New York.</location>
<contexts>
<context position="17624" citStr="Flickinger et al., 2000" startWordPosition="2803" endWordPosition="2806">he in-domain speech utterances and then building a language model ( ) from the corpus of recognized text ( ). This bootstrapping mechanism can be used to derive an domain-specific corpus and language model without any transcriptions. Similar techniques for unsupervised language model adaptation are presented in (Bacchiani and Roark, 2003; Souvignier and Kellner, 1998). 3.7 Adapting a wide-coverage grammar There have been a number of computational implementations of wide-coverage, domain-independent, syntactic grammars for English in various formalisms (XTAG, 2001; Clark and Hockenmaier, 2002; Flickinger et al., 2000). Here, we describe a method that exploits one such grammar implementation in the Lexicalized TreeAdjoining Grammar (LTAG) formalism, for deriving domain-specific corpora. An LTAG consists of a set of elementary trees (Supertags) (Bangalore and Joshi, 1999) each associated with a lexical item. The set of sentences generated by an LTAG can be obtained by combining supertags using substitution and adjunction operations. In related work (Rambow et al., 2002), it has been shown that for a restricted version of LTAG, the combinations of a set of supertags can be represented as an FSM. This FSM comp</context>
</contexts>
<marker>Flickinger, Copestake, Sag, 2000</marker>
<rawString>D. Flickinger, A. Copestake, and I. Sag. 2000. Hpsg analysis of english. In W. Wahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation, pages 254–263. Springer–Verlag, Berlin, Heidelberg, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Experiments with a new boosting alogrithm.</title>
<date>1996</date>
<booktitle>In Machine Learning: Proceedings of the Thirteenth International Conference,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="22995" citStr="Freund and Schapire, 1996" startWordPosition="3720" endWordPosition="3723">s shown in Equation (8). We approximate the posterior probability using independence assumptions as User cheap thai upper west side Utterance Argument price cheap /price cuisine Annotation thai /cuisine place upper west side /place IOB cheap price B thai cuisine B Encoding upper place I west place I side place I Table 1: The I,O,B encoding for argument extraction. shown in Equation (9). Owing to the large set of features that are used for predicate identification and argument extraction, we estimate the probabilities using a classification model. In particular, we use the Adaboost classifier (Freund and Schapire, 1996) wherein a highly accurate classifier is build by combining many “weak” or “simple” base classifiers , each of which may only be moderately accurate. The selection of the weak classifiers proceeds iteratively picking the weak classifier that correctly classifies the examples that are misclassified by the previously selected weak classifiers. Each weak classifier is associated with a weight ( ) that reflects its contribution towards minimizing the classification error. The posterior probability of is computed as in Equation 10. (10) It should be noted that the data for training the classifiers </context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and R. E. Schapire. 1996. Experiments with a new boosting alogrithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Galescu</author>
<author>E K Ringger</author>
<author>J F Allen</author>
</authors>
<title>Rapid language model development for new task domains.</title>
<date>1998</date>
<booktitle>In Proceedings of the ELRA First International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Granada,</location>
<marker>Galescu, Ringger, Allen, 1998</marker>
<rawString>L. Galescu, E. K. Ringger, and J. F. Allen. 1998. Rapid language model development for new task domains. In Proceedings of the ELRA First International Conference on Language Resources and Evaluation (LREC), Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
</authors>
<title>Finite-state multimodal parsing and understanding.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING, Saarbr¨ucken,</booktitle>
<location>Germany.</location>
<contexts>
<context position="5162" citStr="Johnston and Bangalore, 2000" startWordPosition="793" endWordPosition="796">he underlying architecture that supports MATCH consists of a series of re-usable components which communicate over sockets through a facilitator (MCUBE) (Figure 2). Users interact with the system through a Multimodal User Interface Client (MUI). Their speech and ink are processed by speech recognition (Sharp et al., 1997) (ASR) and handwriting/gesture recognition (GESTURE, HW RECO) components respectively. These recognition processes result in lattices of potential words and gestures. These are then combined and assigned a meaning representation using a multimodal finite-state device (MMFST) (Johnston and Bangalore, 2000; Johnston et al., 2002b). This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs. This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al., 2002b), which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM enters into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating a</context>
<context position="6463" citStr="Johnston and Bangalore, 2000" startWordPosition="995" endWordPosition="999">s passed back to the Multimodal UI (MUI). The Multimodal UI coordinates presentation of graphical content with synthetic speech output using the AT&amp;T Natural Voices TTS engine (Beutnagel et al., 1999). The subway route constraint solver (SUBWAY) identifies the best route between any two points in New York City. Figure 2: Multimodal Architecture 2.2 Multimodal Integration and Understanding Our approach to integrating and interpreting multimodal inputs (Johnston et al., 2002b; Johnston et al., 2002a) is an extension of the finite-state approach previously proposed (Bangalore and Johnston, 2000; Johnston and Bangalore, 2000). In this approach, a declarative multimodal grammar captures both the structure and the interpretation of multimodal and unimodal commands. The grammar consists of a set of context-free rules. The multimodal aspects of the grammar become apparent in the terminals, each of which is a triple W:G:M, consisting of speech (words, W), gesture (gesture symbols, G), and meaning (meaning symbols, M). The multimodal grammar encodes not just multimodal integration patterns but also the syntax of speech and gesture, and the assignment of meaning. The meaning is represented in XML, facilitating parsing an</context>
<context position="9529" citStr="Johnston and Bangalore, 2000" startWordPosition="1491" endWordPosition="1495"> DEICNP DDETPL :area: :sel: NUM BEADPL DDETPL these:G: those:G: BEADPL restaurants:rest: rest SEM:SEM: :: /rest NUM two:2: three:3: ... ten:10: Figure 3: Multimodal grammar fragment the XML output. The example above yields the following meaning representation for concept accuracy. The multimodal grammar can be used to create language models for ASR, align the speech and gesture results from the respective recognizers and transform the multimodal utterance to a meaning representation. All these operations are achieved using finite-state transducer operations (See (Bangalore and Johnston, 2000; Johnston and Bangalore, 2000) for details). However, this approach to recognition needs to be more robust to extragrammaticality and language variation in user’s utterances and the interpretation needs to be more robust to speech recognition errors. We address these issues in the rest of the paper. 3 Bootstrapping Corpora for Language Models The problem of speech recognition can be succinctly represented as a search for the most likely word sequence ( ) through the network created by the composition of a language of acoustic observations ( ), an acoustic model which is a transduction from acoustic observations to phone se</context>
</contexts>
<marker>Johnston, Bangalore, 2000</marker>
<rawString>M. Johnston and S. Bangalore. 2000. Finite-state multimodal parsing and understanding. In Proceedings of COLING, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>A Stent</author>
<author>G Vasireddy</author>
<author>P Ehlen</author>
</authors>
<title>Multimodal language processing for mobile information access.</title>
<date>2002</date>
<booktitle>In In Proceedings of ICSLP,</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="3502" citStr="Johnston et al., 2002" startWordPosition="528" endWordPosition="531">e architecture of the system and the apparatus for multimodal understanding. In Section 3, we discuss various approaches to rapid prototyping of the language model for the speech recognizer and in Section 4 we present two approaches to robust multimodal understanding. Section 5 presents the results for speech recognition and multimodal understanding using the different approaches we consider. 2 The MATCH application MATCH (Multimodal Access To City Help) is a working city guide and navigation system that enables mobile users to access restaurant and subway information for New York City (NYC) (Johnston et al., 2002b; Johnston et al., 2002a). The user interacts with a graphical interface displaying restaurant listings and a dynamic map showing locations and street information. The inputs can be speech, drawing on the display with a stylus, or synchronous multimodal combinations of the two modes. The user can ask for the review, cuisine, phone number, address, or other information about restaurants and subway directions to locations. The system responds with graphical callouts on the display, synchronized with synthetic speech output. For example, if the user says phone numbers for these two restaurants a</context>
<context position="5185" citStr="Johnston et al., 2002" startWordPosition="797" endWordPosition="800">t supports MATCH consists of a series of re-usable components which communicate over sockets through a facilitator (MCUBE) (Figure 2). Users interact with the system through a Multimodal User Interface Client (MUI). Their speech and ink are processed by speech recognition (Sharp et al., 1997) (ASR) and handwriting/gesture recognition (GESTURE, HW RECO) components respectively. These recognition processes result in lattices of potential words and gestures. These are then combined and assigned a meaning representation using a multimodal finite-state device (MMFST) (Johnston and Bangalore, 2000; Johnston et al., 2002b). This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs. This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al., 2002b), which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM enters into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating a coordinated sequence o</context>
</contexts>
<marker>Johnston, Bangalore, Stent, Vasireddy, Ehlen, 2002</marker>
<rawString>M. Johnston, S. Bangalore, A. Stent, G. Vasireddy, and P. Ehlen. 2002a. Multimodal language processing for mobile information access. In In Proceedings of ICSLP, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>G Vasireddy</author>
<author>A Stent</author>
<author>P Ehlen</author>
<author>M Walker</author>
<author>S Whittaker</author>
<author>P Maloor</author>
</authors>
<title>MATCH: An architecture for multimodal dialog systems.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="3502" citStr="Johnston et al., 2002" startWordPosition="528" endWordPosition="531">e architecture of the system and the apparatus for multimodal understanding. In Section 3, we discuss various approaches to rapid prototyping of the language model for the speech recognizer and in Section 4 we present two approaches to robust multimodal understanding. Section 5 presents the results for speech recognition and multimodal understanding using the different approaches we consider. 2 The MATCH application MATCH (Multimodal Access To City Help) is a working city guide and navigation system that enables mobile users to access restaurant and subway information for New York City (NYC) (Johnston et al., 2002b; Johnston et al., 2002a). The user interacts with a graphical interface displaying restaurant listings and a dynamic map showing locations and street information. The inputs can be speech, drawing on the display with a stylus, or synchronous multimodal combinations of the two modes. The user can ask for the review, cuisine, phone number, address, or other information about restaurants and subway directions to locations. The system responds with graphical callouts on the display, synchronized with synthetic speech output. For example, if the user says phone numbers for these two restaurants a</context>
<context position="5185" citStr="Johnston et al., 2002" startWordPosition="797" endWordPosition="800">t supports MATCH consists of a series of re-usable components which communicate over sockets through a facilitator (MCUBE) (Figure 2). Users interact with the system through a Multimodal User Interface Client (MUI). Their speech and ink are processed by speech recognition (Sharp et al., 1997) (ASR) and handwriting/gesture recognition (GESTURE, HW RECO) components respectively. These recognition processes result in lattices of potential words and gestures. These are then combined and assigned a meaning representation using a multimodal finite-state device (MMFST) (Johnston and Bangalore, 2000; Johnston et al., 2002b). This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs. This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al., 2002b), which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM enters into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating a coordinated sequence o</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen, M. Walker, S. Whittaker, and P. Maloor. 2002b. MATCH: An architecture for multimodal dialog systems. In Proceedings ofACL, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
</authors>
<title>GLR*: A Robust Grammar-Focused Parser for Spontaneously Spoken Language.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="31994" citStr="Lavie, 1996" startWordPosition="5124" endWordPosition="5125">d is more conducive for identifying the simple predicates in MATCH. Second, the patternmatching approach uses the entire grammar as a model for matching while the classification approach is trained on the training data which is significantly smaller when compared to the number of examples encoded in the grammar. 6 Discussion Although we are not aware of any attempts to address the issue of robust understanding in the context of multimodal systems, this issue has been of great interest in the context of speech-only conversational systems (Dowding et al., 1993; Seneff, 1992; Allen et al., 2000; Lavie, 1996). The output of the recognizer in these systems usually is parsed using a handcrafted grammar that assigns a meaning representation suited for the downstream dialog component. The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a finite-state-based interpreter without the need f</context>
</contexts>
<marker>Lavie, 1996</marker>
<rawString>A. Lavie. 1996. GLR*: A Robust Grammar-Focused Parser for Spontaneously Spoken Language. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Regular approximations of CFLs: A grammatical view.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technology,</booktitle>
<location>Boston.</location>
<contexts>
<context position="13440" citStr="Nederhof, 1997" startWordPosition="2133" endWordPosition="2134">e built a class-based trigram language model ( ) using the 709 multimodal and speech-only utterances as the corpus ( ). The performance of this model serves as the point of reference to compare the performance of language models trained on derived corpora. 3.2 Grammar as Language Model The multimodal CFG (a fragment is presented in Section 2) encodes the repertoire of language and gesture commands allowed by the system and their combined interpretations. The CFG can be approximated by an FSM with arcs labeled with language, gesture and meaning symbols, using well-known compilation techniques (Nederhof, 1997). The resulting FSM can be projected on the language component and can be used as the language model acceptor ( ) for speech recognition. Note that the resulting language model acceptor is unweighted if the grammar is unweighted and suffers from not being robust to language variations in user’s input. However, due to the tight coupling of the grammar used for recognition and interpretion, every recognized string can be assigned an interpretation (though it may not necessarily be the intended interpretation). 3.3 Grammar-based N-gram Language Model As mentioned earlier, a hand-crafted grammar t</context>
</contexts>
<marker>Nederhof, 1997</marker>
<rawString>M-J. Nederhof. 1997. Regular approximations of CFLs: A grammatical view. In Proceedings of the International Workshop on Parsing Technology, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Michael D Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata.</title>
<date>1997</date>
<booktitle>Finite State Devices for Natural Language Processing,</booktitle>
<pages>431--456</pages>
<editor>In E. Roche and Schabes Y., editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="10292" citStr="Pereira and Riley, 1997" startWordPosition="1620" endWordPosition="1624">nces and the interpretation needs to be more robust to speech recognition errors. We address these issues in the rest of the paper. 3 Bootstrapping Corpora for Language Models The problem of speech recognition can be succinctly represented as a search for the most likely word sequence ( ) through the network created by the composition of a language of acoustic observations ( ), an acoustic model which is a transduction from acoustic observations to phone sequences ( ), a pronounciation model which is a transduction from phone sequences to word sequences ( ), and a language model acceptor ( ) (Pereira and Riley, 1997). The language model acceptor encodes the (weighted) word sequences permitted in an application. Typically, is built using either a hand-crafted grammar or using a statistical language model derived from a corpus of sentences from the application domain. While a grammar could be written so as to be easily portable across applications, it suffers from being too prescriptive and has no metric for relative likelihood of users’ utterances. In contrast, in the data-driven approach a weighted grammar is automatically induced from a corpus and the weights can be interpreted as a measure for relative </context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>Fernando C.N. Pereira and Michael D. Riley. 1997. Speech recognition by composition of weighted finite automata. In E. Roche and Schabes Y., editors, Finite State Devices for Natural Language Processing, pages 431–456. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Srinivas Bangalore</author>
<author>Tahir Butt</author>
<author>Alexis Nasr</author>
<author>Richard Sproat</author>
</authors>
<title>Creating a finitestate parser with application semantics.</title>
<date>2002</date>
<booktitle>In In Proceedings of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="18083" citStr="Rambow et al., 2002" startWordPosition="2873" endWordPosition="2876">ons of wide-coverage, domain-independent, syntactic grammars for English in various formalisms (XTAG, 2001; Clark and Hockenmaier, 2002; Flickinger et al., 2000). Here, we describe a method that exploits one such grammar implementation in the Lexicalized TreeAdjoining Grammar (LTAG) formalism, for deriving domain-specific corpora. An LTAG consists of a set of elementary trees (Supertags) (Bangalore and Joshi, 1999) each associated with a lexical item. The set of sentences generated by an LTAG can be obtained by combining supertags using substitution and adjunction operations. In related work (Rambow et al., 2002), it has been shown that for a restricted version of LTAG, the combinations of a set of supertags can be represented as an FSM. This FSM compactly encodes the set of sentences generated by an LTAG grammar. We derive a domain-specific corpus by constructing a lexicon consisting of pairings of words with their supertags that are relevant to that domain. We then compile the grammar to build an FSM of all sentences upto a given length. We sample this FSM and build a language model as discussed in Section 3.3. Given untranscribed utterances from a specific domain, we can also adapt the language mod</context>
</contexts>
<marker>Rambow, Bangalore, Butt, Nasr, Sproat, 2002</marker>
<rawString>Owen Rambow, Srinivas Bangalore, Tahir Butt, Alexis Nasr, and Richard Sproat. 2002. Creating a finitestate parser with application semantics. In In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002), Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora, MIT,</booktitle>
<location>Cambridge, Boston.</location>
<contexts>
<context position="22047" citStr="Ramshaw and Marcus, 1995" startWordPosition="3557" endWordPosition="3561">on of the user’s intent. For example, in (1), is the predicate and is the set of arguments to the predicate. We determine the predicate ( ) for a token multimodal utterance ( ) by maximizing the posterior probability as shown in Equation 7. (7) We view the problem of identifying and extracting arguments from a multimodal input as a problem of associating each token of the input with a specific tag that encodes the label of the argument and the span of the argument. These tags are drawn from a tagset which is constructed by extending each argument label by three additional symbols , following (Ramshaw and Marcus, 1995). These symbols correspond to cases when a token is inside ( ) an argument span, outside ( ) an argument span or at the boundary of two argument spans ( ) (See Table 1). Given this encoding, the problem of extracting the arguments is a search for the most likely sequence of tags ( ) given the input multimodal utterance as shown in Equation (8). We approximate the posterior probability using independence assumptions as User cheap thai upper west side Utterance Argument price cheap /price cuisine Annotation thai /cuisine place upper west side /place IOB cheap price B thai cuisine B Encoding uppe</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, MIT, Cambridge, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
</authors>
<title>Transparent combination of rule-based and data-driven approaches in speech understanding.</title>
<date>2003</date>
<booktitle>In In Proceedings of the EACL</booktitle>
<contexts>
<context position="32793" citStr="Rayner and Hockey, 2003" startWordPosition="5241" endWordPosition="5244">The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a finite-state-based interpreter without the need for combining fragments from a parser. The issue of combining rule-based and data-driven approaches has received less attention, with the exception of a few (Wang et al., 2000; Rayner and Hockey, 2003; Wang and Acero, 2003). In a recent paper (Rayner and Hockey, 2003), the authors address this issue by employing a decision-list-based speech understanding system as a means of progressing from rule-based models to data-driven models when data becomes available. The decision-list-based understanding system also provides a method for robust understanding. In contrast, the approach presented in this paper can be used on lattices of speech and gestures to produce a lattice of meaning representations. 7 Conclusion In this paper, we have addressed how to rapidly prototype multimodal conversational</context>
</contexts>
<marker>Rayner, Hockey, 2003</marker>
<rawString>M. Rayner and B. A. Hockey. 2003. Transparent combination of rule-based and data-driven approaches in speech understanding. In In Proceedings of the EACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>A relaxation method for understanding spontaneous speech utterances.</title>
<date>1992</date>
<booktitle>In Proceedings, Speech and Natural Language Workshop,</booktitle>
<location>San Mateo, CA.</location>
<contexts>
<context position="31960" citStr="Seneff, 1992" startWordPosition="5118" endWordPosition="5119">ach relies on local information and is more conducive for identifying the simple predicates in MATCH. Second, the patternmatching approach uses the entire grammar as a model for matching while the classification approach is trained on the training data which is significantly smaller when compared to the number of examples encoded in the grammar. 6 Discussion Although we are not aware of any attempts to address the issue of robust understanding in the context of multimodal systems, this issue has been of great interest in the context of speech-only conversational systems (Dowding et al., 1993; Seneff, 1992; Allen et al., 2000; Lavie, 1996). The output of the recognizer in these systems usually is parsed using a handcrafted grammar that assigns a meaning representation suited for the downstream dialog component. The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a finite-state-ba</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>S. Seneff. 1992. A relaxation method for understanding spontaneous speech utterances. In Proceedings, Speech and Natural Language Workshop, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Sharp</author>
<author>E Bocchieri</author>
<author>C Castillo</author>
<author>S Parthasarathy</author>
<author>C Rath</author>
<author>M Riley</author>
<author>J Rowland</author>
</authors>
<title>The Watson speech recognition engine. In</title>
<date>1997</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>4065--4068</pages>
<contexts>
<context position="4857" citStr="Sharp et al., 1997" startWordPosition="752" endWordPosition="755">ime Cafe can be reached at 212-533-7000, for each restaurant in turn (Figure 1 [b]). If the immediate environment is too noisy or public, the same command can be given completely in pen by circling the restaurants and writing phone. Figure 1: Two area gestures 2.1 MATCH Multimodal Architecture The underlying architecture that supports MATCH consists of a series of re-usable components which communicate over sockets through a facilitator (MCUBE) (Figure 2). Users interact with the system through a Multimodal User Interface Client (MUI). Their speech and ink are processed by speech recognition (Sharp et al., 1997) (ASR) and handwriting/gesture recognition (GESTURE, HW RECO) components respectively. These recognition processes result in lattices of potential words and gestures. These are then combined and assigned a meaning representation using a multimodal finite-state device (MMFST) (Johnston and Bangalore, 2000; Johnston et al., 2002b). This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs. This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al., 2002b), which re-ranks them in accordance</context>
</contexts>
<marker>Sharp, Bocchieri, Castillo, Parthasarathy, Rath, Riley, Rowland, 1997</marker>
<rawString>R.D. Sharp, E. Bocchieri, C. Castillo, S. Parthasarathy, C. Rath, M. Riley, and J.Rowland. 1997. The Watson speech recognition engine. In In Proceedings of ICASSP, pages 4065–4068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Souvignier</author>
<author>A Kellner</author>
</authors>
<title>Online adaptation for language models in spoken dialogue systems.</title>
<date>1998</date>
<booktitle>In Int. Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="17370" citStr="Souvignier and Kellner, 1998" startWordPosition="2768" endWordPosition="2771">versational speech corpus. We built a trigram model ( ) using the 5.4 million word corpus and investigated the effect of adapting the Switchboard language model given in-domain untranscribed speech utterances ( ). The adaptation is done by first recognizing the in-domain speech utterances and then building a language model ( ) from the corpus of recognized text ( ). This bootstrapping mechanism can be used to derive an domain-specific corpus and language model without any transcriptions. Similar techniques for unsupervised language model adaptation are presented in (Bacchiani and Roark, 2003; Souvignier and Kellner, 1998). 3.7 Adapting a wide-coverage grammar There have been a number of computational implementations of wide-coverage, domain-independent, syntactic grammars for English in various formalisms (XTAG, 2001; Clark and Hockenmaier, 2002; Flickinger et al., 2000). Here, we describe a method that exploits one such grammar implementation in the Lexicalized TreeAdjoining Grammar (LTAG) formalism, for deriving domain-specific corpora. An LTAG consists of a set of elementary trees (Supertags) (Bangalore and Joshi, 1999) each associated with a lexical item. The set of sentences generated by an LTAG can be ob</context>
</contexts>
<marker>Souvignier, Kellner, 1998</marker>
<rawString>B. Souvignier and A. Kellner. 1998. Online adaptation for language models in spoken dialogue systems. In Int. Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>A Acero</author>
</authors>
<title>Combination of cfg and n-gram modeling in semantic grammar learning. In</title>
<date>2003</date>
<booktitle>In Proceedings ofthe Eurospeech Conference,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="32816" citStr="Wang and Acero, 2003" startWordPosition="5245" endWordPosition="5248">the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a finite-state-based interpreter without the need for combining fragments from a parser. The issue of combining rule-based and data-driven approaches has received less attention, with the exception of a few (Wang et al., 2000; Rayner and Hockey, 2003; Wang and Acero, 2003). In a recent paper (Rayner and Hockey, 2003), the authors address this issue by employing a decision-list-based speech understanding system as a means of progressing from rule-based models to data-driven models when data becomes available. The decision-list-based understanding system also provides a method for robust understanding. In contrast, the approach presented in this paper can be used on lattices of speech and gestures to produce a lattice of meaning representations. 7 Conclusion In this paper, we have addressed how to rapidly prototype multimodal conversational systems without relyin</context>
</contexts>
<marker>Wang, Acero, 2003</marker>
<rawString>Y. Wang and A. Acero. 2003. Combination of cfg and n-gram modeling in semantic grammar learning. In In Proceedings ofthe Eurospeech Conference, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Y Wang</author>
<author>M Mahajan</author>
<author>X Huang</author>
</authors>
<title>Unified Context-Free Grammar and N-Gram Model for Spoken Language Processing.</title>
<date>2000</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<contexts>
<context position="32768" citStr="Wang et al., 2000" startWordPosition="5237" endWordPosition="5240"> dialog component. The coverage problems of the grammar and parsing of extra-grammatical utterances is typically addressed by retrieving fragments from the parse chart and incorporating operations that combine fragments to derive a meaning of the recognized utterance. We have presented an approach that achieves robust multimodal utterance understanding using the edit-distance automaton in a finite-state-based interpreter without the need for combining fragments from a parser. The issue of combining rule-based and data-driven approaches has received less attention, with the exception of a few (Wang et al., 2000; Rayner and Hockey, 2003; Wang and Acero, 2003). In a recent paper (Rayner and Hockey, 2003), the authors address this issue by employing a decision-list-based speech understanding system as a means of progressing from rule-based models to data-driven models when data becomes available. The decision-list-based understanding system also provides a method for robust understanding. In contrast, the approach presented in this paper can be used on lattices of speech and gestures to produce a lattice of meaning representations. 7 Conclusion In this paper, we have addressed how to rapidly prototype </context>
</contexts>
<marker>Wang, Mahajan, Huang, 2000</marker>
<rawString>Y.Y. Wang, M. Mahajan, and X. Huang. 2000. Unified Context-Free Grammar and N-Gram Model for Spoken Language Processing. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>XTAG</author>
</authors>
<title>A lexicalized tree-adjoining grammar for english.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania,</institution>
<note>http://www.cis.upenn.edu/ xtag/gramrelease.html.</note>
<contexts>
<context position="17569" citStr="XTAG, 2001" startWordPosition="2797" endWordPosition="2798">adaptation is done by first recognizing the in-domain speech utterances and then building a language model ( ) from the corpus of recognized text ( ). This bootstrapping mechanism can be used to derive an domain-specific corpus and language model without any transcriptions. Similar techniques for unsupervised language model adaptation are presented in (Bacchiani and Roark, 2003; Souvignier and Kellner, 1998). 3.7 Adapting a wide-coverage grammar There have been a number of computational implementations of wide-coverage, domain-independent, syntactic grammars for English in various formalisms (XTAG, 2001; Clark and Hockenmaier, 2002; Flickinger et al., 2000). Here, we describe a method that exploits one such grammar implementation in the Lexicalized TreeAdjoining Grammar (LTAG) formalism, for deriving domain-specific corpora. An LTAG consists of a set of elementary trees (Supertags) (Bangalore and Joshi, 1999) each associated with a lexical item. The set of sentences generated by an LTAG can be obtained by combining supertags using substitution and adjunction operations. In related work (Rambow et al., 2002), it has been shown that for a restricted version of LTAG, the combinations of a set o</context>
</contexts>
<marker>XTAG, 2001</marker>
<rawString>XTAG. 2001. A lexicalized tree-adjoining grammar for english. Technical report, University of Pennsylvania, http://www.cis.upenn.edu/ xtag/gramrelease.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>