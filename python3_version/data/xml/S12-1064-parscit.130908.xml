<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000148">
<title confidence="0.702515">
HDU: Cross-lingual Textual Entailment with SMT Features
</title>
<author confidence="0.982143">
Katharina W¨aschle and Sascha Fendrich
</author>
<affiliation confidence="0.9928945">
Department of Computational Linguistics
Heidelberg University
</affiliation>
<address confidence="0.646663">
Heidelberg, Germany
</address>
<email confidence="0.992582">
{waeschle, fendrich}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.998538" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927384615385">
We describe the Heidelberg University system
for the Cross-lingual Textual Entailment task
at SemEval-2012. The system relies on fea-
tures extracted with statistical machine trans-
lation methods and tools, combining mono-
lingual and cross-lingual word alignments
as well as standard textual entailment dis-
tance and bag-of-words features in a statisti-
cal learning framework. We learn separate bi-
nary classifiers for each entailment direction
and combine them to obtain four entailment
relations. Our system yielded the best overall
score for three out of four language pairs.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999844533333333">
Cross-lingual textual entailment (CLTE) (Mehdad et
al., 2010) is an extension of textual entailment (TE)
(Dagan and Glickman, 2004). The task of recog-
nizing entailment is to determine whether a hypoth-
esis H can be semantically inferred from a text T.
The CLTE task adds a cross-lingual dimension to the
problem by considering sentence pairs, where T and
H are in different languages. The SemEval-2012
CLTE task (Negri et al., 2012) asks participants to
judge entailment pairs in four language combina-
tions1, defining four target entailment relations, for-
ward, backward, bidirectional and no entailment.
We investigate this problem in a statistical learn-
ing framework, which allows us to combine cross-
lingual word alignment features as well as common
</bodyText>
<footnote confidence="0.733633">
1Spanish-English (es-en), Italian-English (it-en), French-
English (fr-en) and German-English (de-en).
</footnote>
<bodyText confidence="0.998043272727273">
monolingual entailment metrics, such as bag-of-
words overlap, edit distance and monolingual align-
ments on translations of T and H, using standard
statistical machine translation (SMT) tools and re-
sources. Our goal is to address this task without deep
processing components to make it easily portable
across languages. We argue that the cross-lingual
entailment task can benefit from direct alignments
between T and H, since a large amount of bilin-
gual parallel data is available, which naturally mod-
els synonymy and paraphrasing across languages.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999898285714286">
With the yearly Recognizing Textual Entailment
(RTE) challenge (Dagan et al., 2006), there has been
a lot of work on monolingual TE. We therefore in-
clude established monolingual features in our ap-
proach, such as alignment scores (MacCartney et
al., 2008), edit distance and bag-of-words lexical
overlap measures (Kouylekov and Negri, 2010). So
far, the only work on CLTE that we are aware of is
Mehdad et al. (2010), where the problem is reduced
to monolingual entailment using machine transla-
tion, and Mehdad et al. (2011), which exploits par-
allel corpora for generating features based on phrase
alignments as input to an SVM. Our approach com-
bines ideas from both, mostly resembling Mehdad
et al. (2011). There are, however, several differ-
ences; we use word translation probabilities instead
of phrase tables and model monolingual and cross-
lingual alignment separately. We also include addi-
tional similarity measures derived from the MT eval-
uation metric Meteor, which was used in Volokh and
Neumann (2011) for the monolingual TE task. Con-
</bodyText>
<page confidence="0.989821">
467
</page>
<note confidence="0.534683">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 467–471,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9983716875">
versely, Pad´o et al. (2009) showed that textual entail-
ment features can be used for measuring MT quality,
indicating a strong relatedness of the two problems.
The CLTE task is also related to the problem of
identifying parallel sentence pairs in a non-parallel
corpus, so we adapt alignment-based features from
Munteanu and Marcu (2005), where a Maximum
Entropy classifier was used to judge if two sentences
are sufficiently parallel.
Regarding the view on entailment, MacCartney
and Manning (2007) proposed the decomposition
of top-level entailment, such as equivalence (which
corresponds to the CLTE bidirectional class), into
atomic forward and backward entailment predic-
tions, which is mirrored in our multi-label approach
with two binary classifiers.
</bodyText>
<sectionHeader confidence="0.994512" genericHeader="method">
3 SMT Features for CLTE
</sectionHeader>
<bodyText confidence="0.999723642857143">
The SemEval-2012 CLTE task emerges from the
monolingual RTE task; however the perception of
entailment differs slightly. In CLTE, the sentences
T1 and T2 are of roughly the same length and the
entailment is predicted in both directions. Negri et
al. (2011) states that the CLTE pairs were created
by paraphrasing an English sentence E and leaving
out or adding information to construct a modified
sentence E&apos;, which was then translated into a dif-
ferent language2, yielding sentence F and thus cre-
ating a bilingual entailment pair. For this reason,
we believe that our system should be less inference-
oriented than some previous RTE systems and rather
should capture
</bodyText>
<listItem confidence="0.8045972">
• paraphrases and synonymy to identify semantic
equivalence,
• phrases that have no matching correspondent in
the other sentence, indicating missing (respec-
tively, additional) information.
</listItem>
<bodyText confidence="0.931418888888889">
To this end, we define a number of similarity
metrics based on different views on the data pairs,
which we combine as features in a statistical learn-
ing framework. Our features are both cross- and
monolingual. We obtain monolingual pairs by trans-
lating the English sentence E into the foreign lan-
2We refer to the non-English language sentence as F.
guage, yielding T (E) and vice versa T (F) from F,
using Google Translate3.
</bodyText>
<subsectionHeader confidence="0.999209">
3.1 Token ratio features
</subsectionHeader>
<bodyText confidence="0.99838175">
A first indicator for additional or missing informa-
tion are simple token ratio features, i.e. the fraction
of the number of tokens in T1 and T2. We define
three token ratio measures:
</bodyText>
<figure confidence="0.875378428571429">
• English-to-Foreign, |E|
|F |
• English-to-English-Translation, |E|
|T
�F )|
• Foreign-to-Foreign-Translation,|T�E)|
|F |
</figure>
<subsectionHeader confidence="0.999336">
3.2 Bag-of-words and distance features
</subsectionHeader>
<bodyText confidence="0.9994544">
Typical similarity measures used in monolingual
TE are lexical overlap metrics, computed on bag-
of-words representations of both sentences. We
use the following similarities, computing both
sim(E, T (F)) and sim(F, T (E)).
</bodyText>
<listItem confidence="0.9868295">
• Jaccard coefficient, sim(A, B) = |A�B|
|AUB|
• Overlap coefficient, sim(A, B) = |A�B|
m���|A|,|B|)
</listItem>
<bodyText confidence="0.99905375">
We also compute the lexical overlap on bigrams
and trigrams.
In addition, we include a simple distance measure
based on string edit distance ed, summing up over
all distances between every token a in A and its most
similar token b in B, where we assume that the cor-
responding token is the one with the smallest edit
distance:
</bodyText>
<equation confidence="0.748321666666667">
•
dist(A, B) = log E
aEA
</equation>
<subsectionHeader confidence="0.995145">
3.3 Meteor features
</subsectionHeader>
<bodyText confidence="0.999949125">
The Meteor scoring tool (Denkowski and Lavie,
2011) for evaluating the output of statistical machine
translation systems can be used to calculate the simi-
larity of two sentences in the same language. Meteor
uses stemming, paraphrase tables and synonym col-
lections to align words between the two sentences
and scores the resulting alignment. We include the
overall weighted Meteor score both for (E,T(F))
</bodyText>
<equation confidence="0.81436475">
3http://translate.google.com/
min
bEB
ed(a, b)
</equation>
<page confidence="0.993459">
468
</page>
<bodyText confidence="0.997388">
and (F, T (E))4 as well as separate alignment preci-
sion, recall and fragmentation scores for (E, T (F)).
</bodyText>
<subsectionHeader confidence="0.992453">
3.4 Monolingual alignment features
</subsectionHeader>
<bodyText confidence="0.992452666666667">
We use the alignments output by the Meteor-1.3
scorer for (E,T(F))5 to calculate the following
metrics:
</bodyText>
<listItem confidence="0.999824">
• number of unaligned words
• percentage of aligned words
• length of the longest unaligned subsequence
</listItem>
<subsectionHeader confidence="0.885739">
3.5 Cross-lingual alignment features
</subsectionHeader>
<bodyText confidence="0.681662666666667">
We calculate IBM model 1 word alignments (Brown
et al., 1993) with GIZA++ (Och and Ney, 2003) on
a data set concatenated from Europarl-v66 (Koehn,
2005) and a bilingual dictionary obtained from
dict.cc7 for coverage. We then heuristically align
each word e in E with the word f in F for which we
find the highest word translation probability p(e|f)
and vice versa. Words for which no translation is
found are considered unaligned. From this align-
ment a, we derive the following features both for
E and F (resulting in a total of eight cross-lingual
alignment features):
</bodyText>
<listItem confidence="0.870089833333333">
• number of unaligned words
• percentage of aligned words
• alignment score 1
JE� eEE
� p(e|a(e))
• length of the longest unaligned subsequence
</listItem>
<sectionHeader confidence="0.910066" genericHeader="method">
4 Classification
</sectionHeader>
<bodyText confidence="0.99577">
To account for the different data ranges, we normal-
ized all feature value distributions to the normal dis-
tribution N(0, 1�), so that 99% of the feature values
are in [−1, 1]. We employed SVMl�ght (Joachims,
1999) for learning different classifiers to output the
four entailment classes. We submitted a second
</bodyText>
<footnote confidence="0.889129857142857">
4Meteor-1.3 supports English, Spanish, French and German.
We used the Spanish version for scoring Italian, since those lan-
guages are related.
5Since the synonymy module is only available for English,
we do not use the alignment of (F, T(E)).
6http://www.statmt.org/europarl/
7http://www.dict.cc/
</footnote>
<table confidence="0.9754292">
T1 → T2 T2 → T1 entailment
1 1 bidirectional
1 0 forward
0 1 backward
0 0 no entailment
</table>
<tableCaption confidence="0.999812">
Table 1: Combination of atomic entailment relations.
</tableCaption>
<bodyText confidence="0.99953844">
run to evaluate our recently implemented stochastic
learning toolkit Sol (Fendrich, 2012), which imple-
ments binary, multi-class, and multi-label classifica-
tion.
For development, we split the training set in two
parts, which were alternatingly used as training and
test set. We first experimented with a multi-class
classifier that learned all four entailment classes at
once. However, although the task defines four tar-
get entailment relations, those can be broken down
into two atomic relations, namely directional entail-
ment from T1 to T2 and from T2 to T1 (table 1). We
therefore learned a binary classifier for each atomic
entailment relation and combined the output to ob-
tain the final entailment class. We found this view to
be a much better fit for the problem, improving the
accuracy score on the development set by more than
10 percentage points (table 2). This two-classifiers
approach can also be seen as a variant of multi-label
learning, with the two atomic entailment relations
as labels. We therefore also trained a direct imple-
mentation of multi-label classification. Although it
substantially outperformed the multi-class approach,
the system yielded considerably lower scores than
the version using two binary classifiers.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99994475">
The accuracy scores of our two runs on the
SemEval-2012 CLTE test set are presented in ta-
ble 3. Our system performed best out of ten sys-
tems for the language pairs es-en and de-en and tied
in first place for fr-en. For it-en, our system came
in second. Regarding the choice of the learner, our
toolkit slightly outperformed SVMl�ght on three of
the four language pairs.
To determine the contribution of different fea-
ture types for each language combination, we per-
formed ablation tests on the development set, where
we switched off groups of features and measured the
</bodyText>
<page confidence="0.998767">
469
</page>
<table confidence="0.99900525">
es-en it-en fr-en de-en
multi-class 0.47 0.456 0.466 0.458
multi-label 0.586 0.526 0.568 0.522
2x binary 0.646 0.614 0.628 0.588
</table>
<tableCaption confidence="0.73927">
Table 2: Different classifiers on development set.
</tableCaption>
<table confidence="0.987923666666667">
es-en it-en fr-en de-en
SVMti�ght 0.630 0.554 0.564 0.558
Sol 0.632 0.562 0.570 0.552
</table>
<tableCaption confidence="0.99994">
Table 3: Results on test set.
</tableCaption>
<bodyText confidence="0.99994345">
impact on the accuracy score (table 4). We assessed
the statistical significance of differences in score
with an approximate randomization test8 (Noreen,
1989), indicating a significant impact in bold font.
The results show that only in two cases a single fea-
ture group significantly impacts the score, namely
the Meteor score features for es-en and the cross-
lingual alignment features for de-en. However, no
feature group hurts the score either, since negative
variations in score are not significant. To ensure
that the different feature groups actually express di-
verse information, we also evaluated our system us-
ing only one group of features at a time. The re-
sults confirm the most significant feature type for
each language pair, but even the best-scoring feature
group for each pair always yielded scores 3-6 per-
centage points lower than the system with all feature
groups combined. We therefore conclude that the
combination of diverse features is one key aspect of
our system.
</bodyText>
<subsectionHeader confidence="0.592544">
8Using a significance level of 0.05.
</subsectionHeader>
<sectionHeader confidence="0.977541" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999637">
We have shown that SMT methods can be profitably
applied for the problem of CLTE and that combining
different feature types improves accuracy. Key to
our approach is furthermore the view of the four-
class entailment problem as a bidirectional binary or
multi-label problem. A possible explanation for the
superior performance of the multi-label approach is
that the overlap of the bidirectional entailment with
forward and backward entailment might confuse the
multi-class learner.
Regarding future work, we think that our results
can be improved by building on better alignments,
i.e. using more data for estimating cross-lingual
alignments and larger paraphrase tables. Further-
more, we would like to investigate more thoroughly
in what way the representation of the problem in
terms of machine learning impacts the system per-
formance on the task – in particular, why the two-
classifiers approach substantially outperforms the
multi-label implementation.
</bodyText>
<sectionHeader confidence="0.998861" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.885048076923077">
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational linguistics, 19(2):263–311.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability.
Ido Dagan, Oren Glickman, and Bernado Magnini. 2006.
The pascal recognising textual entailment challenge.
Machine Learning Challenges. Evaluating Predictive
Uncertainty, Visual Object Classification, and Recog-
nising Tectual Entailment, pages 177–190.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
</reference>
<table confidence="0.96931925">
feature group (#/features) es-en it-en fr-en de-en
score impact score impact score impact score impact
Meteor scores (5) 0.616 0.03 0.6 0.014 0.618 0.01 0.59 -0.002
distance/bow (10) 0.644 0.002 0.608 0.006 0.62 0.008 0.596 -0.008
token ratio (3) 0.652 -0.006 0.606 0.008 0.62 0.008 0.588 0
cross-lingual alignment (8) 0.638 0.008 0.592 0.022 0.62 0.008 0.526 0.062
monolingual alignment (3) 0.648 -0.002 0.624 -0.01 0.59 0.038 0.596 -0.008
all (29) 0.646 0.614 0.628 0.588
</table>
<tableCaption confidence="0.9998">
Table 4: Ablation tests on development set.
</tableCaption>
<page confidence="0.99684">
470
</page>
<reference confidence="0.99801203125">
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Sascha Fendrich. 2012. Sol – Stochastic Learning
Toolkit. Technical report, Department of Computa-
tional Linguistics, Heidelberg University.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods Sup-
port Vector Learning, pages 169–184.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations,
pages 42–47.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 193–200.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 802–811.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 321–
324.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. Proceedings of ACL-HLT.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477–504.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 670–679.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19–51.
Sebastian Pad´o, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2):181–193.
Alexander Volokh and G¨unter Neumann. 2011. Using
MT-based metrics for RTE. In The Fourth Text Analy-
sis Conference. NIST.
</reference>
<page confidence="0.998728">
471
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.402242">
<title confidence="0.997755">HDU: Cross-lingual Textual Entailment with SMT Features</title>
<author confidence="0.871058">W¨aschle</author>
<affiliation confidence="0.998803">Department of Computational</affiliation>
<address confidence="0.706218">Heidelberg Heidelberg,</address>
<abstract confidence="0.999329428571429">We describe the Heidelberg University system for the Cross-lingual Textual Entailment task at SemEval-2012. The system relies on features extracted with statistical machine translation methods and tools, combining monolingual and cross-lingual word alignments as well as standard textual entailment distance and bag-of-words features in a statistical learning framework. We learn separate binary classifiers for each entailment direction and combine them to obtain four entailment relations. Our system yielded the best overall score for three out of four language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7466" citStr="Brown et al., 1993" startWordPosition="1151" endWordPosition="1154">ween the two sentences and scores the resulting alignment. We include the overall weighted Meteor score both for (E,T(F)) 3http://translate.google.com/ min bEB ed(a, b) 468 and (F, T (E))4 as well as separate alignment precision, recall and fragmentation scores for (E, T (F)). 3.4 Monolingual alignment features We use the alignments output by the Meteor-1.3 scorer for (E,T(F))5 to calculate the following metrics: • number of unaligned words • percentage of aligned words • length of the longest unaligned subsequence 3.5 Cross-lingual alignment features We calculate IBM model 1 word alignments (Brown et al., 1993) with GIZA++ (Och and Ney, 2003) on a data set concatenated from Europarl-v66 (Koehn, 2005) and a bilingual dictionary obtained from dict.cc7 for coverage. We then heuristically align each word e in E with the word f in F for which we find the highest word translation probability p(e|f) and vice versa. Words for which no translation is found are considered unaligned. From this alignment a, we derive the following features both for E and F (resulting in a total of eight cross-lingual alignment features): • number of unaligned words • percentage of aligned words • alignment score 1 JE� eEE � p(e</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modeling of language variability.</title>
<date>2004</date>
<contexts>
<context position="946" citStr="Dagan and Glickman, 2004" startWordPosition="125" endWordPosition="128"> SemEval-2012. The system relies on features extracted with statistical machine translation methods and tools, combining monolingual and cross-lingual word alignments as well as standard textual entailment distance and bag-of-words features in a statistical learning framework. We learn separate binary classifiers for each entailment direction and combine them to obtain four entailment relations. Our system yielded the best overall score for three out of four language pairs. 1 Introduction Cross-lingual textual entailment (CLTE) (Mehdad et al., 2010) is an extension of textual entailment (TE) (Dagan and Glickman, 2004). The task of recognizing entailment is to determine whether a hypothesis H can be semantically inferred from a text T. The CLTE task adds a cross-lingual dimension to the problem by considering sentence pairs, where T and H are in different languages. The SemEval-2012 CLTE task (Negri et al., 2012) asks participants to judge entailment pairs in four language combinations1, defining four target entailment relations, forward, backward, bidirectional and no entailment. We investigate this problem in a statistical learning framework, which allows us to combine crosslingual word alignment features</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>Ido Dagan and Oren Glickman. 2004. Probabilistic textual entailment: Generic applied modeling of language variability.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernado Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<contexts>
<context position="2310" citStr="Dagan et al., 2006" startWordPosition="333" endWordPosition="336">trics, such as bag-ofwords overlap, edit distance and monolingual alignments on translations of T and H, using standard statistical machine translation (SMT) tools and resources. Our goal is to address this task without deep processing components to make it easily portable across languages. We argue that the cross-lingual entailment task can benefit from direct alignments between T and H, since a large amount of bilingual parallel data is available, which naturally models synonymy and paraphrasing across languages. 2 Related Work With the yearly Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006), there has been a lot of work on monolingual TE. We therefore include established monolingual features in our approach, such as alignment scores (MacCartney et al., 2008), edit distance and bag-of-words lexical overlap measures (Kouylekov and Negri, 2010). So far, the only work on CLTE that we are aware of is Mehdad et al. (2010), where the problem is reduced to monolingual entailment using machine translation, and Mehdad et al. (2011), which exploits parallel corpora for generating features based on phrase alignments as input to an SVM. Our approach combines ideas from both, mostly resemblin</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernado Magnini. 2006. The pascal recognising textual entailment challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="6616" citStr="Denkowski and Lavie, 2011" startWordPosition="1019" endWordPosition="1022"> of both sentences. We use the following similarities, computing both sim(E, T (F)) and sim(F, T (E)). • Jaccard coefficient, sim(A, B) = |A�B| |AUB| • Overlap coefficient, sim(A, B) = |A�B| m���|A|,|B|) We also compute the lexical overlap on bigrams and trigrams. In addition, we include a simple distance measure based on string edit distance ed, summing up over all distances between every token a in A and its most similar token b in B, where we assume that the corresponding token is the one with the smallest edit distance: • dist(A, B) = log E aEA 3.3 Meteor features The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language. Meteor uses stemming, paraphrase tables and synonym collections to align words between the two sentences and scores the resulting alignment. We include the overall weighted Meteor score both for (E,T(F)) 3http://translate.google.com/ min bEB ed(a, b) 468 and (F, T (E))4 as well as separate alignment precision, recall and fragmentation scores for (E, T (F)). 3.4 Monolingual alignment features We use the alignments output by the Meteor-1.3 scorer fo</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sascha Fendrich</author>
</authors>
<title>Sol – Stochastic Learning Toolkit.</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>Department of Computational Linguistics, Heidelberg University.</institution>
<contexts>
<context position="8971" citStr="Fendrich, 2012" startWordPosition="1397" endWordPosition="1398">rning different classifiers to output the four entailment classes. We submitted a second 4Meteor-1.3 supports English, Spanish, French and German. We used the Spanish version for scoring Italian, since those languages are related. 5Since the synonymy module is only available for English, we do not use the alignment of (F, T(E)). 6http://www.statmt.org/europarl/ 7http://www.dict.cc/ T1 → T2 T2 → T1 entailment 1 1 bidirectional 1 0 forward 0 1 backward 0 0 no entailment Table 1: Combination of atomic entailment relations. run to evaluate our recently implemented stochastic learning toolkit Sol (Fendrich, 2012), which implements binary, multi-class, and multi-label classification. For development, we split the training set in two parts, which were alternatingly used as training and test set. We first experimented with a multi-class classifier that learned all four entailment classes at once. However, although the task defines four target entailment relations, those can be broken down into two atomic relations, namely directional entailment from T1 to T2 and from T2 to T1 (table 1). We therefore learned a binary classifier for each atomic entailment relation and combined the output to obtain the fina</context>
</contexts>
<marker>Fendrich, 2012</marker>
<rawString>Sascha Fendrich. 2012. Sol – Stochastic Learning Toolkit. Technical report, Department of Computational Linguistics, Heidelberg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods Support Vector Learning,</booktitle>
<pages>169--184</pages>
<contexts>
<context position="8348" citStr="Joachims, 1999" startWordPosition="1303" endWordPosition="1304">n probability p(e|f) and vice versa. Words for which no translation is found are considered unaligned. From this alignment a, we derive the following features both for E and F (resulting in a total of eight cross-lingual alignment features): • number of unaligned words • percentage of aligned words • alignment score 1 JE� eEE � p(e|a(e)) • length of the longest unaligned subsequence 4 Classification To account for the different data ranges, we normalized all feature value distributions to the normal distribution N(0, 1�), so that 99% of the feature values are in [−1, 1]. We employed SVMl�ght (Joachims, 1999) for learning different classifiers to output the four entailment classes. We submitted a second 4Meteor-1.3 supports English, Spanish, French and German. We used the Spanish version for scoring Italian, since those languages are related. 5Since the synonymy module is only available for English, we do not use the alignment of (F, T(E)). 6http://www.statmt.org/europarl/ 7http://www.dict.cc/ T1 → T2 T2 → T1 entailment 1 1 bidirectional 1 0 forward 0 1 backward 0 0 no entailment Table 1: Combination of atomic entailment relations. run to evaluate our recently implemented stochastic learning toolk</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. Advances in Kernel Methods Support Vector Learning, pages 169–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit,</booktitle>
<volume>5</volume>
<contexts>
<context position="7557" citStr="Koehn, 2005" startWordPosition="1168" endWordPosition="1169">score both for (E,T(F)) 3http://translate.google.com/ min bEB ed(a, b) 468 and (F, T (E))4 as well as separate alignment precision, recall and fragmentation scores for (E, T (F)). 3.4 Monolingual alignment features We use the alignments output by the Meteor-1.3 scorer for (E,T(F))5 to calculate the following metrics: • number of unaligned words • percentage of aligned words • length of the longest unaligned subsequence 3.5 Cross-lingual alignment features We calculate IBM model 1 word alignments (Brown et al., 1993) with GIZA++ (Och and Ney, 2003) on a data set concatenated from Europarl-v66 (Koehn, 2005) and a bilingual dictionary obtained from dict.cc7 for coverage. We then heuristically align each word e in E with the word f in F for which we find the highest word translation probability p(e|f) and vice versa. Words for which no translation is found are considered unaligned. From this alignment a, we derive the following features both for E and F (resulting in a total of eight cross-lingual alignment features): • number of unaligned words • percentage of aligned words • alignment score 1 JE� eEE � p(e|a(e)) • length of the longest unaligned subsequence 4 Classification To account for the di</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Matteo Negri</author>
</authors>
<title>An opensource package for recognizing textual entailment.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>42--47</pages>
<contexts>
<context position="2566" citStr="Kouylekov and Negri, 2010" startWordPosition="373" endWordPosition="376"> make it easily portable across languages. We argue that the cross-lingual entailment task can benefit from direct alignments between T and H, since a large amount of bilingual parallel data is available, which naturally models synonymy and paraphrasing across languages. 2 Related Work With the yearly Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006), there has been a lot of work on monolingual TE. We therefore include established monolingual features in our approach, such as alignment scores (MacCartney et al., 2008), edit distance and bag-of-words lexical overlap measures (Kouylekov and Negri, 2010). So far, the only work on CLTE that we are aware of is Mehdad et al. (2010), where the problem is reduced to monolingual entailment using machine translation, and Mehdad et al. (2011), which exploits parallel corpora for generating features based on phrase alignments as input to an SVM. Our approach combines ideas from both, mostly resembling Mehdad et al. (2011). There are, however, several differences; we use word translation probabilities instead of phrase tables and model monolingual and crosslingual alignment separately. We also include additional similarity measures derived from the MT </context>
</contexts>
<marker>Kouylekov, Negri, 2010</marker>
<rawString>Milen Kouylekov and Matteo Negri. 2010. An opensource package for recognizing textual entailment. In Proceedings of the ACL 2010 System Demonstrations, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Natural logic for textual inference.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="3940" citStr="MacCartney and Manning (2007)" startWordPosition="589" endWordPosition="592">omputational Semantics (*SEM), pages 467–471, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics versely, Pad´o et al. (2009) showed that textual entailment features can be used for measuring MT quality, indicating a strong relatedness of the two problems. The CLTE task is also related to the problem of identifying parallel sentence pairs in a non-parallel corpus, so we adapt alignment-based features from Munteanu and Marcu (2005), where a Maximum Entropy classifier was used to judge if two sentences are sufficiently parallel. Regarding the view on entailment, MacCartney and Manning (2007) proposed the decomposition of top-level entailment, such as equivalence (which corresponds to the CLTE bidirectional class), into atomic forward and backward entailment predictions, which is mirrored in our multi-label approach with two binary classifiers. 3 SMT Features for CLTE The SemEval-2012 CLTE task emerges from the monolingual RTE task; however the perception of entailment differs slightly. In CLTE, the sentences T1 and T2 are of roughly the same length and the entailment is predicted in both directions. Negri et al. (2011) states that the CLTE pairs were created by paraphrasing an En</context>
</contexts>
<marker>MacCartney, Manning, 2007</marker>
<rawString>Bill MacCartney and Christopher D. Manning. 2007. Natural logic for textual inference. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>802--811</pages>
<contexts>
<context position="2481" citStr="MacCartney et al., 2008" startWordPosition="362" endWordPosition="365">d resources. Our goal is to address this task without deep processing components to make it easily portable across languages. We argue that the cross-lingual entailment task can benefit from direct alignments between T and H, since a large amount of bilingual parallel data is available, which naturally models synonymy and paraphrasing across languages. 2 Related Work With the yearly Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006), there has been a lot of work on monolingual TE. We therefore include established monolingual features in our approach, such as alignment scores (MacCartney et al., 2008), edit distance and bag-of-words lexical overlap measures (Kouylekov and Negri, 2010). So far, the only work on CLTE that we are aware of is Mehdad et al. (2010), where the problem is reduced to monolingual entailment using machine translation, and Mehdad et al. (2011), which exploits parallel corpora for generating features based on phrase alignments as input to an SVM. Our approach combines ideas from both, mostly resembling Mehdad et al. (2011). There are, however, several differences; we use word translation probabilities instead of phrase tables and model monolingual and crosslingual alig</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 802–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Towards cross-lingual textual entailment.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>321--324</pages>
<contexts>
<context position="876" citStr="Mehdad et al., 2010" startWordPosition="114" endWordPosition="117">niversity system for the Cross-lingual Textual Entailment task at SemEval-2012. The system relies on features extracted with statistical machine translation methods and tools, combining monolingual and cross-lingual word alignments as well as standard textual entailment distance and bag-of-words features in a statistical learning framework. We learn separate binary classifiers for each entailment direction and combine them to obtain four entailment relations. Our system yielded the best overall score for three out of four language pairs. 1 Introduction Cross-lingual textual entailment (CLTE) (Mehdad et al., 2010) is an extension of textual entailment (TE) (Dagan and Glickman, 2004). The task of recognizing entailment is to determine whether a hypothesis H can be semantically inferred from a text T. The CLTE task adds a cross-lingual dimension to the problem by considering sentence pairs, where T and H are in different languages. The SemEval-2012 CLTE task (Negri et al., 2012) asks participants to judge entailment pairs in four language combinations1, defining four target entailment relations, forward, backward, bidirectional and no entailment. We investigate this problem in a statistical learning fram</context>
<context position="2642" citStr="Mehdad et al. (2010)" startWordPosition="390" endWordPosition="393">ent task can benefit from direct alignments between T and H, since a large amount of bilingual parallel data is available, which naturally models synonymy and paraphrasing across languages. 2 Related Work With the yearly Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006), there has been a lot of work on monolingual TE. We therefore include established monolingual features in our approach, such as alignment scores (MacCartney et al., 2008), edit distance and bag-of-words lexical overlap measures (Kouylekov and Negri, 2010). So far, the only work on CLTE that we are aware of is Mehdad et al. (2010), where the problem is reduced to monolingual entailment using machine translation, and Mehdad et al. (2011), which exploits parallel corpora for generating features based on phrase alignments as input to an SVM. Our approach combines ideas from both, mostly resembling Mehdad et al. (2011). There are, however, several differences; we use word translation probabilities instead of phrase tables and model monolingual and crosslingual alignment separately. We also include additional similarity measures derived from the MT evaluation metric Meteor, which was used in Volokh and Neumann (2011) for th</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2010</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2010. Towards cross-lingual textual entailment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 321– 324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Using bilingual parallel corpora for crosslingual textual entailment.</title>
<date>2011</date>
<booktitle>Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="2750" citStr="Mehdad et al. (2011)" startWordPosition="407" endWordPosition="410"> is available, which naturally models synonymy and paraphrasing across languages. 2 Related Work With the yearly Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006), there has been a lot of work on monolingual TE. We therefore include established monolingual features in our approach, such as alignment scores (MacCartney et al., 2008), edit distance and bag-of-words lexical overlap measures (Kouylekov and Negri, 2010). So far, the only work on CLTE that we are aware of is Mehdad et al. (2010), where the problem is reduced to monolingual entailment using machine translation, and Mehdad et al. (2011), which exploits parallel corpora for generating features based on phrase alignments as input to an SVM. Our approach combines ideas from both, mostly resembling Mehdad et al. (2011). There are, however, several differences; we use word translation probabilities instead of phrase tables and model monolingual and crosslingual alignment separately. We also include additional similarity measures derived from the MT evaluation metric Meteor, which was used in Volokh and Neumann (2011) for the monolingual TE task. Con467 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 46</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2011</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2011. Using bilingual parallel corpora for crosslingual textual entailment. Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="3778" citStr="Munteanu and Marcu (2005)" startWordPosition="565" endWordPosition="568"> from the MT evaluation metric Meteor, which was used in Volokh and Neumann (2011) for the monolingual TE task. Con467 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 467–471, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics versely, Pad´o et al. (2009) showed that textual entailment features can be used for measuring MT quality, indicating a strong relatedness of the two problems. The CLTE task is also related to the problem of identifying parallel sentence pairs in a non-parallel corpus, so we adapt alignment-based features from Munteanu and Marcu (2005), where a Maximum Entropy classifier was used to judge if two sentences are sufficiently parallel. Regarding the view on entailment, MacCartney and Manning (2007) proposed the decomposition of top-level entailment, such as equivalence (which corresponds to the CLTE bidirectional class), into atomic forward and backward entailment predictions, which is mirrored in our multi-label approach with two binary classifiers. 3 SMT Features for CLTE The SemEval-2012 CLTE task emerges from the monolingual RTE task; however the perception of entailment differs slightly. In CLTE, the sentences T1 and T2 ar</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Luisa Bentivogli</author>
<author>Yashar Mehdad</author>
<author>Danilo Giampiccolo</author>
<author>Alessandro Marchetti</author>
</authors>
<title>Divide and conquer: crowdsourcing the creation of crosslingual textual entailment corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>670--679</pages>
<contexts>
<context position="4478" citStr="Negri et al. (2011)" startWordPosition="671" endWordPosition="674">ciently parallel. Regarding the view on entailment, MacCartney and Manning (2007) proposed the decomposition of top-level entailment, such as equivalence (which corresponds to the CLTE bidirectional class), into atomic forward and backward entailment predictions, which is mirrored in our multi-label approach with two binary classifiers. 3 SMT Features for CLTE The SemEval-2012 CLTE task emerges from the monolingual RTE task; however the perception of entailment differs slightly. In CLTE, the sentences T1 and T2 are of roughly the same length and the entailment is predicted in both directions. Negri et al. (2011) states that the CLTE pairs were created by paraphrasing an English sentence E and leaving out or adding information to construct a modified sentence E&apos;, which was then translated into a different language2, yielding sentence F and thus creating a bilingual entailment pair. For this reason, we believe that our system should be less inferenceoriented than some previous RTE systems and rather should capture • paraphrases and synonymy to identify semantic equivalence, • phrases that have no matching correspondent in the other sentence, indicating missing (respectively, additional) information. To</context>
</contexts>
<marker>Negri, Bentivogli, Mehdad, Giampiccolo, Marchetti, 2011</marker>
<rawString>Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide and conquer: crowdsourcing the creation of crosslingual textual entailment corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 670–679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Alessandro Marchetti</author>
<author>Yashar Mehdad</author>
<author>Luisa Bentivogli</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1246" citStr="Negri et al., 2012" startWordPosition="177" endWordPosition="180">ssifiers for each entailment direction and combine them to obtain four entailment relations. Our system yielded the best overall score for three out of four language pairs. 1 Introduction Cross-lingual textual entailment (CLTE) (Mehdad et al., 2010) is an extension of textual entailment (TE) (Dagan and Glickman, 2004). The task of recognizing entailment is to determine whether a hypothesis H can be semantically inferred from a text T. The CLTE task adds a cross-lingual dimension to the problem by considering sentence pairs, where T and H are in different languages. The SemEval-2012 CLTE task (Negri et al., 2012) asks participants to judge entailment pairs in four language combinations1, defining four target entailment relations, forward, backward, bidirectional and no entailment. We investigate this problem in a statistical learning framework, which allows us to combine crosslingual word alignment features as well as common 1Spanish-English (es-en), Italian-English (it-en), FrenchEnglish (fr-en) and German-English (de-en). monolingual entailment metrics, such as bag-ofwords overlap, edit distance and monolingual alignments on translations of T and H, using standard statistical machine translation (SM</context>
</contexts>
<marker>Negri, Marchetti, Mehdad, Bentivogli, Giampiccolo, 2012</marker>
<rawString>Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa Bentivogli, and Danilo Giampiccolo. 2012. Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="11160" citStr="Noreen, 1989" startWordPosition="1749" endWordPosition="1750">eature types for each language combination, we performed ablation tests on the development set, where we switched off groups of features and measured the 469 es-en it-en fr-en de-en multi-class 0.47 0.456 0.466 0.458 multi-label 0.586 0.526 0.568 0.522 2x binary 0.646 0.614 0.628 0.588 Table 2: Different classifiers on development set. es-en it-en fr-en de-en SVMti�ght 0.630 0.554 0.564 0.558 Sol 0.632 0.562 0.570 0.552 Table 3: Results on test set. impact on the accuracy score (table 4). We assessed the statistical significance of differences in score with an approximate randomization test8 (Noreen, 1989), indicating a significant impact in bold font. The results show that only in two cases a single feature group significantly impacts the score, namely the Meteor score features for es-en and the crosslingual alignment features for de-en. However, no feature group hurts the score either, since negative variations in score are not significant. To ensure that the different feature groups actually express diverse information, we also evaluated our system using only one group of features at a time. The results confirm the most significant feature type for each language pair, but even the best-scori</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="7498" citStr="Och and Ney, 2003" startWordPosition="1157" endWordPosition="1160"> the resulting alignment. We include the overall weighted Meteor score both for (E,T(F)) 3http://translate.google.com/ min bEB ed(a, b) 468 and (F, T (E))4 as well as separate alignment precision, recall and fragmentation scores for (E, T (F)). 3.4 Monolingual alignment features We use the alignments output by the Meteor-1.3 scorer for (E,T(F))5 to calculate the following metrics: • number of unaligned words • percentage of aligned words • length of the longest unaligned subsequence 3.5 Cross-lingual alignment features We calculate IBM model 1 word alignments (Brown et al., 1993) with GIZA++ (Och and Ney, 2003) on a data set concatenated from Europarl-v66 (Koehn, 2005) and a bilingual dictionary obtained from dict.cc7 for coverage. We then heuristically align each word e in E with the word f in F for which we find the highest word translation probability p(e|f) and vice versa. Words for which no translation is found are considered unaligned. From this alignment a, we derive the following features both for E and F (resulting in a total of eight cross-lingual alignment features): • number of unaligned words • percentage of aligned words • alignment score 1 JE� eEE � p(e|a(e)) • length of the longest u</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Daniel Cer</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Measuring machine translation quality as semantic equivalence: A metric based on entailment features.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<volume>23</volume>
<issue>2</issue>
<marker>Pad´o, Cer, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pad´o, Daniel Cer, Michel Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Measuring machine translation quality as semantic equivalence: A metric based on entailment features. Machine Translation, 23(2):181–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Volokh</author>
<author>G¨unter Neumann</author>
</authors>
<title>Using MT-based metrics for RTE. In The Fourth Text Analysis Conference.</title>
<date>2011</date>
<publisher>NIST.</publisher>
<contexts>
<context position="3235" citStr="Volokh and Neumann (2011)" startWordPosition="483" endWordPosition="486">aware of is Mehdad et al. (2010), where the problem is reduced to monolingual entailment using machine translation, and Mehdad et al. (2011), which exploits parallel corpora for generating features based on phrase alignments as input to an SVM. Our approach combines ideas from both, mostly resembling Mehdad et al. (2011). There are, however, several differences; we use word translation probabilities instead of phrase tables and model monolingual and crosslingual alignment separately. We also include additional similarity measures derived from the MT evaluation metric Meteor, which was used in Volokh and Neumann (2011) for the monolingual TE task. Con467 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 467–471, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics versely, Pad´o et al. (2009) showed that textual entailment features can be used for measuring MT quality, indicating a strong relatedness of the two problems. The CLTE task is also related to the problem of identifying parallel sentence pairs in a non-parallel corpus, so we adapt alignment-based features from Munteanu and Marcu (2005), where a Maximum Entropy classifier was used to judge if</context>
</contexts>
<marker>Volokh, Neumann, 2011</marker>
<rawString>Alexander Volokh and G¨unter Neumann. 2011. Using MT-based metrics for RTE. In The Fourth Text Analysis Conference. NIST.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>