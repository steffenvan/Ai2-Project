<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997829">
Exploiting Feature Hierarchy for Transfer Learning in Named Entity
Recognition
</title>
<author confidence="0.995734">
Andrew Arnold, Ramesh Nallapati and William W. Cohen
</author>
<affiliation confidence="0.99022">
Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA
</affiliation>
<email confidence="0.988761">
{aarnold, nmramesh, wcohen}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840821428571">
We present a novel hierarchical prior struc-
ture for supervised transfer learning in named
entity recognition, motivated by the common
structure of feature spaces for this task across
natural language data sets. The problem of
transfer learning, where information gained in
one learning task is used to improve perfor-
mance in another related task, is an important
new area of research. In the subproblem of do-
main adaptation, a model trained over a source
domain is generalized to perform well on a re-
lated target domain, where the two domains’
data are distributed similarly, but not identi-
cally. We introduce the concept of groups
of closely-related domains, called genres, and
show how inter-genre adaptation is related to
domain adaptation. We also examine multi-
task learning, where two domains may be re-
lated, but where the concept to be learned in
each case is distinct. We show that our prior
conveys useful information across domains,
genres and tasks, while remaining robust to
spurious signals not related to the target do-
main and concept. We further show that our
model generalizes a class of similar hierarchi-
cal priors, smoothed to varying degrees, and
lay the groundwork for future exploration in
this area.
</bodyText>
<sectionHeader confidence="0.999751" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.999092">
1.1 Problem definition
</subsectionHeader>
<bodyText confidence="0.999980476190476">
Consider the task of named entity recognition
(NER). Specifically, you are given a corpus of news
articles in which all tokens have been labeled as ei-
ther belonging to personal name mentions or not.
The standard supervised machine learning problem
is to learn a classifier over this training data that will
successfully label unseen test data drawn from the
same distribution as the training data, where “same
distribution” could mean anything from having the
train and test articles written by the same author to
having them written in the same language. Having
successfully trained a named entity classifier on this
news data, now consider the problem of learning to
classify tokens as names in e-mail data. An intuitive
solution might be to simply retrain the classifier, de
novo, on the e-mail data. Practically, however, large,
labeled datasets are often expensive to build and this
solution would not scale across a large number of
different datasets.
Clearly the problems of identifying names in
news articles and e-mails are closely related, and
learning to do well on one should help your per-
formance on the other. At the same time, however,
there are serious differences between the two prob-
lems that need to be addressed. For instance, cap-
italization, which will certainly be a useful feature
in the news problem, may prove less informative in
the e-mail data since the rules of capitalization are
followed less strictly in that domain.
These are the problems we address in this paper.
In particular, we develop a novel prior for named
entity recognition that exploits the hierarchical fea-
ture space often found in natural language domains
(§1.2) and allows for the transfer of information
from labeled datasets in other domains (§1.3). §2
introduces the maximum entropy (maxent) and con-
ditional random field (CRF) learning techniques em-
ployed, along with specifications for the design and
training of our hierarchical prior. Finally, in §3 we
present an empirical investigation of our prior’s per-
formance against a number of baselines, demonstrat-
ing both its effectiveness and robustness.
</bodyText>
<subsectionHeader confidence="0.982019">
1.2 Hierarchical feature trees
</subsectionHeader>
<bodyText confidence="0.9995268">
In many NER problems, features are often con-
structed as a series of transformations of the input
training data, performed in sequence. Thus, if our
task is to identify tokens as either being (O)utside or
(I)nside person names, and we are given the labeled
</bodyText>
<page confidence="0.984065">
245
</page>
<note confidence="0.688233">
Proceedings of ACL-08: HLT, pages 245–253,
</note>
<page confidence="0.359422">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.437893">
sample training sentence:
</bodyText>
<equation confidence="0.658349">
O O O O O I
</equation>
<bodyText confidence="0.94899352631579">
Give the book to Professor Caldwell
(1)
one such useful feature might be: Is the token one
slot to the left of the current token Professor?
We can represent this symbolically as L.1.Professor
where we describe the whole space of useful features
of this form as: {direction = (L)eft, (C)urrent,
(R)ight}.{distance= 1, 2, 3, ...}.{value= Pro-
fessor, book, ...}. We can conceptualize this struc-
ture as a tree, where each slot in the symbolic name
of a feature is a branch and each period between slots
represents another level, going from root to leaf as
read left to right. Thus a subsection of the entire fea-
ture tree for the token Caldwell could be drawn
as in Figure 1 (zoomed in on the section of the tree
where the L.1.Professor feature resides).
direction
distance ... ...
true false ...
</bodyText>
<figureCaption confidence="0.997145">
Figure 1: Graphical representation of a hierarchical fea-
ture tree for token Caldwell in example Sentence 1.
</figureCaption>
<bodyText confidence="0.999970705882353">
Representing feature spaces with this kind of tree,
besides often coinciding with the explicit language
used by common natural language toolkits (Cohen,
2004), has the added benefit of allowing a model to
easily back-off, or smooth, to decreasing levels of
specificity. For example, the leaf level of the fea-
ture tree for our sample Sentence 1 tells us that the
word Professor is important, with respect to la-
beling person names, when located one slot to the
left of the current word being classified. This may
be useful in the context of an academic corpus, but
might be less useful in a medical domain where the
word Professor occurs less often. Instead, we
might want to learn the related feature L.1.Dr. In
fact, it might be useful to generalize across multiple
domains the fact that the word immediately preced-
ing the current word is often important with respect
</bodyText>
<equation confidence="0.779912">
LeftToken.*
LeftToken.IsWord.*
LeftToken.IsWord.IsTitle.*
LeftToken.IsWord.IsTitle.equals.*
LeftToken.IsWord.IsTitle.equals.mr
</equation>
<tableCaption confidence="0.992448">
Table 1: A few examples of the feature hierarchy
</tableCaption>
<bodyText confidence="0.999865461538462">
to the named entity status of the current word. This
is easily accomplished by backing up one level from
a leaf in the tree structure to its parent, to represent
a class of features such as L.1.*. It has been shown
empirically that, while the significance ofparticular
features might vary between domains and tasks, cer-
tain generalized classes of features retain their im-
portance across domains (Minkov et al., 2005). By
backing-off in this way, we can use the feature hier-
archy as a prior for transferring beliefs about the sig-
nificance of entire classes of features across domains
and tasks. Some examples illustrating this idea are
shown in table 1.
</bodyText>
<subsectionHeader confidence="0.986852">
1.3 Transfer learning
</subsectionHeader>
<bodyText confidence="0.973832384615385">
When only the type of data being examined is al-
lowed to vary (from news articles to e-mails, for
example), the problem is called domain adapta-
tion (Daum´e III and Marcu, 2006). When the task
being learned varies (say, from identifying person
names to identifying protein names), the problem
is called multi-task learning (Caruana, 1997). Both
of these are considered specific types of the over-
arching transfer learning problem, and both seem
to require a way of altering the classifier learned
on the first problem (called the source domain, or
source task) to fit the specifics of the second prob-
lem (called the target domain, or target task).
More formally, given an example x and a class
label y, the standard statistical classification task
is to assign a probability, p(y|x), to x of belong-
ing to class y. In the binary classification case the
labels are Y ∈ {0, 1}. In the case we examine,
each example xi is represented as a vector of bi-
nary features (f1(xi), · · · , fF(xi)) where F is the
number of features. The data consists of two dis-
joint subsets: the training set (Xtrain, Ytrain) =
{(x1, y1) · · · , (xN, yN)}, available to the model for
its training and the test set Xtest = (x1, · · · , xM),
upon which we want to use our trained classifier to
make predictions.
</bodyText>
<figure confidence="0.994425777777778">
L
C R
1
2
...
value
Professor
book...
... ...
</figure>
<page confidence="0.998103">
246
</page>
<bodyText confidence="0.965397928571429">
In the paradigm of inductive learning,
(Xtrain, Ytrain) are known, while both Xtest and
Ytest are completely hidden during training time. In
this cases Xtest and Xtrain are both assumed to have
been drawn from the same distribution, D. In the
setting of transfer learning, however, we would like
to apply our trained classifier to examples drawn
from a distribution different from the one upon
which it was trained. We therefore assume there
are two different distributions, Dsource and Dtarget,
from which data may be drawn. Given this notation
we can then precisely state the transfer learning
problem as trying to assign labels Y target
test to test
</bodyText>
<subsectionHeader confidence="0.527652">
data Xtarget
</subsectionHeader>
<bodyText confidence="0.598953">
test drawn from Dtarget, given training
data (Xsource Y source) drawn from Dsource.
train , train
</bodyText>
<listItem confidence="0.921485">
In this paper we focus on two subproblems of
transfer learning:
• domain adaptation, where we assume Y (the set
of possible labels) is the same for both Dsource
and Dtarget, while Dsource and Dtarget them-
selves are allowed to vary between domains.
• multi-task learning (Ando and Zhang, 2005;
Caruana, 1997; Sutton and McCallum, 2005;
Zhang et al., 2005) in which the task (and label
set) is allowed to vary from source to target.
</listItem>
<bodyText confidence="0.999790347826087">
Domain adaptation can be further distinguished by
the degree of relatedness between the source and tar-
get domains. For example, in this work we group
data collected in the same medium (e.g., all anno-
tated e-mails or all annotated news articles) as be-
longing to the same genre. Although the specific
boundary between domain and genre for a particu-
lar set of data is often subjective, it is nevertheless a
useful distinction to draw.
One common way of addressing the transfer
learning problem is to use aprior which, in conjunc-
tion with a probabilistic model, allows one to spec-
ify a priori beliefs about a distribution, thus bias-
ing the results a learning algorithm would have pro-
duced had it only been allowed to see the training
data (Raina et al., 2006). In the example from §1.1,
our belief that capitalization is less strict in e-mails
than in news articles could be encoded in a prior that
biased the importance of the capitalization
feature to be lower for e-mails than news articles.
In the next section we address the problem of how
to come up with a suitable prior for transfer learning
across named entity recognition problems.
</bodyText>
<sectionHeader confidence="0.931557" genericHeader="method">
2 Models considered
</sectionHeader>
<subsectionHeader confidence="0.999408">
2.1 Basic Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.99986825">
In this work, we will base our work on Condi-
tional Random Fields (CRF’s) (Lafferty et al., 2001),
which are now one of the most preferred sequential
models for many natural language processing tasks.
The parametric form of the CRF for a sentence of
length n is given as follows:
(2)
where Z(x) is the normalization term. CRF learns a
model consisting of a set of weights Λ = {λ1...λF}
over the features so as to maximize the conditional
likelihood of the training data, p(Ytrain|Xtrain),
given the model pΛ.
</bodyText>
<subsectionHeader confidence="0.999254">
2.2 CRF with Gaussian priors
</subsectionHeader>
<bodyText confidence="0.85537975">
To avoid overfitting the training data, these λ’s are
often further constrained by the use of a Gaussian
prior (Chen and Rosenfeld, 1999) with diagonal co-
variance, N(µ, σ2), which tries to maximize:
argmax )2
Λ (log PA(YkIXk) I — Q� (Aj — µi
1
2
k=1 j2σj
where β &gt; 0 is a parameter controlling the amount
of regularization, and N is the number of sentences
in the training set.
</bodyText>
<subsectionHeader confidence="0.998993">
2.3 Source trained priors
</subsectionHeader>
<bodyText confidence="0.985518">
One recently proposed method (Chelba and Acero,
2004) for transfer learning in Maximum Entropy
models 1 involves modifying the µ’s of this Gaussian
prior. First a model of the source domain, Λsource,
is learned by training on {Xsource
train , Y trsoain urce}. Then a
</bodyText>
<listItem confidence="0.7933345">
model of the target domain is trained over a limited
set of labeled target data {Xtaa nt,Y t arnet}, but in-
</listItem>
<bodyText confidence="0.9944014">
stead of regularizing this Λtarget to be near zero (i.e.
setting µ = 0), Λtarget is instead regularized to-
wards the previously learned source values Λsource
(by setting µ = Λsource, while σ2 remains 1) and
thus minimizing (
</bodyText>
<footnote confidence="0.884392333333333">
1Maximum Entropy models are special cases of CRFs that
use the I.I.D. assumption. The method under discussion can
also be extended to CRF directly.
</footnote>
<equation confidence="0.991028166666667">
n F
1
pΛ(Y = y|x) =
Z(x) exp( fj(x,yi)λj)
i=1j=1
Λtarget − Λsource)2.
</equation>
<page confidence="0.984942">
247
</page>
<bodyText confidence="0.997148818181818">
Note that, since this model requires Y target
train in or-
der to learn Λtarget, it, in effect, requires two distinct
labeled training datasets: one on which to train the
prior, and another on which to learn the model’s fi-
nal weights (which we call tuning), using the previ-
ously trained prior for regularization. If we are un-
able to find a match between features in the training
and tuning datasets (for instance, if a word appears
in the tuning corpus but not the training), we back-
off to a standard N(0,1) prior for that feature.
</bodyText>
<figureCaption confidence="0.995243">
Figure 2: Graphical representation of the hierarchical
transfer model.
</figureCaption>
<subsectionHeader confidence="0.975853">
2.4 New model: Hierarchical prior model
</subsectionHeader>
<bodyText confidence="0.995824090909091">
In this section, we will present a new model that
learns simultaneously from multiple domains, by
taking advantage of our feature hierarchy.
We will assume that there are D domains on
which we are learning simultaneously. Let there be
Md training data in each domain d. For our experi-
ments with non-identically distributed, independent
data, we use conditional random fields (cf. §2.1).
However, this model can be extended to any dis-
criminative probabilistic model such as the MaxEnt
model. Let Λ(d) = (λ(d)
</bodyText>
<equation confidence="0.942835">
1 , · · · , λ(d)
</equation>
<bodyText confidence="0.983176105263158">
Fd ) be the param-
eters of the discriminative model in the domain d
where Fd represents the number of features in the
domain d.
Further, we will also assume that the features of
different domains share a common hierarchy repre-
sented by a tree T, whose leaf nodes are the features
themselves (cf. Figure 1). The model parameters
Λ(d), then, form the parameters of the leaves of this
hierarchy. Each non-leaf node n ∈ non-leaf(T) of
the tree is also associated with a hyper-parameter zn.
Note that since the hierarchy is a tree, each node n
has only one parent, represented by pa(n). Simi-
larly, we represent the set of children nodes of a node
n as ch(n).
The entire graphical model for an example con-
sisting of three domains is shown in Figure 2.
The conditional likelihood of the entire training
data (y, x) = {(y(d)
</bodyText>
<equation confidence="0.998754538461538">
1 , x(d)
1 ), ··· , (y(d) Md, x(d) Md)}D d=1 is
given by:
� HD HMd
P(y|x, w, z) = P(y(d)
k |x(d)
k , Λ(d))
d=1 k=1
�
�× �
�
�× �
(3)
</equation>
<bodyText confidence="0.999969166666667">
where the terms in the first line of eq. (3) represent
the likelihood of data in each domain given their cor-
responding model parameters, the second line repre-
sents the likelihood of each model parameter in each
domain given the hyper-parameter of its parent in the
tree hierarchy of features and the last term goes over
the entire tree T except the leaf nodes. Note that in
the last term, the hyper-parameters are shared across
the domains, so there is no product over d.
We perform a MAP estimation for each model pa-
rameter as well as the hyper-parameters. Accord-
ingly, the estimates are given as follows:
</bodyText>
<equation confidence="0.9993115">
∂∂ d) I ())�
log P(yd X(d) � A d
f
+ zpa(f(d))
zpa(n) + EiEch(n)(λ|z)i (4)
1 + |ch(n)|
</equation>
<bodyText confidence="0.999978142857143">
where we used the notation (λ|z)i because node i,
the child node of n, could be a parameter node or
a hyper-parameter node depending on the position
of the node n in the hierarchy. Essentially, in this
model, the weights of the leaf nodes (model param-
eters) depend on the log-likelihood as well as the
prior weight of its parent. Additionally, the weight
</bodyText>
<equation confidence="0.9523742">
w (1)
1
w(1) w (1) w (1) w1(2) (2)
w (2)
w w1(3) w(3)
</equation>
<figure confidence="0.786903060606061">
2 3 4 2 3 2
(1)
xi
(1)
y i
(1)
M
z1 z2
(2)
xi
(2)
yi
M(2)
z
3
(3)
xi
( 3)
y i
M(3)
N(λ(d) f|zpa(f(d)),1) }
H N(zn|zpa(n), 1) }
nETnonleaf
D
H
d=1
Fd
H
f=1
Md
λ(d) f=
i=1
zn =
</figure>
<page confidence="0.983392">
248
</page>
<bodyText confidence="0.99984475">
of each hyper-parameter node in the tree is com-
puted as the average of all its children nodes and its
parent, resulting in a smoothing effect, both up and
down the tree.
</bodyText>
<subsectionHeader confidence="0.992674">
2.5 An approximate Hierarchical prior model
</subsectionHeader>
<bodyText confidence="0.99998580952381">
The Hierarchical prior model is a theoretically well
founded model for transfer learning through feature
heirarchy. However, our preliminary experiments
indicated that its performance on real-life data sets is
not as good as expected. Although a more thorough
investigation needs to be carried out, our analysis in-
dicates that the main reason for this phenomenon is
over-smoothing. In other words, by letting the infor-
mation propagate from the leaf nodes in the hierar-
chy all the way to the root node, the model loses its
ability to discriminate between its features.
As a solution to this problem, we propose an
approximate version of this model that weds ideas
from the exact heirarchical prior model and the
Chelba model.
As with the Chelba prior method in §2.3, this ap-
proximate hierarchical method also requires two dis-
tinct data sets, one for training the prior and another
for tuning the final weights. Unlike Chelba, we
smooth the weights of the priors using the feature-
tree hierarchy presented in §1.1, like the hierarchical
prior model.
For smoothing of each feature weight, we chose to
back-off in the tree as little as possible until we had a
large enough sample of prior data (measured as M,
the number of subtrees below the current node) on
which to form a reliable estimate of the mean and
variance of each feature or class of features. For
example, if the tuning data set is as in Sentence
1, but the prior contains no instances of the word
Professor, then we would back-off and compute
the prior mean and variance on the next higher level
in the tree. Thus the prior for L.1.Professor would
be N(mean(L.1.*), variance(L.1.*)), where mean()
and variance() of L.1.* are the sample mean and
variance of all the features in the prior dataset that
match the pattern L.1.* – or, put another way, all the
siblings of L.1.Professor in the feature tree. If fewer
than M such siblings exist, we continue backing-off,
up the tree, until an ancestor with sufficient descen-
dants is found. A detailed description of the approx-
imate hierarchical algorithm is shown in table 2.
</bodyText>
<figure confidence="0.944772722222222">
Input: Dsource = (Xsource
train , Y source
train )
Dtarget = (Xtarget
train , Ytarget train );
Feature sets Fsource, Ftarget;
Feature Hierarchies Hsource, Htarget
Minimum membership size M
Train CRF using Dsource to obtain
feature weights Asource
For each feature f ∈ Ftarget
Initialize: node n = f
While (n ∈/ Hsource
or |Leaves(Hsource(n)) |≤ M)
and n =6 root(Htarget)
n ← Pa(Htarget(n))
Compute µf and σf using the sample
{λsource i |i ∈ Leaves(Hsource(n))}
</figure>
<bodyText confidence="0.816997">
Train Gaussian prior CRF using Dtarget as data
and {µf} and {σf} as Gaussian prior parameters.
Output:Parameters of the new CRF Atarg
</bodyText>
<tableCaption confidence="0.5647352">
Table 2: Algorithm for approximate hierarchical prior:
Pa(Hsource(n)) is the parent of node n in feature hierar-
chy Hsource; |Leaves(Hsource(n)) |indicates the num-
ber of leaf nodes (basic features) under a node n in the
hierarchy Hsource.
</tableCaption>
<bodyText confidence="0.9754212">
It is important to note that this smoothed tree is
an approximation of the exact model presented in
§2.4 and thus an important parameter of this method
in practice is the degree to which one chooses to
smooth up or down the tree. One of the benefits
of this model is that the semantics of the hierarchy
(how to define a feature, a parent, how and when
to back-off and up the tree, etc.) can be specified
by the user, in reference to the specific datasets and
tasks under consideration. For our experiments, the
semantics of the tree are as presented in §1.1.
The Chelba method can be thought of as a hier-
archical prior in which no smoothing is performed
on the tree at all. Only the leaf nodes of the
prior’s feature tree are considered, and, if no match
can be found between the tuning and prior’s train-
ing datasets’ features, a N(0,1) prior is used in-
stead. However, in the new approximate hierarchical
model, even if a certain feature in the tuning dataset
does not have an analog in the training dataset, we
can always back-off until an appropriate match is
found, even to the level of the root.
Henceforth, we will use only the approximate hi-
erarchical model in our experiments and discussion.
et.
</bodyText>
<page confidence="0.998427">
249
</page>
<tableCaption confidence="0.999731">
Table 3: Summary of data used in experiments
</tableCaption>
<table confidence="0.9969665">
Corpus Genre Task
UTexas Bio Protein
Yapex Bio Protein
MUC6 News Person
MUC7 News Person
CSPACE E-mail Person
</table>
<sectionHeader confidence="0.998488" genericHeader="method">
3 Investigation
</sectionHeader>
<subsectionHeader confidence="0.999904">
3.1 Data, domains and tasks
</subsectionHeader>
<bodyText confidence="0.99984675">
For our experiments, we have chosen five differ-
ent corpora (summarized in Table 3). Although
each corpus can be considered its own domain (due
to variations in annotation standards, specific task,
date of collection, etc), they can also be roughly
grouped into three different genres. These are: ab-
stracts from biological journals [UT (Bunescu et al.,
2004), Yapex (Franz´en et al., 2002)]; news articles
[MUC6 (Fisher et al., 1995), MUC7 (Borthwick et
al., 1998)]; and personal e-mails [CSPACE (Kraut
et al., 2004)]. Each corpus, depending on its genre,
is labeled with one of two name-finding tasks:
</bodyText>
<listItem confidence="0.9991175">
• protein names in biological abstracts
• person names in news articles and e-mails
</listItem>
<bodyText confidence="0.999978739130435">
We chose this array of corpora so that we could
evaluate our hierarchical prior’s ability to generalize
across and incorporate information from a variety of
domains, genres and tasks.
In each case, each item (abstract, article or e-mail)
was tokenized and each token was hand-labeled as
either being part of a name (protein or person) or
not, respectively. We used a standard natural lan-
guage toolkit (Cohen, 2004) to compute tens of
thousands of binary features on each of these to-
kens, encoding such information as capitalization
patterns and contextual information from surround-
ing words. This toolkit produces features of the type
described in §1.2 and thus was amenable to our hi-
erarchical prior model. In particular, we chose to
use the simplest default, out-of-the-box feature gen-
erator and purposefully did not use specifically en-
gineered features, dictionaries, or other techniques
commonly employed to boost performance on such
tasks. The goal of our experiments was to see to
what degree named entity recognition problems nat-
urally conformed to hierarchical methods, and not
just to achieve the highest performance possible.
</bodyText>
<figure confidence="0.9914855">
0 20 40 60 80 100
Percent of target-domain data used for tuning
</figure>
<figureCaption confidence="0.99329525">
Figure 3: Adding a relevant HIER prior helps compared
to the GAUSS baseline ((c) &gt; (a)), while simply CAT’ing
or using CHELBA can hurt ((d) ,: (b) &lt; (a), except with
very little data), and never beats HIER ((c) &gt; (b) ,: (d)).
</figureCaption>
<subsectionHeader confidence="0.998513">
3.2 Experiments &amp; results
</subsectionHeader>
<bodyText confidence="0.9999296">
We evaluated the performance of various transfer
learning methods on the data and tasks described
in §3.1. Specifically, we compared our approximate
hierarchical prior model (HIER), implemented as a
CRF, against three baselines:
</bodyText>
<listItem confidence="0.91862">
• GAUSS: CRF model tuned on a single domain’s
data, using a standard N(0,1) prior
• CAT: CRF model tuned on a concatenation of
multiple domains’ data, using a N(0,1) prior
• CHELBA: CRF model tuned on one domain’s
data, using a prior trained on a different, related
domain’s data (cf. §2.3)
</listItem>
<bodyText confidence="0.999519">
We use token-level F1 as our main evaluation mea-
sure, combining precision and recall into one metric.
</bodyText>
<subsectionHeader confidence="0.677013">
3.2.1 Intra-genre, same-task transfer learning
</subsectionHeader>
<bodyText confidence="0.999921846153846">
Figure 3 shows the results of an experiment in
learning to recognize person names in MUC6 news
articles. In this experiment we examined the effect
of adding extra data from a different, but related do-
main from the same genre, namely, MUC7. Line
a shows the F1 performance of a CRF model tuned
only on the target MUC6 domain (GAUSS) across a
range of tuning data sizes. Line b shows the same
experiment, but this time the CRF model has been
tuned on a dataset comprised of a simple concate-
nation of the training MUC6 data from (a), along
with a different training set from MUC7 (CAT). We
can see that adding extra data in this way, though
</bodyText>
<figure confidence="0.983296375">
Intra-genre transfer performance evaluated on MUC6
F1
0.7
0.6
0.5
0.4
0.3
0.2
0.1
(a) GAUSS: tuned on MUC6
(b) CAT: tuned on MUC6+7
(c) HIER: MUC6+7 prior, tuned on MUC6
(d) CHELBA: MUC6+7 prior, tuned on MUC6
250
0 20 40 60 80 100
Percent of target-domain data used for tuning
</figure>
<figureCaption confidence="0.9801215">
Figure 4: Transfer aware priors CHELBA and HIER ef-
fectively filter irrelevant data. Adding more irrelevant
data to the priors doesn’t hurt ((e) ,: (g) ,: (h)), while
simply CAT’ing it, in this case, is disastrous ((f) &lt;&lt; (e).
</figureCaption>
<bodyText confidence="0.999972846153846">
the data is closely related both in domain and task,
has actually hurt the performance of our recognizer
for training sizes of moderate to large size. This is
most likely because, although the MUC6 and MUC7
datasets are closely related, they are still drawn from
different distributions and thus cannot be intermin-
gled indiscriminately. Line c shows the same com-
bination of MUC6 and MUC7, only this time the
datasets have been combined using the HIER prior.
In this case, the performance actually does improve,
both with respect to the single-dataset trained base-
line (a) and the naively trained double-dataset (b).
Finally, line d shows the results of the CHELBA
prior. Curiously, though the domains are closely re-
lated, it does more poorly than even the non-transfer
GAUSS. One possible explanation is that, although
much of the vocabulary is shared across domains,
the interpretation of the features of these words may
differ. Since CHELBA doesn’t model the hierarchy
among features like HIER, it is unable to smooth
away these discrepancies. In contrast, we see that
our HIER prior is able to successfully combine the
relevant parts of data across domains while filtering
the irrelevant, and possibly detrimental, ones. This
experiment was repeated for other sets of intra-genre
tasks, and the results are summarized in §3.2.3.
</bodyText>
<subsubsectionHeader confidence="0.42161">
3.2.2 Inter-genre, multi-task transfer learning
</subsubsectionHeader>
<bodyText confidence="0.99821076">
In Figure 4 we see that the properties of the hi-
erarchical prior hold even when transferring across
tasks. Here again we are trying to learn to recognize
person names in MUC6 e-mails, but this time, in-
stead of adding only other datasets similarly labeled
with person names, we are additionally adding bi-
ological corpora (UT &amp; YAPEX), labeled not with
person names but with protein names instead, along
with the CSPACE e-mail and MUC7 news article
corpora. The robustness of our prior prevents a
model trained on all five domains (g) from degrading
away from the intra-genre, same-task baseline (e),
unlike the model trained on concatenated data (f).
CHELBA (h) performs similarly well in this case,
perhaps because the domains are so different that al-
most none of the features match between prior and
tuning data, and thus CHELBA backs-off to a stan-
dard JV (0, 1) prior.
This robustness in the face of less similarly related
data is very important since these types of transfer
methods are most useful when one possesses only
very little target domain data. In this situation, it
is often difficult to accurately estimate performance
and so one would like assurance than any transfer
method being applied will not have negative effects.
</bodyText>
<subsectionHeader confidence="0.981869">
3.2.3 Comparison of HIER prior to baselines
</subsectionHeader>
<bodyText confidence="0.999979826086956">
Each scatter plot in Figure 5 shows the relative
performance of a baseline method against HIER.
Each point represents the results of two experi-
ments: the y-coordinate is the F1 score of the base-
line method (shown on the y-axis), while the x-
coordinate represents the score of the HIER method
in the same experiment. Thus, points lying be-
low the y = x line represent experiments for which
HIER received a higher F1 value than did the base-
line. While all three plots show HIER outperform-
ing each of the three baselines, not surprisingly,
the non-transfer GAUSS method suffers the worst,
followed by the naive concatenation (CAT) base-
line. Both methods fail to make any explicit dis-
tinction between the source and target domains and
thus suffer when the domains differ even slightly
from each other. Although the differences are
more subtle, the right-most plot of Figure 5 sug-
gests HIER is likewise able to outperform the non-
hierarchical CHELBA prior in certain transfer sce-
narios. CHELBA is able to avoid suffering as much
as the other baselines when faced with large differ-
ence between domains, but is still unable to capture
</bodyText>
<figure confidence="0.9938555">
Inter-genre transfer performance evaluated on MUC6
F1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
1
(e) HIER: MUC6+7 prior, tuned on MUC6
(f) CAT: tuned on all domains
(g) HIER: all domains prior, tuned on MUC6
(h) CHELBA: all domains prior, tuned on MUC6
</figure>
<page confidence="0.88739">
251
</page>
<table confidence="0.87711825">
GAUSS (F1) 1 CAT (F1) 1 CHELBA (F1) .8
.8 .8 .6
.6 .6 .4
.4 .4
.2 .2
0 0
0 .2 .4 .6 .8 1 0 .2 .4 .6 .8 1 .4 .6 .8
HIER (F1) HIER (F1) HIER (F1)
y = x MUC6@13% MUC6@100% CSPACE@13% CSPACE@100%
MUC6@3% MUC6@25% CSPACE@3% CSPACE@25%
MUC6@6% MUC6@50% CSPACE@6% CSPACE@50%
˜
</table>
<figureCaption confidence="0.787875666666667">
Figure 5: Comparative performance of baseline methods (GAUSS, CAT, CHELBA) vs. HIER prior, as trained on nine
prior datasets (both pure and concatenated) of various sample sizes, evaluated on MUC6 and CSPACE datasets. Points
below the y = x line indicate HIER outperforming baselines.
</figureCaption>
<bodyText confidence="0.991638">
as many dependencies between domains as HIER.
</bodyText>
<sectionHeader confidence="0.96524" genericHeader="method">
4 Conclusions, related &amp; future work
</sectionHeader>
<bodyText confidence="0.999910346153846">
In this work we have introduced hierarchical feature
tree priors for use in transfer learning on named en-
tity extraction tasks. We have provided evidence that
motivates these models on intuitive, theoretical and
empirical grounds, and have gone on to demonstrate
their effectiveness in relation to other, competitive
transfer methods. Specifically, we have shown that
hierarchical priors allow the user enough flexibil-
ity to customize their semantics to a specific prob-
lem, while providing enough structure to resist un-
intended negative effects when used inappropriately.
Thus hierarchical priors seem a natural, effective
and robust choice for transferring learning across
NER datasets and tasks.
Some of the first formulations of the transfer
learning problem were presented over 10 years
ago (Thrun, 1996; Baxter, 1997). Other techniques
have tried to quantify the generalizability of cer-
tain features across domains (Daum´e III and Marcu,
2006; Jiang and Zhai, 2006), or tried to exploit the
common structure of related problems (Ben-David
et al., 2007; Sch¨olkopf et al., 2005). Most of
this prior work deals with supervised transfer learn-
ing, and thus requires labeled source domain data,
though there are examples of unsupervised (Arnold
et al., 2007), semi-supervised (Grandvalet and Ben-
gio, 2005; Blitzer et al., 2006), and transductive ap-
proaches (Taskar et al., 2003).
Recent work using so-called meta-level priors to
transfer information across tasks (Lee et al., 2007),
while related, does not take into explicit account the
hierarchical structure of these meta-level features of-
ten found in NLP tasks. Daum´e allows an extra de-
gree of freedom among the features of his domains,
implicitly creating a two-level feature hierarchy with
one branch for general features, and another for do-
main specific ones, but does not extend his hierar-
chy further (Daum´e III, 2007)). Similarly, work on
hierarchical penalization (Szafranski et al., 2007) in
two-level trees tries to produce models that rely only
on a relatively small number of groups of variable,
as structured by the tree, as opposed to transferring
knowledge between branches themselves.
Our future work is focused on designing an al-
gorithm to optimally choose a smoothing regime
for the learned feature trees so as to better exploit
the similarities between domains while neutralizing
their differences. Along these lines, we are working
on methods to reduce the amount of labeled target
domain data needed to tune the prior-based mod-
els, looking forward to semi-supervised and unsu-
pervised transfer methods.
</bodyText>
<page confidence="0.995037">
252
</page>
<sectionHeader confidence="0.995746" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99974490909091">
Rie K. Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. In JMLR 6, pages 1817 – 1853.
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2007. A comparative study of methods for trans-
ductive transfer learning. In Proceedings of the IEEE
International Conference on Data Mining (ICDM)
2007 Workshop on Mining and Management of Bio-
logical Data.
Jonathan Baxter. 1997. A Bayesian/information theo-
retic model of learning to learn via multiple task sam-
pling. Machine Learning, 28(1):7–39.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations for
domain adaptation. In NIPS 20, Cambridge, MA. MIT
Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP, Sydney, Australia.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. NYU: Description of the MENE named entity
system as used in MUC-7.
R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney,
A. Ramani, and Y. Wong. 2004. Comparative experi-
ments on learning information extractors for proteins
and their interactions. In Journal of AI in Medicine.
Data from ftp://ftp.cs.utexas.edu/pub/mooney/bio-
data/proteins.tar.gz.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41–75.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Dekang Lin and Dekai Wu, editors, EMNLP
2004, pages 285–292. ACL.
S. Chen and R. Rosenfeld. 1999. A gaussian prior for
smoothing maximum entropy models.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. In Journal ofArtificial
Intelligence Research 26, pages 101–126.
Hal Daum´e III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
David Fisher, Stephen Soderland, Joseph McCarthy,
Fangfang Feng, and Wendy Lehnert. 1995. Descrip-
tion of the UMass system as used for MUC-6.
Kristofer Franz´en, Gunnar Eriksson, Fredrik Olsson, Lars
Asker, Per Lidn, and Joakim C¨oster. 2002. Protein
names and how to find them. In International Journal
ofMedical Informatics.
Yves Grandvalet and Yoshua Bengio. 2005. Semi-
supervised learning by entropy minimization. In CAP,
Nice, France.
Jing Jiang and ChengXiang Zhai. 2006. Exploiting do-
main structure for named entity recognition. In Hu-
man Language Technology Conference, pages 74 – 81.
R. Kraut, S. Fussell, F. Lerch, and J. Espinosa. 2004. Co-
ordination in teams: evidence from a simulated man-
agement game.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282–289. Morgan Kaufmann, San Francisco, CA.
S.-I. Lee, V. Chatalbashev, D. Vickrey, and D. Koller.
2007. Learning a meta-level prior for feature relevance
from multiple related tasks. In Proceedings ofInterna-
tional Conference on Machine Learning (ICML).
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: Ap-
plying named entity recognition to informal text. In
HLT/EMNLP.
Rajat Raina, Andrew Y. Ng, and Daphne Koller. 2006.
Transfer learning by constructing informative priors.
In ICML 22.
Bernhard Sch¨olkopf, Florian Steinke, and Volker Blanz.
2005. Object correspondence as a machine learning
problem. In ICML ’05: Proceedings of the 22nd inter-
national conference on Machine learning, pages 776–
783, New York, NY, USA. ACM.
Charles Sutton and Andrew McCallum. 2005. Composi-
tion of conditional random fields for transfer learning.
In HLT/EMLNLP.
M. Szafranski, Y. Grandvalet, and P. Morizet-
Mahoudeaux. 2007. Hierarchical penalization.
In Advances in Neural Information Processing
Systems 20. MIT press.
B. Taskar, M.-F. Wong, and D. Koller. 2003. Learn-
ing on the test data: Leveraging ‘unseen’ features. In
Proc. Twentieth International Conference on Machine
Learning (ICML).
Sebastian Thrun. 1996. Is learning the n-th thing any
easier than learning the first? In NIPS, volume 8,
pages 640–646. MIT.
J. Zhang, Z. Ghahramani, and Y. Yang. 2005. Learning
multiple related tasks using latent independent compo-
nent analysis.
</reference>
<page confidence="0.998929">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890737">
<title confidence="0.9996165">Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition</title>
<author confidence="0.999829">Andrew Arnold</author>
<author confidence="0.999829">Ramesh Nallapati</author>
<author confidence="0.999829">William W Cohen</author>
<address confidence="0.939583">Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA</address>
<email confidence="0.983509">nmramesh,</email>
<abstract confidence="0.998715827586207">We present a novel hierarchical prior structure for supervised transfer learning in named entity recognition, motivated by the common structure of feature spaces for this task across natural language data sets. The problem of transfer learning, where information gained in one learning task is used to improve performance in another related task, is an important new area of research. In the subproblem of domain adaptation, a model trained over a source domain is generalized to perform well on a related target domain, where the two domains’ data are distributed similarly, but not identically. We introduce the concept of groups closely-related domains, called and show how inter-genre adaptation is related to domain adaptation. We also examine multitask learning, where two domains may be related, but where the concept to be learned in each case is distinct. We show that our prior conveys useful information across domains, genres and tasks, while remaining robust to spurious signals not related to the target domain and concept. We further show that our model generalizes a class of similar hierarchical priors, smoothed to varying degrees, and lay the groundwork for future exploration in this area.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie K Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<booktitle>In JMLR 6,</booktitle>
<pages>1817--1853</pages>
<contexts>
<context position="9072" citStr="Ando and Zhang, 2005" startWordPosition="1498" endWordPosition="1501"> there are two different distributions, Dsource and Dtarget, from which data may be drawn. Given this notation we can then precisely state the transfer learning problem as trying to assign labels Y target test to test data Xtarget test drawn from Dtarget, given training data (Xsource Y source) drawn from Dsource. train , train In this paper we focus on two subproblems of transfer learning: • domain adaptation, where we assume Y (the set of possible labels) is the same for both Dsource and Dtarget, while Dsource and Dtarget themselves are allowed to vary between domains. • multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Sutton and McCallum, 2005; Zhang et al., 2005) in which the task (and label set) is allowed to vary from source to target. Domain adaptation can be further distinguished by the degree of relatedness between the source and target domains. For example, in this work we group data collected in the same medium (e.g., all annotated e-mails or all annotated news articles) as belonging to the same genre. Although the specific boundary between domain and genre for a particular set of data is often subjective, it is nevertheless a useful distinction to draw. One common way of addressing</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie K. Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. In JMLR 6, pages 1817 – 1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Arnold</author>
<author>Ramesh Nallapati</author>
<author>William W Cohen</author>
</authors>
<title>A comparative study of methods for transductive transfer learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM) 2007 Workshop on Mining and Management of Biological Data.</booktitle>
<contexts>
<context position="29979" citStr="Arnold et al., 2007" startWordPosition="5109" endWordPosition="5112">ce for transferring learning across NER datasets and tasks. Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks. Daum´e allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy furthe</context>
</contexts>
<marker>Arnold, Nallapati, Cohen, 2007</marker>
<rawString>Andrew Arnold, Ramesh Nallapati, and William W. Cohen. 2007. A comparative study of methods for transductive transfer learning. In Proceedings of the IEEE International Conference on Data Mining (ICDM) 2007 Workshop on Mining and Management of Biological Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Baxter</author>
</authors>
<title>A Bayesian/information theoretic model of learning to learn via multiple task sampling.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="29543" citStr="Baxter, 1997" startWordPosition="5042" endWordPosition="5043">nd empirical grounds, and have gone on to demonstrate their effectiveness in relation to other, competitive transfer methods. Specifically, we have shown that hierarchical priors allow the user enough flexibility to customize their semantics to a specific problem, while providing enough structure to resist unintended negative effects when used inappropriately. Thus hierarchical priors seem a natural, effective and robust choice for transferring learning across NER datasets and tasks. Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level pri</context>
</contexts>
<marker>Baxter, 1997</marker>
<rawString>Jonathan Baxter. 1997. A Bayesian/information theoretic model of learning to learn via multiple task sampling. Machine Learning, 28(1):7–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2007</date>
<booktitle>In NIPS 20,</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="29777" citStr="Ben-David et al., 2007" startWordPosition="5077" endWordPosition="5080"> their semantics to a specific problem, while providing enough structure to resist unintended negative effects when used inappropriately. Thus hierarchical priors seem a natural, effective and robust choice for transferring learning across NER datasets and tasks. Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks. Daum´e allows an extra degree of freedom</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2007</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2007. Analysis of representations for domain adaptation. In NIPS 20, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="30048" citStr="Blitzer et al., 2006" startWordPosition="5119" endWordPosition="5122">the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks. Daum´e allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further (Daum´e III, 2007)). Similarly, work on hierarchical penalization (</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<title>NYU: Description of the MENE named entity system as used in MUC-7.</title>
<date>1998</date>
<contexts>
<context position="20559" citStr="Borthwick et al., 1998" startWordPosition="3536" endWordPosition="3539">ments Corpus Genre Task UTexas Bio Protein Yapex Bio Protein MUC6 News Person MUC7 News Person CSPACE E-mail Person 3 Investigation 3.1 Data, domains and tasks For our experiments, we have chosen five different corpora (summarized in Table 3). Although each corpus can be considered its own domain (due to variations in annotation standards, specific task, date of collection, etc), they can also be roughly grouped into three different genres. These are: abstracts from biological journals [UT (Bunescu et al., 2004), Yapex (Franz´en et al., 2002)]; news articles [MUC6 (Fisher et al., 1995), MUC7 (Borthwick et al., 1998)]; and personal e-mails [CSPACE (Kraut et al., 2004)]. Each corpus, depending on its genre, is labeled with one of two name-finding tasks: • protein names in biological abstracts • person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical prior’s ability to generalize across and incorporate information from a variety of domains, genres and tasks. In each case, each item (abstract, article or e-mail) was tokenized and each token was hand-labeled as either being part of a name (protein or person) or not, respectively. We used a standard n</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998. NYU: Description of the MENE named entity system as used in MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Ge</author>
<author>R Kate</author>
<author>E Marcotte</author>
<author>R Mooney</author>
<author>A Ramani</author>
<author>Y Wong</author>
</authors>
<title>Comparative experiments on learning information extractors for proteins and their interactions.</title>
<date>2004</date>
<booktitle>In Journal of AI in Medicine. Data from ftp://ftp.cs.utexas.edu/pub/mooney/biodata/proteins.tar.gz.</booktitle>
<contexts>
<context position="20453" citStr="Bunescu et al., 2004" startWordPosition="3519" endWordPosition="3522">te hierarchical model in our experiments and discussion. et. 249 Table 3: Summary of data used in experiments Corpus Genre Task UTexas Bio Protein Yapex Bio Protein MUC6 News Person MUC7 News Person CSPACE E-mail Person 3 Investigation 3.1 Data, domains and tasks For our experiments, we have chosen five different corpora (summarized in Table 3). Although each corpus can be considered its own domain (due to variations in annotation standards, specific task, date of collection, etc), they can also be roughly grouped into three different genres. These are: abstracts from biological journals [UT (Bunescu et al., 2004), Yapex (Franz´en et al., 2002)]; news articles [MUC6 (Fisher et al., 1995), MUC7 (Borthwick et al., 1998)]; and personal e-mails [CSPACE (Kraut et al., 2004)]. Each corpus, depending on its genre, is labeled with one of two name-finding tasks: • protein names in biological abstracts • person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical prior’s ability to generalize across and incorporate information from a variety of domains, genres and tasks. In each case, each item (abstract, article or e-mail) was tokenized and each token was </context>
</contexts>
<marker>Bunescu, Ge, Kate, Marcotte, Mooney, Ramani, Wong, 2004</marker>
<rawString>R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney, A. Ramani, and Y. Wong. 2004. Comparative experiments on learning information extractors for proteins and their interactions. In Journal of AI in Medicine. Data from ftp://ftp.cs.utexas.edu/pub/mooney/biodata/proteins.tar.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning,</booktitle>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="7024" citStr="Caruana, 1997" startWordPosition="1134" endWordPosition="1135">inkov et al., 2005). By backing-off in this way, we can use the feature hierarchy as a prior for transferring beliefs about the significance of entire classes of features across domains and tasks. Some examples illustrating this idea are shown in table 1. 1.3 Transfer learning When only the type of data being examined is allowed to vary (from news articles to e-mails, for example), the problem is called domain adaptation (Daum´e III and Marcu, 2006). When the task being learned varies (say, from identifying person names to identifying protein names), the problem is called multi-task learning (Caruana, 1997). Both of these are considered specific types of the overarching transfer learning problem, and both seem to require a way of altering the classifier learned on the first problem (called the source domain, or source task) to fit the specifics of the second problem (called the target domain, or target task). More formally, given an example x and a class label y, the standard statistical classification task is to assign a probability, p(y|x), to x of belonging to class y. In the binary classification case the labels are Y ∈ {0, 1}. In the case we examine, each example xi is represented as a vect</context>
<context position="9087" citStr="Caruana, 1997" startWordPosition="1502" endWordPosition="1503">nt distributions, Dsource and Dtarget, from which data may be drawn. Given this notation we can then precisely state the transfer learning problem as trying to assign labels Y target test to test data Xtarget test drawn from Dtarget, given training data (Xsource Y source) drawn from Dsource. train , train In this paper we focus on two subproblems of transfer learning: • domain adaptation, where we assume Y (the set of possible labels) is the same for both Dsource and Dtarget, while Dsource and Dtarget themselves are allowed to vary between domains. • multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Sutton and McCallum, 2005; Zhang et al., 2005) in which the task (and label set) is allowed to vary from source to target. Domain adaptation can be further distinguished by the degree of relatedness between the source and target domains. For example, in this work we group data collected in the same medium (e.g., all annotated e-mails or all annotated news articles) as belonging to the same genre. Although the specific boundary between domain and genre for a particular set of data is often subjective, it is nevertheless a useful distinction to draw. One common way of addressing the transfer l</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2004</date>
<pages>285--292</pages>
<editor>In Dekang Lin and Dekai Wu, editors, EMNLP</editor>
<publisher>ACL.</publisher>
<contexts>
<context position="11400" citStr="Chelba and Acero, 2004" startWordPosition="1905" endWordPosition="1908">Λ = {λ1...λF} over the features so as to maximize the conditional likelihood of the training data, p(Ytrain|Xtrain), given the model pΛ. 2.2 CRF with Gaussian priors To avoid overfitting the training data, these λ’s are often further constrained by the use of a Gaussian prior (Chen and Rosenfeld, 1999) with diagonal covariance, N(µ, σ2), which tries to maximize: argmax )2 Λ (log PA(YkIXk) I — Q� (Aj — µi 1 2 k=1 j2σj where β &gt; 0 is a parameter controlling the amount of regularization, and N is the number of sentences in the training set. 2.3 Source trained priors One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models 1 involves modifying the µ’s of this Gaussian prior. First a model of the source domain, Λsource, is learned by training on {Xsource train , Y trsoain urce}. Then a model of the target domain is trained over a limited set of labeled target data {Xtaa nt,Y t arnet}, but instead of regularizing this Λtarget to be near zero (i.e. setting µ = 0), Λtarget is instead regularized towards the previously learned source values Λsource (by setting µ = Λsource, while σ2 remains 1) and thus minimizing ( 1Maximum Entropy models are special cases of CRFs that </context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. In Dekang Lin and Dekai Wu, editors, EMNLP 2004, pages 285–292. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<contexts>
<context position="11080" citStr="Chen and Rosenfeld, 1999" startWordPosition="1845" endWordPosition="1848">l Random Fields (CRF’s) (Lafferty et al., 2001), which are now one of the most preferred sequential models for many natural language processing tasks. The parametric form of the CRF for a sentence of length n is given as follows: (2) where Z(x) is the normalization term. CRF learns a model consisting of a set of weights Λ = {λ1...λF} over the features so as to maximize the conditional likelihood of the training data, p(Ytrain|Xtrain), given the model pΛ. 2.2 CRF with Gaussian priors To avoid overfitting the training data, these λ’s are often further constrained by the use of a Gaussian prior (Chen and Rosenfeld, 1999) with diagonal covariance, N(µ, σ2), which tries to maximize: argmax )2 Λ (log PA(YkIXk) I — Q� (Aj — µi 1 2 k=1 j2σj where β &gt; 0 is a parameter controlling the amount of regularization, and N is the number of sentences in the training set. 2.3 Source trained priors One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models 1 involves modifying the µ’s of this Gaussian prior. First a model of the source domain, Λsource, is learned by training on {Xsource train , Y trsoain urce}. Then a model of the target domain is trained over a limited set of labele</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. Chen and R. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Minorthird: Methods for identifying names and ontological relations in text using heuristics for inducing regularities from data.</title>
<date>2004</date>
<note>http://minorthird.sourceforge.net.</note>
<contexts>
<context position="5127" citStr="Cohen, 2004" startWordPosition="826" endWordPosition="827"> a feature is a branch and each period between slots represents another level, going from root to leaf as read left to right. Thus a subsection of the entire feature tree for the token Caldwell could be drawn as in Figure 1 (zoomed in on the section of the tree where the L.1.Professor feature resides). direction distance ... ... true false ... Figure 1: Graphical representation of a hierarchical feature tree for token Caldwell in example Sentence 1. Representing feature spaces with this kind of tree, besides often coinciding with the explicit language used by common natural language toolkits (Cohen, 2004), has the added benefit of allowing a model to easily back-off, or smooth, to decreasing levels of specificity. For example, the leaf level of the feature tree for our sample Sentence 1 tells us that the word Professor is important, with respect to labeling person names, when located one slot to the left of the current word being classified. This may be useful in the context of an academic corpus, but might be less useful in a medical domain where the word Professor occurs less often. Instead, we might want to learn the related feature L.1.Dr. In fact, it might be useful to generalize across m</context>
<context position="21196" citStr="Cohen, 2004" startWordPosition="3642" endWordPosition="3643">CSPACE (Kraut et al., 2004)]. Each corpus, depending on its genre, is labeled with one of two name-finding tasks: • protein names in biological abstracts • person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical prior’s ability to generalize across and incorporate information from a variety of domains, genres and tasks. In each case, each item (abstract, article or e-mail) was tokenized and each token was hand-labeled as either being part of a name (protein or person) or not, respectively. We used a standard natural language toolkit (Cohen, 2004) to compute tens of thousands of binary features on each of these tokens, encoding such information as capitalization patterns and contextual information from surrounding words. This toolkit produces features of the type described in §1.2 and thus was amenable to our hierarchical prior model. In particular, we chose to use the simplest default, out-of-the-box feature generator and purposefully did not use specifically engineered features, dictionaries, or other techniques commonly employed to boost performance on such tasks. The goal of our experiments was to see to what degree named entity re</context>
</contexts>
<marker>Cohen, 2004</marker>
<rawString>William W. Cohen. 2004. Minorthird: Methods for identifying names and ontological relations in text using heuristics for inducing regularities from data. http://minorthird.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>In Journal ofArtificial Intelligence Research</journal>
<volume>26</volume>
<pages>101--126</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. In Journal ofArtificial Intelligence Research 26, pages 101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Fisher</author>
<author>Stephen Soderland</author>
<author>Joseph McCarthy</author>
<author>Fangfang Feng</author>
<author>Wendy Lehnert</author>
</authors>
<title>Description of the UMass system as used for MUC-6.</title>
<date>1995</date>
<contexts>
<context position="20528" citStr="Fisher et al., 1995" startWordPosition="3531" endWordPosition="3534">mmary of data used in experiments Corpus Genre Task UTexas Bio Protein Yapex Bio Protein MUC6 News Person MUC7 News Person CSPACE E-mail Person 3 Investigation 3.1 Data, domains and tasks For our experiments, we have chosen five different corpora (summarized in Table 3). Although each corpus can be considered its own domain (due to variations in annotation standards, specific task, date of collection, etc), they can also be roughly grouped into three different genres. These are: abstracts from biological journals [UT (Bunescu et al., 2004), Yapex (Franz´en et al., 2002)]; news articles [MUC6 (Fisher et al., 1995), MUC7 (Borthwick et al., 1998)]; and personal e-mails [CSPACE (Kraut et al., 2004)]. Each corpus, depending on its genre, is labeled with one of two name-finding tasks: • protein names in biological abstracts • person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical prior’s ability to generalize across and incorporate information from a variety of domains, genres and tasks. In each case, each item (abstract, article or e-mail) was tokenized and each token was hand-labeled as either being part of a name (protein or person) or not, res</context>
</contexts>
<marker>Fisher, Soderland, McCarthy, Feng, Lehnert, 1995</marker>
<rawString>David Fisher, Stephen Soderland, Joseph McCarthy, Fangfang Feng, and Wendy Lehnert. 1995. Description of the UMass system as used for MUC-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristofer Franz´en</author>
</authors>
<title>Gunnar Eriksson, Fredrik Olsson, Lars Asker, Per Lidn, and Joakim C¨oster.</title>
<date>2002</date>
<booktitle>In International Journal ofMedical Informatics.</booktitle>
<marker>Franz´en, 2002</marker>
<rawString>Kristofer Franz´en, Gunnar Eriksson, Fredrik Olsson, Lars Asker, Per Lidn, and Joakim C¨oster. 2002. Protein names and how to find them. In International Journal ofMedical Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Grandvalet</author>
<author>Yoshua Bengio</author>
</authors>
<title>Semisupervised learning by entropy minimization.</title>
<date>2005</date>
<booktitle>In CAP,</booktitle>
<location>Nice, France.</location>
<contexts>
<context position="30025" citStr="Grandvalet and Bengio, 2005" startWordPosition="5114" endWordPosition="5118"> datasets and tasks. Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks. Daum´e allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further (Daum´e III, 2007)). Similarly, work on hier</context>
</contexts>
<marker>Grandvalet, Bengio, 2005</marker>
<rawString>Yves Grandvalet and Yoshua Bengio. 2005. Semisupervised learning by entropy minimization. In CAP, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Exploiting domain structure for named entity recognition.</title>
<date>2006</date>
<booktitle>In Human Language Technology Conference,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="29691" citStr="Jiang and Zhai, 2006" startWordPosition="5063" endWordPosition="5066">we have shown that hierarchical priors allow the user enough flexibility to customize their semantics to a specific problem, while providing enough structure to resist unintended negative effects when used inappropriately. Thus hierarchical priors seem a natural, effective and robust choice for transferring learning across NER datasets and tasks. Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these </context>
</contexts>
<marker>Jiang, Zhai, 2006</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2006. Exploiting domain structure for named entity recognition. In Human Language Technology Conference, pages 74 – 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kraut</author>
<author>S Fussell</author>
<author>F Lerch</author>
<author>J Espinosa</author>
</authors>
<title>Coordination in teams: evidence from a simulated management game.</title>
<date>2004</date>
<contexts>
<context position="20611" citStr="Kraut et al., 2004" startWordPosition="3544" endWordPosition="3547">tein MUC6 News Person MUC7 News Person CSPACE E-mail Person 3 Investigation 3.1 Data, domains and tasks For our experiments, we have chosen five different corpora (summarized in Table 3). Although each corpus can be considered its own domain (due to variations in annotation standards, specific task, date of collection, etc), they can also be roughly grouped into three different genres. These are: abstracts from biological journals [UT (Bunescu et al., 2004), Yapex (Franz´en et al., 2002)]; news articles [MUC6 (Fisher et al., 1995), MUC7 (Borthwick et al., 1998)]; and personal e-mails [CSPACE (Kraut et al., 2004)]. Each corpus, depending on its genre, is labeled with one of two name-finding tasks: • protein names in biological abstracts • person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical prior’s ability to generalize across and incorporate information from a variety of domains, genres and tasks. In each case, each item (abstract, article or e-mail) was tokenized and each token was hand-labeled as either being part of a name (protein or person) or not, respectively. We used a standard natural language toolkit (Cohen, 2004) to compute ten</context>
</contexts>
<marker>Kraut, Fussell, Lerch, Espinosa, 2004</marker>
<rawString>R. Kraut, S. Fussell, F. Lerch, and J. Espinosa. 2004. Coordination in teams: evidence from a simulated management game.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="10502" citStr="Lafferty et al., 2001" startWordPosition="1746" endWordPosition="1749">ld have produced had it only been allowed to see the training data (Raina et al., 2006). In the example from §1.1, our belief that capitalization is less strict in e-mails than in news articles could be encoded in a prior that biased the importance of the capitalization feature to be lower for e-mails than news articles. In the next section we address the problem of how to come up with a suitable prior for transfer learning across named entity recognition problems. 2 Models considered 2.1 Basic Conditional Random Fields In this work, we will base our work on Conditional Random Fields (CRF’s) (Lafferty et al., 2001), which are now one of the most preferred sequential models for many natural language processing tasks. The parametric form of the CRF for a sentence of length n is given as follows: (2) where Z(x) is the normalization term. CRF learns a model consisting of a set of weights Λ = {λ1...λF} over the features so as to maximize the conditional likelihood of the training data, p(Ytrain|Xtrain), given the model pΛ. 2.2 CRF with Gaussian priors To avoid overfitting the training data, these λ’s are often further constrained by the use of a Gaussian prior (Chen and Rosenfeld, 1999) with diagonal covaria</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-I Lee</author>
<author>V Chatalbashev</author>
<author>D Vickrey</author>
<author>D Koller</author>
</authors>
<title>Learning a meta-level prior for feature relevance from multiple related tasks.</title>
<date>2007</date>
<booktitle>In Proceedings ofInternational Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="30202" citStr="Lee et al., 2007" startWordPosition="5142" endWordPosition="5145">the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks. Daum´e allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further (Daum´e III, 2007)). Similarly, work on hierarchical penalization (Szafranski et al., 2007) in two-level trees tries to produce models that rely only on a relatively small number of groups of variable, as structured by th</context>
</contexts>
<marker>Lee, Chatalbashev, Vickrey, Koller, 2007</marker>
<rawString>S.-I. Lee, V. Chatalbashev, D. Vickrey, and D. Koller. 2007. Learning a meta-level prior for feature relevance from multiple related tasks. In Proceedings ofInternational Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Extracting personal names from email: Applying named entity recognition to informal text.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="6429" citStr="Minkov et al., 2005" startWordPosition="1033" endWordPosition="1036">ften important with respect LeftToken.* LeftToken.IsWord.* LeftToken.IsWord.IsTitle.* LeftToken.IsWord.IsTitle.equals.* LeftToken.IsWord.IsTitle.equals.mr Table 1: A few examples of the feature hierarchy to the named entity status of the current word. This is easily accomplished by backing up one level from a leaf in the tree structure to its parent, to represent a class of features such as L.1.*. It has been shown empirically that, while the significance ofparticular features might vary between domains and tasks, certain generalized classes of features retain their importance across domains (Minkov et al., 2005). By backing-off in this way, we can use the feature hierarchy as a prior for transferring beliefs about the significance of entire classes of features across domains and tasks. Some examples illustrating this idea are shown in table 1. 1.3 Transfer learning When only the type of data being examined is allowed to vary (from news articles to e-mails, for example), the problem is called domain adaptation (Daum´e III and Marcu, 2006). When the task being learned varies (say, from identifying person names to identifying protein names), the problem is called multi-task learning (Caruana, 1997). Bot</context>
</contexts>
<marker>Minkov, Wang, Cohen, 2005</marker>
<rawString>Einat Minkov, Richard C. Wang, and William W. Cohen. 2005. Extracting personal names from email: Applying named entity recognition to informal text. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Daphne Koller</author>
</authors>
<title>Transfer learning by constructing informative priors.</title>
<date>2006</date>
<booktitle>In ICML 22.</booktitle>
<contexts>
<context position="9967" citStr="Raina et al., 2006" startWordPosition="1655" endWordPosition="1658">work we group data collected in the same medium (e.g., all annotated e-mails or all annotated news articles) as belonging to the same genre. Although the specific boundary between domain and genre for a particular set of data is often subjective, it is nevertheless a useful distinction to draw. One common way of addressing the transfer learning problem is to use aprior which, in conjunction with a probabilistic model, allows one to specify a priori beliefs about a distribution, thus biasing the results a learning algorithm would have produced had it only been allowed to see the training data (Raina et al., 2006). In the example from §1.1, our belief that capitalization is less strict in e-mails than in news articles could be encoded in a prior that biased the importance of the capitalization feature to be lower for e-mails than news articles. In the next section we address the problem of how to come up with a suitable prior for transfer learning across named entity recognition problems. 2 Models considered 2.1 Basic Conditional Random Fields In this work, we will base our work on Conditional Random Fields (CRF’s) (Lafferty et al., 2001), which are now one of the most preferred sequential models for m</context>
</contexts>
<marker>Raina, Ng, Koller, 2006</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Daphne Koller. 2006. Transfer learning by constructing informative priors. In ICML 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Florian Steinke</author>
<author>Volker Blanz</author>
</authors>
<title>Object correspondence as a machine learning problem.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>776--783</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Sch¨olkopf, Steinke, Blanz, 2005</marker>
<rawString>Bernhard Sch¨olkopf, Florian Steinke, and Volker Blanz. 2005. Object correspondence as a machine learning problem. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 776– 783, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Composition of conditional random fields for transfer learning.</title>
<date>2005</date>
<booktitle>In HLT/EMLNLP.</booktitle>
<contexts>
<context position="9114" citStr="Sutton and McCallum, 2005" startWordPosition="1504" endWordPosition="1507">s, Dsource and Dtarget, from which data may be drawn. Given this notation we can then precisely state the transfer learning problem as trying to assign labels Y target test to test data Xtarget test drawn from Dtarget, given training data (Xsource Y source) drawn from Dsource. train , train In this paper we focus on two subproblems of transfer learning: • domain adaptation, where we assume Y (the set of possible labels) is the same for both Dsource and Dtarget, while Dsource and Dtarget themselves are allowed to vary between domains. • multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Sutton and McCallum, 2005; Zhang et al., 2005) in which the task (and label set) is allowed to vary from source to target. Domain adaptation can be further distinguished by the degree of relatedness between the source and target domains. For example, in this work we group data collected in the same medium (e.g., all annotated e-mails or all annotated news articles) as belonging to the same genre. Although the specific boundary between domain and genre for a particular set of data is often subjective, it is nevertheless a useful distinction to draw. One common way of addressing the transfer learning problem is to use a</context>
</contexts>
<marker>Sutton, McCallum, 2005</marker>
<rawString>Charles Sutton and Andrew McCallum. 2005. Composition of conditional random fields for transfer learning. In HLT/EMLNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Szafranski</author>
<author>Y Grandvalet</author>
<author>P MorizetMahoudeaux</author>
</authors>
<title>Hierarchical penalization.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 20.</booktitle>
<publisher>MIT press.</publisher>
<contexts>
<context position="30672" citStr="Szafranski et al., 2007" startWordPosition="5216" endWordPosition="5219">, and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks. Daum´e allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further (Daum´e III, 2007)). Similarly, work on hierarchical penalization (Szafranski et al., 2007) in two-level trees tries to produce models that rely only on a relatively small number of groups of variable, as structured by the tree, as opposed to transferring knowledge between branches themselves. Our future work is focused on designing an algorithm to optimally choose a smoothing regime for the learned feature trees so as to better exploit the similarities between domains while neutralizing their differences. Along these lines, we are working on methods to reduce the amount of labeled target domain data needed to tune the prior-based models, looking forward to semi-supervised and unsup</context>
</contexts>
<marker>Szafranski, Grandvalet, MorizetMahoudeaux, 2007</marker>
<rawString>M. Szafranski, Y. Grandvalet, and P. MorizetMahoudeaux. 2007. Hierarchical penalization. In Advances in Neural Information Processing Systems 20. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>M-F Wong</author>
<author>D Koller</author>
</authors>
<title>Learning on the test data: Leveraging ‘unseen’ features.</title>
<date>2003</date>
<booktitle>In Proc. Twentieth International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="30099" citStr="Taskar et al., 2003" startWordPosition="5127" endWordPosition="5130">lem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks. Daum´e allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further (Daum´e III, 2007)). Similarly, work on hierarchical penalization (Szafranski et al., 2007) in two-level trees tries t</context>
</contexts>
<marker>Taskar, Wong, Koller, 2003</marker>
<rawString>B. Taskar, M.-F. Wong, and D. Koller. 2003. Learning on the test data: Leveraging ‘unseen’ features. In Proc. Twentieth International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Thrun</author>
</authors>
<title>Is learning the n-th thing any easier than learning the first?</title>
<date>1996</date>
<booktitle>In NIPS,</booktitle>
<volume>8</volume>
<pages>640--646</pages>
<publisher>MIT.</publisher>
<contexts>
<context position="29528" citStr="Thrun, 1996" startWordPosition="5040" endWordPosition="5041">theoretical and empirical grounds, and have gone on to demonstrate their effectiveness in relation to other, competitive transfer methods. Specifically, we have shown that hierarchical priors allow the user enough flexibility to customize their semantics to a specific problem, while providing enough structure to resist unintended negative effects when used inappropriately. Thus hierarchical priors seem a natural, effective and robust choice for transferring learning across NER datasets and tasks. Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called</context>
</contexts>
<marker>Thrun, 1996</marker>
<rawString>Sebastian Thrun. 1996. Is learning the n-th thing any easier than learning the first? In NIPS, volume 8, pages 640–646. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhang</author>
<author>Z Ghahramani</author>
<author>Y Yang</author>
</authors>
<title>Learning multiple related tasks using latent independent component analysis.</title>
<date>2005</date>
<contexts>
<context position="9135" citStr="Zhang et al., 2005" startWordPosition="1508" endWordPosition="1511">m which data may be drawn. Given this notation we can then precisely state the transfer learning problem as trying to assign labels Y target test to test data Xtarget test drawn from Dtarget, given training data (Xsource Y source) drawn from Dsource. train , train In this paper we focus on two subproblems of transfer learning: • domain adaptation, where we assume Y (the set of possible labels) is the same for both Dsource and Dtarget, while Dsource and Dtarget themselves are allowed to vary between domains. • multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Sutton and McCallum, 2005; Zhang et al., 2005) in which the task (and label set) is allowed to vary from source to target. Domain adaptation can be further distinguished by the degree of relatedness between the source and target domains. For example, in this work we group data collected in the same medium (e.g., all annotated e-mails or all annotated news articles) as belonging to the same genre. Although the specific boundary between domain and genre for a particular set of data is often subjective, it is nevertheless a useful distinction to draw. One common way of addressing the transfer learning problem is to use aprior which, in conju</context>
</contexts>
<marker>Zhang, Ghahramani, Yang, 2005</marker>
<rawString>J. Zhang, Z. Ghahramani, and Y. Yang. 2005. Learning multiple related tasks using latent independent component analysis.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>