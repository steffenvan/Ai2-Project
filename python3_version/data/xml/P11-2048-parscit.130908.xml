<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001830">
<title confidence="0.9990785">
End-to-End Relation Extraction Using Distant Supervision
from External Semantic Repositories
</title>
<author confidence="0.980201">
Truc-Vien T. Nguyen and Alessandro Moschitti
</author>
<affiliation confidence="0.998594">
Department of Information Engineering and Computer Science
University of Trento
</affiliation>
<address confidence="0.696833">
38123 Povo (TN), Italy
</address>
<email confidence="0.999031">
{nguyenthi,moschitti}@disi.unitn.it
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99916475">
In this paper, we extend distant supervision
(DS) based on Wikipedia for Relation Extrac-
tion (RE) by considering (i) relations defined
in external repositories, e.g. YAGO, and (ii)
any subset of Wikipedia documents. We show
that training data constituted by sentences
containing pairs of named entities in target re-
lations is enough to produce reliable supervi-
sion. Our experiments with state-of-the-art re-
lation extraction models, trained on the above
data, show a meaningful F1 of 74.29% on a
manually annotated test set: this highly im-
proves the state-of-art in RE using DS. Addi-
tionally, our end-to-end experiments demon-
strated that our extractors can be applied to
any general text document.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999615142857143">
Relation Extraction (RE) from text as defined in
ACE (Doddington et al., 2004) concerns the extrac-
tion of relationships between two entities. This is
typically carried out by applying supervised learn-
ing, e.g. (Zelenko et al., 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005) by using a hand-
labeled corpus. Although, the resulting models are
far more accurate than unsupervised approaches,
they suffer from the following drawbacks: (i) they
require labeled data, which is usually costly to pro-
duce; (ii) they are typically domain-dependent as
different domains involve different relations; and
(iii), even in case the relations do not change, they
result biased toward the text feature distributions of
the training domain.
The drawbacks above would be alleviated if data
from several different domains and relationships
were available. A form of weakly supervision,
specifically named distant supervision (DS) when
applied to Wikipedia, e.g. (Banko et al., 2007; Mintz
et al., 2009; Hoffmann et al., 2010) has been recently
developed to meet the requirement above. The main
idea is to exploit (i) relation repositories, e.g. the
Infobox, x, of Wikipedia to define a set of relation
types RT (x) and (ii) the text in the page associated
with x to produce the training sentences, which are
supposed to express instances of RT(x).
Previous work has shown that selecting the sen-
tences containing the entities targeted by a given re-
lation is enough accurate (Banko et al., 2007; Mintz
et al., 2009) to provide reliable training data. How-
ever, only (Hoffmann et al., 2010) used DS to de-
fine extractors that are supposed to detect all the re-
lation instances from a given input text. This is a
harder test for the applicability of DS but, at the
same time, the resulting extractor is very valuable:
it can find rare relation instances that might be ex-
pressed in only one document. For example, the re-
lation President(Barrack Obama, United States) can
be extracted from thousands of documents thus there
is a large chance of acquiring it. In contrast, Pres-
ident(Eneko Agirre, SIGLEX) is probably expressed
in very few documents, increasing the complexity
for obtaining it.
In this paper, we extend DS by (i) considering
relations from semantic repositories different from
Wikipedia, i.e. YAGO, and (2) using training in-
stances derived from any Wikipedia document. This
allows for (i) potentially obtaining training data
</bodyText>
<page confidence="0.954717">
277
</page>
<note confidence="0.579994">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 277–282,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999937785714286">
for many more relation types, defined in different
sources; (ii) meaningfully enlarging the size of the
DS data since the relation examples can be extracted
from any Wikipedia document 1.
Additionally, by following previous work, we
define state-of-the-art RE models based on kernel
methods (KM) applied to syntactic/semantic struc-
tures. We use tree and sequence kernels that can
exploit structural information and interdependencies
among labels. Experiments show that our models
are flexible and robust to Web documents as we
achieve the interesting F1 of 74.29% on 52 YAGO
relations. This is even more appreciable if we ap-
proximately compare with the previous result on RE
using DS, i.e. 61% (Hoffmann et al., 2010). Al-
though the experiment setting is different from ours,
the improvement of about 13 absolute percent points
demonstrates the quality of our model.
Finally, we also provide a system for extracting
relations from any text. This required the definition
of a robust Named Entity Recognizer (NER), which
is also trained on weakly supervised Wikipedia data.
Consequently, our end-to-end RE system is appli-
cable to any document. This is another major im-
provement on previous work. The satisfactory RE
F1 of 67% for 52 Wikipedia relations suggests that
our model is also successfully applicable in real sce-
narios.
</bodyText>
<subsectionHeader confidence="0.60865">
1.1 Related Work
</subsectionHeader>
<bodyText confidence="0.9966644">
RE generally relates to the extraction of relational
facts, or world knowledge from the Web (Yates,
2009). To identify semantic relations using ma-
chine learning, three learning settings have been ap-
plied, namely supervised methods, e.g. (Zelenko
et al., 2002; Culotta and Sorensen, 2004; Kamb-
hatla, 2004), semi supervised methods, e.g. (Brin,
1998; Agichtein and Gravano, 2000), and unsuper-
vised method, e.g. (Hasegawa et al., 2004; Banko
et al., 2007). Work on supervised Relation Extrac-
tion has mostly employed kernel-based approaches,
e.g. (Zelenko et al., 2002; Culotta and Sorensen,
2004; Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005; Zhang et al., 2005; Bunescu, 2007;
Nguyen et al., 2009; Zhang et al., 2006). However,
</bodyText>
<footnote confidence="0.6958995">
1Previous work assumes the page related to the Infobox as
the only source for the training data.
</footnote>
<construct confidence="0.54157">
Algorithm 2.1: ACQUIRE LABELED DATA()
</construct>
<equation confidence="0.900152666666667">
DS=O
Y AGO(R) : Instances of Relation R
for each (Wikipedia article: W) E Freebase
S +— set of sentences from W
for each s E S
8 +— set of entities from s
for each E1 E 8 and E2 E 8 and
R E YAGO
if R(E1, E2) E YAGO(R)
then DS +— DS U is, R+}
else DS +— DS U is, R−}
return (DS)
</equation>
<bodyText confidence="0.9994558">
such approaches can be applied to few relation types
thus distant supervised learning (Mintz et al., 2009)
was introduced to tackle such problem. Another so-
lution proposed in (Riedel et al., 2010) was to adapt
models trained in one domain to other text domains.
</bodyText>
<sectionHeader confidence="0.988008" genericHeader="introduction">
2 Resources and Dataset Creation
</sectionHeader>
<bodyText confidence="0.999988222222222">
In this section, we describe the resources for the cre-
ation of an annotated dataset based on distant super-
vision. We use YAGO, a large knowledge base of
entities and relations, and Freebase, a collection of
Wikipedia articles. Our procedure uses entities and
facts from YAGO to provide relation instances. For
each pair of entities that appears in some YAGO re-
lation, we retrieve all the sentences of the Freebase
documents that contain such entities.
</bodyText>
<subsectionHeader confidence="0.846374">
2.1 YAGO
</subsectionHeader>
<bodyText confidence="0.999739529411765">
YAGO (Suchanek et al., 2007) is a huge seman-
tic knowledge base derived from WordNet and
Wikipedia. It comprises more than 2 million entities
(like persons, organizations, cities, etc.) and 20 mil-
lion facts connecting these entities. These include
the taxonomic Is-A hierarchy as well as semantic re-
lations between entities.
We use the YAGO version of 2008-w40-2 with a
manually confirmed accuracy of 95% for 99 rela-
tions. However, some of them are (a) trivial, e.g.
familyNameOf; (b) numerical attributes that change
over time, e.g. hasPopulation; (c) symmetric, e.g.
hasPredecessor; (d) used only for data management,
e.g. describes or foundIn. Therefore, we removed
those irrelevant relations and obtained 1,489,156 in-
stances of 52 relation types to be used with our DS
approach.
</bodyText>
<figure confidence="0.995888166666667">
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
do
⎧
⎨⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎩
do
⎧
⎨
⎩
do
</figure>
<page confidence="0.953311">
278
</page>
<subsectionHeader confidence="0.988106">
2.2 Freebase
</subsectionHeader>
<bodyText confidence="0.996720875">
To access to Wikipedia documents, we used Free-
base (March 27, 2010 (Metaweb Technologies,
2010)), which is a dump of the full text of all
Wikipedia articles. For our experiments, we used
100,000 articles. Out of them, only 28,074 articles
contain at least one relation for a total of 68,429 of
relation instances. These connect 744,060 entities,
97,828 dates and 203,981 numerical attributes.
</bodyText>
<subsectionHeader confidence="0.702013">
Temporal and Numerical Expression
</subsectionHeader>
<bodyText confidence="0.999965636363636">
Wikipedia articles are marked with entities like Per-
son or Organization but not with dates or numeri-
cal attributes. This prevents to extract interesting
relations between entities and dates, e.g. John F.
Kennedy was born on May 29, 1917 or between en-
tities and numerical attributes, e.g. The novel Gone
with the wind has 1037 pages. Thus we designed
18 regular expressions to extract dates and other 25
to extract numerical attributes, which range from in-
teger number to ordinal number, percentage, mone-
tary, speed, height, weight, area, time, and ISBN.
</bodyText>
<subsectionHeader confidence="0.998738">
2.3 Distant Supervision and generalization
</subsectionHeader>
<bodyText confidence="0.999978047619048">
Distant supervision (DS) for RE is based on the
following assumption: (i) a sentence is connected
in some way to a database of relations and (ii)
such sentence contains the pair of entities partic-
ipating in a target relation; (iii) then it is likely
that such sentence expresses the relation. In tra-
ditional DS the point (i) is implemented by the
Infobox, which is connected to the sentences by
a proximity relation (same page of the sentence).
In our extended DS, we relax (i) by allowing
for the use of an external DB of relations such
as YAGO and any document of Freebase (a col-
lection of Wikipedia documents). The alignment
between YAGO and Freebase is implemented by
the Wikipedia page link: for example the link
http://en.wikipedia.org/wiki/James Cameron refers
to the entity James Cameron.
We use an efficient procedure formally described
in Alg. 2.1: for each Wikipedia article in Free-
base, we scan all of its NEs. Then, for each pair
of entities2 seen in the sentence, we query YAGO to
</bodyText>
<footnote confidence="0.856482">
2Our algorithm is robust to the lack of knowledge about the
existence of any relation between two entities. If the relation
</footnote>
<bodyText confidence="0.997138625">
retrieve the relation instance connecting these enti-
ties. Note that a simplified version of our approach
is the following: for any YAGO relation instance,
scan all the sentences of all Wikipedia articles to test
point (ii). Unfortunately, this procedure is impossi-
ble in practice due to millions of relation instances
in YAGO and millions of Wikipedia articles in Free-
base, i.e. an order of magnitude of 1014 iterations3.
</bodyText>
<sectionHeader confidence="0.994199" genericHeader="method">
3 Distant Supervised Learning with
Kernels
</sectionHeader>
<bodyText confidence="0.999958625">
We model relation extraction (RE) using state-of-
the-art classifiers based on kernel methods. The
main idea is that syntactic/semantic structures are
used to represent relation instances. We followed the
model in (Nguyen et al., 2009) that has shown sig-
nificant improvement on the state-of-the-art. This
combines a syntactic tree kernel and a polynomial
kernel over feature extracted from the entities:
</bodyText>
<equation confidence="0.999069">
CK1 = α · KP + (1 − α) · TK (1)
</equation>
<bodyText confidence="0.999817333333333">
where α is a coefficient to give more or less impact
to the polynomial kernel, KP, and TK is the syntac-
tic tree kernel (Collins and Duffy, 2001). The best
model combines the advantages of the two parsing
paradigms by adding the kernel above with six se-
quence kernels (described in (Nguyen et al., 2009)).
</bodyText>
<equation confidence="0.992239">
C5K = α·KP +(1−α)·(TK + � 5Ki) (2)
i=1,..,6
</equation>
<bodyText confidence="0.999605857142857">
Such kernels cannot be applied to Wikipedia doc-
uments as the entity category, e.g. Person or Orga-
nization, is in general missing. Thus, we adapted
them by simply removing the category label in the
nodes of the trees and in the sequences. This data
transformation corresponds to different kernels (see
(Cristianini and Shawe-Taylor, 2000)).
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999745666666667">
We carried out test to demonstrate that our DS ap-
proach produces reliable and practically usable re-
lation extractors. For this purpose, we test them on
</bodyText>
<footnote confidence="0.998755333333333">
instance is not in YAGO, it is simply assumed as a negative
instance even if such relation is present in other DBs.
3Assuming 100 sentences for each article.
</footnote>
<page confidence="0.998454">
279
</page>
<bodyText confidence="0.998526">
DS data by also carrying out end-to-end RE evalua-
tion. This requires to experiment with a state-of-the-
art Named Entity Recognizer trained on Wikipedia
entities.
</bodyText>
<table confidence="0.9997345">
Class Precision Recall F-measure
bornOnDate 97.99 95.22 96.58
created 92.00 68.56 78.57
dealsWith 92.30 73.47 81.82
directed 85.19 51.11 63.89
hasCapital 93.69 61.54 74.29
isAffiliatedTo 86.32 71.30 78.10
locatedIn 87.85 78.33 82.82
wrote 82.61 42.22 55.88
Overall 91.42 62.57 74.29
</table>
<tableCaption confidence="0.9944115">
Table 1: Performance of 8 out of 52 individual relations
with overall F1.
</tableCaption>
<subsectionHeader confidence="0.98857">
4.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.9959957">
We used the DS dataset generated from YAGO and
Wikipedia articles, as described in the algorithm
(Alg. 2.1). The candidate relations are generated
by iterating all pairs of entity mentions in the same
sentence. Relation detection is formulated as a mul-
ticlass classification problem. The One vs. Rest
strategy is employed by selecting the instance with
largest margin as the final answer. We carried out
5-fold cross-validation with the tree kernel toolkit4
(Moschitti, 2004; Moschitti, 2008).
</bodyText>
<subsectionHeader confidence="0.915304">
4.2 Results on Wikipedia RE
</subsectionHeader>
<bodyText confidence="0.999395">
We created a test set by sampling 200 articles from
Freebase (these articles are not used for training).
An expert annotator, for each sentence, labeled all
possible pairs of entities with one of the 52 rela-
tions from YAGO, where the entities were already
marked. This process resulted in 2,601 relation in-
stances.
Table 1 shows the performance of individual clas-
sifiers as well as the overall Micro-average F1 for
our adapted C5K: we note that it reaches an F1-
score of 74.29%. This can be compared with the
Micro-average F1 of CKI, i.e. 71.21%. The lower
result suggests that the combination of dependency
and constituent syntactic structures is very impor-
tant: +3.08 absolute percent points on CKI, which
only uses constituency trees.
</bodyText>
<footnote confidence="0.996166">
4http://disi.unitn.it/ moschitt/Tree-Kernel.htm
</footnote>
<table confidence="0.994332">
Entity Detection 68.84 64.56 66.63
End-to-End RE 82.16 56.57 67.00
</table>
<tableCaption confidence="0.9865135">
Table 2: Entity Detection and End-to-end Relation Ex-
traction.
</tableCaption>
<subsectionHeader confidence="0.997407">
4.3 End-to-end Relation Extraction
</subsectionHeader>
<bodyText confidence="0.99984385">
Previous work in RE uses gold entities available in
the annotated corpus (i.e. ACE) but in real appli-
cations these are not available. Therefore, we per-
form experiments with automatic entities. For their
extraction, we follow the feature design in (Nguyen
et al., 2010), using CRF++ 5 with unigram/features
and Freebase as learning source. Dates and numer-
ical attributes required a different treatment, so we
use the patterns described in Section 2.3. The results
reported in Table 2 are rather lower than in standard
NE recognition. This is due to the high complexity
of predicting the boundaries of thousands of differ-
ent categories in YAGO.
Our end-to-end RE system can be applied to any
text fragment so we could experiment with it and
any Wikipedia document. This allowed us to carry
out an accurate evaluation. The results are shown in
Table 2. We note that, without gold entities, RE from
Wikipedia still achieves a satisfactory performance
of 67.00% F1.
</bodyText>
<sectionHeader confidence="0.996462" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9645294">
This paper proposes two main contributions to Re-
lation Extraction: (i) a new approach to distant su-
pervision (DS) to create training data using relations
defined in different sources, i.e. YAGO, and poten-
tially using any Wikipedia document; and (ii) end-
to-end systems applicable both to Wikipedia pages
as well as to any natural language text.
The results show:
1. A high F1 of 74.29% on extracting 52 YAGO
relations from any Wikipedia document (not
only from Infobox related pages). This re-
sult improves on previous work by 13.29 abso-
lute percent points (approximated comparison).
This is a rough approximation since on one
hand, (Hoffmann et al., 2010) experimented
</bodyText>
<footnote confidence="0.955548">
5http://crfpp.sourceforge.net
</footnote>
<table confidence="0.466512">
Class Precision Recall F-measure
</table>
<page confidence="0.993151">
280
</page>
<bodyText confidence="0.891381290322581">
with 5,025 relations, which indicate that our re-
sults based on 52 relations cannot be compared
with it (i.e. our multi-classifier has two orders
of magnitude less of categories). On the other
hand, the only experiment that can give a re-
alistic measurement is the one on hand-labeled
test set (testing on data automatically labelled
by DS does not provide a realistic outcome).
The size of such test set is comparable with
ours, i.e. 100 documents vs. our set of 200
documents. Although, we do not know how
many types of relations were involved in the
test of (Hoffmann et al., 2010), it is clear that
only a small subset of the 5000 relations could
have been measured. Also, we have to consider
that, in (Hoffmann et al., 2010), only one rela-
tion extractor is supposed to be learnt from one
article (by using Infobox) whereas we can po-
tentially extract several relations even from the
same sentence.
2. The importance of using both dependency and
constituent structures (+3.08% when adding
dependency information to RE based on con-
stituent trees).
3. Our end-to-end system is useful for real appli-
cations as it shows a meaningful accuracy, i.e.
67% on 52 relations.
For this reason, we decided to make available the
DS dataset, the manually annotated test set and the
computational data (tree and sequential structures
with labels).
</bodyText>
<sectionHeader confidence="0.997598" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996397136363636">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries, pages 85–94.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of IJCAI, pages 2670–2676.
Sergey Brin. 1998. Extracting patterns and relations
from world wide web. In Proceedings of WebDB
Workshop at 6th International Conference on Extend-
ing Database Technology, pages 172–183.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings ofHLT-EMNLP, pages 724–731, Vancou-
ver, British Columbia, Canada, October.
Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems (NIPS’2001), pages
625–632.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press, Cambridge, United Kingdom.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423–429, Barcelona, Spain, July.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) programtasks, data, and evaluation. In Proceed-
ings of LREC, pages 837–840, Barcelona, Spain.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of ACL, pages
415–422, Barcelona, Spain, July.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of ACL, pages 286–295, Uppsala, Sweden,
July.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for information extraction. In The Companion Volume
to the Proceedings ofACL, pages 178–181, Barcelona,
Spain, July.
Metaweb Technologies. 2010. Freebase wikipedia ex-
traction (wex), March.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-AFNLP,
pages 1003–1011, Suntec, Singapore, August.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL, pages 335–342, Barcelona, Spain, July.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceedings of CIKM, pages 253–262, New York, NY,
USA. ACM.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378–1387, Singapore, August.
</reference>
<page confidence="0.962562">
281
</page>
<reference confidence="0.999768433333333">
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2010. Kernel-based re-ranking for
named-entity extraction. In Proceedings of COLING,
pages 901–909, China, August.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases, volume 6323 of Lecture
Notes in Computer Science, pages 148–163. Springer
Berlin / Heidelberg.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago - a core of semantic knowl-
edge. In 16th international World Wide Web confer-
ence, pages 697–706.
Alexander Yates. 2009. Extracting world knowledge
from the web. IEEE Computer, 42(6):94–97, June.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181–201.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP’2005, Lecture Notes in Computer Science (LNCS
3651), pages 378–389, Jeju Island, South Korea.
Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou.
2006. A composite kernel to extract relations between
entities with both flat and structured features. In Pro-
ceedings of COLING-ACL 2006, pages 825–832.
</reference>
<page confidence="0.997442">
282
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728365">
<title confidence="0.998896">End-to-End Relation Extraction Using Distant from External Semantic Repositories</title>
<author confidence="0.999961">T Nguyen</author>
<affiliation confidence="0.999436">Department of Information Engineering and Computer University of</affiliation>
<address confidence="0.773255">38123 Povo (TN),</address>
<abstract confidence="0.996019176470588">In this paper, we extend distant supervision (DS) based on Wikipedia for Relation Extraction (RE) by considering (i) relations defined in external repositories, e.g. YAGO, and (ii) any subset of Wikipedia documents. We show that training data constituted by sentences containing pairs of named entities in target relations is enough to produce reliable supervision. Our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th ACM International Conference on Digital Libraries,</booktitle>
<pages>85--94</pages>
<contexts>
<context position="5345" citStr="Agichtein and Gravano, 2000" startWordPosition="826" endWordPosition="829">d-to-end RE system is applicable to any document. This is another major improvement on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real scenarios. 1.1 Related Work RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences fro</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the 5th ACM International Conference on Digital Libraries, pages 85–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="1971" citStr="Banko et al., 2007" startWordPosition="288" endWordPosition="291">r more accurate than unsupervised approaches, they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased toward the text feature distributions of the training domain. The drawbacks above would be alleviated if data from several different domains and relationships were available. A form of weakly supervision, specifically named distant supervision (DS) when applied to Wikipedia, e.g. (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010) has been recently developed to meet the requirement above. The main idea is to exploit (i) relation repositories, e.g. the Infobox, x, of Wikipedia to define a set of relation types RT (x) and (ii) the text in the page associated with x to produce the training sentences, which are supposed to express instances of RT(x). Previous work has shown that selecting the sentences containing the entities targeted by a given relation is enough accurate (Banko et al., 2007; Mintz et al., 2009) to provide reliable training data. However, only (Hoffmann et al., </context>
<context position="5420" citStr="Banko et al., 2007" startWordPosition="839" endWordPosition="842">on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real scenarios. 1.1 Related Work RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for each E1 E 8 and E2 E 8 a</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of IJCAI, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from world wide web.</title>
<date>1998</date>
<booktitle>In Proceedings of WebDB Workshop at 6th International Conference on Extending Database Technology,</booktitle>
<pages>172--183</pages>
<contexts>
<context position="5315" citStr="Brin, 1998" startWordPosition="824" endWordPosition="825">ntly, our end-to-end RE system is applicable to any document. This is another major improvement on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real scenarios. 1.1 Related Work RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Free</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from world wide web. In Proceedings of WebDB Workshop at 6th International Conference on Extending Database Technology, pages 172–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT-EMNLP,</booktitle>
<pages>724--731</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1284" citStr="Bunescu and Mooney, 2005" startWordPosition="185" endWordPosition="188">eriments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document. 1 Introduction Relation Extraction (RE) from text as defined in ACE (Doddington et al., 2004) concerns the extraction of relationships between two entities. This is typically carried out by applying supervised learning, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) by using a handlabeled corpus. Although, the resulting models are far more accurate than unsupervised approaches, they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased toward the text feature distributions of the training domain. The drawbacks above would be alleviated if data from several different domains and relationships were available. A form of weakly supervision, specific</context>
<context position="5614" citStr="Bunescu and Mooney, 2005" startWordPosition="867" endWordPosition="870">o the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for each E1 E 8 and E2 E 8 and R E YAGO if R(E1, E2) E YAGO(R) then DS +— DS U is, R+} else DS +— DS U is, R−} return (DS) such approaches can be applied to few relation types thus distant supervised learning (Mintz et al.</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings ofHLT-EMNLP, pages 724–731, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5649" citStr="Bunescu, 2007" startWordPosition="875" endWordPosition="876"> knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for each E1 E 8 and E2 E 8 and R E YAGO if R(E1, E2) E YAGO(R) then DS +— DS U is, R+} else DS +— DS U is, R−} return (DS) such approaches can be applied to few relation types thus distant supervised learning (Mintz et al., 2009) was introduced to tackle su</context>
</contexts>
<marker>Bunescu, 2007</marker>
<rawString>Razvan C. Bunescu. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS’2001),</booktitle>
<pages>625--632</pages>
<contexts>
<context position="10913" citStr="Collins and Duffy, 2001" startWordPosition="1767" endWordPosition="1770">3. 3 Distant Supervised Learning with Kernels We model relation extraction (RE) using state-ofthe-art classifiers based on kernel methods. The main idea is that syntactic/semantic structures are used to represent relation instances. We followed the model in (Nguyen et al., 2009) that has shown significant improvement on the state-of-the-art. This combines a syntactic tree kernel and a polynomial kernel over feature extracted from the entities: CK1 = α · KP + (1 − α) · TK (1) where α is a coefficient to give more or less impact to the polynomial kernel, KP, and TK is the syntactic tree kernel (Collins and Duffy, 2001). The best model combines the advantages of the two parsing paradigms by adding the kernel above with six sequence kernels (described in (Nguyen et al., 2009)). C5K = α·KP +(1−α)·(TK + � 5Ki) (2) i=1,..,6 Such kernels cannot be applied to Wikipedia documents as the entity category, e.g. Person or Organization, is in general missing. Thus, we adapted them by simply removing the category label in the nodes of the trees and in the sequences. This data transformation corresponds to different kernels (see (Cristianini and Shawe-Taylor, 2000)). 4 Experiments We carried out test to demonstrate that o</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of Neural Information Processing Systems (NIPS’2001), pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, United Kingdom.</location>
<contexts>
<context position="11455" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="1857" endWordPosition="1860">t to the polynomial kernel, KP, and TK is the syntactic tree kernel (Collins and Duffy, 2001). The best model combines the advantages of the two parsing paradigms by adding the kernel above with six sequence kernels (described in (Nguyen et al., 2009)). C5K = α·KP +(1−α)·(TK + � 5Ki) (2) i=1,..,6 Such kernels cannot be applied to Wikipedia documents as the entity category, e.g. Person or Organization, is in general missing. Thus, we adapted them by simply removing the category label in the nodes of the trees and in the sequences. This data transformation corresponds to different kernels (see (Cristianini and Shawe-Taylor, 2000)). 4 Experiments We carried out test to demonstrate that our DS approach produces reliable and practically usable relation extractors. For this purpose, we test them on instance is not in YAGO, it is simply assumed as a negative instance even if such relation is present in other DBs. 3Assuming 100 sentences for each article. 279 DS data by also carrying out end-to-end RE evaluation. This requires to experiment with a state-of-theart Named Entity Recognizer trained on Wikipedia entities. Class Precision Recall F-measure bornOnDate 97.99 95.22 96.58 created 92.00 68.56 78.57 dealsWith 92.30 73.4</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, Cambridge, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--429</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1257" citStr="Culotta and Sorensen, 2004" startWordPosition="181" endWordPosition="184">eliable supervision. Our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document. 1 Introduction Relation Extraction (RE) from text as defined in ACE (Doddington et al., 2004) concerns the extraction of relationships between two entities. This is typically carried out by applying supervised learning, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) by using a handlabeled corpus. Although, the resulting models are far more accurate than unsupervised approaches, they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased toward the text feature distributions of the training domain. The drawbacks above would be alleviated if data from several different domains and relationships were available. A form of w</context>
<context position="5254" citStr="Culotta and Sorensen, 2004" startWordPosition="813" endWordPosition="816">er (NER), which is also trained on weakly supervised Wikipedia data. Consequently, our end-to-end RE system is applicable to any document. This is another major improvement on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real scenarios. 1.1 Related Work RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : I</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL, pages 423–429, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The automatic content extraction (ace) programtasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>837--840</pages>
<location>Barcelona,</location>
<contexts>
<context position="1076" citStr="Doddington et al., 2004" startWordPosition="153" endWordPosition="156">YAGO, and (ii) any subset of Wikipedia documents. We show that training data constituted by sentences containing pairs of named entities in target relations is enough to produce reliable supervision. Our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document. 1 Introduction Relation Extraction (RE) from text as defined in ACE (Doddington et al., 2004) concerns the extraction of relationships between two entities. This is typically carried out by applying supervised learning, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) by using a handlabeled corpus. Although, the resulting models are far more accurate than unsupervised approaches, they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased towar</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ace) programtasks, data, and evaluation. In Proceedings of LREC, pages 837–840, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>415--422</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5399" citStr="Hasegawa et al., 2004" startWordPosition="835" endWordPosition="838">ther major improvement on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real scenarios. 1.1 Related Work RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for eac</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from large corpora. In Proceedings of ACL, pages 415–422, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>286--295</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2015" citStr="Hoffmann et al., 2010" startWordPosition="296" endWordPosition="299">ches, they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased toward the text feature distributions of the training domain. The drawbacks above would be alleviated if data from several different domains and relationships were available. A form of weakly supervision, specifically named distant supervision (DS) when applied to Wikipedia, e.g. (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010) has been recently developed to meet the requirement above. The main idea is to exploit (i) relation repositories, e.g. the Infobox, x, of Wikipedia to define a set of relation types RT (x) and (ii) the text in the page associated with x to produce the training sentences, which are supposed to express instances of RT(x). Previous work has shown that selecting the sentences containing the entities targeted by a given relation is enough accurate (Banko et al., 2007; Mintz et al., 2009) to provide reliable training data. However, only (Hoffmann et al., 2010) used DS to define extractors that are </context>
<context position="4343" citStr="Hoffmann et al., 2010" startWordPosition="669" endWordPosition="672">e of the DS data since the relation examples can be extracted from any Wikipedia document 1. Additionally, by following previous work, we define state-of-the-art RE models based on kernel methods (KM) applied to syntactic/semantic structures. We use tree and sequence kernels that can exploit structural information and interdependencies among labels. Experiments show that our models are flexible and robust to Web documents as we achieve the interesting F1 of 74.29% on 52 YAGO relations. This is even more appreciable if we approximately compare with the previous result on RE using DS, i.e. 61% (Hoffmann et al., 2010). Although the experiment setting is different from ours, the improvement of about 13 absolute percent points demonstrates the quality of our model. Finally, we also provide a system for extracting relations from any text. This required the definition of a robust Named Entity Recognizer (NER), which is also trained on weakly supervised Wikipedia data. Consequently, our end-to-end RE system is applicable to any document. This is another major improvement on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real sce</context>
<context position="15430" citStr="Hoffmann et al., 2010" startWordPosition="2493" endWordPosition="2496">tributions to Relation Extraction: (i) a new approach to distant supervision (DS) to create training data using relations defined in different sources, i.e. YAGO, and potentially using any Wikipedia document; and (ii) endto-end systems applicable both to Wikipedia pages as well as to any natural language text. The results show: 1. A high F1 of 74.29% on extracting 52 YAGO relations from any Wikipedia document (not only from Infobox related pages). This result improves on previous work by 13.29 absolute percent points (approximated comparison). This is a rough approximation since on one hand, (Hoffmann et al., 2010) experimented 5http://crfpp.sourceforge.net Class Precision Recall F-measure 280 with 5,025 relations, which indicate that our results based on 52 relations cannot be compared with it (i.e. our multi-classifier has two orders of magnitude less of categories). On the other hand, the only experiment that can give a realistic measurement is the one on hand-labeled test set (testing on data automatically labelled by DS does not provide a realistic outcome). The size of such test set is comparable with ours, i.e. 100 documents vs. our set of 200 documents. Although, we do not know how many types of</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of ACL, pages 286–295, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction.</title>
<date>2004</date>
<booktitle>In The Companion Volume to the Proceedings ofACL,</booktitle>
<pages>178--181</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5272" citStr="Kambhatla, 2004" startWordPosition="817" endWordPosition="819">ned on weakly supervised Wikipedia data. Consequently, our end-to-end RE system is applicable to any document. This is another major improvement on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real scenarios. 1.1 Related Work RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relati</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction. In The Companion Volume to the Proceedings ofACL, pages 178–181, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Metaweb Technologies</author>
</authors>
<title>Freebase wikipedia extraction (wex),</title>
<date>2010</date>
<contexts>
<context position="7822" citStr="Technologies, 2010" startWordPosition="1255" endWordPosition="1256">he YAGO version of 2008-w40-2 with a manually confirmed accuracy of 95% for 99 relations. However, some of them are (a) trivial, e.g. familyNameOf; (b) numerical attributes that change over time, e.g. hasPopulation; (c) symmetric, e.g. hasPredecessor; (d) used only for data management, e.g. describes or foundIn. Therefore, we removed those irrelevant relations and obtained 1,489,156 instances of 52 relation types to be used with our DS approach. ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪⎪ ⎪ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ do ⎧ ⎨⎪⎪⎪⎪ ⎪ ⎪⎪⎪⎪⎪⎩ do ⎧ ⎨ ⎩ do 278 2.2 Freebase To access to Wikipedia documents, we used Freebase (March 27, 2010 (Metaweb Technologies, 2010)), which is a dump of the full text of all Wikipedia articles. For our experiments, we used 100,000 articles. Out of them, only 28,074 articles contain at least one relation for a total of 68,429 of relation instances. These connect 744,060 entities, 97,828 dates and 203,981 numerical attributes. Temporal and Numerical Expression Wikipedia articles are marked with entities like Person or Organization but not with dates or numerical attributes. This prevents to extract interesting relations between entities and dates, e.g. John F. Kennedy was born on May 29, 1917 or between entities and numeric</context>
</contexts>
<marker>Technologies, 2010</marker>
<rawString>Metaweb Technologies. 2010. Freebase wikipedia extraction (wex), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-AFNLP,</booktitle>
<pages>1003--1011</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1991" citStr="Mintz et al., 2009" startWordPosition="292" endWordPosition="295"> unsupervised approaches, they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased toward the text feature distributions of the training domain. The drawbacks above would be alleviated if data from several different domains and relationships were available. A form of weakly supervision, specifically named distant supervision (DS) when applied to Wikipedia, e.g. (Banko et al., 2007; Mintz et al., 2009; Hoffmann et al., 2010) has been recently developed to meet the requirement above. The main idea is to exploit (i) relation repositories, e.g. the Infobox, x, of Wikipedia to define a set of relation types RT (x) and (ii) the text in the page associated with x to produce the training sentences, which are supposed to express instances of RT(x). Previous work has shown that selecting the sentences containing the entities targeted by a given relation is enough accurate (Banko et al., 2007; Mintz et al., 2009) to provide reliable training data. However, only (Hoffmann et al., 2010) used DS to def</context>
<context position="6221" citStr="Mintz et al., 2009" startWordPosition="989" endWordPosition="992">Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for each E1 E 8 and E2 E 8 and R E YAGO if R(E1, E2) E YAGO(R) then DS +— DS U is, R+} else DS +— DS U is, R−} return (DS) such approaches can be applied to few relation types thus distant supervised learning (Mintz et al., 2009) was introduced to tackle such problem. Another solution proposed in (Riedel et al., 2010) was to adapt models trained in one domain to other text domains. 2 Resources and Dataset Creation In this section, we describe the resources for the creation of an annotated dataset based on distant supervision. We use YAGO, a large knowledge base of entities and relations, and Freebase, a collection of Wikipedia articles. Our procedure uses entities and facts from YAGO to provide relation instances. For each pair of entities that appears in some YAGO relation, we retrieve all the sentences of the Freeba</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of ACL-AFNLP, pages 1003–1011, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow statistic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>335--342</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="12803" citStr="Moschitti, 2004" startWordPosition="2071" endWordPosition="2072">.61 42.22 55.88 Overall 91.42 62.57 74.29 Table 1: Performance of 8 out of 52 individual relations with overall F1. 4.1 Experimental setting We used the DS dataset generated from YAGO and Wikipedia articles, as described in the algorithm (Alg. 2.1). The candidate relations are generated by iterating all pairs of entity mentions in the same sentence. Relation detection is formulated as a multiclass classification problem. The One vs. Rest strategy is employed by selecting the instance with largest margin as the final answer. We carried out 5-fold cross-validation with the tree kernel toolkit4 (Moschitti, 2004; Moschitti, 2008). 4.2 Results on Wikipedia RE We created a test set by sampling 200 articles from Freebase (these articles are not used for training). An expert annotator, for each sentence, labeled all possible pairs of entities with one of the 52 relations from YAGO, where the entities were already marked. This process resulted in 2,601 relation instances. Table 1 shows the performance of individual classifiers as well as the overall Micro-average F1 for our adapted C5K: we note that it reaches an F1- score of 74.29%. This can be compared with the Micro-average F1 of CKI, i.e. 71.21%. The </context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow statistic parsing. In Proceedings of ACL, pages 335–342, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>253--262</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="12821" citStr="Moschitti, 2008" startWordPosition="2073" endWordPosition="2074">verall 91.42 62.57 74.29 Table 1: Performance of 8 out of 52 individual relations with overall F1. 4.1 Experimental setting We used the DS dataset generated from YAGO and Wikipedia articles, as described in the algorithm (Alg. 2.1). The candidate relations are generated by iterating all pairs of entity mentions in the same sentence. Relation detection is formulated as a multiclass classification problem. The One vs. Rest strategy is employed by selecting the instance with largest margin as the final answer. We carried out 5-fold cross-validation with the tree kernel toolkit4 (Moschitti, 2004; Moschitti, 2008). 4.2 Results on Wikipedia RE We created a test set by sampling 200 articles from Freebase (these articles are not used for training). An expert annotator, for each sentence, labeled all possible pairs of entities with one of the 52 relations from YAGO, where the entities were already marked. This process resulted in 2,601 relation instances. Table 1 shows the performance of individual classifiers as well as the overall Micro-average F1 for our adapted C5K: we note that it reaches an F1- score of 74.29%. This can be compared with the Micro-average F1 of CKI, i.e. 71.21%. The lower result sugge</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In Proceedings of CIKM, pages 253–262, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1378--1387</pages>
<location>Singapore,</location>
<contexts>
<context position="5670" citStr="Nguyen et al., 2009" startWordPosition="877" endWordPosition="880"> the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for each E1 E 8 and E2 E 8 and R E YAGO if R(E1, E2) E YAGO(R) then DS +— DS U is, R+} else DS +— DS U is, R−} return (DS) such approaches can be applied to few relation types thus distant supervised learning (Mintz et al., 2009) was introduced to tackle such problem. Another s</context>
<context position="10568" citStr="Nguyen et al., 2009" startWordPosition="1702" endWordPosition="1705">ed version of our approach is the following: for any YAGO relation instance, scan all the sentences of all Wikipedia articles to test point (ii). Unfortunately, this procedure is impossible in practice due to millions of relation instances in YAGO and millions of Wikipedia articles in Freebase, i.e. an order of magnitude of 1014 iterations3. 3 Distant Supervised Learning with Kernels We model relation extraction (RE) using state-ofthe-art classifiers based on kernel methods. The main idea is that syntactic/semantic structures are used to represent relation instances. We followed the model in (Nguyen et al., 2009) that has shown significant improvement on the state-of-the-art. This combines a syntactic tree kernel and a polynomial kernel over feature extracted from the entities: CK1 = α · KP + (1 − α) · TK (1) where α is a coefficient to give more or less impact to the polynomial kernel, KP, and TK is the syntactic tree kernel (Collins and Duffy, 2001). The best model combines the advantages of the two parsing paradigms by adding the kernel above with six sequence kernels (described in (Nguyen et al., 2009)). C5K = α·KP +(1−α)·(TK + � 5Ki) (2) i=1,..,6 Such kernels cannot be applied to Wikipedia docume</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In Proceedings of EMNLP, pages 1378–1387, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Kernel-based re-ranking for named-entity extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>901--909</pages>
<location>China,</location>
<contexts>
<context position="14070" citStr="Nguyen et al., 2010" startWordPosition="2268" endWordPosition="2271">ependency and constituent syntactic structures is very important: +3.08 absolute percent points on CKI, which only uses constituency trees. 4http://disi.unitn.it/ moschitt/Tree-Kernel.htm Entity Detection 68.84 64.56 66.63 End-to-End RE 82.16 56.57 67.00 Table 2: Entity Detection and End-to-end Relation Extraction. 4.3 End-to-end Relation Extraction Previous work in RE uses gold entities available in the annotated corpus (i.e. ACE) but in real applications these are not available. Therefore, we perform experiments with automatic entities. For their extraction, we follow the feature design in (Nguyen et al., 2010), using CRF++ 5 with unigram/features and Freebase as learning source. Dates and numerical attributes required a different treatment, so we use the patterns described in Section 2.3. The results reported in Table 2 are rather lower than in standard NE recognition. This is due to the high complexity of predicting the boundaries of thousands of different categories in YAGO. Our end-to-end RE system can be applied to any text fragment so we could experiment with it and any Wikipedia document. This allowed us to carry out an accurate evaluation. The results are shown in Table 2. We note that, with</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2010</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2010. Kernel-based re-ranking for named-entity extraction. In Proceedings of COLING, pages 901–909, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<volume>6323</volume>
<pages>148--163</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="6311" citStr="Riedel et al., 2010" startWordPosition="1004" endWordPosition="1007">. However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for each E1 E 8 and E2 E 8 and R E YAGO if R(E1, E2) E YAGO(R) then DS +— DS U is, R+} else DS +— DS U is, R−} return (DS) such approaches can be applied to few relation types thus distant supervised learning (Mintz et al., 2009) was introduced to tackle such problem. Another solution proposed in (Riedel et al., 2010) was to adapt models trained in one domain to other text domains. 2 Resources and Dataset Creation In this section, we describe the resources for the creation of an annotated dataset based on distant supervision. We use YAGO, a large knowledge base of entities and relations, and Freebase, a collection of Wikipedia articles. Our procedure uses entities and facts from YAGO to provide relation instances. For each pair of entities that appears in some YAGO relation, we retrieve all the sentences of the Freebase documents that contain such entities. 2.1 YAGO YAGO (Suchanek et al., 2007) is a huge s</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, volume 6323 of Lecture Notes in Computer Science, pages 148–163. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago - a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In 16th international World Wide Web conference,</booktitle>
<pages>697--706</pages>
<contexts>
<context position="6899" citStr="Suchanek et al., 2007" startWordPosition="1103" endWordPosition="1106"> proposed in (Riedel et al., 2010) was to adapt models trained in one domain to other text domains. 2 Resources and Dataset Creation In this section, we describe the resources for the creation of an annotated dataset based on distant supervision. We use YAGO, a large knowledge base of entities and relations, and Freebase, a collection of Wikipedia articles. Our procedure uses entities and facts from YAGO to provide relation instances. For each pair of entities that appears in some YAGO relation, we retrieve all the sentences of the Freebase documents that contain such entities. 2.1 YAGO YAGO (Suchanek et al., 2007) is a huge semantic knowledge base derived from WordNet and Wikipedia. It comprises more than 2 million entities (like persons, organizations, cities, etc.) and 20 million facts connecting these entities. These include the taxonomic Is-A hierarchy as well as semantic relations between entities. We use the YAGO version of 2008-w40-2 with a manually confirmed accuracy of 95% for 99 relations. However, some of them are (a) trivial, e.g. familyNameOf; (b) numerical attributes that change over time, e.g. hasPopulation; (c) symmetric, e.g. hasPredecessor; (d) used only for data management, e.g. desc</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago - a core of semantic knowledge. In 16th international World Wide Web conference, pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
</authors>
<title>Extracting world knowledge from the web.</title>
<date>2009</date>
<journal>IEEE Computer,</journal>
<volume>42</volume>
<issue>6</issue>
<contexts>
<context position="5073" citStr="Yates, 2009" startWordPosition="788" endWordPosition="789">ates the quality of our model. Finally, we also provide a system for extracting relations from any text. This required the definition of a robust Named Entity Recognizer (NER), which is also trained on weakly supervised Wikipedia data. Consequently, our end-to-end RE system is applicable to any document. This is another major improvement on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real scenarios. 1.1 Related Work RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Z</context>
</contexts>
<marker>Yates, 2009</marker>
<rawString>Alexander Yates. 2009. Extracting world knowledge from the web. IEEE Computer, 42(6):94–97, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>181--201</pages>
<contexts>
<context position="1229" citStr="Zelenko et al., 2002" startWordPosition="177" endWordPosition="180">is enough to produce reliable supervision. Our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document. 1 Introduction Relation Extraction (RE) from text as defined in ACE (Doddington et al., 2004) concerns the extraction of relationships between two entities. This is typically carried out by applying supervised learning, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) by using a handlabeled corpus. Although, the resulting models are far more accurate than unsupervised approaches, they suffer from the following drawbacks: (i) they require labeled data, which is usually costly to produce; (ii) they are typically domain-dependent as different domains involve different relations; and (iii), even in case the relations do not change, they result biased toward the text feature distributions of the training domain. The drawbacks above would be alleviated if data from several different domains and relationships</context>
<context position="5226" citStr="Zelenko et al., 2002" startWordPosition="809" endWordPosition="812"> Named Entity Recognizer (NER), which is also trained on weakly supervised Wikipedia data. Consequently, our end-to-end RE system is applicable to any document. This is another major improvement on previous work. The satisfactory RE F1 of 67% for 52 Wikipedia relations suggests that our model is also successfully applicable in real scenarios. 1.1 Related Work RE generally relates to the extraction of relational facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABE</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2002</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2002. Kernel methods for relation extraction. In Proceedings of EMNLP-ACL, pages 181–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jian Su</author>
<author>Danmei Wang</author>
<author>Guodong Zhou</author>
<author>Chew Lim Tan</author>
</authors>
<title>Discovering relations between named entities from a large raw corpus using tree similarity-based clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP’2005, Lecture Notes in Computer Science (LNCS 3651),</booktitle>
<pages>378--389</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="5634" citStr="Zhang et al., 2005" startWordPosition="871" endWordPosition="874">onal facts, or world knowledge from the Web (Yates, 2009). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for each E1 E 8 and E2 E 8 and R E YAGO if R(E1, E2) E YAGO(R) then DS +— DS U is, R+} else DS +— DS U is, R−} return (DS) such approaches can be applied to few relation types thus distant supervised learning (Mintz et al., 2009) was introduc</context>
</contexts>
<marker>Zhang, Su, Wang, Zhou, Tan, 2005</marker>
<rawString>Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and Chew Lim Tan. 2005. Discovering relations between named entities from a large raw corpus using tree similarity-based clustering. In Proceedings of IJCNLP’2005, Lecture Notes in Computer Science (LNCS 3651), pages 378–389, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>825--832</pages>
<contexts>
<context position="5691" citStr="Zhang et al., 2006" startWordPosition="881" endWordPosition="884">). To identify semantic relations using machine learning, three learning settings have been applied, namely supervised methods, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004), semi supervised methods, e.g. (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method, e.g. (Hasegawa et al., 2004; Banko et al., 2007). Work on supervised Relation Extraction has mostly employed kernel-based approaches, e.g. (Zelenko et al., 2002; Culotta and Sorensen, 2004; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009; Zhang et al., 2006). However, 1Previous work assumes the page related to the Infobox as the only source for the training data. Algorithm 2.1: ACQUIRE LABELED DATA() DS=O Y AGO(R) : Instances of Relation R for each (Wikipedia article: W) E Freebase S +— set of sentences from W for each s E S 8 +— set of entities from s for each E1 E 8 and E2 E 8 and R E YAGO if R(E1, E2) E YAGO(R) then DS +— DS U is, R+} else DS +— DS U is, R−} return (DS) such approaches can be applied to few relation types thus distant supervised learning (Mintz et al., 2009) was introduced to tackle such problem. Another solution proposed in (</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of COLING-ACL 2006, pages 825–832.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>