<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003508">
<title confidence="0.839755">
A Succinct N-gram Language Model
</title>
<author confidence="0.628148">
Taro Watanabe Hajime Tsukada Hideki Isozaki
</author>
<affiliation confidence="0.464322">
NTT Communication Science Laboratories
</affiliation>
<address confidence="0.793239">
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
</address>
<email confidence="0.993808">
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.997327" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999763928571428">
Efficient processing of tera-scale text data
is an important research topic. This pa-
per proposes lossless compression of N-
gram language models based on LOUDS,
a succinct data structure. LOUDS suc-
cinctly represents a trie with M nodes as a
2M + 1 bit string. We compress it further
for the N-gram language model structure.
We also use ‘variable length coding’ and
‘block-wise compression’ to compress val-
ues associated with nodes. Experimental
results for three large-scale N-gram com-
pression tasks achieved a significant com-
pression rate without any loss.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998969129032258">
There has been an increase in available N-gram
data and a large amount of web-scaled N-gram
data has been successfully deployed in statistical
machine translation. However, we need either a
machine with hundreds of gigabytes of memory
or a large computer cluster to handle them.
Either pruning (Stolcke, 1998; Church et al.,
2007) or lossy randomizing approaches (Talbot
and Brants, 2008) may result in a compact repre-
sentation for the application run-time. However,
the lossy approaches may reduce accuracy, and
tuning is necessary. A lossless approach is obvi-
ously better than a lossy one if other conditions
are the same. In addtion, a lossless approach can
easly combined with pruning. Therefore, lossless
representation of N-gram is a key issue even for
lossy approaches.
Raj and Whittaker (2003) showed a general N-
gram language model structure and introduced a
lossless algorithm that compressed a sorted integer
vector by recursively shifting a certain number of
bits and by emitting index-value inverted vectors.
However, we need more compact representation.
In this work, we propose a succinct way to
represent the N-gram language model structure
based on LOUDS (Jacobson, 1989; Delpratt et
al., 2006). It was first introduced by Jacobson
(1989) and requires only a small space close to
the information-theoretic lower bound. For an M
node ordinal trie, its information-theoretical lower
bound is 2M − O(lg M) bits (lg(x) = log2(x))
</bodyText>
<note confidence="0.411097">
1-gram 2-gram 3-gram
</note>
<figureCaption confidence="0.713456">
Figure 1: Data structure for language model
and LOUDS succinctly represents it by a 2M + 1
bit string. The space is further reduced by consid-
ering the N-gram structure. We also use variable
</figureCaption>
<bodyText confidence="0.998605555555556">
length coding and block-wise compression to com-
press the values associated with each node, such as
word ids, probabilities or counts.
We experimented with English Web 1T 5-gram
from LDC consisting of 25 GB of gzipped raw
text N-gram counts. By using 8-bit floating point
quantization 1, N-gram language models are com-
pressed into 10 GB, which is comparable to a lossy
representation (Talbot and Brants, 2008).
</bodyText>
<sectionHeader confidence="0.985711" genericHeader="method">
2 N-gram Language Model
</sectionHeader>
<bodyText confidence="0.9011842">
We assume a back-off N-gram language model in
which the conditional probability Pr(wn|wn−1
1 )
for an arbitrary N-gram wn1 = (w1,..., wn) is re-
cursively computed as follows.
</bodyText>
<equation confidence="0.99129125">
α(wn1 ) if wn1 exists.
Q(wi−1)Pr(wn |w2−1) if wn−1
1 exists.
Pr(wn|wn−1
</equation>
<bodyText confidence="0.998386692307692">
2 ) otherwise.
α(wn1 ) and Q(wn1)are smoothed probabilities and
back-off coefficients, respectively.
The N-grams are stored in a trie structure as
shown in Figure 1. N-grams of different orders
are stored in different tables and each row corre-
sponds to a particular wn1 , consisting of a word id
for wn, α(wn1 ), Q(wn1) and a pointer to the first po-
sition of the succeeding (n + 1)-grams that share
the same prefix wn1. The succeeding (n+1)-grams
are stored in a contiguous region and sorted by the
word id of wn+1. The boundary of the region is de-
termined by the pointer of the next N-gram in the
</bodyText>
<footnote confidence="0.982751">
1The compact representation of the floating point is out of
the scope of this paper. Therefore, we use the term lossless
even when using floating point quantization.
</footnote>
<figure confidence="0.979162166666667">
word id back-off
probability pointer
probability pointer
back-off
word id back-off
probability pointer
</figure>
<page confidence="0.969312">
341
</page>
<note confidence="0.9551115">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 341–344,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.998740640625">
0
1 2 3 4
5 6 7 8 9 10
11 12 13 14 15
(a) Trie structure
(b) Corresponding LOUDS bit string
node id
0
1
2
4
5
1415
6
7
8
9
1011
1213
3
bit position 01
LOUDS bit 10
2345678910
11110111 0
1112 13 14
1 1 0 0
15 16 17 18 19
1 0 1 1 0
20 2122
23 24 25 26
27 28
29 30
3132
0 1 0
0 1 1 0
0 0
0 0
0 0
0 1 2 3
4 5 6 7 8 9
10 11 12 13 14
(c) Trie structure for N-gram
(d) Corresponding N-gram optimized LOUDS bit string
node id
0
4
5
7
6
13 14 15
0
1 0 0
16
8 9
17 18 19 20
1 1 0 0
1
2 3
4 5 6 7
8 9 10 11 12
bit position 0 1 2 3
LOUDS bit 1 1 1 0
1 0 1 1 0
1 1 0 0
</figure>
<figureCaption confidence="0.999968">
Figure 2: Optimization of LOUDS bit string for N-gram data
</figureCaption>
<bodyText confidence="0.999816611111111">
row. When an N-gram is traversed, binary search
is performed N times. If each word id corresponds
to its node position in the unigram table, we can
remove the word ids for the first order.
Our implementation merges across different or-
ders of N-grams, then separates into multiple ta-
bles such as word ids, smoothed probabilities,
back-off coefficients, and pointers. The starting
positions of different orders are memorized to al-
low access to arbitrary orders. To store N-gram
counts, we use three tables for word ids, counts
and pointers. We share the same tables for word
ids and pointers with additional probability and
back-off coefficient tables.
To support distributed computation (Brants et
al., 2007), we further split the N-gram data into
“shards” by hash values of the first bigram. Uni-
gram data are shared across shards for efficiency.
</bodyText>
<sectionHeader confidence="0.993848" genericHeader="method">
3 Succinct N-gram Structure
</sectionHeader>
<bodyText confidence="0.9973986">
The table of pointers described in the previous
section represents a trie. We use a succinct data
structure LOUDS (Jacobson, 1989; Delpratt et al.,
2006) for compact representation of the trie.
For an M node ordinal trie, there exist
</bodyText>
<equation confidence="0.994422285714286">
1
2M+1 ( 2M+1M ) different tries. Therefore,
its information-theoretical lower bound is
1 1(2M+1)] � 2M − O(lg M) bits.
lg [2M+ M
LOUDS represents a trie with M nodes as a
2M + O(M) bit string.
</equation>
<bodyText confidence="0.982164304347826">
The LOUDS bit string is constructed as follows.
Starting from the root node, we traverse a trie in
level order. For each node with d &gt; 0 children, the
bit string 1d0 is emitted. In addition, 10 is prefixed
to the bit string emitted by an imaginary super-root
node pointing to the root node. Figure 2(a) shows
an example trie structure. The nodes are numbered
in level order, and from left to right. The cor-
responding LOUDS bit string is shown in Figure
2(b). Since the root node 0 has four child nodes,
it emits four 1s followed by 0, which marks the
end of the node. Before the root node, we assume
an imaginary super root node emits 10 for its only
child, i.e., the root node. After the root node, its
first child or node 1 follows. Since (M + 1)0s and
M1s are emitted for a trie with M nodes, LOUDS
occupies 2M + 1 bits.
We define a basic operation on the bit string.
sel1(i) returns the position of the i-th 1. We can
also define similar operations over zero bit strings,
sel0(i). Given selb, we define two operations for
a node x. parent(x) gives x’s parent node and
firstch(x) gives x’s first child node:
</bodyText>
<equation confidence="0.999997">
parent(x) = sel1(x + 1) − x − 1, (1)
firstch(x) = sel0(x + 1) − x. (2)
</equation>
<bodyText confidence="0.9963545">
To test whether a child node exists, we sim-
ply check firstch(x) =� firstch(x + 1). Sim-
ilarly, the child node range is determined by
[firstch(x), firstch(x + 1)).
</bodyText>
<subsectionHeader confidence="0.99857">
3.1 Optimizing N-gram Structure for Space
</subsectionHeader>
<bodyText confidence="0.99989215">
We propose removing redundant bits from the
baseline LOUDS representation assuming N-
gram structures. Since we do not store any infor-
mation in the root node, we can safely remove the
root so that the imaginary super-root node directly
points to unigram nodes. The node ids are renum-
bered and the first unigram is 0. In this way, 2 bits
are saved.
The N-gram data structure has a fixed depth N
and takes a flat structure. Since the highest or-
der N-grams have no child nodes, they emit 0NN
in the tail of the bit stream, where Nn stands for
the number of n-grams. By memorizing the start-
ing position of the highest order N-grams, we can
completely remove NN bits.
The imaginary super-root emits 1N10 at the be-
ginning of the bit stream. By memorizing the bi-
gram starting position, we can remove the N1 + 1
bits.
Finally, parent(x) and firstch(x) are rewritten as
</bodyText>
<page confidence="0.996706">
342
</page>
<figureCaption confidence="0.9201815">
Figure 3: Example of variable length coding
follows:
</figureCaption>
<equation confidence="0.9998885">
parent(x) = sel1(x + 1 − N1) + N1 − x, (3)
firstch(x) = sel0(x) + N1 + 1 − x. (4)
</equation>
<bodyText confidence="0.984928454545454">
Figure 2(c) shows the N-gram optimized trie
structure (N = 3) from Figure 2 with N1 = 4
and N3 = 5. The parent of node 8 is found by
sel1(8+1−4) = 5 and 5+4−8 = 1. The first child
is located by sel0(8) = 16 and 16+4+1−8 = 13.
When accessing the N-gram data structure,
selb(i) operations are used extensively. We use an
auxiliary dictionary structure proposed by Kim et
al. (2005) and Jacobson (1989) that supports an
efficient sel1(i) (sel0(i)) with the dictionary. We
omit the details due to lack of space.
</bodyText>
<subsectionHeader confidence="0.998998">
3.2 Variable Length Coding
</subsectionHeader>
<bodyText confidence="0.987469304347826">
The above method compactly represents pointers,
but not associated values, such as word ids or
counts. Raj and Whittaker (2003) proposed in-
teger compression on each range of the word id
sequence that shared the same N-gram prefix.
Here, we introduce a simple but more effec-
tive variable length coding for integer sequences
of word ids and counts. The basic idea comes from
encoding each integer by the smallest number of
required bytes. Specifically, an integer within the
range of 0 to 255 is coded as a 1-byte integer,
the integers within the range of 256 to 65,535 are
stored as 2-byte integers, and so on. We use an ad-
ditional bit vector to indicate the boundary of the
byte sequences. Figure 3 presents an example in-
teger sequence, 52, 156, 260 and 364 with coded
integers in hex decimals with boundary bits.
In spite of the length variability, the system
can directly access a value at index i as bytes
in [sel1(i) + 1, sel1(i + 1) + 1) by the efficient
sel1 operation assuming that sel1(0) yields −1.
For example, the value 260 at index 2 in Figure
3 is mapped onto the byte range of [sel1(2) +
</bodyText>
<equation confidence="0.596366">
1, sel1(3) + 1) = [2, 4).
</equation>
<subsectionHeader confidence="0.983595">
3.3 Block-wise Compression
</subsectionHeader>
<bodyText confidence="0.999969230769231">
We further compress every 8K-byte data block of
all tables in N-grams by using a generic com-
pression library, zlib, employed in UNIX gzip.
We treat a sequence of 4-byte floats in the prob-
ability table as a byte stream, and compress ev-
ery 8K-byte block. To facilitate random access to
the compressed block, we keep track of the com-
pressed block’s starting offsets. Since the offsets
are in sorted order, we can apply sorted integer
compression (Raj and Whittaker, 2003). Since N-
gram language model access preserves some local-
ity, N-gram with block compression is still practi-
cal enough to be usable in our system.
</bodyText>
<sectionHeader confidence="0.998868" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999966692307692">
We applied the proposed representation to 5-gram
trained by “English Gigaword 3rd Edition,” “En-
glish Web 1T 5-gram” from LDC, and “Japanese
Web 1T 7-gram” from GSK. Since their tendencies
are the same, we only report in this paper the re-
sults on English Web 1T 5-gram, where the size
of the count data in gzipped raw text format is
25GB, the number of N-grams is 3.8G, the vocab-
ulary size is 13.6M words, and the number of the
highest order N-grams is 1.2G.
We implemented an N-gram indexer/estimator
using MPI inspired by the MapReduce imple-
mentation of N-gram language model index-
ing/estimation pipeline (Brants et al., 2007).
Table 1 summarizes the overall results. We
show the initial indexed counts and the final lan-
guage model size by differentiating compression
strategies for the pointers, namely the 4-byte raw
value (Trie), the sorted integer compression (In-
teger) and our succinct representation (Succinct).
The “block” indicates block compression. For the
sake of implementation simplicity, the sorted in-
teger compression used a fixed 8-bit shift amount,
although the original paper proposed recursively
determined optimum shift amounts (Raj and Whit-
taker, 2003). 8-bit quantization was performed
for probabilities and back-off coefficients using a
simple binning approach (Federico and Cettolo,
2007).
N-gram counts were reduced from 23.59GB
to 10.57GB by our succinct representation with
block compression. N-gram language models of
42.65GB were compressed to 18.37GB. Finally,
the 8-bit quantized N-gram language models are
represented by 9.83GB of space.
Table 2 shows the compression ratio for the
pointer table alone. Block compression employed
on raw 4-byte pointers attained a large reduc-
tion that was almost comparable to sorted inte-
ger compression. Since large pointer value tables
are sorted, even a generic compression algorithm
could achieve better compression. Using our suc-
cinct representation, 2.4 bits are required for each
N-gram. By using the “flat” trie structure, we
approach closer to its information-theoretic lower
bound beyond the LOUDS baseline. With block
compression, we achieved 1.8 bits per N-gram.
Table 3 shows the effect of variable length
coding and block compression for the word ids,
counts, probabilities and back-off coefficients. Af-
ter variable-length coding, the word id is almost
half its original size. We assign a word id for each
</bodyText>
<figure confidence="0.980526333333333">
integer seq.
52
156
260
364
coding
0x34
0x9c
0x01 0x04
0x01 0x6c
boundary
1
1
0 1
0 1
</figure>
<page confidence="0.995676">
343
</page>
<table confidence="0.9984712">
w/o block w/ block
Counts Trie 23.59 GB 12.21 GB
Integer 14.59 GB 11.18 GB
Succinct 12.62 GB 10.57 GB
Language Trie 42.65 GB 20.01 GB
model Integer 33.65 GB 18.98 GB
Succinct 31.67 GB 18.37 GB
Quantized Trie 24.73 GB 11.47 GB
language Integer 15.73 GB 10.44 GB
model Succinct 13.75 GB 9.83 GB
</table>
<tableCaption confidence="0.992503">
Table 1: Summary of N-gram compression
</tableCaption>
<table confidence="0.999761571428571">
total per N-gram
4-byte Pointer 12.04 GB 27.24 bits
+block compression 2.42 GB 5.48 bits
Sorted Integer 3.04 GB 6.87 bits
+block compression 1.39 GB 3.15 bits
Succinct 1.06 GB 2.40 bits
+block compression 0.78 GB 1.76 bits
</table>
<tableCaption confidence="0.999691">
Table 2: Compression ratio for pointers
</tableCaption>
<bodyText confidence="0.999232384615385">
word according to its reverse sorted order of fre-
quency. Therefore, highly frequent words are as-
signed smaller values, which in turn occupies less
space in our variable length coding. With block
compression, we achieved further 1 GB reduction
in space. Since the word id sequence preserves
local ordering for a certain range, even a generic
compression algorithm is effective.
The most frequently observed count in N-gram
data is one. Therefore, we can reduce the space
by the variable length coding. Large compression
rates are achieved for both probabilities and back-
off coefficients.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998873958333333">
We provided a succinct representation of the N-
gram language model without any loss. Our
method approaches closer to the information-
theoretic lower bound beyond the LOUDS base-
line. Experimental results showed our succinct
representation drastically reduces the space for
the pointers compared to the sorted integer com-
pression approach. Furthermore, the space of
N-grams was significantly reduced by variable
total per N-gram
word id size (4 bytes) 14.09 GB 31.89 bits
+variable length 6.72 GB 15.20 bits
+block compression 5.57 GB 12.60 bits
count size (8 bytes) 28.28 GB 64.00 bits
+variable length 4.85 GB 10.96 bits
+block compression 4.22 GB 9.56 bits
probability size (4 bytes) 14.14 GB 32.00 bits
+block compression 9.55 GB 21.61 bits
8-bit quantization 3.54 GB 8.00 bits
+block compression 2.64 GB 5.97 bits
backoff size (4 bytes) 9.76 GB 22.08 bits
+block compression 2.48 GB 5.61 bits
8-bit quantization 2.44 GB 5.52 bits
+block compression 0.85 GB 1.92 bits
</bodyText>
<tableCaption confidence="0.994708">
Table 3: Effects of block compression
</tableCaption>
<bodyText confidence="0.999931846153846">
length coding and block compression. A large
amount of N-gram data is reduced from unin-
dexed gzipped 25 GB text counts to 10 GB of
indexed language models. Our representation is
practical enough though we did not experimen-
tally investigate the runtime efficiency in this pa-
per. The proposed representation enables us to
utilize a web-scaled N-gram in our MT compe-
tition system (Watanabe et al., 2008). Our suc-
cinct representation will encourage new research
on web-scaled N-gram data without requiring a
larger computer cluster or hundreds of gigabytes
of memory.
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99801875">
We would like to thank Daisuke Okanohara for his
open source implementation and extensive docu-
mentation of LOUDS, which helped our original
coding.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999664">
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In Proc. of EMNLP-CoNLL 2007.
K. Church, T. Hart, and J. Gao. 2007. Compressing
trigram language models with Golomb coding. In
Proc. of EMNLP-CoNLL 2007.
O. Delpratt, N. Rahman, and R. Raman. 2006. Engi-
neering the LOUDS succinct tree representation. In
Proc. of the 5th International Workshop on Experi-
mental Algorithms.
M. Federico and M. Cettolo. 2007. Efficient handling
of n-gram language models for statistical machine
translation. In Proc. of the 2nd Workshop on Statis-
tical Machine Translation.
G. Jacobson. 1989. Space-efficient static trees and
graphs. In 30th Annual Symposium on Foundations
of Computer Science, Nov.
D. K. Kim, J. C. Na, J. E. Kim, and K. Park. 2005. Ef-
ficient implementation of rank and select functions
for succinct representation. In Proc. of the 5th Inter-
national Workshop on Experimental Algorithms.
B. Raj and E. W. D. Whittaker. 2003. Lossless com-
pression of language model structure and word iden-
tifiers. In Proc. of ICASSP 2003, volume 1.
A. Stolcke. 1998. Entropy-based pruning of backoff
language models. In Proc. of the ARPA Workshop
on Human Language Technology.
D. Talbot and T. Brants. 2008. Randomized language
models via perfect hash functions. In Proc. of ACL-
08: HLT.
T. Watanabe, H. Tsukada, and H. Isozaki. 2008. NTT
SMT system 2008 at NTCIR-7. In Proc. of the 7th
NTCIR Workshop, pages 420–422.
</reference>
<page confidence="0.998982">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.458643">
<title confidence="0.999827">A Succinct N-gram Language Model</title>
<author confidence="0.992046">Taro Watanabe Hajime Tsukada Hideki Isozaki</author>
<affiliation confidence="0.999271">NTT Communication Science Laboratories</affiliation>
<address confidence="0.956899">2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan</address>
<abstract confidence="0.999724928571428">Efficient processing of tera-scale text data is an important research topic. This paproposes of gram language models based on LOUDS, a succinct data structure. LOUDS sucrepresents a trie with as a 1 string. We compress it further the language model structure. also use length and to compress valwith nodes. Experimental for three large-scale compression tasks achieved a significant com-</abstract>
<intro confidence="0.485381">rate any</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A C Popat</author>
<author>P Xu</author>
<author>F J Och</author>
<author>J Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL</booktitle>
<contexts>
<context position="5452" citStr="Brants et al., 2007" startWordPosition="955" endWordPosition="958">orresponds to its node position in the unigram table, we can remove the word ids for the first order. Our implementation merges across different orders of N-grams, then separates into multiple tables such as word ids, smoothed probabilities, back-off coefficients, and pointers. The starting positions of different orders are memorized to allow access to arbitrary orders. To store N-gram counts, we use three tables for word ids, counts and pointers. We share the same tables for word ids and pointers with additional probability and back-off coefficient tables. To support distributed computation (Brants et al., 2007), we further split the N-gram data into “shards” by hash values of the first bigram. Unigram data are shared across shards for efficiency. 3 Succinct N-gram Structure The table of pointers described in the previous section represents a trie. We use a succinct data structure LOUDS (Jacobson, 1989; Delpratt et al., 2006) for compact representation of the trie. For an M node ordinal trie, there exist 1 2M+1 ( 2M+1M ) different tries. Therefore, its information-theoretical lower bound is 1 1(2M+1)] � 2M − O(lg M) bits. lg [2M+ M LOUDS represents a trie with M nodes as a 2M + O(M) bit string. The L</context>
<context position="11375" citStr="Brants et al., 2007" startWordPosition="2033" endWordPosition="2036">ied the proposed representation to 5-gram trained by “English Gigaword 3rd Edition,” “English Web 1T 5-gram” from LDC, and “Japanese Web 1T 7-gram” from GSK. Since their tendencies are the same, we only report in this paper the results on English Web 1T 5-gram, where the size of the count data in gzipped raw text format is 25GB, the number of N-grams is 3.8G, the vocabulary size is 13.6M words, and the number of the highest order N-grams is 1.2G. We implemented an N-gram indexer/estimator using MPI inspired by the MapReduce implementation of N-gram language model indexing/estimation pipeline (Brants et al., 2007). Table 1 summarizes the overall results. We show the initial indexed counts and the final language model size by differentiating compression strategies for the pointers, namely the 4-byte raw value (Trie), the sorted integer compression (Integer) and our succinct representation (Succinct). The “block” indicates block compression. For the sake of implementation simplicity, the sorted integer compression used a fixed 8-bit shift amount, although the original paper proposed recursively determined optimum shift amounts (Raj and Whittaker, 2003). 8-bit quantization was performed for probabilities </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. 2007. Large language models in machine translation. In Proc. of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>T Hart</author>
<author>J Gao</author>
</authors>
<title>Compressing trigram language models with Golomb coding.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL</booktitle>
<contexts>
<context position="1126" citStr="Church et al., 2007" startWordPosition="164" endWordPosition="167">for the N-gram language model structure. We also use ‘variable length coding’ and ‘block-wise compression’ to compress values associated with nodes. Experimental results for three large-scale N-gram compression tasks achieved a significant compression rate without any loss. 1 Introduction There has been an increase in available N-gram data and a large amount of web-scaled N-gram data has been successfully deployed in statistical machine translation. However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them. Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lossy one if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N-gram is a key issue even for lossy approaches. Raj and Whittaker (2003) showed a general Ngram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by r</context>
</contexts>
<marker>Church, Hart, Gao, 2007</marker>
<rawString>K. Church, T. Hart, and J. Gao. 2007. Compressing trigram language models with Golomb coding. In Proc. of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Delpratt</author>
<author>N Rahman</author>
<author>R Raman</author>
</authors>
<title>Engineering the LOUDS succinct tree representation.</title>
<date>2006</date>
<booktitle>In Proc. of the 5th International Workshop on Experimental Algorithms.</booktitle>
<contexts>
<context position="2006" citStr="Delpratt et al., 2006" startWordPosition="301" endWordPosition="304">ne if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N-gram is a key issue even for lossy approaches. Raj and Whittaker (2003) showed a general Ngram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by recursively shifting a certain number of bits and by emitting index-value inverted vectors. However, we need more compact representation. In this work, we propose a succinct way to represent the N-gram language model structure based on LOUDS (Jacobson, 1989; Delpratt et al., 2006). It was first introduced by Jacobson (1989) and requires only a small space close to the information-theoretic lower bound. For an M node ordinal trie, its information-theoretical lower bound is 2M − O(lg M) bits (lg(x) = log2(x)) 1-gram 2-gram 3-gram Figure 1: Data structure for language model and LOUDS succinctly represents it by a 2M + 1 bit string. The space is further reduced by considering the N-gram structure. We also use variable length coding and block-wise compression to compress the values associated with each node, such as word ids, probabilities or counts. We experimented with En</context>
<context position="5772" citStr="Delpratt et al., 2006" startWordPosition="1008" endWordPosition="1011">ers are memorized to allow access to arbitrary orders. To store N-gram counts, we use three tables for word ids, counts and pointers. We share the same tables for word ids and pointers with additional probability and back-off coefficient tables. To support distributed computation (Brants et al., 2007), we further split the N-gram data into “shards” by hash values of the first bigram. Unigram data are shared across shards for efficiency. 3 Succinct N-gram Structure The table of pointers described in the previous section represents a trie. We use a succinct data structure LOUDS (Jacobson, 1989; Delpratt et al., 2006) for compact representation of the trie. For an M node ordinal trie, there exist 1 2M+1 ( 2M+1M ) different tries. Therefore, its information-theoretical lower bound is 1 1(2M+1)] � 2M − O(lg M) bits. lg [2M+ M LOUDS represents a trie with M nodes as a 2M + O(M) bit string. The LOUDS bit string is constructed as follows. Starting from the root node, we traverse a trie in level order. For each node with d &gt; 0 children, the bit string 1d0 is emitted. In addition, 10 is prefixed to the bit string emitted by an imaginary super-root node pointing to the root node. Figure 2(a) shows an example trie </context>
</contexts>
<marker>Delpratt, Rahman, Raman, 2006</marker>
<rawString>O. Delpratt, N. Rahman, and R. Raman. 2006. Engineering the LOUDS succinct tree representation. In Proc. of the 5th International Workshop on Experimental Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
<author>M Cettolo</author>
</authors>
<title>Efficient handling of n-gram language models for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 2nd Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="12061" citStr="Federico and Cettolo, 2007" startWordPosition="2131" endWordPosition="2134">al indexed counts and the final language model size by differentiating compression strategies for the pointers, namely the 4-byte raw value (Trie), the sorted integer compression (Integer) and our succinct representation (Succinct). The “block” indicates block compression. For the sake of implementation simplicity, the sorted integer compression used a fixed 8-bit shift amount, although the original paper proposed recursively determined optimum shift amounts (Raj and Whittaker, 2003). 8-bit quantization was performed for probabilities and back-off coefficients using a simple binning approach (Federico and Cettolo, 2007). N-gram counts were reduced from 23.59GB to 10.57GB by our succinct representation with block compression. N-gram language models of 42.65GB were compressed to 18.37GB. Finally, the 8-bit quantized N-gram language models are represented by 9.83GB of space. Table 2 shows the compression ratio for the pointer table alone. Block compression employed on raw 4-byte pointers attained a large reduction that was almost comparable to sorted integer compression. Since large pointer value tables are sorted, even a generic compression algorithm could achieve better compression. Using our succinct represe</context>
</contexts>
<marker>Federico, Cettolo, 2007</marker>
<rawString>M. Federico and M. Cettolo. 2007. Efficient handling of n-gram language models for statistical machine translation. In Proc. of the 2nd Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Jacobson</author>
</authors>
<title>Space-efficient static trees and graphs.</title>
<date>1989</date>
<booktitle>In 30th Annual Symposium on Foundations of Computer Science,</booktitle>
<contexts>
<context position="1982" citStr="Jacobson, 1989" startWordPosition="299" endWordPosition="300">r than a lossy one if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N-gram is a key issue even for lossy approaches. Raj and Whittaker (2003) showed a general Ngram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by recursively shifting a certain number of bits and by emitting index-value inverted vectors. However, we need more compact representation. In this work, we propose a succinct way to represent the N-gram language model structure based on LOUDS (Jacobson, 1989; Delpratt et al., 2006). It was first introduced by Jacobson (1989) and requires only a small space close to the information-theoretic lower bound. For an M node ordinal trie, its information-theoretical lower bound is 2M − O(lg M) bits (lg(x) = log2(x)) 1-gram 2-gram 3-gram Figure 1: Data structure for language model and LOUDS succinctly represents it by a 2M + 1 bit string. The space is further reduced by considering the N-gram structure. We also use variable length coding and block-wise compression to compress the values associated with each node, such as word ids, probabilities or counts.</context>
<context position="5748" citStr="Jacobson, 1989" startWordPosition="1006" endWordPosition="1007">of different orders are memorized to allow access to arbitrary orders. To store N-gram counts, we use three tables for word ids, counts and pointers. We share the same tables for word ids and pointers with additional probability and back-off coefficient tables. To support distributed computation (Brants et al., 2007), we further split the N-gram data into “shards” by hash values of the first bigram. Unigram data are shared across shards for efficiency. 3 Succinct N-gram Structure The table of pointers described in the previous section represents a trie. We use a succinct data structure LOUDS (Jacobson, 1989; Delpratt et al., 2006) for compact representation of the trie. For an M node ordinal trie, there exist 1 2M+1 ( 2M+1M ) different tries. Therefore, its information-theoretical lower bound is 1 1(2M+1)] � 2M − O(lg M) bits. lg [2M+ M LOUDS represents a trie with M nodes as a 2M + O(M) bit string. The LOUDS bit string is constructed as follows. Starting from the root node, we traverse a trie in level order. For each node with d &gt; 0 children, the bit string 1d0 is emitted. In addition, 10 is prefixed to the bit string emitted by an imaginary super-root node pointing to the root node. Figure 2(a</context>
<context position="8830" citStr="Jacobson (1989)" startWordPosition="1587" endWordPosition="1588">n remove the N1 + 1 bits. Finally, parent(x) and firstch(x) are rewritten as 342 Figure 3: Example of variable length coding follows: parent(x) = sel1(x + 1 − N1) + N1 − x, (3) firstch(x) = sel0(x) + N1 + 1 − x. (4) Figure 2(c) shows the N-gram optimized trie structure (N = 3) from Figure 2 with N1 = 4 and N3 = 5. The parent of node 8 is found by sel1(8+1−4) = 5 and 5+4−8 = 1. The first child is located by sel0(8) = 16 and 16+4+1−8 = 13. When accessing the N-gram data structure, selb(i) operations are used extensively. We use an auxiliary dictionary structure proposed by Kim et al. (2005) and Jacobson (1989) that supports an efficient sel1(i) (sel0(i)) with the dictionary. We omit the details due to lack of space. 3.2 Variable Length Coding The above method compactly represents pointers, but not associated values, such as word ids or counts. Raj and Whittaker (2003) proposed integer compression on each range of the word id sequence that shared the same N-gram prefix. Here, we introduce a simple but more effective variable length coding for integer sequences of word ids and counts. The basic idea comes from encoding each integer by the smallest number of required bytes. Specifically, an integer wi</context>
</contexts>
<marker>Jacobson, 1989</marker>
<rawString>G. Jacobson. 1989. Space-efficient static trees and graphs. In 30th Annual Symposium on Foundations of Computer Science, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Kim</author>
<author>J C Na</author>
<author>J E Kim</author>
<author>K Park</author>
</authors>
<title>Efficient implementation of rank and select functions for succinct representation.</title>
<date>2005</date>
<booktitle>In Proc. of the 5th International Workshop on Experimental Algorithms.</booktitle>
<contexts>
<context position="8810" citStr="Kim et al. (2005)" startWordPosition="1582" endWordPosition="1585">arting position, we can remove the N1 + 1 bits. Finally, parent(x) and firstch(x) are rewritten as 342 Figure 3: Example of variable length coding follows: parent(x) = sel1(x + 1 − N1) + N1 − x, (3) firstch(x) = sel0(x) + N1 + 1 − x. (4) Figure 2(c) shows the N-gram optimized trie structure (N = 3) from Figure 2 with N1 = 4 and N3 = 5. The parent of node 8 is found by sel1(8+1−4) = 5 and 5+4−8 = 1. The first child is located by sel0(8) = 16 and 16+4+1−8 = 13. When accessing the N-gram data structure, selb(i) operations are used extensively. We use an auxiliary dictionary structure proposed by Kim et al. (2005) and Jacobson (1989) that supports an efficient sel1(i) (sel0(i)) with the dictionary. We omit the details due to lack of space. 3.2 Variable Length Coding The above method compactly represents pointers, but not associated values, such as word ids or counts. Raj and Whittaker (2003) proposed integer compression on each range of the word id sequence that shared the same N-gram prefix. Here, we introduce a simple but more effective variable length coding for integer sequences of word ids and counts. The basic idea comes from encoding each integer by the smallest number of required bytes. Specifi</context>
</contexts>
<marker>Kim, Na, Kim, Park, 2005</marker>
<rawString>D. K. Kim, J. C. Na, J. E. Kim, and K. Park. 2005. Efficient implementation of rank and select functions for succinct representation. In Proc. of the 5th International Workshop on Experimental Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Raj</author>
<author>E W D Whittaker</author>
</authors>
<title>Lossless compression of language model structure and word identifiers.</title>
<date>2003</date>
<booktitle>In Proc. of ICASSP</booktitle>
<volume>1</volume>
<contexts>
<context position="1597" citStr="Raj and Whittaker (2003)" startWordPosition="238" endWordPosition="241">eed either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them. Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lossy one if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N-gram is a key issue even for lossy approaches. Raj and Whittaker (2003) showed a general Ngram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by recursively shifting a certain number of bits and by emitting index-value inverted vectors. However, we need more compact representation. In this work, we propose a succinct way to represent the N-gram language model structure based on LOUDS (Jacobson, 1989; Delpratt et al., 2006). It was first introduced by Jacobson (1989) and requires only a small space close to the information-theoretic lower bound. For an M node ordinal trie, its information-theoretical lower boun</context>
<context position="9093" citStr="Raj and Whittaker (2003)" startWordPosition="1627" endWordPosition="1630"> trie structure (N = 3) from Figure 2 with N1 = 4 and N3 = 5. The parent of node 8 is found by sel1(8+1−4) = 5 and 5+4−8 = 1. The first child is located by sel0(8) = 16 and 16+4+1−8 = 13. When accessing the N-gram data structure, selb(i) operations are used extensively. We use an auxiliary dictionary structure proposed by Kim et al. (2005) and Jacobson (1989) that supports an efficient sel1(i) (sel0(i)) with the dictionary. We omit the details due to lack of space. 3.2 Variable Length Coding The above method compactly represents pointers, but not associated values, such as word ids or counts. Raj and Whittaker (2003) proposed integer compression on each range of the word id sequence that shared the same N-gram prefix. Here, we introduce a simple but more effective variable length coding for integer sequences of word ids and counts. The basic idea comes from encoding each integer by the smallest number of required bytes. Specifically, an integer within the range of 0 to 255 is coded as a 1-byte integer, the integers within the range of 256 to 65,535 are stored as 2-byte integers, and so on. We use an additional bit vector to indicate the boundary of the byte sequences. Figure 3 presents an example integer </context>
<context position="10589" citStr="Raj and Whittaker, 2003" startWordPosition="1898" endWordPosition="1901">elds −1. For example, the value 260 at index 2 in Figure 3 is mapped onto the byte range of [sel1(2) + 1, sel1(3) + 1) = [2, 4). 3.3 Block-wise Compression We further compress every 8K-byte data block of all tables in N-grams by using a generic compression library, zlib, employed in UNIX gzip. We treat a sequence of 4-byte floats in the probability table as a byte stream, and compress every 8K-byte block. To facilitate random access to the compressed block, we keep track of the compressed block’s starting offsets. Since the offsets are in sorted order, we can apply sorted integer compression (Raj and Whittaker, 2003). Since Ngram language model access preserves some locality, N-gram with block compression is still practical enough to be usable in our system. 4 Experiments We applied the proposed representation to 5-gram trained by “English Gigaword 3rd Edition,” “English Web 1T 5-gram” from LDC, and “Japanese Web 1T 7-gram” from GSK. Since their tendencies are the same, we only report in this paper the results on English Web 1T 5-gram, where the size of the count data in gzipped raw text format is 25GB, the number of N-grams is 3.8G, the vocabulary size is 13.6M words, and the number of the highest order </context>
<context position="11922" citStr="Raj and Whittaker, 2003" startWordPosition="2112" endWordPosition="2116">n of N-gram language model indexing/estimation pipeline (Brants et al., 2007). Table 1 summarizes the overall results. We show the initial indexed counts and the final language model size by differentiating compression strategies for the pointers, namely the 4-byte raw value (Trie), the sorted integer compression (Integer) and our succinct representation (Succinct). The “block” indicates block compression. For the sake of implementation simplicity, the sorted integer compression used a fixed 8-bit shift amount, although the original paper proposed recursively determined optimum shift amounts (Raj and Whittaker, 2003). 8-bit quantization was performed for probabilities and back-off coefficients using a simple binning approach (Federico and Cettolo, 2007). N-gram counts were reduced from 23.59GB to 10.57GB by our succinct representation with block compression. N-gram language models of 42.65GB were compressed to 18.37GB. Finally, the 8-bit quantized N-gram language models are represented by 9.83GB of space. Table 2 shows the compression ratio for the pointer table alone. Block compression employed on raw 4-byte pointers attained a large reduction that was almost comparable to sorted integer compression. Sin</context>
</contexts>
<marker>Raj, Whittaker, 2003</marker>
<rawString>B. Raj and E. W. D. Whittaker. 2003. Lossless compression of language model structure and word identifiers. In Proc. of ICASSP 2003, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. of the ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="1104" citStr="Stolcke, 1998" startWordPosition="162" endWordPosition="163">ess it further for the N-gram language model structure. We also use ‘variable length coding’ and ‘block-wise compression’ to compress values associated with nodes. Experimental results for three large-scale N-gram compression tasks achieved a significant compression rate without any loss. 1 Introduction There has been an increase in available N-gram data and a large amount of web-scaled N-gram data has been successfully deployed in statistical machine translation. However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them. Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lossy one if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N-gram is a key issue even for lossy approaches. Raj and Whittaker (2003) showed a general Ngram language model structure and introduced a lossless algorithm that compressed a sort</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>A. Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. of the ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Talbot</author>
<author>T Brants</author>
</authors>
<title>Randomized language models via perfect hash functions.</title>
<date>2008</date>
<booktitle>In Proc. of ACL08:</booktitle>
<publisher>HLT.</publisher>
<contexts>
<context position="1184" citStr="Talbot and Brants, 2008" startWordPosition="172" endWordPosition="175">variable length coding’ and ‘block-wise compression’ to compress values associated with nodes. Experimental results for three large-scale N-gram compression tasks achieved a significant compression rate without any loss. 1 Introduction There has been an increase in available N-gram data and a large amount of web-scaled N-gram data has been successfully deployed in statistical machine translation. However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them. Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lossy one if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N-gram is a key issue even for lossy approaches. Raj and Whittaker (2003) showed a general Ngram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by recursively shifting a certain number of bits and by emitti</context>
<context position="2857" citStr="Talbot and Brants, 2008" startWordPosition="443" endWordPosition="446">(x)) 1-gram 2-gram 3-gram Figure 1: Data structure for language model and LOUDS succinctly represents it by a 2M + 1 bit string. The space is further reduced by considering the N-gram structure. We also use variable length coding and block-wise compression to compress the values associated with each node, such as word ids, probabilities or counts. We experimented with English Web 1T 5-gram from LDC consisting of 25 GB of gzipped raw text N-gram counts. By using 8-bit floating point quantization 1, N-gram language models are compressed into 10 GB, which is comparable to a lossy representation (Talbot and Brants, 2008). 2 N-gram Language Model We assume a back-off N-gram language model in which the conditional probability Pr(wn|wn−1 1 ) for an arbitrary N-gram wn1 = (w1,..., wn) is recursively computed as follows. α(wn1 ) if wn1 exists. Q(wi−1)Pr(wn |w2−1) if wn−1 1 exists. Pr(wn|wn−1 2 ) otherwise. α(wn1 ) and Q(wn1)are smoothed probabilities and back-off coefficients, respectively. The N-grams are stored in a trie structure as shown in Figure 1. N-grams of different orders are stored in different tables and each row corresponds to a particular wn1 , consisting of a word id for wn, α(wn1 ), Q(wn1) and a po</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>D. Talbot and T. Brants. 2008. Randomized language models via perfect hash functions. In Proc. of ACL08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>H Tsukada</author>
<author>H Isozaki</author>
</authors>
<date>2008</date>
<booktitle>NTT SMT system 2008 at NTCIR-7. In Proc. of the 7th NTCIR Workshop,</booktitle>
<pages>420--422</pages>
<marker>Watanabe, Tsukada, Isozaki, 2008</marker>
<rawString>T. Watanabe, H. Tsukada, and H. Isozaki. 2008. NTT SMT system 2008 at NTCIR-7. In Proc. of the 7th NTCIR Workshop, pages 420–422.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>