<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9520515">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 279-286.
</note>
<title confidence="0.9994455">
Dynamic programming for parsing and estimation of
stochastic unification-based grammars*
</title>
<author confidence="0.801">
Stuart Geman
</author>
<affiliation confidence="0.6952565">
Division of Applied Mathematics
Brown University
</affiliation>
<email confidence="0.996502">
geman@dam.brown.edu
</email>
<author confidence="0.729147">
Mark Johnson
Cognitive and Linguistic Sciences
</author>
<affiliation confidence="0.483027">
Brown University
</affiliation>
<email confidence="0.905063">
Mark Johnson@Brown.edu
</email>
<sectionHeader confidence="0.992738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999646769230769">
Stochastic unification-based grammars
(SUBGs) define exponential distributions
over the parses generated by a unification-
based grammar (UBG). Existing algo-
rithms for parsing and estimation require
the enumeration of all of the parses of a
string in order to determine the most likely
one, or in order to calculate the statis-
tics needed to estimate a grammar from
a training corpus. This paper describes a
graph-based dynamic programming algo-
rithm for calculating these statistics from
the packed UBG parse representations of
Maxwell and Kaplan (1995) which does
not require enumerating all parses. Like
many graphical algorithms, the dynamic
programming algorithm’s complexity is
worst-case exponential, but is often poly-
nomial. The key observation is that by
using Maxwell and Kaplan packed repre-
sentations, the required statistics can be
rewritten as either the max or the sum of
a product of functions. This is exactly
the kind of problem which can be solved
by dynamic programming over graphical
models.
</bodyText>
<note confidence="0.9583775">
* We would like to thank Eugene Charniak, Miyao
Yusuke, Mark Steedman as well as Stefan Riezler and the team
at PARC; naturally all errors remain our own. This research was
supported by NSF awards DMS 0074276 and ITR IIS 0085940.
</note>
<sectionHeader confidence="0.997858" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99983561764706">
Stochastic Unification-Based Grammars (SUBGs)
use log-linear models (also known as exponential or
MaxEnt models and Markov Random Fields) to de-
fine probability distributions over the parses of a uni-
fication grammar. These grammars can incorporate
virtually all kinds of linguistically important con-
straints (including non-local and non-context-free
constraints), and are equipped with a statistically
sound framework for estimation and learning.
Abney (1997) pointed out that the non-context-
free dependencies of a unification grammar require
stochastic models more general than Probabilis-
tic Context-Free Grammars (PCFGs) and Markov
Branching Processes, and proposed the use of log-
linear models for defining probability distributions
over the parses of a unification grammar. Un-
fortunately, the maximum likelihood estimator Ab-
ney proposed for SUBGs seems computationally in-
tractable since it requires statistics that depend on
the set of all parses of all strings generated by the
grammar. This set is infinite (so exhaustive enumer-
ation is impossible) and presumably has a very com-
plex structure (so sampling estimates might take an
extremely long time to converge).
Johnson et al. (1999) observed that parsing and
related tasks only require conditional distributions
over parses given strings, and that such conditional
distributions are considerably easier to estimate than
joint distributions of strings and their parses. The
conditional maximum likelihood estimator proposed
by Johnson et al. requires statistics that depend on
the set of all parses of the strings in the training cor-
pus. For most linguistically realistic grammars this
set is finite, and for moderate sized grammars and
training corpora this estimation procedure is quite
feasible.
However, our recent experiments involve training
from the Wall Street Journal Penn Tree-bank, and
repeatedly enumerating the parses of its 50,000 sen-
tences is quite time-consuming. Matters are only
made worse because we have moved some of the
constraints in the grammar from the unification com-
ponent to the stochastic component. This broadens
the coverage of the grammar, but at the expense of
massively expanding the number of possible parses
of each sentence.
In the mid-1990s unification-based parsers were
developed that do not enumerate all parses of a string
but instead manipulate and return a “packed” rep-
resentation of the set of parses. This paper de-
scribes how to find the most probable parse and
the statistics required for estimating a SUBG from
the packed parse set representations proposed by
Maxwell III and Kaplan (1995). This makes it pos-
sible to avoid explicitly enumerating the parses of
the strings in the training corpus.
The methods proposed here are analogues of
the well-known dynamic programming algorithms
for Probabilistic Context-Free Grammars (PCFGs);
specifically the Viterbi algorithm for finding the
most probable parse of a string, and the Inside-
Outside algorithm for estimating a PCFG from un-
parsed training data.&apos; In fact, because Maxwell and
Kaplan packed representations are just Truth Main-
tenance System (TMS) representations (Forbus and
de Kleer, 1993), the statistical techniques described
here should extend to non-linguistic applications of
TMSs as well.
Dynamic programming techniques have
been applied to log-linear models before.
Lafferty et al. (2001) mention that dynamic
programming can be used to compute the statistics
required for conditional estimation of log-linear
models based on context-free grammars where
the properties can include arbitrary functions of
the input string. Miyao and Tsujii (2002) (which
&apos;However, because we use conditional estimation, also
known as discriminative training, we require at least some dis-
criminating information about the correct parse of a string in
order to estimate a stochastic unification grammar.
appeared after this paper was accepted) is the closest
related work we know of. They describe a technique
for calculating the statistics required to estimate a
log-linear parsing model with non-local properties
from packed feature forests.
The rest of this paper is structured as follows.
The next section describes unification grammars
and Maxwell and Kaplan packed representation.
The following section reviews stochastic unifica-
tion grammars (Abney, 1997) and the statistical
quantities required for efficiently estimating such
grammars from parsed training data (Johnson et al.,
1999). The final substantive section of this paper
shows how these quantities can be defined directly
in terms of the Maxwell and Kaplan packed repre-
sentations.
The notation used in this paper is as follows. Vari-
ables are written in upper case italic, e.g., X, Y , etc.,
the sets they range over are written in script, e.g.,
X, Y, etc., while specific values are written in lower
case italic, e.g., x, y, etc. In the case of vector-valued
entities, subscripts indicate particular components.
</bodyText>
<sectionHeader confidence="0.842026" genericHeader="introduction">
2 Maxwell and Kaplan packed
representations
</sectionHeader>
<bodyText confidence="0.98707192">
This section characterises the properties of unifica-
tion grammars and the Maxwell and Kaplan packed
parse representations that will be important for what
follows. This characterisation omits many details
about unification grammars and the algorithm by
which the packed representations are actually con-
structed; see Maxwell III and Kaplan (1995) for de-
tails.
A parse generated by a unification grammar is a
finite subset of a set F of features. Features are parse
fragments, e.g., chart edges or arcs from attribute-
value structures, out of which the packed representa-
tions are constructed. For this paper it does not mat-
ter exactly what features are, but they are intended
to be the atomic entities manipulated by a dynamic
programming parsing algorithm. A grammar defines
a set Q of well-formed or grammatical parses. Each
parse w  Q is associated with a string of words
Y (w) called its yield. Note that except for trivial
grammars F and Q are infinite.
If y is a string, then let Q(y) = {w  Q|Y (w) =
y} and F(y) = UwEQ(Y){f  w}. That is, Q(y) is
the set of parses of a string y and F(y) is the set of
features appearing in the parses of y. In the gram-
mars of interest here Q(y) and hence also F(y) are
finite.
Maxwell and Kaplan’s packed representations of-
ten provide a more compact representation of the
set of parses of a sentence than would be obtained
by merely listing each parse separately. The intu-
ition behind these packed representations is that for
most strings y, many of the features in F(y) occur
in many of the parses Q(y). This is often the case
in natural language, since the same substructure can
appear as a component of many different parses.
Packed feature representations are defined in
terms of conditions on the values assigned to a vec-
tor of variables X. These variables have no direct
linguistic interpretation; rather, each different as-
signment of values to these variables identifies a set
of features which constitutes one of the parses in
the packed representation. A condition a on X is
a function from X to {0, 1}. While for uniformity
we write conditions as functions on the entire vec-
tor X, in practice Maxwell and Kaplan’s approach
produces conditions whose value depends only on a
few of the variables in X, and the efficiency of the
algorithms described here depends on this.
A packed representation of a finite set of parses is
a quadruple R = (F, X, N, a), where:
</bodyText>
<listItem confidence="0.998892">
• F D F(y) is a finite set of features,
• X is a finite vector of variables, where each
variable Xt ranges over the finite set Xt,
• N is a finite set of conditions on X called the
no-goods,2 and
• a is a function that maps each feature f E F
to a condition af on X.
</listItem>
<bodyText confidence="0.9792259">
A vector of values x satisfies the no-goods N iff
N(x) = 1, where N(x) = H,N q(x). Each x
that satisfies the no-goods identifies a parse w(x) =
{f E F|af(x) = 1}, i.e., w is the set of features
whose conditions are satisfied by x. We require that
each parse be identified by a unique value satisfying
2The name “no-good” comes from the TMS literature, and
was used by Maxwell and Kaplan. However, here the no-goods
actually identify the good variable assignments.
the no-goods. That is, we require that:
</bodyText>
<equation confidence="0.9715815">
bx, x E X if N(x) = N(x) = 1 and
w(x) = w(x) then x = x
</equation>
<bodyText confidence="0.99814225">
Finally, a packed representation R represents the
set of parses Q(R) that are identified by values
that satisfy the no-goods, i.e., Q(R) = {w(x)|x E
X, N(x) = 1}.
Maxwell III and Kaplan (1995) describes a pars-
ing algorithm for unification-based grammars that
takes as input a string y and returns a packed rep-
resentation R such that Q(R) = Q(y), i.e., R rep-
resents the set of parses of the string y. The SUBG
parsing and estimation algorithms described in this
paper use Maxwell and Kaplan’s parsing algorithm
as a subroutine.
</bodyText>
<sectionHeader confidence="0.96314" genericHeader="method">
3 Stochastic Unification-Based Grammars
</sectionHeader>
<bodyText confidence="0.999925">
This section reviews the probabilistic framework
used in SUBGs, and describes the statistics that
must be calculated in order to estimate the pa-
rameters of a SUBG from parsed training data.
For a more detailed exposition and descriptions
of regularization and other important details, see
Johnson et al. (1999).
The probability distribution over parses is defined
in terms of a finite vector g = (gi, ... , gm) of
properties. A property is a real-valued function of
parses Q. Johnson et al. (1999) placed no restric-
tions on what functions could be properties, permit-
ting properties to encode arbitrary global informa-
tion about a parse. However, the dynamic program-
ming algorithms presented here require the informa-
tion encoded in properties to be local with respect to
the features F used in the packed parse representa-
tion. Specifically, we require that properties be de-
fined on features rather than parses, i.e., each feature
f E F is associated with a finite vector of real values
(g1(f), ... , gm(f)) which define the property func-
tions for parses as follows:
</bodyText>
<equation confidence="0.828538">
gk(w) = � gk(f), for k = 1... m. (2)
fw
</equation>
<bodyText confidence="0.99987525">
That is, the property values of a parse are the sum
of the property values of its features. In the usual
case, some features will be associated with a single
property (i.e., gk(f) is equal to 1 for a specific value
</bodyText>
<equation confidence="0.936631">
 (1)
</equation>
<bodyText confidence="0.99997321875">
of k and 0 otherwise), and other features will be as-
sociated with no properties at all (i.e., g(f) = 0).
This requires properties be very local with re-
spect to features, which means that we give up the
ability to define properties arbitrarily. Note how-
ever that we can still encode essentially arbitrary
linguistic information in properties by adding spe-
cialised features to the underlying unification gram-
mar. For example, suppose we want a property that
indicates whether the parse contains a reduced rela-
tive clauses headed by a past participle (such “gar-
den path” constructions are grammatical but often
almost incomprehensible, and alternative parses not
including such constructions would probably be pre-
ferred). Under the current definition of properties,
we can introduce such a property by modifying the
underlying unification grammar to produce a certain
“diacritic” feature in a parse just in case the parse ac-
tually contains the appropriate reduced relative con-
struction. Thus, while properties are required to be
local relative to features, we can use the ability of
the underlying unification grammar to encode essen-
tially arbitrary non-local information in features to
introduce properties that also encode non-local in-
formation.
A Stochastic Unification-Based Grammar is a
triple (U, g, ), where U is a unification grammar
that defines a set Q of parses as described above,
g = (g1, ... , gm) is a vector of property functions as
just described, and  = (1, ... , m) is a vector of
non-negative real-valued parameters called property
weights. The probability P() of a parse  E Q is:
</bodyText>
<equation confidence="0.995630428571429">
W()
P() = , where:
Z
gj() and
j ,
Z =  W(�)
E
</equation>
<bodyText confidence="0.999848769230769">
Intuitively, if gj() is the number of times that prop-
erty j occurs in  then j is the ‘weight’ or ‘cost’ of
each occurrence of property j and Z is a normal-
ising constant that ensures that the probability of all
parses sums to 1.
Now we discuss the calculation of several impor-
tant quantities for SUBGs. In each case we show
that the quantity can be expressed as the value that
maximises a product of functions or else as the sum
of a product of functions, each of which depends
on a small subset of the variables X. These are the
kinds of quantities for which dynamic programming
graphical model algorithms have been developed.
</bodyText>
<subsectionHeader confidence="0.996316">
3.1 The most probable parse
</subsectionHeader>
<bodyText confidence="0.999886">
In parsing applications it is important to be able to
extract the most probable (or MAP) parse ˆ(y) of
string y with respect to a SUBG. This parse is:
</bodyText>
<equation confidence="0.997711">
ˆ(y) = argmax W()
E(y)
</equation>
<bodyText confidence="0.922767">
Given a packed representation (F�, X, N, ) for the
parses Q(y), let ˆx(y) be the x that identifies ˆ(y).
Since W(ˆ(y)) &gt; 0, it can be shown that:
</bodyText>
<equation confidence="0.9704935625">
ˆx(y) = argmax
xEX
f-(x) gj(f)
j
fF  f (x)gj (f)
j
f (x)gj(f)
j
 f(x)
gj(f) 
j
= argmax
xEX EN
where h,f(x) = m j=1 gj(f)
j if f(x) = 1 and
h,f(x) = 1 if f(x) = 0. Note that h,f(x) de-
</equation>
<bodyText confidence="0.999766666666667">
pends on exactly the same variables in X as f does.
As (3) makes clear, finding ˆx(y) involves maximis-
ing a product of functions where each function de-
pends on a subset of the variables X. As explained
below, this is exactly the kind of maximisation that
can be solved using graphical model techniques.
</bodyText>
<subsectionHeader confidence="0.999007">
3.2 Conditional likelihood
</subsectionHeader>
<bodyText confidence="0.99995225">
We now turn to the estimation of the property
weights  from a training corpus of parsed data D =
(1, ... , n). As explained in Johnson et al. (1999),
one way to do this is to find the  that maximises the
</bodyText>
<equation confidence="0.998591548387097">
m
W() = 
j=1
m

j=1
N(x)
gj((x))
j
= argmax
xEX
= argmax
xEX
= argmax
xEX
m
N(x) 
j=1
m
N(x) 
j=1
m
N(x) 
j=1

fE.P
= argmax  
xEX N(x) m
fE.P j=1
(x)  h,f(x) (3)
fE.P
</equation>
<bodyText confidence="0.999969666666667">
conditional likelihood of the training corpus parses
given their yields. (Johnson et al. actually maximise
conditional likelihood regularized with a Gaussian
prior, but for simplicity we ignore this here). If yi is
the yield of the parse i, the conditional likelihood
of the parses given their yields is:
</bodyText>
<equation confidence="0.86581975">
LD() =
where Q(y) is the set of parses with yield y and:
Z(S) =  W().
ES
</equation>
<bodyText confidence="0.958721461538462">
Then the maximum conditional likelihood estimate
ˆ of  is ˆ = argmax LD().
Now calculating W(i) poses no computational
problems, but since Q(yi) (the set of parses for yi)
can be large, calculating Z(Q(yi)) by enumerating
each  E Q(yi) can be computationally expensive.
However, there is an alternative method for calcu-
lating Z(Q(yi)) that does not involve this enumera-
tion. As noted above, for each yield yi, i = 1, ... , n,
Maxwell’s parsing algorithm returns a packed fea-
ture structure Ri that represents the parses of yi, i.e.,
Q(yi) = Q(Ri). A derivation parallel to the one for
(3) shows that for R = (P, X, N, ):
</bodyText>
<equation confidence="0.849888333333333">

Z(Q(R)) =
xEX
</equation>
<bodyText confidence="0.9993135">
(This derivation relies on the isomorphism between
parses and variable assignments in (1)). It turns out
that this type of sum can also be calculated using
graphical model techniques.
</bodyText>
<subsectionHeader confidence="0.999236">
3.3 Conditional Expectations
</subsectionHeader>
<bodyText confidence="0.99996375">
In general, iterative numerical procedures are re-
quired to find the property weights  that maximise
the conditional likelihood LD(). While there are
a number of different techniques that can be used,
all of the efficient techniques require the calculation
of conditional expectations E[gk|yi] for each prop-
erty gk and each sentence yi in the training corpus,
where:
</bodyText>
<equation confidence="0.99679825">
E[g|y] =  g()P(|y)
EQ(y)
EQ(y) g()W()
Z(Q(y))
</equation>
<bodyText confidence="0.93862725">
For example, the Conjugate Gradient algorithm,
which was used by Johnson et al., requires the cal-
culation not just of LD() but also its derivatives
LD()/k. It is straight-forward to show:
(gk(i) − E[gk|yi]) .
We have just described the calculation of LD(),
so if we can calculate E[gk|yi] then we can calcu-
late the partial derivatives required by the Conjugate
Gradient algorithm as well.
Again, let R = (P, X, N, ) be a packed repre-
sentation such that Q(R) = Q(yi). First, note that
(2) implies that:
</bodyText>
<equation confidence="0.9742195">
E[gk|yi] =  gk(f) P({ : f E }|yi).
fEF&apos;
</equation>
<bodyText confidence="0.9732896">
Note that P({ : f E }|yi) involves the sum of
weights over all x E X subject to the conditions
that N(x) = 1 and f(x) = 1. Thus P({ : f E
}|yi) can also be expressed in a form that is easy
to evaluate using graphical techniques.
</bodyText>
<equation confidence="0.978918">
Z(Q(R))P({ : f E }|yi)
= f(x)  (x)  h,f&apos;(x) (5)
xEX EN f&apos;EF&apos;
</equation>
<sectionHeader confidence="0.989704" genericHeader="method">
4 Graphical model calculations
</sectionHeader>
<bodyText confidence="0.999863444444444">
In this section we briefly review graphical model
algorithms for maximising and summing products
of functions of the kind presented above. It turns
out that the algorithm for maximisation is a gener-
alisation of the Viterbi algorithm for HMMs, and
the algorithm for computing the summation in (5)
is a generalisation of the forward-backward algo-
rithm for HMMs (Smyth et al., 1997). Viewed
abstractly, these algorithms simplify these expres-
sions by moving common factors over the max or
sum operators respectively. These techniques are
now relatively standard; the most well-known ap-
proach involves junction trees (Pearl, 1988; Cow-
ell, 1999). We adopt the approach approach de-
scribed by Geman and Kochanek (2000), which is
a straightforward generalization of HMM dynamic
programming with minimal assumptions and pro-
gramming overhead. However, in principle any of
</bodyText>
<equation confidence="0.860746625">
W(i)
Z(Q(yi))
n
i=1
 (x)  h,f(x) (4)
EN fEF&apos;
LD() LD() n
k k i=1
</equation>
<bodyText confidence="0.98884727027027">
the graphical model computational algorithms can
be used.
The quantities (3), (4) and (5) involve maximisa-
tion or summation over a product of functions, each
of which depends only on the values of a subset of
the variables X. There are dynamic programming
algorithms for calculating all of these quantities, but
for reasons of space we only describe an algorithm
for finding the maximum value of a product of func-
tions. These graph algorithms are rather involved.
It may be easier to follow if one reads Example 1
before or in parallel with the definitions below.
To explain the algorithm we use the following no-
tation. If x and x&apos; are both vectors of length m
then x =j x&apos; iff x and x&apos; disagree on at most their
jth components, i.e., xk = x&apos;k for k = 1,...,j −
1, j + 1,... m. If f is a function whose domain
is X, we say that f depends on the set of variables
d(f) = {Xj|Ix, x&apos; E X, x =j x&apos;, f(x) =� f(x&apos;)}.
That is, Xj E d(f) iff changing the value of Xj can
change the value of f.
The algorithm relies on the fact that the variables
in X = (X1, ... , Xn) are ordered (e.g., X1 pre-
cedes X2, etc.), and while the algorithm is correct
for any variable ordering, its efficiency may vary
dramatically depending on the ordering as described
below. Let H be any set of functions whose do-
mains are X. We partition H into disjoint subsets
H1, ... , Hn+1, where Hj is the subset of H that de-
pend on Xj but do not depend on any variables or-
dered before Xj, and Hn+1 is the subset of H that do
not depend on any variables at all (i.e., they are con-
stants).3 That is, Hj = {H E H|Xj E d(H), Vi &lt;
j Xi E� d(H)} and Hn+1 = {H E H|d(H) = 0}.
As explained in section 3.1, there is a set of func-
tions A such that the quantities we need to calculate
have the general form:
</bodyText>
<equation confidence="0.9859315">
Mmax = max
xEX AEA
xˆ = argmax A(x). (7)
xEX AEA
</equation>
<bodyText confidence="0.999654">
Mmax is the maximum value of the product expres-
sion while xˆ is the value of the variables at which the
maximum occurs. In a SUBG parsing application xˆ
identifies the MAP parse.
</bodyText>
<subsectionHeader confidence="0.513978">
3Strictly speaking this does not necessarily define a parti-
</subsectionHeader>
<bodyText confidence="0.965087388888889">
tion, as some of the subsets 9 j may be empty.
The procedure depends on two sequences of func-
tions Mi,i = 1,...,n + 1 and Vi,i = 1,...,n.
Informally, Mi is the maximum value attained by
the subset of the functions A that depend on one of
the variables X1, ... , Xi, and Vi gives information
about the value of Xi at which this maximum is at-
tained.
To simplify notation we write these functions as
functions of the entire set of variables X, but usu-
ally depend on a much smaller set of variables. The
Mi are real valued, while each Vi ranges over Xi.
Let M = {M1, ... , Mn}. Recall that the sets of
functions A and M can be both be partitioned into
disjoint subsets A1, ... , An+1 and M1, ... , Mn+1
respectively on the basis of the variables each Ai
and Mi depend on. The definition of the Mi and
Vi, i = 1, ... , n is as follows:
</bodyText>
<equation confidence="0.9627381">
Mi(x) = max
x&apos;EX
s.t. x&apos;=ix
Vi(x) = argmax
x&apos;EX
s.t. x&apos;=ix
Mn+1 receives a special definition, since there is no
variable Xn+1.
Mn+1 =  A)( M (9)
(AEAn+1 MEMn+1
</equation>
<bodyText confidence="0.999894">
The definition of Mi in (8) may look circular (since
M appears in the right-hand side), but in fact it is
not. First, note that Mi depends only on variables
ordered after Xi, so if Mj E Mi then j &lt; i. More
specifically,
</bodyText>
<equation confidence="0.981342">
d(Mi) =  d(A) U  d(M) \ {Xi}.
(AEAiMEMi
</equation>
<bodyText confidence="0.884286875">
Thus we can compute the
in the order
... , Mn+1, inserting
into the appropriate set
Mk, where k &gt; i, when
is computed.
We claim that
= Mn+1. (Note that Mn+1
</bodyText>
<footnote confidence="0.715849">
and Mn are constants, since there are no variables
ordered after Xn). To see this, consider the tree T
whose nodes are the
and which has a directed
edge from
to
iff
</footnote>
<equation confidence="0.424678">
E
(i.e.,
appears
</equation>
<bodyText confidence="0.942719">
in the right hand side of the definition (8) of
T has a unique root Mn+1, so there is a path fr
</bodyText>
<equation confidence="0.881006333333333">
Mi
M1,
Mi
Mi
Mmax
Mi,
Mi
Mj
Mi
Mj
Mi
Mj).
</equation>
<bodyText confidence="0.250396">
om
</bodyText>
<figure confidence="0.8743202">
A(x) (6)
 A(x&apos;)  M(x&apos;) (8)
AEAi MEMi
 A(x&apos;)  M(x&apos;)
AEAi MEMi
</figure>
<bodyText confidence="0.995922636363636">
every Mi to Mn+1. Let i --&lt; j iff there is a path
from Mi to Mj in this tree. Then a simple induction
shows that Mj is a function from d(Mj) to a max-
imisation over each of the variables Xi where i --&lt; j
of Fi�j,AEA, A.
Further, it is straightforward to show that Vi(ˆx) =
ˆxi (the value xˆ assigns to Xi). By the same argu-
ments as above, d(Vi) only contains variables or-
dered after Xi, so Vn = ˆxn. Thus we can evaluate
the Vi in the order Vn, ... , V1 to find the maximising
assignment ˆx.
</bodyText>
<construct confidence="0.965155285714286">
Example 1 Let X = { X1, X2, X3, X4, X5,
X6, X7} and set A = {a(X1, X3), b(X2, X4),
c(X3, X4, X5), d(X4, X5), e(X6, X7)}. We can
represent the sharing of variables in A by means of a
undirected graph !9A, where the nodes of !9A are the
variables X and there is an edge in !9A connecting
Xi to Xj iff ]A E A such that both Xi, Xj E d(A).
</construct>
<table confidence="0.974020875">
!9A is depicted below. X6
X1 X3 X5 
  
��
�
  � 
X2 X4
X7
</table>
<construct confidence="0.478598">
Starting with the variable X1, we compute M1
and V1:
</construct>
<equation confidence="0.99186475">
M1(x3) = max
x1EX1 a(x1, x3)
V1(x3) = argmax a(x1, x3)
x1EX1
</equation>
<bodyText confidence="0.756177">
We now proceed to the variable X2.
</bodyText>
<equation confidence="0.979138">
M2(x4) = max
x2EX2 b(x2, x4)
V2(x4) = argmax b(x2, x4)
x2EX2
</equation>
<construct confidence="0.7043355">
Since M1 belongs to M3, it appears in the definition
of M3.
</construct>
<equation confidence="0.99127575">
M3(x4, x5) = max
x3EX3 c(x3, x4, x5)M1(x3)
V3(x4, x5) = argmax c(x3, x4, x5)M1(x3)
x3EX3
Similarly, M4 is defined in terms of M2 and M3.
M4(x5) = max
x4EX4 d(x4, x5)M2(x4)M3(x4, x5)
V4(x5) = argmax d(x4, x5)M2(x4)M3(x4, x5)
</equation>
<page confidence="0.475382">
x4EX4
</page>
<note confidence="0.645882333333333">
Note that M5 is a constant, reflecting the fact that
in !9A the node X5 is not connected to any node or-
dered after it.
</note>
<equation confidence="0.975478846153846">
M5 = max
x5EX5
V5 = argmax M4(x5)
x5EX5
The second component is defined in the same way:
M6(x7) = max
x6EX6 e(x6, x7)
V6(x7) = argmax e(x6, x7)
x6EX6
M7 = max
x7EX7 M6(x7)
V7 = argmax M6(x7)
x7EX7
</equation>
<bodyText confidence="0.986729">
The maximum value for the product M8 = Mmax is
defined in terms of M5 and M7.
</bodyText>
<equation confidence="0.979495">
Mmax = M8 = M5M7
</equation>
<bodyText confidence="0.9798575">
Finally, we evaluate V7, ... , V1 to find the maximis-
ing assignment ˆx.
</bodyText>
<equation confidence="0.999849">
ˆx7 = V7
ˆx6 = V6(ˆx7)
ˆx5 = V5
ˆx4 = V4(ˆx5)
ˆx3 = V3(ˆx4,ˆx5)
ˆx2 = V2(ˆx4)
ˆx1 = V1(ˆx3)
</equation>
<bodyText confidence="0.999992733333333">
We now briefly consider the computational com-
plexity of this process. Clearly, the number of steps
required to compute each Mi is a polynomial of or-
der |d(Mi) |+ 1, since we need to enumerate all pos-
sible values for the argument variables d(Mi) and
for each of these, maximise over the set Xi. Fur-
ther, it is easy to show that in terms of the graph !9A,
d(Mj) consists of those variables Xk, k &gt; j reach-
able by a path starting at Xj and all of whose nodes
except the last are variables that precede Xj.
Since computational effort is bounded above by a
polynomial of order |d(Mi) |+ 1, we seek a variable
ordering that bounds the maximum value of |d(Mi)|.
Unfortunately, finding the ordering that minimises
the maximum value of |d(Mi) |is an NP-complete
</bodyText>
<equation confidence="0.547054">
M4(x5)
</equation>
<bodyText confidence="0.9998872">
problem. However, there are several efficient heuris-
tics that are reputed in graphical models community
to produce good visitation schedules. It may be that
they will perform well in the SUBG parsing applica-
tions as well.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999995705882353">
This paper shows how to apply dynamic program-
ming methods developed for graphical models to
SUBGs to find the most probable parse and to ob-
tain the statistics needed for estimation directly from
Maxwell and Kaplan packed parse representations.
i.e., without expanding these into individual parses.
The algorithm rests on the observation that so long
as features are local to the parse fragments used in
the packed representations, the statistics required for
parsing and estimation are the kinds of quantities
that dynamic programming algorithms for graphical
models can perform. Since neither Maxwell and Ka-
plan’s packed parsing algorithm nor the procedures
described here depend on the details of the underly-
ing linguistic theory, the approach should apply to
virtually any kind of underlying grammar.
Obviously, an empirical evaluation of the algo-
rithms described here would be extremely useful.
The algorithms described here are exact, but be-
cause we are working with unification grammars
and apparently arbitrary graphical models we can-
not polynomially bound their computational com-
plexity. However, it seems reasonable to expect
that if the linguistic dependencies in a sentence typ-
ically factorize into largely non-interacting cliques
then the dynamic programming methods may offer
dramatic computational savings compared to current
methods that enumerate all possible parses.
It might be interesting to compare these dy-
namic programming algorithms with a standard
unification-based parser using a best-first search
heuristic. (To our knowledge such an approach has
not yet been explored, but it seems straightforward:
the figure of merit could simply be the sum of the
weights of the properties of each partial parse’s frag-
ments). Because such parsers prune the search space
they cannot guarantee correct results, unlike the al-
gorithms proposed here. Such a best-first parser
might be accurate when parsing with a trained gram-
mar, but its results may be poor at the beginning
of parameter weight estimation when the parameter
weight estimates are themselves inaccurate.
Finally, it would be extremely interesting to com-
pare these dynamic programming algorithms to
the ones described by Miyao and Tsujii (2002). It
seems that the Maxwell and Kaplan packed repre-
sentation may permit more compact representations
than the disjunctive representations used by Miyao
et al., but this does not imply that the algorithms
proposed here are more efficient. Further theoreti-
cal and empirical investigation is required.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936486486486">
Steven Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23(4):597–617.
Robert Cowell. 1999. Introduction to inference for Bayesian
networks. In Michael Jordan, editor, Learning in Graphi-
cal Models, pages 9–26. The MIT Press, Cambridge, Mas-
sachusetts.
Kenneth D. Forbus and Johan de Kleer. 1993. Building problem
solvers. The MIT Press, Cambridge, Massachusetts.
Stuart Geman and Kevin Kochanek. 2000. Dynamic program-
ming and the representation of soft-decodable codes. Tech-
nical report, Division of Applied Mathematics, Brown Uni-
versity.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic “unification-
based” grammars. In The Proceedings of the 37th Annual
Conference of the Association for Computational Linguis-
tics, pages 535–541, San Francisco. Morgan Kaufmann.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Machine Learn-
ing: Proceedings of the Eighteenth International Conference
(ICML 2001), Stanford, California.
John T. Maxwell III and Ronald M. Kaplan. 1995. A method
for disjunctive constraint satisfaction. In Mary Dalrymple,
Ronald M. Kaplan, John T. Maxwell III, and Annie Zae-
nen, editors, Formal Issues in Lexical-Functional Grammar,
number 47 in CSLI Lecture Notes Series, chapter 14, pages
381–481. CSLI Publications.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy
estimation for feature forests. In Proceedings of Human
Language Technology Conference 2002, March.
Judea Pearl. 1988. Probabalistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference. Morgan Kaufmann,
San Mateo, California.
Padhraic Smyth, David Heckerman, and Michael Jordan. 1997.
Probabilistic Independence Networks for Hidden Markov
Models. Neural Computation, 9(2):227–269.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289295">
<note confidence="0.9979805">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 279-286.</note>
<title confidence="0.7044055">Dynamic programming for parsing and estimation of unification-based</title>
<author confidence="0.994692">Stuart Geman</author>
<affiliation confidence="0.9992155">Division of Applied Mathematics Brown University</affiliation>
<email confidence="0.998889">geman@dam.brown.edu</email>
<author confidence="0.999992">Mark Johnson</author>
<affiliation confidence="0.9775905">Cognitive and Linguistic Sciences Brown University</affiliation>
<author confidence="0.997908">Mark JohnsonBrown edu</author>
<abstract confidence="0.979564533333333">Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unificationbased grammar (UBG). Existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one, or in order to calculate the statistics needed to estimate a grammar from a training corpus. This paper describes a graph-based dynamic programming algorithm for calculating these statistics from the packed UBG parse representations of Maxwell and Kaplan (1995) which does not require enumerating all parses. Like many graphical algorithms, the dynamic programming algorithm’s complexity is worst-case exponential, but is often polynomial. The key observation is that by using Maxwell and Kaplan packed representations, the required statistics can be rewritten as either the max or the sum of a product of functions. This is exactly the kind of problem which can be solved by dynamic programming over graphical models. would like to thank Eugene Charniak, Miyao Yusuke, Mark Steedman as well as Stefan Riezler and the team naturally all errors remain our own. This research was</abstract>
<note confidence="0.698889">supported by NSF awards DMS 0074276 and ITR IIS 0085940.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Stochastic Attribute-Value Grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="2111" citStr="Abney (1997)" startWordPosition="307" endWordPosition="308">zler and the team at PARC; naturally all errors remain our own. This research was supported by NSF awards DMS 0074276 and ITR IIS 0085940. 1 Introduction Stochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar. These grammars can incorporate virtually all kinds of linguistically important constraints (including non-local and non-context-free constraints), and are equipped with a statistically sound framework for estimation and learning. Abney (1997) pointed out that the non-contextfree dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of loglinear models for defining probability distributions over the parses of a unification grammar. Unfortunately, the maximum likelihood estimator Abney proposed for SUBGs seems computationally intractable since it requires statistics that depend on the set of all parses of all strings generated by the grammar. This set is infinite (so exhaustive enumeration is impossible) and p</context>
<context position="5961" citStr="Abney, 1997" startWordPosition="887" endWordPosition="888"> training, we require at least some discriminating information about the correct parse of a string in order to estimate a stochastic unification grammar. appeared after this paper was accepted) is the closest related work we know of. They describe a technique for calculating the statistics required to estimate a log-linear parsing model with non-local properties from packed feature forests. The rest of this paper is structured as follows. The next section describes unification grammars and Maxwell and Kaplan packed representation. The following section reviews stochastic unification grammars (Abney, 1997) and the statistical quantities required for efficiently estimating such grammars from parsed training data (Johnson et al., 1999). The final substantive section of this paper shows how these quantities can be defined directly in terms of the Maxwell and Kaplan packed representations. The notation used in this paper is as follows. Variables are written in upper case italic, e.g., X, Y , etc., the sets they range over are written in script, e.g., X, Y, etc., while specific values are written in lower case italic, e.g., x, y, etc. In the case of vector-valued entities, subscripts indicate partic</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven Abney. 1997. Stochastic Attribute-Value Grammars. Computational Linguistics, 23(4):597–617.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Cowell</author>
</authors>
<title>Introduction to inference for Bayesian networks.</title>
<date>1999</date>
<booktitle>Learning in Graphical Models,</booktitle>
<pages>9--26</pages>
<editor>In Michael Jordan, editor,</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="18547" citStr="Cowell, 1999" startWordPosition="3122" endWordPosition="3124"> review graphical model algorithms for maximising and summing products of functions of the kind presented above. It turns out that the algorithm for maximisation is a generalisation of the Viterbi algorithm for HMMs, and the algorithm for computing the summation in (5) is a generalisation of the forward-backward algorithm for HMMs (Smyth et al., 1997). Viewed abstractly, these algorithms simplify these expressions by moving common factors over the max or sum operators respectively. These techniques are now relatively standard; the most well-known approach involves junction trees (Pearl, 1988; Cowell, 1999). We adopt the approach approach described by Geman and Kochanek (2000), which is a straightforward generalization of HMM dynamic programming with minimal assumptions and programming overhead. However, in principle any of W(i) Z(Q(yi)) n i=1  (x)  h,f(x) (4) EN fEF&apos; LD() LD() n k k i=1 the graphical model computational algorithms can be used. The quantities (3), (4) and (5) involve maximisation or summation over a product of functions, each of which depends only on the values of a subset of the variables X. There are dynamic programming algorithms for calculating all of these q</context>
</contexts>
<marker>Cowell, 1999</marker>
<rawString>Robert Cowell. 1999. Introduction to inference for Bayesian networks. In Michael Jordan, editor, Learning in Graphical Models, pages 9–26. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth D Forbus</author>
<author>Johan de Kleer</author>
</authors>
<title>Building problem solvers.</title>
<date>1993</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Forbus, de Kleer, 1993</marker>
<rawString>Kenneth D. Forbus and Johan de Kleer. 1993. Building problem solvers. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Kevin Kochanek</author>
</authors>
<title>Dynamic programming and the representation of soft-decodable codes.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Division of Applied Mathematics, Brown University.</institution>
<contexts>
<context position="18618" citStr="Geman and Kochanek (2000)" startWordPosition="3133" endWordPosition="3136">ng products of functions of the kind presented above. It turns out that the algorithm for maximisation is a generalisation of the Viterbi algorithm for HMMs, and the algorithm for computing the summation in (5) is a generalisation of the forward-backward algorithm for HMMs (Smyth et al., 1997). Viewed abstractly, these algorithms simplify these expressions by moving common factors over the max or sum operators respectively. These techniques are now relatively standard; the most well-known approach involves junction trees (Pearl, 1988; Cowell, 1999). We adopt the approach approach described by Geman and Kochanek (2000), which is a straightforward generalization of HMM dynamic programming with minimal assumptions and programming overhead. However, in principle any of W(i) Z(Q(yi)) n i=1  (x)  h,f(x) (4) EN fEF&apos; LD() LD() n k k i=1 the graphical model computational algorithms can be used. The quantities (3), (4) and (5) involve maximisation or summation over a product of functions, each of which depends only on the values of a subset of the variables X. There are dynamic programming algorithms for calculating all of these quantities, but for reasons of space we only describe an algorithm for f</context>
</contexts>
<marker>Geman, Kochanek, 2000</marker>
<rawString>Stuart Geman and Kevin Kochanek. 2000. Dynamic programming and the representation of soft-decodable codes. Technical report, Division of Applied Mathematics, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unificationbased” grammars.</title>
<date>1999</date>
<booktitle>In The Proceedings of the 37th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>535--541</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="2842" citStr="Johnson et al. (1999)" startWordPosition="417" endWordPosition="420">eral than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of loglinear models for defining probability distributions over the parses of a unification grammar. Unfortunately, the maximum likelihood estimator Abney proposed for SUBGs seems computationally intractable since it requires statistics that depend on the set of all parses of all strings generated by the grammar. This set is infinite (so exhaustive enumeration is impossible) and presumably has a very complex structure (so sampling estimates might take an extremely long time to converge). Johnson et al. (1999) observed that parsing and related tasks only require conditional distributions over parses given strings, and that such conditional distributions are considerably easier to estimate than joint distributions of strings and their parses. The conditional maximum likelihood estimator proposed by Johnson et al. requires statistics that depend on the set of all parses of the strings in the training corpus. For most linguistically realistic grammars this set is finite, and for moderate sized grammars and training corpora this estimation procedure is quite feasible. However, our recent experiments in</context>
<context position="6091" citStr="Johnson et al., 1999" startWordPosition="903" endWordPosition="906"> stochastic unification grammar. appeared after this paper was accepted) is the closest related work we know of. They describe a technique for calculating the statistics required to estimate a log-linear parsing model with non-local properties from packed feature forests. The rest of this paper is structured as follows. The next section describes unification grammars and Maxwell and Kaplan packed representation. The following section reviews stochastic unification grammars (Abney, 1997) and the statistical quantities required for efficiently estimating such grammars from parsed training data (Johnson et al., 1999). The final substantive section of this paper shows how these quantities can be defined directly in terms of the Maxwell and Kaplan packed representations. The notation used in this paper is as follows. Variables are written in upper case italic, e.g., X, Y , etc., the sets they range over are written in script, e.g., X, Y, etc., while specific values are written in lower case italic, e.g., x, y, etc. In the case of vector-valued entities, subscripts indicate particular components. 2 Maxwell and Kaplan packed representations This section characterises the properties of unification grammars and</context>
<context position="10742" citStr="Johnson et al. (1999)" startWordPosition="1731" endWordPosition="1734">at takes as input a string y and returns a packed representation R such that Q(R) = Q(y), i.e., R represents the set of parses of the string y. The SUBG parsing and estimation algorithms described in this paper use Maxwell and Kaplan’s parsing algorithm as a subroutine. 3 Stochastic Unification-Based Grammars This section reviews the probabilistic framework used in SUBGs, and describes the statistics that must be calculated in order to estimate the parameters of a SUBG from parsed training data. For a more detailed exposition and descriptions of regularization and other important details, see Johnson et al. (1999). The probability distribution over parses is defined in terms of a finite vector g = (gi, ... , gm) of properties. A property is a real-valued function of parses Q. Johnson et al. (1999) placed no restrictions on what functions could be properties, permitting properties to encode arbitrary global information about a parse. However, the dynamic programming algorithms presented here require the information encoded in properties to be local with respect to the features F used in the packed parse representation. Specifically, we require that properties be defined on features rather than parses, i</context>
<context position="15100" citStr="Johnson et al. (1999)" startWordPosition="2512" endWordPosition="2515">j(f) j  f(x) gj(f)  j = argmax xEX EN where h,f(x) = m j=1 gj(f) j if f(x) = 1 and h,f(x) = 1 if f(x) = 0. Note that h,f(x) depends on exactly the same variables in X as f does. As (3) makes clear, finding ˆx(y) involves maximising a product of functions where each function depends on a subset of the variables X. As explained below, this is exactly the kind of maximisation that can be solved using graphical model techniques. 3.2 Conditional likelihood We now turn to the estimation of the property weights  from a training corpus of parsed data D = (1, ... , n). As explained in Johnson et al. (1999), one way to do this is to find the  that maximises the m W() =  j=1 m  j=1 N(x) gj((x)) j = argmax xEX = argmax xEX = argmax xEX m N(x)  j=1 m N(x)  j=1 m N(x)  j=1  fE.P = argmax   xEX N(x) m fE.P j=1 (x)  h,f(x) (3) fE.P conditional likelihood of the training corpus parses given their yields. (Johnson et al. actually maximise conditional likelihood regularized with a Gaussian prior, but for simplicity we ignore this here). If yi is the yield of the parse i, the conditional likelihood of the parses given their yields is: LD() = where Q(y) is the set of parses with yield y</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unificationbased” grammars. In The Proceedings of the 37th Annual Conference of the Association for Computational Linguistics, pages 535–541, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Machine Learning: Proceedings of the Eighteenth International Conference (ICML 2001),</booktitle>
<location>Stanford, California.</location>
<contexts>
<context position="5007" citStr="Lafferty et al. (2001)" startWordPosition="746" endWordPosition="749">alogues of the well-known dynamic programming algorithms for Probabilistic Context-Free Grammars (PCFGs); specifically the Viterbi algorithm for finding the most probable parse of a string, and the InsideOutside algorithm for estimating a PCFG from unparsed training data.&apos; In fact, because Maxwell and Kaplan packed representations are just Truth Maintenance System (TMS) representations (Forbus and de Kleer, 1993), the statistical techniques described here should extend to non-linguistic applications of TMSs as well. Dynamic programming techniques have been applied to log-linear models before. Lafferty et al. (2001) mention that dynamic programming can be used to compute the statistics required for conditional estimation of log-linear models based on context-free grammars where the properties can include arbitrary functions of the input string. Miyao and Tsujii (2002) (which &apos;However, because we use conditional estimation, also known as discriminative training, we require at least some discriminating information about the correct parse of a string in order to estimate a stochastic unification grammar. appeared after this paper was accepted) is the closest related work we know of. They describe a techniqu</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In Machine Learning: Proceedings of the Eighteenth International Conference (ICML 2001), Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John T Maxwell</author>
<author>Ronald M Kaplan</author>
</authors>
<title>A method for disjunctive constraint satisfaction.</title>
<date>1995</date>
<booktitle>Formal Issues in Lexical-Functional Grammar, number 47 in CSLI Lecture Notes Series, chapter 14,</booktitle>
<pages>381--481</pages>
<editor>In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="950" citStr="Maxwell and Kaplan (1995)" startWordPosition="128" endWordPosition="131">e and Linguistic Sciences Brown University Mark Johnson@Brown.edu Abstract Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unificationbased grammar (UBG). Existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one, or in order to calculate the statistics needed to estimate a grammar from a training corpus. This paper describes a graph-based dynamic programming algorithm for calculating these statistics from the packed UBG parse representations of Maxwell and Kaplan (1995) which does not require enumerating all parses. Like many graphical algorithms, the dynamic programming algorithm’s complexity is worst-case exponential, but is often polynomial. The key observation is that by using Maxwell and Kaplan packed representations, the required statistics can be rewritten as either the max or the sum of a product of functions. This is exactly the kind of problem which can be solved by dynamic programming over graphical models. * We would like to thank Eugene Charniak, Miyao Yusuke, Mark Steedman as well as Stefan Riezler and the team at PARC; naturally all errors rem</context>
</contexts>
<marker>Maxwell, Kaplan, 1995</marker>
<rawString>John T. Maxwell III and Ronald M. Kaplan. 1995. A method for disjunctive constraint satisfaction. In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors, Formal Issues in Lexical-Functional Grammar, number 47 in CSLI Lecture Notes Series, chapter 14, pages 381–481. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology Conference</booktitle>
<contexts>
<context position="5264" citStr="Miyao and Tsujii (2002)" startWordPosition="783" endWordPosition="786">d training data.&apos; In fact, because Maxwell and Kaplan packed representations are just Truth Maintenance System (TMS) representations (Forbus and de Kleer, 1993), the statistical techniques described here should extend to non-linguistic applications of TMSs as well. Dynamic programming techniques have been applied to log-linear models before. Lafferty et al. (2001) mention that dynamic programming can be used to compute the statistics required for conditional estimation of log-linear models based on context-free grammars where the properties can include arbitrary functions of the input string. Miyao and Tsujii (2002) (which &apos;However, because we use conditional estimation, also known as discriminative training, we require at least some discriminating information about the correct parse of a string in order to estimate a stochastic unification grammar. appeared after this paper was accepted) is the closest related work we know of. They describe a technique for calculating the statistics required to estimate a log-linear parsing model with non-local properties from packed feature forests. The rest of this paper is structured as follows. The next section describes unification grammars and Maxwell and Kaplan p</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceedings of Human Language Technology Conference 2002, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabalistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="18532" citStr="Pearl, 1988" startWordPosition="3120" endWordPosition="3121">on we briefly review graphical model algorithms for maximising and summing products of functions of the kind presented above. It turns out that the algorithm for maximisation is a generalisation of the Viterbi algorithm for HMMs, and the algorithm for computing the summation in (5) is a generalisation of the forward-backward algorithm for HMMs (Smyth et al., 1997). Viewed abstractly, these algorithms simplify these expressions by moving common factors over the max or sum operators respectively. These techniques are now relatively standard; the most well-known approach involves junction trees (Pearl, 1988; Cowell, 1999). We adopt the approach approach described by Geman and Kochanek (2000), which is a straightforward generalization of HMM dynamic programming with minimal assumptions and programming overhead. However, in principle any of W(i) Z(Q(yi)) n i=1  (x)  h,f(x) (4) EN fEF&apos; LD() LD() n k k i=1 the graphical model computational algorithms can be used. The quantities (3), (4) and (5) involve maximisation or summation over a product of functions, each of which depends only on the values of a subset of the variables X. There are dynamic programming algorithms for calculating</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabalistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
<author>David Heckerman</author>
<author>Michael Jordan</author>
</authors>
<title>Probabilistic Independence Networks for Hidden Markov Models.</title>
<date>1997</date>
<journal>Neural Computation,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="18287" citStr="Smyth et al., 1997" startWordPosition="3083" endWordPosition="3086">that N(x) = 1 and f(x) = 1. Thus P({ : f E }|yi) can also be expressed in a form that is easy to evaluate using graphical techniques. Z(Q(R))P({ : f E }|yi) = f(x)  (x)  h,f&apos;(x) (5) xEX EN f&apos;EF&apos; 4 Graphical model calculations In this section we briefly review graphical model algorithms for maximising and summing products of functions of the kind presented above. It turns out that the algorithm for maximisation is a generalisation of the Viterbi algorithm for HMMs, and the algorithm for computing the summation in (5) is a generalisation of the forward-backward algorithm for HMMs (Smyth et al., 1997). Viewed abstractly, these algorithms simplify these expressions by moving common factors over the max or sum operators respectively. These techniques are now relatively standard; the most well-known approach involves junction trees (Pearl, 1988; Cowell, 1999). We adopt the approach approach described by Geman and Kochanek (2000), which is a straightforward generalization of HMM dynamic programming with minimal assumptions and programming overhead. However, in principle any of W(i) Z(Q(yi)) n i=1  (x)  h,f(x) (4) EN fEF&apos; LD() LD() n k k i=1 the graphical model computational alg</context>
</contexts>
<marker>Smyth, Heckerman, Jordan, 1997</marker>
<rawString>Padhraic Smyth, David Heckerman, and Michael Jordan. 1997. Probabilistic Independence Networks for Hidden Markov Models. Neural Computation, 9(2):227–269.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>