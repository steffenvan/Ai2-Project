<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.996814">
Maximum Expected BLEU Training of Phrase and Lexicon
Translation Models
</title>
<author confidence="0.999289">
Xiaodong He Li Deng
</author>
<affiliation confidence="0.918691">
Microsoft Research Microsoft Research
One Microsoft Way, Redmond, WA, USA One Microsoft Way, Redmond, WA, USA
</affiliation>
<email confidence="0.996968">
xiaohe@microsoft.com deng@microsoft.com
</email>
<sectionHeader confidence="0.994742" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983578947368">
This paper proposes a new discriminative
training method in constructing phrase and
lexicon translation models. In order to
reliably learn a myriad of parameters in
these models, we propose an expected
BLEU score-based utility function with KL
regularization as the objective, and train the
models on a large parallel dataset. For
training, we derive growth transformations
for phrase and lexicon translation
probabilities to iteratively improve the
objective. The proposed method, evaluated
on the Europarl German-to-English dataset,
leads to a 1.1 BLEU point improvement
over a state-of-the-art baseline translation
system. In IWSLT 2011 Benchmark, our
system using the proposed method achieves
the best Chinese-to-English translation
result on the task of translating TED talks.
</bodyText>
<sectionHeader confidence="0.997653" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99994075925926">
Discriminative training is an active area in
statistical machine translation (SMT) (e.g., Och et
al., 2002, 2003, Liang et al., 2006, Blunsom et al.,
2008, Chiang et al., 2009, Foster et al, 2010, Xiao
et al. 2011). Och (2003) proposed using a log-
linear model to incorporate multiple features for
translation, and proposed a minimum error rate
training (MERT) method to train the feature
weights to optimize a desirable translation metric.
While the log-linear model itself is
discriminative, the phrase and lexicon translation
features, which are among the most important
components of SMT, are derived from either
generative models or heuristics (Koehn et al.,
2003, Brown et al., 1993). Moreover, the
parameters in the phrase and lexicon translation
models are estimated by relative frequency or
maximizing joint likelihood, which may not
correspond closely to the translation measure, e.g.,
bilingual evaluation understudy (BLEU) (Papineni
et al., 2002). Therefore, it is desirable to train all
these parameters to directly maximize an objective
that directly links to translation quality.
However, there are a large number of
parameters in these models, making discriminative
training for them non-trivial (e.g., Liang et al.,
2006, Chiang et al., 2009). Liang et al. (2006)
proposed a large set of lexical and Part-of-Speech
features and trained the model weights associated
with these features using perceptron. Since many
of the reference translations are non-reachable, an
empirical local updating strategy had to be devised
to fix this problem by picking a pseudo reference.
Many such non-desirable heuristics led to
moderate gains reported in that work. Chiang et al.
(2009) improved a syntactic SMT system by
adding as many as ten thousand syntactic features,
and used Margin Infused Relaxed Algorithm
(MIRA) to train the feature weights. However, the
number of parameters in common phrase and
lexicon translation models is much larger.
In this work, we present a new, highly effective
discriminative learning method for phrase and
lexicon translation models. The training objective
is an expected BLEU score, which is closely linked
to translation quality. Further, we apply a
Kullback‚ÄìLeibler (KL) divergence regularization
to prevent over-fitting.
For effective optimization, we derive updating
formulas of growth transformation (GT) for phrase
and lexicon translation probabilities. A GT is a
transformation of the probabilities that guarantees
strict non-decrease of the objective over each GT
iteration unless a local maximum is reached. A
</bodyText>
<page confidence="0.916042">
292
</page>
<note confidence="0.9863925">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 292‚Äì301,
Jeju, Republic of Korea, 8-14 July 2012. cÔøΩ2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.98525315">
similar GT technique has been successfully used in
speech recognition (Gopalakrishnan et al., 1991,
Povey, 2004, He et al., 2008). Our work
demonstrates that it works with large scale
discriminative training of SMT model as well.
Our work is based on a phrase-based SMT
system. Experiments on the Europarl German-to-
English dataset show that the proposed method
leads to a 1.1 BLEU point improvement over a
strong baseline. The proposed method is also
successfully evaluated on the IWSLT 2011
benchmark test set, where the task is to translate
TED talks (www.ted.com). Our experimental
results on this open-domain spoken language
translation task show that the proposed method
leads to significant translation performance
improvement over a state-of-the-art baseline, and
the system using the proposed method achieved the
best single system translation result in the Chinese-
to-English MT track.
</bodyText>
<sectionHeader confidence="0.999859" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.99999102739726">
One best known approach in discriminative
training for SMT is proposed by Och (2003). In
that work, multiple features, most of them are
derived from generative models, are incorporated
into a log-linear model, and the relative weights of
them are tuned discriminatively on a small tuning
set. However, in practice, this approach only works
with a handful of parameters.
More closely related to our work, Liang et al.
(2006) proposed a large set of lexical and Part-of-
Speech features in addition to the phrase
translation model. Weights of these features are
trained using perceptron on a training set of 67K
sentences. In that paper, the authors pointed out
that forcing the model to update towards the
reference translation could be problematic. This is
because the hidden structure such as phrase
segmentation and alignment could be abused if the
system is forced to produce a reference translation.
Therefore, instead of pushing the parameter update
towards the reference translation (a.k.a. bold
updating), the author proposed a local updating
strategy where the model parameters are updated
towards a pseudo-reference (i.e., the hypothesis in
the n-best list that gives the best BLEU score).
Experimental results showed that their approach
outperformed a baseline by 0.8 BLEU point when
using monotonic decoding, but there was no
significant gain over a stronger baseline with a
full-distortion model. In our work, we use the
expectation of BLEU scores as the objective. This
avoids the heuristics of picking the updating
reference and therefore gives a more principal way
of setting the training objective.
As another closely related study, Chiang et al.
(2009) incorporated about ten thousand syntactic
features in addition to the baseline features. The
feature weights are trained on a tuning set with
2010 sentences using MIRA. In our work, we have
many more parameters to train, and the training is
conducted on the entire training corpora. Our GT
based optimization algorithm is highly
parallelizable and efficient, which is the key for
large scale discriminative training.
As a further related work, Rosti et al. (2011)
have proposed using differentiable expected BLEU
score as the objective to train system combination
parameters. Other work related to the computation
of expected BLEU in common with ours includes
minimum Bayes risk approaches (Smith and Eisner
2006, Tromble et al., 2008) and lattice-based
MERT (Macherey et al., 2008). In these earlier work,
however, the phrase and lexicon translation models
used remained unchanged.
Another line of research that is closely related to
our work is phrase table refinement and pruning.
Wuebker et al. (2010) proposed a method to train
the phrase translation model using Expectation-
Maximization algorithm with a leave-one-out
strategy. The parallel sentences were forced to be
aligned at the phrase level using the phrase table
and other features as in a decoding process. Then
the phrase translation probabilities were estimated
based on the phrase alignments. To prevent
overfitting, the statistics of phrase pairs from a
particular sentence was excluded from the phrase
table when aligning that sentence. However, as
pointed out by Liang et al (2006), the same
problem as in the bold updating existed, i.e., forced
alignment between a source sentence and its
reference translation was tricky, and the proposed
alignment was likely to be unreliable. The method
presented in this paper is free from this problem.
</bodyText>
<sectionHeader confidence="0.718788" genericHeader="method">
3. Phrase-based Translation System
</sectionHeader>
<bodyText confidence="0.987077">
The translation process of phrase-based SMT can
be briefly described in three steps: segment source
sentence into a sequence of phrases, translate each
</bodyText>
<page confidence="0.997272">
293
</page>
<bodyText confidence="0.9990606">
source phrase to a target phrase, re-order target
phrases into target sentence (Koehn et al., 2003).
In decoding, the optimal translation ùê∏ given the
source sentence F is obtained according to
where
</bodyText>
<subsectionHeader confidence="0.986415">
3.2. Lexicon translation model
</subsectionHeader>
<bodyText confidence="0.999030125">
There are several variations in lexicon translation
features (Ayan and Dorr 2006, Koehn et al., 2003,
Quirk et al., 2005). We use the word translation
table from IBM Model 1 (Brown et al., 1993) and
compute the sum over all possible word alignments
within a phrase pair without normalizing for length
(Quirk et al., 2005). The source-to-target forward
lexicon (FL) translation feature is:
</bodyText>
<equation confidence="0.9424105">
‚ÑéFL ùê∏, ùêπ = ùëù ùëík,m ùëìk,r
1
ùëç ùëíùë•ùëù ùúÜmlog ‚Ñém(ùê∏, ùêπ)
ùëÉùê∏ùêπ=
ùê∏ = argmax
E
ùëÉ ùê∏ ùêπ (1)
(2)
(5)
and ùëç= E ùëíùë•ùëù m ùúÜmlog ‚Ñém(ùê∏, ùêπ) is the
</equation>
<bodyText confidence="0.9947835">
normalization denominator to ensure that the
probabilities sum to one. Note that we define the
feature functions {‚Ñém(ùê∏, ùêπ)} in log domain to
simplify the notation in later sections. Feature
weights ùõå = [ùúÜm) are usually tuned by MERT.
Features used in a phrase-based system usually
include LM, reordering model, word and phrase
counts, and phrase and lexicon translation models.
Given the focus of this paper, we review only the
phrase and lexicon translation models below.
</bodyText>
<subsectionHeader confidence="0.979197">
3.1. Phrase translation model
</subsectionHeader>
<bodyText confidence="0.995380625">
A set of phrase pairs are extracted from word-
aligned parallel corpus according to phrase
extraction rules (Koehn et al., 2003). Phrase
translation probabilities are then computed as
relative frequencies of phrases over the training
dataset. i.e., the probability of translating a source
phrase ùëì to a target phrase ùëí is computed by
where ùê∂ (ùëí, ùëì) is the joint counts of ùëí and ùëì , and
</bodyText>
<equation confidence="0.855224">
ùê∂ (ùëì) is the marginal counts of ùëì.
</equation>
<bodyText confidence="0.999907285714286">
In translation, the input sentence is segmented
into K phrases, and the source-to-target forward
phrase (FP) translation feature is scored as:
where ùëík and ùëìk are the k-th phrase in E and F,
respectively. The target-to-source (backward)
phrase translation model is defined similarly.
where ùëík,m is the m-th word of the k-th target
phrase ùëík, ùëìk,r is the r-th word in the k-th source
phrase ùëìk, and ùëù(ùëík,m|ùëìk,r) is the probability of
translating word ùëìk,r to word ùëík,1T1. In IBM model
1, these probabilities are learned via maximizing a
joint likelihood between the source and target
sentences. The target-to-source (backward) lexicon
translation model is defined similarly.
</bodyText>
<sectionHeader confidence="0.97976" genericHeader="method">
4. Maximum Expected-BLEU Training
</sectionHeader>
<subsectionHeader confidence="0.992316">
4.1. Objective function
</subsectionHeader>
<bodyText confidence="0.972577875">
We denote by ùõâ the set of all the parameters to be
optimized, including forward phrase and lexicon
translation probabilities and their backward
counterparts. For simplification of notation, ùõâ is
formed as a matrix, where its elements {ùúÉij) are
probabilities subject to i ùúÉii = 1. E.g., each row
is a probability distribution.
The utility function over the entire training set is
defined as:
(6)
where N is the number of sentences in the training
set, ùê∏n is the reference translation of the n-th
source sentence ùêπn, and ùê∏n ‚àä ùêªùë¶ùëù(ùêπn) that denotes
the list of translation hypotheses of ùêπn. Since the
sentences are independent with each other, the
joint posterior can be decomposed:
</bodyText>
<equation confidence="0.998370545454545">
‚ÑéFF ùê∏, ùêπ = ùëù ùëík ùëìk
k
(4)
ùëà(ùõâ)
=ùëÉùõâ(ùê∏1, ... , ùê∏N|ùêπ1, ... , ùêπN) BLEU(ùê∏., ùê∏n)
ÔøΩÔøΩ,...,ÔøΩÔøΩ
ùëÉùõâ ùê∏1, ... , ùê∏N ùêπ1, ... , ùêπN = ùëÉùõâ ùê∏n ùêπn
(7)
ùê∂ (ùëí, ùëì) (3)
ùê∂(ùëì)
ùëù ùëí ùëì =
</equation>
<page confidence="0.974238">
294
</page>
<bodyText confidence="0.999840125">
and ùëÉùõâ ùê∏n ùêπn is the posterior defined in (2), the
subscript ùõâ indicates that it is computed based on
the parameter set ùõâ. ùëà ùõâ is proportional (with a
factor of N) to the expected sentence BLEU score
over the entire training set, i.e., after some algebra,
In a phrase-based SMT system, the total number
of parameters of phrase and lexicon translation
models, which we aim to learn discriminatively, is
very large (see Table 1). Therefore, regularization
is critical to prevent over-fitting. In this work, we
regularize the parameters with KL regularization.
KL divergence is commonly used to measure
the distance between two probability distributions.
For the whole parameter set ùõâ , the KL
regularization is defined in this work as the sum of
KL divergence over the entire parameter space:
</bodyText>
<equation confidence="0.98930325">
ùúÉ¬∞
ùêæùêø(ùõâ0||ùõâ) = ùúÉ¬∞ log (8)
i j
ùúÉiJ
</equation>
<bodyText confidence="0.9998726">
where ùõâ0 is a constant prior parameter set. In
training, we want to improve the utility function
while keeping the changes of the parameters from
ùõâ0 at minimum. Therefore, we design the objective
function to be maximized as:
</bodyText>
<equation confidence="0.96728">
ùëÇ ùõâ = logùëà ùõâ ‚àí ùúè ¬∑ ùêæùêø(ùõâ0||ùõâ) (9)
</equation>
<bodyText confidence="0.999996">
where the prior model ùõâ0 in our approach is the
relative-frequency-based phrase translation model
and the maximum-likelihood-estimated IBM
model 1 (word translation model). ùúè is a hyper-
parameter controlling the degree of regularization.
</bodyText>
<subsectionHeader confidence="0.966177">
4.2. Optimization
</subsectionHeader>
<bodyText confidence="0.999990333333333">
In this section, we derived GT formulas for
iteratively updating the parameters so as to
optimize objective (9). GT is based on extended
Baum-Welch (EBW) algorithm first proposed by
Gopalakrishnan et al. (1991) and commonly used
in speech recognition (e.g., He et al. 2008).
</bodyText>
<subsubsectionHeader confidence="0.91096">
4.2.1. Extended Baum-Welch Algorithm
</subsubsectionHeader>
<bodyText confidence="0.958652882352941">
Baum-Eagon inequality (Baum and Eagon, 1967)
gives the GT formula to iteratively maximize
positive-coefficient polynomials of random
variables that are subject to sum-to-one constants.
Baum-Welch algorithm is a model update
algorithm for hidden Markov model which uses
this GT. Gopalakrishnan et al. (1991) extended the
algorithm to handle rational function, i.e., a ratio of
two polynomials, which is more commonly
encountered in discriminative training.
Here we briefly review EBW. Assuming a set of
random variables ùê© = {ùëùij} that subject to the
constraint that j ùëùij = 1, and assume ùëî (ùê©)and
‚Ñé(ùê©) are two positive polynomial functions of ùê© , a
GT of ùê© for the rational function ùëü ùê© = h(p) can
be obtained through the following two steps:
i) Construct the auxiliary function:
</bodyText>
<equation confidence="0.907074">
ùëì ùê© = ùëîùê© ‚àí ùëüùê©&apos; ‚Ñé ùê© (10)
</equation>
<bodyText confidence="0.9999105">
where ùê©&apos; are the values from the previous iteration.
Increasing f guarantees an increase of r, i.e., ‚Ñé ùê©
</bodyText>
<equation confidence="0.886198857142857">
ÔøΩ
&gt; 0 and ùëü ùê© ‚àí ùëü ùê©‚Ä≤ = ÔøΩ ùê© ùëì ùê© ‚àí ùëì ùê©‚Ä≤ .
ii) Derive GT formula for ùëì ùê©
ùëùi - , + ùê∑‚àô ùëùV
j apij ùúïùëì(ùê©)
ùê© P
J ùëù, ùúïùëì (ùê©) + ùê∑ (11) ùúïùëùÔøΩÔøΩ
</equation>
<bodyText confidence="0.972192">
where D is a smoothing factor.
</bodyText>
<subsubsectionHeader confidence="0.928031">
4.2.2. GT of Translation Models
</subsubsectionHeader>
<bodyText confidence="0.986948">
Now we derive the GTs of translation models for
our objective. Since maximizing ùëÇ ùõâ is
equivalent to maximizing ùëí ¬∞ ùõâ , we have the
following auxiliary function:
</bodyText>
<equation confidence="0.865556">
ùëÖ ùõâ = ùëà(ùõâ)ùëí-T¬∑xL(ùõâ¬∞||ùõâ) (12)
</equation>
<bodyText confidence="0.98947">
After substituting (2) and (7) into (6), and drop
optimization irrelevant terms in KL regularization,
</bodyText>
<equation confidence="0.902566583333333">
we have ùëÖ ùõâ in a rational function form:
ùëÖ ùõâ = ùê∫ H(ùõâ) ùõâ (13)
N IM
where ùêª ùõâ = E1,...,EN n=1 m &lt; ùê∏n, ùêπn
Te and ùê∫ ùõâ =
ùêΩ ùõâ = ÔøΩ ; ùúÉl; ,
ùëà(ùõâ) = ùëÉùõâ(ùê∏n|ùêπn)BLEU(ùê∏n, ùê∏n)
ùëùij =
ùê©=ùê©1
295
N=1 m ‚Ñém ùê∏n, ùêπn tt=1 BLEU ùê∏n, ùê∏n
E1,...,EN
</equation>
<bodyText confidence="0.995061571428571">
are all positive polynomials of ùõâ. Therefore, we
can follow the two steps of EBW to derive the GT
formulas for ùõâ.
If we denote by ùëùij the probability of
translating the source phrase i to the target phrase j.
Then, the updating formula is (derivation omitted):
where ùúèFP = ùúè/ùúÜFp and
</bodyText>
<equation confidence="0.771453">
ùõæFP ùê∏n, ùëõ, ùëñ, ùëó = ùëÉùõâ, ùê∏n ùêπn ¬∑ BLEU ùê∏n, ùê∏n ‚àí
ùëàn ùõâ‚Ä≤ ¬∑ k ùüè(ùëìn,k = ùëñ, ùëín,k = ùëó) . In which
</equation>
<bodyText confidence="0.994860272727273">
ùëàn ùõâ‚Ä≤ takes a form similar to (6), but is the
expected BLEU score for sentence n using models
from the previous iteration. ùëìn,k and ùëín,k are the k-
th phrases of ùêπn and ùê∏n, respectively.
The smoothing factor set of ùê∑i according to the
Baum-Eagon inequality is usually far too large for
practical use. In practice, one general guide of
setting ùê∑i is to make all updated value positive.
Similar to (Povey 2004), we set ùê∑i by
to ensure the denominator of (15) is positive.
Further, we set a low-bound of ùê∑i as
</bodyText>
<equation confidence="0.5979215">
maxj{ n EnYFP En,n,i,j } to guarantee the
P!.
</equation>
<bodyText confidence="0.966236">
numerator to be positive.
We denote by ùëôij the probability of translating
the source word i to the target word j. Then
following the same derivation, we get the updating
formula for forward lexicon translation model:
</bodyText>
<equation confidence="0.9391065">
ùëôij = ,
ÔøΩ ÔøΩÔøΩùõæÔøΩÔøΩ(ùê∏ÔøΩ, ùëõ, ùëñ, ùëó) + ùëà ùõâ‚Ä≤ ùúèÔøΩÔøΩùëôÔøΩÔøΩ 9 + ùê∑ÔøΩùëô!ÔøΩ
n En j ùõæFL(ùê∏n, ùêπn, ùëñ, ùëó) + ùëà ùõâ ùúèFL + ùê∑i
!
(16)
where ùúèFL = ùúè/ùúÜFL and
ùõæFL ùê∏n, ùëõ, ùëñ, ùëó = ùëÉùõâ, ùê∏n ùêπn ¬∑ BLEU ùê∏n, ùê∏n ‚àí
ùëàn ùõâ‚Ä≤ ¬∑ k m ùüè(ùëín,k,m = ùëó) ùõæ ùëõ, ùëò, ùëö, ùëñ , and
ùõæ ùëõ, ùëò, ùëö, ùëñ = rùüèUn,k,r=0P1(en,k,m|fn,k,r), in which
r P 1(en,k,m  |f n,k,r)
</equation>
<bodyText confidence="0.996289285714286">
ùëìÔøΩ,ÔøΩ,ÔøΩ and ùëín,k,m are the r-th and m-th word in the
k-th phrase of the source sentence ùêπn and the target
hypothesis ùê∏n, respectively. Value of ùê∑i is set in a
way similar to (15).
GTs for updating backward phrase and lexicon
translation models can be derived in a similar way,
and is omitted here.
</bodyText>
<subsectionHeader confidence="0.973893">
4.3. Implementation issues
</subsectionHeader>
<subsubsectionHeader confidence="0.959058">
4.3.1. Normalizing ùùÄ
</subsubsectionHeader>
<bodyText confidence="0.999806125">
The posterior ùëùùõâ, ùê∏n ùêπn in the model updating
formula is computed according to (2). In decoding,
only the relative values of ùõå matters. However, the
absolute value will affect the posterior distribution,
e.g., an overly large absolute value of ùõå would lead
to a very sharp posterior distribution. In order to
control the sharpness of the posterior distribution,
we normalize ùõå by its L1 norm:
</bodyText>
<equation confidence="0.686766">
ùúÜm =
</equation>
<subsubsectionHeader confidence="0.805345">
4.3.2. Computing the sentence BLEU sore
</subsubsectionHeader>
<bodyText confidence="0.999175">
The commonly used BLEU-4 score is computed by
In the updating formula, we need to compute the
sentence-level BLEU ùê∏n, ùê∏n . Since the matching
count may be sparse at the sentence level, we
smooth raw precisions of high-order n-grams by:
</bodyText>
<equation confidence="0.99971825">
matc‚Ñéùëíùëë) + ùúÇ ‚àô ùëùÔøΩ ¬∞
#(ùëõ-¬≠‚Äê gram
ùëùn = (19)
#(ùëõ-¬≠‚Äêùëîùëüùëéùëö) + ùúÇ
</equation>
<bodyText confidence="0.999942666666667">
where ùëùn¬∞, is the prior value of ùëùn, ùúÇ is a smoothing
factor usually takes a value of 5 and ùëùn¬∞, can be set
by ùëùn = ùëùn-1 ‚àô ùëùn-1 ùëùn-2, for n = 3, 4. ùëù1 and ùëù2
are estimated empirically. Brevity penalty (BP)
also plays a key role. Instead of clip it at 1, we use
a non-clipped BP, ùêµùëÉ = ùëí(1-D, for sentence-level
BLEU1. We further scale the reference length, r, by
a factor such that the total length of references on
the training set equals that of the baseline output2.
</bodyText>
<footnote confidence="0.997813166666667">
1 This is to better approximate corpus-level BLEU, i.e., as
discussed in (Chiang, et al., 2008), the per-sentence BP might
effectively exceed unity in corpus-level BLEU computation.
2 This is to focus the training on improving BLEU by
improving n-gram match instead of by improving BP, e.g., this
makes the BP of the baseline output already being perfect.
</footnote>
<equation confidence="0.989139133333333">
ÔøΩ
ÔøΩ ÔøΩÔøΩùõæÔøΩÔøΩ(ùê∏ÔøΩ, ùëõ, ùëñ, ùëó) + ùëà ùõâ‚Ä≤ ùúèÔøΩÔøΩùëùÔøΩÔøΩ ÔøΩ + ùê∑ÔøΩùëùÔøΩ]
n En j ùõæFP(ùê∏n, ùëõ, ùëñ, ùëó) + ùëà ùõâ‚Ä≤ ùúèFP + ùê∑i
(14)
ùëùij =
BLEU--‚Äê 4 = BP ‚àô exp
(18)
1
4
logùëùn
ùê∑i = max (0, ‚àíùõæFP(ùê∏n, ùëõ, ùëñ, ùëó)
) (15)
ùúÜM
|ùúÜM|
(17)
</equation>
<page confidence="0.994936">
296
</page>
<subsubsectionHeader confidence="0.890076">
4.3.3. Training procedure
</subsubsectionHeader>
<bodyText confidence="0.999512928571428">
The parameter set Œ∏ is optimized on the training set
while the feature weights Œª are tuned on a small
tuning set3. Since Œ∏ and Œª affect the training of
each other, we train them in alternation. I.e., at
each iteration, we first fix Œª and update Œ∏, then we
re-tune Œª given the new Œ∏. Due to mismatch
between training and tuning data, the training
process might not always converge. Therefore, we
need a validation set to determine the stop point of
training. At the end, Œ∏ and Œª that give the best
score on the validation set are selected and applied
to the test set. Fig. 1 gives a summary of the
training procedure. Note that step 2 and 4 are
parallelize-able across multiple processors.
</bodyText>
<listItem confidence="0.833023642857143">
1. Build the baseline system, estimate { Œ∏, Œª }.
2. Decode N-best list for training corpus using
the baseline system, compute BLEU (ùê∏n, ùê∏n).
3. set ùõâ‚Ä≤ = ùõâ, ùõå&apos; = ùõå.
4. Max expected BLEU training
a. Go through the training set.
i. Compute ùëÉùõâ,(ùê∏n|ùêπn) and ùëàn(ùõâ‚Ä≤) .
ii. Accumulate statistics {ùõæ}.
b. Update: ùõâ&apos; ‚Üí ùõâ by one iteration of GT.
5. MERT on the tuning set: ùõå&apos; ‚Üí ùõå.
6. Test on the validation set using { Œ∏, Œª }.
7. Go to step 3 unless training converges or
reaches a certain number of iterations.
8. Pick the best { Œ∏, Œª } on the validation set.
</listItem>
<figureCaption confidence="0.999106">
Figure 1. The max expected-BLEU training algorithm.
</figureCaption>
<sectionHeader confidence="0.992206" genericHeader="evaluation">
5. Evaluation
</sectionHeader>
<bodyText confidence="0.9999942">
In evaluating the proposed method, we use two
separate datasets. We first describe the
experiments with the Europarl dataset (Koehn
2002), followed by the experiments with the more
recent IWSLT-2011 task (Federico et al., 2011).
</bodyText>
<subsectionHeader confidence="0.995425">
5.1 Experimental setup in the Europarl task
</subsectionHeader>
<bodyText confidence="0.999736">
In evaluating the proposed method, we use two
separate datasets. First, we conduct experiments on
the Europarl German-to-English dataset. The
training corpus contains 751K sentence pairs, 21
words per sentence on average. 2000 sentences are
provided in the development set. We use the first
1000 sentences for ùõå tuning, and the rest for
validation. The test set consists of 2000 sentences.
</bodyText>
<footnote confidence="0.9444365">
3 Usually, the tuning set matches the test condition better, and
therefore is preferable for Œª tuning.
</footnote>
<bodyText confidence="0.980983555555555">
To build the baseline phrase-based SMT system,
we first perform word alignment on the training set
using a hidden Markov model with lexicalized
distortion (He 2007), then extract the phrase table
from the word aligned bilingual texts (Koehn et al.,
2003). The maximum phrase length is set to four.
Other models used in the baseline system include
lexicalized ordering model, word count and phrase
count, and a 3-gram LM trained on the English
side of the parallel training corpus. Feature weights
are tuned by MERT. A fast beam-search phrase-
based decoder (Moore and Quirk 2007) is used and
the distortion limit is set to four. Details of the
phrase and lexicon translation models are given in
Table 1. This baseline achieves a BLEU score of
26.22% on the test set. This baseline system is also
used to generate a 100-best list of the training
corpus during maximum expected BLEU training.
</bodyText>
<table confidence="0.99946775">
Translation model # parameters
Phrase models (fore. &amp; back.) 9.2 M
Lexicon model (IBM-1 src-to-tgt) 12.9 M
Lexicon model (IBM-1 tgt-to-src) 11.9 M
</table>
<tableCaption confidence="0.9830885">
Table 1. Summary of phrase and lexicon translation
models
</tableCaption>
<subsectionHeader confidence="0.968004">
5.2 Experimental results on the Europarl task
</subsectionHeader>
<bodyText confidence="0.992017916666667">
During training, we first tune the regularization
factor œÑ based on the performance on the validation
set. For simplicity reasons, the tuning of œÑ makes
use of only the phrase translation models. Table 2
reports the BLEU scores and gains over the
baseline given different values of œÑ. The results
highlight the importance of regularization. While œÑ
= 5√ó10-5 gives the best score on the validation
set, the gain is shown to be substantially reduced to
merely 0.2 BLEU point when œÑ = 0, i.e., no
regularization. We set the optimal value of œÑ =
5√ó10-5 in all remaining experiments.
</bodyText>
<table confidence="0.9825655">
Test on Validation Set BLEU% ŒîBLEU%
Baseline 26.70 --
œÑ = 0 (no regularization) 26.91 +0.21
œÑ = 1√ó10-5 27.31 +0.61
œÑ = 5√ó10-5 27.44 +0.74
œÑ = 10√ó10-5 27.27 +0.57
</table>
<tableCaption confidence="0.7824295">
Table 2. Results on degrees of regularizations. BLEU
scores are reported on the validation set. ŒîBLEU
</tableCaption>
<bodyText confidence="0.790127333333333">
denotes the gain over the baseline.
Fixing the optimal regularization factor œÑ, we
then study the relationship between the expected
</bodyText>
<page confidence="0.991436">
297
</page>
<bodyText confidence="0.999442230769231">
sentence-level BLEU (Exp. BLEU) score of N-best
lists and the corpus-level BLEU score of 1-best
translations. The conjectured close relationship
between the two is important in justifying our use
of the former as the training objective. Fig. 2
shows these two scores on the training set over
training iterations. Since the expected BLEU is
affected by Œª strongly, we fix the value of Œª in
order to make the expected BLEU comparable
across different iterations. From Fig. 2 it is clear
that the expected BLEU score correlates strongly
with the real BLEU score, justifying its use as our
training objective.
</bodyText>
<figure confidence="0.992990875">
30%
29%
28%
27%
26%
25%
0 1 2 3 4 5 6 7 8
#iteration
</figure>
<figureCaption confidence="0.9961965">
Figure 2. Expected sentence BLEU and 1-best corpus
BLEU on the 751K sentence of training data.
</figureCaption>
<bodyText confidence="0.999858277777778">
Next, we study the effects of training the phrase
translation probabilities and the lexicon translation
probabilities according to the GT formulas
presented in the preceding section. The break-
down results are shown in Table 3. Compared with
the baseline, training phrase or lexicon models
alone gives a gain of 0.7 and 0.5 BLEU points,
respectively, on the test set. For a full training of
both phrase and lexicon models, we adopt two
learning schedules: update both models together at
each iteration (simultaneously), or update them in
two stages (two-stage), where the phrase models
are trained first until reaching the best score on the
validation set and then the lexicon models are
trained. Both learning schedules give significant
improvements over the baseline and also over
training phrase or lexicon models alone. The two-
stage training of both models gives the best result
of 27.33%, outperforming the baseline by 1.1
BLEU points.
More detail of the two-stage training is provided
in Fig. 3, where BLEU scores in each stage are
shown as a function of the GT training iteration.
The phrase translation probabilities (PT) are
trained alone in the first stage, shown in blue color.
After five iterations, the BLEU score on the
validation set reaches the peak value, with further
iteration giving BLEU score fluctuation. Hence,
we perform lexicon model (LEX) training starting
from the sixth iteration with the corresponding
BLEU scores shown in red color in Fig. 3. The
BLEU score is further improved by 0.4 points after
additional three iterations of training the lexicon
models. In total, nine iterations are performed to
comalete the two-stage GT training of all phrase
and Nxicon models.
</bodyText>
<table confidence="0.924707142857143">
ple
BLEU (%) validation test
Baseline 26.70 26.22
Train phrase models alone 27.44 26.94*
Train lexicon models alone 27.36 26.71
Both models: simultaneously 27.65 27.13*
Both models: two-stage 27.82 27.33*
</table>
<tableCaption confidence="0.996485">
Table 3. Results on the Europarl German-to-English
</tableCaption>
<bodyText confidence="0.980736166666667">
dataset. The BLEU measures from various settings of
maximum expected BLEU training are compared with
the baseline, where * denotes that the gain over the
baseline is statistically significant with a significance
level &gt; 99%, measured by paired bootstrap resampling
method proposed by Koehn (2004).
</bodyText>
<figure confidence="0.9196865">
0 1 2 3 4 5 6 7 8 9 10
#iteration
</figure>
<figureCaption confidence="0.942629">
Figure 3. BLEU scores on the validation set as a
function of the GT training iteration in two-stage
training of both the phrase translation models (PT) and
the lexicon models (LEX). The BLEU scores on
training phrase models are shown in blue, and on
training lexicon models in red.
</figureCaption>
<figure confidence="0.999681666666667">
28.0%
27.5%
27.0%
26.5%
26.0%
BLEU
PT
LEX
Expected BLEU
Exp. BLEU
BLEU
37%
36%
35%
34%
33%
32%
B L EU
</figure>
<page confidence="0.928736">
298
</page>
<subsectionHeader confidence="0.447153">
5.3 Experiments on the IWSLT2011 benchmark
</subsectionHeader>
<bodyText confidence="0.999933733333334">
As the second evaluation task, we apply our new
method described in this paper to the 2011 IWSLT
Chinese-to-English machine translation benchmark
(Federico et al., 2011). The main focus of the
IWSLT2011 Evaluation is the translation of TED
talks (www.ted.com). These talks are originally
given in English. In the Chinese-to-English
translation task, we are provided with human
translated Chinese text with punctuations inserted.
The goal is to match the human transcribed English
speech with punctuations.
This is an open-domain spoken language
translation task. The training data consist of 110K
sentences in the transcripts of the TED talks and
their translations, in English and Chinese,
respectively. Each sentence consists of 20 words
on average. Two development sets are provided,
namely, dev2010 and tst2010. They consist of 934
sentences and 1664 sentences, respectively. We
use dev2010 for Œª tuning and tst2010 for
validation. The test set tst2011 consists of 1450
sentences.
In our system, a primary phrase table is trained
from the 110K TED parallel training data, and a 3-
gram LM is trained on the English side of the
parallel data. We are also provided additional out-
of-domain data for potential usage. From them, we
train a secondary 5-gram LM on 115M sentences
of supplementary English data, and a secondary
phrase table from 500K sentences selected from
the supplementary UN corpus by the method
proposed by Axelrod et al. (2011).
In carrying out the maximum expected BLEU
training, we use 100-best list and tune the
regularization factor to the optimal value of œÑ =
1√ó10-5. We only train the parameters of the
primary phrase table. The secondary phrase table
and LM are excluded from the training process
since the out-of-domain phrase table is less
relevant to the TED translation task, and the large
LM slows down the N-best generation process
significantly.
At the end, we perform one final MERT to tune
the relative weights with all features including the
secondary phrase table and LM.
The translation results are presented in Table 4.
The baseline is a phrase-based system with all
features including the secondary phrase table and
LM. The new system uses the same features except
that the primary phrase table is discriminatively
trained using maximum expected-BLEU and GT
optimization as described earlier in this paper.
The results are obtained using the two-stage
training schedule, including six iterations for
training phrase translation models and two
iterations for training lexicon translation models.
The results in Table 4 show that the proposed
method leads to an improvement of 1.2 BLEU
point over the baseline. This gives the best single
system result on this task.
</bodyText>
<table confidence="0.998790666666667">
BLEU (%) Validation Test
Baseline 11.48 14.68
Max expected BLEU training 12.39 15.92
</table>
<tableCaption confidence="0.9884955">
Table 4. The translation results on IWSLT 2011
MT_CE task.
</tableCaption>
<sectionHeader confidence="0.995412" genericHeader="conclusions">
6. Summary
</sectionHeader>
<bodyText confidence="0.999989576923077">
The contributions of this work can be summarized
as follows. First, we propose a new objective
function (Eq. 9) for training of large-scale
translation models, including phrase and lexicon
models, with more parameters than all previous
methods have attempted. The objective function
consists of 1) the utility function of expected
BLEU score, and 2) the regularization term taking
the form of KL divergence in the parameter space.
The expected BLEU score is closely linked to
translation quality and the regularization is
essential when many parameters are trained at
scale. The importance of both is verified
experimentally with the results presented in this
paper.
Second, through non-trivial derivation, we show
that the novel objective function of Eq. (9) is
amenable to iterative GT updates, where each
update is equipped with a closed-form formula.
Third, the new objective function and new
optimization technique are successfully applied to
two important machine translation tasks, with
implementation issues resolved (e.g., training
schedule and hyper-parameter tuning, etc.). The
superior results clearly demonstrate the
effectiveness of the proposed algorithm.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999892666666667">
The authors are grateful to Chris Quirk, Mei-Yuh
Hwang, and Bowen Zhou for the assistance with
the MT system and/or for the valuable discussions.
</bodyText>
<page confidence="0.997347">
299
</page>
<sectionHeader confidence="0.983676" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999505595505618">
Amittai Axelrod, Xiaodong He, Jianfeng Gao.
2011, Domain adaptation via pseudo in-domain
data selection. In Proc. of EMNLP, 2011.
Necip Fazil Ayan, and Bonnie J. Dorr. Going.
2006. Beyond AER: an extensive analysis of
word alignments and their impact on MT. In
Proc. of COLING-ACL, 2006.
Leonard Baum and J. A. Eagon. 1967. An
inequality with applications to statistical
prediction for functions of Markov processes
and to a model of ecology, Bulletin of the
American Mathematical Society, Jan. 1967.
Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008. A discriminative latent variable model for
statistical machine translation. In Proc. of ACL
2008.
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J.Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational
Linguistics, 1993.
David Chiang, Steve DeNeefe, Yee Seng Chan,
and Hwee Tou Ng, 2008. Decomposability of
translation metrics for improved evaluation and
efficient algorithms. In Proc. of EMNLP, 2008.
David Chiang, Kevin Knight and Weri Wang,
2009. 11,001 new features for statistical
machine translation. In Proc. of NAACL-HLT,
2009.
Marcello Federico, L. Bentivogli, M. Paul, and S.
Stueker. 2011. Overview of the IWSLT 2011
Evaluation Campaign. In Proc. of IWSLT, 2011.
George Foster, Cyril Goutte and Roland Kuhn.
2010. Discriminative Instance Weighting for
Domain Adaptation in Statistical Machine
Translation. In Proc. of EMNLP, 2010.
P. S. Gopalakrishnan, Dimitri Kanevsky, Arthur
Nadas, and David Nahamoo. 1991. An
inequality for rational functions with
applications to some statistical estimation
problems. IEEE Trans. Inform. Theory, 1991.
Xiaodong He. 2007. Using Word-Dependent
Transition Models in HMM based Word
Alignment for Statistical Machine Translation.
In Proc. of the Second ACL Workshop on
Statistical Machine Translation.
Xiaodong He, Li Deng, Wu Chou, 2008.
Discriminative learning in sequential pattern
recognition. IEEE Signal Processing Magazine,
Sept. 2008.
Philipp Koehn. 2002. Europarl: A Multilingual
Corpus for Evaluation of Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase based
translation. In Proc. of NAACL. 2003.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proc. of
EMNLP 2004.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein
and Ben. Taskar. 2006. An end-to-end
discriminative approach to machine translation,
In Proc. of COLING-ACL, 2006.
Wolfgang Macherey, Franz Josef Och, gnacio
Thayer, and Jakob Uskoreit. 2008. Lattice-based
minimum error rate training for statistical
machine translation. In Proc. of EMNLP 2008.
Robert Moore and Chris Quirk. 2007. Faster
Beam-Search Decoding for Phrasal Statistical
Machine Translation. In Proc. of MT Summit
XI.
Franz Josef Och and H. Ney. 2002. Discriminative
training and maximum entropy models for
statistical machine translation, In Proc. of ACL
2002.
Franz Josef Och, 2003, Minimum error rate
training in statistical machine translation. In
Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for
automatic evaluation of machine translation. In
Proc. of ACL 2002.
Daniel Povey. 2004. Discriminative Training for
large Vocabulary Speech Recognition. Ph.D.
dissertation, Cambridge University, Cambridge,
UK, 2004.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency treelet translation:
Syntactically informed phrasal SMT. In Proc. of
ACL 2005.
</reference>
<page confidence="0.970976">
300
</page>
<reference confidence="0.999502">
Antti-Veikko Rosti, Bing hang, Spyros Matsoukas,
and Richard Schard Schwartz. 2011. Expected
BLEU training for graphs: bbn system
description for WMT system combination task.
In Proc. of workshop on statistical machine
translation 2011.
David A Smith, Jason Eisner. 2006. Minimum risk
annealing for training log-linear models, In
Proc. of COLING-ACL 2006.
Joern Wuebker, Arne Mauser and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out, In Proc. of ACL 2010.
Roy Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine
translation. In Proc. of EMNLP 2008.
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun
Lin. 2011. Fast Generation of Translation Forest
for Large-Scale SMT Discriminative
Training. In Proc. Of EMNLP 2011.
</reference>
<page confidence="0.998919">
301
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964361">
<title confidence="0.999335">Maximum Expected BLEU Training of Phrase and Translation Models</title>
<author confidence="0.986349">Xiaodong He Li Deng</author>
<affiliation confidence="0.999845">Microsoft Research Microsoft Research</affiliation>
<address confidence="0.995124">One Microsoft Way, Redmond, WA, USA One Microsoft Way, Redmond, WA, USA</address>
<email confidence="0.999156">xiaohe@microsoft.comdeng@microsoft.com</email>
<abstract confidence="0.99922045">This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
</authors>
<title>Xiaodong He, Jianfeng Gao.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<marker>Axelrod, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, Jianfeng Gao. 2011, Domain adaptation via pseudo in-domain data selection. In Proc. of EMNLP, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Going</author>
</authors>
<title>Beyond AER: an extensive analysis of word alignments and their impact on MT.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<marker>Going, 2006</marker>
<rawString>Necip Fazil Ayan, and Bonnie J. Dorr. Going. 2006. Beyond AER: an extensive analysis of word alignments and their impact on MT. In Proc. of COLING-ACL, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Baum</author>
<author>J A Eagon</author>
</authors>
<title>An inequality with applications to statistical prediction for functions of Markov processes and to a model of ecology,</title>
<date>1967</date>
<journal>Bulletin of the American</journal>
<publisher>Mathematical Society,</publisher>
<contexts>
<context position="13307" citStr="Baum and Eagon, 1967" startWordPosition="2106" endWordPosition="2109">0||ùõâ) (9) where the prior model ùõâ0 in our approach is the relative-frequency-based phrase translation model and the maximum-likelihood-estimated IBM model 1 (word translation model). ùúè is a hyperparameter controlling the degree of regularization. 4.2. Optimization In this section, we derived GT formulas for iteratively updating the parameters so as to optimize objective (9). GT is based on extended Baum-Welch (EBW) algorithm first proposed by Gopalakrishnan et al. (1991) and commonly used in speech recognition (e.g., He et al. 2008). 4.2.1. Extended Baum-Welch Algorithm Baum-Eagon inequality (Baum and Eagon, 1967) gives the GT formula to iteratively maximize positive-coefficient polynomials of random variables that are subject to sum-to-one constants. Baum-Welch algorithm is a model update algorithm for hidden Markov model which uses this GT. Gopalakrishnan et al. (1991) extended the algorithm to handle rational function, i.e., a ratio of two polynomials, which is more commonly encountered in discriminative training. Here we briefly review EBW. Assuming a set of random variables ùê© = {ùëùij} that subject to the constraint that j ùëùij = 1, and assume ùëî (ùê©)and ‚Ñé(ùê©) are two positive polynomial functions of ùê© </context>
</contexts>
<marker>Baum, Eagon, 1967</marker>
<rawString>Leonard Baum and J. A. Eagon. 1967. An inequality with applications to statistical prediction for functions of Markov processes and to a model of ecology, Bulletin of the American Mathematical Society, Jan. 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="1203" citStr="Blunsom et al., 2008" startWordPosition="168" endWordPosition="171">raining, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation m</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent Della J Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1737" citStr="Brown et al., 1993" startWordPosition="252" endWordPosition="255">slation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al., 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al., 2006, Chiang et al., 2009). Liang et al. (2006) proposed</context>
<context position="8809" citStr="Brown et al., 1993" startWordPosition="1339" endWordPosition="1342">em. 3. Phrase-based Translation System The translation process of phrase-based SMT can be briefly described in three steps: segment source sentence into a sequence of phrases, translate each 293 source phrase to a target phrase, re-order target phrases into target sentence (Koehn et al., 2003). In decoding, the optimal translation ùê∏ given the source sentence F is obtained according to where 3.2. Lexicon translation model There are several variations in lexicon translation features (Ayan and Dorr 2006, Koehn et al., 2003, Quirk et al., 2005). We use the word translation table from IBM Model 1 (Brown et al., 1993) and compute the sum over all possible word alignments within a phrase pair without normalizing for length (Quirk et al., 2005). The source-to-target forward lexicon (FL) translation feature is: ‚ÑéFL ùê∏, ùêπ = ùëù ùëík,m ùëìk,r 1 ùëç ùëíùë•ùëù ùúÜmlog ‚Ñém(ùê∏, ùêπ) ùëÉùê∏ùêπ= ùê∏ = argmax E ùëÉ ùê∏ ùêπ (1) (2) (5) and ùëç= E ùëíùë•ùëù m ùúÜmlog ‚Ñém(ùê∏, ùêπ) is the normalization denominator to ensure that the probabilities sum to one. Note that we define the feature functions {‚Ñém(ùê∏, ùêπ)} in log domain to simplify the notation in later sections. Feature weights ùõå = [ùúÜm) are usually tuned by MERT. Features used in a phrase-based system usually inclu</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Steve DeNeefe</author>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Decomposability of translation metrics for improved evaluation and efficient algorithms.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<contexts>
<context position="17940" citStr="Chiang, et al., 2008" startWordPosition="3012" endWordPosition="3015">atc‚Ñéùëíùëë) + ùúÇ ‚àô ùëùÔøΩ ¬∞ #(ùëõ-‚Äê gram ùëùn = (19) #(ùëõ-‚Äêùëîùëüùëéùëö) + ùúÇ where ùëùn¬∞, is the prior value of ùëùn, ùúÇ is a smoothing factor usually takes a value of 5 and ùëùn¬∞, can be set by ùëùn = ùëùn-1 ‚àô ùëùn-1 ùëùn-2, for n = 3, 4. ùëù1 and ùëù2 are estimated empirically. Brevity penalty (BP) also plays a key role. Instead of clip it at 1, we use a non-clipped BP, ùêµùëÉ = ùëí(1-D, for sentence-level BLEU1. We further scale the reference length, r, by a factor such that the total length of references on the training set equals that of the baseline output2. 1 This is to better approximate corpus-level BLEU, i.e., as discussed in (Chiang, et al., 2008), the per-sentence BP might effectively exceed unity in corpus-level BLEU computation. 2 This is to focus the training on improving BLEU by improving n-gram match instead of by improving BP, e.g., this makes the BP of the baseline output already being perfect. ÔøΩ ÔøΩ ÔøΩÔøΩùõæÔøΩÔøΩ(ùê∏ÔøΩ, ùëõ, ùëñ, ùëó) + ùëà ùõâ‚Ä≤ ùúèÔøΩÔøΩùëùÔøΩÔøΩ ÔøΩ + ùê∑ÔøΩùëùÔøΩ] n En j ùõæFP(ùê∏n, ùëõ, ùëñ, ùëó) + ùëà ùõâ‚Ä≤ ùúèFP + ùê∑i (14) ùëùij = BLEU--‚Äê 4 = BP ‚àô exp (18) 1 4 logùëùn ùê∑i = max (0, ‚àíùõæFP(ùê∏n, ùëõ, ùëñ, ùëó) ) (15) ùúÜM |ùúÜM| (17) 296 4.3.3. Training procedure The parameter set Œ∏ is optimized on the training set while the feature weights Œª are tuned on a small tuning set3. Since Œ∏ a</context>
</contexts>
<marker>Chiang, DeNeefe, Chan, Ng, 2008</marker>
<rawString>David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng, 2008. Decomposability of translation metrics for improved evaluation and efficient algorithms. In Proc. of EMNLP, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Weri Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<contexts>
<context position="1224" citStr="Chiang et al., 2009" startWordPosition="172" endWordPosition="175">wth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated b</context>
<context position="2734" citStr="Chiang et al. (2009)" startWordPosition="401" endWordPosition="404">inks to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al., 2006, Chiang et al., 2009). Liang et al. (2006) proposed a large set of lexical and Part-of-Speech features and trained the model weights associated with these features using perceptron. Since many of the reference translations are non-reachable, an empirical local updating strategy had to be devised to fix this problem by picking a pseudo reference. Many such non-desirable heuristics led to moderate gains reported in that work. Chiang et al. (2009) improved a syntactic SMT system by adding as many as ten thousand syntactic features, and used Margin Infused Relaxed Algorithm (MIRA) to train the feature weights. However, the number of parameters in common phrase and lexicon translation models is much larger. In this work, we present a new, highly effective discriminative learning method for phrase and lexicon translation models. The training objective is an expected BLEU score, which is closely linked to translation quality. Further, we apply a Kullback‚ÄìLeibler (KL) divergence regularization to prevent over-fitting. For effective optimiza</context>
<context position="6382" citStr="Chiang et al. (2009)" startWordPosition="958" endWordPosition="961"> strategy where the model parameters are updated towards a pseudo-reference (i.e., the hypothesis in the n-best list that gives the best BLEU score). Experimental results showed that their approach outperformed a baseline by 0.8 BLEU point when using monotonic decoding, but there was no significant gain over a stronger baseline with a full-distortion model. In our work, we use the expectation of BLEU scores as the objective. This avoids the heuristics of picking the updating reference and therefore gives a more principal way of setting the training objective. As another closely related study, Chiang et al. (2009) incorporated about ten thousand syntactic features in addition to the baseline features. The feature weights are trained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to t</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight and Weri Wang, 2009. 11,001 new features for statistical machine translation. In Proc. of NAACL-HLT, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>L Bentivogli</author>
<author>M Paul</author>
<author>S Stueker</author>
</authors>
<title>Evaluation Campaign.</title>
<date>2011</date>
<journal>Overview of the IWSLT</journal>
<booktitle>In Proc. of IWSLT,</booktitle>
<contexts>
<context position="19947" citStr="Federico et al., 2011" startWordPosition="3392" endWordPosition="3395"> set. i. Compute ùëÉùõâ,(ùê∏n|ùêπn) and ùëàn(ùõâ‚Ä≤) . ii. Accumulate statistics {ùõæ}. b. Update: ùõâ&apos; ‚Üí ùõâ by one iteration of GT. 5. MERT on the tuning set: ùõå&apos; ‚Üí ùõå. 6. Test on the validation set using { Œ∏, Œª }. 7. Go to step 3 unless training converges or reaches a certain number of iterations. 8. Pick the best { Œ∏, Œª } on the validation set. Figure 1. The max expected-BLEU training algorithm. 5. Evaluation In evaluating the proposed method, we use two separate datasets. We first describe the experiments with the Europarl dataset (Koehn 2002), followed by the experiments with the more recent IWSLT-2011 task (Federico et al., 2011). 5.1 Experimental setup in the Europarl task In evaluating the proposed method, we use two separate datasets. First, we conduct experiments on the Europarl German-to-English dataset. The training corpus contains 751K sentence pairs, 21 words per sentence on average. 2000 sentences are provided in the development set. We use the first 1000 sentences for ùõå tuning, and the rest for validation. The test set consists of 2000 sentences. 3 Usually, the tuning set matches the test condition better, and therefore is preferable for Œª tuning. To build the baseline phrase-based SMT system, we first perfo</context>
<context position="26245" citStr="Federico et al., 2011" startWordPosition="4438" endWordPosition="4441">6 7 8 9 10 #iteration Figure 3. BLEU scores on the validation set as a function of the GT training iteration in two-stage training of both the phrase translation models (PT) and the lexicon models (LEX). The BLEU scores on training phrase models are shown in blue, and on training lexicon models in red. 28.0% 27.5% 27.0% 26.5% 26.0% BLEU PT LEX Expected BLEU Exp. BLEU BLEU 37% 36% 35% 34% 33% 32% B L EU 298 5.3 Experiments on the IWSLT2011 benchmark As the second evaluation task, we apply our new method described in this paper to the 2011 IWSLT Chinese-to-English machine translation benchmark (Federico et al., 2011). The main focus of the IWSLT2011 Evaluation is the translation of TED talks (www.ted.com). These talks are originally given in English. In the Chinese-to-English translation task, we are provided with human translated Chinese text with punctuations inserted. The goal is to match the human transcribed English speech with punctuations. This is an open-domain spoken language translation task. The training data consist of 110K sentences in the transcripts of the TED talks and their translations, in English and Chinese, respectively. Each sentence consists of 20 words on average. Two development s</context>
</contexts>
<marker>Federico, Bentivogli, Paul, Stueker, 2011</marker>
<rawString>Marcello Federico, L. Bentivogli, M. Paul, and S. Stueker. 2011. Overview of the IWSLT 2011 Evaluation Campaign. In Proc. of IWSLT, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<contexts>
<context position="1244" citStr="Foster et al, 2010" startWordPosition="176" endWordPosition="179">or phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation. In Proc. of EMNLP, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Gopalakrishnan</author>
<author>Dimitri Kanevsky</author>
<author>Arthur Nadas</author>
<author>David Nahamoo</author>
</authors>
<title>An inequality for rational functions with applications to some statistical estimation problems.</title>
<date>1991</date>
<journal>IEEE Trans. Inform. Theory,</journal>
<contexts>
<context position="3903" citStr="Gopalakrishnan et al., 1991" startWordPosition="570" endWordPosition="573">larization to prevent over-fitting. For effective optimization, we derive updating formulas of growth transformation (GT) for phrase and lexicon translation probabilities. A GT is a transformation of the probabilities that guarantees strict non-decrease of the objective over each GT iteration unless a local maximum is reached. A 292 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 292‚Äì301, Jeju, Republic of Korea, 8-14 July 2012. cÔøΩ2012 Association for Computational Linguistics similar GT technique has been successfully used in speech recognition (Gopalakrishnan et al., 1991, Povey, 2004, He et al., 2008). Our work demonstrates that it works with large scale discriminative training of SMT model as well. Our work is based on a phrase-based SMT system. Experiments on the Europarl German-toEnglish dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set, where the task is to translate TED talks (www.ted.com). Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant t</context>
<context position="13161" citStr="Gopalakrishnan et al. (1991)" startWordPosition="2085" endWordPosition="2088">n while keeping the changes of the parameters from ùõâ0 at minimum. Therefore, we design the objective function to be maximized as: ùëÇ ùõâ = logùëà ùõâ ‚àí ùúè ¬∑ ùêæùêø(ùõâ0||ùõâ) (9) where the prior model ùõâ0 in our approach is the relative-frequency-based phrase translation model and the maximum-likelihood-estimated IBM model 1 (word translation model). ùúè is a hyperparameter controlling the degree of regularization. 4.2. Optimization In this section, we derived GT formulas for iteratively updating the parameters so as to optimize objective (9). GT is based on extended Baum-Welch (EBW) algorithm first proposed by Gopalakrishnan et al. (1991) and commonly used in speech recognition (e.g., He et al. 2008). 4.2.1. Extended Baum-Welch Algorithm Baum-Eagon inequality (Baum and Eagon, 1967) gives the GT formula to iteratively maximize positive-coefficient polynomials of random variables that are subject to sum-to-one constants. Baum-Welch algorithm is a model update algorithm for hidden Markov model which uses this GT. Gopalakrishnan et al. (1991) extended the algorithm to handle rational function, i.e., a ratio of two polynomials, which is more commonly encountered in discriminative training. Here we briefly review EBW. Assuming a set</context>
</contexts>
<marker>Gopalakrishnan, Kanevsky, Nadas, Nahamoo, 1991</marker>
<rawString>P. S. Gopalakrishnan, Dimitri Kanevsky, Arthur Nadas, and David Nahamoo. 1991. An inequality for rational functions with applications to some statistical estimation problems. IEEE Trans. Inform. Theory, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
</authors>
<title>Using Word-Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation.</title>
<date>2007</date>
<contexts>
<context position="20650" citStr="He 2007" startWordPosition="3505" endWordPosition="3506">arate datasets. First, we conduct experiments on the Europarl German-to-English dataset. The training corpus contains 751K sentence pairs, 21 words per sentence on average. 2000 sentences are provided in the development set. We use the first 1000 sentences for ùõå tuning, and the rest for validation. The test set consists of 2000 sentences. 3 Usually, the tuning set matches the test condition better, and therefore is preferable for Œª tuning. To build the baseline phrase-based SMT system, we first perform word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extract the phrase table from the word aligned bilingual texts (Koehn et al., 2003). The maximum phrase length is set to four. Other models used in the baseline system include lexicalized ordering model, word count and phrase count, and a 3-gram LM trained on the English side of the parallel training corpus. Feature weights are tuned by MERT. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. Details of the phrase and lexicon translation models are given in Table 1. This baseline achieves a BLEU score of 26.22% on the test set.</context>
</contexts>
<marker>He, 2007</marker>
<rawString>Xiaodong He. 2007. Using Word-Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proc. of the Second ACL Workshop on Statistical Machine Translation.</booktitle>
<marker></marker>
<rawString>In Proc. of the Second ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Wu Chou</author>
</authors>
<title>Discriminative learning in sequential pattern recognition.</title>
<date>2008</date>
<journal>IEEE Signal Processing Magazine,</journal>
<contexts>
<context position="3934" citStr="He et al., 2008" startWordPosition="576" endWordPosition="579">fective optimization, we derive updating formulas of growth transformation (GT) for phrase and lexicon translation probabilities. A GT is a transformation of the probabilities that guarantees strict non-decrease of the objective over each GT iteration unless a local maximum is reached. A 292 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 292‚Äì301, Jeju, Republic of Korea, 8-14 July 2012. cÔøΩ2012 Association for Computational Linguistics similar GT technique has been successfully used in speech recognition (Gopalakrishnan et al., 1991, Povey, 2004, He et al., 2008). Our work demonstrates that it works with large scale discriminative training of SMT model as well. Our work is based on a phrase-based SMT system. Experiments on the Europarl German-toEnglish dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set, where the task is to translate TED talks (www.ted.com). Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant translation performance improvem</context>
<context position="13224" citStr="He et al. 2008" startWordPosition="2096" endWordPosition="2099"> we design the objective function to be maximized as: ùëÇ ùõâ = logùëà ùõâ ‚àí ùúè ¬∑ ùêæùêø(ùõâ0||ùõâ) (9) where the prior model ùõâ0 in our approach is the relative-frequency-based phrase translation model and the maximum-likelihood-estimated IBM model 1 (word translation model). ùúè is a hyperparameter controlling the degree of regularization. 4.2. Optimization In this section, we derived GT formulas for iteratively updating the parameters so as to optimize objective (9). GT is based on extended Baum-Welch (EBW) algorithm first proposed by Gopalakrishnan et al. (1991) and commonly used in speech recognition (e.g., He et al. 2008). 4.2.1. Extended Baum-Welch Algorithm Baum-Eagon inequality (Baum and Eagon, 1967) gives the GT formula to iteratively maximize positive-coefficient polynomials of random variables that are subject to sum-to-one constants. Baum-Welch algorithm is a model update algorithm for hidden Markov model which uses this GT. Gopalakrishnan et al. (1991) extended the algorithm to handle rational function, i.e., a ratio of two polynomials, which is more commonly encountered in discriminative training. Here we briefly review EBW. Assuming a set of random variables ùê© = {ùëùij} that subject to the constraint t</context>
</contexts>
<marker>He, Deng, Chou, 2008</marker>
<rawString>Xiaodong He, Li Deng, Wu Chou, 2008. Discriminative learning in sequential pattern recognition. IEEE Signal Processing Magazine, Sept. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</title>
<date>2002</date>
<contexts>
<context position="19857" citStr="Koehn 2002" startWordPosition="3380" endWordPosition="3381">3. set ùõâ‚Ä≤ = ùõâ, ùõå&apos; = ùõå. 4. Max expected BLEU training a. Go through the training set. i. Compute ùëÉùõâ,(ùê∏n|ùêπn) and ùëàn(ùõâ‚Ä≤) . ii. Accumulate statistics {ùõæ}. b. Update: ùõâ&apos; ‚Üí ùõâ by one iteration of GT. 5. MERT on the tuning set: ùõå&apos; ‚Üí ùõå. 6. Test on the validation set using { Œ∏, Œª }. 7. Go to step 3 unless training converges or reaches a certain number of iterations. 8. Pick the best { Œ∏, Œª } on the validation set. Figure 1. The max expected-BLEU training algorithm. 5. Evaluation In evaluating the proposed method, we use two separate datasets. We first describe the experiments with the Europarl dataset (Koehn 2002), followed by the experiments with the more recent IWSLT-2011 task (Federico et al., 2011). 5.1 Experimental setup in the Europarl task In evaluating the proposed method, we use two separate datasets. First, we conduct experiments on the Europarl German-to-English dataset. The training corpus contains 751K sentence pairs, 21 words per sentence on average. 2000 sentences are provided in the development set. We use the first 1000 sentences for ùõå tuning, and the rest for validation. The test set consists of 2000 sentences. 3 Usually, the tuning set matches the test condition better, and therefore</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A Multilingual Corpus for Evaluation of Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1716" citStr="Koehn et al., 2003" startWordPosition="248" endWordPosition="251">istical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al., 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al., 2006, Chiang et al., 2009). Liang e</context>
<context position="8484" citStr="Koehn et al., 2003" startWordPosition="1285" endWordPosition="1288">ligning that sentence. However, as pointed out by Liang et al (2006), the same problem as in the bold updating existed, i.e., forced alignment between a source sentence and its reference translation was tricky, and the proposed alignment was likely to be unreliable. The method presented in this paper is free from this problem. 3. Phrase-based Translation System The translation process of phrase-based SMT can be briefly described in three steps: segment source sentence into a sequence of phrases, translate each 293 source phrase to a target phrase, re-order target phrases into target sentence (Koehn et al., 2003). In decoding, the optimal translation ùê∏ given the source sentence F is obtained according to where 3.2. Lexicon translation model There are several variations in lexicon translation features (Ayan and Dorr 2006, Koehn et al., 2003, Quirk et al., 2005). We use the word translation table from IBM Model 1 (Brown et al., 1993) and compute the sum over all possible word alignments within a phrase pair without normalizing for length (Quirk et al., 2005). The source-to-target forward lexicon (FL) translation feature is: ‚ÑéFL ùê∏, ùêπ = ùëù ùëík,m ùëìk,r 1 ùëç ùëíùë•ùëù ùúÜmlog ‚Ñém(ùê∏, ùêπ) ùëÉùê∏ùêπ= ùê∏ = argmax E ùëÉ ùê∏ ùêπ (1) (2) (5</context>
<context position="9752" citStr="Koehn et al., 2003" startWordPosition="1503" endWordPosition="1506">ion denominator to ensure that the probabilities sum to one. Note that we define the feature functions {‚Ñém(ùê∏, ùêπ)} in log domain to simplify the notation in later sections. Feature weights ùõå = [ùúÜm) are usually tuned by MERT. Features used in a phrase-based system usually include LM, reordering model, word and phrase counts, and phrase and lexicon translation models. Given the focus of this paper, we review only the phrase and lexicon translation models below. 3.1. Phrase translation model A set of phrase pairs are extracted from wordaligned parallel corpus according to phrase extraction rules (Koehn et al., 2003). Phrase translation probabilities are then computed as relative frequencies of phrases over the training dataset. i.e., the probability of translating a source phrase ùëì to a target phrase ùëí is computed by where ùê∂ (ùëí, ùëì) is the joint counts of ùëí and ùëì , and ùê∂ (ùëì) is the marginal counts of ùëì. In translation, the input sentence is segmented into K phrases, and the source-to-target forward phrase (FP) translation feature is scored as: where ùëík and ùëìk are the k-th phrase in E and F, respectively. The target-to-source (backward) phrase translation model is defined similarly. where ùëík,m is the m-th </context>
<context position="20740" citStr="Koehn et al., 2003" startWordPosition="3518" endWordPosition="3521"> dataset. The training corpus contains 751K sentence pairs, 21 words per sentence on average. 2000 sentences are provided in the development set. We use the first 1000 sentences for ùõå tuning, and the rest for validation. The test set consists of 2000 sentences. 3 Usually, the tuning set matches the test condition better, and therefore is preferable for Œª tuning. To build the baseline phrase-based SMT system, we first perform word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extract the phrase table from the word aligned bilingual texts (Koehn et al., 2003). The maximum phrase length is set to four. Other models used in the baseline system include lexicalized ordering model, word count and phrase count, and a 3-gram LM trained on the English side of the parallel training corpus. Feature weights are tuned by MERT. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. Details of the phrase and lexicon translation models are given in Table 1. This baseline achieves a BLEU score of 26.22% on the test set. This baseline system is also used to generate a 100-best list of the training corpus duri</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase based translation. In Proc. of NAACL. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="25609" citStr="Koehn (2004)" startWordPosition="4324" endWordPosition="4325"> the two-stage GT training of all phrase and Nxicon models. ple BLEU (%) validation test Baseline 26.70 26.22 Train phrase models alone 27.44 26.94* Train lexicon models alone 27.36 26.71 Both models: simultaneously 27.65 27.13* Both models: two-stage 27.82 27.33* Table 3. Results on the Europarl German-to-English dataset. The BLEU measures from various settings of maximum expected BLEU training are compared with the baseline, where * denotes that the gain over the baseline is statistically significant with a significance level &gt; 99%, measured by paired bootstrap resampling method proposed by Koehn (2004). 0 1 2 3 4 5 6 7 8 9 10 #iteration Figure 3. BLEU scores on the validation set as a function of the GT training iteration in two-stage training of both the phrase translation models (PT) and the lexicon models (LEX). The BLEU scores on training phrase models are shown in blue, and on training lexicon models in red. 28.0% 27.5% 27.0% 26.5% 26.0% BLEU PT LEX Expected BLEU Exp. BLEU BLEU 37% 36% 35% 34% 33% 32% B L EU 298 5.3 Experiments on the IWSLT2011 benchmark As the second evaluation task, we apply our new method described in this paper to the 2011 IWSLT Chinese-to-English machine translati</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation,</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<marker>Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cote, Dan Klein and Ben. Taskar. 2006. An end-to-end discriminative approach to machine translation, In Proc. of COLING-ACL, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Josef Och</author>
<author>gnacio Thayer</author>
<author>Jakob Uskoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="7165" citStr="Macherey et al., 2008" startWordPosition="1080" endWordPosition="1083">ng MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting,</context>
</contexts>
<marker>Macherey, Och, Thayer, Uskoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz Josef Och, gnacio Thayer, and Jakob Uskoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proc. of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Faster Beam-Search Decoding for Phrasal Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of MT</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="21063" citStr="Moore and Quirk 2007" startWordPosition="3572" endWordPosition="3575">, and therefore is preferable for Œª tuning. To build the baseline phrase-based SMT system, we first perform word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extract the phrase table from the word aligned bilingual texts (Koehn et al., 2003). The maximum phrase length is set to four. Other models used in the baseline system include lexicalized ordering model, word count and phrase count, and a 3-gram LM trained on the English side of the parallel training corpus. Feature weights are tuned by MERT. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. Details of the phrase and lexicon translation models are given in Table 1. This baseline achieves a BLEU score of 26.22% on the test set. This baseline system is also used to generate a 100-best list of the training corpus during maximum expected BLEU training. Translation model # parameters Phrase models (fore. &amp; back.) 9.2 M Lexicon model (IBM-1 src-to-tgt) 12.9 M Lexicon model (IBM-1 tgt-to-src) 11.9 M Table 1. Summary of phrase and lexicon translation models 5.2 Experimental results on the Europarl task During training, we first tune the re</context>
</contexts>
<marker>Moore, Quirk, 2007</marker>
<rawString>Robert Moore and Chris Quirk. 2007. Faster Beam-Search Decoding for Phrasal Statistical Machine Translation. In Proc. of MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation,</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation, In Proc. of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="1275" citStr="Och (2003)" startWordPosition="184" endWordPosition="185">bilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood</context>
<context position="4799" citStr="Och (2003)" startWordPosition="711" endWordPosition="712">nt improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set, where the task is to translate TED talks (www.ted.com). Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant translation performance improvement over a state-of-the-art baseline, and the system using the proposed method achieved the best single system translation result in the Chineseto-English MT track. 2. Related Work One best known approach in discriminative training for SMT is proposed by Och (2003). In that work, multiple features, most of them are derived from generative models, are incorporated into a log-linear model, and the relative weights of them are tuned discriminatively on a small tuning set. However, in practice, this approach only works with a handful of parameters. More closely related to our work, Liang et al. (2006) proposed a large set of lexical and Part-ofSpeech features in addition to the phrase translation model. Weights of these features are trained using perceptron on a training set of 67K sentences. In that paper, the authors pointed out that forcing the model to </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och, 2003, Minimum error rate training in statistical machine translation. In Proc. of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="2006" citStr="Papineni et al., 2002" startWordPosition="289" endWordPosition="292">ror rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al., 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al., 2006, Chiang et al., 2009). Liang et al. (2006) proposed a large set of lexical and Part-of-Speech features and trained the model weights associated with these features using perceptron. Since many of the reference translations are non-reachable, an empirical local updating strategy had to be devised to fix this problem by </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Povey</author>
</authors>
<title>Discriminative Training for large Vocabulary Speech Recognition.</title>
<date>2004</date>
<institution>Cambridge University,</institution>
<location>Cambridge, UK,</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="3916" citStr="Povey, 2004" startWordPosition="574" endWordPosition="575">tting. For effective optimization, we derive updating formulas of growth transformation (GT) for phrase and lexicon translation probabilities. A GT is a transformation of the probabilities that guarantees strict non-decrease of the objective over each GT iteration unless a local maximum is reached. A 292 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 292‚Äì301, Jeju, Republic of Korea, 8-14 July 2012. cÔøΩ2012 Association for Computational Linguistics similar GT technique has been successfully used in speech recognition (Gopalakrishnan et al., 1991, Povey, 2004, He et al., 2008). Our work demonstrates that it works with large scale discriminative training of SMT model as well. Our work is based on a phrase-based SMT system. Experiments on the Europarl German-toEnglish dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set, where the task is to translate TED talks (www.ted.com). Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant translation pe</context>
<context position="15650" citStr="Povey 2004" startWordPosition="2571" endWordPosition="2572">the source phrase i to the target phrase j. Then, the updating formula is (derivation omitted): where ùúèFP = ùúè/ùúÜFp and ùõæFP ùê∏n, ùëõ, ùëñ, ùëó = ùëÉùõâ, ùê∏n ùêπn ¬∑ BLEU ùê∏n, ùê∏n ‚àí ùëàn ùõâ‚Ä≤ ¬∑ k ùüè(ùëìn,k = ùëñ, ùëín,k = ùëó) . In which ùëàn ùõâ‚Ä≤ takes a form similar to (6), but is the expected BLEU score for sentence n using models from the previous iteration. ùëìn,k and ùëín,k are the kth phrases of ùêπn and ùê∏n, respectively. The smoothing factor set of ùê∑i according to the Baum-Eagon inequality is usually far too large for practical use. In practice, one general guide of setting ùê∑i is to make all updated value positive. Similar to (Povey 2004), we set ùê∑i by to ensure the denominator of (15) is positive. Further, we set a low-bound of ùê∑i as maxj{ n EnYFP En,n,i,j } to guarantee the P!. numerator to be positive. We denote by ùëôij the probability of translating the source word i to the target word j. Then following the same derivation, we get the updating formula for forward lexicon translation model: ùëôij = , ÔøΩ ÔøΩÔøΩùõæÔøΩÔøΩ(ùê∏ÔøΩ, ùëõ, ùëñ, ùëó) + ùëà ùõâ‚Ä≤ ùúèÔøΩÔøΩùëôÔøΩÔøΩ 9 + ùê∑ÔøΩùëô!ÔøΩ n En j ùõæFL(ùê∏n, ùêπn, ùëñ, ùëó) + ùëà ùõâ ùúèFL + ùê∑i ! (16) where ùúèFL = ùúè/ùúÜFL and ùõæFL ùê∏n, ùëõ, ùëñ, ùëó = ùëÉùõâ, ùê∏n ùêπn ¬∑ BLEU ùê∏n, ùê∏n ‚àí ùëàn ùõâ‚Ä≤ ¬∑ k m ùüè(ùëín,k,m = ùëó) ùõæ ùëõ, ùëò, ùëö, ùëñ , and ùõæ ùëõ, ùëò, ùëö, ùëñ = rùüèUn,k,r=0P1</context>
</contexts>
<marker>Povey, 2004</marker>
<rawString>Daniel Povey. 2004. Discriminative Training for large Vocabulary Speech Recognition. Ph.D. dissertation, Cambridge University, Cambridge, UK, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="8736" citStr="Quirk et al., 2005" startWordPosition="1325" endWordPosition="1328">be unreliable. The method presented in this paper is free from this problem. 3. Phrase-based Translation System The translation process of phrase-based SMT can be briefly described in three steps: segment source sentence into a sequence of phrases, translate each 293 source phrase to a target phrase, re-order target phrases into target sentence (Koehn et al., 2003). In decoding, the optimal translation ùê∏ given the source sentence F is obtained according to where 3.2. Lexicon translation model There are several variations in lexicon translation features (Ayan and Dorr 2006, Koehn et al., 2003, Quirk et al., 2005). We use the word translation table from IBM Model 1 (Brown et al., 1993) and compute the sum over all possible word alignments within a phrase pair without normalizing for length (Quirk et al., 2005). The source-to-target forward lexicon (FL) translation feature is: ‚ÑéFL ùê∏, ùêπ = ùëù ùëík,m ùëìk,r 1 ùëç ùëíùë•ùëù ùúÜmlog ‚Ñém(ùê∏, ùêπ) ùëÉùê∏ùêπ= ùê∏ = argmax E ùëÉ ùê∏ ùêπ (1) (2) (5) and ùëç= E ùëíùë•ùëù m ùúÜmlog ‚Ñém(ùê∏, ùêπ) is the normalization denominator to ensure that the probabilities sum to one. Note that we define the feature functions {‚Ñém(ùê∏, ùêπ)} in log domain to simplify the notation in later sections. Feature weights ùõå = [ùúÜm) are us</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proc. of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing hang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schard Schwartz</author>
</authors>
<title>Expected BLEU training for graphs: bbn system description for WMT system combination task.</title>
<date>2011</date>
<booktitle>In Proc. of workshop on statistical machine translation</booktitle>
<contexts>
<context position="6846" citStr="Rosti et al. (2011)" startWordPosition="1032" endWordPosition="1035">g the updating reference and therefore gives a more principal way of setting the training objective. As another closely related study, Chiang et al. (2009) incorporated about ten thousand syntactic features in addition to the baseline features. The feature weights are trained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model u</context>
</contexts>
<marker>Rosti, hang, Matsoukas, Schwartz, 2011</marker>
<rawString>Antti-Veikko Rosti, Bing hang, Spyros Matsoukas, and Richard Schard Schwartz. 2011. Expected BLEU training for graphs: bbn system description for WMT system combination task. In Proc. of workshop on statistical machine translation 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models,</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<contexts>
<context position="7095" citStr="Smith and Eisner 2006" startWordPosition="1069" endWordPosition="1072">e feature weights are trained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities </context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A Smith, Jason Eisner. 2006. Minimum risk annealing for training log-linear models, In Proc. of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training phrase translation models with leaving-one-out,</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="7388" citStr="Wuebker et al. (2010)" startWordPosition="1115" endWordPosition="1118">e scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting, the statistics of phrase pairs from a particular sentence was excluded from the phrase table when aligning that sentence. However, as pointed out by Liang et al (2006), the same problem as in the bold updating existed, i.e</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser and Hermann Ney. 2010. Training phrase translation models with leaving-one-out, In Proc. of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="7118" citStr="Tromble et al., 2008" startWordPosition="1073" endWordPosition="1076">rained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Proc. of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training.</title>
<date>2011</date>
<booktitle>In Proc. Of EMNLP</booktitle>
<contexts>
<context position="1263" citStr="Xiao et al. 2011" startWordPosition="180" endWordPosition="183">n translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing join</context>
</contexts>
<marker>Xiao, Liu, Liu, Lin, 2011</marker>
<rawString>Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin. 2011. Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training. In Proc. Of EMNLP 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>