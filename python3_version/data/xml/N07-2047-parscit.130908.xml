<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000341">
<title confidence="0.986377">
Kernel Regression Based Machine Translation
</title>
<author confidence="0.998672">
Zhuoran Wang and John Shawe-Taylor Sandor Szedmak
</author>
<affiliation confidence="0.991512">
Department of Computer Science
University College London
</affiliation>
<address confidence="0.9529105">
London, WC1E 613T
United Kingdom
</address>
<email confidence="0.993261">
{z.wang, jst}@cs.ucl.ac.uk
</email>
<affiliation confidence="0.9532055">
School of Electronics and Computer Science
University of Southampton
</affiliation>
<address confidence="0.9669555">
Southampton, SO17 113J
United Kingdom
</address>
<email confidence="0.997378">
ss03v@ecs.soton.ac.uk
</email>
<sectionHeader confidence="0.995614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999676">
We present a novel machine translation
framework based on kernel regression
techniques. In our model, the translation
task is viewed as a string-to-string map-
ping, for which a regression type learning
is employed with both the source and the
target sentences embedded into their ker-
nel induced feature spaces. We report the
experiments on a French-English transla-
tion task showing encouraging results.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997149894736842">
Fig. 1 illustrates an example of phrase alignment
for statistical machine translation (SMT). A rough
linear relation is shown by the co-occurences of
phrases in bilingual sentence pairs, which motivates
us to introduce a novel study on the SMT task:
If we define the feature space Hx of our source
language X as all its possible phrases (i.e. informa-
tive blended word n-grams), and define the mapping
Φx : X → Hx, then a sentence x E X can be ex-
pressed by its feature vector Φx(x) E Hx. Each
component of Φx(x) is indexed by a phrase with the
value being the frequency of it in x. The definition
of the feature space Hy of our target language Y can
be made in a similar way, with corresponding map-
ping Φy : Y → Hy. Now in the machine translation
task, given 5 = {(xi, yi) : xi E X, yi E Y, i =
1, ... , m}, a set of sample sentence pairs where yi
is the translation of xi, we are trying to learn W a
matrix represented linear operator, such that:
</bodyText>
<equation confidence="0.997449">
Φy(y) = f(x) = WΦx(x) (1)
</equation>
<figureCaption confidence="0.998017">
Figure 1: Phrase alignment in SMT
</figureCaption>
<bodyText confidence="0.999478142857143">
to predict the translation y for a new sentence x.
Comparing with traditional methods, this model
gives us a theoretical framework to capture higher-
dimensional dependencies within the sentences. To
solve the multi-output regression problem, we inves-
tigate two models, least squares regression (LSR)
similar to the technique presented in (Cortes et al.,
2005), and maximum margin regression (MMR) in-
troduced in (Szedmak et al., 2006).
The rest of the paper is organized as follows. Sec-
tion 2 gives a brief review of the regression models.
Section 3 details the solution to the pre-image prob-
lem. We report the experimental results in Section
4, with discussions in Section 5.
</bodyText>
<sectionHeader confidence="0.939477" genericHeader="method">
2 Kernel Regression with Vector Outputs
</sectionHeader>
<subsectionHeader confidence="0.926702">
2.1 Kernel Induced Feature Space
</subsectionHeader>
<bodyText confidence="0.997502833333333">
In the practical learning process, only the inner prod-
ucts of the feature vectors are needed (see Section
2.2, 2.3 and 3), so we can perform the so-called
kernel trick to avoid dealing with the very high-
dimensional feature vectors explicitly. That is, for
x, z E X, a kernel function is defined as:
</bodyText>
<equation confidence="0.969764">
Kx(x, z) = (Φx(x), Φx(z)) = Φx(x)⊤Φx(z) (2)
</equation>
<bodyText confidence="0.627164">
marquées
questions
aux
revenous
nous
</bodyText>
<page confidence="0.973102">
185
</page>
<note confidence="0.4616865">
Proceedings of NAACL HLT 2007, Companion Volume, pages 185–188,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999263222222222">
Similarly, a kernel function κy(·, ·) is defined in Hy.
In our case, the blended n-spectrum string ker-
nel (Lodhi et al., 2002) that compares two strings
by counting how many (contiguous) substrings of
length from 1 up to n they have in common, is a good
choice for the kernel function to induce our feature
spaces Hx and Hy implicitly, even though it brings
in some uninformative features (word n-grams) as
well, when compared to our original definition.
</bodyText>
<subsectionHeader confidence="0.999667">
2.2 Least Squares Regression
</subsectionHeader>
<bodyText confidence="0.998022">
A basic method to solve the problem in Eq. 1 is least
squares regression that seeks the matrix W mini-
mizing the squared loss in Hy on the training set S:
</bodyText>
<equation confidence="0.3589045">
min kWMx − Myk2 (3)
F
</equation>
<bodyText confidence="0.9944328">
where Mx = [Φx(x1), ..., Φx(xm)], My =
[Φy(y1), ..., Φy(ym)], and k · kF denotes the Frobe-
nius norm.
Differentiating the expression and setting it to
zero gives:
</bodyText>
<equation confidence="0.9802245">
2WMxM⊤x − 2MyM⊤x = 0
⇒ W = MyK−1
x M⊤ (4)
x
</equation>
<bodyText confidence="0.996857">
where Kx = M⊤x Mx = (κx(xi, xj)1≤i,j≤m) is the
Gram matrix.
</bodyText>
<subsectionHeader confidence="0.997975">
2.3 Maximum Margin Regression
</subsectionHeader>
<bodyText confidence="0.9974690625">
An alternative solution to our regression learn-
ing problem is proposed in (Szedmak et al.,
2006), called maximum margin regression. If L2-
normalized feature vectors are used in Eq. 1, de-
noted by ¯Φx(·) and ¯Φy(·), MMR solves the follow-
ing optimization:
where ¯κx(·, ·) and ¯κy(·, ·) denote the kernel func-
tions associated to the respective normalized feature
vectors.
This dual problem can be solved efficiently with
a perceptron algorithm based on an incremental
subgradient method, of which the bounds on the
complexity and achievable margin can be found in
(Szedmak et al., 2006).
Then according to Karush-Kuhn-Tucker theory,
W is expressed as:
</bodyText>
<equation confidence="0.966164">
¯Φy(yi)¯Φx(xi)⊤ (7)
</equation>
<bodyText confidence="0.999772555555555">
In practice, MMR works better when the distribu-
tion of the training points are symmetrical. So we
center the data before normalizing them. If ΦSx =
1Ei=m1 x Φ (xi) is the centre of mass of the source
sentence sample set {xi} in the feature space, the
new feature map is given by ˆΦx(·) = Φx(·) − ΦSx.
The similar operation is performed on Φy(·) to ob-
tain ˆΦy(·). Then the L2-normalizations of ˆΦx(·) and
ˆΦy(·) yield our final feature vectors Φx(·) and Φy(·).
</bodyText>
<sectionHeader confidence="0.993265" genericHeader="method">
3 Pre-image Solution
</sectionHeader>
<bodyText confidence="0.999701">
To find the pre-image sentence y = f−1(x) can be
achieved by seeking yt that has the minimum loss
between its feature vector Φy(yt) and our prediction
f(x). That is (Eq. 8: LSR, Eq. 9: MMR):
</bodyText>
<equation confidence="0.991596611111111">
yt = arg min kWΦx(x) − Φy(y)k2
y∈Y(x)
= arg min
y∈Y(x)
Φy(y),W Φx(x)iHy
αi
W =
�m
i=1
κy(y, y) − 2ky(y)K−1
x kx(x) (8)
yt = arg min
y∈Y(x)
1 − h
min 2kWk2 �m ξi (5) = arg max �m αi ¯κy(yi, y)¯κx(xi, x) (9)
1 F + C i=1 y∈Y(x) i=1
s.t. h ¯Φy(yi),W¯Φx(xi)iHy ≥ 1 − ξi,
ξi &gt; 0, i = 1, ... , m.
</equation>
<bodyText confidence="0.874944529411765">
where C &gt; 0 is the regularization coefficient, and
ξi are the slack variables. The Lagrange dual form
with dual variables αi gives:
where Y(x) ⊂ Y is a finite set covering all po-
tential translations for the given source sentence
x, and kx(·) = (κx(·, xi)1≤i≤m) and ky(·) =
(κy(·, yi)1≤i≤m) are m × 1 column matrices.
A proper Y(x) can be generated according to a
lexicon that contains possible translations for every
component (word or phrase) in x. But the size of it
will grow exponentially with the length of x, which
poses implementation problem for a decoding algo-
rithm.
min �m αiαj ¯κx(xi, xj)¯κy(yi, yj) −
i,j=1
s.t. 0 ≤ αi ≤ C, i = 1, ... , m. (6)
αi
</bodyText>
<page confidence="0.898425666666667">
�m
i=1
186
</page>
<bodyText confidence="0.99999175">
In earlier systems, several heuristic search meth-
ods were developed, of which a typical example
is Koehn (2004)’s beam search decoder for phrase-
based models. However, in our case, because of the
κy(y, y) item in Eq. 8 and the normalization opera-
tion in MMR, neither the expression in Eq. 8 nor
the one in Eq. 9 can be decomposed into a sum
of subfunctions each involving feature components
in a local area only. It means we cannot estimate
exactly how well a part of the source sentence is
translated, until we obtain a translation for the entire
sentence, which prevents us doing a straightforward
beam search similar to (Koehn, 2004).
To simplify the situation, we restrict the reorder-
ing (distortion) of phrases that yield the output sen-
tences by only allowing adjacent phrases to ex-
change their positions. (The discussion of this strat-
egy can be found in (Tillmann, 2004).) We use x[i:j]
and y[i:j] to denote the substrings of x and y that be-
gin with the ith word and end with the jth. Now, if
we go back to the implementation of a beam search,
the current distortion restriction guarantees that in
each expansion of the search states (hypotheses) we
have x[1:lx] translated to a y[1:ly], either like state (a)
or like state (b) in Fig. 2, where lx is the number of
words translated in the source sentence, and ly is the
number of words obtained in the translation.
We assume that if y is a good translation of x,
then y[1:ly] is a good translation of x[1:lx] as well. So
we can expect that the squared loss kWΦx(x[1:lx])−
Φy (yL:ly] ) k2 in the LSR is small, or the inner prod-
uct h Φy(y[1:ly]),W4bx(x[1:lx])iHy in the MMR is
large, for the hypothesis yielding a good translation.
According to Eq. 8 and Eq. 9, the hypotheses in the
search stacks can thus be reranked with the follow-
ing score functions (Eq. 10: LSR, Eq. 11: MMR):
</bodyText>
<equation confidence="0.999662142857143">
Score(x[1:lx],y[1:ly]) = (10)
κy(y[1:ly], y[1:ly]) − 2ky(y[1:ly])K−1
x kx(x[1:lx])
Score(x[1:lx],y[1:ly]) =
m
� αi¯κy(yi,y[1:ly])¯κx(xi,x[1:lx]) (11)
i=1
</equation>
<bodyText confidence="0.998935">
Therefore, to solve the pre-image problem, we
just employ the same beam search algorithm as
(Koehn, 2004), except we limit the derivation of new
hypotheses with the distortion restriction mentioned
</bodyText>
<figure confidence="0.9976765">
(a) nous revenous aux
we return to
(b) nous revenous aux
we return to
</figure>
<figureCaption confidence="0.999909">
Figure 2: Search states with the limited distortion.
</figureCaption>
<bodyText confidence="0.9770172">
above. However, our score functions will bring
more runtime complexities when compared with tra-
ditional probabilistic methods. The time complexity
of a naive implementation of the blended n-spectrum
string kernel between two sentences si and sj is
O(n|si||sj|), where |· |denotes the length of the sen-
tence. So the score function in Eq. 11 results in an
average runtime complexity of O(mnlyl), where l is
the average length of the sentences yi in the training
set. Note here ¯κx(x[1:lx], xi) can be pre-computed
for lx from 1 to |x |before the beam search, which
calls for O(m|x|) space. The average runtime com-
plexity of the score function in Eq. 10 will be the
same if we pre-compute K−1
x kx(x[1:lx]).
</bodyText>
<sectionHeader confidence="0.987052" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.98577">
4.1 Resource Description
</subsectionHeader>
<bodyText confidence="0.999582095238095">
Baseline System To compare with previous work,
we take Pharaoh (Koehn, 2004) as a baseline system,
with its default settings (translation table size 10,
beam size 100). We train a trigram language model
with the SRILM toolkit (Stocke, 2002). Whilst, the
parameters for the maximum entropy model are de-
veloped based on the minimum error rate training
method (Och, 2003).
In the following experiments, to facilitate com-
parison, each time we train our regression models
and the language model and translation model for
Pharaoh on a common corpus, and use the same
phrase translation table as Pharaoh’s to decode our
systems. According to our preliminary experiments,
with the beam size of 100, the search errors of our
systems can be limited within 1.5%.
Corpora To evaluate our models, we randomly
take 12,000 sentences from the French-English por-
tion of the 1996–2003 Europarl corpus (Koehn,
2005) for scaling-up training, 300 for test (Test), and
300 for the development of Pharaoh (Dev). Some
</bodyText>
<figure confidence="0.806712962962963">
questions
marquées ...
questions
...
questions
marquées
marked
questions
187
Vocabulary Words Perplexity
Fr En Fr En Dev Test
4k 5084 4039 43k 39k 32.25 31.92
6k 6426 5058 64k 59k 30.81 29.03
8k 7377 5716 85k 79k 29.91 28.94
10k 8252 6339 106k 98k 27.55 27.09
12k 9006 6861 127k 118k 27.19 26.41
42
40
38
36
34
32
30
MMR
LSR
26
2 3 4 5 6 7
</figure>
<figureCaption confidence="0.975258">
Figure 3: BLEU(%) versus n-spectrum
</figureCaption>
<page confidence="0.802616">
28
</page>
<tableCaption confidence="0.998558">
Table 1: Statistics of the corpora.
</tableCaption>
<bodyText confidence="0.808983">
characteristics of the corpora are summarized in Ta-
ble 1.
</bodyText>
<sectionHeader confidence="0.888536" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999989666666667">
Based on the 4k training corpus, we test the per-
formance of the blended n-spectrum string kernel in
LSR and MMR using BLEU score, with n increas-
ing from 2 to 7. Fig. 3 shows the results. It can be
found that the performance becomes stable when n
reaches a certain value. Finally, we choose the 3-
spectrum for LSR, and the 5-spectrum for MMR.
Then we scale up the training set, and compare the
performance of our models with Pharaoh in Fig. 4.
We can see that the LSR model performs almost as
well as Pharaoh, whose differences of BLEU score
are within 0.5% when the training set is larger than
6k. But MMR model performs worse than the base-
line. With the training set of 12k, it is outperformed
by Pharaoh by 3.5%.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="conclusions">
5 Discussions
</sectionHeader>
<bodyText confidence="0.999964375">
Although at this stage the main contribution is
still conceptual, the capability of our approach to
be applied to machine translation is still demon-
strated. Comparable performance to previous work
is achieved by the LSR model.
But a main problem we face is to scale-up the
training set, as in practice the training set for SMT
will be much larger than several thousand sentences.
A method to speed up the training is proposed in
(Cortes et al., 2005). By approximating the Gram
matrix with a n × m (n ≪ m) low-rank matrix,
the time complexity of the matrix inversion opera-
tion can be reduced from O(m3) to O(n2m). But
the space complexity of O(nm) in their algorithm is
still too expensive for SMT tasks. Subset selection
techniques could give a solution to this problem, of
</bodyText>
<figureCaption confidence="0.97787">
Figure 4: BLEU(%) versus training set size
</figureCaption>
<bodyText confidence="0.996682">
which we will leave the further exploration to future
work.
</bodyText>
<sectionHeader confidence="0.997594" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999823">
The authors acknowledge the support of the EU un-
der the IST project No. FP6-033917.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999300608695652">
C. Cortes, M. Mohri, and J. Weston. 2005. A general re-
gression technique for learning transductions. In Proc.
ofICML’05.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. ofAMTA 2004.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit X.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text classification using string
kernels. J. Mach. Learn. Res., 2:419–444.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ofACL’03.
A. Stocke. 2002. SRILM – an extensible language mod-
eling toolkit. In Proc. ofICSLP’02.
S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez.
2006. Learning via linear operators: Maximum mar-
gin regression; multiclass and multiview learning at
one-class complexity. Technical report, PASCAL,
Southampton, UK.
C. Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
NAACL’04.
</reference>
<page confidence="0.615352">
44
</page>
<figure confidence="0.974899181818182">
42
40
38
36
34
32
30
40006000 8000 10000 12000
Pharaoh
LSR
MMR
</figure>
<page confidence="0.933229">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.139858">
<title confidence="0.999716">Kernel Regression Based Machine Translation</title>
<author confidence="0.999903">Wang Shawe-Taylor Sandor Szedmak</author>
<affiliation confidence="0.9999715">Department of Computer University College</affiliation>
<address confidence="0.796355">London, WC1E</address>
<affiliation confidence="0.866637333333333">United School of Electronics and Computer University of</affiliation>
<address confidence="0.878533">Southampton, SO17</address>
<note confidence="0.648942">United</note>
<email confidence="0.671208">ss03v@ecs.soton.ac.uk</email>
<abstract confidence="0.998491181818182">We present a novel machine translation framework based on kernel regression techniques. In our model, the translation task is viewed as a string-to-string mapping, for which a regression type learning is employed with both the source and the target sentences embedded into their kernel induced feature spaces. We report the experiments on a French-English translation task showing encouraging results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>M Mohri</author>
<author>J Weston</author>
</authors>
<title>A general regression technique for learning transductions.</title>
<date>2005</date>
<booktitle>In Proc. ofICML’05.</booktitle>
<contexts>
<context position="2131" citStr="Cortes et al., 2005" startWordPosition="354" endWordPosition="357"> task, given 5 = {(xi, yi) : xi E X, yi E Y, i = 1, ... , m}, a set of sample sentence pairs where yi is the translation of xi, we are trying to learn W a matrix represented linear operator, such that: Φy(y) = f(x) = WΦx(x) (1) Figure 1: Phrase alignment in SMT to predict the translation y for a new sentence x. Comparing with traditional methods, this model gives us a theoretical framework to capture higherdimensional dependencies within the sentences. To solve the multi-output regression problem, we investigate two models, least squares regression (LSR) similar to the technique presented in (Cortes et al., 2005), and maximum margin regression (MMR) introduced in (Szedmak et al., 2006). The rest of the paper is organized as follows. Section 2 gives a brief review of the regression models. Section 3 details the solution to the pre-image problem. We report the experimental results in Section 4, with discussions in Section 5. 2 Kernel Regression with Vector Outputs 2.1 Kernel Induced Feature Space In the practical learning process, only the inner products of the feature vectors are needed (see Section 2.2, 2.3 and 3), so we can perform the so-called kernel trick to avoid dealing with the very highdimensi</context>
</contexts>
<marker>Cortes, Mohri, Weston, 2005</marker>
<rawString>C. Cortes, M. Mohri, and J. Weston. 2005. A general regression technique for learning transductions. In Proc. ofICML’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. ofAMTA</booktitle>
<contexts>
<context position="6413" citStr="Koehn (2004)" startWordPosition="1131" endWordPosition="1132">ng all potential translations for the given source sentence x, and kx(·) = (κx(·, xi)1≤i≤m) and ky(·) = (κy(·, yi)1≤i≤m) are m × 1 column matrices. A proper Y(x) can be generated according to a lexicon that contains possible translations for every component (word or phrase) in x. But the size of it will grow exponentially with the length of x, which poses implementation problem for a decoding algorithm. min �m αiαj ¯κx(xi, xj)¯κy(yi, yj) − i,j=1 s.t. 0 ≤ αi ≤ C, i = 1, ... , m. (6) αi �m i=1 186 In earlier systems, several heuristic search methods were developed, of which a typical example is Koehn (2004)’s beam search decoder for phrasebased models. However, in our case, because of the κy(y, y) item in Eq. 8 and the normalization operation in MMR, neither the expression in Eq. 8 nor the one in Eq. 9 can be decomposed into a sum of subfunctions each involving feature components in a local area only. It means we cannot estimate exactly how well a part of the source sentence is translated, until we obtain a translation for the entire sentence, which prevents us doing a straightforward beam search similar to (Koehn, 2004). To simplify the situation, we restrict the reordering (distortion) of phra</context>
<context position="8396" citStr="Koehn, 2004" startWordPosition="1476" endWordPosition="1477"> expect that the squared loss kWΦx(x[1:lx])− Φy (yL:ly] ) k2 in the LSR is small, or the inner product h Φy(y[1:ly]),W4bx(x[1:lx])iHy in the MMR is large, for the hypothesis yielding a good translation. According to Eq. 8 and Eq. 9, the hypotheses in the search stacks can thus be reranked with the following score functions (Eq. 10: LSR, Eq. 11: MMR): Score(x[1:lx],y[1:ly]) = (10) κy(y[1:ly], y[1:ly]) − 2ky(y[1:ly])K−1 x kx(x[1:lx]) Score(x[1:lx],y[1:ly]) = m � αi¯κy(yi,y[1:ly])¯κx(xi,x[1:lx]) (11) i=1 Therefore, to solve the pre-image problem, we just employ the same beam search algorithm as (Koehn, 2004), except we limit the derivation of new hypotheses with the distortion restriction mentioned (a) nous revenous aux we return to (b) nous revenous aux we return to Figure 2: Search states with the limited distortion. above. However, our score functions will bring more runtime complexities when compared with traditional probabilistic methods. The time complexity of a naive implementation of the blended n-spectrum string kernel between two sentences si and sj is O(n|si||sj|), where |· |denotes the length of the sentence. So the score function in Eq. 11 results in an average runtime complexity of </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proc. ofAMTA 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit X.</booktitle>
<contexts>
<context position="10261" citStr="Koehn, 2005" startWordPosition="1782" endWordPosition="1783">ropy model are developed based on the minimum error rate training method (Och, 2003). In the following experiments, to facilitate comparison, each time we train our regression models and the language model and translation model for Pharaoh on a common corpus, and use the same phrase translation table as Pharaoh’s to decode our systems. According to our preliminary experiments, with the beam size of 100, the search errors of our systems can be limited within 1.5%. Corpora To evaluate our models, we randomly take 12,000 sentences from the French-English portion of the 1996–2003 Europarl corpus (Koehn, 2005) for scaling-up training, 300 for test (Test), and 300 for the development of Pharaoh (Dev). Some questions marquées ... questions ... questions marquées marked questions 187 Vocabulary Words Perplexity Fr En Fr En Dev Test 4k 5084 4039 43k 39k 32.25 31.92 6k 6426 5058 64k 59k 30.81 29.03 8k 7377 5716 85k 79k 29.91 28.94 10k 8252 6339 106k 98k 27.55 27.09 12k 9006 6861 127k 118k 27.19 26.41 42 40 38 36 34 32 30 MMR LSR 26 2 3 4 5 6 7 Figure 3: BLEU(%) versus n-spectrum 28 Table 1: Statistics of the corpora. characteristics of the corpora are summarized in Table 1. 4.2 Results Based on the 4k t</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>C Saunders</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
<author>C Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>2--419</pages>
<contexts>
<context position="3171" citStr="Lodhi et al., 2002" startWordPosition="529" endWordPosition="532">ss, only the inner products of the feature vectors are needed (see Section 2.2, 2.3 and 3), so we can perform the so-called kernel trick to avoid dealing with the very highdimensional feature vectors explicitly. That is, for x, z E X, a kernel function is defined as: Kx(x, z) = (Φx(x), Φx(z)) = Φx(x)⊤Φx(z) (2) marquées questions aux revenous nous 185 Proceedings of NAACL HLT 2007, Companion Volume, pages 185–188, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics Similarly, a kernel function κy(·, ·) is defined in Hy. In our case, the blended n-spectrum string kernel (Lodhi et al., 2002) that compares two strings by counting how many (contiguous) substrings of length from 1 up to n they have in common, is a good choice for the kernel function to induce our feature spaces Hx and Hy implicitly, even though it brings in some uninformative features (word n-grams) as well, when compared to our original definition. 2.2 Least Squares Regression A basic method to solve the problem in Eq. 1 is least squares regression that seeks the matrix W minimizing the squared loss in Hy on the training set S: min kWMx − Myk2 (3) F where Mx = [Φx(x1), ..., Φx(xm)], My = [Φy(y1), ..., Φy(ym)], and </context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text classification using string kernels. J. Mach. Learn. Res., 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL’03.</booktitle>
<contexts>
<context position="9733" citStr="Och, 2003" startWordPosition="1697" endWordPosition="1698"> for lx from 1 to |x |before the beam search, which calls for O(m|x|) space. The average runtime complexity of the score function in Eq. 10 will be the same if we pre-compute K−1 x kx(x[1:lx]). 4 Experimental Results 4.1 Resource Description Baseline System To compare with previous work, we take Pharaoh (Koehn, 2004) as a baseline system, with its default settings (translation table size 10, beam size 100). We train a trigram language model with the SRILM toolkit (Stocke, 2002). Whilst, the parameters for the maximum entropy model are developed based on the minimum error rate training method (Och, 2003). In the following experiments, to facilitate comparison, each time we train our regression models and the language model and translation model for Pharaoh on a common corpus, and use the same phrase translation table as Pharaoh’s to decode our systems. According to our preliminary experiments, with the beam size of 100, the search errors of our systems can be limited within 1.5%. Corpora To evaluate our models, we randomly take 12,000 sentences from the French-English portion of the 1996–2003 Europarl corpus (Koehn, 2005) for scaling-up training, 300 for test (Test), and 300 for the developme</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ofACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stocke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ofICSLP’02.</booktitle>
<contexts>
<context position="9605" citStr="Stocke, 2002" startWordPosition="1676" endWordPosition="1677"> of O(mnlyl), where l is the average length of the sentences yi in the training set. Note here ¯κx(x[1:lx], xi) can be pre-computed for lx from 1 to |x |before the beam search, which calls for O(m|x|) space. The average runtime complexity of the score function in Eq. 10 will be the same if we pre-compute K−1 x kx(x[1:lx]). 4 Experimental Results 4.1 Resource Description Baseline System To compare with previous work, we take Pharaoh (Koehn, 2004) as a baseline system, with its default settings (translation table size 10, beam size 100). We train a trigram language model with the SRILM toolkit (Stocke, 2002). Whilst, the parameters for the maximum entropy model are developed based on the minimum error rate training method (Och, 2003). In the following experiments, to facilitate comparison, each time we train our regression models and the language model and translation model for Pharaoh on a common corpus, and use the same phrase translation table as Pharaoh’s to decode our systems. According to our preliminary experiments, with the beam size of 100, the search errors of our systems can be limited within 1.5%. Corpora To evaluate our models, we randomly take 12,000 sentences from the French-Englis</context>
</contexts>
<marker>Stocke, 2002</marker>
<rawString>A. Stocke. 2002. SRILM – an extensible language modeling toolkit. In Proc. ofICSLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Szedmak</author>
<author>J Shawe-Taylor</author>
<author>E Parado-Hernandez</author>
</authors>
<title>Learning via linear operators: Maximum margin regression; multiclass and multiview learning at one-class complexity.</title>
<date>2006</date>
<tech>Technical report,</tech>
<location>PASCAL, Southampton, UK.</location>
<contexts>
<context position="2205" citStr="Szedmak et al., 2006" startWordPosition="366" endWordPosition="369">mple sentence pairs where yi is the translation of xi, we are trying to learn W a matrix represented linear operator, such that: Φy(y) = f(x) = WΦx(x) (1) Figure 1: Phrase alignment in SMT to predict the translation y for a new sentence x. Comparing with traditional methods, this model gives us a theoretical framework to capture higherdimensional dependencies within the sentences. To solve the multi-output regression problem, we investigate two models, least squares regression (LSR) similar to the technique presented in (Cortes et al., 2005), and maximum margin regression (MMR) introduced in (Szedmak et al., 2006). The rest of the paper is organized as follows. Section 2 gives a brief review of the regression models. Section 3 details the solution to the pre-image problem. We report the experimental results in Section 4, with discussions in Section 5. 2 Kernel Regression with Vector Outputs 2.1 Kernel Induced Feature Space In the practical learning process, only the inner products of the feature vectors are needed (see Section 2.2, 2.3 and 3), so we can perform the so-called kernel trick to avoid dealing with the very highdimensional feature vectors explicitly. That is, for x, z E X, a kernel function </context>
<context position="4097" citStr="Szedmak et al., 2006" startWordPosition="700" endWordPosition="703">ur original definition. 2.2 Least Squares Regression A basic method to solve the problem in Eq. 1 is least squares regression that seeks the matrix W minimizing the squared loss in Hy on the training set S: min kWMx − Myk2 (3) F where Mx = [Φx(x1), ..., Φx(xm)], My = [Φy(y1), ..., Φy(ym)], and k · kF denotes the Frobenius norm. Differentiating the expression and setting it to zero gives: 2WMxM⊤x − 2MyM⊤x = 0 ⇒ W = MyK−1 x M⊤ (4) x where Kx = M⊤x Mx = (κx(xi, xj)1≤i,j≤m) is the Gram matrix. 2.3 Maximum Margin Regression An alternative solution to our regression learning problem is proposed in (Szedmak et al., 2006), called maximum margin regression. If L2- normalized feature vectors are used in Eq. 1, denoted by ¯Φx(·) and ¯Φy(·), MMR solves the following optimization: where ¯κx(·, ·) and ¯κy(·, ·) denote the kernel functions associated to the respective normalized feature vectors. This dual problem can be solved efficiently with a perceptron algorithm based on an incremental subgradient method, of which the bounds on the complexity and achievable margin can be found in (Szedmak et al., 2006). Then according to Karush-Kuhn-Tucker theory, W is expressed as: ¯Φy(yi)¯Φx(xi)⊤ (7) In practice, MMR works bett</context>
</contexts>
<marker>Szedmak, Shawe-Taylor, Parado-Hernandez, 2006</marker>
<rawString>S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez. 2006. Learning via linear operators: Maximum margin regression; multiclass and multiview learning at one-class complexity. Technical report, PASCAL, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of HLTNAACL’04.</booktitle>
<contexts>
<context position="7177" citStr="Tillmann, 2004" startWordPosition="1264" endWordPosition="1265">ither the expression in Eq. 8 nor the one in Eq. 9 can be decomposed into a sum of subfunctions each involving feature components in a local area only. It means we cannot estimate exactly how well a part of the source sentence is translated, until we obtain a translation for the entire sentence, which prevents us doing a straightforward beam search similar to (Koehn, 2004). To simplify the situation, we restrict the reordering (distortion) of phrases that yield the output sentences by only allowing adjacent phrases to exchange their positions. (The discussion of this strategy can be found in (Tillmann, 2004).) We use x[i:j] and y[i:j] to denote the substrings of x and y that begin with the ith word and end with the jth. Now, if we go back to the implementation of a beam search, the current distortion restriction guarantees that in each expansion of the search states (hypotheses) we have x[1:lx] translated to a y[1:ly], either like state (a) or like state (b) in Fig. 2, where lx is the number of words translated in the source sentence, and ly is the number of words obtained in the translation. We assume that if y is a good translation of x, then y[1:ly] is a good translation of x[1:lx] as well. So</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>C. Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proc. of HLTNAACL’04.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>