<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007317">
<title confidence="0.999205">
Named Entity Recognition for Catalan
Using Spanish Resources
</title>
<author confidence="0.998109">
Xavier Carreras, Lluis Marquez, and Lluis Padro
</author>
<affiliation confidence="0.969103">
TALP Research Center, LSI Department
</affiliation>
<address confidence="0.7696625">
Universitat Politecnica de Catalunya
Jordi Girona, 1-3, E-08034, Barcelona
</address>
<email confidence="0.997812">
fcarreras,lluism,padrol@lsi.upc.es
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999774866666667">
This work studies Named Entity Recog-
nition (NER) for Catalan without mak-
ing use of annotated resources of this
language. The approach presented is
based on machine learning techniques
and exploits Spanish resources, either
by first training models for Spanish and
then translating them into Catalan, or by
directly training bilingual models. The
resulting models are retrained on unla-
belled Catalan data using bootstrapping
techniques. Exhaustive experimentation
has been conducted on real data, show-
ing competitive results for the obtained
NER systems.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918916666667">
A Named Entity (NE) is a lexical unit consisting
of a sequence of contiguous words which refers to
a concrete entity —such as a person, a location, an
organization or an artifact. Figure 1 contains an
example sentence, extracted from the Spanish cor-
pus referred in section 2 and translated into Cata-
lan, including several entities.
There is a wide consensus about that Named
Entity Recognition and Classification (NERC) are
Natural Language Processing tasks which may im-
prove the performance of many applications, such
as Information Extraction, Machine Translation,
Question Answering, Topic Detection and Track-
ing, etc. Thus, interest on detecting and classify-
ing those units in a text has kept on growing during
the last years.
Named Entity processing consists of two steps,
which are usually approached sequentially. First,
NEs are detected in the text, and their boundaries
delimited (Named Entity Recognition, NER). Sec-
ond, entities are classified in a predefined set of
classes, which usually contain labels such as per-
son, organization, location, etc. (Named Entity
Classification, NEC). In this paper we will focus
on the first of these stages, that is, Named Entity
boundary detection.
Previous work in this topic is mainly framed in
the Message Understanding Conferences (MUC),
devoted to Information Extraction, which included
a NERC task. Some MUC systems rely on
data–driven approaches, such as Nymble (Bikel
et al., 1997) which uses Hidden Markov Mod-
els, or ALEMBIC (Aberdeen et al., 1995), based
on Error Driven Transformation Based Learn-
ing. Others use only hand–coded knowledge, such
as FACILE (Black et al., 1998) which relies on
hand written unification context rules with cer-
tainty factors, or FASTUS (Appelt et al., 1995),
PLUM (Weischedel, 1995) and NetOwl Extrac-
tor (Krupka and Hausman, 1998) which are based
on cascaded finite state transducers or pattern
matching. There are also hybrid systems combin-
ing corpus evidence and gazetteer information (Yu
et al., 1998; Borthwick et al., 1998), or combining
hand–written rules with Maximum Entropy mod-
els to solve correference (Mikheev et al., 1998).
More recent approaches can be found in the pro-
ceedings of the shared task at the 2002 edition
</bodyText>
<page confidence="0.999383">
43
</page>
<note confidence="0.88024">
&amp;quot;El presidente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes
en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&amp;quot;
&amp;quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a
[Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&amp;quot;
</note>
<figureCaption confidence="0.9943455">
Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities
between brackets (PER=person, Loc=location, oRG=organization).
</figureCaption>
<bodyText confidence="0.999092738461539">
of the Conference on Natural Language Learning,
CoNLL&apos;02 (Tjong Kim Sang, 2002a), where sev-
eral machine–learning systems were compared at
the NERC task. Usually, machine learning (ML)
systems rely on algorithms that take as input a
set of labelled examples for the target task and
produce as output a model (which may take dif-
ferent forms, depending on the used algorithm)
that can be applied to new examples to obtain a
prediction. CoNLL&apos;02 participants used different
state–of–the–art ML algorithms, such as Support
Vector Machines (McNamee and Mayfield, 2002),
AdaBoost (Carreras et al., 2002; Tsukamoto et
al., 2002), Transformation–Based methods (Black
and Vasilakopoulos, 2002), Memory–based tech-
niques (Tjong Kim Sang, 2002b) or Hidden
Markov Models (Malouf, 2002), among others.
One remarkable aspect of most widely used ML
algorithms is that they are supervised, that is, they
require a set of labelled data to be trained on. This
may cause a severe bottleneck when such data
is not available or is expensive to obtain, which
is usually the case for minority languages with
few pre–existing linguistic resources and/or lim-
ited funding possibilities.
Our goal in this paper is to develop a low–cost
Named Entity recognition system for Catalan. To
achieve this, we take advantage of the facts that
Spanish and Catalan are two Romance languages
with similar syntactic structure, and that —since
Spanish and Catalan social and cultural environ-
ments greatly overlap— many Named Entities ap-
pear in both languages corpora. Relying on this
structural and content similarity, we will build our
Catalan NE recognizer on the following assump-
tions: (a) Named Entities appear in the same con-
texts in both languages, and (b) Named Entities are
composed by similar patterns in both languages.
The work departs from the use of existing anno-
tated Spanish corpora and machine learning tech-
niques to obtain Spanish NER models. We first
build low–cost resources (about 10 person–hours
each), namely a small Catalan training corpus
and translation dictionaries from Spanish to Cata-
lan. We then present and evaluate several strate-
gies to obtain a low–cost Catalan system. Sim-
ple naive strategies consist of learning from the
large Spanish corpus a model which makes no
use of lexical information, or learning a model
for Catalan using the small Catalan corpus. More
sophisticated strategies are translating a Spanish
model into Catalan, or directly learning a bilingual
model applicable to both languages. Experimen-
tation shows that the latter strategies, specially the
bilingual models, provide very good performance,
somewhat better than the former techniques. We
also study the evolution of these models within
a bootstrapping process, observing no significant
improvement.
Next section of the paper describes the used cor-
pora and evaluation measures. Section 3 describes
the NER learning system. Section 4 presents the
strategies to obtain a low–cost Catalan NER sys-
tem and provides results. Bootstrapping is studied
in section 5, and, finally, section 6 concludes.
</bodyText>
<sectionHeader confidence="0.956711" genericHeader="introduction">
2 Data and Evaluation
</sectionHeader>
<bodyText confidence="0.999837230769231">
The experimentation of this work has been car-
ried on two corpora, one for each language. In
both cases, the corpora consist of sentences ex-
tracted from news articles of same year, namely
year 2,000. The Spanish data corresponds to
the CoNLL 2002 Shared Task Spanish data, the
original source being the EFE Spanish Newswire
Agency. It consists of three files: a training set, a
development set and a test set. The first two are
used respectively to train and tune a system, and
the latter is used to evaluate and compare systems.
Table 1 shows the number of sentences, words and
Named Entities in each set. For Catalan, we had
</bodyText>
<page confidence="0.995254">
44
</page>
<table confidence="0.715321428571429">
lang. set #sent. #words #NEs
es train. 8,322 264,715 18,797
es dev. 1,914 52,923 4,351
es test 1,516 51,533 3,558
ca train. 817 23,177 1,232
ca test 844 23,595 1,338
ca unlab. 83,725 2,201,712 —
</table>
<tableCaption confidence="0.998072">
Table 1: Sizes of Spanish and Catalan data sets
</tableCaption>
<bodyText confidence="0.959054733333333">
available a large amount of news articles extracted
from the Catalan edition of the daily newspaper
El Periodic° de Catalunya (also from year 2,000).
From this corpus, we selected two sets for manual
annotation: a training set, to train a system, and a
test set, to perform the evaluation. The remaining
data was left as unlabelled data.
As evaluation method we use the common mea-
sures for recognition tasks: precision, recall and
F1. Precision is the percentage of NEs predicted
by a system which are correct. Recall is the per-
centage of NEs in the data that a system correctly
recognizes. Finally, the F1 measure computes the
harmonic mean of precision (p) and recall (r) as
2 p • rl(p + r).
</bodyText>
<sectionHeader confidence="0.915432" genericHeader="method">
3 The Spanish NER System
</sectionHeader>
<bodyText confidence="0.999926783783784">
The Spanish NER system is based on the best sys-
tem at CoNLL&apos;02, which makes use of a set of
AdaBoost–based binary classifiers for recognizing
the Named Entities in running text. See (Carreras
et al., 2002) for details.
The NE recognition task is performed as a se-
quence tagging problem through the well–known
BIO labelling scheme. Here, the input sentence
is treated as a word sequence and the output tag-
ging codifies the NEs in the sentence. In particu-
lar, each word is tagged as either the beginning of
a NE (B tag), a word inside a NE (I tag), or a word
outside a NE (0 tag). In our case, a NER model is
composed by: (a) a representation function, which
maps a word and its context into a set of features,
and (b) three binary classifiers (one correspond-
ing to each tag) which, operating on the features,
are used for tagging each word. When tagging, a
sentence is processed from left to right, selecting
for each word the tag with maximum confidence
that is coherent with the current solution (I–tag
sequences must be preceded by a B–tag). When
learning a model, all the words in the training set
are used as training examples, applying a one–vs–
all binarization of the 3–class classification prob-
lem.
The representation consists in a shifting win-
dow anchored in a word w, which encodes the lo-
cal context of w with which a classifier will oper-
ate. In the window, each word around w is codi-
fied with a set of primitive features, together with
its relative position to w. Each primitive feature
with each relative position and each possible value
forms a final binary feature for the classifier (e.g.,
&amp;quot;the word_form at position -2 is calle&amp;quot;). Particu-
larly, the set of primitive features applied to each
word in the window is the following:
</bodyText>
<listItem confidence="0.994258928571429">
• Lexical Features The word forms.
• Orthographic Features These are binary
and not mutually exclusive features that test
whether the following predicates hold in the
word: initial-caps, all-caps, contains-digits,
all-digits, alphanumeric, roman-number,
contains-dots, contains-hyphen, acronym,
lonely-initial, punctuation-mark, single-
char, functional-word, and URL. Functional
words are determiners and prepositions
which typically appear inside NEs.
• Affixes Test whether a word beginning (or
ending) matches with a common NE prefix
(or suffix). The list of affixes has been auto-
matically extracted from the Spanish training
set, by taking those NE affixes of up to 4 sym-
bols which occur more than 100 times.
• Word Type Patterns The type of a word
is either functional, capitalized, lowercased,
punctuation mark, quote or other. Each con-
junction of types of contiguous words is a
word type pattern, but only patterns in the
window which include the anchoring word
are considered.
• Left Predictions The tags being predicted in
the current classification. These features only
apply to the words in the window to the left
of the anchoring word w.
</listItem>
<bodyText confidence="0.979163">
As learning algorithm we use the binary
AdaBoost with confidence rated predictions. The
</bodyText>
<page confidence="0.995965">
45
</page>
<bodyText confidence="0.999975352941177">
idea of this algorithm is to learn an accurate strong
classifier by linearly combining, in a weighted
voting scheme, many simple and moderately—
accurate base classifiers or rules. Each base rule
is learned sequentially by presenting the base
learning algorithm a weighting over the examples,
which is dynamically adjusted depending on the
behavior of the previously learned rules. We refer
the reader to (Schapire and Singer, 1999) for de-
tails about the general algorithm, and to (Schapire,
2002) for successful applications to many areas,
including several NLP tasks.
In our setting, the boosting algorithm combines
several small fixed—depth decision trees. Each
branch of a tree is, in fact, a conjunction of binary
features, allowing the strong boosting classifier to
work with complex and expressive rules.
</bodyText>
<sectionHeader confidence="0.925161" genericHeader="method">
4 Porting to Catalan
</sectionHeader>
<bodyText confidence="0.99987625">
In this section we study the portability of a NER
system from Spanish to Catalan. Our approach is
to port a NER system by porting the model fea-
tures from Spanish to Catalan. In particular, we
concentrate on the features which are language
dependent, namely, the lexical features (or word
forms) and the functional words. All other fea-
tures are left unchanged.
Two alternative translation dictionaries from
Spanish to Catalan and vice-versa have been built
for the task. They contain a one to one correspon-
dence between Spanish and Catalan words. For in-
stance, an entry in a dictionary is &amp;quot;calle caner&amp;quot;,
meaning that the Spanish word &amp;quot;calle&amp;quot; (&amp;quot;street&amp;quot; in
English) corresponds to the Catalan word &amp;quot;caner&amp;quot;.
In order to obtain the relevant vocabulary for
NER, we have run several trainings of the Span-
ish NER system by varying the system parameters,
and we have extracted from the learned models all
the involved Spanish lexical features. These Span-
ish words form a set of 5,024 entries.
The first dictionary has been manually com-
pleted, with an estimated cost of about 10 person
hours of a bilingual speaker (7.2 sec/word). Note
that translations are made with no context infor-
mation, and with no linguistic criteria. The trans-
lator&apos;s common sense is blindly assumed to select
the best choice among all possible translations.
The second dictionary has been automatically
completed using the InterNOSTRUM Spanish—
Catalan machine translation system developed by
the Software Department of the University of Ala-
cane . In this case, the translations have also been
resolved without any context information, and the
entries not recognized by InterNOSTRUM (about
17%) have been left unchanged.
</bodyText>
<subsectionHeader confidence="0.997144">
4.1 Model Translation
</subsectionHeader>
<bodyText confidence="0.999972052631579">
Our first approach to obtain a NER model for
Catalan consists in first learning a NER model for
Spanish using Spanish annotated data, and then
translating its lexical features from Spanish into
Catalan using the translation dictionary.
In our particular case, a NER model is com-
posed by the B, I and 0 classifiers, each of which
is a combination of a number of base decision
trees. The model translation, therefore, consists
in translating every decision tree by translating
those nodes in the tree which evaluate lexical fea-
tures. For instance, considering the translation
&amp;quot;calle caner&amp;quot;, a node for Spanish with feature
&amp;quot;word:-2:calle&amp;quot;, testing whether the word form at
relative position -2 is &amp;quot;calle&amp;quot;, will be translated
into the node for Catalan &amp;quot;word:-2:carrer&amp;quot;, which
will test whether the -2 word is &amp;quot;caner&amp;quot;.
As a result, we obtain models which are trained
on Spanish and applied to Catalan text.
</bodyText>
<subsectionHeader confidence="0.914065">
4.2 Cross—Linguistic Features
</subsectionHeader>
<bodyText confidence="0.9997322">
As a more sophisticated alternative, we propose
a bilingual model which works for Spanish and
Catalan at the same time. We do this by using
what we call cross—linguistic features, instead of
the monolingual word forms specified above. As-
sume a feature lang which takes value es or ca,
depending on the language under consideration.
A cross—linguistic feature is just a binary feature
corresponding to an entry in the translation dictio-
nary, &amp;quot;es_w ca_w&amp;quot;, which is satisfied as follows:
</bodyText>
<listItem confidence="0.8099608">
1 if w = es_w
and lang = es
1 if w = ca_w
and lang = ca
0 otherwise
</listItem>
<footnote confidence="0.882277666666667">
1The InterNOSTRUM system is freely available at the fol-
lowing URL: http://www.internostrum.com.
X—Linges_wr,ca_w (w)
</footnote>
<page confidence="0.996132">
46
</page>
<bodyText confidence="0.999537363636363">
This representation allows to learn from a cor-
pus consisting of mixed Spanish and Catalan ex-
amples. The idea here is to take advantage of the
fact that the concept of NE is mostly shared by
both languages, but differs in the lexical informa-
tion, which we exploit through the lexical trans-
lations. With this we can learn a bilingual model
which is able to recognize NEs both for Spanish
and Catalan, but that may be trained with few —
or even any— data of one language, in our case
Catalan.
</bodyText>
<subsectionHeader confidence="0.99943">
4.3 Direct Learning in Catalan
</subsectionHeader>
<bodyText confidence="0.99817625">
A third approach is the usual learning of a NER
system using training data of the same language.
Since our interest relies on developing a low–cost
NER system for Catalan, we have performed stan-
dard learning on a small training set (described in
table 1), with an annotation cost comparable to the
cost of building the translation dictionary (about
10 person hours).
</bodyText>
<subsectionHeader confidence="0.619449">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.99994598630137">
Preliminary tuning on Spanish was performed on
the Spanish development set, in order to fix learn-
ing parameters. The window sizes were set to 3
words around, except for the orthographic win-
dow, with size of 1 word around. Concerning clas-
sifiers, the depth of the base decision trees was
fixed to 4 levels (i.e., tree branches represent con-
junctions of up to 4 basic features). When appli-
cable, the number of decision trees per classifier
was automatically tuned in the Spanish develop-
ment set selecting, from up to 2,000 base trees, the
number which maximizes the F1 measure. Other-
wise it was fixed to 800.
First, in order to have a baseline for the data
sets, two basic models were learned. The first,
NO_LEX, makes no use of lexical information at
all, that is, focuses only on orthographic features,
affixes, type patterns and left predictions. We
trained this model on the Spanish training data
and we directly applied it to both languages. As
a second baseline, a model for Catalan (including
lexical information) LEx.ca, was trained using the
small Catalan training set.
Following the approach described in Section
4.1, a model was learned on the Spanish training
set, and then translated into Catalan, generating
the model LEx.es2ca. Note that this model is also
applicable both to Spanish and Catalan, consider-
ing, respectively, the learned set of Spanish lexical
forms or the translated Catalan ones. In addition,
we tested the influence of cross–linguistic features
presented in Section 4.2. We trained one model,
X-LING„, only with the Spanish training data, and
a second model, X-LINGmix, using both the Span-
ish training data and the Catalan training set. In
both approaches the experiments were replicated
using the two available translation dictionaries.
Table 2 presents the results of all the learned
models on the test sets. Clearly, comparing the
performance of the NO_LEX model versus the oth-
ers, it can be stated that lexical information signif-
icantly helps on the NER task on both languages.
Looking at the results on the Catalan test (right
block), all the models using the manual dictionary
achieve a very competitive performance over 90%
of F1 measure. Therefore, the techniques to adapt
a NER model to Catalan seem to work consider-
ably well. The LEx.ca model performs somewhat
worse (89.18%) than others (probably because of
the reduced size of the training set), indicating
that, in similar conditions of annotation effort, it
is preferable to translate the models than to learn
from the small Catalan corpus.
The LEx.es2ca and X-LING„ models perform
nearly the same. Actually, since they are trained
on the same Spanish data, the models are fairly
equivalent, and the minor differences may be at-
tributed to the fixed vocabulary of the cross–
linguistic model. Besides, the X-LINGInix model,
trained with mixed corpora, achieves the best re-
sults (91.18%), which supports our arguments on
learning simultaneously from both languages.
Another positive result shown in table 2 is that
the X-LING models using the automatically gen-
erated dictionary perform almost as well as using
the manual dictionary (a loss of about 0.5 points in
F1 is observed in both cases). After a manual in-
spection, we explain the bad results of LEx.es2ca
with the automatic dictionary (87.53% compared
to 90.55%) by the large number of errors coming
from the translation of Spanish words, which are
directly applied on the Catalan data. X-LING mod-
els perform instead a new training step and they
</bodyText>
<page confidence="0.997143">
47
</page>
<table confidence="0.8856292">
es train ca train dicc. es test ca test
prec. rec. Fi prec. rec. Fi
NO_LEX yes no - 89.31 88.03 88.67 82.80 82.21 82.50
LEX.ca no yes - - - - 90.98 87.44 89.18
LEX.es2ca yes no man. 92.81 92.89 92&apos;85 89.14 92.00 90.55
aut. 83.85 91.55 87.53
X-LINGes yes no man. 92.25 92.64 92.44 90.78 89.76 90.27
aut. 92.23 92.69 92.46 89.95 89.61 89.78
X-LINGmix Yes yes man. 92.27 92.53 92.40 91.95 90.43 91.18
aut. 92.57 92.39 92.48 91.29 90.13 90.71
</table>
<tableCaption confidence="0.961176">
Table 2: Evaluation of the learned models on the test datasets for Spanish (es) and Catalan (ca). The &amp;quot;es&amp;quot;
</tableCaption>
<bodyText confidence="0.995465371428571">
and &amp;quot;ca train&amp;quot; columns indicate the training material used in each model. The &amp;quot;dim&amp;quot; column specifies
the dicctionary (either manual or automatic) used for translating models. The NO_LEX model learns
without making use of lexical information. The LEX.ca model is a baseline standard model developed on
Catalan. The LEx.es2ca is a translated model from Spanish to Catalan. The X-LING models are bilingual
models using cross-linguistic features.
are capable of discarding useless erroneous cross-
linguistic features.
Regarding the performance on Spanish (left
block), the original model, LEx.es2ca, working
with Spanish lexical information, obtains the best
results (92.85%), but cross-linguistic models are
still competitive (with a small loss of 0.4 points in
F1). This fact indicates that training with both lan-
guages at the same time does not significantly hurt
the performance of the individual Spanish model.
Additionally, the multilingual models are simpler
to use, since they work straightforwardly with both
languages, whereas form-based translated models
are specific for each language.
We would like to note also that the systems
achieve the same order of performance for both
languages, which was shown to be very competi-
tive in CoNLL&apos; 02. Although the table figures cor-
respond to evaluations in different sets, and thus,
can not be directly compared, the two corpora are
similar, since both consist of news article from the
same dates and geographical area.
As far as the cost concerns, it happens that the
better the performance of a model, the more the
resources needed to obtain it. Probably, the best
tradeoff is observed in the case of X-LINGmix with
the automatic dictionary, which allows to almost
automatically construct an accurate NER system
for Catalan (90.71%) at the only cost of 10 person
hours of corpus annotation.
</bodyText>
<sectionHeader confidence="0.913325" genericHeader="method">
5 Bootstrapping the models
</sectionHeader>
<bodyText confidence="0.999918909090909">
This section describes an attempt to improve the
NER models via bootstrapping techniques, that is,
making use of the available large amount of unla-
belled data in Catalan.
We describe a simple, naive strategy for the
bootstrapping process. The unlabelled data in
Catalan has been randomly divided into a number
of equal-sized disjoint subsets Si . . . SN, contain-
ing 1,000 sentences each. Given an initial NER
model Mo and a base labelled data set TL, the pro-
cess is as follows:
</bodyText>
<listItem confidence="0.999136166666667">
1. For i = 1 N do :
(a) Identify the Named Entities in Si ...
using model
(b) Learn a new model Mi using as training
data TL U Vi=1 S.
2. Output Model MN.
</listItem>
<bodyText confidence="0.996799333333333">
At each iteration, a new unlabelled fold is in-
cluded in the learning process. First, the folds
are labelled by the current model, and then, a new
model is learned using the base training data plus
the label-predicted folds.
We have run the process for three of the mod-
els above, always using the manual dictionary:
LEX.ca, with Catalan training set as TL; X-LINGes,
with Spanish training set as TL; and X-LINGmix,
</bodyText>
<page confidence="0.999069">
48
</page>
<figureCaption confidence="0.993964">
Figure 2: Progress of the F1 measure through
bootstrapping iterations.
</figureCaption>
<bodyText confidence="0.998321820512821">
with Tr, as the union of the Spanish and Catalan
training material. Since the LEx.es2ca model can
not mix its initial Spanish training with the Cata-
lan folds, we have avoided the model in the ex-
periment. Figure 2 depicts the evolution of the F1
measure through the bootstrapping process, for 7
iterations.
The model LEx.ca experiments a sharp drop of
2 points in the first iteration, and beyond iteration
5 gets stable at 87.41%. In our opinion, the Cata-
lan training set is not big enough and the errors in
the retraining folds degrade the performance of the
bootstrapped model. On the other hand, the cross—
linguistic models show a slightly better behav-
ior, achieving a maximum increase of about 0.5
points, getting also somewhat stable beyond itera-
tion 5. Again, X-LINGnaix is slightly better than X-
LING„. Bootstrapping, therefore, is not very help-
ful on improving models. However, these models
seem to have learned a robust concept which over-
comes the errors produced when relabelling folds.
It is also interesting to realize that the inclusion of
the Catalan training is crucial in the difference in
performance between the cross—linguistic models:
the X-LING„ model is not able to acquire from
the unlabelled data the same behavior than the X-
LINGmix model, which has access to the manually
annotated Catalan set (nearly of the same size than
each fold).
More complex variations to the above boot-
strapping strategy have been experimented. Ba-
sically, our direction has concentrated on select-
ing from the unlabelled material only the &amp;quot;good&amp;quot;
sentences for the learning process, by taking those
which maximize a mean of the confidences of the
predictions on a sentence, or those in which two
different models agree on the prediction. In all
cases, results lead to conclusions similar to the
ones described above.
</bodyText>
<sectionHeader confidence="0.997247" genericHeader="evaluation">
6 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.99996141025641">
We have presented an experimental work on de-
veloping low—cost Named Entity recognizers for
a language with no available annotated resources,
using as a starting point existing resources for a
similar language. We have devised and evaluated
several strategies to build a Catalan NER system
using only annotated Spanish data and unlabelled
Catalan text, and compared our approach with a
classical bootstrapping setting where a small ini-
tial corpus in the target language is hand tagged.
The main conclusions drawn form the presented
results are: 1) At same cost, the hand translation of
a Spanish model is better than hand annotating a
small Catalan training corpus from which directly
learn a model. 2) The translation of the Span-
ish model can be automatically done by using a
Spanish—Catalan machine translation system, ob-
taining also very competitive results. 3) The best
strategy turned out to be the use of cross—linguistic
features, which enables the training of models us-
ing mixed corpora, and results in a system able to
work reasonably on both languages.
Results of the experiments with a simple boot-
strapping strategy suggest several conclusions.
First, LEx.ca is not improved via bootstrapping,
probably due to the small size of the Catalan train-
ing corpus. Second, bootstrapping slightly im-
proves initial X-LING models, producing robust
models which are not degraded by the noise intro-
duced in subsequent iterations of bootstrapping.
Some open issues that should be addressed in
the future include an improvement of the quality
and coverage of the automatic translation of dic-
tionary entries, and a further development of the
idea of cross—linguistic features, extending it ei-
ther from bilingual to multilingual translations, or
including semantic relations, through the use of
WordNet or similar ontologies. This could open
the door to apply the method to groups of similar
</bodyText>
<figure confidence="0.939544785714286">
Lex.ca
X-Ling es
- X-Ling mix
N
87
0 2 3 4 5 6
Bootstrapping Iteration
93
92
91
90
89
88
7
</figure>
<page confidence="0.998439">
49
</page>
<bodyText confidence="0.987103181818182">
languages (e.g., between Romance languages like
Catalan, French, Galician, Italian, Spanish, etc.).
In addition, bootstrapping techniques should be
better studied in this domain, in order to take ad-
vantage of the large quantities of available unla-
belled data. Particularly, we think that it is worth
investigating the size and selection of the retrain-
ing corpora, and the combination of several algo-
rithms or example views like in the co-training al-
gorithms presented in (Collins and Singer, 1999;
Abney, 2002).
</bodyText>
<sectionHeader confidence="0.829667" genericHeader="conclusions">
Acnowledgements
</sectionHeader>
<bodyText confidence="0.999952181818182">
The authors thank the anonymous reviewers for
their valuable comments and suggestions in order
to prepare the final version of this paper.
This research has been partially funded by
the Spanish Research Department (HERMES
T1C2000-0335-0O3-02, PETRA T1C2000-1735-
CO2-02), by the European Comission (MEAN-
ING IST-2001-34460), and by the Catalan Re-
search Department (CIRIT&apos;s consolidated re-
search group 2001SGR-00254 and research grant
2001FI-00663).
</bodyText>
<sectionHeader confidence="0.998077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953851851852">
J. Aberdeen, J. Burger, D. Day, L. Hirschman,
P. Robinson, and M. Vilain. 1995. MITRE: De-
scription of the ALEMBIC System Used for MUC-
6. In Proceedings of the 6th Messsage Understand-
ing Conference, pages 141-155, Columbia, Mary-
land.
S. Abney. 2002. Bootstrapping. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, Taipei, Taiwan.
D. Appelt, J. Hobbs, J. Bear, D. Israel, M. Kameyama,
A. Kehler, D. Martin, K. Myers, and M. Tyson.
1995. SRI International Fastus System MUC-6 Test
Results and Analysis. In Proceedings of the 6th
Messsage Understanding Conference, pages 237-
248, Columbia, Maryland.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: A High Performance Learning
Name-Finder. In Proceedings of the 5th Conference
on Applied Natural Language Processing, ANLP,
Washington DC.
W. Black and A. Vasilakopoulos. 2002. Language-
Independent Named Entity Classification by Mod-
ified Transformation-Based Learning and by De-
cision Tree Induction. In Proceedings of CoNLL-
2002, pages 159-162. Taipei, Taiwan.
W. Black, F. Rinaldi, and D. Mowatt. 1998. Facile:
Description of the NE System Used for MUC-7.
In Proceedings of the 7th Message Understanding
Conference.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grish-
man. 1998. NYU: Description of the MENE Named
Entity System as Used in MUC-7. In Proceedings of
the 7th Message Understanding Conference.
X. Carreras, L. Marquez, and L. Padro. 2002. Named
Entity Extraction Using AdaBoost. In Proceedings
of CoNLL-2002, pages 167-170. Taipei, Taiwan.
M. Collins and Y. Singer. 1999. Unsupervised Models
for Named Entity Classification. In Proceedings of
EMNLPNLC-99, College Park MD, USA.
G. Krupka and K. Hausman. 1998. IsoQuest, Inc.:
Description of the NetOwlTM Extractor System as
Used for MUC-7. In Proceedings of the 7th Mes-
sage Understanding Conference.
R. Malouf. 2002. Markov Models for Language-
Independent Named Entity Recognition. In Pro-
ceedings of CoNLL-2002, pages 187-190. Taipei,
Taiwan.
P. McNamee and J. Mayfield. 2002. Entity Extraction
Without Language-Specific Resources. In Proceed-
ings of CoNLL-2002, pages 183-186. Taipei, Tai-
wan.
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the LTG System Used for MUC-7. In Pro-
ceedings of the 7th Message Understanding Confer-
ence.
R. Schapire and Y. Singer. 1999. Improved Boost-
ing Algorithms Using Confidence-rated Predictions.
Machine Learning, 37(3):297-336.
R. Schapire. 2002. The Boosting Approach to Ma-
chine Learning. An Overview. In Proceedings of the
MSRI Workshop on Nonlinear Estimation and Clas-
sification, Berkeley, CA.
E. Tjong Kim Sang. 2002a. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In Proceedings of
CoNLL-2002, pages 155-158. Taipei, Taiwan.
E. Tjong Kim Sang. 2002b. Memory-Based Named
Entity Recognition. In Proceedings of CoNLL-2002,
pages 203-206. Taipei, Taiwan.
K. Tsukamoto, Y Mitsuishi, and M. Sassano. 2002.
Learning with Multiple Stacking for Named Entity
Recognition. In Proceedings of CoNLL-2002, pages
191-194. Taipei, Taiwan.
R. Weischedel. 1995. BBN: Description of the PLUM
System as Used for MUC-6. In Proceedings of the
6th Messsage Understanding Conference, pages 55-
69, Columbia, Maryland.
S. Yu, S. Bai, and P. Wu. 1998. Description of the
Kent Ridge Digital Labs System Used for MUC-7.
In Proceedings of the 7th Message Understanding
Conference.
</reference>
<page confidence="0.997689">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897389">
<title confidence="0.994546">Named Entity Recognition for Catalan Using Spanish Resources</title>
<author confidence="0.990204">Lluis Marquez Carreras</author>
<author confidence="0.990204">Padro</author>
<affiliation confidence="0.999101">TALP Research Center, LSI Department Universitat Politecnica de Catalunya</affiliation>
<address confidence="0.988599">Jordi Girona, 1-3, E-08034, Barcelona</address>
<email confidence="0.997343">fcarreras,lluism,padrol@lsi.upc.es</email>
<abstract confidence="0.9954868125">This work studies Named Entity Recognition (NER) for Catalan without making use of annotated resources of this language. The approach presented is based on machine learning techniques and exploits Spanish resources, either by first training models for Spanish and then translating them into Catalan, or by directly training bilingual models. The resulting models are retrained on unlabelled Catalan data using bootstrapping techniques. Exhaustive experimentation has been conducted on real data, showing competitive results for the obtained NER systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aberdeen</author>
<author>J Burger</author>
<author>D Day</author>
<author>L Hirschman</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>MITRE: Description of the ALEMBIC System Used for MUC6.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Messsage Understanding Conference,</booktitle>
<pages>141--155</pages>
<location>Columbia, Maryland.</location>
<contexts>
<context position="2338" citStr="Aberdeen et al., 1995" startWordPosition="348" endWordPosition="351">imited (Named Entity Recognition, NER). Second, entities are classified in a predefined set of classes, which usually contain labels such as person, organization, location, etc. (Named Entity Classification, NEC). In this paper we will focus on the first of these stages, that is, Named Entity boundary detection. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikhe</context>
</contexts>
<marker>Aberdeen, Burger, Day, Hirschman, Robinson, Vilain, 1995</marker>
<rawString>J. Aberdeen, J. Burger, D. Day, L. Hirschman, P. Robinson, and M. Vilain. 1995. MITRE: Description of the ALEMBIC System Used for MUC6. In Proceedings of the 6th Messsage Understanding Conference, pages 141-155, Columbia, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Abney, 2002</marker>
<rawString>S. Abney. 2002. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Appelt</author>
<author>J Hobbs</author>
<author>J Bear</author>
<author>D Israel</author>
<author>M Kameyama</author>
<author>A Kehler</author>
<author>D Martin</author>
<author>K Myers</author>
<author>M Tyson</author>
</authors>
<date>1995</date>
<booktitle>SRI International Fastus System MUC-6 Test Results and Analysis. In Proceedings of the 6th Messsage Understanding Conference,</booktitle>
<pages>237--248</pages>
<location>Columbia, Maryland.</location>
<contexts>
<context position="2577" citStr="Appelt et al., 1995" startWordPosition="386" endWordPosition="389"> on the first of these stages, that is, Named Entity boundary detection. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &amp;quot;El presidente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon</context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Kameyama, Kehler, Martin, Myers, Tyson, 1995</marker>
<rawString>D. Appelt, J. Hobbs, J. Bear, D. Israel, M. Kameyama, A. Kehler, D. Martin, K. Myers, and M. Tyson. 1995. SRI International Fastus System MUC-6 Test Results and Analysis. In Proceedings of the 6th Messsage Understanding Conference, pages 237-248, Columbia, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: A High Performance Learning Name-Finder.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<location>ANLP, Washington DC.</location>
<contexts>
<context position="2270" citStr="Bikel et al., 1997" startWordPosition="336" endWordPosition="339">ly. First, NEs are detected in the text, and their boundaries delimited (Named Entity Recognition, NER). Second, entities are classified in a predefined set of classes, which usually contain labels such as person, organization, location, etc. (Named Entity Classification, NEC). In this paper we will focus on the first of these stages, that is, Named Entity boundary detection. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–wr</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: A High Performance Learning Name-Finder. In Proceedings of the 5th Conference on Applied Natural Language Processing, ANLP, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Black</author>
<author>A Vasilakopoulos</author>
</authors>
<title>LanguageIndependent Named Entity Classification by Modified Transformation-Based Learning and by Decision Tree Induction.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL2002,</booktitle>
<pages>159--162</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4278" citStr="Black and Vasilakopoulos, 2002" startWordPosition="636" endWordPosition="639">L&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Carreras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity recognition system for Catalan. To achieve this, we take advant</context>
</contexts>
<marker>Black, Vasilakopoulos, 2002</marker>
<rawString>W. Black and A. Vasilakopoulos. 2002. LanguageIndependent Named Entity Classification by Modified Transformation-Based Learning and by Decision Tree Induction. In Proceedings of CoNLL2002, pages 159-162. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Black</author>
<author>F Rinaldi</author>
<author>D Mowatt</author>
</authors>
<title>Facile: Description of the NE System Used for MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference.</booktitle>
<contexts>
<context position="2466" citStr="Black et al., 1998" startWordPosition="368" endWordPosition="371">s such as person, organization, location, etc. (Named Entity Classification, NEC). In this paper we will focus on the first of these stages, that is, Named Entity boundary detection. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &amp;quot;El president</context>
</contexts>
<marker>Black, Rinaldi, Mowatt, 1998</marker>
<rawString>W. Black, F. Rinaldi, and D. Mowatt. 1998. Facile: Description of the NE System Used for MUC-7. In Proceedings of the 7th Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<date>1998</date>
<booktitle>NYU: Description of the MENE Named Entity System as Used in MUC-7. In Proceedings of the 7th Message Understanding Conference.</booktitle>
<contexts>
<context position="2848" citStr="Borthwick et al., 1998" startWordPosition="428" endWordPosition="431">proaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &amp;quot;El presidente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&amp;quot; &amp;quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&amp;quot; Figure 1: Exampl</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998. NYU: Description of the MENE Named Entity System as Used in MUC-7. In Proceedings of the 7th Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Marquez</author>
<author>L Padro</author>
</authors>
<title>Named Entity Extraction Using AdaBoost.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>167--170</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4190" citStr="Carreras et al., 2002" startWordPosition="626" endWordPosition="629">ation, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Carreras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop</context>
<context position="8470" citStr="Carreras et al., 2002" startWordPosition="1333" endWordPosition="1336">g data was left as unlabelled data. As evaluation method we use the common measures for recognition tasks: precision, recall and F1. Precision is the percentage of NEs predicted by a system which are correct. Recall is the percentage of NEs in the data that a system correctly recognizes. Finally, the F1 measure computes the harmonic mean of precision (p) and recall (r) as 2 p • rl(p + r). 3 The Spanish NER System The Spanish NER system is based on the best system at CoNLL&apos;02, which makes use of a set of AdaBoost–based binary classifiers for recognizing the Named Entities in running text. See (Carreras et al., 2002) for details. The NE recognition task is performed as a sequence tagging problem through the well–known BIO labelling scheme. Here, the input sentence is treated as a word sequence and the output tagging codifies the NEs in the sentence. In particular, each word is tagged as either the beginning of a NE (B tag), a word inside a NE (I tag), or a word outside a NE (0 tag). In our case, a NER model is composed by: (a) a representation function, which maps a word and its context into a set of features, and (b) three binary classifiers (one corresponding to each tag) which, operating on the feature</context>
</contexts>
<marker>Carreras, Marquez, Padro, 2002</marker>
<rawString>X. Carreras, L. Marquez, and L. Padro. 2002. Named Entity Extraction Using AdaBoost. In Proceedings of CoNLL-2002, pages 167-170. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLPNLC-99,</booktitle>
<location>College Park MD, USA.</location>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceedings of EMNLPNLC-99, College Park MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Krupka</author>
<author>K Hausman</author>
</authors>
<title>IsoQuest, Inc.: Description of the NetOwlTM Extractor System as Used for MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference.</booktitle>
<contexts>
<context position="2650" citStr="Krupka and Hausman, 1998" startWordPosition="397" endWordPosition="400">ion. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &amp;quot;El presidente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&amp;quot; &amp;quot;El pr</context>
</contexts>
<marker>Krupka, Hausman, 1998</marker>
<rawString>G. Krupka and K. Hausman. 1998. IsoQuest, Inc.: Description of the NetOwlTM Extractor System as Used for MUC-7. In Proceedings of the 7th Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>Markov Models for LanguageIndependent Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>187--190</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4366" citStr="Malouf, 2002" startWordPosition="651" endWordPosition="652">ly, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Carreras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity recognition system for Catalan. To achieve this, we take advantage of the facts that Spanish and Catalan are two Romance languages with similar syntact</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. 2002. Markov Models for LanguageIndependent Named Entity Recognition. In Proceedings of CoNLL-2002, pages 187-190. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McNamee</author>
<author>J Mayfield</author>
</authors>
<title>Entity Extraction Without Language-Specific Resources.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>183--186</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4157" citStr="McNamee and Mayfield, 2002" startWordPosition="621" endWordPosition="624">s between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Carreras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our</context>
</contexts>
<marker>McNamee, Mayfield, 2002</marker>
<rawString>P. McNamee and J. Mayfield. 2002. Entity Extraction Without Language-Specific Resources. In Proceedings of CoNLL-2002, pages 183-186. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>C Grover</author>
<author>M Moens</author>
</authors>
<title>Description of the LTG System Used for MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference.</booktitle>
<contexts>
<context position="2954" citStr="Mikheev et al., 1998" startWordPosition="444" endWordPosition="447">1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &amp;quot;El presidente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&amp;quot; &amp;quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&amp;quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=</context>
</contexts>
<marker>Mikheev, Grover, Moens, 1998</marker>
<rawString>A. Mikheev, C. Grover, and M. Moens. 1998. Description of the LTG System Used for MUC-7. In Proceedings of the 7th Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved Boosting Algorithms Using Confidence-rated Predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="11681" citStr="Schapire and Singer, 1999" startWordPosition="1865" endWordPosition="1868">ssification. These features only apply to the words in the window to the left of the anchoring word w. As learning algorithm we use the binary AdaBoost with confidence rated predictions. The 45 idea of this algorithm is to learn an accurate strong classifier by linearly combining, in a weighted voting scheme, many simple and moderately— accurate base classifiers or rules. Each base rule is learned sequentially by presenting the base learning algorithm a weighting over the examples, which is dynamically adjusted depending on the behavior of the previously learned rules. We refer the reader to (Schapire and Singer, 1999) for details about the general algorithm, and to (Schapire, 2002) for successful applications to many areas, including several NLP tasks. In our setting, the boosting algorithm combines several small fixed—depth decision trees. Each branch of a tree is, in fact, a conjunction of binary features, allowing the strong boosting classifier to work with complex and expressive rules. 4 Porting to Catalan In this section we study the portability of a NER system from Spanish to Catalan. Our approach is to port a NER system by porting the model features from Spanish to Catalan. In particular, we concent</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R. Schapire and Y. Singer. 1999. Improved Boosting Algorithms Using Confidence-rated Predictions. Machine Learning, 37(3):297-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schapire</author>
</authors>
<title>The Boosting Approach to Machine Learning. An Overview.</title>
<date>2002</date>
<booktitle>In Proceedings of the MSRI Workshop on Nonlinear Estimation and Classification,</booktitle>
<location>Berkeley, CA.</location>
<contexts>
<context position="11746" citStr="Schapire, 2002" startWordPosition="1878" endWordPosition="1879">t of the anchoring word w. As learning algorithm we use the binary AdaBoost with confidence rated predictions. The 45 idea of this algorithm is to learn an accurate strong classifier by linearly combining, in a weighted voting scheme, many simple and moderately— accurate base classifiers or rules. Each base rule is learned sequentially by presenting the base learning algorithm a weighting over the examples, which is dynamically adjusted depending on the behavior of the previously learned rules. We refer the reader to (Schapire and Singer, 1999) for details about the general algorithm, and to (Schapire, 2002) for successful applications to many areas, including several NLP tasks. In our setting, the boosting algorithm combines several small fixed—depth decision trees. Each branch of a tree is, in fact, a conjunction of binary features, allowing the strong boosting classifier to work with complex and expressive rules. 4 Porting to Catalan In this section we study the portability of a NER system from Spanish to Catalan. Our approach is to port a NER system by porting the model features from Spanish to Catalan. In particular, we concentrate on the features which are language dependent, namely, the le</context>
</contexts>
<marker>Schapire, 2002</marker>
<rawString>R. Schapire. 2002. The Boosting Approach to Machine Learning. An Overview. In Proceedings of the MSRI Workshop on Nonlinear Estimation and Classification, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>155--158</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3673" citStr="Sang, 2002" startWordPosition="547" endWordPosition="548">ente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&amp;quot; &amp;quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&amp;quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Carreras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E. Tjong Kim Sang. 2002a. Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of CoNLL-2002, pages 155-158. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
</authors>
<title>Memory-Based Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>203--206</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3673" citStr="Sang, 2002" startWordPosition="547" endWordPosition="548">ente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&amp;quot; &amp;quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&amp;quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Carreras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E. Tjong Kim Sang. 2002b. Memory-Based Named Entity Recognition. In Proceedings of CoNLL-2002, pages 203-206. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tsukamoto</author>
<author>Y Mitsuishi</author>
<author>M Sassano</author>
</authors>
<title>Learning with Multiple Stacking for Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>191--194</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4215" citStr="Tsukamoto et al., 2002" startWordPosition="630" endWordPosition="633">). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Carreras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity </context>
</contexts>
<marker>Tsukamoto, Mitsuishi, Sassano, 2002</marker>
<rawString>K. Tsukamoto, Y Mitsuishi, and M. Sassano. 2002. Learning with Multiple Stacking for Named Entity Recognition. In Proceedings of CoNLL-2002, pages 191-194. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
</authors>
<title>BBN: Description of the PLUM System as Used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Messsage Understanding Conference,</booktitle>
<pages>55--69</pages>
<location>Columbia, Maryland.</location>
<contexts>
<context position="2602" citStr="Weischedel, 1995" startWordPosition="391" endWordPosition="392">s, that is, Named Entity boundary detection. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &amp;quot;El presidente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]</context>
</contexts>
<marker>Weischedel, 1995</marker>
<rawString>R. Weischedel. 1995. BBN: Description of the PLUM System as Used for MUC-6. In Proceedings of the 6th Messsage Understanding Conference, pages 55-69, Columbia, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yu</author>
<author>S Bai</author>
<author>P Wu</author>
</authors>
<title>Description of the Kent Ridge Digital Labs System Used for MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference.</booktitle>
<contexts>
<context position="2823" citStr="Yu et al., 1998" startWordPosition="424" endWordPosition="427">on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &amp;quot;El presidente del [Comite OlImpico Internacional]oRG, [José Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&amp;quot; &amp;quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justic</context>
</contexts>
<marker>Yu, Bai, Wu, 1998</marker>
<rawString>S. Yu, S. Bai, and P. Wu. 1998. Description of the Kent Ridge Digital Labs System Used for MUC-7. In Proceedings of the 7th Message Understanding Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>