<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015329">
<title confidence="0.996791">
What Decisions Have You Made: Automatic Decision Detection in
Conversational Speech
</title>
<author confidence="0.995709">
Pei-Yun Hsueh
</author>
<affiliation confidence="0.997762">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.965737">
Edinburgh EH9 8WL, UK
</address>
<email confidence="0.997714">
p.hsueh@ed.ac.uk
</email>
<author confidence="0.991083">
Johanna Moore
</author>
<affiliation confidence="0.997767">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.965728">
Edinburgh EH9 8WL, UK
</address>
<email confidence="0.997631">
J.Moore@ed.ac.uk
</email>
<sectionHeader confidence="0.995614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931176470588">
This study addresses the problem of au-
tomatically detecting decisions in conver-
sational speech. We formulate the prob-
lem as classifying decision-making units
at two levels of granularity: dialogue acts
and topic segments. We conduct an em-
pirical analysis to determine the charac-
teristic features of decision-making dia-
logue acts, and train MaxEnt models using
these features for the classification tasks.
We find that models that combine lexi-
cal, prosodic, contextual and topical fea-
tures yield the best results on both tasks,
achieving 72% and 86% precision, respec-
tively. The study also provides a quantita-
tive analysis of the relative importance of
the feature types.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889583333333">
Making decisions is an important aspect of conver-
sations in collaborative work. In the context of meet-
ings, the proposed argumentative models, e.g., in
Pallotta et al. (2005) and Rienks et al. (2005), have
specified decisions as an essential outcome of meet-
ings. Whittaker et al. (2005) have also described
how reviewing decisions is critical to the re-use of
meeting recordings. For example, a new engineer
who just get assigned to a project will need to know
what major decisions have been made in previous
meetings. Unless all decisions are recorded in meet-
ing minutes or annotated in the speech recordings, it
</bodyText>
<page confidence="0.968928">
25
</page>
<bodyText confidence="0.999802444444445">
is difficult to locate the decision points by the brows-
ing and playback utilities alone.
Banerjee and Rudnicky (2005) have shown that
it is easier for users to retrieve the information
they seek if the meeting record includes information
about topic segmentation, speaker role, and meet-
ing state (e.g., discussion, presentation, briefing). To
assist users in identifying or revisiting decisions in
meeting archives, our goal is to automatically iden-
tify the dialogue acts and segments where decisions
are made. Because reviewing decisions is indis-
pensable in collaborative work, automatic decision
detection is expected to lend support to computer-
assisted meeting tracking and understanding (e.g.,
assisting in the fulfilment of the decisions made in
the meetings) and the development of group infor-
mation management applications (e.g., constructing
group memory).
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998377857142857">
Spontaneous face-to-face dialogues in meetings vi-
olate many assumptions made by techniques pre-
viously developed for broadcast news (e.g., TDT
and TRECVID), telephone conversations (e.g.,
Switchboard), and human-computer dialogues (e.g.,
DARPA Communicator). In order to develop
techniques for understanding multiparty dialogues,
smart meeting rooms have been built at several insti-
tutes to record large corpora of meetings in natural
contexts, including CMU (Waibel et al., 2001), LDC
(Cieri et al., 2002), NIST (Garofolo et al., 2004),
ICSI (Janin et al., 2003), and in the context of the
IM2/M4 project (Marchand-Mailet, 2003). More
recently, scenario-based meetings, in which partic-
</bodyText>
<note confidence="0.829643">
Proceedings of NAACL HLT 2007, pages 25–32,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999980456521739">
ipants are assigned to different roles and given spe-
cific tasks, have been recorded in the context of
the CALO project (the Y2 Scenario Data) (CALO,
2003) and the AMI project (Carletta et al., 2005).
The availability of meeting corpora has enabled
researchers to begin to develop descriptive models
of meeting discussions. Some researchers are mod-
elling the dynamics of the meeting, exploiting dia-
logue models previously proposed for dialogue man-
agement. For example, Niekrasz et al. (2005) use
the Issue-Based Information System (IBIS) model
(Kunz and Ritte, 1970) to incorporate the history
of dialogue moves into the Multi-Modal Discourse
(MMD) ontology. Other researchers are modelling
the content of the meeting using the type of struc-
tures proposed in work on argumentation. For ex-
ample, Rienks et al. (2005) have developed an ar-
gument diagramming scheme to visualize the rela-
tions (e.g., positive, negative, uncertain) between ut-
terances (e.g., statement, open issue), and Marchand
et al. (2003) propose a schema to model different ar-
gumentation acts (e.g., accept, request, reject) and
their organization and synchronization. Decisions
are often seen as a by-product of these models.
Automatically extracting these argument mod-
els is a challenging task. However, researchers
have begun to make progress towards this goal.
For example, Gatica et al. (2005) and Wrede and
Shriberg (2003) automatically identify the level of
emotion in meeting spurts (e.g., group level of in-
terest, hot spots). Other researchers have developed
models for detecting agreement and disagreement
in meetings, using models that combine lexical fea-
tures with prosodic features (e.g., pause, duration,
F0, speech rate) (Hillard et al., 2003) and struc-
tural information (e.g., the previous and following
speaker) (Galley et al., 2004). More recently, Purver
et al. (2006) have tackled the problem of detecting
one type of decision, namely action items, which
embody the transfer of group responsibility. How-
ever, no prior work has addressed the problem of au-
tomatically identifying decision-making units more
generally in multiparty meetings. Moreover, no pre-
vious research has provided a quantitative account
of the effects of different feature types on the task of
automatic decision detection.
</bodyText>
<sectionHeader confidence="0.967031" genericHeader="method">
3 Research Goal
</sectionHeader>
<bodyText confidence="0.999903178571429">
Our aim is to develop models for automatically de-
tecting segments of conversation that contain deci-
sions directly from the audio recordings and tran-
scripts of the meetings, and to identify the feature
combinations that are most effective for this task.
Meetings can be viewed at different levels of
granularity. In this study, we first consider how to
detect the dialogue acts that contain decision-related
information (DM DAs). Since it is often difficult
to interpret a decision without knowing the current
topic of discussion, we are also interested in detect-
ing decision-making segments at a coarser level of
granularity: topic segments. The task of automatic
decision detection can therefore be divided into two
subtasks: detecting DM DAs and detecting decision-
making topic segments (DM Segments).
In this study we propose to first empirically
identify the features that are most characteristic of
decision-making dialogue acts and then computa-
tionally integrate the characteristic features to locate
the DM DAs in meeting archives. For the latter task,
previous research on automatic meeting understand-
ing and tracking has commonly utilized a classifica-
tion framework, in which variants of generative and
conditional models are computed directly from data.
In this study, we use a Maximum Entropy (MaxEnt)
classifier to combine the decision characteristic fea-
tures to predict DM DAs and DM Segments.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="method">
4 Data
</sectionHeader>
<subsectionHeader confidence="0.999246">
4.1 Decision Annotation
</subsectionHeader>
<bodyText confidence="0.999848933333333">
In this study, we use a set of 50 scenario-driven
meetings (approximately 37,400 dialogue acts) that
have been segmented into dialogue acts and anno-
tated with decision information in the AMI meet-
ing corpus. These meetings are driven by a sce-
nario, wherein four participants play the role of
Project Manager, Marketing Expert, Industrial De-
signer, and User Interface Designer in a design team
in a series of four meetings. Each series of meet-
ing recordings uses four distinctive speakers differ-
ent from other series. The corpus includes manual
transcripts for all meetings. It also comes with in-
dividual sound files recorded by close-talking head-
mounted microphones and cross-talking sound files
recorded by desktop microphones.
</bodyText>
<page confidence="0.991416">
26
</page>
<subsectionHeader confidence="0.947368">
4.1.1 Decision-Making Dialogue Acts
</subsectionHeader>
<bodyText confidence="0.999980175">
In fact, it is difficult to determine whether a di-
alogue act contains information relevant to any de-
cision point without knowing what decisions have
been made in the meeting. Therefore, in this study
DM DAs are annotated in a two-phase process:
First, annotators are asked to browse through the
meeting record and write an abstractive summary
directed to the project manager about the decisions
that have been made in the meeting. Next, another
group of three annotators are asked to produce ex-
tractive summaries by selecting a subset (around
10%) of dialogue acts which form a summary of this
meeting for the absent manager to understand what
has transpired in the meeting.
Finally, this group of annotators are asked to go
through the extractive dialogue acts one by one and
judge whether they support any of the sentences in
the decision section of the abstractive summary; if a
dialogue act is related to any sentence in the decision
section, a “decision link” from the dialogue act to
the decision sentence is added. For those extracted
dialogue acts that do not have any closely related
sentence, the annotators are not obligated to specify
a link. We then label the dialogue acts that have one
or more decision links as DM DAs.
In the 50 meetings we used for the experiments,
the annotators have on average found four decisions
per meeting and specified around two decision links
to each sentence in the decision summary section.
Overall, 554 out of 37,400 dialogue acts have been
annotated as DM DAs, accounting for 1.4% of all di-
alogue acts in the data set and 12.7% of the orginal
extractive summary (which is consisted of the ex-
tracted dialogue acts). An earlier analysis has es-
tablished the intercoder reliability of the two-phase
process at the level of kappa ranging from 0.5 to
0.8. In this round of experiment, for each meeting
in the 50-meeting dataset we randomly choose the
DM DA annotation of one annotator as the sourec of
its ground truth data.
</bodyText>
<subsectionHeader confidence="0.90651">
4.1.2 Decision-Making Topic Segments
</subsectionHeader>
<bodyText confidence="0.999942678571428">
Topic segmentation has also been annotated for
the AMI meeting corpus. Annotators had the free-
dom to mark a topic as subordinated (down to two
levels) wherever appropriate. As the AMI meetings
are scenario-driven, annotators are expected to find
that most topics recur. Therefore, they are given a
standard set of topic descriptions that can be used
as labels for each identified topic segment. Annota-
tors will only add a new label if they cannot find a
match in the standard set. The AMI scenario meet-
ings contain around 14 topic segments per meeting.
Each segment lasts on average 44 dialogue acts long
and contains two DM DAs.
DM Segments are operationalized as topic seg-
ments that contain one or more DM DAs. Over-
all, 198 out of 623 (31.78%) topic segments in the
50-meeting dataset are DM Segments. As the meet-
ings we use are driven by a predetermined agenda,
we expect to find that interlocutors are more likely
to reach decisions when certain topics are brought
up. Analysis shows that some topics are indeed more
likely to contain decisions than others. For example,
80% of the segments labelled as Costing and 58%
of those labelled Budget are DM Segments, whereas
only 7% of the Existing Product segments and none
of the Trend-Watching segments are DM Segments.
Functional segments, such as Chitchat, Opening and
Closing, almost never include decisions.
</bodyText>
<subsectionHeader confidence="0.977785">
4.2 Features Used
</subsectionHeader>
<bodyText confidence="0.999996">
To provide a qualitative account of the effect of dif-
ferent feature types on the task of automatic decision
detection, we have conducted empirical analysis on
four major types of features: lexical, prosodic, con-
textual and topical features.
</bodyText>
<subsubsectionHeader confidence="0.526951">
4.2.1 Lexical Features
</subsubsectionHeader>
<bodyText confidence="0.9999088125">
Previous research has studied lexical differences
(i.e., occurrence counts of N-grams) between var-
ious aspects of speech, such as topics (Hsueh and
Moore, 2006), speaker gender (Boulis and Osten-
dorf, 2005), and story-telling conversation (Gordon
and Ganesan, 2005). As we expect that lexical dif-
ferences also exist in DM conversations, we gener-
ated language models from the DM Dialogue Acts in
the corpus. The comparison of the language models
generated from the DM dialogue Acts and the rest of
the conversations shows that some differences exist
between the two models: (1) decision making con-
versations are more likely to contain we than I and
You; (2) in decision-making conversations there are
more explicit mentions of topical words, such as ad-
vanced chips and functional design; (3) in decision-
</bodyText>
<page confidence="0.983927">
27
</page>
<table confidence="0.9994165">
Type Feature
Duration Number of words spoken in current, previous and next subdialogue
Duration (in seconds) of current, previous and next subdialogue
Pause Amount of silence (in seconds) preceding a subdialogue
Amount of silence (in seconds) following a subdialogue
Speech rate Number of words spoken per second in current, previous and next subdialogue
Number of syllables per second in current, previous and next subdialogue
Energy Overall energy level
Average energy level in the first, second, third, and fourth quarter of a subdialogue
Pitch Maximum and minimum F0, overall slope and variance
Slope and variance at the first 100 and 200 ms and last 100 and 200 ms,
at the first and second half, and at each quarter of the subdialogue
</table>
<tableCaption confidence="0.999221">
Table 1: Prosodic features used in this study.
</tableCaption>
<bodyText confidence="0.99974575">
making conversations, there are fewer negative ex-
pressions, such as I don’t think and I don’t know.
In an exploratory study using unigrams, as well as
bigrams and trigrams, we found that using bigrams
and trigrams does not improve the accuracy of clas-
sifying DM DAs, and therefore we include only uni-
grams in the set of lexical features in the experiments
reported in Section 6.
</bodyText>
<subsubsectionHeader confidence="0.903906">
4.2.2 Prosodic Features
</subsubsectionHeader>
<bodyText confidence="0.99990025">
Functionally, prosodic features, i.e., energy, and
fundamental frequency (F0), are indicative of seg-
mentation and saliency. In this study, we follow
Shriberg and Stolcke’s (2001) direct modelling ap-
proach to manifest prosodic features as duration,
pause, speech rate, pitch contour, and energy level.
We utilize the individual sound files provided in the
AMI corpus. To extract prosodic features from the
sound files, we use the Snack Sound Toolkit to com-
pute a list of pitch and energy values delimited by
frames of 10 ms, using the normalized cross correla-
tion function. Then we apply a piecewise linearisa-
tion procedure to remove the outliers and average the
linearised values of the units within the time frame
of a word. Pitch contour of a dialogue act is ap-
proximated by measuring the pitch slope at multi-
ple points within the dialogue act, e.g., the first and
last 100 and 200 ms. The rate of speech is calcu-
lated as both the number of words spoken per sec-
ond and the number of syllables per second. We
use Festival’s speech synthesis front-end to return
phonemes and syllabification information. An ex-
ploratory study has shown the benefits of including
immediate prosodic contexts, and thus we also in-
clude prosodic features of the immediately preced-
ing and following dialogue acts. Table 1 contains
a list of automatically generated prosodic features
used in this study.
</bodyText>
<subsectionHeader confidence="0.512144">
4.2.3 Contextual Features
</subsectionHeader>
<bodyText confidence="0.999949105263158">
From our qualitative analysis, we expect that con-
textual features specific to the AMI corpus, such as
the speaker role (i.e., PM, ME, ID, UID) and meet-
ing type (i.e., kick-off, conceptual design, functional
design, detailed design) to be characteristic of the
DM DAs. Analysis shows that (1) participants as-
signed to the role of PM produce 42.5% of the DM
DAs, and (2) participants make relatively fewer de-
cisions in the kick-off meetings. Analysis has also
demonstrated a difference in the type, the reflexiv-
ity1 and the number of addressees, between the DM
DAs and the non-DM DAs. For example, dialogue
acts of type inform, suggest, elicit assessment and
elicit inform are more likely to be DM DAs.
We have also found that immediately preceding
and following dialogue acts are important for iden-
tifying DM DAs. For example, stalls and frag-
ments preceding and fragments following a DM
DA are more likely than for non-DM DAs.2 In
</bodyText>
<footnote confidence="0.84617575">
1According to the annotation guideline, the reflexivity re-
flects on how the group is carrying on the task. In this case, the
interlocutors pause to evaluate the group performance less often
when it comes to decision making.
2STALL is where people start talking before they are ready,
or keep speaking when they haven’t figured out what to say;
FRAGMENT is the segment which is not really speech or is
unclear enough to be transcribed, or where the speaker did not
</footnote>
<page confidence="0.997839">
28
</page>
<bodyText confidence="0.99968175">
contrast, there is a lower chance of seeing sug-
gest and elicit-type DAs (i.e., elicit-inform, elicit-
suggestion, elicit-assessment) in the preceding and
following DM DAs.
</bodyText>
<subsubsectionHeader confidence="0.696713">
4.2.4 Topical Features
</subsubsectionHeader>
<bodyText confidence="0.999971083333334">
As reported in Section 4.1.2, we find that inter-
locutors are more likely to reach decisions when cer-
tain topics are brought up. Also, we expect decision-
making conversations to take place towards the end
of a topic segment. Therefore, in this study we in-
clude the following features: the label of the current
topic segment, the position of the DA in a topic seg-
ment (measured in words, in seconds, and in %), the
distance to the previous topic shift (both at the top-
level and sub-topic level)(measured in seconds), the
duration of the current topic segment (both at the
top-level and sub-topic level)(measured in seconds).
</bodyText>
<sectionHeader confidence="0.999756" genericHeader="method">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998828">
5.1 Classifying DM DAs
</subsectionHeader>
<bodyText confidence="0.995430037037037">
Detecting DM DAs is the first step of automatic de-
cision detection. For this purpose, we trained Max-
Ent models to classify each unseen sample as ei-
ther DM DA (POS) or non-DM DA (NEG). We per-
formed a 5-fold cross validation on the set of 50
meetings. In each fold, we trained MaxEnt mod-
els from the feature combinations in the training
set, wherein each of the extracted dialogue acts has
been labelled as either POS or NEG. Then, the
models were used to classify unseen instances in
the test set as either POS or NEG. In Section 4.2,
we described the four major types of features used
in this study: unigrams (LX1), prosodic (PROS),
contextual (CONT), and topical (TOPIC) features.
For comparison, we report the naive baseline ob-
tained by training the models on the prosodic fea-
tures alone, since the prosodic features can be gen-
erated fully automatically. The different combina-
tions of features we used for training models can
be divided into the following four groups: (A) us-
ing prosodic features alone (BASELINE), (B) us-
ing lexical, contextual and topical features alone
(LX1, CONT, TOPIC); (C) using all available fea-
tures except one of the four types of features (ALL-
LX1, ALL-PROS, ALL-CONT, ALL-TOPIC); and
get far enough to express the intention.
(D) using all available features (ALL).
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="method">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.999778">
6.1 Classifying DM Segments
</subsectionHeader>
<bodyText confidence="0.9997552">
Detecting DM segments is necessary for interpret-
ing decisions, as it provides information about the
current topic of discussion. Here we combine the
predictions of the DM DAs to classify each unseen
topic segment in the test set as either DM Segment
(POS) or non-DM Segment (NEG). Recall that we
defined a DM Segment as a segment that contains
one or more hypothesized DM DAs. The task of de-
tecting DM Segments can thus be viewed as that of
detecting DM Dialogue Acts in a wider window.
</bodyText>
<subsectionHeader confidence="0.971448">
6.2 EXP1: Classifying DM DAs
</subsectionHeader>
<bodyText confidence="0.99977235483871">
Table 2 reports the performance on the test set. The
results show that models trained with all features
(ALL), including lexical, prosodic, contextual and
topical features, yield substantially better perfor-
mance than the baseline on the task of detecting DM
DAs. We carried out a one-way ANOVA to exam-
ine the effect of different feature combinations on
overall accuracy (F1). The ANOVA suggests a reli-
able effect of feature type (F(9,286) = 3.44;p &lt;
0.001). Rows 2-4 in Table 2 report the performance
of models in Group B that are trained with a sin-
gle type of feature. Lexical features are the most
predictive features when used alone. We performed
sign tests to determine whether there are statistical
differences among these models and the baseline.
We find that when used alone, only lexical features
(LX1) can train a better model than the baseline
(p &lt; 0.001). However, none of these models yields
a comparable performance to the ALL model.
To study the relative effect of the different fea-
ture types, Rows 5-8 in the table report the perfor-
mance of models in Group C, which are trained with
all available features except LX1, PROS, CONT and
TOPIC features respectively. The amount of degra-
dation in the overall accuracy (F1) of each of the
models in relation to that of the ALL model indi-
cates the contribution of the feature type that has
been left out of the model. We performed sign tests
to examine the differences among these models and
the ALL model. We find that the ALL model out-
performs all of these models (p &lt; 0.001) except
</bodyText>
<page confidence="0.997037">
29
</page>
<table confidence="0.999947090909091">
Exact Match Lenient Match
Accuracy Precision Recall F1 Precision Recall F1
BASELINE(PROS) 0.32 0.06 0.1 0.32 0.1 0.15
LX1 0.53 0.3 0.38 0.6 0.43 0.5
CONT 0 0 0 0 0 0
TOPIC 0.49 0.11 0.17 0.57 0.11 0.17
ALL-PROS 0.63 0.47 0.54 0.71 0.57 0.63
ALL-LX1 0.61 0.34 0.44 0.65 0.43 0.52
ALL-CONT 0.66 0.62 0.64 0.69 0.68 0.69
ALL-TOPIC 0.72 0.54 0.62 0.7 0.52 0.59
ALL 0.72 0.54 0.62 0.76 0.64 0.7
</table>
<tableCaption confidence="0.999285">
Table 2: Effects of different combinations offeatures on detecting DMDAs.
</tableCaption>
<bodyText confidence="0.9999193125">
the model trained by leaving out contextual features
(ALL-CONT). A closer investigation of the preci-
sion and recall of the ALL-CONT model shows that
the contextual features are detrimental to recall but
beneficial for precision. The mixed result is due to
the fact that models trained with contextual features
are tailored to recognize particular types of DM di-
alogue acts. Therefore, using these contextual fea-
tures improves the precision for these types of DM
DAs but reduces the overall recognition accuracy.
The last three columns of Table 2 are the results
obtained using a lenient match measure, allowing a
window of 10 seconds preceding and following a hy-
pothesized DM DA for recognition. The better re-
sults show that there is room for ambiguity in the
assessment of the exact timing of DM DAs.
</bodyText>
<subsectionHeader confidence="0.985603">
6.3 EXP2: Classifying DM Segments
</subsectionHeader>
<bodyText confidence="0.999994230769231">
As expected, the results in Table 3 are better than
those reported in Table 2, achieving at best 83%
overall accuracy.The model that combines all fea-
tures (ALL) yields significantly better results than
the baseline. The ANOVA shows a reliable effect of
different feature types on the task of detecting DM
Segments (F(11,284) = 2.33;p &lt;= 0.01). Rows
2-4 suggest that lexical features are the most pre-
dictive in terms of overall accuracy. Sign tests con-
firm the advantage of using lexical features (LX1)
over the baseline (PROS) (p &lt; 0.05). Interest-
ingly, the model that is trained with topical features
alone (TOPIC) yields substantially better precision
(p &lt; 0.001). The increase from 49% precision for
the task of detecting DM DAs (in Table 2) to 91%
for that of detecting DM Segments stems from the
fact that decisions are more likely to occur in certain
types of topic segments. In turn, training models
with topical features helps eliminate incorrect pre-
dictions of DM DAs in these types of topic seg-
ments. However, the accuracy gain of the TOPIC
model on detecting certain types of DM Segments
does not extend to all types of DM Segments. This is
shown by the significantly lower recall of the TOPIC
model over the baseline (p &lt; 0.001).
Finally, Rows 5-8 report the performance of the
models in Group (C) on the task of detecting DM
Segments. Sign tests again show that the model that
is trained with all available features (ALL) outper-
forms the models that leave out lexical, prosodic,
or topical features (p &lt; 0.05). However, the ALL
model does not outperform the model that leaves out
contextual features. In addition, the contextual fea-
tures degrade the recall but improve the precision
on the task of detecting DM Segments. Calculat-
ing how much the overall accuracy of the models in
Group C degrades from the ALL model shows that
the most predictive features are the lexical features,
followed by the topical and prosodic features.
</bodyText>
<sectionHeader confidence="0.998049" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999833857142857">
As suggested by the mixed results obtained by the
model that is trained without the contextual features,
the two-phase decision annotation procedure (as de-
scribed in Section 4.1) may have caused annota-
tors to select dialogue acts that serve different func-
tional roles in a decision-making process in the set
of DM DAs. For example, in the dialogue shown
</bodyText>
<page confidence="0.992152">
30
</page>
<table confidence="0.999884">
Exact Match
Accuracy Precision Recall F1
BASELINE(PROS) 0.67 0.39 0.49
LX1 0.69 0.69 0.69
CONT 0 0 0
TOPIC 0.91 0.17 0.29
ALL-PROS 0.82 0.76 0.79
ALL-LX1 0.79 0.64 0.7
ALL-CONT 0.79 0.86 0.83
ALL-TOPIC 0.75 0.73 0.74
ALL 0.86 0.8 0.82
</table>
<tableCaption confidence="0.777782">
Table 3: Effects of different combinations offeatures
on detecting DM Segments.
</tableCaption>
<listItem confidence="0.976580888888889">
(1) A: but um the feature that we considered for it
not getting lost.
(2) B: Right. Well
(3) B: were talking about that a little bit
(4) B: when we got that email
(5) B: and we think that each of these are so
distinctive, that it it’s not just like another piece of
technology around your house.
(6) B: It’s gonna be somewhere that it can be seen.
(7) A: Mm-hmm.
(8) B: So we’re we’re not thinking that it’s gonna
be as critical to have the loss
(9) D: But if it’s like under covers or like in a couch
you still can’t see it.
. . .
(10) A: Okay, that’s a fair evaluation.
(11) A: Um we so we do we’ve decided not to
worry about that for now.
</listItem>
<bodyText confidence="0.999940411764706">
in Figure 1, the annotators have marked dialogue
act (1), (5), (8), and (11) as the DM DAs related
to this decision: “There will be no feature to help
find the remote when it is misplaced”. Among the
four DM DAs, (1) describes the topic of what this
decision is about; (5) and (8) describe the arguments
that support the decision-making process; (11) in-
dicates the level of agreement or disagreement for
this decision. Yet these DM DAs which play dif-
ferent functional roles in the DM process may each
have their own characteristic features. Training one
model to recognize DM DAs of all functional roles
may have degraded the performance on the classifi-
cation tasks. Developing models for detecting DM
DAs that play different functional roles requires a
larger scale study to discover the anatomy of gen-
eral decision-making discussions.
</bodyText>
<sectionHeader confidence="0.996216" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999849769230769">
This is the first study that aimed to detect segments
of the conversation that contain decisions. We have
(1) empirically analyzed the characteristic features
of DM dialogue acts, and (2) computational devel-
oped models to detect DM dialogue acts and DM
topic segments, given the set of characteristic fea-
tures. Empirical analysis has provided a qualitative
account of the DM-characteristic features, whereas
training the computational models on different fea-
ture combinations has provided a quantitative ac-
count of the effect of different feature types on
the task of automatic decision detection. Empiri-
cal analysis has exhibited demonstrable differences
</bodyText>
<figureCaption confidence="0.999475">
Figure 1: Example decision-making discussion
</figureCaption>
<bodyText confidence="0.9999831">
in the words (e.g., we), the contextual features (e.g.,
meeting type, speaker role, dialogue act type), and
the topical features. The experimental results have
suggested that (1) the model combining all the avail-
able features performs substantially better, achiev-
ing 62% and 82% overall accuracy on the task of
detecting DM DAs and that of detecting DM Seg-
ments, respectively, (2) lexical features are the best
indicators for both the task of detecting DM DAs and
that of detecting DM Segments, and (3) combining
topical features is important for improving the pre-
cision for the task of detecting DM Segments.
Many of the features used in this study require hu-
man intervention, such as manual transcriptions, an-
notated dialogue act segmentations and labels, anno-
tated topic segmentations and labels, and other types
of meeting-specific features. Our ultimate goal is to
identify decisions using automatically induced fea-
tures. Therefore, studying the performance degra-
dation when using the automatically generated ver-
sions of these features (e.g., ASR words) is essen-
tial for developing a fully automated component on
detecting decisions immediately after a meeting or
even for when a meeting is still in progress. An-
other problem that has been pointed out in Section 6
and in Section 7 is the different functional roles of
DM dialogue acts in current annotations. Purver et
al. (2006) have suggested a hierarchical annotation
scheme to accommodate the different aspects of ac-
tion items. The same technique may be applicable
</bodyText>
<page confidence="0.999582">
31
</page>
<bodyText confidence="0.972328">
in a more general decision detection task.
</bodyText>
<sectionHeader confidence="0.988679" genericHeader="acknowledgments">
9 Acknowledgement
</sectionHeader>
<bodyText confidence="0.997604666666667">
This work was supported by the European Union In-
tegrated Project AMI (Augmented Multi-party Inter-
action, FP6-506811, publication AMI-204).
</bodyText>
<sectionHeader confidence="0.998432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999792896551724">
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback system,
and the benefit of topic-level annotations to meeting
browsing. In Proceedings of the Tenth International
Conference on Human-Computer Interaction.
C. Boulis and M. Ostendorf. 2005. A quantitative anal-
ysis of lexical differences between genders in tele-
phone conversation. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics. ACM Press.
CALO. 2003. http://www.ai.sri.com/project/calo.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Wellner.
2005. The ami meeting corpus: A pre-announcement.
In Proceedings of 2nd Joint Workshop on Multi-
modal Interaction and Related Machine Learning Al-
gorithms.
C. Cieri, D. Miller, and K. Walker. 2002. Research
methodologies, observations and outcomes in conver-
sational speech data collection. In Proceedings of the
Human Language Technologies Conference (HLT).
M. Galley, J. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use ofbayesian networks to model
pragmatic dependencies. In Proceedings of the 42nd
Annual Meeting of the ACL.
J. S. Garofolo, C. D. Laprun, M. Michel, V.M. Stanford,
and E. Tabassi. 2004. The nist meeting room pilot
corpus. In Proceedings ofLREC04.
D. Gatica-Perez, I. McCowan, D. Zhang, and S. Bengio.
2005. Detecting group interest level in meetings. In
IEEE Int. Conf. on Acoustics, Speech, and Signal Pro-
cessing (ICASSP).
Andrew S. Gordon and Kavita Ganesan. 2005. Auto-
mated story extraction from conversational speech. In
Proceedings of the Third International Conference on
Knowledge Capture (K-CAP 05).
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. Detec-
tion of agreement vs. disagreement in meetings: Train-
ing with unlabeled data. In Proc. HLT-NAACL.
P. Hsueh and J. Moore. 2006. Automatic topic segmen-
tation and lablelling in multiparty dialogue. In the first
IEEE/ACM workshop on Spoken Language Technol-
ogy (SLT). IEEE/ACM.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The icsi meeting corpus.
In Proceedings ofICASSP-2003, Hong Kong.
W. Kunz and H. W. J. Ritte. 1970. Issue as elements
of information system. Technical Report Working Pa-
per 131, Institute of Urban and Regional Development
Research, University of California, Berkeley.
S. Marchand-Mailet. 2003. Meeting record modeling for
enhanced browsing. Technical report, Computer Vi-
sion and Multimedia Lab, Computer Centre, Univer-
sity of Geneva, Switzerland.
J. Niekrasz, M. Purver, J. Dowding, and S. Peters. 2005.
Ontology-based discourse understanding for a persis-
tent meeting assistant. In Proc. of the AAAI Spring
Symposium.
V. Pallotta, J. Niekrasz, and M. Purver. 2005. Collab-
orative and argumentative models of meeting discus-
sions. In Proceeding of CMNA-05 workshop on Com-
putational Models ofNatural Arguments in IJCAI 05.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Shallow
discourse structure for action item detection. In the
Workshop ofHLT-NAACL: Analyzing Conversations in
Text and Speech. ACM Press.
R. J. Rienks, D. Heylen, and E. van der Weijden. 2005.
Argument diagramming of meeting conversations. In
Multimodal Multiparty Meeting Processing Workshop
at the ICMI.
E. Shriberg and A. Stolcke. 2001. Direct modeling of
prosody: An overview of applications in automatic
speech processing.
A. Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf amd
T. Schultz, H. Soltau, H. Yu, and K. Zechner. 2001.
Advances in automatic meeting record creation and ac-
cess. In Proceedings ofICASSP.
S. Whittaker, R. Laban, and S. Tucker. 2005. Analysing
meeting records: An ethnographic study and techno-
logical implications. In Proceedings ofMLMI2005.
B. Wrede and E. Shriberg. 2003. Spotting hot spots in
meetings: Human judgements and prosodic cues. In
Proceedings ofEUROSPEECH2003.
</reference>
<page confidence="0.999299">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.203135">
<title confidence="0.991245">What Decisions Have You Made: Automatic Decision Detection Conversational Speech</title>
<author confidence="0.926855">Pei-Yun</author>
<affiliation confidence="0.9955705">School of University of</affiliation>
<address confidence="0.501754">Edinburgh EH9 8WL,</address>
<email confidence="0.987896">p.hsueh@ed.ac.uk</email>
<author confidence="0.874517">Johanna</author>
<affiliation confidence="0.9968445">School of University of</affiliation>
<address confidence="0.510095">Edinburgh EH9 8WL,</address>
<email confidence="0.996825">J.Moore@ed.ac.uk</email>
<abstract confidence="0.999798722222222">This study addresses the problem of automatically detecting decisions in conversational speech. We formulate the problem as classifying decision-making units at two levels of granularity: dialogue acts and topic segments. We conduct an empirical analysis to determine the characteristic features of decision-making dialogue acts, and train MaxEnt models using these features for the classification tasks. We find that models that combine lexical, prosodic, contextual and topical features yield the best results on both tasks, achieving 72% and 86% precision, respectively. The study also provides a quantitative analysis of the relative importance of the feature types.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>C Rose</author>
<author>A I Rudnicky</author>
</authors>
<title>The necessity of a meeting recording and playback system, and the benefit of topic-level annotations to meeting browsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth International Conference on Human-Computer Interaction.</booktitle>
<marker>Banerjee, Rose, Rudnicky, 2005</marker>
<rawString>S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The necessity of a meeting recording and playback system, and the benefit of topic-level annotations to meeting browsing. In Proceedings of the Tenth International Conference on Human-Computer Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Boulis</author>
<author>M Ostendorf</author>
</authors>
<title>A quantitative analysis of lexical differences between genders in telephone conversation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="11631" citStr="Boulis and Ostendorf, 2005" startWordPosition="1829" endWordPosition="1833">of the Trend-Watching segments are DM Segments. Functional segments, such as Chitchat, Opening and Closing, almost never include decisions. 4.2 Features Used To provide a qualitative account of the effect of different feature types on the task of automatic decision detection, we have conducted empirical analysis on four major types of features: lexical, prosodic, contextual and topical features. 4.2.1 Lexical Features Previous research has studied lexical differences (i.e., occurrence counts of N-grams) between various aspects of speech, such as topics (Hsueh and Moore, 2006), speaker gender (Boulis and Ostendorf, 2005), and story-telling conversation (Gordon and Ganesan, 2005). As we expect that lexical differences also exist in DM conversations, we generated language models from the DM Dialogue Acts in the corpus. The comparison of the language models generated from the DM dialogue Acts and the rest of the conversations shows that some differences exist between the two models: (1) decision making conversations are more likely to contain we than I and You; (2) in decision-making conversations there are more explicit mentions of topical words, such as advanced chips and functional design; (3) in decision27 T</context>
</contexts>
<marker>Boulis, Ostendorf, 2005</marker>
<rawString>C. Boulis and M. Ostendorf. 2005. A quantitative analysis of lexical differences between genders in telephone conversation. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CALO</author>
</authors>
<date>2003</date>
<note>http://www.ai.sri.com/project/calo.</note>
<contexts>
<context position="3433" citStr="CALO, 2003" startWordPosition="518" endWordPosition="519"> have been built at several institutes to record large corpora of meetings in natural contexts, including CMU (Waibel et al., 2001), LDC (Cieri et al., 2002), NIST (Garofolo et al., 2004), ICSI (Janin et al., 2003), and in the context of the IM2/M4 project (Marchand-Mailet, 2003). More recently, scenario-based meetings, in which particProceedings of NAACL HLT 2007, pages 25–32, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to develop descriptive models of meeting discussions. Some researchers are modelling the dynamics of the meeting, exploiting dialogue models previously proposed for dialogue management. For example, Niekrasz et al. (2005) use the Issue-Based Information System (IBIS) model (Kunz and Ritte, 1970) to incorporate the history of dialogue moves into the Multi-Modal Discourse (MMD) ontology. Other researchers are modelling the content of the meeting using the type of structures proposed</context>
</contexts>
<marker>CALO, 2003</marker>
<rawString>CALO. 2003. http://www.ai.sri.com/project/calo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>W Kraaij</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>I McCowan</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The ami meeting corpus: A pre-announcement.</title>
<date>2005</date>
<booktitle>In Proceedings of 2nd Joint Workshop on Multimodal Interaction and Related Machine Learning Algorithms.</booktitle>
<contexts>
<context position="3477" citStr="Carletta et al., 2005" startWordPosition="524" endWordPosition="527">tutes to record large corpora of meetings in natural contexts, including CMU (Waibel et al., 2001), LDC (Cieri et al., 2002), NIST (Garofolo et al., 2004), ICSI (Janin et al., 2003), and in the context of the IM2/M4 project (Marchand-Mailet, 2003). More recently, scenario-based meetings, in which particProceedings of NAACL HLT 2007, pages 25–32, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to develop descriptive models of meeting discussions. Some researchers are modelling the dynamics of the meeting, exploiting dialogue models previously proposed for dialogue management. For example, Niekrasz et al. (2005) use the Issue-Based Information System (IBIS) model (Kunz and Ritte, 1970) to incorporate the history of dialogue moves into the Multi-Modal Discourse (MMD) ontology. Other researchers are modelling the content of the meeting using the type of structures proposed in work on argumentation. For example, Rien</context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kraaij, Kronenthal, Lathoud, Lincoln, Lisowska, McCowan, Post, Reidsma, Wellner, 2005</marker>
<rawString>J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2005. The ami meeting corpus: A pre-announcement. In Proceedings of 2nd Joint Workshop on Multimodal Interaction and Related Machine Learning Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cieri</author>
<author>D Miller</author>
<author>K Walker</author>
</authors>
<title>Research methodologies, observations and outcomes in conversational speech data collection.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technologies Conference (HLT).</booktitle>
<contexts>
<context position="2979" citStr="Cieri et al., 2002" startWordPosition="446" endWordPosition="449">nd the development of group information management applications (e.g., constructing group memory). 2 Related Work Spontaneous face-to-face dialogues in meetings violate many assumptions made by techniques previously developed for broadcast news (e.g., TDT and TRECVID), telephone conversations (e.g., Switchboard), and human-computer dialogues (e.g., DARPA Communicator). In order to develop techniques for understanding multiparty dialogues, smart meeting rooms have been built at several institutes to record large corpora of meetings in natural contexts, including CMU (Waibel et al., 2001), LDC (Cieri et al., 2002), NIST (Garofolo et al., 2004), ICSI (Janin et al., 2003), and in the context of the IM2/M4 project (Marchand-Mailet, 2003). More recently, scenario-based meetings, in which particProceedings of NAACL HLT 2007, pages 25–32, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to develop descriptive models o</context>
</contexts>
<marker>Cieri, Miller, Walker, 2002</marker>
<rawString>C. Cieri, D. Miller, and K. Walker. 2002. Research methodologies, observations and outcomes in conversational speech data collection. In Proceedings of the Human Language Technologies Conference (HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use ofbayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="5094" citStr="Galley et al., 2004" startWordPosition="768" endWordPosition="771">omatically extracting these argument models is a challenging task. However, researchers have begun to make progress towards this goal. For example, Gatica et al. (2005) and Wrede and Shriberg (2003) automatically identify the level of emotion in meeting spurts (e.g., group level of interest, hot spots). Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al., 2003) and structural information (e.g., the previous and following speaker) (Galley et al., 2004). More recently, Purver et al. (2006) have tackled the problem of detecting one type of decision, namely action items, which embody the transfer of group responsibility. However, no prior work has addressed the problem of automatically identifying decision-making units more generally in multiparty meetings. Moreover, no previous research has provided a quantitative account of the effects of different feature types on the task of automatic decision detection. 3 Research Goal Our aim is to develop models for automatically detecting segments of conversation that contain decisions directly from th</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>M. Galley, J. McKeown, J. Hirschberg, and E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use ofbayesian networks to model pragmatic dependencies. In Proceedings of the 42nd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Garofolo</author>
<author>C D Laprun</author>
<author>M Michel</author>
<author>V M Stanford</author>
<author>E Tabassi</author>
</authors>
<title>The nist meeting room pilot corpus.</title>
<date>2004</date>
<booktitle>In Proceedings ofLREC04.</booktitle>
<contexts>
<context position="3009" citStr="Garofolo et al., 2004" startWordPosition="451" endWordPosition="454"> information management applications (e.g., constructing group memory). 2 Related Work Spontaneous face-to-face dialogues in meetings violate many assumptions made by techniques previously developed for broadcast news (e.g., TDT and TRECVID), telephone conversations (e.g., Switchboard), and human-computer dialogues (e.g., DARPA Communicator). In order to develop techniques for understanding multiparty dialogues, smart meeting rooms have been built at several institutes to record large corpora of meetings in natural contexts, including CMU (Waibel et al., 2001), LDC (Cieri et al., 2002), NIST (Garofolo et al., 2004), ICSI (Janin et al., 2003), and in the context of the IM2/M4 project (Marchand-Mailet, 2003). More recently, scenario-based meetings, in which particProceedings of NAACL HLT 2007, pages 25–32, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to develop descriptive models of meeting discussions. Some re</context>
</contexts>
<marker>Garofolo, Laprun, Michel, Stanford, Tabassi, 2004</marker>
<rawString>J. S. Garofolo, C. D. Laprun, M. Michel, V.M. Stanford, and E. Tabassi. 2004. The nist meeting room pilot corpus. In Proceedings ofLREC04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gatica-Perez</author>
<author>I McCowan</author>
<author>D Zhang</author>
<author>S Bengio</author>
</authors>
<title>Detecting group interest level in meetings.</title>
<date>2005</date>
<booktitle>In IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<marker>Gatica-Perez, McCowan, Zhang, Bengio, 2005</marker>
<rawString>D. Gatica-Perez, I. McCowan, D. Zhang, and S. Bengio. 2005. Detecting group interest level in meetings. In IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew S Gordon</author>
<author>Kavita Ganesan</author>
</authors>
<title>Automated story extraction from conversational speech.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Conference on Knowledge Capture (K-CAP 05).</booktitle>
<contexts>
<context position="11690" citStr="Gordon and Ganesan, 2005" startWordPosition="1837" endWordPosition="1840">gments, such as Chitchat, Opening and Closing, almost never include decisions. 4.2 Features Used To provide a qualitative account of the effect of different feature types on the task of automatic decision detection, we have conducted empirical analysis on four major types of features: lexical, prosodic, contextual and topical features. 4.2.1 Lexical Features Previous research has studied lexical differences (i.e., occurrence counts of N-grams) between various aspects of speech, such as topics (Hsueh and Moore, 2006), speaker gender (Boulis and Ostendorf, 2005), and story-telling conversation (Gordon and Ganesan, 2005). As we expect that lexical differences also exist in DM conversations, we generated language models from the DM Dialogue Acts in the corpus. The comparison of the language models generated from the DM dialogue Acts and the rest of the conversations shows that some differences exist between the two models: (1) decision making conversations are more likely to contain we than I and You; (2) in decision-making conversations there are more explicit mentions of topical words, such as advanced chips and functional design; (3) in decision27 Type Feature Duration Number of words spoken in current, pre</context>
</contexts>
<marker>Gordon, Ganesan, 2005</marker>
<rawString>Andrew S. Gordon and Kavita Ganesan. 2005. Automated story extraction from conversational speech. In Proceedings of the Third International Conference on Knowledge Capture (K-CAP 05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>E Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: Training with unlabeled data. In</title>
<date>2003</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="5002" citStr="Hillard et al., 2003" startWordPosition="754" endWordPosition="757">ganization and synchronization. Decisions are often seen as a by-product of these models. Automatically extracting these argument models is a challenging task. However, researchers have begun to make progress towards this goal. For example, Gatica et al. (2005) and Wrede and Shriberg (2003) automatically identify the level of emotion in meeting spurts (e.g., group level of interest, hot spots). Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al., 2003) and structural information (e.g., the previous and following speaker) (Galley et al., 2004). More recently, Purver et al. (2006) have tackled the problem of detecting one type of decision, namely action items, which embody the transfer of group responsibility. However, no prior work has addressed the problem of automatically identifying decision-making units more generally in multiparty meetings. Moreover, no previous research has provided a quantitative account of the effects of different feature types on the task of automatic decision detection. 3 Research Goal Our aim is to develop models </context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>D. Hillard, M. Ostendorf, and E. Shriberg. 2003. Detection of agreement vs. disagreement in meetings: Training with unlabeled data. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hsueh</author>
<author>J Moore</author>
</authors>
<title>Automatic topic segmentation and lablelling in multiparty dialogue.</title>
<date>2006</date>
<booktitle>In the first IEEE/ACM workshop on Spoken Language Technology (SLT). IEEE/ACM.</booktitle>
<contexts>
<context position="11586" citStr="Hsueh and Moore, 2006" startWordPosition="1823" endWordPosition="1826"> the Existing Product segments and none of the Trend-Watching segments are DM Segments. Functional segments, such as Chitchat, Opening and Closing, almost never include decisions. 4.2 Features Used To provide a qualitative account of the effect of different feature types on the task of automatic decision detection, we have conducted empirical analysis on four major types of features: lexical, prosodic, contextual and topical features. 4.2.1 Lexical Features Previous research has studied lexical differences (i.e., occurrence counts of N-grams) between various aspects of speech, such as topics (Hsueh and Moore, 2006), speaker gender (Boulis and Ostendorf, 2005), and story-telling conversation (Gordon and Ganesan, 2005). As we expect that lexical differences also exist in DM conversations, we generated language models from the DM Dialogue Acts in the corpus. The comparison of the language models generated from the DM dialogue Acts and the rest of the conversations shows that some differences exist between the two models: (1) decision making conversations are more likely to contain we than I and You; (2) in decision-making conversations there are more explicit mentions of topical words, such as advanced chi</context>
</contexts>
<marker>Hsueh, Moore, 2006</marker>
<rawString>P. Hsueh and J. Moore. 2006. Automatic topic segmentation and lablelling in multiparty dialogue. In the first IEEE/ACM workshop on Spoken Language Technology (SLT). IEEE/ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The icsi meeting corpus.</title>
<date>2003</date>
<booktitle>In Proceedings ofICASSP-2003, Hong Kong.</booktitle>
<contexts>
<context position="3036" citStr="Janin et al., 2003" startWordPosition="456" endWordPosition="459">ations (e.g., constructing group memory). 2 Related Work Spontaneous face-to-face dialogues in meetings violate many assumptions made by techniques previously developed for broadcast news (e.g., TDT and TRECVID), telephone conversations (e.g., Switchboard), and human-computer dialogues (e.g., DARPA Communicator). In order to develop techniques for understanding multiparty dialogues, smart meeting rooms have been built at several institutes to record large corpora of meetings in natural contexts, including CMU (Waibel et al., 2001), LDC (Cieri et al., 2002), NIST (Garofolo et al., 2004), ICSI (Janin et al., 2003), and in the context of the IM2/M4 project (Marchand-Mailet, 2003). More recently, scenario-based meetings, in which particProceedings of NAACL HLT 2007, pages 25–32, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to develop descriptive models of meeting discussions. Some researchers are modelling the</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The icsi meeting corpus. In Proceedings ofICASSP-2003, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kunz</author>
<author>H W J Ritte</author>
</authors>
<title>Issue as elements of information system.</title>
<date>1970</date>
<tech>Technical Report Working Paper 131,</tech>
<institution>Institute of Urban and Regional Development Research, University of California, Berkeley.</institution>
<contexts>
<context position="3844" citStr="Kunz and Ritte, 1970" startWordPosition="578" endWordPosition="581"> 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to develop descriptive models of meeting discussions. Some researchers are modelling the dynamics of the meeting, exploiting dialogue models previously proposed for dialogue management. For example, Niekrasz et al. (2005) use the Issue-Based Information System (IBIS) model (Kunz and Ritte, 1970) to incorporate the history of dialogue moves into the Multi-Modal Discourse (MMD) ontology. Other researchers are modelling the content of the meeting using the type of structures proposed in work on argumentation. For example, Rienks et al. (2005) have developed an argument diagramming scheme to visualize the relations (e.g., positive, negative, uncertain) between utterances (e.g., statement, open issue), and Marchand et al. (2003) propose a schema to model different argumentation acts (e.g., accept, request, reject) and their organization and synchronization. Decisions are often seen as a b</context>
</contexts>
<marker>Kunz, Ritte, 1970</marker>
<rawString>W. Kunz and H. W. J. Ritte. 1970. Issue as elements of information system. Technical Report Working Paper 131, Institute of Urban and Regional Development Research, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Marchand-Mailet</author>
</authors>
<title>Meeting record modeling for enhanced browsing.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Computer Vision and Multimedia Lab, Computer Centre, University of Geneva, Switzerland.</institution>
<contexts>
<context position="3102" citStr="Marchand-Mailet, 2003" startWordPosition="468" endWordPosition="469">neous face-to-face dialogues in meetings violate many assumptions made by techniques previously developed for broadcast news (e.g., TDT and TRECVID), telephone conversations (e.g., Switchboard), and human-computer dialogues (e.g., DARPA Communicator). In order to develop techniques for understanding multiparty dialogues, smart meeting rooms have been built at several institutes to record large corpora of meetings in natural contexts, including CMU (Waibel et al., 2001), LDC (Cieri et al., 2002), NIST (Garofolo et al., 2004), ICSI (Janin et al., 2003), and in the context of the IM2/M4 project (Marchand-Mailet, 2003). More recently, scenario-based meetings, in which particProceedings of NAACL HLT 2007, pages 25–32, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to develop descriptive models of meeting discussions. Some researchers are modelling the dynamics of the meeting, exploiting dialogue models previously pr</context>
</contexts>
<marker>Marchand-Mailet, 2003</marker>
<rawString>S. Marchand-Mailet. 2003. Meeting record modeling for enhanced browsing. Technical report, Computer Vision and Multimedia Lab, Computer Centre, University of Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Niekrasz</author>
<author>M Purver</author>
<author>J Dowding</author>
<author>S Peters</author>
</authors>
<title>Ontology-based discourse understanding for a persistent meeting assistant.</title>
<date>2005</date>
<booktitle>In Proc. of the</booktitle>
<publisher>AAAI Spring Symposium.</publisher>
<contexts>
<context position="3769" citStr="Niekrasz et al. (2005)" startWordPosition="567" endWordPosition="570">which particProceedings of NAACL HLT 2007, pages 25–32, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to develop descriptive models of meeting discussions. Some researchers are modelling the dynamics of the meeting, exploiting dialogue models previously proposed for dialogue management. For example, Niekrasz et al. (2005) use the Issue-Based Information System (IBIS) model (Kunz and Ritte, 1970) to incorporate the history of dialogue moves into the Multi-Modal Discourse (MMD) ontology. Other researchers are modelling the content of the meeting using the type of structures proposed in work on argumentation. For example, Rienks et al. (2005) have developed an argument diagramming scheme to visualize the relations (e.g., positive, negative, uncertain) between utterances (e.g., statement, open issue), and Marchand et al. (2003) propose a schema to model different argumentation acts (e.g., accept, request, reject) </context>
</contexts>
<marker>Niekrasz, Purver, Dowding, Peters, 2005</marker>
<rawString>J. Niekrasz, M. Purver, J. Dowding, and S. Peters. 2005. Ontology-based discourse understanding for a persistent meeting assistant. In Proc. of the AAAI Spring Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Pallotta</author>
<author>J Niekrasz</author>
<author>M Purver</author>
</authors>
<title>Collaborative and argumentative models of meeting discussions.</title>
<date>2005</date>
<booktitle>In Proceeding of CMNA-05 workshop on Computational Models ofNatural Arguments in IJCAI 05.</booktitle>
<contexts>
<context position="1151" citStr="Pallotta et al. (2005)" startWordPosition="169" endWordPosition="172">pirical analysis to determine the characteristic features of decision-making dialogue acts, and train MaxEnt models using these features for the classification tasks. We find that models that combine lexical, prosodic, contextual and topical features yield the best results on both tasks, achieving 72% and 86% precision, respectively. The study also provides a quantitative analysis of the relative importance of the feature types. 1 Introduction Making decisions is an important aspect of conversations in collaborative work. In the context of meetings, the proposed argumentative models, e.g., in Pallotta et al. (2005) and Rienks et al. (2005), have specified decisions as an essential outcome of meetings. Whittaker et al. (2005) have also described how reviewing decisions is critical to the re-use of meeting recordings. For example, a new engineer who just get assigned to a project will need to know what major decisions have been made in previous meetings. Unless all decisions are recorded in meeting minutes or annotated in the speech recordings, it 25 is difficult to locate the decision points by the browsing and playback utilities alone. Banerjee and Rudnicky (2005) have shown that it is easier for users </context>
</contexts>
<marker>Pallotta, Niekrasz, Purver, 2005</marker>
<rawString>V. Pallotta, J. Niekrasz, and M. Purver. 2005. Collaborative and argumentative models of meeting discussions. In Proceeding of CMNA-05 workshop on Computational Models ofNatural Arguments in IJCAI 05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>P Ehlen</author>
<author>J Niekrasz</author>
</authors>
<title>Shallow discourse structure for action item detection.</title>
<date>2006</date>
<booktitle>In the Workshop ofHLT-NAACL: Analyzing Conversations in Text and Speech.</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="5131" citStr="Purver et al. (2006)" startWordPosition="774" endWordPosition="777">models is a challenging task. However, researchers have begun to make progress towards this goal. For example, Gatica et al. (2005) and Wrede and Shriberg (2003) automatically identify the level of emotion in meeting spurts (e.g., group level of interest, hot spots). Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al., 2003) and structural information (e.g., the previous and following speaker) (Galley et al., 2004). More recently, Purver et al. (2006) have tackled the problem of detecting one type of decision, namely action items, which embody the transfer of group responsibility. However, no prior work has addressed the problem of automatically identifying decision-making units more generally in multiparty meetings. Moreover, no previous research has provided a quantitative account of the effects of different feature types on the task of automatic decision detection. 3 Research Goal Our aim is to develop models for automatically detecting segments of conversation that contain decisions directly from the audio recordings and transcripts of</context>
</contexts>
<marker>Purver, Ehlen, Niekrasz, 2006</marker>
<rawString>M. Purver, P. Ehlen, and J. Niekrasz. 2006. Shallow discourse structure for action item detection. In the Workshop ofHLT-NAACL: Analyzing Conversations in Text and Speech. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Rienks</author>
<author>D Heylen</author>
<author>E van der Weijden</author>
</authors>
<title>Argument diagramming of meeting conversations.</title>
<date>2005</date>
<booktitle>In Multimodal Multiparty Meeting Processing Workshop at the ICMI.</booktitle>
<marker>Rienks, Heylen, van der Weijden, 2005</marker>
<rawString>R. J. Rienks, D. Heylen, and E. van der Weijden. 2005. Argument diagramming of meeting conversations. In Multimodal Multiparty Meeting Processing Workshop at the ICMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Direct modeling of prosody: An overview of applications in automatic speech processing.</title>
<date>2001</date>
<marker>Shriberg, Stolcke, 2001</marker>
<rawString>E. Shriberg and A. Stolcke. 2001. Direct modeling of prosody: An overview of applications in automatic speech processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waibel</author>
<author>M Bett</author>
<author>F Metze</author>
<author>K Ries</author>
<author>T Schaaf amd T Schultz</author>
<author>H Soltau</author>
<author>H Yu</author>
<author>K Zechner</author>
</authors>
<title>Advances in automatic meeting record creation and access.</title>
<date>2001</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<contexts>
<context position="2953" citStr="Waibel et al., 2001" startWordPosition="441" endWordPosition="444">ons made in the meetings) and the development of group information management applications (e.g., constructing group memory). 2 Related Work Spontaneous face-to-face dialogues in meetings violate many assumptions made by techniques previously developed for broadcast news (e.g., TDT and TRECVID), telephone conversations (e.g., Switchboard), and human-computer dialogues (e.g., DARPA Communicator). In order to develop techniques for understanding multiparty dialogues, smart meeting rooms have been built at several institutes to record large corpora of meetings in natural contexts, including CMU (Waibel et al., 2001), LDC (Cieri et al., 2002), NIST (Garofolo et al., 2004), ICSI (Janin et al., 2003), and in the context of the IM2/M4 project (Marchand-Mailet, 2003). More recently, scenario-based meetings, in which particProceedings of NAACL HLT 2007, pages 25–32, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics ipants are assigned to different roles and given specific tasks, have been recorded in the context of the CALO project (the Y2 Scenario Data) (CALO, 2003) and the AMI project (Carletta et al., 2005). The availability of meeting corpora has enabled researchers to begin to de</context>
</contexts>
<marker>Waibel, Bett, Metze, Ries, Schultz, Soltau, Yu, Zechner, 2001</marker>
<rawString>A. Waibel, M. Bett, F. Metze, K. Ries, T. Schaaf amd T. Schultz, H. Soltau, H. Yu, and K. Zechner. 2001. Advances in automatic meeting record creation and access. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Whittaker</author>
<author>R Laban</author>
<author>S Tucker</author>
</authors>
<title>Analysing meeting records: An ethnographic study and technological implications.</title>
<date>2005</date>
<booktitle>In Proceedings ofMLMI2005.</booktitle>
<contexts>
<context position="1263" citStr="Whittaker et al. (2005)" startWordPosition="188" endWordPosition="191">dels using these features for the classification tasks. We find that models that combine lexical, prosodic, contextual and topical features yield the best results on both tasks, achieving 72% and 86% precision, respectively. The study also provides a quantitative analysis of the relative importance of the feature types. 1 Introduction Making decisions is an important aspect of conversations in collaborative work. In the context of meetings, the proposed argumentative models, e.g., in Pallotta et al. (2005) and Rienks et al. (2005), have specified decisions as an essential outcome of meetings. Whittaker et al. (2005) have also described how reviewing decisions is critical to the re-use of meeting recordings. For example, a new engineer who just get assigned to a project will need to know what major decisions have been made in previous meetings. Unless all decisions are recorded in meeting minutes or annotated in the speech recordings, it 25 is difficult to locate the decision points by the browsing and playback utilities alone. Banerjee and Rudnicky (2005) have shown that it is easier for users to retrieve the information they seek if the meeting record includes information about topic segmentation, speak</context>
</contexts>
<marker>Whittaker, Laban, Tucker, 2005</marker>
<rawString>S. Whittaker, R. Laban, and S. Tucker. 2005. Analysing meeting records: An ethnographic study and technological implications. In Proceedings ofMLMI2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wrede</author>
<author>E Shriberg</author>
</authors>
<title>Spotting hot spots in meetings: Human judgements and prosodic cues.</title>
<date>2003</date>
<booktitle>In Proceedings ofEUROSPEECH2003.</booktitle>
<contexts>
<context position="4672" citStr="Wrede and Shriberg (2003)" startWordPosition="705" endWordPosition="708">gumentation. For example, Rienks et al. (2005) have developed an argument diagramming scheme to visualize the relations (e.g., positive, negative, uncertain) between utterances (e.g., statement, open issue), and Marchand et al. (2003) propose a schema to model different argumentation acts (e.g., accept, request, reject) and their organization and synchronization. Decisions are often seen as a by-product of these models. Automatically extracting these argument models is a challenging task. However, researchers have begun to make progress towards this goal. For example, Gatica et al. (2005) and Wrede and Shriberg (2003) automatically identify the level of emotion in meeting spurts (e.g., group level of interest, hot spots). Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al., 2003) and structural information (e.g., the previous and following speaker) (Galley et al., 2004). More recently, Purver et al. (2006) have tackled the problem of detecting one type of decision, namely action items, which embody the transfer of group responsibility. However,</context>
</contexts>
<marker>Wrede, Shriberg, 2003</marker>
<rawString>B. Wrede and E. Shriberg. 2003. Spotting hot spots in meetings: Human judgements and prosodic cues. In Proceedings ofEUROSPEECH2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>