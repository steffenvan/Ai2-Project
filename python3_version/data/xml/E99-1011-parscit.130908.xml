<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.715744">
Proceedings of EACL &apos;99
</note>
<title confidence="0.960081">
The TIPSTER SUMMAC Text Summarization Evaluation
</title>
<author confidence="0.92313175">
Inderjeet Mani
David House
Gary Klein
Lynette Hirschman*
</author>
<affiliation confidence="0.571379166666667">
The MITRE Corporation
11493 Sunset Hills Rd.
Reston, VA 22090
USA
Therese Firmin
Department of Defense
</affiliation>
<address confidence="0.966568125">
9800 Savage Rd.
Ft. Meade, MD 20755
USA
Beth Sundheim
SPAWAR Systems Center
Code D44208
53140 Gatchell Rd.
San Diego, CA 92152
</address>
<email confidence="0.430208">
USA
</email>
<sectionHeader confidence="0.979056" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999778">
The TIPSTER. Text Summarization
Evaluation (SUMMAC) has established
definitively that automatic text summa-
rization is very effective in relevance as-
sessment tasks. Summaries as short as
17% of full text length sped up decision-
making by almost a factor of 2 with no
statistically significant degradation in F-
score accuracy. SUMMAC has also in-
troduced a new intrinsic method for au-
tomated evaluation of informative sum-
maries.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999119">
In May 1998, the U.S. government completed
the TIPSTER Text Summarization Evaluation
(SUMMAC), which was the first large-scale,
developer-independent evaluation of automatic
text summarization systems. The goals of the
SUMMAC evaluation were to judge individual
summarization systems in terms of their useful-
ness in specific summarization tasks and to gain
a better understanding of the issues involved in
building and evaluating such systems.
</bodyText>
<subsectionHeader confidence="0.996687">
1.1 Text Summarization
</subsectionHeader>
<bodyText confidence="0.9999545">
Text summarization is the process of distilling the
most important information from a set of sources
to produce an abridged version for particular users
and tasks (Maybury 1995). Since abridgment is
crucial, an important parameter to summariza-
tion is the level of compression (ratio of summary
length to source length) desired. Summaries can
be used to indicate what topics are addressed in
the source text, and thus can be used to alert the
user as to source content (the indicative function).
In addition, summaries can also be used to stand
in place of the source (the informative function).
</bodyText>
<page confidence="0.954491">
202 Burlington Rd., Bedford, MA 01730
</page>
<bodyText confidence="0.9998751">
They can even offer a critique of the source (the
evaluative function) (Sparck-Jones 1998). Often,
summaries are tailored to a reader&apos;s interests and
expertise, yielding topic-relatedsummaries, or else
they can be aimed at a broad readership com-
munity, as in the case of generic summaries. It
is also useful to distinguish between summaries
which are extracts of source material, and those
which are abstracts containing new text generated
by the summarizer.
</bodyText>
<subsectionHeader confidence="0.98854">
1.2 Summarization Evaluation Methods
</subsectionHeader>
<bodyText confidence="0.9995703">
Methods for evaluating text summarization can
be broadly classified into two categories.
The first, an intrinsic (or normative) evalua-
tion, judges the quality of the summary directly
based on analysis in terms of some set of norms.
This can involve user judgments of fluency of the
summary (Minel et al. 1997), (Brandow et al.
1994), coverage of stipulated &amp;quot;key/essential ideas&amp;quot;
in the source (Paice 1990), (Brandow et al. 1994),
or similarity to an &amp;quot;ideal&amp;quot; summary, e.g., (Ed-
mundson 1969), (Kupiec et al. 1995).
The problem with matching a system summary
against an ideal summary is that the ideal sum-
mary is hard to establish. There can be a large
number of generic and topic-related abstracts that
could summarize a given document. Also, there
have been several reports of low inter-annotator
agreement on sentence extracts, e.g., (Rath et al.
1961), (Salton et al. 1997), although judges may
agree more on the most important sentences to
include (Jing et al. 1998).
The second category, an extrinsic evaluation,
judges the quality of the summarization based on
how it affects the completion of some other task.
There have been a number of extrinsic evalua-
tions, including question-answering and compre-
hension tasks, e.g., (Morris et al. 1992), as well
as tasks which measure the impact of summariza-
tion on determining the relevance of a document
to a topic (Mani and Bloedorn 1997), Ping et al.
</bodyText>
<page confidence="0.996216">
77
</page>
<bodyText confidence="0.751968666666667">
Proceedings of EACL &apos;99
1998), (Tombros et al. 1998), (Brandow et al.
1994).
</bodyText>
<subsectionHeader confidence="0.997209">
1.3 Participant Technologies
</subsectionHeader>
<bodyText confidence="0.953499192307692">
Sixteen systems participated in the SUM MAC
Evaluation: Carnegie Group Inc. and Carnegie-
Mellon University (CGI/CMU), Cornell Univer-
sity and SabIR Research, Inc. (Cornell/SabIR),
GE Research and Development (GE), New
Mexico State University (NMSU), the Univer-
sity of Pennsylvania (Penn), the University of
Southern California-Information Sciences Insti-
tute (ISI), Lexis-Nexis (LN), the University of
Surrey (Surrey), IBM Thomas J. Watson Re-
search (IBM), TextWise LLC, SRA International,
British Telecommunications (BT), Intelligent Al-
gorithms (IA), the Center for Intelligent Infor-
mation Retrieval at the University of Massachus-
setts (UMass), the Russian Center for Information
Research (CIR), and the National Taiwan Uni-
versity (NTU). Table 1 offers a high-level sum-
mary of the features used by the different par-
ticipants. Most participants confined their sum-
maries to extracts of passages from the source
text; TextWise, however, extracted combinations
of passages, phrases, named entities, and subject
fields. Two participants modified the extracted
text: Penn replaced pronouns with coreferential
noun phrases, and Penn and NMSU both short-
ened sentences by dropping constituents.
</bodyText>
<sectionHeader confidence="0.990904" genericHeader="introduction">
2 SUMMAC Summarization Tasks
</sectionHeader>
<bodyText confidence="0.999977142857143">
In order to address the goals of the evaluation,
two main extrinsic evaluation tasks were defined,
based on activities typically carried out by infor-
mation analysts in the U.S. Government. In the
adhoc task, the focus was on indicative summaries
which were tailored to a particular topic. This
task relates to the real-world activity of an analyst
conducting full-text searches using an IR system
to quickly determine the relevance of a retrieved
document. Given a document (which could be a
summary or a full-text source - the subject was
not told which), and a topic description, the hu-
man subject was asked to determine whether the
document was relevant to the topic. The accuracy
of the subject&apos;s relevance assessment decision was
measured in terms of &amp;quot;ground-truth&amp;quot; judgments
of the full-text source relevance, which were sepa-
rately obtained from the Text Retrieval (TREC)
(Harman and Voorhees 1996) conferences. Thus,
an indicative summary would be &amp;quot;accurate&amp;quot; if it
accurately reflected the relevance or irrelevance of
the corresponding source.
In the categorization task, the evaluation sought
to find out whether a generic summary could ef-
fectively present enough information to allow an
analyst to quickly and correctly categorize a doc-
ument. Here the topic was not known to the
summarization system. Given a document, which
could be a generic summary or a full-text source
(the subject was not told which), the human sub-
ject would choose a single category out of five cat-
egories (each of which had an associated topic de-
scription) to which the document was relevant, or
else choose &amp;quot;none of the above&amp;quot;.
The final task, a question-answering task, was
intended to support an information analyst writ-
ing a report. This involved an intrinsic evaluation
where a topic-related summary for a document
was evaluated in terms of its &amp;quot;informativeness&amp;quot;,
namely, the degree to which it contained answers
found in the source document to a set of topic-
related questions.
</bodyText>
<sectionHeader confidence="0.983075" genericHeader="method">
3 Data Selection
</sectionHeader>
<bodyText confidence="0.999952423076923">
In the adhoc task, 20 topics were selected. For
each topic, a 50-document subset was created from
the top 200 ranked documents retrieved by a stan-
dard IR system. For the categorization task, only
10 topics were selected, with 100 documents used
per topic. For both tasks, the subsets were con-
structed such that 25%-75% of the documents
were relevant to the topic, with full-text docu-
ments being 2000-20,000 bytes (300-2700 words)
long, so that they were long enough to be worth
summarizing but short enough to be read within
the time-frame of the experiment.
The documents were all newspaper sources, the
vast majority of which were news stories, but
which also included sundry material such as letters
to the editor. Reliance on TREC data for docu-
ments and topics, and internal criteria for length,
relevance, and non-overlap among test sets, re-
sulted in the evaluation focusing mostly on short
newswire texts. We recognize that larger-sized
texts from a wider range of genres might challenge
the summarizers to a greater extent.
In each task, participants submitted two sum-
maries: a fixed-length (Si) summary limited to
10% of the length of the source, and a summary
which was not limited in length (S2).
</bodyText>
<sectionHeader confidence="0.927842" genericHeader="method">
4 Experimental Hypotheses and
Method
</sectionHeader>
<bodyText confidence="0.999881">
In meeting the evaluation goals, the main question
to be answered was whether summarization saved
time in relevance assessment, without impairing
accuracy.
</bodyText>
<page confidence="0.915089">
78
</page>
<equation confidence="0.935224733333333">
Proceedings of EACL &apos;99
Participant tf loc disc coref co-occ syn
BT + + - + + -
CGI/CMU + + - - + -
CIR + + - - - +
Cornell/SabIR + - - - + -
GE + + + + + -
IA + - - - + -
IBM + + - - - -
ISI + + - - - +
LN + - - - + -
NMSU + - + -I- - -
NTU + - + + - -
Penn - + - + - -
SRA -I- + - ± - +
</equation>
<table confidence="0.698905">
Surrey + - + - + A-
Text Wise A- - - ± + +
UMass + - - - _ + -
</table>
<tableCaption confidence="0.969542">
Table 1: Participant Summarization Features. tf: term frequency; loc: location; disc:discourse (e.g., use
of discourse model); coref: coreference; co-occ: co-occurrence; syn: synonyms.
</tableCaption>
<table confidence="0.95108025">
Ground Truth Subject&apos;s Judgment
Relevant Irrelevant
Relevant is True TP FN
Irrelevant is True FP TN
</table>
<tableCaption confidence="0.843963333333333">
Table 2: Adhoc Task Contingency Table.
TP=true positive, FP = false positive, TN= true
negative, FN=false negative.
</tableCaption>
<table confidence="0.967243">
Ground Truth Subject&apos;s Judgment
X Y None
X is True TP FN FN
None is True FP FP TN
</table>
<tableCaption confidence="0.814980666666667">
Table 3: Categorization Task Contingency Table.
X and Y are distinct categories other than None-
of-the- above, represented as None.
</tableCaption>
<bodyText confidence="0.99934005">
The first test was a summarization condition
test: to determine whether subjects&apos; relevance as-
sessment performance in terms of time and accu-
racy was affected by different conditions: full-text
(F), fixed-length summaries (Si), variable-length
summaries (S2), and baseline summaries (B). The
latter were comprised of the first 10% of the body
of the source text.
The second test was a participant technology
test: to compare the performance of different par-
ticipants&apos; systems.
The third test was a consistency test: to deter-
mine how much agreement there was between sub-
jects&apos; relevance decisions based on showing them
only full-text versions of the documents from the
main adhoc and categorization tasks. In the ad-
hoc and categorization tasks, the 1000 documents
assigned to a subject for each task were allocated
among F, B, Si, and S2 conditions through ran-
dom selection without replacement (20 F, 20 B,
480 Si, and 480 S2&apos;). For the consistency tasks,
each subject was assigned full-text versions of the
same 1000 documents. In all tasks, the presenta-
tion order was varied among subjects. The evalu-
ation used 51 professional information analysts as
subjects, each of whom took approximately 16-
20 hours. The main adhoc task used 21 sub-
jects, the main categorization 24 subjects; the
consistency adhoc task had 14 subjects, the con-
sistency categorization 7 subjects (some subjects
from the main task also did a different consistency
task). The subjects were told they were work-
ing with documents that included summaries, and
that their goal, on being presented with a topic-
document pair, was to examine each document to
determine if it was relevant to the topic. The con-
tingency tables for the adhoc and categorization
tasks are shown in Tables 2 and 3.
We used the following aggregate accuracy met-
rics:
</bodyText>
<equation confidence="0.9974955">
Precision = TP/(TP + FP) (1)
Recall = TP/(TP FN) (2)
Fscore = 2* Precision* Recall l(Precision+ Recall)
(3)
</equation>
<sectionHeader confidence="0.9835215" genericHeader="method">
5 Results: Adhoc and
Categorization Tasks
</sectionHeader>
<subsectionHeader confidence="0.99969">
5.1 Performance by Condition
</subsectionHeader>
<bodyText confidence="0.997455333333333">
In the adhoc task, summaries at compressions as
low as 17% of full text length were not significantly
&apos;This distribution assures sufficient statistical sen-
sitivity for expected effect sizes for both the sum-
marization condition and the participant technology
tests.
</bodyText>
<page confidence="0.996854">
79
</page>
<table confidence="0.988869333333333">
Proceedings of EACL &apos;99
Condition Time Time SD F-score TP FP FN TN P R
F 58.89 56.86 .67 .38 .08 .26 .28 .83 .22
S2 33.12 36.19 .64 .35 .08 .28 .28 .80 .23
Si 19.75 26.96 .53 .27 .07 .35 .31 .79 .19
B 23.15 21.82 .42 .18 .05 .41 .35 .81 .12
</table>
<tableCaption confidence="0.8927395">
Table 4: Adhoc Time and Accuracy by Condition. TP, FP, FN, TN are expressed as percentage of
totals observed in all four categories. All time differences are significant except between B and Si
</tableCaption>
<table confidence="0.865613375">
(HSD=9.8). All F-score differences are significant, except between F (Full-Text) and S2 (HSD=.10).
Precision (13) differences aren&apos;t significant. All Recall (R) differences between conditions are significant,
except between F and S2 (HSD=.12). &amp;quot;SD&amp;quot; = standard deviation.
Condition Time Time SD F-score TP FP FN TN P R
F 43.11 52.84 .50 24.3 13.3 28.5 33.9 .63 .45
S2 43.15 42.16 .50 19.3 10.5 36.9 33.3 .68 .42
51 25.48 29.81 .43 27.1 10.7 30.9 31.3 .68 .34
B 27.36 30.35 .03 7.5 11.9 52.5 28.1 .04 .02
</table>
<tableCaption confidence="0.993677">
Table 5: Categorization Time and Accuracy by Condition. Here TP, FP, FN, TN are expressed as
</tableCaption>
<bodyText confidence="0.969616518518518">
percentage of totals in all four categories. All time differences are significant except between F and
S2, and between B and Si (HSD=15.6).Only the F-score of B is significantly less than the others
(HSD=.09). Precision (P) and Recall (R) of B is significantly less than the others: HSD(Precision)=.11;
HSD(Recall)=.11.
different in accuracy from full text (Table 4), while
speeding up decision-making by almost a factor of
2 (33.12 seconds per decision average time for S2
compared to 58.89 for F in 4). Tukey&apos;s Honestly
Significant Difference test (HSD) is used to com-
pare multiple differences2.
In the categorization task, the F-score on full-
text was only .5, suggesting the task was very
hard. Here summaries at 10% of the full-text
length were not significantly different in accuracy
from full-text (Table 5) while reducing decision
time by 40% compared to full text (25.48 seconds
for Si compared to 43.11 for F in 5). The very
low F-scores for the Bs can be explained by a
bug which resulted in the same 20 relatively less-
effective B summaries being offered to each sub-
ject. However, in this task, summaries longer than
10% of the full text, while not significantly differ-
ent in accuracy from full-text, did not take less
time than full-text. In both tasks, the main ac-
curacy losses in summarization came from FNs,
not FPs, indicating the summaries were missing
topic-relevant information from the source.
</bodyText>
<subsectionHeader confidence="0.998614">
5.2 Performance by Participant
</subsectionHeader>
<bodyText confidence="0.9966526">
In the adhoc task, the systems were all very close
in accuracy for both summary types (Table 6).
Three groups of systems were evident in the ad-
hoc S2 F-score accuracy data, as shown in Table 8.
Interestingly, the Group I systems both used only
</bodyText>
<table confidence="0.9985236">
Group Members
Group I CGI/CMU, Cornell/SabIR
Group II GE, LN, NMSU, NTU,
Penn, SRA, TextWise, UMass
Group III ISI
</table>
<tableCaption confidence="0.855378">
Table 8: Adhoc Accuracy: Participant Groups for
S2 summaries. Groups I and III are significantly
</tableCaption>
<bodyText confidence="0.686655">
different in F-score (albeit with a small effect size).
Accuracy differences within groups and between
Group II and the others are not significant.
</bodyText>
<table confidence="0.915574">
Adhoc: F-Score vs. lime by Party for Best-Length Surrrnaries
0.74 &apos; CGIICMU amid / SaMR
0.70
GE +
0.66 Penn
+ U14ass NMSU
0.62 TrAtAise SRA
■NIU
0.59
ISI
0.54
0.50
0 46
</table>
<figureCaption confidence="0.987869">
Figure 1: Adhoc F-score versus Time by Partic-
</figureCaption>
<bodyText confidence="0.9406304">
ipant (variable-length summaries). HSD(F-score)
is 0.13. HSD(Time) = 12.88. Decisions based
on summaries from GE, Penn, and TextWise are
significantly faster than based on SRA and Cor-
nell/SabIR.
</bodyText>
<figure confidence="0.763040333333333">
EVV
16 20 24 28 32 36 40 IA
3561 DIE
</figure>
<footnote confidence="0.832877">
2The significance level a &lt; .05 throughout this pa-
</footnote>
<bodyText confidence="0.521096">
per, unless noted otherwise. term frequency and co-occurrence (Table 1), in
</bodyText>
<page confidence="0.943607">
80
</page>
<table confidence="0.998054428571429">
Proceedings of EACL &apos;99
System S2 Si
P R F-score P R F-score
CGI/CMU .82 .66 .72 .76 .52 .60
Cornell/SabIR .78 .67 .70 .79 .47 .56
GE .78 .60 .67 .77 .45 .55
LN .78 .58 .65 .81 .45 .55
Penn .81 .57 .65 .76 .45 .53
UMass .80 .54 .63 .81 .47 .56
NMSU .80 .54 .63 .80 .40 .52
TextWise .81 .51 .61 .79 .41 .52
SRA .82 .49 .60 .79 .37 .48
NTU .80 .49 .59 .82 .34 .46
ISI .80 .46 .56 .82 .36 .47
</table>
<tableCaption confidence="0.903284">
Table 6: Adhoc Accuracy by Participant. For variable-length: Precision (P) differences aren&apos;t signifi-
cant; CGI/CMU and Cornell/SabIR are significantly different from SRA, NTU, and ISI in Recall (R)
(HSD=0.17) and from ISI in F-score (HSD=0.13). For fixed-length, no significant differences on any of
the measures.
</tableCaption>
<table confidence="0.9999084375">
System S2 Si
P R F-score P R F-score
CIR .71 .47 .54 .68 .35 .43
IBM .68 .47 .51 .63 .37 .44
NMSU .69 .46 .51 .69 .34 .43
Surrey .69 .43 .51 .69 .31 .39
Penn .70 .42 .50 .66 .29 .38
ISI .71 .42 .49 .71 .35 .44
IA .69 .42 .49 .67 .33 .41
BT .63 .43 .48 .70 .33 .41
NTU .66 .41 .48 .68 .33 .43
SRA .65 .42 .48 .73 .37 .45
LN .68 .41 .47 .68 .37 .45
Cornell/SabIR .66 .40 .47 .62 .36 .42
GE .69 .40 .47 .69 .33 .42
CGI/CMU .74 .39 .47 .69 .33 .42
</table>
<tableCaption confidence="0.999022">
Table 7: Categorization Accuracy by Participant. No significant differences on any of the measures.
</tableCaption>
<figure confidence="0.7874335">
Adhoc: F-Score vs. Trne by Party for Fixed-Length Summaries
HVGTIME
</figure>
<figureCaption confidence="0.761579333333333">
Figure 2: Adhoc F-score versus Time by Partici-
pant (fixed-length summaries). No significant dif-
ferences in F-score, or in Time.
</figureCaption>
<bodyText confidence="0.97074096">
particular, exploiting similarity computations be-
tween text passages. For the S2 summaries (Fig-
ure 1), the Group I systems (average compression
25% for CGI/CMU and 30% for Cornell/SabIR)
were not the fastest in terms of human decision
time; in terms of both accuracy and time, Text-
Wise, GE and Penn (equivalent in accuracy) were
the closest in terms of Cartesian distance from the
ideal performance. For Si summaries (Figure 2),
the accuracy and time differences aren&apos;t signifi-
cant. Finally, clustering the systems based on de-
gree of overlap between the sets of sentences they
extracted for summaries judged TP resulted in
CGI/CMU, GE, LN, UMass, and Cornell/SabIR
clustering together on both Si and S2 summaries.
It is striking that this cluster, shown with the &amp;quot;+&amp;quot;
icon in Figures 1 and 2, corresponds to the sys-
tems with the highest F-scores, all of whom, with
the exception of GE, used similar features in anal-
ysis (Table 1).
In the categorization task, by contrast, the 14
participating systems3 had no significant differ-
ences in F-score accuracy whatsoever (Table 7,
3Note that some participants participated in only
one of the two tasks.
</bodyText>
<page confidence="0.994805">
81
</page>
<figure confidence="0.95229">
Proceedings of EACL &apos;99
Categ: F—Score vs. Time by Party for Best—Length Summaries
AVGF
</figure>
<figureCaption confidence="0.9752275">
Figure 3: Categorization F-score versus Time
by Participant (variable-length summaries). F-
scores are not significantly different. HSD(Time)
= 17.23. GE is significantly faster than SRA and
Surrey. The latter two are also significantly slower
than Penn, ISI, LN, NTU, IA, and CGI/CMU.
</figureCaption>
<figure confidence="0.976225666666667">
Categ: F—Score vs. Time by Party for Fixed—Length Summaries
21 25 29 33 37 41 45 49 53 57
ASCII*
</figure>
<figureCaption confidence="0.78224125">
Figure 4: Categorization F-score versus Time by
Participant (fixed-length summaries). F-scores
are not significantly different, and neither are time
differences.
</figureCaption>
<bodyText confidence="0.987020833333333">
Figures 3 and 4). In this task, in the absence
of a topic, the statistical salience systems which
performed relatively more accurately in the ad-
hoc task had no advantage over the others, and so
their. performance more closely resemble that of
other systems. Instead, the systems more often re-
lied on inclusion of the first sentence of the source
- a useful strategy for newswire (Brandow et al.
1994): the generic (categorization) summaries had
a higher percentage of selections of first sentences
from the source than the adhoc summaries (35% of
Si and 41% of S2 for categorization, compared to
21% Si and 32% S2 for adhoc). We may surmise
that in this task, where performance on full-text
was hard to begin with, the systems were all find-
ing the categorization task equally hard, with no
particular technique for producing generic sum-
maries standing out.
</bodyText>
<subsectionHeader confidence="0.999582">
5.3 Agreement between Subjects
</subsectionHeader>
<bodyText confidence="0.999958740740741">
As indicated in Table 9, the unanimous agreement
of just 16.6% and 19.5% in the adhoc and cat-
egorization tasks respectively is low: the agree-
ment data has Kappa (Carletta et al. 1997) of
.38 for adhoc and .29 for categorization&apos;. The ad-
hoc pairwise and 3-way agreement (i.e., agreement
between groups of 3 subjects) is consistent with a
3-subject &amp;quot;dry-run&amp;quot; adhoc consistency task car-
ried out earlier. However, it is much lower than
reported in 3-subject adhoc experiments in TREC
(Harman and Voorhees 1996). One possible expla-
nation is that in contrast to our subjects, TREC
subjects had years of experience in this task. It is
also possible that our mix of documents had fewer
obviously relevant or obviously irrelevant docu-
ments than TREC. However, as (Voorhees 1998)
has shown in her TREC study, system perfor-
mance rankings can remain relatively stable even
with lack of agreement in relevance judgments.
Further, (Voorhees 1998) found, when only rel-
evant documents were considered (and measuring
agreement by intersection over union), 44.7% pair-
wise agreement and 30.1% 3-way agreement with
3 subjects, which is comparable to our scores on
this latter measure (52.9% pairwise, 36.9% 3-way
on adhoc, 45.9% pairwise, 29.7% 3-way on cate-
gorization).
</bodyText>
<sectionHeader confidence="0.984745" genericHeader="method">
6 Question-answering (Q&amp;A) task
</sectionHeader>
<bodyText confidence="0.999959090909091">
In this task, the summarization system, given a
document and a topic, needed to produce an in-
formative, topic-related summary that contained
the answers found in that document to a set of
topic-related questions. These questions covered
&amp;quot;obligatory&amp;quot; information that had to be provided
in any document judged relevant to the topic. For
example, for a topic concerning prison overcrowd-
ing, a topic-related question would be &amp;quot;What is
the name of each correction facility where the re-
ported overcrowding exists?&amp;quot;
</bodyText>
<subsectionHeader confidence="0.99729">
6.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.982959166666667">
The topics we chose were a subset of the 20 adhoc
TREC topics selected. For each topic, 30 rele-
vant documents from the adhoc task corpus were
chosen as the source texts for topic-related sum-
marization. The principal tasks of each evaluator
(one evaluator per topic, 3 in all) were to prepare
the questions and answer keys and to score the
&apos;Dropping two outlier assessors in the categoriza-
tion task - the fastest and the slowest - resulted in the
pairwise and three-way agreement going up to 69.3%
and 54.0% respectively, making the agreement com-
parable with the adhoc task.
</bodyText>
<figure confidence="0.9578605">
0.56: GE Pere. • CIR
0.53 &apos; • IA • els! Surrey
0.50 *mu •NMSLI •
0.47 Cs, 1&amp;quot; • SRA
0.44 &apos; Carel / SabIR ET
0.41
038&apos;
,1•11111/■,11■■,13,-1-771-1-77-r, • p Ir1■11“,77-1-1W /11,111,1.11,1,
25 25 25 33 37 41. 45 49 53 57
MGT HIE
</figure>
<page confidence="0.978436">
82
</page>
<table confidence="0.991402333333333">
Proceedings of EACL &apos;99
Task Pairwise 3-way All 7 All 14
Adhoc 69.1 53.7 NA 16.6
Categorization 56.4 50.6 19.5 NA
Adhoc Dry-Run 72.7 59.1 NA NA
TREC 88.0 71.7 NA NA
</table>
<tableCaption confidence="0.999734">
Table 9: Percentage of decisions subjects agreed on when viewing full-text (consistency tasks).
</tableCaption>
<bodyText confidence="0.999366375">
system summaries. To construct the answer key,
each evaluator marked off any passages in the text
that provided an answer to a question (example
shown in Table 10).
The summaries generated by the participants
(who were given the topics and the documents
to be summarized, but not the questions) were
scored against the answer key. The evaluators
used a common set of guidelines for writing ques-
tions, creating answer keys, and scoring sum-
maries that were intended to minimize variability
across evaluators in the methods used&apos;.
Eight of the adhoc participants also submitted
summaries for the Q&amp;A evaluation. Thirty sum-
maries per topic were scored against the answer
keys.
</bodyText>
<subsectionHeader confidence="0.999637">
6.2 Scoring
</subsectionHeader>
<bodyText confidence="0.9998681">
Each summary was compared manually to the an-
swer key for a given document. If a summary con-
tained a passage that was tagged in the answer
key as the only available answer to a question,
the summary was judged Correct for that ques-
tion as long as the summary provided sufficient
context for the passage; if there was insufficient
context, the summary was judged Partially Cor-
rect. If needed context was totally lacking or was
misleading, or if the summary did not contain the
expected passage at all, the summary was judged
Missing for that question. In the case where (a)
the answer key contained multiple tagged passages
as answer(s) to a single question and (b) the sum-
mary did not contain all of those passages, asses-
sors applied additional scoring criteria to deter-
mine the amount of credit to assign.
Two accuracy metrics were defined, ARL (An-
swer Recall Lenient) and ARS (Answer Recall
Strict):
</bodyText>
<equation confidence="0.999704">
ARL = (n1 ± (.5 * n2))/n3 (4)
ARS = nl/n3 (5)
</equation>
<bodyText confidence="0.99959675">
where n1 is the number of Correct answers in the
summary, n2 is the number of Partially Correct
answers in the summary, and n3 is the number of
questions answered in the key. A third measure,
</bodyText>
<footnote confidence="0.733629333333333">
5We also had each of the evaluators score a portion
of each others&apos; test data; the scores across evaluators
were very similar, with one exception.
</footnote>
<bodyText confidence="0.8114565">
ARA (Answer Recall Average), was defined as the
average of ARL and ARS.
</bodyText>
<subsectionHeader confidence="0.811055">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999987944444444">
Figure 5 shows a plot of the ARA against com-
pression. The &amp;quot;model&amp;quot; summaries were sentence-
extraction summaries created by the evaluators
from the answer keys but not used to evaluate
the summaries. For the machine-generated sum-
maries, the highest ARA was associated with the
least reduction (35-40% compression). The sys-
tems which were in Group I in accuracy on the
adhoc task, CGI/CMU and Cornell/SabIR, were
at the top of the ARA ordering of systems on
topics 257 and 271. The participants&apos; human-
evaluated ARA scores were strongly correlated
with scores computed by a program from Cor-
nell/SabIR which measured overlap between sum-
maries and answers in the key (Pearson r &gt; .97,
a &lt; 0.0001). The Q&amp;A evaluation is therefore
promising as a new method for automated evalu-
ation of informative summaries.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.990203956521739">
SUMMAC has established definitively in a large-
scale evaluation that automatic text summariza-
tion is very effective in relevance assessment tasks.
Summaries at relatively low compression rates
(summaries as short as 17% of source length for
adhoc, 10% for categorization) allowed for rele-
vance assessment almost as accurate as with full-
text (5% degradation in F-score for adhoc and
14% degradation for categorization, both degra-
dations not being statistically significant), while
reducing decision-making time by 40% (catego-
rization) and 50% (adhoc). Analysis of feed-
back forms filled in after each decision indicated
that the intelligibility of present-day machine-
generated summaries is high, due to use of sen-
tence extraction and coherence &amp;quot;smoothing&amp;quot;6.
The task of topic-related summarization, when
limited to passage extraction, can be character-
ized as a passage ranking problem, and as such
lends itself very well to information retrieval tech-
60n the adhoc task, 99% of F were judged &amp;quot;intel-
ligible&amp;quot;, as were 93% S2, 96% B, 83% Si; similar data
for categorization.
</bodyText>
<page confidence="0.993935">
83
</page>
<figure confidence="0.989742">
Proceedings of EACL &apos;99
0.9 257 271 _ ___________ _____ — _ __ ___
0.8 258
7,6. OM
67 215 MCI G .0 ze
0. 6 A A 2TTT 2&amp;quot;
OS 0258 • zsa 217A 25?
271 La
OA 211
0.2
62 211 0 0-251 *211
lazy
0.1
a
*COI/COO
/3Com OSSA/
AGE
.16
.A MOO
401444
XS RA
018 Are in
son
OA 0.1 0.3 04 0.5
Osop r4 4105
</figure>
<figureCaption confidence="0.999779">
Figure 5: ARA versus Compression by Participant. &amp;quot;Modsumms&amp;quot; are model summaries.
</figureCaption>
<table confidence="0.762936">
Title: Computer Security
Description : Identify instances of illegal entry into sensitive
computer networks by nonauthorized personnel.
Narrative : Illegal entry into sensitive computer networks
</table>
<bodyText confidence="0.976835227272727">
is a serious and potentially menacing problem. Both &apos;hackers&apos; and
foreign agents have been known to acquire unauthorized entry into
various networks. Items relative this subject would include but not
be limited to instances of illegally entering networks containing
information of a sensitive nature to specific countries, such as
defense or technology information, international banking, etc. Items
of a personal nature (e.g. credit card fraud, changing of college
test scores) should not be considered relevant.
Questions
1)Who is the known or suspected hacker accessing a sensitive computer or computer network?
2) How is the hacking accomplished or putatively achieved?
3) Who is the apparent target of the hacker?
4) What did the hacker accomplish once the violation occurred?
What was the purpose in performing the violation?
5) What is the time period over which the breakins were occurring?
As a federal grand jury decides whether he should be prosecuted, &lt;Q1&gt;a graduate
student&lt;/Q1&gt; linked to a &amp;quot;virus&amp;quot; that disrupted computers nationwide &lt;Q5&gt;last
month&lt;/Q5&gt;has been teaching his lawyer about the technical subject and turning down
offers for his life story. ....No charges have been filed against &lt;Q1&gt;Morris&lt;/Q1&gt;,
who reportedly told friends that he designed the virus that temporarily clogged about
&lt;Q3&gt;6,000 university and military computers&lt;/Q3&gt; &lt;Q2&gt;linked to the Pentagon&apos;s Arpanet
network&lt;/Q2&gt;.
</bodyText>
<tableCaption confidence="0.8147525">
Table 10: Q&amp;A Topic 258, topic-related questions, and part of a relevant source document showing
answer key annotations.
</tableCaption>
<page confidence="0.996763">
84
</page>
<bodyText confidence="0.98801412">
Proceedings of EACL &apos;99
niques. Summarizers that performed most accu-
rately in the adhoc task used statistical passage
similarity and passage ranking methods common
in information retrieval. Overall, the most accu-
rate systems in this task used similar features and
had similar sentence extraction behavior.
However, for the generic summaries in the cat-
egorization task (which was hard even for hu-
mans with full-text), in the absence of a topic, the
summarization methods in use by these systems
were indistinguishable in accuracy. Whether this
suggests an inherent limitation to summarization
methods which produce extracts of the source, as
opposed to generating abstracts, remains to be
seen.
In future, text summarization evaluations will
benefit greatly from the availability of test sets
covering a wider variety of genres, and including
much longer documents. The extrinsic and in-
trinsic evaluations reported here are also relevant
to the evaluation of other NLP technologies where
there may be many potentially acceptable outputs
(e.g., machine translation, text generation, speech
synthesis).
</bodyText>
<sectionHeader confidence="0.994857" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999951222222222">
The authors wish to thank Eric Bloedorn, John
Burger, Mike Chrzanowski, Barbara Gates, Glenn
Iwerks, Leo Obrst, Sara Shelton, and Sandra Wag-
ner, as well as 51 experimental subjects. We are
also grateful to the Linguistic Data Consortium
for making the TREC documents available to us,
and to the National Institute of Standards and
Technology for providing TREC data and the ini-
tial version of the ASSESS tool.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999386514705882">
Brandow, R., K. Mitze, and L. Rau. 1994. Auto-
matic condensation of electronic publications by
sentence selection. Information Processing and
Management, 31(5).
Carletta, J., A. Isard, S. Isard, J. C. Jowtko, G.
Doherty-Sneddon, and A. H. Anderson. 1997.
The Reliability of a Dialogue Structure Coding
Scheme. Computational Linguistics, 23, 1, 13-
32.
Edmundson, H.P. 1969. New methods in auto-
matic abstracting. The Association for Comput-
ing Machinery, 16(2).
Harman, D.K. and E.M. Voorhees. 1996. The fifth
text retrieval conference (trec-5). National In-
stitute of Standards and Technology NIST SP
500-238.
Jing, H., R. Barzilay, K. McKeown, and M. El-
hadad. 1998. Summarization evaluation meth-
ods: Experiments and analysis. in Working
Notes of the AAAI Spring Symposium on Intel-
ligent Text Summarization, Spring 1998, Tech-
nical Report, AAAI, 1998.
Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. Proceedings of the
18th ACM SIGIR Conference (SIGIR &apos;95).
Mani, I. and E. Bloedorn. 1997. Multi-document
Summarization by Graph Search and Merging.
Proceedings of the Fourteenth National Con-
ference on Artificial Intelligence (AAAI-97),
Providence, RI, July 27-31, 1997, 622-628.
Maybury, M. 1995. Generating Summaries from
Event Data. Information Processing and Man-
agement, 31,5, 735-751.
Minel, J-L., S. Nugier, and G. Piat. 1997. How to
appreciate the quality of automatic text sum-
marization. In Mani, I. and Maybury, M., eds.,
Proceedings of the ACL/EACL &apos;97 Workshop on
Intelligent Scalable Text Summarization.
Morris, A., G. Kasper, and D. Adams. 1992.
The Effects and Limitations of Automatic Text
Condensing on Reading Comprehension Perfor-
mance. Information Systems Research, 3(1).
Paice, C. 1990. Constructing literature abstracts
by computer: Techniques and prospects. Infor-
mation Processing and Management, 26(1).
Rath, G.J., A. Resnick, and T.R. Savage. 1961.
The formation of abstracts by the selection of
sentences. American Documentation, 12(2).
Salton, G., A. Singhal, M. Mitra, and C. Buckley.
1997. Automatic Text Structuring and Summa-
rization. Information Processing and Manage-
ment, 33(2).
Sparck-Jones, K. 1998. Summarizing: Where are
we now? where should we go? Mani, I.
and Maybury, M., eds., Proceedings of the
ACL/EACL&apos;97 Workshop on Intelligent Scal-
able Text Summarization.
Tombros, A., and M. Sanderson. 1998. Advan-
tages of query biased summaries in information
retrieval. in Proceedings of the 21st ACM SIGIR
Conference (SIGIR&apos;98), 2-10.
Voorhees, Ellen M. 1998. Variations in Relevance
Judgments and the Measurement of Retrieval
Effectiveness. In Proceedings of the 21st An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval (SIGIR-98), Melbourne, Australia. 315-
323.
</reference>
<page confidence="0.999698">
85
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.356949">
<note confidence="0.806223">Proceedings of EACL &apos;99</note>
<title confidence="0.763704">The TIPSTER SUMMAC Text Summarization Evaluation Inderjeet Mani</title>
<author confidence="0.988889">David House Gary Klein Lynette Hirschman</author>
<affiliation confidence="0.999613">The MITRE Corporation</affiliation>
<address confidence="0.998718333333333">11493 Sunset Hills Rd. Reston, VA 22090 USA</address>
<author confidence="0.99842">Therese Firmin</author>
<affiliation confidence="0.999636">Department of Defense</affiliation>
<address confidence="0.998166333333333">9800 Savage Rd. Ft. Meade, MD 20755 USA</address>
<author confidence="0.997473">Beth Sundheim</author>
<affiliation confidence="0.999602">SPAWAR Systems Center</affiliation>
<address confidence="0.999055">Code D44208 53140 Gatchell Rd. San Diego, CA 92152 USA</address>
<abstract confidence="0.990781692307692">The TIPSTER. Text Summarization Evaluation (SUMMAC) has established definitively that automatic text summarization is very effective in relevance assessment tasks. Summaries as short as 17% of full text length sped up decisionmaking by almost a factor of 2 with no statistically significant degradation in Fscore accuracy. SUMMAC has also introduced a new intrinsic method for automated evaluation of informative summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Brandow</author>
<author>K Mitze</author>
<author>L Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1994</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>31</volume>
<issue>5</issue>
<contexts>
<context position="2740" citStr="Brandow et al. 1994" startWordPosition="423" endWordPosition="426">an be aimed at a broad readership community, as in the case of generic summaries. It is also useful to distinguish between summaries which are extracts of source material, and those which are abstracts containing new text generated by the summarizer. 1.2 Summarization Evaluation Methods Methods for evaluating text summarization can be broadly classified into two categories. The first, an intrinsic (or normative) evaluation, judges the quality of the summary directly based on analysis in terms of some set of norms. This can involve user judgments of fluency of the summary (Minel et al. 1997), (Brandow et al. 1994), coverage of stipulated &amp;quot;key/essential ideas&amp;quot; in the source (Paice 1990), (Brandow et al. 1994), or similarity to an &amp;quot;ideal&amp;quot; summary, e.g., (Edmundson 1969), (Kupiec et al. 1995). The problem with matching a system summary against an ideal summary is that the ideal summary is hard to establish. There can be a large number of generic and topic-related abstracts that could summarize a given document. Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on the most important sen</context>
<context position="19008" citStr="Brandow et al. 1994" startWordPosition="3199" endWordPosition="3202">xed—Length Summaries 21 25 29 33 37 41 45 49 53 57 ASCII* Figure 4: Categorization F-score versus Time by Participant (fixed-length summaries). F-scores are not significantly different, and neither are time differences. Figures 3 and 4). In this task, in the absence of a topic, the statistical salience systems which performed relatively more accurately in the adhoc task had no advantage over the others, and so their. performance more closely resemble that of other systems. Instead, the systems more often relied on inclusion of the first sentence of the source - a useful strategy for newswire (Brandow et al. 1994): the generic (categorization) summaries had a higher percentage of selections of first sentences from the source than the adhoc summaries (35% of Si and 41% of S2 for categorization, compared to 21% Si and 32% S2 for adhoc). We may surmise that in this task, where performance on full-text was hard to begin with, the systems were all finding the categorization task equally hard, with no particular technique for producing generic summaries standing out. 5.3 Agreement between Subjects As indicated in Table 9, the unanimous agreement of just 16.6% and 19.5% in the adhoc and categorization tasks r</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1994</marker>
<rawString>Brandow, R., K. Mitze, and L. Rau. 1994. Automatic condensation of electronic publications by sentence selection. Information Processing and Management, 31(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>A Isard</author>
<author>S Isard</author>
<author>J C Jowtko</author>
<author>G Doherty-Sneddon</author>
<author>A H Anderson</author>
</authors>
<title>The Reliability of a Dialogue Structure Coding Scheme.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<pages>13--32</pages>
<contexts>
<context position="19679" citStr="Carletta et al. 1997" startWordPosition="3312" endWordPosition="3315">her percentage of selections of first sentences from the source than the adhoc summaries (35% of Si and 41% of S2 for categorization, compared to 21% Si and 32% S2 for adhoc). We may surmise that in this task, where performance on full-text was hard to begin with, the systems were all finding the categorization task equally hard, with no particular technique for producing generic summaries standing out. 5.3 Agreement between Subjects As indicated in Table 9, the unanimous agreement of just 16.6% and 19.5% in the adhoc and categorization tasks respectively is low: the agreement data has Kappa (Carletta et al. 1997) of .38 for adhoc and .29 for categorization&apos;. The adhoc pairwise and 3-way agreement (i.e., agreement between groups of 3 subjects) is consistent with a 3-subject &amp;quot;dry-run&amp;quot; adhoc consistency task carried out earlier. However, it is much lower than reported in 3-subject adhoc experiments in TREC (Harman and Voorhees 1996). One possible explanation is that in contrast to our subjects, TREC subjects had years of experience in this task. It is also possible that our mix of documents had fewer obviously relevant or obviously irrelevant documents than TREC. However, as (Voorhees 1998) has shown in </context>
</contexts>
<marker>Carletta, Isard, Isard, Jowtko, Doherty-Sneddon, Anderson, 1997</marker>
<rawString>Carletta, J., A. Isard, S. Isard, J. C. Jowtko, G. Doherty-Sneddon, and A. H. Anderson. 1997. The Reliability of a Dialogue Structure Coding Scheme. Computational Linguistics, 23, 1, 13-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic abstracting.</title>
<date>1969</date>
<journal>The Association for Computing Machinery,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="2897" citStr="Edmundson 1969" startWordPosition="448" endWordPosition="450">material, and those which are abstracts containing new text generated by the summarizer. 1.2 Summarization Evaluation Methods Methods for evaluating text summarization can be broadly classified into two categories. The first, an intrinsic (or normative) evaluation, judges the quality of the summary directly based on analysis in terms of some set of norms. This can involve user judgments of fluency of the summary (Minel et al. 1997), (Brandow et al. 1994), coverage of stipulated &amp;quot;key/essential ideas&amp;quot; in the source (Paice 1990), (Brandow et al. 1994), or similarity to an &amp;quot;ideal&amp;quot; summary, e.g., (Edmundson 1969), (Kupiec et al. 1995). The problem with matching a system summary against an ideal summary is that the ideal summary is hard to establish. There can be a large number of generic and topic-related abstracts that could summarize a given document. Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on the most important sentences to include (Jing et al. 1998). The second category, an extrinsic evaluation, judges the quality of the summarization based on how it affects the compl</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H.P. 1969. New methods in automatic abstracting. The Association for Computing Machinery, 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Harman</author>
<author>E M Voorhees</author>
</authors>
<date>1996</date>
<booktitle>The fifth text retrieval conference (trec-5). National Institute of Standards and Technology NIST SP</booktitle>
<pages>500--238</pages>
<contexts>
<context position="6032" citStr="Harman and Voorhees 1996" startWordPosition="933" endWordPosition="936">ar topic. This task relates to the real-world activity of an analyst conducting full-text searches using an IR system to quickly determine the relevance of a retrieved document. Given a document (which could be a summary or a full-text source - the subject was not told which), and a topic description, the human subject was asked to determine whether the document was relevant to the topic. The accuracy of the subject&apos;s relevance assessment decision was measured in terms of &amp;quot;ground-truth&amp;quot; judgments of the full-text source relevance, which were separately obtained from the Text Retrieval (TREC) (Harman and Voorhees 1996) conferences. Thus, an indicative summary would be &amp;quot;accurate&amp;quot; if it accurately reflected the relevance or irrelevance of the corresponding source. In the categorization task, the evaluation sought to find out whether a generic summary could effectively present enough information to allow an analyst to quickly and correctly categorize a document. Here the topic was not known to the summarization system. Given a document, which could be a generic summary or a full-text source (the subject was not told which), the human subject would choose a single category out of five categories (each of which </context>
<context position="20002" citStr="Harman and Voorhees 1996" startWordPosition="3364" endWordPosition="3367">qually hard, with no particular technique for producing generic summaries standing out. 5.3 Agreement between Subjects As indicated in Table 9, the unanimous agreement of just 16.6% and 19.5% in the adhoc and categorization tasks respectively is low: the agreement data has Kappa (Carletta et al. 1997) of .38 for adhoc and .29 for categorization&apos;. The adhoc pairwise and 3-way agreement (i.e., agreement between groups of 3 subjects) is consistent with a 3-subject &amp;quot;dry-run&amp;quot; adhoc consistency task carried out earlier. However, it is much lower than reported in 3-subject adhoc experiments in TREC (Harman and Voorhees 1996). One possible explanation is that in contrast to our subjects, TREC subjects had years of experience in this task. It is also possible that our mix of documents had fewer obviously relevant or obviously irrelevant documents than TREC. However, as (Voorhees 1998) has shown in her TREC study, system performance rankings can remain relatively stable even with lack of agreement in relevance judgments. Further, (Voorhees 1998) found, when only relevant documents were considered (and measuring agreement by intersection over union), 44.7% pairwise agreement and 30.1% 3-way agreement with 3 subjects,</context>
</contexts>
<marker>Harman, Voorhees, 1996</marker>
<rawString>Harman, D.K. and E.M. Voorhees. 1996. The fifth text retrieval conference (trec-5). National Institute of Standards and Technology NIST SP 500-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Barzilay</author>
<author>K McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Summarization evaluation methods: Experiments and analysis.</title>
<date>1998</date>
<booktitle>in Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization,</booktitle>
<tech>Technical Report, AAAI,</tech>
<location>Spring</location>
<contexts>
<context position="3376" citStr="Jing et al. 1998" startWordPosition="527" endWordPosition="530">lated &amp;quot;key/essential ideas&amp;quot; in the source (Paice 1990), (Brandow et al. 1994), or similarity to an &amp;quot;ideal&amp;quot; summary, e.g., (Edmundson 1969), (Kupiec et al. 1995). The problem with matching a system summary against an ideal summary is that the ideal summary is hard to establish. There can be a large number of generic and topic-related abstracts that could summarize a given document. Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on the most important sentences to include (Jing et al. 1998). The second category, an extrinsic evaluation, judges the quality of the summarization based on how it affects the completion of some other task. There have been a number of extrinsic evaluations, including question-answering and comprehension tasks, e.g., (Morris et al. 1992), as well as tasks which measure the impact of summarization on determining the relevance of a document to a topic (Mani and Bloedorn 1997), Ping et al. 77 Proceedings of EACL &apos;99 1998), (Tombros et al. 1998), (Brandow et al. 1994). 1.3 Participant Technologies Sixteen systems participated in the SUM MAC Evaluation: Carn</context>
</contexts>
<marker>Jing, Barzilay, McKeown, Elhadad, 1998</marker>
<rawString>Jing, H., R. Barzilay, K. McKeown, and M. Elhadad. 1998. Summarization evaluation methods: Experiments and analysis. in Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization, Spring 1998, Technical Report, AAAI, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pedersen Kupiec</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>Proceedings of the 18th ACM SIGIR Conference (SIGIR &apos;95).</booktitle>
<marker>Kupiec, Chen, 1995</marker>
<rawString>Kupiec, J. Pedersen, and F. Chen. 1995. A trainable document summarizer. Proceedings of the 18th ACM SIGIR Conference (SIGIR &apos;95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedorn</author>
</authors>
<title>Multi-document Summarization by Graph Search and Merging.</title>
<date>1997</date>
<booktitle>Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97),</booktitle>
<pages>622--628</pages>
<location>Providence, RI,</location>
<contexts>
<context position="3793" citStr="Mani and Bloedorn 1997" startWordPosition="595" endWordPosition="598">al reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on the most important sentences to include (Jing et al. 1998). The second category, an extrinsic evaluation, judges the quality of the summarization based on how it affects the completion of some other task. There have been a number of extrinsic evaluations, including question-answering and comprehension tasks, e.g., (Morris et al. 1992), as well as tasks which measure the impact of summarization on determining the relevance of a document to a topic (Mani and Bloedorn 1997), Ping et al. 77 Proceedings of EACL &apos;99 1998), (Tombros et al. 1998), (Brandow et al. 1994). 1.3 Participant Technologies Sixteen systems participated in the SUM MAC Evaluation: Carnegie Group Inc. and CarnegieMellon University (CGI/CMU), Cornell University and SabIR Research, Inc. (Cornell/SabIR), GE Research and Development (GE), New Mexico State University (NMSU), the University of Pennsylvania (Penn), the University of Southern California-Information Sciences Institute (ISI), Lexis-Nexis (LN), the University of Surrey (Surrey), IBM Thomas J. Watson Research (IBM), TextWise LLC, SRA Intern</context>
</contexts>
<marker>Mani, Bloedorn, 1997</marker>
<rawString>Mani, I. and E. Bloedorn. 1997. Multi-document Summarization by Graph Search and Merging. Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97), Providence, RI, July 27-31, 1997, 622-628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maybury</author>
</authors>
<title>Generating Summaries from Event Data.</title>
<date>1995</date>
<journal>Information Processing and Management,</journal>
<volume>31</volume>
<pages>735--751</pages>
<contexts>
<context position="1456" citStr="Maybury 1995" startWordPosition="219" endWordPosition="220">t completed the TIPSTER Text Summarization Evaluation (SUMMAC), which was the first large-scale, developer-independent evaluation of automatic text summarization systems. The goals of the SUMMAC evaluation were to judge individual summarization systems in terms of their usefulness in specific summarization tasks and to gain a better understanding of the issues involved in building and evaluating such systems. 1.1 Text Summarization Text summarization is the process of distilling the most important information from a set of sources to produce an abridged version for particular users and tasks (Maybury 1995). Since abridgment is crucial, an important parameter to summarization is the level of compression (ratio of summary length to source length) desired. Summaries can be used to indicate what topics are addressed in the source text, and thus can be used to alert the user as to source content (the indicative function). In addition, summaries can also be used to stand in place of the source (the informative function). 202 Burlington Rd., Bedford, MA 01730 They can even offer a critique of the source (the evaluative function) (Sparck-Jones 1998). Often, summaries are tailored to a reader&apos;s interest</context>
</contexts>
<marker>Maybury, 1995</marker>
<rawString>Maybury, M. 1995. Generating Summaries from Event Data. Information Processing and Management, 31,5, 735-751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Minel</author>
<author>S Nugier</author>
<author>G Piat</author>
</authors>
<title>How to appreciate the quality of automatic text summarization.</title>
<date>1997</date>
<booktitle>Proceedings of the ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization.</booktitle>
<editor>In Mani, I. and Maybury, M., eds.,</editor>
<contexts>
<context position="2717" citStr="Minel et al. 1997" startWordPosition="419" endWordPosition="422">aries, or else they can be aimed at a broad readership community, as in the case of generic summaries. It is also useful to distinguish between summaries which are extracts of source material, and those which are abstracts containing new text generated by the summarizer. 1.2 Summarization Evaluation Methods Methods for evaluating text summarization can be broadly classified into two categories. The first, an intrinsic (or normative) evaluation, judges the quality of the summary directly based on analysis in terms of some set of norms. This can involve user judgments of fluency of the summary (Minel et al. 1997), (Brandow et al. 1994), coverage of stipulated &amp;quot;key/essential ideas&amp;quot; in the source (Paice 1990), (Brandow et al. 1994), or similarity to an &amp;quot;ideal&amp;quot; summary, e.g., (Edmundson 1969), (Kupiec et al. 1995). The problem with matching a system summary against an ideal summary is that the ideal summary is hard to establish. There can be a large number of generic and topic-related abstracts that could summarize a given document. Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on</context>
</contexts>
<marker>Minel, Nugier, Piat, 1997</marker>
<rawString>Minel, J-L., S. Nugier, and G. Piat. 1997. How to appreciate the quality of automatic text summarization. In Mani, I. and Maybury, M., eds., Proceedings of the ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Morris</author>
<author>G Kasper</author>
<author>D Adams</author>
</authors>
<title>The Effects and Limitations of Automatic Text Condensing on Reading Comprehension Performance.</title>
<date>1992</date>
<journal>Information Systems Research,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="3654" citStr="Morris et al. 1992" startWordPosition="570" endWordPosition="573">. There can be a large number of generic and topic-related abstracts that could summarize a given document. Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on the most important sentences to include (Jing et al. 1998). The second category, an extrinsic evaluation, judges the quality of the summarization based on how it affects the completion of some other task. There have been a number of extrinsic evaluations, including question-answering and comprehension tasks, e.g., (Morris et al. 1992), as well as tasks which measure the impact of summarization on determining the relevance of a document to a topic (Mani and Bloedorn 1997), Ping et al. 77 Proceedings of EACL &apos;99 1998), (Tombros et al. 1998), (Brandow et al. 1994). 1.3 Participant Technologies Sixteen systems participated in the SUM MAC Evaluation: Carnegie Group Inc. and CarnegieMellon University (CGI/CMU), Cornell University and SabIR Research, Inc. (Cornell/SabIR), GE Research and Development (GE), New Mexico State University (NMSU), the University of Pennsylvania (Penn), the University of Southern California-Information S</context>
</contexts>
<marker>Morris, Kasper, Adams, 1992</marker>
<rawString>Morris, A., G. Kasper, and D. Adams. 1992. The Effects and Limitations of Automatic Text Condensing on Reading Comprehension Performance. Information Systems Research, 3(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paice</author>
</authors>
<title>Constructing literature abstracts by computer: Techniques and prospects.</title>
<date>1990</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="2813" citStr="Paice 1990" startWordPosition="435" endWordPosition="436">It is also useful to distinguish between summaries which are extracts of source material, and those which are abstracts containing new text generated by the summarizer. 1.2 Summarization Evaluation Methods Methods for evaluating text summarization can be broadly classified into two categories. The first, an intrinsic (or normative) evaluation, judges the quality of the summary directly based on analysis in terms of some set of norms. This can involve user judgments of fluency of the summary (Minel et al. 1997), (Brandow et al. 1994), coverage of stipulated &amp;quot;key/essential ideas&amp;quot; in the source (Paice 1990), (Brandow et al. 1994), or similarity to an &amp;quot;ideal&amp;quot; summary, e.g., (Edmundson 1969), (Kupiec et al. 1995). The problem with matching a system summary against an ideal summary is that the ideal summary is hard to establish. There can be a large number of generic and topic-related abstracts that could summarize a given document. Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on the most important sentences to include (Jing et al. 1998). The second category, an extrinsic e</context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Paice, C. 1990. Constructing literature abstracts by computer: Techniques and prospects. Information Processing and Management, 26(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J Rath</author>
<author>A Resnick</author>
<author>T R Savage</author>
</authors>
<title>The formation of abstracts by the selection of sentences.</title>
<date>1961</date>
<journal>American Documentation,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="3260" citStr="Rath et al. 1961" startWordPosition="507" endWordPosition="510">s can involve user judgments of fluency of the summary (Minel et al. 1997), (Brandow et al. 1994), coverage of stipulated &amp;quot;key/essential ideas&amp;quot; in the source (Paice 1990), (Brandow et al. 1994), or similarity to an &amp;quot;ideal&amp;quot; summary, e.g., (Edmundson 1969), (Kupiec et al. 1995). The problem with matching a system summary against an ideal summary is that the ideal summary is hard to establish. There can be a large number of generic and topic-related abstracts that could summarize a given document. Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on the most important sentences to include (Jing et al. 1998). The second category, an extrinsic evaluation, judges the quality of the summarization based on how it affects the completion of some other task. There have been a number of extrinsic evaluations, including question-answering and comprehension tasks, e.g., (Morris et al. 1992), as well as tasks which measure the impact of summarization on determining the relevance of a document to a topic (Mani and Bloedorn 1997), Ping et al. 77 Proceedings of EACL &apos;99 1998), (Tombros et al. 199</context>
</contexts>
<marker>Rath, Resnick, Savage, 1961</marker>
<rawString>Rath, G.J., A. Resnick, and T.R. Savage. 1961. The formation of abstracts by the selection of sentences. American Documentation, 12(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Singhal</author>
<author>M Mitra</author>
<author>C Buckley</author>
</authors>
<date>1997</date>
<booktitle>Automatic Text Structuring and Summarization. Information Processing and Management,</booktitle>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3282" citStr="Salton et al. 1997" startWordPosition="511" endWordPosition="514">udgments of fluency of the summary (Minel et al. 1997), (Brandow et al. 1994), coverage of stipulated &amp;quot;key/essential ideas&amp;quot; in the source (Paice 1990), (Brandow et al. 1994), or similarity to an &amp;quot;ideal&amp;quot; summary, e.g., (Edmundson 1969), (Kupiec et al. 1995). The problem with matching a system summary against an ideal summary is that the ideal summary is hard to establish. There can be a large number of generic and topic-related abstracts that could summarize a given document. Also, there have been several reports of low inter-annotator agreement on sentence extracts, e.g., (Rath et al. 1961), (Salton et al. 1997), although judges may agree more on the most important sentences to include (Jing et al. 1998). The second category, an extrinsic evaluation, judges the quality of the summarization based on how it affects the completion of some other task. There have been a number of extrinsic evaluations, including question-answering and comprehension tasks, e.g., (Morris et al. 1992), as well as tasks which measure the impact of summarization on determining the relevance of a document to a topic (Mani and Bloedorn 1997), Ping et al. 77 Proceedings of EACL &apos;99 1998), (Tombros et al. 1998), (Brandow et al. 19</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1997</marker>
<rawString>Salton, G., A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic Text Structuring and Summarization. Information Processing and Management, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck-Jones</author>
</authors>
<title>Summarizing: Where are we now? where should we go?</title>
<date>1998</date>
<booktitle>Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization.</booktitle>
<editor>Mani, I. and Maybury, M., eds.,</editor>
<contexts>
<context position="2002" citStr="Sparck-Jones 1998" startWordPosition="309" endWordPosition="310">roduce an abridged version for particular users and tasks (Maybury 1995). Since abridgment is crucial, an important parameter to summarization is the level of compression (ratio of summary length to source length) desired. Summaries can be used to indicate what topics are addressed in the source text, and thus can be used to alert the user as to source content (the indicative function). In addition, summaries can also be used to stand in place of the source (the informative function). 202 Burlington Rd., Bedford, MA 01730 They can even offer a critique of the source (the evaluative function) (Sparck-Jones 1998). Often, summaries are tailored to a reader&apos;s interests and expertise, yielding topic-relatedsummaries, or else they can be aimed at a broad readership community, as in the case of generic summaries. It is also useful to distinguish between summaries which are extracts of source material, and those which are abstracts containing new text generated by the summarizer. 1.2 Summarization Evaluation Methods Methods for evaluating text summarization can be broadly classified into two categories. The first, an intrinsic (or normative) evaluation, judges the quality of the summary directly based on an</context>
</contexts>
<marker>Sparck-Jones, 1998</marker>
<rawString>Sparck-Jones, K. 1998. Summarizing: Where are we now? where should we go? Mani, I. and Maybury, M., eds., Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tombros</author>
<author>M Sanderson</author>
</authors>
<title>Advantages of query biased summaries in information retrieval.</title>
<date>1998</date>
<booktitle>in Proceedings of the 21st ACM SIGIR Conference (SIGIR&apos;98),</booktitle>
<pages>2--10</pages>
<marker>Tombros, Sanderson, 1998</marker>
<rawString>Tombros, A., and M. Sanderson. 1998. Advantages of query biased summaries in information retrieval. in Proceedings of the 21st ACM SIGIR Conference (SIGIR&apos;98), 2-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-98),</booktitle>
<pages>315--323</pages>
<location>Melbourne,</location>
<contexts>
<context position="20265" citStr="Voorhees 1998" startWordPosition="3410" endWordPosition="3411">Kappa (Carletta et al. 1997) of .38 for adhoc and .29 for categorization&apos;. The adhoc pairwise and 3-way agreement (i.e., agreement between groups of 3 subjects) is consistent with a 3-subject &amp;quot;dry-run&amp;quot; adhoc consistency task carried out earlier. However, it is much lower than reported in 3-subject adhoc experiments in TREC (Harman and Voorhees 1996). One possible explanation is that in contrast to our subjects, TREC subjects had years of experience in this task. It is also possible that our mix of documents had fewer obviously relevant or obviously irrelevant documents than TREC. However, as (Voorhees 1998) has shown in her TREC study, system performance rankings can remain relatively stable even with lack of agreement in relevance judgments. Further, (Voorhees 1998) found, when only relevant documents were considered (and measuring agreement by intersection over union), 44.7% pairwise agreement and 30.1% 3-way agreement with 3 subjects, which is comparable to our scores on this latter measure (52.9% pairwise, 36.9% 3-way on adhoc, 45.9% pairwise, 29.7% 3-way on categorization). 6 Question-answering (Q&amp;A) task In this task, the summarization system, given a document and a topic, needed to produc</context>
</contexts>
<marker>Voorhees, 1998</marker>
<rawString>Voorhees, Ellen M. 1998. Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-98), Melbourne, Australia. 315-323.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>