<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000992">
<title confidence="0.998159">
N-gram Weighting: Reducing Training Data Mismatch
in Cross-Domain Language Model Estimation
</title>
<author confidence="0.955945">
Bo-June (Paul) Hsu, James Glass
</author>
<affiliation confidence="0.87009">
MIT Computer Science and Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.941158">
32 Vassar Street, Cambridge, MA, 02139 USA
</address>
<email confidence="0.999599">
{bohsu,glass}@csail.mit.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876466666667">
In domains with insufficient matched training
data, language models are often constructed
by interpolating component models trained
from partially matched corpora. Since the n-
grams from such corpora may not be of equal
relevance to the target domain, we propose
an n-gram weighting technique to adjust the
component n-gram probabilities based on fea-
tures derived from readily available segmen-
tation and metadata information for each cor-
pus. Using a log-linear combination of such
features, the resulting model achieves up to a
1.2% absolute word error rate reduction over a
linearly interpolated baseline language model
on a lecture transcription task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991614">
Many application domains in machine learning suf-
fer from a dearth of matched training data. However,
partially matched data sets are often available in
abundance. Past attempts to utilize the mismatched
data for training often result in models that exhibit
biases not observed in the target domain. In this
work, we will investigate the use of the often readily
available data segmentation and metadata attributes
associated with each corpus to reduce the effect of
such bias. We will examine this approach in the con-
text of language modeling for lecture transcription.
Compared with other types of audio data, lecture
speech often exhibits a high degree of spontaneity
and focuses on narrow topics with special termi-
nologies (Glass et al., 2004). While we may have
existing transcripts from general lectures or written
text on the precise topic, training data that matches
both the style and topic of the target lecture is often
scarce. Thus, past research has investigated various
adaptation and interpolation techniques to make use
of partially matched corpora (Bellegarda, 2004).
Training corpora are often segmented into docu-
ments with associated metadata, such as title, date,
and speaker. For lectures, if the data contains even
a few lectures on linear algebra, conventional lan-
guage modeling methods that lump the documents
together will tend to assign disproportionately high
probability to frequent terms like vector and matrix.
Can we utilize the segmentation and metadata infor-
mation to reduce the biases resulting from training
data mismatch?
In this work, we present such a technique where
we weight each n-gram count in a standard n-gram
language model (LM) estimation procedure by a rel-
evance factor computed via a log-linear combina-
tion of n-gram features. Utilizing features that cor-
relate with the specificity of n-grams to subsets of
the training documents, we effectively de-emphasize
out-of-domain n-grams. By interpolating models,
such as general lectures and course textbook, that
match the target domain in complementary ways,
and optimizing the weighting and interpolation pa-
rameters jointly, we allow each n-gram probabil-
ity to be modeled by the most relevant interpolation
component. Using a combination of features derived
from multiple partitions of the training documents,
the resulting weighted n-gram model achieves up
to a 1.2% absolute word error rate (WER) reduc-
tion over a linearly interpolated baseline on a lecture
transcription task.
</bodyText>
<page confidence="0.982352">
829
</page>
<note confidence="0.9744155">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 829–838,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997438" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999990464285714">
To reduce topic mismatch in LM estimation, we
(2006) have previously assigned topic labels to each
word by applying HMM-LDA (Griffiths et al., 2005)
to the training documents. Using an ad hoc method
to reduce the effective counts of n-grams ending
on topic words, we achieved better perplexity and
WER than standard trigram LMs. Intuitively, de-
emphasizing such n-grams will lower the transition
probability to out-of-domain topic words from the
training data. In this work, we further explore this
intuition with a principled feature-based model, in-
tegrated with LM smoothing and estimation to allow
simultaneous optimization of all model parameters.
As Gao and Lee (2000) observed, even purported
matched training data may exhibit topic, style, or
temporal biases not present in the test set. To ad-
dress the mismatch, they partition the training doc-
uments by their metadata attributes and compute a
measure of the likelihood that an n-gram will appear
in a new partitioned segment. By pruning n-grams
with generality probability below a given threshold,
the resulting model achieves lower perplexity than a
count-cutoff model of equal size. Complementary to
our work, this technique also utilizes segmentation
and metadata information. However, our model en-
ables the simultaneous use of all metadata attributes
by combining features derived from different parti-
tions of the training documents.
</bodyText>
<sectionHeader confidence="0.998532" genericHeader="method">
3 N-gram Weighting
</sectionHeader>
<bodyText confidence="0.99997024137931">
Given a limited amount of training data, an n-gram
appearing frequently in a single document may be
assigned a disproportionately high probability. For
example, an LM trained from lecture transcripts
tends to assign excessive probability to words from
observed lecture topics due to insufficient coverage
of the underlying document topics. On the other
hand, excessive probabilities may also be assigned
to n-grams appearing consistently across documents
with mismatched style, such as the course textbook
in the written style. Traditional n-gram smoothing
techniques do not address such issues of insufficient
topic coverage and style mismatch.
One approach to addressing the above issues is
to weight the counts of the n-grams according to
the concentration of their document distributions.
Assigning higher weights to n-grams with evenly
spread distributions captures the style of a data set,
as reflected across all documents. On the other hand,
emphasizing the n-grams concentrated within a few
documents focuses the model on the topics of the
individual documents.
In theory, n-gram weighting can be applied to any
smoothing algorithm based on counts. However,
because many of these algorithms assume integer
counts, we will apply the weighting factors to the
smoothed counts, instead. For modified Kneser-Ney
smoothing (Chen and Goodman, 1998), applying n-
gram weighting yields:
</bodyText>
<equation confidence="0.997217">
p(w|h) = Q(hw)c&apos;(hw) + α(h)p(w |h&apos;)
E. Q(hw)c(hw)
</equation>
<bodyText confidence="0.999822290322581">
where p(w|h) is the probability of word w given his-
tory h, c� is the adjusted Kneser-Ney count, c&apos; is the
discounted count, Q is the n-gram weighting factor,
α is the normalizing backoff weight, and h&apos; is the
backoff history.
Although the weighting factor Q can in general be
any function of the n-gram, in this work, we will
consider a log-linear combination of n-gram fea-
tures, or Q(hw) = exp(4)(hw) · 0), where 4)(hw)
is the feature vector for n-gram hw and 0 specifies
the parameter vector to be learned. To better fit the
data, we allow independent parameter vectors 0, for
each n-gram order o. Note that with Q(hw) = 1, the
model degenerates to the original modified Kneser-
Ney formulation. Furthermore, Q only specifies the
relative weighting among n-grams with a common
history h. Thus, scaling Q(hw) by an arbitrary func-
tion g(h) has no effect on the model.
In isolation, n-gram weighting shifts probability
mass from out-of-domain n-grams via backoff to
the uniform distribution to improve the generality
of the resulting model. However, in combination
with LM interpolation, it can also distribute prob-
abilities to LM components that better model spe-
cific n-grams. For example, n-gram weighting can
de-emphasize off-topic and off-style n-grams from
general lectures and course textbook, respectively.
Tuning the weighting and interpolation parameters
jointly further allows the estimation of the n-gram
probabilities to utilize the best matching LM com-
ponents.
</bodyText>
<page confidence="0.989428">
830
</page>
<figure confidence="0.982672875">
k means
emf
of the
i think
a lot of
big o of
the sun
this is a
</figure>
<subsectionHeader confidence="0.981933">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999985789473684">
To address the issue of sparsity in the document
topic distribution, we can apply n-gram weight-
ing with features that measure the concentration of
the n-gram distribution across documents. Simi-
lar features can also be computed from documents
partitioned by their categorical metadata attributes,
such as course and speaker for lecture transcripts.
Whereas the features derived from the corpus docu-
ments should correlate with the topic specificity of
the n-grams, the same features computed from the
speaker partitions might correspond to the speaker
specificity. By combining features from multiple
partitions of the training data to compute the weight-
ing factors, n-gram weighting allows the resulting
model to better generalize across categories.
To guide the presentation of the n-gram features
below, we will consider the following example parti-
tion of the training corpus. Words tagged by HMM-
LDA as topic words appear in bold.
</bodyText>
<sectionHeader confidence="0.999508666666667" genericHeader="method">
ABA ACCA BAB
BAAC CBA ABA
ACB AAC ABBA
</sectionHeader>
<bodyText confidence="0.977411461538461">
One way to estimate the specificity of an n-gram
across partitions is to measure the n-gram frequency
f, or the fraction of partitions containing an n-gram.
For instance, f(A) = 3/3, f(C) = 2/3. However,
as the size of each partition increases, this ratio in-
creases to 1, since most n-grams have a non-zero
probability of appearing in each partition. Thus,
an alternative is to compute the normalized entropy
of the n-gram distribution across the S partitions,
s �3 i p(s) logp(s), where p(s) is the
or h = log
fraction of an n-gram appearing in partition s. For
example, the normalized entropy of the unigram C is
</bodyText>
<equation confidence="0.999423">
h(C) = −1
log 3[2 6 log 2 6 + 4 6 log 4 6 +0] = .58. N-grams
</equation>
<bodyText confidence="0.999558">
clustered in fewer partitions have lower entropy than
ones that are more evenly spread out.
Following (Hsu and Glass, 2006), we also con-
sider features derived from the HMM-LDA word
topic labels.1 Specifically, we compute the empir-
ical probability t that the target word of the n-gram
</bodyText>
<footnote confidence="0.59139025">
1HMM-LDA is performed using 20 states and 50 topics with
a 3rd-order HMM. Hyperparameters are sampled with a log-
normal Metropolis proposal. The model with the highest likeli-
hood from among 10,000 iterations of Gibbs sampling is used.
</footnote>
<table confidence="0.971068416666667">
Feature
Random 0.03 0.32 0.33 0.19 0.53 0.24 0.37 0.80
log(c) 9.29 8.09 3.47 5.86 6.82 7.16 3.09 4.92
fdoc 1.00 0.93 0.00 0.18 0.92 0.76 0.00 0.04
fcourse 1.00 1.00 0.06 0.56 0.94 0.94 0.06 0.06
fspeaker 0.83 0.70 0.00 0.06 0.41 0.55 0.01 0.00
hdoc 0.96 0.84 0.00 0.56 0.93 0.85 0.00 0.34
hcourse 0.75 0.61 0.00 0.55 0.78 0.65 0.00 0.00
hspeaker 0.76 0.81 0.00 0.09 0.65 0.80 0.12 0.00
tdoc 0.00 0.00 0.91 1.00 0.01 0.00 0.00 0.04
tcourse 0.00 0.00 0.88 0.28 0.01 0.00 0.00 1.00
tspeaker 0.00 0.00 0.94 0.92 0.01 0.00 0.09 0.99
</table>
<tableCaption confidence="0.988871">
Table 1: A list of n-gram weighting features. f: n-gram
frequency, h: normalized entropy, t: topic probability.
</tableCaption>
<equation confidence="0.8679865">
is labeled as a topic word. In the example corpus,
t(C) = 3/6, t(A C) = 2/4.
</equation>
<bodyText confidence="0.999932444444444">
All of the above features can be computed for any
partitioning of the training data. To better illustrate
the differences, we compute the features on a set of
lecture transcripts (see Section 4.1) partitioned by
lecture (doc), course, and speaker. Furthermore, we
include the log of the n-gram counts c and random
values between 0 and 1 as baseline features. Table 1
lists all the features examined in this work and their
values on a select subset of n-grams.
</bodyText>
<subsectionHeader confidence="0.999082">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.9999555">
To tune the n-gram weighting parameters 0, we ap-
ply Powell’s method (Press et al., 2007) to numeri-
cally minimize the development set perplexity (Hsu
and Glass, 2008). Although there is no guarantee
against converging to a local minimum when jointly
tuning both the n-gram weighting and interpolation
parameters, we have found that initializing the pa-
rameters to zero generally yields good performance.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.980802">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.9999935">
In this work, we evaluate the perplexity and WER of
various trigram LMs trained with n-gram weighting
on a lecture transcription task (Glass et al., 2007).
The target data consists of 20 lectures from an in-
troductory computer science course, from which we
withhold the first 10 lectures for the development
</bodyText>
<page confidence="0.994118">
831
</page>
<table confidence="0.999201">
Dataset # Words # Sents # Docs
Textbook 131,280 6,762 271
Lectures 1,994,225 128,895 230
Switchboard 3,162,544 262,744 4,876
CS Dev 93,353 4,126 10
CS Test 87,527 3,611 10
</table>
<tableCaption confidence="0.976641">
Table 2: Summary of evaluation corpora.
</tableCaption>
<table confidence="0.999883142857143">
Model Perplexity WER
Dev Test Dev Test
FixKN(1) 174.7 196.7 34.9% 36.8%
+ W(hdoc) 172.9 194.8 34.7% 36.7%
FixKN(3) 168.6 189.3 34.9% 36.9%
+ W(hdoc) 166.8 187.8 34.6% 36.6%
FixKN(10) 167.5 187.6 35.0% 37.2%
+ W(hdoc) 165.3 185.8 34.7% 36.8%
KN(1) 169.7 190.4 35.0% 37.0%
+ W(hdoc) 167.3 188.2 34.8% 36.7%
KN(3) 163.4 183.1 35.0% 37.1%
+ W(hdoc) 161.1 181.2 34.7% 36.8%
KN(10) 162.3 181.8 35.1% 37.1%
+ W(hdoc) 160.1 180.0 34.8% 36.8%
</table>
<tableCaption confidence="0.97845825">
Table 3: Performance of n-gram weighting with a variety
of Kneser-Ney settings. FixKN(d): Kneser-Ney with d
fixed discount parameters. KN(d): FixKN(d) with tuned
values. W(feat): n-gram weighting with feat feature.
</tableCaption>
<bodyText confidence="0.999323473684211">
set (CS Dev) and use the last 10 for the test set
(CS Test). For training, we will consider the course
textbook with topic-specific vocabulary (Textbook),
numerous high-fidelity transcripts from a variety of
general seminars and lectures (Lectures), and the
out-of-domain LDC Switchboard corpus of spon-
taneous conversational speech (Switchboard) (God-
frey and Holliman, 1993). Table 2 summarizes all
the evaluation data.
To compute the word error rate, we use a speaker-
independent speech recognizer (Glass, 2003) with a
large-margin discriminative acoustic model (Chang,
2008). The lectures are pre-segmented into utter-
ances via forced alignment against the reference
transcripts (Hazen, 2006). Since all the models con-
sidered in this work can be encoded as n-gram back-
off models, they are applied directly during the first
recognition pass instead of through a subsequent n-
best rescoring step.
</bodyText>
<table confidence="0.999024285714286">
Model Perplexity WER
Lectures 189.3 36.9%
+ W(hdoc) 187.8 (-0.8%) 36.6%
Textbook 326.1 43.1%
+ W(hdoc) 317.5 (-2.6%) 43.1%
LI(Lectures + Textbook) 141.6 33.7%
+ W(hdoc) 136.6 (-3.5%) 32.7%
</table>
<tableCaption confidence="0.999706">
Table 4: N-gram weighting with linear interpolation.
</tableCaption>
<subsectionHeader confidence="0.994192">
4.2 Smoothing
</subsectionHeader>
<bodyText confidence="0.999978">
In Table 3, we compare the performance of n-gram
weighting with the hdoc document entropy feature
for various modified Kneser-Ney smoothing config-
urations (Chen and Goodman, 1998) on the Lec-
tures dataset. Specifically, we considered varying
the number of discount parameters per n-gram order
from 1 to 10. The original and modified Kneser-Ney
smoothing algorithms correspond to a setting of 1
and 3, respectively. Furthermore, we explored using
both fixed parameter values estimated from n-gram
count statistics and tuned values that minimize the
development set perplexity.
In this task, while the test set perplexity tracks
the development set perplexity well, the WER corre-
lates surprisingly poorly with the perplexity on both
the development and test sets. Nevertheless, n-gram
weighting consistently reduces the absolute test set
WER by a statistically significant average of 0.3%,
according to the Matched Pairs Sentence Segment
Word Error test (Pallet et al., 1990). Given that we
obtained the lowest development set WER with the
fixed 3-parameter modified Kneser-Ney smoothing,
all subsequent experiments are conducted using this
smoothing configuration.
</bodyText>
<subsectionHeader confidence="0.99221">
4.3 Linear Interpolation
</subsectionHeader>
<bodyText confidence="0.999755636363636">
Applied to the Lectures dataset in isolation, n-gram
weighting with the hdoc feature reduces the test set
WER by 0.3% by de-emphasizing the probability
contributions from off-topic n-grams and shifting
their weights to the backoff distributions. Ideally
though, such weights should be distributed to on-
topic n-grams, perhaps from other LM components.
In Table 4, we present the performance of apply-
ing n-gram weighting to the Lectures and Textbook
models individually versus in combination via linear
interpolation (LI), where we optimize the n-gram
</bodyText>
<page confidence="0.978622">
832
</page>
<table confidence="0.999896153846154">
Model Perplexity WER
LI(Lectures + Textbook) 141.6 33.7%
+W(Random) 141.5 (-0.0 %) 33.7%
+ W(log(c)) 137.5 (-2.9 %) 32.8%
+ W(fdoc) 136.3 (-3.7 %) 32.8%
+ W(fcourse) 136.5 (-3.6 %) 32.7%
+ W(fspeaker) 138.1 (-2.5 %) 33.0%
+ W(hdoc) 136.6 (-3.5 %) 32.7%
+ W(hcourse) ,t 136.1 (-3.9 %) 32.7%
+ W(hspeaker) 138.6 (-2.1 %) 33.1%
+ W(tdoc) 134.8 (-4.8 %) 33.2%
+ W(tcourse) 136.4 (-3.6 %) 33.1%
+ W(tspeaker) 136.4 (-3.7 %) 33.2%
</table>
<tableCaption confidence="0.999691">
Table 5: N-gram weighting with various features.
</tableCaption>
<bodyText confidence="0.9992873">
weighting and interpolation parameters jointly. The
interpolated model with n-gram weighting achieves
perplexity improvements roughly additive of the re-
ductions obtained with the individual models. How-
ever, the 1.0% WER drop for the interpolated model
significantly exceeds the sum of the individual re-
ductions. Thus, as we will examine in more detail
in Section 5.1, n-gram weighting allows probabili-
ties to be shifted from less relevant n-grams in one
component to more specific n-grams in another.
</bodyText>
<subsectionHeader confidence="0.873575">
4.4 Features
</subsectionHeader>
<bodyText confidence="0.999844529411765">
With n-gram weighting, we can model the weight-
ing function Q(hw) as a log-linear combination of
any n-gram features. In Table 5, we show the effect
various features have on the performance of linearly
interpolating Lectures and Textbook. As the docu-
ments from the Lectures dataset is annotated with
course and speaker metadata attributes, we include
the n-gram frequency f, entropy h, and topic proba-
bility t features computed from the lectures grouped
by the 16 unique courses and 299 unique speakers.2
In terms of perplexity, the use of the Random
feature has negligible impact on the test set per-
formance, as expected. On the other hand, the
log(c) count feature reduces the perplexity by nearly
3%, as it correlates with the generality of the n-
grams. By using features that leverage the infor-
mation from document segmentation and associated
</bodyText>
<tableCaption confidence="0.836056">
2Features that are not applicable to a particular corpus (e.g.
hcourse for Textbook) are removed from the n-gram weighting
computation for that component. Thus, models with course and
speaker features have fewer tunable parameters than the others.
</tableCaption>
<bodyText confidence="0.999987529411765">
metadata, we are generally able to achieve further
perplexity reductions. Overall, the frequency and
entropy features perform roughly equally. However,
by considering information from the more sophisti-
cated HMM-LDA topic model, the topic probability
feature tdoc achieves significantly lower perplexity
than any other feature in isolation.
In terms of WER, the Random feature again
shows no effect on the baseline WER of 33.7%.
However, to our surprise, the use of the simple
log(c) feature achieves nearly the same WER im-
provement as the best segmentation-based feature,
whereas the more sophisticated features computed
from HMM-LDA labels only obtain half of the re-
duction even though they have the best perplexities.
When comparing the performance of different n-
gram weighting features on this data set, the per-
plexity correlates poorly with the WER, on both the
development and test sets. Fortunately, the features
that yield the lowest perplexity and WER on the de-
velopment set also yield one of the lowest perplex-
ities and WERs, respectively, on the test set. Thus,
during feature selection for speech recognition ap-
plications, we should consider the development set
WER. Specifically, since the differences in WER
are often statistically insignificant, we will select the
feature that minimizes the sum of the development
set WER and log perplexity, or cross-entropy.3
In Tables 5 and 6, we have underlined the per-
plexities and WERs of the features with the lowest
corresponding development set values (not shown)
and bolded the lowest test set values. The features
that achieve the lowest combined cross-entropy and
WER on the development set are starred.
</bodyText>
<subsectionHeader confidence="0.986048">
4.5 Feature Combination
</subsectionHeader>
<bodyText confidence="0.999990222222222">
Unlike most previous work, n-gram weighting en-
ables a systematic integration of features computed
from multiple document partitions. In Table 6, we
compare the performance of various feature combi-
nations. We experiment with incrementally adding
features that yield the lowest combined development
set cross-entropy and WER. Overall, this metric ap-
pears to better predict the test set WER than either
the development set perplexity or WER alone.
</bodyText>
<footnote confidence="0.759384666666667">
3The choice of cross-entropy instead of perplexity is par-
tially motivated by the linear correlation reported by (Chen and
Goodman, 1998) between cross-entropy and WER.
</footnote>
<page confidence="0.996393">
833
</page>
<table confidence="0.99598035">
Features Perplexity WER
hcourse 136.1 (-0.5%) 32.7%
+ log(c) 135.4 (-0.7%) 32.6%
+ fdoc 135.1 (-0.5%) 32.6%
+ hdoc 135.6 (-2.1%) 32.6%
+ tdoc * 133.2 32.6%
+ fcourse 136.0 (-0.1%) 32.6%
+ tcourse 134.8 (-1.0 %) 32.9%
+ fspeaker 136.0 (-0.1 %) 32.6%
+ hspeaker 136.1 (-0.0 %) 32.8%
+ tspeaker 134.7 (-1.0 %) 32.7%
hcourse + tdoc 133.2 32.6%
+ log(c) 132.8 (-0.3 %) 32.5%
+ fdoc * 132.8 (-0.4 %) 32.5%
+ hdoc 133.0 (-0.2 %) 32.5%
+ fcourse 133.1 (-0.1 %) 32.5%
+tcourse 133.0 (-0.1 %) 32.6%
+ fspeaker 133.1 (-0.1 %) 32.5%
+ hspeaker 133.2 (-0.0 %) 32.6%
+tspeaker 133.1 (-0.1 %) 32.7%
</table>
<bodyText confidence="0.998060888888889">
Using the combined feature selection technique,
we notice that the greedily selected features tend to
differ in the choice of document segmentation and
feature type, suggesting that n-gram weighting can
effectively integrate the information provided by the
document metadata. By combining features, we are
able to further reduce the test set WER by a statis-
tically significant (p &lt; 0.001) 0.2% over the best
single feature model.
</bodyText>
<subsectionHeader confidence="0.973885">
4.6 Advanced Interpolation
</subsectionHeader>
<bodyText confidence="0.999165928571429">
While n-gram weighting with all three features is
able to reduce the test set WER by 1.2% over the
linear interpolation baseline, linear interpolation is
not a particularly effective interpolation technique.
In Table 7, we compare the effectiveness of n-gram
weighting in combination with better interpolation
techniques, such as count merging (CM) (Bacchi-
ani et al., 2006) and generalized linear interpolation
(GLI) (Hsu, 2007). As expected, the use of more
sophisticated interpolation techniques decreases the
perplexity and WER reductions achieved by n-gram
weighting by roughly half for a variety of feature
combinations. However, all improvements remain
statistically significant.
</bodyText>
<table confidence="0.999545615384615">
Model Perplexity WER
Linear(L + T) 141.6 33.7%
+ W(hcourse) 136.1 (-3.9 %) 32.7%
+ W(tdoc) 133.2 (-5.9 %) 32.6%
+ W(fdoc) 132.8 (-6.2 %) 32.5%
CM(L + T) 137.9 33.0%
+ W(hcourse) 135.5 (-1.8 %) 32.4%
+ W(tdoc) 133.4 (-3.3 %) 32.4%
+ W(fdoc) 133.2 (-3.5 %) 32.4%
GLIlog(l+Z)(L + T) 135.9 33.0%
+ W(hcourse) 133.0 (-2.2 %) 32.4%
+ W(tdoc) 130.6 (-3.9 %) 32.4%
+ W(fdoc) 130.5 (-4.2 %) 32.4%
</table>
<tableCaption confidence="0.853811">
Table 7: Effect of interpolation technique. L: Lectures, T:
Textbook.
</tableCaption>
<table confidence="0.962175">
Feature Parameter Values
hdoc θL = [3.42,1.46, 0.12]
θT = [−0.45, −0.35, −0.73]
[AL, AT] = [0.67, 0.33]
</table>
<tableCaption confidence="0.990853333333333">
Table 8: N-gram weighting parameter values. θL, θT:
parameters for each order of the Lectures and Textbook
trigram models, AL,AT: linear interpolation weights.
</tableCaption>
<bodyText confidence="0.9999613">
Although the WER reductions from better inter-
polation techniques are initially statistically signif-
icant, as we add features to n-gram weighting, the
differences among the interpolation methods shrink
significantly. With all three features combined, the
test set WER difference between linear interpolation
and generalized linear interpolation loses its statisti-
cal significance. In fact, we can obtain statistically
the same WER of 32.4% using the simpler model of
count merging and n-gram weighting with hcourse.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="conclusions">
5 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999299">
5.1 Weighting Parameters
</subsectionHeader>
<bodyText confidence="0.999954142857143">
To obtain further insight into how n-gram weighting
improves the resulting n-gram model, we present in
Table 8 the optimized parameter values for the linear
interpolation model between Lectures and Textbook
using n-gram weighting with hdoc and tdoc features.
Using Q(hw) = exp(4)(hw) · θ) to model the n-
gram weights, a positive value of BZ corresponds to
</bodyText>
<tableCaption confidence="0.597408">
Table 6: N-gram weighting with feature combinations. tdoc θL = [−2.33, −1.63, −1.19]
</tableCaption>
<table confidence="0.5405235">
θT = [1.05, 0.46, 0.12]
[AL, AT] = [0.68, 0.32]
</table>
<page confidence="0.981516">
834
</page>
<figureCaption confidence="0.998699">
Figure 1: Test set perplexity vs. development set size.
Figure 2: Test set WER vs. development set size.
</figureCaption>
<figure confidence="0.98975675">
144
LI
CM
GLI
LI+W
CM+W
GLI+W
140
138
136
134
132100 300 1000 3000 10000
Development Set Size (Words)
142
35.0
LI
CM
GLI
LI+W
CM+W
GLI+W
34.0
33.5
33.0
32.5
32.0100 300 1000 3000 10000
Development Set Size (Words)
34.5
</figure>
<bodyText confidence="0.968562695652174">
increasing the weights of the Oh order n-grams with
positive feature values.
For the hdoc normalized entropy feature, values
WE
close to 1 correspond to n-grams that are evenly dis-
tributed across the documents. When interpolating
Lectures and Textbook, we obtain consistently pos-
itive values for the Lectures component, indicating
a de-emphasis on document-specific terms that are
unlikely to be found in the target computer science
domain. On the other hand, the values correspond-
ing to the Textbook component are consistently neg-
ative, suggesting a reduced weight for mismatched
style terms that appear uniformly across textbook
sections.
For tdoc, values close to 1 correspond to n-grams
ending frequently on topic words with uneven dis-
tribution across documents. Thus, as expected, the
signs of the optimized parameter values are flipped.
By de-emphasizing topic n-grams from off-topic
components and style n-grams from off-style com-
ponents, n-gram weighting effectively improves the
performance of the resulting language model.
</bodyText>
<subsectionHeader confidence="0.999815">
5.2 Development Set Size
</subsectionHeader>
<bodyText confidence="0.999981838709677">
So far, we have assumed the availability of a large
development set for parameter tuning. To obtain
a sense of how n-gram weighting performs with
smaller development sets, we randomly select utter-
ances from the full development set and plot the test
set perplexity in Figure 1 as a function of the devel-
opment set size for various modeling techniques.
As expected, GLI outperforms both LI and CM.
However, whereas LI and CM essentially converge
in test set perplexity with only 100 words of devel-
opment data, it takes about 500 words before GLI
converges due to the increased number of parame-
ters. By adding n-gram weighting with the hcourse
feature, we see a significant drop in perplexity for
all models at all development set sizes. However,
the performance does not fully converge until 3,000
words of development set data.
As shown in Figure 2, the test set WER behaves
more erratically, as the parameters are tuned to min-
imize the development set perplexity. Overall, n-
gram weighting decreases the WER significantly,
except when applied to GLI with less than 1000
words of development data when the perplexity of
GLI has not itself converged. In that range, CM with
n-gram weighting performs the best. However, with
more development data, GLI with n-gram weight-
ing generally performs slightly better. From these
results, we conclude that although n-gram weight-
ing increases the number of tuning parameters, they
are effective in improving the test set performance
even with only 100 words of development set data.
</bodyText>
<subsectionHeader confidence="0.999721">
5.3 Training Set Size
</subsectionHeader>
<bodyText confidence="0.998558818181818">
To characterize the effectiveness of n-gram weight-
ing as a function of the training set size, we evalu-
ate the performance of various interpolated models
with increasing subsets of the Lectures corpus and
the full Textbook corpus. Overall, every doubling of
the number of training set documents decreases both
the test set perplexity and WER by approximately 7
points and 0.8%, respectively. To better compare re-
sults, we plot the performance difference between
various models and linear interpolation in Figures 3
and 4.
</bodyText>
<page confidence="0.993872">
835
</page>
<figure confidence="0.993678580645161">
4
2
0
-2
-4
-6
-8
-10
-122 4 8 16 32 64 128 230
Training Set Size (Documents)
LI
CM
GLI
LI+W
CM+W
GLI+W
-0.4
-0.6
-0.8
-1.0
-1.2
-1.42 4 8 16 32 64 128 230
Training Set Size (Documents)
LI
CM
GLI
LI+W
CM+W
GLI+W
0.0
-0.2
</figure>
<figureCaption confidence="0.999887">
Figure 3: Test set perplexity vs. training set size.
</figureCaption>
<bodyText confidence="0.999981285714286">
Interestingly, the peak gain obtained from n-gram
weighting with the hdoc feature appears at around
16 documents for all interpolation techniques. We
suspect that as the number of documents initially
increases, the estimation of the hdoc features im-
proves, resulting in larger perplexity reduction from
n-gram weighting. However, as the diversity of the
training set documents increases beyond a certain
threshold, we experience less document-level spar-
sity. Thus, we see decreasing gain from n-gram
weighting beyond 16 documents.
For all interpolation techniques, even though the
perplexity improvements from n-gram weighting
decrease with more documents, the WER reductions
actually increase. N-gram weighting showed sta-
tistically significant reductions for all configurations
except generalized linear interpolation with less than
8 documents. Although count merging with n-gram
weighting has the lowest WER for most training set
sizes, GLI ultimately achieves the best test set WER
with the full training set.
</bodyText>
<subsectionHeader confidence="0.988208">
5.4 Training Corpora
</subsectionHeader>
<bodyText confidence="0.974494833333334">
In Table 9, we compare the performance of n-gram
weighting with different combination of training
corpora and interpolation techniques to determine
its effectiveness across different training conditions.
With the exception of interpolating Lectures and
Switchboard using count merging, all other model
combinations yield statistically significant improve-
ments with n-gram weighting using hcourse,tdoc,
an
d fdoc features.
The results suggest that n-gram weighting with
these features is most effective when interpolating
</bodyText>
<figureCaption confidence="0.987034">
Figure 4: Test set WER vs. training set size.
</figureCaption>
<table confidence="0.985071142857143">
Model L + T L + S T + S L + T + S
LI 33.7% 36.7% 36.4% 33.6%
LI + W 32.5% 36.4% 35.7% 32.5%
CM 33.0% 36.6% 35.5% 32.9%
CM + W 32.4% 36.5% 35.4% 32.3%
GLI 33.0% 36.6% 35.7% 32.8%
GLI + W 32.4% 36.4% 35.3% 32.2%
</table>
<tableCaption confidence="0.740738333333333">
Table 9: Test set WER with vari ous training corpus com-
binations. L: Lectures, T: Textbook, S: Switchboard, W:
n-gram weighting.
</tableCaption>
<figure confidence="0.8564595">
C
0.2
</figure>
<bodyText confidence="0.9816275">
corpora that differ in how they match the target do-
main. Whereas the Textbook corpus is the only cor-
pus with matching topic, both Lectures and Switch-
board have a similar matching spoken conversa-
tional style. Thus, we see the least benefit from
n-gram weighting when interpolating Lectures and
Switchboard. By combining Lectures, Textbook,
and Switchboard using generalized linear interpola-
tion with n-gram weighting using hcourse, tdoc, and
fdoc features, we achieve our best test set WER of
32.2% on the lecture transcription task, a full 1.5%
over the initial linear interpolation baseli
In this work, we presented the n-gram weighting
technique for adjusting the probabilities of n-grams
according to a set of features. By utilizing features
derived from the document segmentation and asso-
ciated metadata inherent in many training corpora,
we achieved up to a 1.2% and 0.6% WER reduc-
tion over the linear interpolation and count merging
baselines, respectively, using n-gram weighting on
</bodyText>
<figure confidence="0.9769288">
ne.
Work
6 Conclusion &amp; Future
ption task.
a lecture transcri
</figure>
<page confidence="0.995653">
836
</page>
<bodyText confidence="0.999935024390244">
We examined the performance of various n-gram
weighting features and generally found entropy-
based features to offer the best predictive perfor-
mance. Although the topic probability features
derived from HMM-LDA labels yield additional
improvements when applied in combination with
the normalized entropy features, the computational
cost of performing HMM-LDA may not justify the
marginal benefit in all scenarios.
In situations where the document boundaries are
unavailable or when finer segmentation is desired,
automatic techniques for document segmentation
may be applied (Malioutov and Barzilay, 2006).
Synthetic metadata information may also be ob-
tained via clustering techniques (Steinbach et al.,
2000). Although we have primarily focused on n-
gram weighting features derived from segmentation
information, it is also possible to consider other fea-
tures that correlate with n-gram relevance.
N-gram weighting and other approaches to cross-
domain language modeling require a matched devel-
opment set for model parameter tuning. Thus, for
future work, we plan to investigate the use of the ini-
tial recognition hypotheses as the development set,
as well as manually transcribing a subset of the test
set utterances.
As speech and natural language applications shift
towards novel domains with limited matched train-
ing data, better techniques are needed to maximally
utilize the often abundant partially matched data. In
this work, we examined the effectiveness of the n-
gram weighting technique for estimating language
models in these situations. With similar investments
in acoustic modeling and other areas of natural lan-
guage processing, we look forward to an ever in-
creasing diversity of practical speech and natural
language applications.
Availability An implementation of the n-gram
weighting algorithm is available in the MIT Lan-
guage Modeling (MITLM) toolkit (Hsu and Glass,
2008): http://www.sls.csail.mit.edu/mitlm/.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99964425">
We would like to thank the anonymous reviewers for
their constructive feedback. This research is sup-
ported in part by the T-Party Project, a joint research
program between MIT and Quanta Computer Inc.
</bodyText>
<sectionHeader confidence="0.996433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999966705882353">
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech &amp; Language, 20(1):41–
68.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: Review and perspectives. Speech Commu-
nication, 42(1):93–108.
Hung-An Chang. 2008. Large margin Gaussian mix-
ture modeling for automatic speech recognition. Mas-
sachusetts Institute of Technology. Masters Thesis.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. In Technical Report TR-10-98. Computer Science
Group, Harvard University.
Jianfeng Gao and Kai-Fu Lee. 2000. Distribution-based
pruning of backoff language models. In Proc. Asso-
ciation of Computational Linguistics, pages 579–588,
Hong Kong, China.
James Glass, Timothy J. Hazen, Lee Hetherington, and
Chao Wang. 2004. Analysis and processing of lecture
audio data: Preliminary investigations. In Proc. HLT-
NAACL Workshop on Interdisciplinary Approaches to
Speech Indexing and Retrieval, pages 9–12, Boston,
MA, USA.
James Glass, Timothy J. Hazen, Scott Cyphers, Igor
Malioutov, David Huynh, and Regina Barzilay. 2007.
Recent progress in the MIT spoken lecture process-
ing project. In Proc. Interspeech, pages 2553–2556,
Antwerp, Belgium.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
&amp; Language, 17(2-3):137–152.
John J. Godfrey and Ed Holliman. 1993. Switchboard-1
transcripts. Linguistic Data Consortium, Philadelphia,
PA, USA.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in Neural Information Processing
Systems 17, pages 537–544. MIT Press, Cambridge,
MA, USA.
T.J. Hazen. 2006. Automatic alignment and error cor-
rection of human generated transcripts for long speech
recordings. In Proc. Interspeech, Pittsburgh, PA,
USA.
Bo-June (Paul) Hsu and James Glass. 2006. Style &amp;
topic language model adaptation using HMM-LDA.
In Proc. Empirical Methods in Natural Language Pro-
cessing, pages 373–381, Sydney, Australia.
Bo-June (Paul) Hsu and James Glass. 2008. Iterative
language model estimation: Efficient data structure &amp;
algorithms. In Proc. Interspeech, Brisbane, Australia.
</reference>
<page confidence="0.977696">
837
</page>
<reference confidence="0.999824944444444">
Bo-June (Paul) Hsu. 2007. Generalized linear interpola-
tion of language models. In Proc. Automatic Speech
Recognition and Understanding, pages 136–140, Ky-
oto, Japan.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Association for Computational Linguistics, pages 25–
32, Sydney, Australia.
D. Pallet, W. Fisher, and Fiscus. 1990. Tools for the anal-
ysis of benchmark speech recognition tests. In Proc.
ICASSP, Albuquerque, NM, USA.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes.
Cambridge University Press, 3rd edition.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. Technical Report #00-034, University of Min-
nesota.
</reference>
<page confidence="0.997613">
838
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984912">
<title confidence="0.996521">Weighting: Reducing Training Data in Cross-Domain Language Model Estimation</title>
<author confidence="0.995111">Bo-June Hsu</author>
<author confidence="0.995111">James</author>
<affiliation confidence="0.999594">MIT Computer Science and Artificial Intelligence</affiliation>
<address confidence="0.998523">32 Vassar Street, Cambridge, MA, 02139</address>
<abstract confidence="0.9998880625">In domains with insufficient matched training data, language models are often constructed by interpolating component models trained partially matched corpora. Since the grams from such corpora may not be of equal relevance to the target domain, we propose weighting technique to adjust the probabilities based on features derived from readily available segmentation and metadata information for each corpus. Using a log-linear combination of such features, the resulting model achieves up to a 1.2% absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>68</pages>
<contexts>
<context position="21646" citStr="Bacchiani et al., 2006" startWordPosition="3425" endWordPosition="3429">ctively integrate the information provided by the document metadata. By combining features, we are able to further reduce the test set WER by a statistically significant (p &lt; 0.001) 0.2% over the best single feature model. 4.6 Advanced Interpolation While n-gram weighting with all three features is able to reduce the test set WER by 1.2% over the linear interpolation baseline, linear interpolation is not a particularly effective interpolation technique. In Table 7, we compare the effectiveness of n-gram weighting in combination with better interpolation techniques, such as count merging (CM) (Bacchiani et al., 2006) and generalized linear interpolation (GLI) (Hsu, 2007). As expected, the use of more sophisticated interpolation techniques decreases the perplexity and WER reductions achieved by n-gram weighting by roughly half for a variety of feature combinations. However, all improvements remain statistically significant. Model Perplexity WER Linear(L + T) 141.6 33.7% + W(hcourse) 136.1 (-3.9 %) 32.7% + W(tdoc) 133.2 (-5.9 %) 32.6% + W(fdoc) 132.8 (-6.2 %) 32.5% CM(L + T) 137.9 33.0% + W(hcourse) 135.5 (-1.8 %) 32.4% + W(tdoc) 133.4 (-3.3 %) 32.4% + W(fdoc) 133.2 (-3.5 %) 32.4% GLIlog(l+Z)(L + T) 135.9 3</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech &amp; Language, 20(1):41– 68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: Review and perspectives.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="2014" citStr="Bellegarda, 2004" startWordPosition="302" endWordPosition="303"> such bias. We will examine this approach in the context of language modeling for lecture transcription. Compared with other types of audio data, lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with special terminologies (Glass et al., 2004). While we may have existing transcripts from general lectures or written text on the precise topic, training data that matches both the style and topic of the target lecture is often scarce. Thus, past research has investigated various adaptation and interpolation techniques to make use of partially matched corpora (Bellegarda, 2004). Training corpora are often segmented into documents with associated metadata, such as title, date, and speaker. For lectures, if the data contains even a few lectures on linear algebra, conventional language modeling methods that lump the documents together will tend to assign disproportionately high probability to frequent terms like vector and matrix. Can we utilize the segmentation and metadata information to reduce the biases resulting from training data mismatch? In this work, we present such a technique where we weight each n-gram count in a standard n-gram language model (LM) estimati</context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: Review and perspectives. Speech Communication, 42(1):93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hung-An Chang</author>
</authors>
<title>Large margin Gaussian mixture modeling for automatic speech recognition.</title>
<date>2008</date>
<institution>Massachusetts Institute of Technology. Masters Thesis.</institution>
<contexts>
<context position="13529" citStr="Chang, 2008" startWordPosition="2152" endWordPosition="2153"> n-gram weighting with feat feature. set (CS Dev) and use the last 10 for the test set (CS Test). For training, we will consider the course textbook with topic-specific vocabulary (Textbook), numerous high-fidelity transcripts from a variety of general seminars and lectures (Lectures), and the out-of-domain LDC Switchboard corpus of spontaneous conversational speech (Switchboard) (Godfrey and Holliman, 1993). Table 2 summarizes all the evaluation data. To compute the word error rate, we use a speakerindependent speech recognizer (Glass, 2003) with a large-margin discriminative acoustic model (Chang, 2008). The lectures are pre-segmented into utterances via forced alignment against the reference transcripts (Hazen, 2006). Since all the models considered in this work can be encoded as n-gram backoff models, they are applied directly during the first recognition pass instead of through a subsequent nbest rescoring step. Model Perplexity WER Lectures 189.3 36.9% + W(hdoc) 187.8 (-0.8%) 36.6% Textbook 326.1 43.1% + W(hdoc) 317.5 (-2.6%) 43.1% LI(Lectures + Textbook) 141.6 33.7% + W(hdoc) 136.6 (-3.5%) 32.7% Table 4: N-gram weighting with linear interpolation. 4.2 Smoothing In Table 3, we compare th</context>
</contexts>
<marker>Chang, 2008</marker>
<rawString>Hung-An Chang. 2008. Large margin Gaussian mixture modeling for automatic speech recognition. Massachusetts Institute of Technology. Masters Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. In</title>
<date>1998</date>
<tech>Technical Report TR-10-98.</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="6365" citStr="Chen and Goodman, 1998" startWordPosition="957" endWordPosition="960">s according to the concentration of their document distributions. Assigning higher weights to n-grams with evenly spread distributions captures the style of a data set, as reflected across all documents. On the other hand, emphasizing the n-grams concentrated within a few documents focuses the model on the topics of the individual documents. In theory, n-gram weighting can be applied to any smoothing algorithm based on counts. However, because many of these algorithms assume integer counts, we will apply the weighting factors to the smoothed counts, instead. For modified Kneser-Ney smoothing (Chen and Goodman, 1998), applying ngram weighting yields: p(w|h) = Q(hw)c&apos;(hw) + α(h)p(w |h&apos;) E. Q(hw)c(hw) where p(w|h) is the probability of word w given history h, c� is the adjusted Kneser-Ney count, c&apos; is the discounted count, Q is the n-gram weighting factor, α is the normalizing backoff weight, and h&apos; is the backoff history. Although the weighting factor Q can in general be any function of the n-gram, in this work, we will consider a log-linear combination of n-gram features, or Q(hw) = exp(4)(hw) · 0), where 4)(hw) is the feature vector for n-gram hw and 0 specifies the parameter vector to be learned. To bet</context>
<context position="14283" citStr="Chen and Goodman, 1998" startWordPosition="2266" endWordPosition="2269">e models considered in this work can be encoded as n-gram backoff models, they are applied directly during the first recognition pass instead of through a subsequent nbest rescoring step. Model Perplexity WER Lectures 189.3 36.9% + W(hdoc) 187.8 (-0.8%) 36.6% Textbook 326.1 43.1% + W(hdoc) 317.5 (-2.6%) 43.1% LI(Lectures + Textbook) 141.6 33.7% + W(hdoc) 136.6 (-3.5%) 32.7% Table 4: N-gram weighting with linear interpolation. 4.2 Smoothing In Table 3, we compare the performance of n-gram weighting with the hdoc document entropy feature for various modified Kneser-Ney smoothing configurations (Chen and Goodman, 1998) on the Lectures dataset. Specifically, we considered varying the number of discount parameters per n-gram order from 1 to 10. The original and modified Kneser-Ney smoothing algorithms correspond to a setting of 1 and 3, respectively. Furthermore, we explored using both fixed parameter values estimated from n-gram count statistics and tuned values that minimize the development set perplexity. In this task, while the test set perplexity tracks the development set perplexity well, the WER correlates surprisingly poorly with the perplexity on both the development and test sets. Nevertheless, n-gr</context>
<context position="20196" citStr="Chen and Goodman, 1998" startWordPosition="3186" endWordPosition="3189"> set are starred. 4.5 Feature Combination Unlike most previous work, n-gram weighting enables a systematic integration of features computed from multiple document partitions. In Table 6, we compare the performance of various feature combinations. We experiment with incrementally adding features that yield the lowest combined development set cross-entropy and WER. Overall, this metric appears to better predict the test set WER than either the development set perplexity or WER alone. 3The choice of cross-entropy instead of perplexity is partially motivated by the linear correlation reported by (Chen and Goodman, 1998) between cross-entropy and WER. 833 Features Perplexity WER hcourse 136.1 (-0.5%) 32.7% + log(c) 135.4 (-0.7%) 32.6% + fdoc 135.1 (-0.5%) 32.6% + hdoc 135.6 (-2.1%) 32.6% + tdoc * 133.2 32.6% + fcourse 136.0 (-0.1%) 32.6% + tcourse 134.8 (-1.0 %) 32.9% + fspeaker 136.0 (-0.1 %) 32.6% + hspeaker 136.1 (-0.0 %) 32.8% + tspeaker 134.7 (-1.0 %) 32.7% hcourse + tdoc 133.2 32.6% + log(c) 132.8 (-0.3 %) 32.5% + fdoc * 132.8 (-0.4 %) 32.5% + hdoc 133.0 (-0.2 %) 32.5% + fcourse 133.1 (-0.1 %) 32.5% +tcourse 133.0 (-0.1 %) 32.6% + fspeaker 133.1 (-0.1 %) 32.5% + hspeaker 133.2 (-0.0 %) 32.6% +tspeaker 1</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. In Technical Report TR-10-98. Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Kai-Fu Lee</author>
</authors>
<title>Distribution-based pruning of backoff language models.</title>
<date>2000</date>
<booktitle>In Proc. Association of Computational Linguistics,</booktitle>
<pages>579--588</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="4273" citStr="Gao and Lee (2000)" startWordPosition="642" endWordPosition="645">ave previously assigned topic labels to each word by applying HMM-LDA (Griffiths et al., 2005) to the training documents. Using an ad hoc method to reduce the effective counts of n-grams ending on topic words, we achieved better perplexity and WER than standard trigram LMs. Intuitively, deemphasizing such n-grams will lower the transition probability to out-of-domain topic words from the training data. In this work, we further explore this intuition with a principled feature-based model, integrated with LM smoothing and estimation to allow simultaneous optimization of all model parameters. As Gao and Lee (2000) observed, even purported matched training data may exhibit topic, style, or temporal biases not present in the test set. To address the mismatch, they partition the training documents by their metadata attributes and compute a measure of the likelihood that an n-gram will appear in a new partitioned segment. By pruning n-grams with generality probability below a given threshold, the resulting model achieves lower perplexity than a count-cutoff model of equal size. Complementary to our work, this technique also utilizes segmentation and metadata information. However, our model enables the simu</context>
</contexts>
<marker>Gao, Lee, 2000</marker>
<rawString>Jianfeng Gao and Kai-Fu Lee. 2000. Distribution-based pruning of backoff language models. In Proc. Association of Computational Linguistics, pages 579–588, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Glass</author>
<author>Timothy J Hazen</author>
<author>Lee Hetherington</author>
<author>Chao Wang</author>
</authors>
<title>Analysis and processing of lecture audio data: Preliminary investigations.</title>
<date>2004</date>
<booktitle>In Proc. HLTNAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval,</booktitle>
<pages>9--12</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="1678" citStr="Glass et al., 2004" startWordPosition="249" endWordPosition="252">s are often available in abundance. Past attempts to utilize the mismatched data for training often result in models that exhibit biases not observed in the target domain. In this work, we will investigate the use of the often readily available data segmentation and metadata attributes associated with each corpus to reduce the effect of such bias. We will examine this approach in the context of language modeling for lecture transcription. Compared with other types of audio data, lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with special terminologies (Glass et al., 2004). While we may have existing transcripts from general lectures or written text on the precise topic, training data that matches both the style and topic of the target lecture is often scarce. Thus, past research has investigated various adaptation and interpolation techniques to make use of partially matched corpora (Bellegarda, 2004). Training corpora are often segmented into documents with associated metadata, such as title, date, and speaker. For lectures, if the data contains even a few lectures on linear algebra, conventional language modeling methods that lump the documents together will</context>
</contexts>
<marker>Glass, Hazen, Hetherington, Wang, 2004</marker>
<rawString>James Glass, Timothy J. Hazen, Lee Hetherington, and Chao Wang. 2004. Analysis and processing of lecture audio data: Preliminary investigations. In Proc. HLTNAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval, pages 9–12, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Glass</author>
<author>Timothy J Hazen</author>
<author>Scott Cyphers</author>
<author>Igor Malioutov</author>
<author>David Huynh</author>
<author>Regina Barzilay</author>
</authors>
<title>Recent progress in the MIT spoken lecture processing project.</title>
<date>2007</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>2553--2556</pages>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="11936" citStr="Glass et al., 2007" startWordPosition="1901" endWordPosition="1904">select subset of n-grams. 3.2 Training To tune the n-gram weighting parameters 0, we apply Powell’s method (Press et al., 2007) to numerically minimize the development set perplexity (Hsu and Glass, 2008). Although there is no guarantee against converging to a local minimum when jointly tuning both the n-gram weighting and interpolation parameters, we have found that initializing the parameters to zero generally yields good performance. 4 Experiments 4.1 Setup In this work, we evaluate the perplexity and WER of various trigram LMs trained with n-gram weighting on a lecture transcription task (Glass et al., 2007). The target data consists of 20 lectures from an introductory computer science course, from which we withhold the first 10 lectures for the development 831 Dataset # Words # Sents # Docs Textbook 131,280 6,762 271 Lectures 1,994,225 128,895 230 Switchboard 3,162,544 262,744 4,876 CS Dev 93,353 4,126 10 CS Test 87,527 3,611 10 Table 2: Summary of evaluation corpora. Model Perplexity WER Dev Test Dev Test FixKN(1) 174.7 196.7 34.9% 36.8% + W(hdoc) 172.9 194.8 34.7% 36.7% FixKN(3) 168.6 189.3 34.9% 36.9% + W(hdoc) 166.8 187.8 34.6% 36.6% FixKN(10) 167.5 187.6 35.0% 37.2% + W(hdoc) 165.3 185.8 34</context>
</contexts>
<marker>Glass, Hazen, Cyphers, Malioutov, Huynh, Barzilay, 2007</marker>
<rawString>James Glass, Timothy J. Hazen, Scott Cyphers, Igor Malioutov, David Huynh, and Regina Barzilay. 2007. Recent progress in the MIT spoken lecture processing project. In Proc. Interspeech, pages 2553–2556, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Glass</author>
</authors>
<title>A probabilistic framework for segment-based speech recognition.</title>
<date>2003</date>
<journal>Computer Speech &amp; Language,</journal>
<pages>17--2</pages>
<contexts>
<context position="13465" citStr="Glass, 2003" startWordPosition="2144" endWordPosition="2145">discount parameters. KN(d): FixKN(d) with tuned values. W(feat): n-gram weighting with feat feature. set (CS Dev) and use the last 10 for the test set (CS Test). For training, we will consider the course textbook with topic-specific vocabulary (Textbook), numerous high-fidelity transcripts from a variety of general seminars and lectures (Lectures), and the out-of-domain LDC Switchboard corpus of spontaneous conversational speech (Switchboard) (Godfrey and Holliman, 1993). Table 2 summarizes all the evaluation data. To compute the word error rate, we use a speakerindependent speech recognizer (Glass, 2003) with a large-margin discriminative acoustic model (Chang, 2008). The lectures are pre-segmented into utterances via forced alignment against the reference transcripts (Hazen, 2006). Since all the models considered in this work can be encoded as n-gram backoff models, they are applied directly during the first recognition pass instead of through a subsequent nbest rescoring step. Model Perplexity WER Lectures 189.3 36.9% + W(hdoc) 187.8 (-0.8%) 36.6% Textbook 326.1 43.1% + W(hdoc) 317.5 (-2.6%) 43.1% LI(Lectures + Textbook) 141.6 33.7% + W(hdoc) 136.6 (-3.5%) 32.7% Table 4: N-gram weighting wi</context>
</contexts>
<marker>Glass, 2003</marker>
<rawString>James Glass. 2003. A probabilistic framework for segment-based speech recognition. Computer Speech &amp; Language, 17(2-3):137–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Ed Holliman</author>
</authors>
<date>1993</date>
<booktitle>Switchboard-1 transcripts. Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="13328" citStr="Godfrey and Holliman, 1993" startWordPosition="2119" endWordPosition="2123">.1% + W(hdoc) 160.1 180.0 34.8% 36.8% Table 3: Performance of n-gram weighting with a variety of Kneser-Ney settings. FixKN(d): Kneser-Ney with d fixed discount parameters. KN(d): FixKN(d) with tuned values. W(feat): n-gram weighting with feat feature. set (CS Dev) and use the last 10 for the test set (CS Test). For training, we will consider the course textbook with topic-specific vocabulary (Textbook), numerous high-fidelity transcripts from a variety of general seminars and lectures (Lectures), and the out-of-domain LDC Switchboard corpus of spontaneous conversational speech (Switchboard) (Godfrey and Holliman, 1993). Table 2 summarizes all the evaluation data. To compute the word error rate, we use a speakerindependent speech recognizer (Glass, 2003) with a large-margin discriminative acoustic model (Chang, 2008). The lectures are pre-segmented into utterances via forced alignment against the reference transcripts (Hazen, 2006). Since all the models considered in this work can be encoded as n-gram backoff models, they are applied directly during the first recognition pass instead of through a subsequent nbest rescoring step. Model Perplexity WER Lectures 189.3 36.9% + W(hdoc) 187.8 (-0.8%) 36.6% Textbook</context>
</contexts>
<marker>Godfrey, Holliman, 1993</marker>
<rawString>John J. Godfrey and Ed Holliman. 1993. Switchboard-1 transcripts. Linguistic Data Consortium, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="3749" citStr="Griffiths et al., 2005" startWordPosition="561" endWordPosition="564">olation component. Using a combination of features derived from multiple partitions of the training documents, the resulting weighted n-gram model achieves up to a 1.2% absolute word error rate (WER) reduction over a linearly interpolated baseline on a lecture transcription task. 829 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 829–838, Honolulu, October 2008.c�2008 Association for Computational Linguistics 2 Related Work To reduce topic mismatch in LM estimation, we (2006) have previously assigned topic labels to each word by applying HMM-LDA (Griffiths et al., 2005) to the training documents. Using an ad hoc method to reduce the effective counts of n-grams ending on topic words, we achieved better perplexity and WER than standard trigram LMs. Intuitively, deemphasizing such n-grams will lower the transition probability to out-of-domain topic words from the training data. In this work, we further explore this intuition with a principled feature-based model, integrated with LM smoothing and estimation to allow simultaneous optimization of all model parameters. As Gao and Lee (2000) observed, even purported matched training data may exhibit topic, style, or</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Hazen</author>
</authors>
<title>Automatic alignment and error correction of human generated transcripts for long speech recordings.</title>
<date>2006</date>
<booktitle>In Proc. Interspeech,</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="13646" citStr="Hazen, 2006" startWordPosition="2168" endWordPosition="2169">ll consider the course textbook with topic-specific vocabulary (Textbook), numerous high-fidelity transcripts from a variety of general seminars and lectures (Lectures), and the out-of-domain LDC Switchboard corpus of spontaneous conversational speech (Switchboard) (Godfrey and Holliman, 1993). Table 2 summarizes all the evaluation data. To compute the word error rate, we use a speakerindependent speech recognizer (Glass, 2003) with a large-margin discriminative acoustic model (Chang, 2008). The lectures are pre-segmented into utterances via forced alignment against the reference transcripts (Hazen, 2006). Since all the models considered in this work can be encoded as n-gram backoff models, they are applied directly during the first recognition pass instead of through a subsequent nbest rescoring step. Model Perplexity WER Lectures 189.3 36.9% + W(hdoc) 187.8 (-0.8%) 36.6% Textbook 326.1 43.1% + W(hdoc) 317.5 (-2.6%) 43.1% LI(Lectures + Textbook) 141.6 33.7% + W(hdoc) 136.6 (-3.5%) 32.7% Table 4: N-gram weighting with linear interpolation. 4.2 Smoothing In Table 3, we compare the performance of n-gram weighting with the hdoc document entropy feature for various modified Kneser-Ney smoothing co</context>
</contexts>
<marker>Hazen, 2006</marker>
<rawString>T.J. Hazen. 2006. Automatic alignment and error correction of human generated transcripts for long speech recordings. In Proc. Interspeech, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-June Hsu</author>
<author>James Glass</author>
</authors>
<title>Style &amp; topic language model adaptation using HMM-LDA.</title>
<date>2006</date>
<booktitle>In Proc. Empirical Methods in Natural Language Processing,</booktitle>
<pages>373--381</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="9771" citStr="Hsu and Glass, 2006" startWordPosition="1528" endWordPosition="1531">/3, f(C) = 2/3. However, as the size of each partition increases, this ratio increases to 1, since most n-grams have a non-zero probability of appearing in each partition. Thus, an alternative is to compute the normalized entropy of the n-gram distribution across the S partitions, s �3 i p(s) logp(s), where p(s) is the or h = log fraction of an n-gram appearing in partition s. For example, the normalized entropy of the unigram C is h(C) = −1 log 3[2 6 log 2 6 + 4 6 log 4 6 +0] = .58. N-grams clustered in fewer partitions have lower entropy than ones that are more evenly spread out. Following (Hsu and Glass, 2006), we also consider features derived from the HMM-LDA word topic labels.1 Specifically, we compute the empirical probability t that the target word of the n-gram 1HMM-LDA is performed using 20 states and 50 topics with a 3rd-order HMM. Hyperparameters are sampled with a lognormal Metropolis proposal. The model with the highest likelihood from among 10,000 iterations of Gibbs sampling is used. Feature Random 0.03 0.32 0.33 0.19 0.53 0.24 0.37 0.80 log(c) 9.29 8.09 3.47 5.86 6.82 7.16 3.09 4.92 fdoc 1.00 0.93 0.00 0.18 0.92 0.76 0.00 0.04 fcourse 1.00 1.00 0.06 0.56 0.94 0.94 0.06 0.06 fspeaker 0</context>
</contexts>
<marker>Hsu, Glass, 2006</marker>
<rawString>Bo-June (Paul) Hsu and James Glass. 2006. Style &amp; topic language model adaptation using HMM-LDA. In Proc. Empirical Methods in Natural Language Processing, pages 373–381, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-June Hsu</author>
<author>James Glass</author>
</authors>
<title>Iterative language model estimation: Efficient data structure &amp; algorithms.</title>
<date>2008</date>
<booktitle>In Proc. Interspeech,</booktitle>
<location>Brisbane, Australia.</location>
<contexts>
<context position="11521" citStr="Hsu and Glass, 2008" startWordPosition="1836" endWordPosition="1839">eatures can be computed for any partitioning of the training data. To better illustrate the differences, we compute the features on a set of lecture transcripts (see Section 4.1) partitioned by lecture (doc), course, and speaker. Furthermore, we include the log of the n-gram counts c and random values between 0 and 1 as baseline features. Table 1 lists all the features examined in this work and their values on a select subset of n-grams. 3.2 Training To tune the n-gram weighting parameters 0, we apply Powell’s method (Press et al., 2007) to numerically minimize the development set perplexity (Hsu and Glass, 2008). Although there is no guarantee against converging to a local minimum when jointly tuning both the n-gram weighting and interpolation parameters, we have found that initializing the parameters to zero generally yields good performance. 4 Experiments 4.1 Setup In this work, we evaluate the perplexity and WER of various trigram LMs trained with n-gram weighting on a lecture transcription task (Glass et al., 2007). The target data consists of 20 lectures from an introductory computer science course, from which we withhold the first 10 lectures for the development 831 Dataset # Words # Sents # Do</context>
</contexts>
<marker>Hsu, Glass, 2008</marker>
<rawString>Bo-June (Paul) Hsu and James Glass. 2008. Iterative language model estimation: Efficient data structure &amp; algorithms. In Proc. Interspeech, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-June Hsu</author>
</authors>
<title>Generalized linear interpolation of language models.</title>
<date>2007</date>
<booktitle>In Proc. Automatic Speech Recognition and Understanding,</booktitle>
<pages>136--140</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="21701" citStr="Hsu, 2007" startWordPosition="3435" endWordPosition="3436"> By combining features, we are able to further reduce the test set WER by a statistically significant (p &lt; 0.001) 0.2% over the best single feature model. 4.6 Advanced Interpolation While n-gram weighting with all three features is able to reduce the test set WER by 1.2% over the linear interpolation baseline, linear interpolation is not a particularly effective interpolation technique. In Table 7, we compare the effectiveness of n-gram weighting in combination with better interpolation techniques, such as count merging (CM) (Bacchiani et al., 2006) and generalized linear interpolation (GLI) (Hsu, 2007). As expected, the use of more sophisticated interpolation techniques decreases the perplexity and WER reductions achieved by n-gram weighting by roughly half for a variety of feature combinations. However, all improvements remain statistically significant. Model Perplexity WER Linear(L + T) 141.6 33.7% + W(hcourse) 136.1 (-3.9 %) 32.7% + W(tdoc) 133.2 (-5.9 %) 32.6% + W(fdoc) 132.8 (-6.2 %) 32.5% CM(L + T) 137.9 33.0% + W(hcourse) 135.5 (-1.8 %) 32.4% + W(tdoc) 133.4 (-3.3 %) 32.4% + W(fdoc) 133.2 (-3.5 %) 32.4% GLIlog(l+Z)(L + T) 135.9 33.0% + W(hcourse) 133.0 (-2.2 %) 32.4% + W(tdoc) 130.6 </context>
</contexts>
<marker>Hsu, 2007</marker>
<rawString>Bo-June (Paul) Hsu. 2007. Generalized linear interpolation of language models. In Proc. Automatic Speech Recognition and Understanding, pages 136–140, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proc. Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="31054" citStr="Malioutov and Barzilay, 2006" startWordPosition="4922" endWordPosition="4925">. a lecture transcri 836 We examined the performance of various n-gram weighting features and generally found entropybased features to offer the best predictive performance. Although the topic probability features derived from HMM-LDA labels yield additional improvements when applied in combination with the normalized entropy features, the computational cost of performing HMM-LDA may not justify the marginal benefit in all scenarios. In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). Synthetic metadata information may also be obtained via clustering techniques (Steinbach et al., 2000). Although we have primarily focused on ngram weighting features derived from segmentation information, it is also possible to consider other features that correlate with n-gram relevance. N-gram weighting and other approaches to crossdomain language modeling require a matched development set for model parameter tuning. Thus, for future work, we plan to investigate the use of the initial recognition hypotheses as the development set, as well as manually transcribing a subset of the test set </context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proc. Association for Computational Linguistics, pages 25– 32, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pallet</author>
<author>W Fisher</author>
<author>Fiscus</author>
</authors>
<title>Tools for the analysis of benchmark speech recognition tests.</title>
<date>1990</date>
<booktitle>In Proc. ICASSP,</booktitle>
<location>Albuquerque, NM, USA.</location>
<contexts>
<context position="15076" citStr="Pallet et al., 1990" startWordPosition="2386" endWordPosition="2389"> algorithms correspond to a setting of 1 and 3, respectively. Furthermore, we explored using both fixed parameter values estimated from n-gram count statistics and tuned values that minimize the development set perplexity. In this task, while the test set perplexity tracks the development set perplexity well, the WER correlates surprisingly poorly with the perplexity on both the development and test sets. Nevertheless, n-gram weighting consistently reduces the absolute test set WER by a statistically significant average of 0.3%, according to the Matched Pairs Sentence Segment Word Error test (Pallet et al., 1990). Given that we obtained the lowest development set WER with the fixed 3-parameter modified Kneser-Ney smoothing, all subsequent experiments are conducted using this smoothing configuration. 4.3 Linear Interpolation Applied to the Lectures dataset in isolation, n-gram weighting with the hdoc feature reduces the test set WER by 0.3% by de-emphasizing the probability contributions from off-topic n-grams and shifting their weights to the backoff distributions. Ideally though, such weights should be distributed to ontopic n-grams, perhaps from other LM components. In Table 4, we present the perfor</context>
</contexts>
<marker>Pallet, Fisher, Fiscus, 1990</marker>
<rawString>D. Pallet, W. Fisher, and Fiscus. 1990. Tools for the analysis of benchmark speech recognition tests. In Proc. ICASSP, Albuquerque, NM, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes.</title>
<date>2007</date>
<publisher>Cambridge University Press,</publisher>
<note>3rd edition.</note>
<contexts>
<context position="11444" citStr="Press et al., 2007" startWordPosition="1824" endWordPosition="1827">ic word. In the example corpus, t(C) = 3/6, t(A C) = 2/4. All of the above features can be computed for any partitioning of the training data. To better illustrate the differences, we compute the features on a set of lecture transcripts (see Section 4.1) partitioned by lecture (doc), course, and speaker. Furthermore, we include the log of the n-gram counts c and random values between 0 and 1 as baseline features. Table 1 lists all the features examined in this work and their values on a select subset of n-grams. 3.2 Training To tune the n-gram weighting parameters 0, we apply Powell’s method (Press et al., 2007) to numerically minimize the development set perplexity (Hsu and Glass, 2008). Although there is no guarantee against converging to a local minimum when jointly tuning both the n-gram weighting and interpolation parameters, we have found that initializing the parameters to zero generally yields good performance. 4 Experiments 4.1 Setup In this work, we evaluate the perplexity and WER of various trigram LMs trained with n-gram weighting on a lecture transcription task (Glass et al., 2007). The target data consists of 20 lectures from an introductory computer science course, from which we withho</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2007</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2007. Numerical Recipes. Cambridge University Press, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Steinbach</author>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>A comparison of document clustering techniques.</title>
<date>2000</date>
<tech>Technical Report #00-034,</tech>
<institution>University of Minnesota.</institution>
<contexts>
<context position="31158" citStr="Steinbach et al., 2000" startWordPosition="4937" endWordPosition="4940">entropybased features to offer the best predictive performance. Although the topic probability features derived from HMM-LDA labels yield additional improvements when applied in combination with the normalized entropy features, the computational cost of performing HMM-LDA may not justify the marginal benefit in all scenarios. In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). Synthetic metadata information may also be obtained via clustering techniques (Steinbach et al., 2000). Although we have primarily focused on ngram weighting features derived from segmentation information, it is also possible to consider other features that correlate with n-gram relevance. N-gram weighting and other approaches to crossdomain language modeling require a matched development set for model parameter tuning. Thus, for future work, we plan to investigate the use of the initial recognition hypotheses as the development set, as well as manually transcribing a subset of the test set utterances. As speech and natural language applications shift towards novel domains with limited matched</context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>Michael Steinbach, George Karypis, and Vipin Kumar. 2000. A comparison of document clustering techniques. Technical Report #00-034, University of Minnesota.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>