<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003340">
<title confidence="0.950516">
Assessing the relative reading level of sentence pairs for text simplification
</title>
<author confidence="0.91776">
Sowmya Vajjala and Detmar Meurers
</author>
<affiliation confidence="0.905437">
LEAD Graduate School, Seminar f¨ur Sprachwissenschaft
</affiliation>
<address confidence="0.520331">
Universit¨at T¨ubingen
</address>
<email confidence="0.992186">
{sowmya,dm}@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.997284" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994380952381">
While the automatic analysis of the read-
ability of texts has a long history, the use
of readability assessment for text simplifi-
cation has received only little attention so
far. In this paper, we explore readability
models for identifying differences in the
reading levels of simplified and unsimpli-
fied versions of sentences.
Our experiments show that a relative rank-
ing is preferable to an absolute binary one
and that the accuracy of identifying rel-
ative simplification depends on the ini-
tial reading level of the unsimplified ver-
sion. The approach is particularly success-
ful in classifying the relative reading level
of harder sentences.
In terms of practical relevance, the ap-
proach promises to be useful for identi-
fying particularly relevant targets for sim-
plification and to evaluate simplifications
given specific readability constraints.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999842422222222">
Text simplification essentially is the process of
rewriting a given text to make it easier to process
for a given audience. The target audience can ei-
ther be human users trying to understand a text or
machine applications, such as a parser analyzing
text. Text simplification has been used in a vari-
ety of application scenarios, from providing sim-
plified newspaper texts for aphasic readers (Can-
ning and Tait, 1999) to supporting the extraction of
protein-protein interactions in the biomedical do-
main (Jonnalagadda and Gonzalez, 2009).
A related field of research is automatic readabil-
ity assessment, which can be useful for evaluating
text simplification. It can also be relevant for in-
termediate simplification steps, such as the identi-
fication of target sentences for simplification. Yet,
so far there has only been little research connect-
ing the two subfields, possibly because readability
research typically analyzes documents, whereas
simplification approaches generally targeted lex-
ical and syntactic aspects at the sentence level. In
this paper, we attempt to bridge this gap between
readability and simplification by studying read-
ability at a sentence level and exploring how well
can a readability model identify the differences be-
tween unsimplified and simplified sentences.
Our main research questions in this paper are:
1. Can the readability features that worked at the
document level successfully be used at the sen-
tence level? 2. How accurately can we identify the
differences in the sentential reading level before
and after simplification? To pursue these ques-
tions, we started with constructing a document-
level readability model. We then applied it to nor-
mal and simplified versions of sentences drawn
from Wikipedia and Simple Wikipedia.
As context of our work, we first discuss rel-
evant related research. Section 2 then describes
the corpora and the features we used to construct
our readability model. Section 3 discusses the
performance of our readability model in compari-
son with other existing systems. Sections 4 and 5
present our experiments with sentence level read-
ability analysis and the results. In Section 6 we
present our conclusions and plans for future work.
</bodyText>
<sectionHeader confidence="0.67958" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.9988809">
Research into automatic text simplification essen-
tially started with the idea of splitting long sen-
tences into multiple shorter sentences to improve
parsing efficiency (Chandrasekar et al., 1996;
Chandrasekar and Srinivas, 1996). This was
followed by rule-based approaches targeting hu-
man and machine uses (Carroll et al., 1999; Sid-
dharthan, 2002, 2004).
With the availability of a sentence-aligned cor-
pus based on Wikipedia and SimpleWikipedia
</bodyText>
<page confidence="0.960867">
288
</page>
<note confidence="0.9930015">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99938884375">
texts, data-driven approaches, partly inspired by
statistical machine translation, appeared (Specia,
2010; Zhu et al., 2010; Bach et al., 2011; Coster
and Kauchak, 2011; Woodsend and Lapata, 2011).
While simplification methods have evolved, un-
derstanding which parts of a text need to be sim-
plified and methods for evaluating the simplified
text so far received only little attention. The use
of readability assessment for simplification has
mostly been restricted to using traditional read-
ability formulae for evaluating or generating sim-
plified text (Zhu et al., 2010; Wubben et al.,
2012; Klerke and Søgaard, 2013; Stymne et al.,
2013). Some recent work briefly addresses issues
such as classifying sentences by their reading level
(Napoles and Dredze, 2010) and identifying sen-
tential transformations needed for text simplifica-
tion using text complexity features (Medero and
Ostendorf, 2011). Some simplification approaches
for non-English languages (Aluisio et al., 2010;
Gasperin et al., 2009; ˇStajner et al., 2013) also
touch on the use of readability assessment.
In the present paper, we focus on the neglected
connection between readability analysis and sim-
plification. We show through a cross-corpus eval-
uation that a document level, regression-based
readability model successfully identifies the dif-
ferences between simplified vs. unsimplified sen-
tences. This approach can be useful in various
stages of simplification ranging from identifying
simplification targets to the evaluation of simplifi-
cation outcomes.
</bodyText>
<sectionHeader confidence="0.947419" genericHeader="method">
2 Corpora and Features
</sectionHeader>
<subsectionHeader confidence="0.944355">
2.1 Corpora
</subsectionHeader>
<bodyText confidence="0.99940484">
We built and tested our document and sentence
level readability models using three publicly avail-
able text corpora with reading level annotations.
WeeBit Corpus: The WeeBit corpus (Vajjala
and Meurers, 2012) consists of 3,125 articles be-
longing to five reading levels, with 625 articles
per reading level. The texts compiled from the
WeeklyReader and BBC Bitesize target English
language learners from 7 to 16 years of age. We
used this corpus to build our primary readability
model by mapping the five reading levels in the
corpus to a scale of 1–5 and considered readabil-
ity assessment as a regression problem.
Common Core Standards Corpus: This cor-
pus consists of 168 English texts available from
the Appendix B of the Common Core Standards
reading initiative of the U.S. education system
(CCSSO, 2010). They are annotated by experts
with grade bands that cover the grades 1 to 12.
These texts serve as exemplars for the level of
reading ability at a given grade level. This corpus
was introduced as an evaluation corpus for read-
ability models in the recent past (Sheehan et al.,
2010; Nelson et al., 2012; Flor et al., 2013), so we
used it to compare our model with other systems.
</bodyText>
<subsectionHeader confidence="0.478233">
Wiki-SimpleWiki Sentence Aligned Corpus:
</subsectionHeader>
<bodyText confidence="0.999956571428571">
This corpus was created by Zhu et al. (2010) and
consists of ∼100k aligned sentence pairs drawn
from Wikipedia and Simple English Wikipedia.
We removed all pairs of identical sentences, i.e.,
where the Wiki and the SimpleWiki versions are
the same. We used this corpus to study reading
level assessment at the sentence level.
</bodyText>
<subsectionHeader confidence="0.941523">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.998600033333333">
We started with the feature set described in Vajjala
and Meurers (2012) and added new features fo-
cusing on the morphological and psycholinguistic
properties of words. The features can be broadly
classified into four groups.
Lexical richness and POS features: We
adapted the lexical features from Vajjala and
Meurers (2012). This includes measures of lexical
richness from Second Language Acquisition
(SLA) research and measures of lexical variation
(noun, verb, adjective, adverb and modifier vari-
ation). In addition, this feature set also includes
part-of-speech densities (e.g., the average # of
nouns per sentence). The information needed to
calculate these features was extracted using the
Stanford Tagger (Toutanova et al., 2003). None
of the lexical richness and POS features we used
refer to specific words or lemmas.
Syntactic Complexity features: Parse tree
based features and some syntactic complexity
measures derived from SLA research proved
useful for readability classification in the past, so
we made use of all the syntactic features from
Vajjala and Meurers (2012): mean lengths of
various production units (sentence, clause, t-unit),
measures of coordination and subordination
(e.g., # of coordinate clauses per clause), the
presence of particular syntactic structures (e.g.,
VPs per t-unit), the number of phrases of various
categories (e.g., NP, VP, PP), the average lengths
</bodyText>
<page confidence="0.996661">
289
</page>
<bodyText confidence="0.992302958333334">
of phrases, the parse tree height, and the number
of constituents per subtree. None of the syntactic
features refer to specific words or lemmas. We
used the BerkeleyParser (Petrov and Klein, 2007)
for generating the parse trees and the Tregex tool
(Levy and Andrew, 2006) to count the occurrences
of the syntactic patterns.
While the first two feature sets are based on our
previous work, as far as we know the next two are
used in readability assessment for the first time.
Features from the Celex Lexical Database:
The Celex Lexical Database (Baayen et al., 1995)
is a database consisting of information about mor-
phological, syntactic, orthographic and phonolog-
ical properties of words along with word frequen-
cies in various corpora. Celex for English contains
this information for more than 50,000 lemmas. An
overview of the fields in the Celex database is pro-
vided online1 and the Celex user manual2.
We used the morphological and syntactic prop-
erties of lemmas as features. We excluded word
frequency statistics and properties which consisted
of word strings. In all, we used 35 morphologi-
cal and 49 syntactic properties that were expressed
using either character or numeric codes in this
database as features for our task.
The morphological properties in Celex include
information about the derivational, inflectional
and compositional features of the words, their
morphological origins and complexity. The syn-
tactic properties of the words in Celex describe
the attributes of a word depending on its parts of
speech. For the morphological and syntactic prop-
erties from this database, we used the proportion
of occurrences per text as features. For example,
the ratio of transitive verbs, complex morphologi-
cal words, and vocative nouns to number of words.
Lemmas from the text that do not have entries in
the Celex database were ignored.
Word frequency statistics from Celex have been
used before to analyze text difficulty in the past
(Crossley et al., 2007). However, to our knowl-
edge, this is the first time morphological and syn-
tactic information from the Celex database is used
for readability assessment.
Psycholinguistic features: The MRC Psy-
cholinguistic Database (Wilson, 1988) is a freely
available, machine readable dictionary annotated
</bodyText>
<footnote confidence="0.9990255">
1http://celex.mpi.nl/help/elemmas.html
2http://catalog.ldc.upenn.edu/docs/LDC96L14
</footnote>
<bodyText confidence="0.999389863636364">
with 26 linguistic and psychological attributes of
about 1.5 million words.3 We used the measures
of word familiarity, concreteness, imageability,
meaningfulness, and age of acquisition from
this database as our features, by encoding their
average values per text.
Kuperman et al. (2012) compiled a freely avail-
able database that includes Age of Acquisition
(AoA) ratings for over 50,000 English words.4
This database was created through crowd sourcing
and was compared with several other AoA norms,
which are also included in the database. For each
of the five AoA norms, we computed the average
AoA of words per text.
Turning to the final resource used, we included
the average number of senses per word as calcu-
lated using the MIT Java WordNet Interface as a
feature.5 We excluded auxiliary verbs for this cal-
culation as they tend to have multiple senses that
do not necessarily contribute to reading difficulty.
Combining the four feature groups, we encode
151 features for each text.
</bodyText>
<sectionHeader confidence="0.991883" genericHeader="method">
3 Document-Level Readability Model
</sectionHeader>
<bodyText confidence="0.999980333333334">
In our first experiment, we tested the document-
level readability model based on the 151 features
using the WeeBit corpus. Under a regression per-
spective on readability, we evaluated the approach
using Pearson Correlation and Root Mean Square
Error (RMSE) in a 10-fold cross-validation set-
ting. We used the SMO Regression implementa-
tion from WEKA (Hall et al., 2009) and achieved a
Pearson correlation of 0.92 and an RMSE of 0.53.
The document-level performance of our 151
feature model is virtually identical to that of the re-
gression model we presented in Vajjala and Meur-
ers (2013). But compared to our previous work,
the Celex and psycholinguistic features we in-
cluded here provide more lexical information that
is meaningful to compute even for the sentence-
level analysis we turn to in the next section.
To be able to compare our document-level
results with other contemporary readability ap-
proaches, we need a common test corpus. Nel-
son et al. (2012) compared several state of the art
readability assessment systems using five test sets
and showed that the systems that went beyond tra-
ditional formulae and wordlists performed better
</bodyText>
<footnote confidence="0.999954">
3http://www.psych.rl.ac.uk
4http://crr.ugent.be/archives/806
5http://projects.csail.mit.edu/jwi
</footnote>
<page confidence="0.994685">
290
</page>
<bodyText confidence="0.999964923076923">
on these real-life test sets. We tested our model
on one of the publicly accessible test corpora from
this study, the Common Core Standards Corpus.
Flor et al. (2013) used the same test set to study
a measure of lexical tightness, providing a further
performance reference.
Table 1 compares the performance of our model
to that reported for several commercial (indicated
in italics) and research systems on this test set.
Nelson et al. (2012) used Spearman’s Rank Cor-
relation and Flor et al. (2013) used Pearson Corre-
lation as evaluation metrics. To facilitate compar-
ison, for our approach we provide both measures.
</bodyText>
<table confidence="0.996163846153846">
System Spearman Pearson
Our System 0.69 0.61
Nelson et al. (2012):
REAP6 0.54 –
ATOS7 0.59 –
DRP8 0.53 –
Lexile9 0.50 –
Reading Maturity10 0.69 –
SourceRater11 0.75 –
Flor et al. (2013):
Lexical Tightness – -0.44
Flesch-Kincaid – 0.49
Text length – 0.36
</table>
<tableCaption confidence="0.99995">
Table 1: Performance on CommonCore data
</tableCaption>
<bodyText confidence="0.99941525">
As the table shows, our model is the best non-
commercial system and overall second (tied with
the Reading Maturity system) to SourceRater as
the best performing commercial system on this
test set. These results on an independent test set
confirm the validity of our document-level read-
ability model. With this baseline, we turned to a
sentence-level readability analysis.
</bodyText>
<sectionHeader confidence="0.934628" genericHeader="method">
4 Sentence-Level Binary Classification
</sectionHeader>
<bodyText confidence="0.999823285714286">
For each of the pairs in the Wiki-SimpleWiki Sen-
tence Aligned Corpus introduced above, we la-
beled the sentence from Wikipedia as hard and
that from Simple English Wikipedia as simple.
The corpus thus consisted of single sentences,
each labeled either simple or hard. On this basis,
we constructed a binary classification model.
</bodyText>
<footnote confidence="0.9995365">
6http://reap.cs.cmu.edu
7http://renlearn.com/atos
8http://questarai.com/Products/DRPProgram
9http://lexile.com
10http://readingmaturity.com
11http://naeptba.ets.org/SourceRater3
</footnote>
<bodyText confidence="0.999885409090909">
Our document-level readability model does not
include discourse features, so all 151 features can
also be computed for individual sentences. We
built a binary sentence-level classification model
using WEKA’s Sequential Minimal Optimization
(SMO) for training an SVM in WEKA on the
Wiki-SimpleWiki sentence aligned corpus. The
choice of algorithm was primarily motivated by
the fact that it was shown to be efficient in previ-
ous work on readability classification (Feng, 2010;
Hancke et al., 2012; Falkenjack et al., 2013).
The accuracy of the resulting classifier deter-
mining whether a given sentence is simple or
hard was disappointing, reaching only 66% accu-
racy in a 10-fold cross-validation setting. Exper-
iments with different classification algorithms did
not yield any more promising results. To study
how the classification performance is impacted by
the size of the training data, we experimented with
different sizes, using SMO as the classification al-
gorithm. Figure 1 shows the classification accu-
racy with different training set sizes.
</bodyText>
<figure confidence="0.993881">
68.5
68
classification accuracy (in %) 67.5
67
66.5
66
65.5
65
0 10 20 30 40 50 60 70 80 90 100
% of training data used
</figure>
<figureCaption confidence="0.99998">
Figure 1: Training size vs. classification accuracy
</figureCaption>
<bodyText confidence="0.999921785714286">
The graph shows that beyond 10% of the training
data, more training data did not result in signifi-
cant differences in classification accuracy. Even
at 10%, the training set contains around 10k in-
stances per category, so the variability of any of
the patterns distinguished by our features is suffi-
ciently represented.
We also explored whether feature selection
could be useful. A subset of features chosen by re-
moving correlated features using the CfsSubsetE-
val method in WEKA did not improve the results,
yielding an accuracy of 65.8%. A simple base-
line based on the sentence length as single feature
results in an accuracy of 60.5%, underscoring the
</bodyText>
<page confidence="0.987485">
291
</page>
<bodyText confidence="0.999120890909091">
limited value of the rich feature set in this binary
classification setup.
For the sake of a direct comparison with the
document-level model, we also explored modeling
the task as a regression on a 1–2 scale. In compar-
ison to the document-level model, which as dis-
cussed in section 3 had a correlation of 0.92, the
sentence-level model achieves only a correlation
of 0.4. A direct comparison is also possible when
we train the document-level model as a five-class
classifier with SMO. This model achieved a clas-
sification accuracy of ∼90% on the documents,
compared to the 66% accuracy of the sentence-
level model classifying sentences. So under each
of these perspectives, the sentence-level models on
the sentence task are much less successful than the
document-level models on the document task.
But does this indicate that it is not possible to
accurately identify the reading level distinctions
between simplified and unsimplified versions at
the sentence level? Is there not enough informa-
tion available when considering a single sentence?
We hypothesized that the drop in the classi-
fication accuracy instead results from the rela-
tive nature of simplification. For each pair of
the Wiki-SimpleWiki sentence aligned corpus we
used, the Wiki sentence was harder than the Sim-
pleWikipedia sentence. But this does not neces-
sarily mean that each of the Wikipedia sentences
is harder than each of the SimpleWikipedia sen-
tences. The low accuracy of the binary classi-
fier may thus simply result from the inappropriate
assumption of an absolute, binary classification
viewing each of the sentences originating from
SimpleWikipedia as simple and each from the reg-
ular Wiki as hard.
The confusion matrices of the binary classifi-
cation suggests some support for this hypothesis,
as more simple sentences were classified as hard
compared to the other way around. This can result
when a simple sentence is simpler than its hard
version, but could actually be simplified further –
and as such may still be harder than another un-
simplified sentence. The hypothesis thus amounts
to saying that the two-class classification model
mistakenly turned the relative difference between
the sentence pairs into a global classification of in-
dividual sentences, independent of the pairs they
occur in.
How can we verify this hypothesis? The sen-
tence corpus only provides the relative ranking of
the pairs, but we can try to identify more fine-
grained readability levels for sentences by apply-
ing the five class readability model for documents
that was introduced in section 3.
</bodyText>
<sectionHeader confidence="0.993782" genericHeader="method">
5 Relative Reading Levels of Sentences
</sectionHeader>
<bodyText confidence="0.966973166666667">
We applied the document-level readability model
to the individual sentences from the Wiki-
SimpleWiki corpus to study which reading levels
are identified by our model. As we are using a re-
gression model, the values sometimes go beyond
the training corpus’ scale of 1–5. For ease of com-
parison, we rounded off the reading levels to the
five level scale, i.e., 1 means 1 or below, and 5
means 5 or above. Figure 2 shows the distribution
of Wikipedia and SimpleWikipedia sentences ac-
cording to the predictions of our document-level
readability model trained on the WeeBit corpus.
Figure 2: Reading level distribution of the
Wikipedia and SimpleWikipedia sentences
The model determines that a high percentage of
the SimpleWiki sentences belong to lower reading
levels, with over 45% at the lowest reading level;
yet there also are some SimpleWikipedia sen-
tences which are aligned even to the highest read-
ability level. In contrast, the regular Wikipedia
sentences are evenly distributed across all reading
levels.
The distributions identified by the model sup-
port our hypothesis that some Wiki sentences are
simpler than some SimpleWikipedia sentences.
Note that this is fully compatible with the fact that
for each pair of (SimpleWiki,Wiki) sentences in-
cluded in the corpus, the former is higher in read-
ing level than the latter; e.g., just consider two sen-
tence pairs with the levels (1, 2) and (3, 5).
</bodyText>
<figure confidence="0.967884666666667">
50
Wiki
Simple Wiki
40
35
30
25
20
15
10
5
1 1.5 2 2.5 3 3.5 4 4.5 5
Reading level
Percentage of the total sentences at that level
45
</figure>
<page confidence="0.981608">
292
</page>
<subsectionHeader confidence="0.930958">
5.1 On the discriminating power of the model
</subsectionHeader>
<bodyText confidence="0.999980714285714">
Zooming in on the relative reading levels of the
paired unsimplified and simplified sentences, we
wanted to determine for how many sentence pairs
the sentence reading levels determined by our
model are compatible with the pair’s ranking. In
other words, we calculated the percentage of pairs
(5, N) in which the reading level of a simplified
sentence (5) is identified as less than, equal to, or
greater than the unsimplified (normal) version of
the sentence (N), i.e., 5 &lt; N, 5 = N, and 5 &gt; N.
Where simplification split a sentence into multiple
sentences, we computed S as the average reading
level of the split sentences.
Given the regression model setup, we can con-
sider how big the difference between two reading
levels determined by the model should be in or-
der for us to interpret it as a categorical difference
in reading level. Let us call this discriminating
reading-level difference the d-level. For example,
with d = 0.3, a sentence pair determined to be
at levels (3.4, 3.2) would be considered a case of
5 =N, whereas (3.4, 3.7) would be an instance of
5 &lt; N. The d-value can be understood as a mea-
sure of how fine-grained the model is in identify-
ing reading-level differences between sentences.
If we consider the percentage of samples identi-
fied as 5 &lt;= N as an accuracy measure, Figure 3
shows the accuracy for different d-values.
</bodyText>
<figure confidence="0.961683">
0 0.2 0.4 0.6 0.8 1
d-value
</figure>
<figureCaption confidence="0.99989">
Figure 3: Accurately identified 5 &lt;=N
</figureCaption>
<bodyText confidence="0.9998964">
We can observe that the percentage of instances
that the model correctly identifies as 5 &lt;= N
steadily increases from 70% to 90% as d increases.
While the value of d in theory can be anything,
values beyond 1 are uninteresting in the context of
this study. At d = 1, most of the sentence pairs
already belong to 5 =N, so increasing this further
would defeat the purpose of identifying reading-
level differences. The higher the d-value, the more
of the simplified and unsimplified pairs are lumped
together as indistinguishable.
Spelling out the different cases from Figure 3,
the number of pairs identified correctly, equated,
and misclassified as a function of the d-value is
shown in Figure 4.
</bodyText>
<figure confidence="0.9689755">
0 0.2 0.4 0.6 0.8 1
d-value
</figure>
<figureCaption confidence="0.94876">
Figure 4: Correctly (5 &lt; N), equated (5 = N),
and incorrectly (5&gt;N) identified sentence pairs
</figureCaption>
<bodyText confidence="0.999829875">
At d = 0.4, around 50% of the pairs are cor-
rectly classified, 20% are misclassified, and 30%
equated. At d = 0.7, the rate of pairs for which no
distinction can be determined already rises above
50%. For d-values between 0.3 and 0.6, the per-
centage of correctly identified pairs exceeds the
percentage of equated pairs, which in turn exceeds
the percentage of misclassified pairs.
</bodyText>
<subsectionHeader confidence="0.998842">
5.2 Influence of reading-level on accuracy
</subsectionHeader>
<bodyText confidence="0.969452133333333">
We saw in Figure 2 that the Wikipedia sentences
are uniformly distributed across the reading lev-
els, and for each of these sentences, a human sim-
plified version is included in the corpus. Even
sentences identified by our readability model as
belonging to the lower reading levels thus were
further simplified. This leads us to investigate
whether the reading level of the unsimplified sen-
tence influences the ability of our model to cor-
rectly identify the simplification relationship.
To investigate this, we separately analyzed pairs
where the unsimplified sentences had a higher
reading level and those where it had a lower read-
ing level, taking the middle of the scale (2.5) as the
Percentage of the total samples
</bodyText>
<figure confidence="0.998431576923077">
100
90
80
50
40
30
20
70
60
10
S&lt;=N
Percentage of the total samples
55
50
45
40
35
30
25
20
60
15
10
S&lt;N
S=N
S&gt;N
</figure>
<page confidence="0.998309">
293
</page>
<bodyText confidence="0.997841">
cut-off point. Figure 5 shows the accuracies ob-
tained when distinguishing unsimplified sentences
of two readability levels.
</bodyText>
<figure confidence="0.9689835">
0 0.2 0.4 0.6 0.8 1
d-value
</figure>
<figureCaption confidence="0.999847">
Figure 5: Accuracy (S &lt;=N) for different N types
</figureCaption>
<bodyText confidence="0.999939363636364">
For the pairs where the reading level of the unsim-
plified version is high, the accuracy of the read-
ability model is high (80–95%). In the other case,
the accuracy drops to 65–75% (for 0.3 &lt;= d &lt;=
0.6). Presumably the complex sentences for which
the model performs best offer more syntactic and
lexical material informing the features used.
When we split the graph into the three cases
again (S &lt; N, S = N, S &gt; N), the pairs with a
high-level unsimplified sentence in Figure 6 fol-
low the overall picture of Figure 4.
</bodyText>
<figure confidence="0.9450915">
0 0.2 0.4 0.6 0.8 1
d-value
</figure>
<figureCaption confidence="0.999403">
Figure 6: Results for N &gt;=2.5
</figureCaption>
<bodyText confidence="0.99709725">
On the other hand, the results in Figure 7 for the
pairs with an unsimplified sentence at a low read-
ability level establish that the model essentially is
incapable to identify readability differences.
</bodyText>
<figureCaption confidence="0.998096">
Figure 7: Results for N &lt;2.5
</figureCaption>
<bodyText confidence="0.999992">
The correctly identified S &lt;N and the incorrectly
identified S &gt; N cases mostly overlap, indicating
chance-level performance. Increasing the d-level
only increases the number of equated pairs, with-
out much impact on the number of correctly dis-
tinguished pairs.
In real-world terms, this means that it is diffi-
cult to identify simplifications of an already sim-
ple sentence. While some of this difficulty may
stem from the fact that simple sentences are likely
to be shorter and thus offer less linguistic material
on which an analysis can be based, it also points
to a need for more research on features that can
reliably distinguish lower levels of readability.
Summing up, the experiments discussed in this
section show that a document-level readability
model trained on the WeeBit corpus can provide
insightful perspectives on the nature of simplifica-
tion at the sentence level. The results emphasize
the relative nature of readability and the need for
more features capable of identifying characteris-
tics distinguishing sentences at lower levels.
</bodyText>
<sectionHeader confidence="0.996774" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999939090909091">
We started with constructing a document-level
readability model and compared its performance
with other readability systems on a standard test
set. Having established the state-of-the-art perfor-
mance of our document-level model, we moved on
to investigate the use of the features and the model
at the sentence level.
In the sentence-level research, we first used the
same feature set to construct a two-class readabil-
ity model on the sentences from the Wikipedia-
SimpleWikipedia sentence aligned corpus. The
</bodyText>
<figure confidence="0.987523025641026">
Percentage of the total samples having S&lt;=N
100
95
90
85
80
55
75
70
65
60
N&gt;=2.5
N&lt;2.5
Percentage of the total samples
80
50
40
30
20
70
60
10
0
S&lt;N
S=N
S&gt;N
70
60
50
40
30
20
10
S&lt;N
S=N
S&gt;N
0 0.2 0.4 0.6 0.8 1
d-value
Percentage of the total samples
</figure>
<page confidence="0.996727">
294
</page>
<bodyText confidence="0.999952229508197">
model only achieved a classification accuracy of
66%. Exploring the causes for this low perfor-
mance, we studied the sentences in the aligned
pairs through the lens of our document-level read-
ability model, the regression model based on the
five level data of the WeeBit corpus. Our ex-
periment identifies most of the Simple Wikipedia
sentences as belonging to the lower levels, with
some sentences also showing up at higher lev-
els. The sentences from the normal Wikipedia,
on the other hand, display a uniform distribution
across all reading levels. A simplified sentence
(S) can thus be at a lower reading level than its
paired unsimplified sentence (N) while also being
at a higher reading level than another unsimplified
sentence. Given this distribution of reading lev-
els, the low performance of the binary classifier
is expected. Instead of an absolute, binary differ-
ence in reading levels that counts each Wikipedia
sentence from the corpus as hard and each Simple
Wikipedia sentence as simple, a relative ranking
of reading levels seems to better suit the data.
Inspecting the relative difference in the read-
ing levels of the aligned unsimplified-simplified
sentence pairs, we characterized the accuracy of
predicting the relative reading level ranking in a
pair correctly depending on the reading-level dif-
ference d required to required to identify a cate-
gorical difference. While the experiments were
performed to verify the hypothesis that simpli-
fication is relative, they also confirm that the
document-level readability model trained on the
WeeBit corpus generalized well to Wikipedia-
SimpleWikipedia as a different, sentence-level
corpus.
The analysis revealed that the accuracy depends
on the initial reading level of the unsimplified
sentence. The model performs very well when
the reading level of the unsimplified sentence is
higher, but the features seem limited in their abil-
ity to pick up on the differences between sentences
at the lowest levels. In future work, we thus in-
tend to add more features identifying differences
between lower levels of readability.
Taking the focus on the relative ranking of
the readability of sentences one step further, we
are currently studying if modeling the readability
problem as preference learning or ordinal regres-
sion will improve the accuracy in predicting the
relation between simplified and unsimplified sen-
tence versions.
Overall, the paper contributes to the state of the
art by providing a methodology to quantitatively
evaluate the degree of simplification performed
by an automatic system. The results can also be
potentially useful in providing assistive feedback
for human writers preparing simplified texts given
specific target user constraints. We plan to explore
the idea of generating simplified text with read-
ability constraints as suggested in Stymne et al.
(2013) for Machine Translation.
</bodyText>
<sectionHeader confidence="0.996398" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997235">
We thank the anonymous reviewers for their de-
tailed comments. Our research was funded by
the LEAD Graduate School (GSC 1028, http:
//purl.org/lead), a project of the Excellence
Initiative of the German federal and state gov-
ernments, and the European Commission’s 7th
Framework Program under grant agreement num-
ber 238405 (CLARA).
</bodyText>
<sectionHeader confidence="0.998891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987243838709677">
Sandra Aluisio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 1–9.
R. H. Baayen, R. Piepenbrock, and L. Gulikers.
1995. The CELEX lexical databases. CDROM,
http://www.ldc.upenn.edu/Catalog/
readme_files/celex.readme.html.
Nguyen Bach, Qin Gao, Stephan Vogel, and Alex
Waibel. 2011. Tris: A statistical sentence simplifier
with log-linear models and margin-based discrimi-
native training. In Proceedings of 5th International
Joint Conference on Natural Language Processing,
pages 474–482. Asian Federation of Natural Lan-
guage Processing.
Yvonne Canning and John Tait. 1999. Syntactic sim-
plification of newspaper text for aphasic readers. In
Proceedings of SIGIR-99 Workshop on Customised
Information Delivery, pages 6–11.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999.
Simplifying text for language-impaired readers. In
Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 269–270.
CCSSO. 2010. Common core state standards for en-
glish language arts &amp; literacy in history/social stud-
ies, science, and technical subjects. appendix B: Text
exemplars and sample performance tasks. Technical
report, National Governors Association Center for
</reference>
<page confidence="0.990235">
295
</page>
<reference confidence="0.999453926605505">
Best Practices, Council of Chief State School Of-
ficers. http://www.corestandards.org/
assets/Appendix_B.pdf.
R. Chandrasekar and B. Srinivas. 1996. Automatic in-
duction of rules for text simplification. Technical
Report IRCS Report 96–30, Upenn, NSF Science
and Technology Center for Research in Cognitive
Science.
R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifica-
tion. In Proceedings of the 16th International Con-
ference on Computational Linguistics (COLING),
pages 1041–1044.
William Coster and David Kauchak. 2011. Simple en-
glish wikipedia: A new text simplification task. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 665–669, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Scott A. Crossley, David F. Dufty, Philip M. McCarthy,
and Danielle S. McNamara. 2007. Toward a new
readability: A mixed model approach. In Danielle S.
McNamara and Greg Trafton, editors, Proceedings
of the 29th annual conference of the Cognitive Sci-
ence Society. Cognitive Science Society.
Johan Falkenjack, Katarina Heimann M¨uhlenbock, and
Arne J¨onsson. 2013. Features indicating readability
in swedish text. In Proceedings of the 19th Nordic
Conference of Computational Linguistics (NODAL-
IDA).
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Michael Flor, Beata Beigman Klebanov, and Kath-
leen M. Sheehan. 2013. Lexical tightness and text
complexity. In Proceedings of the Second Workshop
on Natural Language Processing for Improving Tex-
tual Accessibility.
Caroline Gasperin, Lucia Specia, Tiago F. Pereira, and
Sandra M. Aluisio. 2009. Learning when to sim-
plify sentences for natural text simplification. In
Encontro Nacional de Inteligˆencia Artificial (ENIA-
2009).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
In The SIGKDD Explorations, volume 11, pages 10–
18.
Julia Hancke, Detmar Meurers, and Sowmya Vajjala.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING), pages 1063–
1080, Mumbay, India.
Siddhartha Jonnalagadda and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of The 3rd
International Symposium on Languages in Biology
and Medicine, Jeju Island, South Korea, November
8-10, 2009.
Sigrid Klerke and Anders Søgaard. 2013. Simple,
readable sub-sentences. In Proceedings of the ACL
Student Research Workshop.
Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior Research Meth-
ods, 44(4):978–990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Julie Medero and Marie Ostendorf. 2011. Identifying
targets for syntactic simplification. In ISCA Interna-
tional Workshop on Speech and Language Technol-
ogy in Education (SLaTE 2011).
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proceedings of the NAACL
HLT 2010 Workshop on Computational Linguistics
and Writing: Writing Processes and Authoring Aids,
CL&amp;W ’10, pages 42–50, Stroudsburg, PA, USA.
Association for Computational Linguistics.
J. Nelson, C. Perfetti, D. Liben, and M. Liben. 2012.
Measures of text difficulty: Testing their predic-
tive value for grade levels and student performance.
Technical report, The Council of Chief State School
Officers.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Kathleen M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text
complexity classifications that are aligned with tar-
geted text complexity standards. Technical Report
RR-10-28, ETS, December.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In In Proceedings of the Lan-
guage Engineering Conference 2002 (LEC 2002).
Advaith Siddharthan. 2004. Syntactic simplification
and text cohesion. Technical Report UCAM-CL-
TR-597, University of Cambridge Computer Labo-
ratory.
Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language (PROPOR’10).
</reference>
<page confidence="0.97706">
296
</page>
<reference confidence="0.999155681818182">
Sara Stymne, J¨org Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical machine trans-
lation with readability constraints. In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013).
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL, pages
252–259, Edmonton, Canada.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
In Proceedings of the 7th Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 163—-173.
Sowmya Vajjala and Detmar Meurers. 2013. On the
applicability of readability models to web texts. In
Proceedings of the Second Workshop on Predicting
and Improving Text Readability for Target Reader
Populations.
M.D. Wilson. 1988. The MRC psycholinguistic
database: Machine readable dictionary, version 2.
Behavioural Research Methods, Instruments and
Computers, 20(1):6–11.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of ACL
2012.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics (COLING), August 2010. Beijing, China.
Sanja ˇStajner, Biljana Drndarevic, and Horaccio Sag-
gion. 2013. Corpus-based sentence deletion and
split decisions for spanish text simplification. In CI-
CLing 2013: The 14th International Conference on
Intelligent Text Processing and Computational Lin-
guistics.
</reference>
<page confidence="0.997371">
297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.433049">
<title confidence="0.999515">Assessing the relative reading level of sentence pairs for text simplification</title>
<author confidence="0.641352">Sowmya Vajjala</author>
<author confidence="0.641352">Detmar</author>
<affiliation confidence="0.58221">LEAD Graduate School, Seminar f¨ur Universit¨at</affiliation>
<abstract confidence="0.998689454545455">While the automatic analysis of the readability of texts has a long history, the use of readability assessment for text simplification has received only little attention so far. In this paper, we explore readability models for identifying differences in the reading levels of simplified and unsimplified versions of sentences. Our experiments show that a relative ranking is preferable to an absolute binary one and that the accuracy of identifying relative simplification depends on the initial reading level of the unsimplified version. The approach is particularly successful in classifying the relative reading level of harder sentences. In terms of practical relevance, the approach promises to be useful for identifying particularly relevant targets for simplification and to evaluate simplifications given specific readability constraints.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sandra Aluisio</author>
<author>Lucia Specia</author>
<author>Caroline Gasperin</author>
<author>Carolina Scarton</author>
</authors>
<title>Readability assessment for text simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="4941" citStr="Aluisio et al., 2010" startWordPosition="741" endWordPosition="744">ceived only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; Gasperin et al., 2009; ˇStajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evaluation that a document level, regression-based readability model successfully identifies the differences between simplified vs. unsimplified sentences. This approach can be useful in various stages of simplification ranging from identifying simplification targets to the evaluation of simplification outcomes. 2 Corpora and Features 2.1 Corpora We built and te</context>
</contexts>
<marker>Aluisio, Specia, Gasperin, Scarton, 2010</marker>
<rawString>Sandra Aluisio, Lucia Specia, Caroline Gasperin, and Carolina Scarton. 2010. Readability assessment for text simplification. In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>L Gulikers</author>
</authors>
<title>The CELEX lexical databases. CDROM,</title>
<date>1995</date>
<note>http://www.ldc.upenn.edu/Catalog/ readme_files/celex.readme.html.</note>
<contexts>
<context position="9055" citStr="Baayen et al., 1995" startWordPosition="1392" endWordPosition="1395">categories (e.g., NP, VP, PP), the average lengths 289 of phrases, the parse tree height, and the number of constituents per subtree. None of the syntactic features refer to specific words or lemmas. We used the BerkeleyParser (Petrov and Klein, 2007) for generating the parse trees and the Tregex tool (Levy and Andrew, 2006) to count the occurrences of the syntactic patterns. While the first two feature sets are based on our previous work, as far as we know the next two are used in readability assessment for the first time. Features from the Celex Lexical Database: The Celex Lexical Database (Baayen et al., 1995) is a database consisting of information about morphological, syntactic, orthographic and phonological properties of words along with word frequencies in various corpora. Celex for English contains this information for more than 50,000 lemmas. An overview of the fields in the Celex database is provided online1 and the Celex user manual2. We used the morphological and syntactic properties of lemmas as features. We excluded word frequency statistics and properties which consisted of word strings. In all, we used 35 morphological and 49 syntactic properties that were expressed using either charac</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1995</marker>
<rawString>R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The CELEX lexical databases. CDROM, http://www.ldc.upenn.edu/Catalog/ readme_files/celex.readme.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Tris: A statistical sentence simplifier with log-linear models and margin-based discriminative training.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>474--482</pages>
<contexts>
<context position="4108" citStr="Bach et al., 2011" startWordPosition="617" endWordPosition="620">al., 1996; Chandrasekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napole</context>
</contexts>
<marker>Bach, Gao, Vogel, Waibel, 2011</marker>
<rawString>Nguyen Bach, Qin Gao, Stephan Vogel, and Alex Waibel. 2011. Tris: A statistical sentence simplifier with log-linear models and margin-based discriminative training. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 474–482. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvonne Canning</author>
<author>John Tait</author>
</authors>
<title>Syntactic simplification of newspaper text for aphasic readers.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR-99 Workshop on Customised Information Delivery,</booktitle>
<pages>6--11</pages>
<contexts>
<context position="1509" citStr="Canning and Tait, 1999" startWordPosition="225" endWordPosition="229">practical relevance, the approach promises to be useful for identifying particularly relevant targets for simplification and to evaluate simplifications given specific readability constraints. 1 Introduction Text simplification essentially is the process of rewriting a given text to make it easier to process for a given audience. The target audience can either be human users trying to understand a text or machine applications, such as a parser analyzing text. Text simplification has been used in a variety of application scenarios, from providing simplified newspaper texts for aphasic readers (Canning and Tait, 1999) to supporting the extraction of protein-protein interactions in the biomedical domain (Jonnalagadda and Gonzalez, 2009). A related field of research is automatic readability assessment, which can be useful for evaluating text simplification. It can also be relevant for intermediate simplification steps, such as the identification of target sentences for simplification. Yet, so far there has only been little research connecting the two subfields, possibly because readability research typically analyzes documents, whereas simplification approaches generally targeted lexical and syntactic aspect</context>
</contexts>
<marker>Canning, Tait, 1999</marker>
<rawString>Yvonne Canning and John Tait. 1999. Syntactic simplification of newspaper text for aphasic readers. In Proceedings of SIGIR-99 Workshop on Customised Information Delivery, pages 6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Darren Pearce</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Simplifying text for language-impaired readers.</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>269--270</pages>
<contexts>
<context position="3633" citStr="Carroll et al., 1999" startWordPosition="552" endWordPosition="555">odel. Section 3 discusses the performance of our readability model in comparison with other existing systems. Sections 4 and 5 present our experiments with sentence level readability analysis and the results. In Section 6 we present our conclusions and plans for future work. 1.1 Related Work Research into automatic text simplification essentially started with the idea of splitting long sentences into multiple shorter sentences to improve parsing efficiency (Chandrasekar et al., 1996; Chandrasekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts </context>
</contexts>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>John Carroll, Guido Minnen, Darren Pearce, Yvonne Canning, Siobhan Devlin, and John Tait. 1999. Simplifying text for language-impaired readers. In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 269–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CCSSO</author>
</authors>
<title>Common core state standards for english language arts &amp; literacy in history/social studies, science, and technical subjects. appendix B: Text exemplars and sample performance tasks.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>National Governors Association Center for</institution>
<contexts>
<context position="6331" citStr="CCSSO, 2010" startWordPosition="959" endWordPosition="960">Meurers, 2012) consists of 3,125 articles belonging to five reading levels, with 625 articles per reading level. The texts compiled from the WeeklyReader and BBC Bitesize target English language learners from 7 to 16 years of age. We used this corpus to build our primary readability model by mapping the five reading levels in the corpus to a scale of 1–5 and considered readability assessment as a regression problem. Common Core Standards Corpus: This corpus consists of 168 English texts available from the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. Wiki-SimpleWiki Sentence Aligned Corpus: This corpus was created by Zhu et al. (2010) and consists of ∼100k aligned sentence pairs drawn from Wikipedia and Simple English Wikipedia. We removed all pairs of identical sent</context>
</contexts>
<marker>CCSSO, 2010</marker>
<rawString>CCSSO. 2010. Common core state standards for english language arts &amp; literacy in history/social studies, science, and technical subjects. appendix B: Text exemplars and sample performance tasks. Technical report, National Governors Association Center for</rawString>
</citation>
<citation valid="false">
<authors>
<author>Best Practices</author>
</authors>
<title>Council of Chief State School Officers.</title>
<note>http://www.corestandards.org/ assets/Appendix_B.pdf.</note>
<marker>Practices, </marker>
<rawString>Best Practices, Council of Chief State School Officers. http://www.corestandards.org/ assets/Appendix_B.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>B Srinivas</author>
</authors>
<title>Automatic induction of rules for text simplification.</title>
<date>1996</date>
<booktitle>Upenn, NSF Science and Technology Center for Research in Cognitive Science.</booktitle>
<tech>Technical Report IRCS Report 96–30,</tech>
<contexts>
<context position="3534" citStr="Chandrasekar and Srinivas, 1996" startWordPosition="536" endWordPosition="539"> related research. Section 2 then describes the corpora and the features we used to construct our readability model. Section 3 discusses the performance of our readability model in comparison with other existing systems. Sections 4 and 5 present our experiments with sentence level readability analysis and the results. In Section 6 we present our conclusions and plans for future work. 1.1 Related Work Research into automatic text simplification essentially started with the idea of splitting long sentences into multiple shorter sentences to improve parsing efficiency (Chandrasekar et al., 1996; Chandrasekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1996</marker>
<rawString>R. Chandrasekar and B. Srinivas. 1996. Automatic induction of rules for text simplification. Technical Report IRCS Report 96–30, Upenn, NSF Science and Technology Center for Research in Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>Christine Doran</author>
<author>B Srinivas</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1041--1044</pages>
<contexts>
<context position="3500" citStr="Chandrasekar et al., 1996" startWordPosition="532" endWordPosition="535">, we first discuss relevant related research. Section 2 then describes the corpora and the features we used to construct our readability model. Section 3 discusses the performance of our readability model in comparison with other existing systems. Sections 4 and 5 present our experiments with sentence level readability analysis and the results. In Section 6 we present our conclusions and plans for future work. 1.1 Related Work Research into automatic text simplification essentially started with the idea of splitting long sentences into multiple shorter sentences to improve parsing efficiency (Chandrasekar et al., 1996; Chandrasekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et a</context>
</contexts>
<marker>Chandrasekar, Doran, Srinivas, 1996</marker>
<rawString>R. Chandrasekar, Christine Doran, and B. Srinivas. 1996. Motivations and methods for text simplification. In Proceedings of the 16th International Conference on Computational Linguistics (COLING), pages 1041–1044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Simple english wikipedia: A new text simplification task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>665--669</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4134" citStr="Coster and Kauchak, 2011" startWordPosition="621" endWordPosition="624">ekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and id</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011. Simple english wikipedia: A new text simplification task. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 665–669, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A Crossley</author>
<author>David F Dufty</author>
<author>Philip M McCarthy</author>
<author>Danielle S McNamara</author>
</authors>
<title>Toward a new readability: A mixed model approach.</title>
<date>2007</date>
<booktitle>Proceedings of the 29th annual conference of the Cognitive Science Society.</booktitle>
<editor>In Danielle S. McNamara and Greg Trafton, editors,</editor>
<publisher>Cognitive Science Society.</publisher>
<contexts>
<context position="10456" citStr="Crossley et al., 2007" startWordPosition="1614" endWordPosition="1617">atures of the words, their morphological origins and complexity. The syntactic properties of the words in Celex describe the attributes of a word depending on its parts of speech. For the morphological and syntactic properties from this database, we used the proportion of occurrences per text as features. For example, the ratio of transitive verbs, complex morphological words, and vocative nouns to number of words. Lemmas from the text that do not have entries in the Celex database were ignored. Word frequency statistics from Celex have been used before to analyze text difficulty in the past (Crossley et al., 2007). However, to our knowledge, this is the first time morphological and syntactic information from the Celex database is used for readability assessment. Psycholinguistic features: The MRC Psycholinguistic Database (Wilson, 1988) is a freely available, machine readable dictionary annotated 1http://celex.mpi.nl/help/elemmas.html 2http://catalog.ldc.upenn.edu/docs/LDC96L14 with 26 linguistic and psychological attributes of about 1.5 million words.3 We used the measures of word familiarity, concreteness, imageability, meaningfulness, and age of acquisition from this database as our features, by enc</context>
</contexts>
<marker>Crossley, Dufty, McCarthy, McNamara, 2007</marker>
<rawString>Scott A. Crossley, David F. Dufty, Philip M. McCarthy, and Danielle S. McNamara. 2007. Toward a new readability: A mixed model approach. In Danielle S. McNamara and Greg Trafton, editors, Proceedings of the 29th annual conference of the Cognitive Science Society. Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Falkenjack</author>
<author>Katarina Heimann M¨uhlenbock</author>
<author>Arne J¨onsson</author>
</authors>
<title>Features indicating readability in swedish text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA).</booktitle>
<marker>Falkenjack, M¨uhlenbock, J¨onsson, 2013</marker>
<rawString>Johan Falkenjack, Katarina Heimann M¨uhlenbock, and Arne J¨onsson. 2013. Features indicating readability in swedish text. In Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijun Feng</author>
</authors>
<title>Automatic Readability Assessment.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>City University of New</institution>
<location>York (CUNY).</location>
<contexts>
<context position="15391" citStr="Feng, 2010" startWordPosition="2360" endWordPosition="2361">om/atos 8http://questarai.com/Products/DRPProgram 9http://lexile.com 10http://readingmaturity.com 11http://naeptba.ets.org/SourceRater3 Our document-level readability model does not include discourse features, so all 151 features can also be computed for individual sentences. We built a binary sentence-level classification model using WEKA’s Sequential Minimal Optimization (SMO) for training an SVM in WEKA on the Wiki-SimpleWiki sentence aligned corpus. The choice of algorithm was primarily motivated by the fact that it was shown to be efficient in previous work on readability classification (Feng, 2010; Hancke et al., 2012; Falkenjack et al., 2013). The accuracy of the resulting classifier determining whether a given sentence is simple or hard was disappointing, reaching only 66% accuracy in a 10-fold cross-validation setting. Experiments with different classification algorithms did not yield any more promising results. To study how the classification performance is impacted by the size of the training data, we experimented with different sizes, using SMO as the classification algorithm. Figure 1 shows the classification accuracy with different training set sizes. 68.5 68 classification acc</context>
</contexts>
<marker>Feng, 2010</marker>
<rawString>Lijun Feng. 2010. Automatic Readability Assessment. Ph.D. thesis, City University of New York (CUNY).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Flor</author>
<author>Beata Beigman Klebanov</author>
<author>Kathleen M Sheehan</author>
</authors>
<title>Lexical tightness and text complexity.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Workshop on Natural Language Processing for Improving Textual Accessibility.</booktitle>
<contexts>
<context position="6654" citStr="Flor et al., 2013" startWordPosition="1016" endWordPosition="1019"> levels in the corpus to a scale of 1–5 and considered readability assessment as a regression problem. Common Core Standards Corpus: This corpus consists of 168 English texts available from the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. Wiki-SimpleWiki Sentence Aligned Corpus: This corpus was created by Zhu et al. (2010) and consists of ∼100k aligned sentence pairs drawn from Wikipedia and Simple English Wikipedia. We removed all pairs of identical sentences, i.e., where the Wiki and the SimpleWiki versions are the same. We used this corpus to study reading level assessment at the sentence level. 2.2 Features We started with the feature set described in Vajjala and Meurers (2012) and added new features focusing on the morphological and psycholinguistic properties of wor</context>
<context position="13257" citStr="Flor et al. (2013)" startWordPosition="2041" endWordPosition="2044">in the next section. To be able to compare our document-level results with other contemporary readability approaches, we need a common test corpus. Nelson et al. (2012) compared several state of the art readability assessment systems using five test sets and showed that the systems that went beyond traditional formulae and wordlists performed better 3http://www.psych.rl.ac.uk 4http://crr.ugent.be/archives/806 5http://projects.csail.mit.edu/jwi 290 on these real-life test sets. We tested our model on one of the publicly accessible test corpora from this study, the Common Core Standards Corpus. Flor et al. (2013) used the same test set to study a measure of lexical tightness, providing a further performance reference. Table 1 compares the performance of our model to that reported for several commercial (indicated in italics) and research systems on this test set. Nelson et al. (2012) used Spearman’s Rank Correlation and Flor et al. (2013) used Pearson Correlation as evaluation metrics. To facilitate comparison, for our approach we provide both measures. System Spearman Pearson Our System 0.69 0.61 Nelson et al. (2012): REAP6 0.54 – ATOS7 0.59 – DRP8 0.53 – Lexile9 0.50 – Reading Maturity10 0.69 – Sour</context>
</contexts>
<marker>Flor, Klebanov, Sheehan, 2013</marker>
<rawString>Michael Flor, Beata Beigman Klebanov, and Kathleen M. Sheehan. 2013. Lexical tightness and text complexity. In Proceedings of the Second Workshop on Natural Language Processing for Improving Textual Accessibility.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Gasperin</author>
<author>Lucia Specia</author>
<author>Tiago F Pereira</author>
<author>Sandra M Aluisio</author>
</authors>
<title>Learning when to simplify sentences for natural text simplification.</title>
<date>2009</date>
<booktitle>In Encontro Nacional de Inteligˆencia Artificial (ENIA2009).</booktitle>
<contexts>
<context position="4964" citStr="Gasperin et al., 2009" startWordPosition="745" endWordPosition="748">ention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; Gasperin et al., 2009; ˇStajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evaluation that a document level, regression-based readability model successfully identifies the differences between simplified vs. unsimplified sentences. This approach can be useful in various stages of simplification ranging from identifying simplification targets to the evaluation of simplification outcomes. 2 Corpora and Features 2.1 Corpora We built and tested our document and s</context>
</contexts>
<marker>Gasperin, Specia, Pereira, Aluisio, 2009</marker>
<rawString>Caroline Gasperin, Lucia Specia, Tiago F. Pereira, and Sandra M. Aluisio. 2009. Learning when to simplify sentences for natural text simplification. In Encontro Nacional de Inteligˆencia Artificial (ENIA2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<booktitle>In The SIGKDD Explorations,</booktitle>
<volume>11</volume>
<pages>10--18</pages>
<contexts>
<context position="12217" citStr="Hall et al., 2009" startWordPosition="1880" endWordPosition="1883"> We excluded auxiliary verbs for this calculation as they tend to have multiple senses that do not necessarily contribute to reading difficulty. Combining the four feature groups, we encode 151 features for each text. 3 Document-Level Readability Model In our first experiment, we tested the documentlevel readability model based on the 151 features using the WeeBit corpus. Under a regression perspective on readability, we evaluated the approach using Pearson Correlation and Root Mean Square Error (RMSE) in a 10-fold cross-validation setting. We used the SMO Regression implementation from WEKA (Hall et al., 2009) and achieved a Pearson correlation of 0.92 and an RMSE of 0.53. The document-level performance of our 151 feature model is virtually identical to that of the regression model we presented in Vajjala and Meurers (2013). But compared to our previous work, the Celex and psycholinguistic features we included here provide more lexical information that is meaningful to compute even for the sentencelevel analysis we turn to in the next section. To be able to compare our document-level results with other contemporary readability approaches, we need a common test corpus. Nelson et al. (2012) compared </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. In The SIGKDD Explorations, volume 11, pages 10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hancke</author>
<author>Detmar Meurers</author>
<author>Sowmya Vajjala</author>
</authors>
<title>Readability classification for german using lexical, syntactic, and morphological features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1063--1080</pages>
<location>Mumbay, India.</location>
<contexts>
<context position="15412" citStr="Hancke et al., 2012" startWordPosition="2362" endWordPosition="2365">p://questarai.com/Products/DRPProgram 9http://lexile.com 10http://readingmaturity.com 11http://naeptba.ets.org/SourceRater3 Our document-level readability model does not include discourse features, so all 151 features can also be computed for individual sentences. We built a binary sentence-level classification model using WEKA’s Sequential Minimal Optimization (SMO) for training an SVM in WEKA on the Wiki-SimpleWiki sentence aligned corpus. The choice of algorithm was primarily motivated by the fact that it was shown to be efficient in previous work on readability classification (Feng, 2010; Hancke et al., 2012; Falkenjack et al., 2013). The accuracy of the resulting classifier determining whether a given sentence is simple or hard was disappointing, reaching only 66% accuracy in a 10-fold cross-validation setting. Experiments with different classification algorithms did not yield any more promising results. To study how the classification performance is impacted by the size of the training data, we experimented with different sizes, using SMO as the classification algorithm. Figure 1 shows the classification accuracy with different training set sizes. 68.5 68 classification accuracy (in %) 67.5 67 </context>
</contexts>
<marker>Hancke, Meurers, Vajjala, 2012</marker>
<rawString>Julia Hancke, Detmar Meurers, and Sowmya Vajjala. 2012. Readability classification for german using lexical, syntactic, and morphological features. In Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 1063– 1080, Mumbay, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddhartha Jonnalagadda</author>
<author>Graciela Gonzalez</author>
</authors>
<title>Sentence simplification aids protein-protein interaction extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of The 3rd International Symposium on Languages in Biology</booktitle>
<contexts>
<context position="1629" citStr="Jonnalagadda and Gonzalez, 2009" startWordPosition="242" endWordPosition="245">ification and to evaluate simplifications given specific readability constraints. 1 Introduction Text simplification essentially is the process of rewriting a given text to make it easier to process for a given audience. The target audience can either be human users trying to understand a text or machine applications, such as a parser analyzing text. Text simplification has been used in a variety of application scenarios, from providing simplified newspaper texts for aphasic readers (Canning and Tait, 1999) to supporting the extraction of protein-protein interactions in the biomedical domain (Jonnalagadda and Gonzalez, 2009). A related field of research is automatic readability assessment, which can be useful for evaluating text simplification. It can also be relevant for intermediate simplification steps, such as the identification of target sentences for simplification. Yet, so far there has only been little research connecting the two subfields, possibly because readability research typically analyzes documents, whereas simplification approaches generally targeted lexical and syntactic aspects at the sentence level. In this paper, we attempt to bridge this gap between readability and simplification by studying</context>
</contexts>
<marker>Jonnalagadda, Gonzalez, 2009</marker>
<rawString>Siddhartha Jonnalagadda and Graciela Gonzalez. 2009. Sentence simplification aids protein-protein interaction extraction. In Proceedings of The 3rd International Symposium on Languages in Biology and Medicine, Jeju Island, South Korea, November 8-10, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sigrid Klerke</author>
<author>Anders Søgaard</author>
</authors>
<title>Simple, readable sub-sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL Student Research Workshop.</booktitle>
<contexts>
<context position="4582" citStr="Klerke and Søgaard, 2013" startWordPosition="691" endWordPosition="694">uistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; Gasperin et al., 2009; ˇStajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evalu</context>
</contexts>
<marker>Klerke, Søgaard, 2013</marker>
<rawString>Sigrid Klerke and Anders Søgaard. 2013. Simple, readable sub-sentences. In Proceedings of the ACL Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Kuperman</author>
<author>Hans Stadthagen-Gonzalez</author>
<author>Marc Brysbaert</author>
</authors>
<title>Age-of-acquisition ratings for 30,000 english words.</title>
<date>2012</date>
<journal>Behavior Research Methods,</journal>
<volume>44</volume>
<issue>4</issue>
<contexts>
<context position="11115" citStr="Kuperman et al. (2012)" startWordPosition="1699" endWordPosition="1702">the first time morphological and syntactic information from the Celex database is used for readability assessment. Psycholinguistic features: The MRC Psycholinguistic Database (Wilson, 1988) is a freely available, machine readable dictionary annotated 1http://celex.mpi.nl/help/elemmas.html 2http://catalog.ldc.upenn.edu/docs/LDC96L14 with 26 linguistic and psychological attributes of about 1.5 million words.3 We used the measures of word familiarity, concreteness, imageability, meaningfulness, and age of acquisition from this database as our features, by encoding their average values per text. Kuperman et al. (2012) compiled a freely available database that includes Age of Acquisition (AoA) ratings for over 50,000 English words.4 This database was created through crowd sourcing and was compared with several other AoA norms, which are also included in the database. For each of the five AoA norms, we computed the average AoA of words per text. Turning to the final resource used, we included the average number of senses per word as calculated using the MIT Java WordNet Interface as a feature.5 We excluded auxiliary verbs for this calculation as they tend to have multiple senses that do not necessarily contr</context>
</contexts>
<marker>Kuperman, Stadthagen-Gonzalez, Brysbaert, 2012</marker>
<rawString>Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc Brysbaert. 2012. Age-of-acquisition ratings for 30,000 english words. Behavior Research Methods, 44(4):978–990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In 5th International Conference on Language Resources and Evaluation,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="8761" citStr="Levy and Andrew, 2006" startWordPosition="1341" endWordPosition="1344">s from Vajjala and Meurers (2012): mean lengths of various production units (sentence, clause, t-unit), measures of coordination and subordination (e.g., # of coordinate clauses per clause), the presence of particular syntactic structures (e.g., VPs per t-unit), the number of phrases of various categories (e.g., NP, VP, PP), the average lengths 289 of phrases, the parse tree height, and the number of constituents per subtree. None of the syntactic features refer to specific words or lemmas. We used the BerkeleyParser (Petrov and Klein, 2007) for generating the parse trees and the Tregex tool (Levy and Andrew, 2006) to count the occurrences of the syntactic patterns. While the first two feature sets are based on our previous work, as far as we know the next two are used in readability assessment for the first time. Features from the Celex Lexical Database: The Celex Lexical Database (Baayen et al., 1995) is a database consisting of information about morphological, syntactic, orthographic and phonological properties of words along with word frequencies in various corpora. Celex for English contains this information for more than 50,000 lemmas. An overview of the fields in the Celex database is provided on</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In 5th International Conference on Language Resources and Evaluation, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Medero</author>
<author>Marie Ostendorf</author>
</authors>
<title>Identifying targets for syntactic simplification.</title>
<date>2011</date>
<booktitle>In ISCA International Workshop on Speech and Language Technology in Education</booktitle>
<contexts>
<context position="4861" citStr="Medero and Ostendorf, 2011" startWordPosition="731" endWordPosition="734">f a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; Gasperin et al., 2009; ˇStajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evaluation that a document level, regression-based readability model successfully identifies the differences between simplified vs. unsimplified sentences. This approach can be useful in various stages of simplification ranging from identifying simplification targets to the evaluatio</context>
</contexts>
<marker>Medero, Ostendorf, 2011</marker>
<rawString>Julie Medero and Marie Ostendorf. 2011. Identifying targets for syntactic simplification. In ISCA International Workshop on Speech and Language Technology in Education (SLaTE 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Mark Dredze</author>
</authors>
<title>Learning simple wikipedia: a cogitation in ascertaining abecedarian language.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids, CL&amp;W ’10,</booktitle>
<pages>42--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4727" citStr="Napoles and Dredze, 2010" startWordPosition="713" endWordPosition="716">, 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; Gasperin et al., 2009; ˇStajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evaluation that a document level, regression-based readability model successfully identifies the differences between simplified vs. unsimplified sente</context>
</contexts>
<marker>Napoles, Dredze, 2010</marker>
<rawString>Courtney Napoles and Mark Dredze. 2010. Learning simple wikipedia: a cogitation in ascertaining abecedarian language. In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids, CL&amp;W ’10, pages 42–50, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nelson</author>
<author>C Perfetti</author>
<author>D Liben</author>
<author>M Liben</author>
</authors>
<title>Measures of text difficulty: Testing their predictive value for grade levels and student performance. Technical report, The Council of Chief State School Officers.</title>
<date>2012</date>
<contexts>
<context position="6634" citStr="Nelson et al., 2012" startWordPosition="1012" endWordPosition="1015">ping the five reading levels in the corpus to a scale of 1–5 and considered readability assessment as a regression problem. Common Core Standards Corpus: This corpus consists of 168 English texts available from the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. Wiki-SimpleWiki Sentence Aligned Corpus: This corpus was created by Zhu et al. (2010) and consists of ∼100k aligned sentence pairs drawn from Wikipedia and Simple English Wikipedia. We removed all pairs of identical sentences, i.e., where the Wiki and the SimpleWiki versions are the same. We used this corpus to study reading level assessment at the sentence level. 2.2 Features We started with the feature set described in Vajjala and Meurers (2012) and added new features focusing on the morphological and psycholinguist</context>
<context position="12807" citStr="Nelson et al. (2012)" startWordPosition="1979" endWordPosition="1983"> from WEKA (Hall et al., 2009) and achieved a Pearson correlation of 0.92 and an RMSE of 0.53. The document-level performance of our 151 feature model is virtually identical to that of the regression model we presented in Vajjala and Meurers (2013). But compared to our previous work, the Celex and psycholinguistic features we included here provide more lexical information that is meaningful to compute even for the sentencelevel analysis we turn to in the next section. To be able to compare our document-level results with other contemporary readability approaches, we need a common test corpus. Nelson et al. (2012) compared several state of the art readability assessment systems using five test sets and showed that the systems that went beyond traditional formulae and wordlists performed better 3http://www.psych.rl.ac.uk 4http://crr.ugent.be/archives/806 5http://projects.csail.mit.edu/jwi 290 on these real-life test sets. We tested our model on one of the publicly accessible test corpora from this study, the Common Core Standards Corpus. Flor et al. (2013) used the same test set to study a measure of lexical tightness, providing a further performance reference. Table 1 compares the performance of our mo</context>
</contexts>
<marker>Nelson, Perfetti, Liben, Liben, 2012</marker>
<rawString>J. Nelson, C. Perfetti, D. Liben, and M. Liben. 2012. Measures of text difficulty: Testing their predictive value for grade levels and student performance. Technical report, The Council of Chief State School Officers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="8686" citStr="Petrov and Klein, 2007" startWordPosition="1328" endWordPosition="1331">lity classification in the past, so we made use of all the syntactic features from Vajjala and Meurers (2012): mean lengths of various production units (sentence, clause, t-unit), measures of coordination and subordination (e.g., # of coordinate clauses per clause), the presence of particular syntactic structures (e.g., VPs per t-unit), the number of phrases of various categories (e.g., NP, VP, PP), the average lengths 289 of phrases, the parse tree height, and the number of constituents per subtree. None of the syntactic features refer to specific words or lemmas. We used the BerkeleyParser (Petrov and Klein, 2007) for generating the parse trees and the Tregex tool (Levy and Andrew, 2006) to count the occurrences of the syntactic patterns. While the first two feature sets are based on our previous work, as far as we know the next two are used in readability assessment for the first time. Features from the Celex Lexical Database: The Celex Lexical Database (Baayen et al., 1995) is a database consisting of information about morphological, syntactic, orthographic and phonological properties of words along with word frequencies in various corpora. Celex for English contains this information for more than 50</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen M Sheehan</author>
<author>Irene Kostin</author>
<author>Yoko Futagi</author>
<author>Michael Flor</author>
</authors>
<title>Generating automated text complexity classifications that are aligned with targeted text complexity standards.</title>
<date>2010</date>
<tech>Technical Report RR-10-28, ETS,</tech>
<contexts>
<context position="6613" citStr="Sheehan et al., 2010" startWordPosition="1008" endWordPosition="1011">adability model by mapping the five reading levels in the corpus to a scale of 1–5 and considered readability assessment as a regression problem. Common Core Standards Corpus: This corpus consists of 168 English texts available from the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. Wiki-SimpleWiki Sentence Aligned Corpus: This corpus was created by Zhu et al. (2010) and consists of ∼100k aligned sentence pairs drawn from Wikipedia and Simple English Wikipedia. We removed all pairs of identical sentences, i.e., where the Wiki and the SimpleWiki versions are the same. We used this corpus to study reading level assessment at the sentence level. 2.2 Features We started with the feature set described in Vajjala and Meurers (2012) and added new features focusing on the morphologic</context>
</contexts>
<marker>Sheehan, Kostin, Futagi, Flor, 2010</marker>
<rawString>Kathleen M. Sheehan, Irene Kostin, Yoko Futagi, and Michael Flor. 2010. Generating automated text complexity classifications that are aligned with targeted text complexity standards. Technical Report RR-10-28, ETS, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>An architecture for a text simplification system. In</title>
<date>2002</date>
<booktitle>In Proceedings of the Language Engineering Conference</booktitle>
<contexts>
<context position="3652" citStr="Siddharthan, 2002" startWordPosition="556" endWordPosition="558">ses the performance of our readability model in comparison with other existing systems. Sections 4 and 5 present our experiments with sentence level readability analysis and the results. In Section 6 we present our conclusions and plans for future work. 1.1 Related Work Research into automatic text simplification essentially started with the idea of splitting long sentences into multiple shorter sentences to improve parsing efficiency (Chandrasekar et al., 1996; Chandrasekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to b</context>
</contexts>
<marker>Siddharthan, 2002</marker>
<rawString>Advaith Siddharthan. 2002. An architecture for a text simplification system. In In Proceedings of the Language Engineering Conference 2002 (LEC 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Syntactic simplification and text cohesion.</title>
<date>2004</date>
<tech>Technical Report UCAM-CLTR-597,</tech>
<institution>University of Cambridge Computer Laboratory.</institution>
<marker>Siddharthan, 2004</marker>
<rawString>Advaith Siddharthan. 2004. Syntactic simplification and text cohesion. Technical Report UCAM-CLTR-597, University of Cambridge Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Translating from complex to simplified sentences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th international conference on Computational Processing of the Portuguese Language (PROPOR’10).</booktitle>
<contexts>
<context position="4071" citStr="Specia, 2010" startWordPosition="611" endWordPosition="612">ing efficiency (Chandrasekar et al., 1996; Chandrasekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sen</context>
</contexts>
<marker>Specia, 2010</marker>
<rawString>Lucia Specia. 2010. Translating from complex to simplified sentences. In Proceedings of the 9th international conference on Computational Processing of the Portuguese Language (PROPOR’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>J¨org Tiedemann</author>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
</authors>
<title>Statistical machine translation with readability constraints.</title>
<date>2013</date>
<booktitle>In Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA</booktitle>
<contexts>
<context position="4604" citStr="Stymne et al., 2013" startWordPosition="695" endWordPosition="698"> approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; Gasperin et al., 2009; ˇStajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evaluation that a document </context>
</contexts>
<marker>Stymne, Tiedemann, Hardmeier, Nivre, 2013</marker>
<rawString>Sara Stymne, J¨org Tiedemann, Christian Hardmeier, and Joakim Nivre. 2013. Statistical machine translation with readability constraints. In Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>252--259</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="7825" citStr="Toutanova et al., 2003" startWordPosition="1196" endWordPosition="1199">morphological and psycholinguistic properties of words. The features can be broadly classified into four groups. Lexical richness and POS features: We adapted the lexical features from Vajjala and Meurers (2012). This includes measures of lexical richness from Second Language Acquisition (SLA) research and measures of lexical variation (noun, verb, adjective, adverb and modifier variation). In addition, this feature set also includes part-of-speech densities (e.g., the average # of nouns per sentence). The information needed to calculate these features was extracted using the Stanford Tagger (Toutanova et al., 2003). None of the lexical richness and POS features we used refer to specific words or lemmas. Syntactic Complexity features: Parse tree based features and some syntactic complexity measures derived from SLA research proved useful for readability classification in the past, so we made use of all the syntactic features from Vajjala and Meurers (2012): mean lengths of various production units (sentence, clause, t-unit), measures of coordination and subordination (e.g., # of coordinate clauses per clause), the presence of particular syntactic structures (e.g., VPs per t-unit), the number of phrases o</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In HLT-NAACL, pages 252–259, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sowmya Vajjala</author>
<author>Detmar Meurers</author>
</authors>
<title>On improving the accuracy of readability classification using insights from second language acquisition. In</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>163--173</pages>
<contexts>
<context position="5733" citStr="Vajjala and Meurers, 2012" startWordPosition="857" endWordPosition="860">adability analysis and simplification. We show through a cross-corpus evaluation that a document level, regression-based readability model successfully identifies the differences between simplified vs. unsimplified sentences. This approach can be useful in various stages of simplification ranging from identifying simplification targets to the evaluation of simplification outcomes. 2 Corpora and Features 2.1 Corpora We built and tested our document and sentence level readability models using three publicly available text corpora with reading level annotations. WeeBit Corpus: The WeeBit corpus (Vajjala and Meurers, 2012) consists of 3,125 articles belonging to five reading levels, with 625 articles per reading level. The texts compiled from the WeeklyReader and BBC Bitesize target English language learners from 7 to 16 years of age. We used this corpus to build our primary readability model by mapping the five reading levels in the corpus to a scale of 1–5 and considered readability assessment as a regression problem. Common Core Standards Corpus: This corpus consists of 168 English texts available from the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). </context>
<context position="7162" citStr="Vajjala and Meurers (2012)" startWordPosition="1099" endWordPosition="1102">uation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. Wiki-SimpleWiki Sentence Aligned Corpus: This corpus was created by Zhu et al. (2010) and consists of ∼100k aligned sentence pairs drawn from Wikipedia and Simple English Wikipedia. We removed all pairs of identical sentences, i.e., where the Wiki and the SimpleWiki versions are the same. We used this corpus to study reading level assessment at the sentence level. 2.2 Features We started with the feature set described in Vajjala and Meurers (2012) and added new features focusing on the morphological and psycholinguistic properties of words. The features can be broadly classified into four groups. Lexical richness and POS features: We adapted the lexical features from Vajjala and Meurers (2012). This includes measures of lexical richness from Second Language Acquisition (SLA) research and measures of lexical variation (noun, verb, adjective, adverb and modifier variation). In addition, this feature set also includes part-of-speech densities (e.g., the average # of nouns per sentence). The information needed to calculate these features w</context>
</contexts>
<marker>Vajjala, Meurers, 2012</marker>
<rawString>Sowmya Vajjala and Detmar Meurers. 2012. On improving the accuracy of readability classification using insights from second language acquisition. In In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications, pages 163—-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sowmya Vajjala</author>
<author>Detmar Meurers</author>
</authors>
<title>On the applicability of readability models to web texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Workshop on Predicting and Improving Text Readability</booktitle>
<institution>for Target Reader Populations.</institution>
<contexts>
<context position="12435" citStr="Vajjala and Meurers (2013)" startWordPosition="1917" endWordPosition="1921">h text. 3 Document-Level Readability Model In our first experiment, we tested the documentlevel readability model based on the 151 features using the WeeBit corpus. Under a regression perspective on readability, we evaluated the approach using Pearson Correlation and Root Mean Square Error (RMSE) in a 10-fold cross-validation setting. We used the SMO Regression implementation from WEKA (Hall et al., 2009) and achieved a Pearson correlation of 0.92 and an RMSE of 0.53. The document-level performance of our 151 feature model is virtually identical to that of the regression model we presented in Vajjala and Meurers (2013). But compared to our previous work, the Celex and psycholinguistic features we included here provide more lexical information that is meaningful to compute even for the sentencelevel analysis we turn to in the next section. To be able to compare our document-level results with other contemporary readability approaches, we need a common test corpus. Nelson et al. (2012) compared several state of the art readability assessment systems using five test sets and showed that the systems that went beyond traditional formulae and wordlists performed better 3http://www.psych.rl.ac.uk 4http://crr.ugent</context>
</contexts>
<marker>Vajjala, Meurers, 2013</marker>
<rawString>Sowmya Vajjala and Detmar Meurers. 2013. On the applicability of readability models to web texts. In Proceedings of the Second Workshop on Predicting and Improving Text Readability for Target Reader Populations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Wilson</author>
</authors>
<title>The MRC psycholinguistic database: Machine readable dictionary, version 2.</title>
<date>1988</date>
<journal>Behavioural Research Methods, Instruments and Computers,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="10683" citStr="Wilson, 1988" startWordPosition="1649" endWordPosition="1650"> database, we used the proportion of occurrences per text as features. For example, the ratio of transitive verbs, complex morphological words, and vocative nouns to number of words. Lemmas from the text that do not have entries in the Celex database were ignored. Word frequency statistics from Celex have been used before to analyze text difficulty in the past (Crossley et al., 2007). However, to our knowledge, this is the first time morphological and syntactic information from the Celex database is used for readability assessment. Psycholinguistic features: The MRC Psycholinguistic Database (Wilson, 1988) is a freely available, machine readable dictionary annotated 1http://celex.mpi.nl/help/elemmas.html 2http://catalog.ldc.upenn.edu/docs/LDC96L14 with 26 linguistic and psychological attributes of about 1.5 million words.3 We used the measures of word familiarity, concreteness, imageability, meaningfulness, and age of acquisition from this database as our features, by encoding their average values per text. Kuperman et al. (2012) compiled a freely available database that includes Age of Acquisition (AoA) ratings for over 50,000 English words.4 This database was created through crowd sourcing an</context>
</contexts>
<marker>Wilson, 1988</marker>
<rawString>M.D. Wilson. 1988. The MRC psycholinguistic database: Machine readable dictionary, version 2. Behavioural Research Methods, Instruments and Computers, 20(1):6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to simplify sentences with quasi-synchronous grammar and integer programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4162" citStr="Woodsend and Lapata, 2011" startWordPosition="625" endWordPosition="628">This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transfo</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal van den Bosch</author>
<author>Emiel Krahmer</author>
</authors>
<title>Sentence simplification by monolingual machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Wubben, van den Bosch, Krahmer, 2012</marker>
<rawString>Sander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhemin Zhu</author>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>A monolingual tree-based translation model for sentence simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of The 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="4089" citStr="Zhu et al., 2010" startWordPosition="613" endWordPosition="616"> (Chandrasekar et al., 1996; Chandrasekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their re</context>
<context position="6796" citStr="Zhu et al. (2010)" startWordPosition="1040" endWordPosition="1043">s consists of 168 English texts available from the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. Wiki-SimpleWiki Sentence Aligned Corpus: This corpus was created by Zhu et al. (2010) and consists of ∼100k aligned sentence pairs drawn from Wikipedia and Simple English Wikipedia. We removed all pairs of identical sentences, i.e., where the Wiki and the SimpleWiki versions are the same. We used this corpus to study reading level assessment at the sentence level. 2.2 Features We started with the feature set described in Vajjala and Meurers (2012) and added new features focusing on the morphological and psycholinguistic properties of words. The features can be broadly classified into four groups. Lexical richness and POS features: We adapted the lexical features from Vajjala a</context>
</contexts>
<marker>Zhu, Bernhard, Gurevych, 2010</marker>
<rawString>Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of The 23rd International Conference on Computational Linguistics (COLING), August 2010. Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanja ˇStajner</author>
<author>Biljana Drndarevic</author>
<author>Horaccio Saggion</author>
</authors>
<title>Corpus-based sentence deletion and split decisions for spanish text simplification.</title>
<date>2013</date>
<booktitle>In CICLing 2013: The 14th International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<marker>ˇStajner, Drndarevic, Saggion, 2013</marker>
<rawString>Sanja ˇStajner, Biljana Drndarevic, and Horaccio Saggion. 2013. Corpus-based sentence deletion and split decisions for spanish text simplification. In CICLing 2013: The 14th International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>