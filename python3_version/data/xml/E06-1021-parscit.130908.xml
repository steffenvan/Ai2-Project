<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000065">
<title confidence="0.96377">
Towards Robust Context-Sensitive Sentence Alignment for Monolingual
Corpora
</title>
<author confidence="0.320435">
Rani Nelken and Stuart M. Shieber
</author>
<affiliation confidence="0.281164">
Division of Engineering and Applied Sciences
Harvard University
</affiliation>
<address confidence="0.478094">
33 Oxford St.
Cambridge, MA 02138
</address>
<email confidence="0.997039">
nelken,shieber @deas.harvard.edu
</email>
<sectionHeader confidence="0.997356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952285714286">
Aligning sentences belonging to compa-
rable monolingual corpora has been sug-
gested as a first step towards training
text rewriting algorithms, for tasks such
as summarization or paraphrasing. We
present here a new monolingual sen-
tence alignment algorithm, combining a
sentence-based TF*IDF score, turned into
a probability distribution using logistic re-
gression, with a global alignment dynamic
programming algorithm. Our approach
provides a simpler and more robust solu-
tion achieving a substantial improvement
in accuracy over existing systems.
</bodyText>
<sectionHeader confidence="0.999508" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997873125">
Sentence-aligned bilingual corpora are a crucial
resource for training statistical machine trans-
lation systems. Several authors have sug-
gested that large-scale aligned monolingual cor-
pora could be similarly used to advance the perfor-
mance of monolingual text-to-text rewriting sys-
tems, for tasks including summarization (Knight
and Marcu, 2000; Jing, 2002) and paraphras-
ing (Barzilay and Elhadad, 2003; Quirk et al.,
2004). Unlike bilingual corpora, such as the Cana-
dian Hansard corpus, which are relatively rare, it is
now fairly easy to amass corpora of related mono-
lingual documents. For instance, with the ad-
vent of news aggregator services such as “Google
News”, one can readily collect multiple news sto-
ries covering the same news item (Dolan et al.,
2004). Utilizing such a resource requires align-
ing related documents at a finer level of resolu-
tion, identifying which sentences from one docu-
ment align with which sentences from the other.
Previous work has shown that aligning related
monolingual documents is quite different from
the well-studied multi-lingual alignment task.
Whereas documents in a bilingual corpus are typ-
ically very closely aligned, monolingual corpora
exhibit a much looser level of alignment, with
similar content expressed using widely divergent
wording, grammatical form, and sentence order.
Consequently, many of the simple surface-based
methods that have proven to be so successful in
bilingual sentence alignment, such as correlation
of sentence length, linearity of alignment, and a
predominance of one-to-one sentence mapping,
are much less likely to be effective for monolin-
gual sentence alignment.
Barzilay and Elhadad (2003) suggested that
these disadvantages could be at least partially off-
set by the recurrence of the same lexical items in
document pairs. Indeed, they showed that a sim-
ple cosine word-overlap score is a good baseline
for the task, outperforming much more sophisti-
cated methods. They also observed that context is
a powerful factor in determining alignment. They
illustrated this on a corpus of Encyclopedia Bri-
tannica entries describing world cities, where each
entry comes in two flavors, the comprehensive en-
cyclopedia entry, and a shorter and simpler ele-
mentary version. Barzilay and Elhadad used con-
text in two different forms. First, using inter-
document context, they took advantage of com-
monalities in the topical structure of the encyclo-
pedia entries to identify paragraphs that are likely
to be about the same topic. They then took ad-
vantage of intra-document context by using dy-
namic programming to locally align sequences of
sentences belonging to paragraphs about the same
topic, yielding improved accuracy on the corpus.
While powerful, such commonalities in document
structure appear to be a special feature of the
Britannica corpus, and therefore cannot be relied
upon for other corpora.
In this paper we present a novel algorithm for
sentence alignment in monolingual corpora. At
the core of the algorithm is a classical similar-
</bodyText>
<page confidence="0.996863">
161
</page>
<bodyText confidence="0.999349923076923">
ity score based on differentially weighting words
according to their Term Frequency-Inverse Doc-
ument Frequency (TF*IDF) (Sp¨arck-Jones, 1972;
Salton and Buckley, 1988). We treat sentences as
documents, and the collection of sentences in the
two documents being compared as the document
collection, and use this score to estimate the prob-
ability that two sentences are aligned using logis-
tic regression. Surprisingly, this approach by it-
self yields competitive accuracy, yielding the same
level of accuracy as Barzilay and Elhadad’s algo-
rithm, and higher than all previous approaches on
the Britannica corpus. Such matching, however,
is still noisy. We further improve accuracy by us-
ing a global alignment dynamic programming al-
gorithm, which prunes many spurious matches.
Our approach validates Barzilay and Elhadad’s
observation regarding the utility of incorporating
context. In fact, we are able to extract more infor-
mation out of the intra-document context. First, by
using TF*IDF at the level of sentences, we weigh
words in a sentence with respect to other sentences
of the document. Second, global alignment takes
advantage of (noisy) linear order of sentences. We
make no use of inter-document context, and in par-
ticular make no assumptions about common topi-
cal structure that are unique to the Britannica cor-
pus, thus ensuring the scalability of the approach.
Indeed, we successfully apply our algorithm to
a very different corpus, the three Synoptic gospels
of the New Testament: Matthew, Mark, and Luke.
Putting aside any religious or theological signifi-
cance of these texts, they offer an excellent data
source for studying alignment, since they contain
many parallels, which have been conveniently an-
notated by bible scholars (Aland, 1985). Our algo-
rithm achieves a significant improvement over the
baseline for this corpus as well, demonstrating the
general applicability of our approach.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99177356">
Several authors have tackled the monolingual sen-
tence correspondence problem. SimFinder (Hatzi-
vassiloglou et al., 1999; Hatzivassiloglou et al.,
2001) examined 43 different features that could
potentially help determine the similarity of two
short text units (sentences or paragraphs). Of
these, they automatically selected 11 features, in-
cluding word overlap, synonymy as determined
by WordNet (Fellbaum, 1998), matching proper
nouns and noun phrases, and sharing semantic
classes of verbs (Levin, 1993).
The Decomposition method (Jing, 2002) re-
lies on the observation that document summaries
are often constructed by extracting sentence frag-
ments from the document. It attempts to identify
such extracts, using a Hidden Markov Model of
the process of extracting words. The HMM uses
features of word identity and document position,
in which transition probabilities are based on lo-
cality assumptions. For instance, after a word is
extracted, an adjacent word or one that belongs to
a nearby sentence is more likely to be extracted
than one that is further away.
Barzilay and Elhadad (2003) apply a 4-step al-
gorithm:
</bodyText>
<listItem confidence="0.988745444444444">
1. Cluster the paragraphs of the training docu-
ments into topic-specific clusters, based on
word overlap. For instance, paragraphs in
the Britannica city entries describing climate
might cluster together.
2. Learn mapping rules between paragraphs of
the full and elementary versions, taking the
word-overlap and the clusters as features.
3. Given a new pair of texts, identify sentence
</listItem>
<bodyText confidence="0.846693043478261">
pairs with high overlap, and take these to be
aligned. Then, classify paragraphs accord-
ing to the clusters learned in Step 1, and use
the mapping rules of Step 2 to match pairs of
paragraphs between the documents.
4. Finally, take advantage of the paragraph clus-
tering and mapping, by locally aligning only
sentences belonging to mapped paragraph
pairs.
Dolan et al. (2004) used Web-aggregated news
stories to learn both sentence-level and word-level
alignments. Having collected a large corpus of
clusters of related news stories from Google and
MSN news aggregator services, they first seek re-
lated sentences, using two methods. First, using
a high Levenshtein distance score they identify
139K sentence pairs of which about 16.7% are es-
timated to be unrelated (using human evaluation of
a sample). Second, assuming that the first two sen-
tences of related news stories should be matched,
provided they have a high enough word-overlap,
yields 214K sentence pairs of which about 40%
are estimated to be unrelated. No recall estimates
</bodyText>
<page confidence="0.996755">
162
</page>
<bodyText confidence="0.999936210526316">
are provided; however, with the release of the an-
notated Microsoft Research Paraphrase Corpus,1
it is apparent that Dolan et al. are seeking much
more tightly related pairs of sentences than Barzi-
lay and Elhadad, ones that are virtually semanti-
cally equivalent. In subsequent work, the same au-
thors (Quirk et al., 2004) used such matched sen-
tence pairs to train Giza++ (Och and Ney, 2003)
on word-level alignment.
The recent PASCAL “Recognizing Textual En-
tailment” (RTE) challenge (Dagan et al., 2005) fo-
cused on the problem of determining whether one
sentence entails another. Beyond the difference in
the definition of the required relation between sen-
tences, the RTE challenge focuses on isolated sen-
tence pairs, as opposed to sentences within a doc-
ument context. The task was judged to be quite
difficult, with many of the systems achieving rela-
tively low accuracy.
</bodyText>
<sectionHeader confidence="0.997776" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999902153846154">
The Britannica corpus, collected and annotated
by Barzilay and Elhadad (2003), consists of 103
pairs of comprehensive and elementary encyclope-
dia entries describing major world cities. Twenty
of these document pairs were annotated by human
judges, who were asked to mark sentence pairs
that contain at least one clause expressing the same
information, and further split into a training and
testing set.
As a rough indication of the diversity of the
dataset and the difference of the task from bilin-
gual alignment, we define the alignment diver-
sity measure (ADM) for two texts, T1T2, to be:
</bodyText>
<subsectionHeader confidence="0.95576">
2matchesT1T2
</subsectionHeader>
<bodyText confidence="0.995507">
T2 , where matches is the number of
</bodyText>
<subsectionHeader confidence="0.790558">
T1
</subsectionHeader>
<bodyText confidence="0.999830916666667">
matching sentence pairs. Intuitively, for closely
aligned document pairs, as prevalent in bilingual
alignment, one would expect an ADM value close
to 1. The average ADM value for the training doc-
ument pairs of the Britannica corpus is 026.
For the gospels, we use the King James ver-
sion, available electronically from the Sacred Text
Archive.2 The gospels’ lengths span from 678
verses (Mark) to 1151 verses (Luke), where we
treat verses as sentences. For training and eval-
uation purposes, we use the list of parallels given
by Aland (1985).3 We use the pair Matthew-Mark
</bodyText>
<footnote confidence="0.9021354">
1http://research.microsoft.com/
research/downloads/
2http://www.sacred-texts.com
3The parallels are available online from http://www.
bible-researcher.com/parallels.html.
</footnote>
<bodyText confidence="0.9999535">
for training and the two pairs: Matthew-Luke and
Mark-Luke for testing. Whereas for the Britannica
corpus parallels were marked at the resolution of
sentences, Aland’s annotation presents parallels as
matched sequences of verses, known as pericopes.
For instance, Matthew:4.1-11 matches Mark:1.12-
13. We write v p to indicate that verse v belongs
to pericope p.4
</bodyText>
<sectionHeader confidence="0.99563" genericHeader="method">
4 Algorithm
</sectionHeader>
<bodyText confidence="0.999961">
We now describe the algorithm, starting with the
TF*IDF similarity score, followed by our use of
logistic regression, and the global alignment.
</bodyText>
<subsectionHeader confidence="0.986402">
4.1 From word overlap to TF*IDF
</subsectionHeader>
<bodyText confidence="0.999979608695652">
Barzilay and Elhadad (2003) use a cosine mea-
sure of word-overlap as a baseline for the task.
As can be expected, word overlap is a relatively
effective indicator of sentence similarity and re-
latedness (Marcu, 1999). Unfortunately, plain
word-overlap assigns all words equal importance,
not even distinguishing between function and con-
tent words. Thus, once the overlap threshold is
decreased to improve recall, precision degrades
rapidly. For instance, if a pair of sentences has
one or two words in common, this is inconclusive
evidence of their similarity or difference.
One way to address this problem is to differ-
entially weight words using the TF*IDF scoring
scheme, which has become standard in Informa-
tion Retrieval (Salton and Buckley, 1988). IDF
was also used for the similar task of directional en-
tailment by Monz and de Rijke (2001). To apply
this scheme for the task at hand we diverge from
the standard IDF definition by viewing each sen-
tence as a document, and the pair of documents as
a combined collection of N single-sentence docu-
ments. For a term t in sentence s, we define TFst
</bodyText>
<sectionHeader confidence="0.618589" genericHeader="method">
5
</sectionHeader>
<bodyText confidence="0.768710133333333">
to be a binary indicator of whether t occurs in s,
and DFtto be the number of sentences in which
t occurs. The TF*IDF weight is:
.
DFt
4The annotation of matched pericopes induces a partial
segmentation of each gospel into paragraph-like segments.
Since this segmentation is part of the gold annotation, we do
not use it in our algorithm.
5Using a binary indicator rather than the more typical
number of occurrences yielded better accuracy on the Bri-
tannica training set. This is probably due to the “documents”
being only of sentence length.
N
wst def TFst log
</bodyText>
<page confidence="0.783752">
163
</page>
<figure confidence="0.99898345">
pg1
pg2
1 2 3 4
a b c
0
0.2
0.4
0.6
0.8
1
1
0.8
0.6
probability
0.5
0.4
0.276
0.2
0
similarity
</figure>
<figureCaption confidence="0.9762895">
Figure 1: Logistic Regression for Britannica train-
ing data
</figureCaption>
<bodyText confidence="0.972764">
We use these scores as the basis of a standard
cosine similarity measure,
</bodyText>
<equation confidence="0.9539305">
sims1s2 s1 s2 ∑t ws1 t ws2 t
s1 s2 ∑t ws21 t ∑t w2s2t .
</equation>
<bodyText confidence="0.99847875">
We normalize terms by using Porter stem-
ming (Porter, 1980). For the Britannica corpus, we
also normalized British/American spelling differ-
ences using a small manually-constructed lexicon.
</bodyText>
<subsectionHeader confidence="0.980522">
4.2 Logistic regression
</subsectionHeader>
<bodyText confidence="0.98488245">
TF*IDF scores provide a numeric measure of sen-
tence similarity. To use them for choosing sen-
tence pairs, we proceeded to learn a probability of
two sentences being matched, given their TF*IDF
similarity score, prmatch 1 sim. We expect
this probability to follow a sigmoid-shaped curve.
While it is always monotonically increasing, the
rate of ascent changes; for very low or very high
values it is not as steep as for middle values. This
reflects the intuition that while we always prefer a
higher scoring pair over a lower scoring pair, this
preference is more pronounced in the middle range
than in the extremities.
Indeed, Figure 1 shows a graph of this distri-
bution on the training part of the Britannica cor-
pus, where pointxyrepresents the fraction y of
correctly matched sentences of similarity x. Over-
layed on top of the points is a logistic regression
model of this distribution, defined as the function
eabx
</bodyText>
<sectionHeader confidence="0.822467" genericHeader="method">
1 eabx ,
</sectionHeader>
<bodyText confidence="0.983453413043478">
where a and b are parameters. We used
Weka (Witten and Frank, 1999) to automatically
learn the parameters of the distribution on the
training data. These are set to a 789 and
b 2756 for the Britannica corpus.
Figure 2: Reciprocal best hit example. Arrows in-
dicate the best hit for each verse. The pairs con-
sidered correct are 2b and 4c .
Logistic regression scales the similarity scores
monotonically but non-linearly. In particular, it
changes the density of points at different score
levels. In addition, we can use this distribution
to choose a threshold, th, for when a similarity
score is indicative of a match. Optimizing the
F-measure on the training data using Weka, we
choose a threshold value of th 0276. Note
that since the logistic regression transformation is
monotonic, the existence of a threshold on proba-
bilities implies the existence of a threshold on the
original sim scores. Moreover, such a threshold
might be obtained by means other than logistic re-
gression. The scaling, however, will become cru-
cial once we do additional calculations with these
probabilities in Section 4.4.
Applying logistic regression to the gospels is
complicated by the fact that we only have a cor-
rect alignment at the resolution of pericopes, and
not individual verses. Verse pairs that do not be-
long to a matched pericope pair can be safely con-
sidered unaligned, but for a matched pericope pair,
pg1pg2, we do not know which verse is matched
with which. We solve this by searching for the
reciprocal best hit, a method often used to find
orthologous genes in related species (Mushegian
and Koonin, 1996). For each verse in each peri-
cope, we find the top matching verse in the other
pericope. We take as correct all and only pairs
of verses xy, such that x is y’s best match and y
is x’s best match. An example is shown in Fig-
ure 2. Taking these pairs as matched yields an
ADM value of 034 for the training pair of doc-
uments.
We used the reciprocally best-matched pairs of
the training portion of the gospels to find logistic
regression parametersa 960b 2500, and
p
</bodyText>
<page confidence="0.991395">
164
</page>
<bodyText confidence="0.997835333333333">
a threshold,th 0250. Note that we rely on this
matching only for training, but not for evaluation
(see Section 5.2).
</bodyText>
<subsectionHeader confidence="0.9569945">
Sentences in comprehensive version
4.3 Method 1: TF*IDF
</subsectionHeader>
<bodyText confidence="0.9999878">
As a simple method for choosing sentence pairs,
we just select all sentence pairs with prmatch
th. We use the following additional heuristics:
We unconditionally match the first sentence
of one document with the first sentence of
the other document. As noted by Quirk et al.
(2004), these are very likely to be matched,
as verified on our training set as well.
We allow many-to-one matching of sen-
tences, but limit them to at most 2-to-1 sen-
tences in both directions (by allowing only
the top two matches per sentence to be cho-
sen), since such multiple matchings often
arise due to splitting a sentence into two, or
conversely, merging two sentences into one.
</bodyText>
<subsectionHeader confidence="0.997405">
4.4 Method 2: TF*IDF + Global alignment
</subsectionHeader>
<bodyText confidence="0.999759655172414">
Matching sentence pairs according to TF*IDF ig-
nores sentence ordering completely. For bilingual
texts, Gale and Church (1991) demonstrated the
extraordinary effectiveness of a global alignment
dynamic programming algorithm, where the basic
similarity score was based on the difference in sen-
tence lengths, measured in characters. Such meth-
ods fail to work in the monolingual case. Gale
and Church’s algorithm (using the implementation
of Danielsson and Ridings (1997)) yields 2% pre-
cision at 2.85% recall on the Britannica corpus.
Moore’s algorithm (2002), which augments sen-
tence length alignment with IBM Model 1 align-
ment, reports zero matching sentence pairs (re-
gardless of threshold).
Nevertheless, we expect sentence ordering can
provide important clues for monolingual align-
ment, bearing in mind two main differences from
the bilingual case. First, as can be expected by the
ADM value, there are many gaps in the alignment.
Second, there can be large segments that diverge
from the linear order predicted by a global align-
ment, as illustrated by the oval in Figure 3 (Figure
2, (Barzilay and Elhadad, 2003)).
To model these features of the data, we use a
variant of Needleman-Wunsch alignment (1970).
We compute the optimal alignment between sen-
tences 1i of the comprehensive text and sentences
1j of the elementary version by
</bodyText>
<figure confidence="0.909937">
0 5 10 15 20 25 30
Sentences in elementary version
</figure>
<figureCaption confidence="0.9933845">
Figure 3: Gold alignment for a text from the Bri-
tannica corpus.
</figureCaption>
<bodyText confidence="0.999985483870968">
Note that the dynamic programming sums match
probabilities, rather than the original sim scores,
making crucial use of the calibration induced by
the logistic regression. Starting from the first pair
of sentences, we find the best path through the ma-
trix indexed by i and j, using dynamic program-
ming. Unlike the standard algorithm, we assign no
penalty to off-diagonal matches, allowing many-
to-one matches as illustrated schematically in Fig-
ure 4. This is because for the loose alignment ex-
hibited by the data, being off-diagonal is not in-
dicative of a bad match. Instead, we prune the
complete path generated by the dynamic program-
ming using two methods. First, as in Section 4.3,
we limit many-to-one matches to 2-to-1, by al-
lowing just the two best matches per sentence to
be included. Second, we eliminate sentence pairs
with very low match probabilitiesprmatch
0005, a value learned on the training data. Fi-
nally, to deal with the divergences from the lin-
ear order, we add the top n pairs with very high
match probability, above a higher threshold, th.
Optimizing on the training data, we set n 5 and
th 065 for both corpora.
Note that although Barzilay and Elhadad also
used an alignment algorithm, they restricted it
only to sentences judged to belong to topically re-
lated paragraphs. As noted above, this restriction
relies on a special feature of the corpus, the fact
that encyclopedia entries follow a relatively regu-
lar structure of paragraphs. By not relying on such
</bodyText>
<figure confidence="0.9531341">
Manual alignment
250
200
150
100
50
0
si 1j 1 prmatchij
sij max si 1j prmatchij
sij 1 prmatchij
</figure>
<page confidence="0.857271">
165
</page>
<figureCaption confidence="0.99898">
Figure 4: Global alignment
</figureCaption>
<bodyText confidence="0.9960395">
corpus-specific features, our approach gains in ro-
bustness.
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999639">
5.1 Britannica corpus
</subsectionHeader>
<bodyText confidence="0.980783545454545">
Precision/recall curves for both methods, aggre-
gated over all the documents of the testing por-
tion of the Britannica corpus are given in Fig-
ure 5. To obtain different precision/recall points,
we vary the threshold above which a sentence pair
is deemed matched. Of course, when practically
applying the algorithm, we have to pick a partic-
ular threshold, as we have done by choosing th.
Precision/recall values at this threshold are also in-
dicated in the figure.6
0.3 0.4 0.5 0.558 0.6 0.7
</bodyText>
<subsectionHeader confidence="0.661049">
Recall
</subsectionHeader>
<bodyText confidence="0.92899275">
Figure 5: Precision/Recall curves for the Britan-
nica corpus
Comparative results with previous algorithms
are given in Table 1, in which the results for Barzi-
lay and Elhadad’s algorithm and previous ones are
taken from Barzilay and Elhadad (2003). The pa-
per reports the precision at 55.8% recall, since
the Decomposition method (Jing, 2002) only pro-
duced results at this level of recall, as some of the
method’s parameters were hard-coded.
Interestingly, the TF*IDF method is highly
competitive in determining sentence similarity.
</bodyText>
<footnote confidence="0.983498666666667">
6Decreasing the threshold to 0.0 does not yield all pairs,
since we only consider pairs with similarity strictly greater
than 0.0, and restrict many-to-one matches to 2-to-1.
</footnote>
<table confidence="0.999766">
Algorithm Precision
SimFinder 24%
Word Overlap 57.9%
Decomposition 64.3%
Barzilay &amp; Elhadad 76.9%
TF*IDF 77.0%
TF*IDF + Align 83.1%
</table>
<tableCaption confidence="0.999943">
Table 1: Precision at 55.8% Recall
</tableCaption>
<bodyText confidence="0.999639555555556">
Despite its simplicity, it achieves the same perfor-
mance as Barzilay and Elhadad’s algorithm,7 and
is better than all previous ones. Significant im-
provement is achieved by adding the global align-
ment.
Clearly, the method is inherently limited in that
it can only match sentences with some lexical
overlap. For instance, the following sentence pair
that should have been matched was missed:
Population soared, reaching 756,000 by
1903, and urban services underwent exten-
sive modification.
At the beginning of the 20th century, Warsaw
had about 700,000 residents.
Matching “1903” with “the beginning of the
20th century” goes beyond the scope of any
method relying predominantly on word identity.
The hope is, however, that such mappings could
be learned by amassing a large corpus of accu-
rately sentence-aligned documents, and then ap-
plying a word-alignment algorithm, as proposed
by Quirk et al. (2004). Incidentally, examining
sentence pairs with high TF*IDF similarity scores,
there are some striking cases that appear to have
been missed by the human judges. Of course, we
faithfully and conservatively relied on the human
annotation in the evaluation, ignoring such cases.
</bodyText>
<subsectionHeader confidence="0.99645">
5.2 Gospels
</subsectionHeader>
<bodyText confidence="0.999937125">
For evaluating our algorithm’s accuracy on the
gospels, we again have to contend with the fact
that the correct alignments are given at the resolu-
tion of pericopes, not verses. We cannot rely on
the reciprocal best hit method we used for train-
ing, since it relies on the TF*IDF similarity scores,
which we are attempting to evaluate. We therefore
devise an alternative evaluation criterion, counting
</bodyText>
<footnote confidence="0.526457">
7We discount the minor difference as insignificant.
</footnote>
<figure confidence="0.976226666666667">
TF*IOF + Align
TF*IOF
Precision @ 55.8 Recall
Precision/Recall @ th
Precision 1
0.9
0.8
0.7
0.6
</figure>
<page confidence="0.995768">
166
</page>
<bodyText confidence="0.998935">
a pair of verses as correctly aligned if they belong
to a matched pericope in the gold annotation.
Let Goldg1g2be the set of matched pericope
pairs for gospels g1g2, according to Aland (1985).
For each pair of matched verses, vg1vg2, we count
the pair as a true positive if and only if there is
a pericope pair pg1pg2 Goldg1g2such that
vgi pgii 12. Otherwise, it is a false positive.
Precision is defined as usual (P tptp fp).
For recall, we note that not all the verses of a
matched pericope should be matched, especially
when one pericope has substantially more verses
than the other. In general, we may expect the num-
ber of verses to be matched to be the minimum of
pg1 and pg2 . We thus define recall as:
The results are given in Figure 6, including the
word-overlap baseline, TF*IDF ranking with lo-
gistic regression, and the added global alignment.
Once again, TF*IDF yields a substantial improve-
ment over the baseline, and results are further im-
proved by adding the global alignment.
</bodyText>
<sectionHeader confidence="0.377614" genericHeader="evaluation">
Recall
</sectionHeader>
<figureCaption confidence="0.946745">
Figure 6: Precision/Recall curves for the gospels
</figureCaption>
<sectionHeader confidence="0.988854" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999983983333333">
For monolingual alignment to achieve its full po-
tential for text rewriting, huge amounts of text
would need to be accurately aligned. Since mono-
lingual corpora are so noisy, simple but effective
methods as described in this paper will be required
to ensure scalability.
We have presented a novel algorithm for align-
ing the sentences of monolingual corpora of com-
parable documents. Our algorithm not only yields
substantially improved accuracy, but is also sim-
pler and more robust than previous approaches.
The efficacy of TF*IDF ranking is remarkable in
the face of previous results. In particular, TF*IDF
was not chosen by the feature selection algorithm
of Hatzivassiloglou et al. (2001), who directly ex-
perimented and rejected TF*IDF measures as be-
ing less effective in determining similarity. We be-
lieve this striking difference can be attributed to
the source of the weights. Recall that our TF*IDF
weights treat each sentence as a separate docu-
ment for the purpose of weighting. TF*IDF scores
used in previous work are likely to have been ob-
tained either by aggregation over the full docu-
ment corpus, or by comparison with an external
general collection, which is bound to yield lower
discriminative power. To illustrate this, consider
two words, such as the name of a city, and the
name of a building in that city. Viewed globally,
both words are likely to belong to the long tail
of the Zipf distribution, having almost indistin-
guishable logarithmic IDF. However, in the ency-
clopedia entry describing the city, the city’s name
is likely to appear in many sentences, while the
building name may appear only in the single sen-
tence that refers to it, and thus the latter should
be scored higher. Conversely, a word that is rela-
tively frequent in general usage, e.g., “river” might
be highly discriminative between sentences.
We further improve on the TF*IDF results by
using a global alignment algorithm. We expect
that more sophisticated sequence alignment tech-
niques, as studied for biological sequence anal-
ysis, might yield improved results, in particular
for comparing loosely matched document pairs in-
volving non-linear text transformations such as in-
versions and translocations. Such methods could
still modularly rely on the TF*IDF scoring.
We reiterate Barzilay and Elhadad’s conclusion
about the effectiveness of using the document con-
text for the alignment of text. In fact, we are
able to take better advantage of the intra-document
context, while not relying on any assumptions
about inter-document context that might be spe-
cific to one particular corpus. Identifying scalable
principles for the use of inter-document context
poses a challenging topic for future research.
We have restricted our attention here to pre-
annotated corpora, allowing better comparison
with previous work, and sidestepping the labor-
intensive task of human annotation. Having es-
</bodyText>
<figure confidence="0.9882916875">
0 0.1 0.2 0.3 0.4 0.5 0.6
Precision
0.9
0.8
0.7
0.6
0.5
0.4
1
TF*IDF + Align
TF*IDF
Overlap
R tp
∑
pg1pg2 Goldg1g2
min pg1 pg2 .
</figure>
<page confidence="0.981884">
167
</page>
<bodyText confidence="0.999223333333333">
tablished a simple and robust document alignment
method, we leave its application to much larger-
scale document sets for future work.
</bodyText>
<sectionHeader confidence="0.995492" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998358">
We thank Regina Barzilay and Noemie Elhadad
for providing access to the annotated Britannica
corpus, and for discussion. This work was sup-
ported in part by National Science Foundation
grant BCS-0236592.
</bodyText>
<sectionHeader confidence="0.998546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889762886598">
Kurt Aland, editor. 1985. Synopsis Quattuor Evange-
liorum. American Bible Society, 13th edition, De-
cember.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entail-
ment challenge. In Proceedings of the PASCAL
Challenges Workshop on Recognising Textual En-
tailment, pages 1–8, April.
Pernilla Danielsson and Daniel Ridings. 1997. Prac-
tical presentation of a vanilla aligner. Research re-
ports from the Department of Swedish, Goeteborg
University GU-ISS-97-2, Sprakdata, February.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th International Con-
ference on Computational Linguistics (COLING-
2004), Geneva, Switzerland, August.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
William A. Gale and Kenneth W. Church. 1991. A
program for aligning sentences in bilingual corpora.
In Meeting of the Association for Computational
Linguistics, pages 177–184.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combi-
nations via machine learning. In Proceedings of the
1999 Joint SIGDAT conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 203–212, College Park, Maryland.
Vasileios Hatzivassiloglou, Judith L. Klavans,
Melissa L. Holcombe, Regina Barzilay, Min-
Yen Kan, and Kathleen R. McKeown. 2001.
SIMFINDER: A flexible clustering tool for sum-
marization. In Proceedings of the Workshop on
Automatic Summarization, pages 41–49. Associa-
tion for Computational Linguistics, 2001.
Hongyan Jing. 2002. Using hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527–543.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization – step one: Sentence compres-
sion. In Proceedings of the American Association
for Artificial Intelligence conference (AAAI).
Beth Levin. 1993. English Verb Classes And Alterna-
tions: A Preliminary Investigation. The University
of Chicago Press.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
SIGIR ’99: Proceedings of the 22ndAnnualInterna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, August 15-19,
1999, Berkeley, CA, USA, pages 137–144. ACM.
Christof Monz and Maarten de Rijke. 2001. Light-
weight subsumption checking for computational
semantics. In Patrick Blackburn and Michael
Kohlhase, editors, Proceedings of the 3rd Workshop
on Inference in Computational Semantics (ICoS-3),
pages 59–72.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Stephen D.
Richardson, editor, AMTA, volume 2499 of Lec-
ture Notes in Computer Science, pages 135–144.
Springer.
Arcady R. Mushegian and Eugene V. Koonin. 1996.
A minimal gene set for cellular life derived by com-
parison of complete bacterial genomes. Proceedings
of the National Academies of Science, 93:10268–
10273, September.
S.B. Needleman and C.D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. J. Mol.
Biol., 48:443–453.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Martin F. Porter. 1980. An algorithm for suffi x strip-
ping. Program, 14(3):130–137.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 142–149, Barcelona Spain, July.
Gerard Salton and Chris Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513–
523.
Karen Sp¨arck-Jones. 1972. Exhaustivity and speci-
fi city. Journal of Documentation, 28(1):11–21.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann.
</reference>
<page confidence="0.997288">
168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920705">
<title confidence="0.970636">Towards Robust Context-Sensitive Sentence Alignment for Monolingual Corpora</title>
<author confidence="0.999877">Rani Nelken</author>
<author confidence="0.999877">Stuart M Shieber</author>
<affiliation confidence="0.9997025">Division of Engineering and Applied Sciences Harvard University</affiliation>
<address confidence="0.992398">33 Oxford St. Cambridge, MA 02138</address>
<email confidence="0.997233">nelken,shieber@deas.harvard.edu</email>
<abstract confidence="0.999732">Aligning sentences belonging to comparable monolingual corpora has been suggested as a first step towards training text rewriting algorithms, for tasks such as summarization or paraphrasing. We present here a new monolingual sentence alignment algorithm, combining a sentence-based TF*IDF score, turned into a probability distribution using logistic regression, with a global alignment dynamic programming algorithm. Our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Synopsis Quattuor Evangeliorum.</title>
<date>1985</date>
<editor>Kurt Aland, editor.</editor>
<publisher>American Bible Society,</publisher>
<note>13th edition,</note>
<contexts>
<context position="10335" citStr="(1985)" startWordPosition="1611" endWordPosition="1611"> T1T2, to be: 2matchesT1T2 T2 , where matches is the number of T1 matching sentence pairs. Intuitively, for closely aligned document pairs, as prevalent in bilingual alignment, one would expect an ADM value close to 1. The average ADM value for the training document pairs of the Britannica corpus is 026. For the gospels, we use the King James version, available electronically from the Sacred Text Archive.2 The gospels’ lengths span from 678 verses (Mark) to 1151 verses (Luke), where we treat verses as sentences. For training and evaluation purposes, we use the list of parallels given by Aland (1985).3 We use the pair Matthew-Mark 1http://research.microsoft.com/ research/downloads/ 2http://www.sacred-texts.com 3The parallels are available online from http://www. bible-researcher.com/parallels.html. for training and the two pairs: Matthew-Luke and Mark-Luke for testing. Whereas for the Britannica corpus parallels were marked at the resolution of sentences, Aland’s annotation presents parallels as matched sequences of verses, known as pericopes. For instance, Matthew:4.1-11 matches Mark:1.12- 13. We write v p to indicate that verse v belongs to pericope p.4 4 Algorithm We now describe the a</context>
<context position="23567" citStr="(1985)" startWordPosition="3804" endWordPosition="3804">he resolution of pericopes, not verses. We cannot rely on the reciprocal best hit method we used for training, since it relies on the TF*IDF similarity scores, which we are attempting to evaluate. We therefore devise an alternative evaluation criterion, counting 7We discount the minor difference as insignificant. TF*IOF + Align TF*IOF Precision @ 55.8 Recall Precision/Recall @ th Precision 1 0.9 0.8 0.7 0.6 166 a pair of verses as correctly aligned if they belong to a matched pericope in the gold annotation. Let Goldg1g2be the set of matched pericope pairs for gospels g1g2, according to Aland (1985). For each pair of matched verses, vg1vg2, we count the pair as a true positive if and only if there is a pericope pair pg1pg2 Goldg1g2such that vgi pgii 12. Otherwise, it is a false positive. Precision is defined as usual (P tptp fp). For recall, we note that not all the verses of a matched pericope should be matched, especially when one pericope has substantially more verses than the other. In general, we may expect the number of verses to be matched to be the minimum of pg1 and pg2 . We thus define recall as: The results are given in Figure 6, including the word-overlap baseline, TF*IDF ran</context>
</contexts>
<marker>1985</marker>
<rawString>Kurt Aland, editor. 1985. Synopsis Quattuor Evangeliorum. American Bible Society, 13th edition, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1211" citStr="Barzilay and Elhadad, 2003" startWordPosition="164" endWordPosition="167">using logistic regression, with a global alignment dynamic programming algorithm. Our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems. 1 Introduction Sentence-aligned bilingual corpora are a crucial resource for training statistical machine translation systems. Several authors have suggested that large-scale aligned monolingual corpora could be similarly used to advance the performance of monolingual text-to-text rewriting systems, for tasks including summarization (Knight and Marcu, 2000; Jing, 2002) and paraphrasing (Barzilay and Elhadad, 2003; Quirk et al., 2004). Unlike bilingual corpora, such as the Canadian Hansard corpus, which are relatively rare, it is now fairly easy to amass corpora of related monolingual documents. For instance, with the advent of news aggregator services such as “Google News”, one can readily collect multiple news stories covering the same news item (Dolan et al., 2004). Utilizing such a resource requires aligning related documents at a finer level of resolution, identifying which sentences from one document align with which sentences from the other. Previous work has shown that aligning related monoling</context>
<context position="2475" citStr="Barzilay and Elhadad (2003)" startWordPosition="358" endWordPosition="361"> the well-studied multi-lingual alignment task. Whereas documents in a bilingual corpus are typically very closely aligned, monolingual corpora exhibit a much looser level of alignment, with similar content expressed using widely divergent wording, grammatical form, and sentence order. Consequently, many of the simple surface-based methods that have proven to be so successful in bilingual sentence alignment, such as correlation of sentence length, linearity of alignment, and a predominance of one-to-one sentence mapping, are much less likely to be effective for monolingual sentence alignment. Barzilay and Elhadad (2003) suggested that these disadvantages could be at least partially offset by the recurrence of the same lexical items in document pairs. Indeed, they showed that a simple cosine word-overlap score is a good baseline for the task, outperforming much more sophisticated methods. They also observed that context is a powerful factor in determining alignment. They illustrated this on a corpus of Encyclopedia Britannica entries describing world cities, where each entry comes in two flavors, the comprehensive encyclopedia entry, and a shorter and simpler elementary version. Barzilay and Elhadad used cont</context>
<context position="6826" citStr="Barzilay and Elhadad (2003)" startWordPosition="1036" endWordPosition="1039">g semantic classes of verbs (Levin, 1993). The Decomposition method (Jing, 2002) relies on the observation that document summaries are often constructed by extracting sentence fragments from the document. It attempts to identify such extracts, using a Hidden Markov Model of the process of extracting words. The HMM uses features of word identity and document position, in which transition probabilities are based on locality assumptions. For instance, after a word is extracted, an adjacent word or one that belongs to a nearby sentence is more likely to be extracted than one that is further away. Barzilay and Elhadad (2003) apply a 4-step algorithm: 1. Cluster the paragraphs of the training documents into topic-specific clusters, based on word overlap. For instance, paragraphs in the Britannica city entries describing climate might cluster together. 2. Learn mapping rules between paragraphs of the full and elementary versions, taking the word-overlap and the clusters as features. 3. Given a new pair of texts, identify sentence pairs with high overlap, and take these to be aligned. Then, classify paragraphs according to the clusters learned in Step 1, and use the mapping rules of Step 2 to match pairs of paragrap</context>
<context position="9230" citStr="Barzilay and Elhadad (2003)" startWordPosition="1423" endWordPosition="1426">ch matched sentence pairs to train Giza++ (Och and Ney, 2003) on word-level alignment. The recent PASCAL “Recognizing Textual Entailment” (RTE) challenge (Dagan et al., 2005) focused on the problem of determining whether one sentence entails another. Beyond the difference in the definition of the required relation between sentences, the RTE challenge focuses on isolated sentence pairs, as opposed to sentences within a document context. The task was judged to be quite difficult, with many of the systems achieving relatively low accuracy. 3 Data The Britannica corpus, collected and annotated by Barzilay and Elhadad (2003), consists of 103 pairs of comprehensive and elementary encyclopedia entries describing major world cities. Twenty of these document pairs were annotated by human judges, who were asked to mark sentence pairs that contain at least one clause expressing the same information, and further split into a training and testing set. As a rough indication of the diversity of the dataset and the difference of the task from bilingual alignment, we define the alignment diversity measure (ADM) for two texts, T1T2, to be: 2matchesT1T2 T2 , where matches is the number of T1 matching sentence pairs. Intuitivel</context>
<context position="11117" citStr="Barzilay and Elhadad (2003)" startWordPosition="1713" endWordPosition="1716">ttp://www. bible-researcher.com/parallels.html. for training and the two pairs: Matthew-Luke and Mark-Luke for testing. Whereas for the Britannica corpus parallels were marked at the resolution of sentences, Aland’s annotation presents parallels as matched sequences of verses, known as pericopes. For instance, Matthew:4.1-11 matches Mark:1.12- 13. We write v p to indicate that verse v belongs to pericope p.4 4 Algorithm We now describe the algorithm, starting with the TF*IDF similarity score, followed by our use of logistic regression, and the global alignment. 4.1 From word overlap to TF*IDF Barzilay and Elhadad (2003) use a cosine measure of word-overlap as a baseline for the task. As can be expected, word overlap is a relatively effective indicator of sentence similarity and relatedness (Marcu, 1999). Unfortunately, plain word-overlap assigns all words equal importance, not even distinguishing between function and content words. Thus, once the overlap threshold is decreased to improve recall, precision degrades rapidly. For instance, if a pair of sentences has one or two words in common, this is inconclusive evidence of their similarity or difference. One way to address this problem is to differentially w</context>
<context position="18230" citStr="Barzilay and Elhadad, 2003" startWordPosition="2927" endWordPosition="2930">on at 2.85% recall on the Britannica corpus. Moore’s algorithm (2002), which augments sentence length alignment with IBM Model 1 alignment, reports zero matching sentence pairs (regardless of threshold). Nevertheless, we expect sentence ordering can provide important clues for monolingual alignment, bearing in mind two main differences from the bilingual case. First, as can be expected by the ADM value, there are many gaps in the alignment. Second, there can be large segments that diverge from the linear order predicted by a global alignment, as illustrated by the oval in Figure 3 (Figure 2, (Barzilay and Elhadad, 2003)). To model these features of the data, we use a variant of Needleman-Wunsch alignment (1970). We compute the optimal alignment between sentences 1i of the comprehensive text and sentences 1j of the elementary version by 0 5 10 15 20 25 30 Sentences in elementary version Figure 3: Gold alignment for a text from the Britannica corpus. Note that the dynamic programming sums match probabilities, rather than the original sim scores, making crucial use of the calibration induced by the logistic regression. Starting from the first pair of sentences, we find the best path through the matrix indexed b</context>
<context position="21008" citStr="Barzilay and Elhadad (2003)" startWordPosition="3397" endWordPosition="3400">ica corpus are given in Figure 5. To obtain different precision/recall points, we vary the threshold above which a sentence pair is deemed matched. Of course, when practically applying the algorithm, we have to pick a particular threshold, as we have done by choosing th. Precision/recall values at this threshold are also indicated in the figure.6 0.3 0.4 0.5 0.558 0.6 0.7 Recall Figure 5: Precision/Recall curves for the Britannica corpus Comparative results with previous algorithms are given in Table 1, in which the results for Barzilay and Elhadad’s algorithm and previous ones are taken from Barzilay and Elhadad (2003). The paper reports the precision at 55.8% recall, since the Decomposition method (Jing, 2002) only produced results at this level of recall, as some of the method’s parameters were hard-coded. Interestingly, the TF*IDF method is highly competitive in determining sentence similarity. 6Decreasing the threshold to 0.0 does not yield all pairs, since we only consider pairs with similarity strictly greater than 0.0, and restrict many-to-one matches to 2-to-1. Algorithm Precision SimFinder 24% Word Overlap 57.9% Decomposition 64.3% Barzilay &amp; Elhadad 76.9% TF*IDF 77.0% TF*IDF + Align 83.1% Table 1:</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8777" citStr="Dagan et al., 2005" startWordPosition="1349" endWordPosition="1352">gh word-overlap, yields 214K sentence pairs of which about 40% are estimated to be unrelated. No recall estimates 162 are provided; however, with the release of the annotated Microsoft Research Paraphrase Corpus,1 it is apparent that Dolan et al. are seeking much more tightly related pairs of sentences than Barzilay and Elhadad, ones that are virtually semantically equivalent. In subsequent work, the same authors (Quirk et al., 2004) used such matched sentence pairs to train Giza++ (Och and Ney, 2003) on word-level alignment. The recent PASCAL “Recognizing Textual Entailment” (RTE) challenge (Dagan et al., 2005) focused on the problem of determining whether one sentence entails another. Beyond the difference in the definition of the required relation between sentences, the RTE challenge focuses on isolated sentence pairs, as opposed to sentences within a document context. The task was judged to be quite difficult, with many of the systems achieving relatively low accuracy. 3 Data The Britannica corpus, collected and annotated by Barzilay and Elhadad (2003), consists of 103 pairs of comprehensive and elementary encyclopedia entries describing major world cities. Twenty of these document pairs were ann</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, pages 1–8, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pernilla Danielsson</author>
<author>Daniel Ridings</author>
</authors>
<title>Practical presentation of a vanilla aligner. Research reports from the Department of Swedish, Goeteborg University GU-ISS-97-2,</title>
<date>1997</date>
<location>Sprakdata,</location>
<contexts>
<context position="17584" citStr="Danielsson and Ridings (1997)" startWordPosition="2821" endWordPosition="2824"> such multiple matchings often arise due to splitting a sentence into two, or conversely, merging two sentences into one. 4.4 Method 2: TF*IDF + Global alignment Matching sentence pairs according to TF*IDF ignores sentence ordering completely. For bilingual texts, Gale and Church (1991) demonstrated the extraordinary effectiveness of a global alignment dynamic programming algorithm, where the basic similarity score was based on the difference in sentence lengths, measured in characters. Such methods fail to work in the monolingual case. Gale and Church’s algorithm (using the implementation of Danielsson and Ridings (1997)) yields 2% precision at 2.85% recall on the Britannica corpus. Moore’s algorithm (2002), which augments sentence length alignment with IBM Model 1 alignment, reports zero matching sentence pairs (regardless of threshold). Nevertheless, we expect sentence ordering can provide important clues for monolingual alignment, bearing in mind two main differences from the bilingual case. First, as can be expected by the ADM value, there are many gaps in the alignment. Second, there can be large segments that diverge from the linear order predicted by a global alignment, as illustrated by the oval in Fi</context>
</contexts>
<marker>Danielsson, Ridings, 1997</marker>
<rawString>Pernilla Danielsson and Daniel Ridings. 1997. Practical presentation of a vanilla aligner. Research reports from the Department of Swedish, Goeteborg University GU-ISS-97-2, Sprakdata, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING2004),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1572" citStr="Dolan et al., 2004" startWordPosition="226" endWordPosition="229">that large-scale aligned monolingual corpora could be similarly used to advance the performance of monolingual text-to-text rewriting systems, for tasks including summarization (Knight and Marcu, 2000; Jing, 2002) and paraphrasing (Barzilay and Elhadad, 2003; Quirk et al., 2004). Unlike bilingual corpora, such as the Canadian Hansard corpus, which are relatively rare, it is now fairly easy to amass corpora of related monolingual documents. For instance, with the advent of news aggregator services such as “Google News”, one can readily collect multiple news stories covering the same news item (Dolan et al., 2004). Utilizing such a resource requires aligning related documents at a finer level of resolution, identifying which sentences from one document align with which sentences from the other. Previous work has shown that aligning related monolingual documents is quite different from the well-studied multi-lingual alignment task. Whereas documents in a bilingual corpus are typically very closely aligned, monolingual corpora exhibit a much looser level of alignment, with similar content expressed using widely divergent wording, grammatical form, and sentence order. Consequently, many of the simple surf</context>
<context position="7611" citStr="Dolan et al. (2004)" startWordPosition="1162" endWordPosition="1165">itannica city entries describing climate might cluster together. 2. Learn mapping rules between paragraphs of the full and elementary versions, taking the word-overlap and the clusters as features. 3. Given a new pair of texts, identify sentence pairs with high overlap, and take these to be aligned. Then, classify paragraphs according to the clusters learned in Step 1, and use the mapping rules of Step 2 to match pairs of paragraphs between the documents. 4. Finally, take advantage of the paragraph clustering and mapping, by locally aligning only sentences belonging to mapped paragraph pairs. Dolan et al. (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. Having collected a large corpus of clusters of related news stories from Google and MSN news aggregator services, they first seek related sentences, using two methods. First, using a high Levenshtein distance score they identify 139K sentence pairs of which about 16.7% are estimated to be unrelated (using human evaluation of a sample). Second, assuming that the first two sentences of related news stories should be matched, provided they have a high enough word-overlap, yields 214K sentence pairs of which </context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics (COLING2004), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6147" citStr="Fellbaum, 1998" startWordPosition="929" endWordPosition="930">otated by bible scholars (Aland, 1985). Our algorithm achieves a significant improvement over the baseline for this corpus as well, demonstrating the general applicability of our approach. 2 Related work Several authors have tackled the monolingual sentence correspondence problem. SimFinder (Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001) examined 43 different features that could potentially help determine the similarity of two short text units (sentences or paragraphs). Of these, they automatically selected 11 features, including word overlap, synonymy as determined by WordNet (Fellbaum, 1998), matching proper nouns and noun phrases, and sharing semantic classes of verbs (Levin, 1993). The Decomposition method (Jing, 2002) relies on the observation that document summaries are often constructed by extracting sentence fragments from the document. It attempts to identify such extracts, using a Hidden Markov Model of the process of extracting words. The HMM uses features of word identity and document position, in which transition probabilities are based on locality assumptions. For instance, after a word is extracted, an adjacent word or one that belongs to a nearby sentence is more li</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--184</pages>
<contexts>
<context position="17242" citStr="Gale and Church (1991)" startWordPosition="2771" endWordPosition="2774">with the first sentence of the other document. As noted by Quirk et al. (2004), these are very likely to be matched, as verified on our training set as well. We allow many-to-one matching of sentences, but limit them to at most 2-to-1 sentences in both directions (by allowing only the top two matches per sentence to be chosen), since such multiple matchings often arise due to splitting a sentence into two, or conversely, merging two sentences into one. 4.4 Method 2: TF*IDF + Global alignment Matching sentence pairs according to TF*IDF ignores sentence ordering completely. For bilingual texts, Gale and Church (1991) demonstrated the extraordinary effectiveness of a global alignment dynamic programming algorithm, where the basic similarity score was based on the difference in sentence lengths, measured in characters. Such methods fail to work in the monolingual case. Gale and Church’s algorithm (using the implementation of Danielsson and Ridings (1997)) yields 2% precision at 2.85% recall on the Britannica corpus. Moore’s algorithm (2002), which augments sentence length alignment with IBM Model 1 alignment, reports zero matching sentence pairs (regardless of threshold). Nevertheless, we expect sentence or</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William A. Gale and Kenneth W. Church. 1991. A program for aligning sentences in bilingual corpora. In Meeting of the Association for Computational Linguistics, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Eleazar Eskin</author>
</authors>
<title>Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>203--212</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="5854" citStr="Hatzivassiloglou et al., 1999" startWordPosition="885" endWordPosition="889">rithm to a very different corpus, the three Synoptic gospels of the New Testament: Matthew, Mark, and Luke. Putting aside any religious or theological significance of these texts, they offer an excellent data source for studying alignment, since they contain many parallels, which have been conveniently annotated by bible scholars (Aland, 1985). Our algorithm achieves a significant improvement over the baseline for this corpus as well, demonstrating the general applicability of our approach. 2 Related work Several authors have tackled the monolingual sentence correspondence problem. SimFinder (Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001) examined 43 different features that could potentially help determine the similarity of two short text units (sentences or paragraphs). Of these, they automatically selected 11 features, including word overlap, synonymy as determined by WordNet (Fellbaum, 1998), matching proper nouns and noun phrases, and sharing semantic classes of verbs (Levin, 1993). The Decomposition method (Jing, 2002) relies on the observation that document summaries are often constructed by extracting sentence fragments from the document. It attempts to identify such extracts, using a Hid</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Eskin, 1999</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans, and Eleazar Eskin. 1999. Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning. In Proceedings of the 1999 Joint SIGDAT conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 203–212, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Melissa L Holcombe</author>
<author>Regina Barzilay</author>
<author>MinYen Kan</author>
<author>Kathleen R McKeown</author>
</authors>
<title>SIMFINDER: A flexible clustering tool for summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization,</booktitle>
<pages>41--49</pages>
<contexts>
<context position="5886" citStr="Hatzivassiloglou et al., 2001" startWordPosition="890" endWordPosition="893">s, the three Synoptic gospels of the New Testament: Matthew, Mark, and Luke. Putting aside any religious or theological significance of these texts, they offer an excellent data source for studying alignment, since they contain many parallels, which have been conveniently annotated by bible scholars (Aland, 1985). Our algorithm achieves a significant improvement over the baseline for this corpus as well, demonstrating the general applicability of our approach. 2 Related work Several authors have tackled the monolingual sentence correspondence problem. SimFinder (Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001) examined 43 different features that could potentially help determine the similarity of two short text units (sentences or paragraphs). Of these, they automatically selected 11 features, including word overlap, synonymy as determined by WordNet (Fellbaum, 1998), matching proper nouns and noun phrases, and sharing semantic classes of verbs (Levin, 1993). The Decomposition method (Jing, 2002) relies on the observation that document summaries are often constructed by extracting sentence fragments from the document. It attempts to identify such extracts, using a Hidden Markov Model of the process </context>
<context position="25142" citStr="Hatzivassiloglou et al. (2001)" startWordPosition="4066" endWordPosition="4069"> text rewriting, huge amounts of text would need to be accurately aligned. Since monolingual corpora are so noisy, simple but effective methods as described in this paper will be required to ensure scalability. We have presented a novel algorithm for aligning the sentences of monolingual corpora of comparable documents. Our algorithm not only yields substantially improved accuracy, but is also simpler and more robust than previous approaches. The efficacy of TF*IDF ranking is remarkable in the face of previous results. In particular, TF*IDF was not chosen by the feature selection algorithm of Hatzivassiloglou et al. (2001), who directly experimented and rejected TF*IDF measures as being less effective in determining similarity. We believe this striking difference can be attributed to the source of the weights. Recall that our TF*IDF weights treat each sentence as a separate document for the purpose of weighting. TF*IDF scores used in previous work are likely to have been obtained either by aggregation over the full document corpus, or by comparison with an external general collection, which is bound to yield lower discriminative power. To illustrate this, consider two words, such as the name of a city, and the </context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L. Holcombe, Regina Barzilay, MinYen Kan, and Kathleen R. McKeown. 2001. SIMFINDER: A flexible clustering tool for summarization. In Proceedings of the Workshop on Automatic Summarization, pages 41–49. Association for Computational Linguistics, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden Markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="1166" citStr="Jing, 2002" startWordPosition="159" endWordPosition="160">to a probability distribution using logistic regression, with a global alignment dynamic programming algorithm. Our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems. 1 Introduction Sentence-aligned bilingual corpora are a crucial resource for training statistical machine translation systems. Several authors have suggested that large-scale aligned monolingual corpora could be similarly used to advance the performance of monolingual text-to-text rewriting systems, for tasks including summarization (Knight and Marcu, 2000; Jing, 2002) and paraphrasing (Barzilay and Elhadad, 2003; Quirk et al., 2004). Unlike bilingual corpora, such as the Canadian Hansard corpus, which are relatively rare, it is now fairly easy to amass corpora of related monolingual documents. For instance, with the advent of news aggregator services such as “Google News”, one can readily collect multiple news stories covering the same news item (Dolan et al., 2004). Utilizing such a resource requires aligning related documents at a finer level of resolution, identifying which sentences from one document align with which sentences from the other. Previous </context>
<context position="6279" citStr="Jing, 2002" startWordPosition="948" endWordPosition="949">nstrating the general applicability of our approach. 2 Related work Several authors have tackled the monolingual sentence correspondence problem. SimFinder (Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001) examined 43 different features that could potentially help determine the similarity of two short text units (sentences or paragraphs). Of these, they automatically selected 11 features, including word overlap, synonymy as determined by WordNet (Fellbaum, 1998), matching proper nouns and noun phrases, and sharing semantic classes of verbs (Levin, 1993). The Decomposition method (Jing, 2002) relies on the observation that document summaries are often constructed by extracting sentence fragments from the document. It attempts to identify such extracts, using a Hidden Markov Model of the process of extracting words. The HMM uses features of word identity and document position, in which transition probabilities are based on locality assumptions. For instance, after a word is extracted, an adjacent word or one that belongs to a nearby sentence is more likely to be extracted than one that is further away. Barzilay and Elhadad (2003) apply a 4-step algorithm: 1. Cluster the paragraphs </context>
<context position="21102" citStr="Jing, 2002" startWordPosition="3414" endWordPosition="3415">ch a sentence pair is deemed matched. Of course, when practically applying the algorithm, we have to pick a particular threshold, as we have done by choosing th. Precision/recall values at this threshold are also indicated in the figure.6 0.3 0.4 0.5 0.558 0.6 0.7 Recall Figure 5: Precision/Recall curves for the Britannica corpus Comparative results with previous algorithms are given in Table 1, in which the results for Barzilay and Elhadad’s algorithm and previous ones are taken from Barzilay and Elhadad (2003). The paper reports the precision at 55.8% recall, since the Decomposition method (Jing, 2002) only produced results at this level of recall, as some of the method’s parameters were hard-coded. Interestingly, the TF*IDF method is highly competitive in determining sentence similarity. 6Decreasing the threshold to 0.0 does not yield all pairs, since we only consider pairs with similarity strictly greater than 0.0, and restrict many-to-one matches to 2-to-1. Algorithm Precision SimFinder 24% Word Overlap 57.9% Decomposition 64.3% Barzilay &amp; Elhadad 76.9% TF*IDF 77.0% TF*IDF + Align 83.1% Table 1: Precision at 55.8% Recall Despite its simplicity, it achieves the same performance as Barzila</context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Hongyan Jing. 2002. Using hidden Markov modeling to decompose human-written summaries. Computational Linguistics, 28(4):527–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization – step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence conference (AAAI).</booktitle>
<contexts>
<context position="1153" citStr="Knight and Marcu, 2000" startWordPosition="155" endWordPosition="158"> TF*IDF score, turned into a probability distribution using logistic regression, with a global alignment dynamic programming algorithm. Our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems. 1 Introduction Sentence-aligned bilingual corpora are a crucial resource for training statistical machine translation systems. Several authors have suggested that large-scale aligned monolingual corpora could be similarly used to advance the performance of monolingual text-to-text rewriting systems, for tasks including summarization (Knight and Marcu, 2000; Jing, 2002) and paraphrasing (Barzilay and Elhadad, 2003; Quirk et al., 2004). Unlike bilingual corpora, such as the Canadian Hansard corpus, which are relatively rare, it is now fairly easy to amass corpora of related monolingual documents. For instance, with the advent of news aggregator services such as “Google News”, one can readily collect multiple news stories covering the same news item (Dolan et al., 2004). Utilizing such a resource requires aligning related documents at a finer level of resolution, identifying which sentences from one document align with which sentences from the oth</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization – step one: Sentence compression. In Proceedings of the American Association for Artificial Intelligence conference (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes And Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="6240" citStr="Levin, 1993" startWordPosition="943" endWordPosition="944">e baseline for this corpus as well, demonstrating the general applicability of our approach. 2 Related work Several authors have tackled the monolingual sentence correspondence problem. SimFinder (Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001) examined 43 different features that could potentially help determine the similarity of two short text units (sentences or paragraphs). Of these, they automatically selected 11 features, including word overlap, synonymy as determined by WordNet (Fellbaum, 1998), matching proper nouns and noun phrases, and sharing semantic classes of verbs (Levin, 1993). The Decomposition method (Jing, 2002) relies on the observation that document summaries are often constructed by extracting sentence fragments from the document. It attempts to identify such extracts, using a Hidden Markov Model of the process of extracting words. The HMM uses features of word identity and document position, in which transition probabilities are based on locality assumptions. For instance, after a word is extracted, an adjacent word or one that belongs to a nearby sentence is more likely to be extracted than one that is further away. Barzilay and Elhadad (2003) apply a 4-ste</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes And Alternations: A Preliminary Investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>In SIGIR ’99: Proceedings of the 22ndAnnualInternational ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>137--144</pages>
<publisher>ACM.</publisher>
<location>Berkeley, CA, USA,</location>
<contexts>
<context position="11304" citStr="Marcu, 1999" startWordPosition="1747" endWordPosition="1748">nces, Aland’s annotation presents parallels as matched sequences of verses, known as pericopes. For instance, Matthew:4.1-11 matches Mark:1.12- 13. We write v p to indicate that verse v belongs to pericope p.4 4 Algorithm We now describe the algorithm, starting with the TF*IDF similarity score, followed by our use of logistic regression, and the global alignment. 4.1 From word overlap to TF*IDF Barzilay and Elhadad (2003) use a cosine measure of word-overlap as a baseline for the task. As can be expected, word overlap is a relatively effective indicator of sentence similarity and relatedness (Marcu, 1999). Unfortunately, plain word-overlap assigns all words equal importance, not even distinguishing between function and content words. Thus, once the overlap threshold is decreased to improve recall, precision degrades rapidly. For instance, if a pair of sentences has one or two words in common, this is inconclusive evidence of their similarity or difference. One way to address this problem is to differentially weight words using the TF*IDF scoring scheme, which has become standard in Information Retrieval (Salton and Buckley, 1988). IDF was also used for the similar task of directional entailmen</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of large-scale corpora for summarization research. In SIGIR ’99: Proceedings of the 22ndAnnualInternational ACM SIGIR Conference on Research and Development in Information Retrieval, August 15-19, 1999, Berkeley, CA, USA, pages 137–144. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
<author>Maarten de Rijke</author>
</authors>
<title>Lightweight subsumption checking for computational semantics.</title>
<date>2001</date>
<booktitle>In Patrick Blackburn and Michael Kohlhase, editors, Proceedings of the 3rd Workshop on Inference in Computational Semantics (ICoS-3),</booktitle>
<pages>59--72</pages>
<marker>Monz, de Rijke, 2001</marker>
<rawString>Christof Monz and Maarten de Rijke. 2001. Lightweight subsumption checking for computational semantics. In Patrick Blackburn and Michael Kohlhase, editors, Proceedings of the 3rd Workshop on Inference in Computational Semantics (ICoS-3), pages 59–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Fast and accurate sentence alignment of bilingual corpora.</title>
<date>2002</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>2499</volume>
<pages>135--144</pages>
<editor>In Stephen D. Richardson, editor, AMTA,</editor>
<publisher>Springer.</publisher>
<marker>Moore, 2002</marker>
<rawString>Robert C. Moore. 2002. Fast and accurate sentence alignment of bilingual corpora. In Stephen D. Richardson, editor, AMTA, volume 2499 of Lecture Notes in Computer Science, pages 135–144. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arcady R Mushegian</author>
<author>Eugene V Koonin</author>
</authors>
<title>A minimal gene set for cellular life derived by comparison of complete bacterial genomes.</title>
<date>1996</date>
<booktitle>Proceedings of the National Academies of Science,</booktitle>
<volume>93</volume>
<pages>10273</pages>
<contexts>
<context position="15778" citStr="Mushegian and Koonin, 1996" startWordPosition="2514" endWordPosition="2517">ssion. The scaling, however, will become crucial once we do additional calculations with these probabilities in Section 4.4. Applying logistic regression to the gospels is complicated by the fact that we only have a correct alignment at the resolution of pericopes, and not individual verses. Verse pairs that do not belong to a matched pericope pair can be safely considered unaligned, but for a matched pericope pair, pg1pg2, we do not know which verse is matched with which. We solve this by searching for the reciprocal best hit, a method often used to find orthologous genes in related species (Mushegian and Koonin, 1996). For each verse in each pericope, we find the top matching verse in the other pericope. We take as correct all and only pairs of verses xy, such that x is y’s best match and y is x’s best match. An example is shown in Figure 2. Taking these pairs as matched yields an ADM value of 034 for the training pair of documents. We used the reciprocally best-matched pairs of the training portion of the gospels to find logistic regression parametersa 960b 2500, and p 164 a threshold,th 0250. Note that we rely on this matching only for training, but not for evaluation (see Section 5.2). Sentences in comp</context>
</contexts>
<marker>Mushegian, Koonin, 1996</marker>
<rawString>Arcady R. Mushegian and Eugene V. Koonin. 1996. A minimal gene set for cellular life derived by comparison of complete bacterial genomes. Proceedings of the National Academies of Science, 93:10268– 10273, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Needleman</author>
<author>C D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins.</title>
<date>1970</date>
<journal>J. Mol. Biol.,</journal>
<pages>48--443</pages>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>S.B. Needleman and C.D. Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. J. Mol. Biol., 48:443–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8664" citStr="Och and Ney, 2003" startWordPosition="1333" endWordPosition="1336"> assuming that the first two sentences of related news stories should be matched, provided they have a high enough word-overlap, yields 214K sentence pairs of which about 40% are estimated to be unrelated. No recall estimates 162 are provided; however, with the release of the annotated Microsoft Research Paraphrase Corpus,1 it is apparent that Dolan et al. are seeking much more tightly related pairs of sentences than Barzilay and Elhadad, ones that are virtually semantically equivalent. In subsequent work, the same authors (Quirk et al., 2004) used such matched sentence pairs to train Giza++ (Och and Ney, 2003) on word-level alignment. The recent PASCAL “Recognizing Textual Entailment” (RTE) challenge (Dagan et al., 2005) focused on the problem of determining whether one sentence entails another. Beyond the difference in the definition of the required relation between sentences, the RTE challenge focuses on isolated sentence pairs, as opposed to sentences within a document context. The task was judged to be quite difficult, with many of the systems achieving relatively low accuracy. 3 Data The Britannica corpus, collected and annotated by Barzilay and Elhadad (2003), consists of 103 pairs of compreh</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffi x stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="13097" citStr="Porter, 1980" startWordPosition="2068" endWordPosition="2069">otation, we do not use it in our algorithm. 5Using a binary indicator rather than the more typical number of occurrences yielded better accuracy on the Britannica training set. This is probably due to the “documents” being only of sentence length. N wst def TFst log 163 pg1 pg2 1 2 3 4 a b c 0 0.2 0.4 0.6 0.8 1 1 0.8 0.6 probability 0.5 0.4 0.276 0.2 0 similarity Figure 1: Logistic Regression for Britannica training data We use these scores as the basis of a standard cosine similarity measure, sims1s2 s1 s2 ∑t ws1 t ws2 t s1 s2 ∑t ws21 t ∑t w2s2t . We normalize terms by using Porter stemming (Porter, 1980). For the Britannica corpus, we also normalized British/American spelling differences using a small manually-constructed lexicon. 4.2 Logistic regression TF*IDF scores provide a numeric measure of sentence similarity. To use them for choosing sentence pairs, we proceeded to learn a probability of two sentences being matched, given their TF*IDF similarity score, prmatch 1 sim. We expect this probability to follow a sigmoid-shaped curve. While it is always monotonically increasing, the rate of ascent changes; for very low or very high values it is not as steep as for middle values. This reflects</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffi x stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William B Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>142--149</pages>
<location>Barcelona</location>
<contexts>
<context position="1232" citStr="Quirk et al., 2004" startWordPosition="168" endWordPosition="171">ith a global alignment dynamic programming algorithm. Our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems. 1 Introduction Sentence-aligned bilingual corpora are a crucial resource for training statistical machine translation systems. Several authors have suggested that large-scale aligned monolingual corpora could be similarly used to advance the performance of monolingual text-to-text rewriting systems, for tasks including summarization (Knight and Marcu, 2000; Jing, 2002) and paraphrasing (Barzilay and Elhadad, 2003; Quirk et al., 2004). Unlike bilingual corpora, such as the Canadian Hansard corpus, which are relatively rare, it is now fairly easy to amass corpora of related monolingual documents. For instance, with the advent of news aggregator services such as “Google News”, one can readily collect multiple news stories covering the same news item (Dolan et al., 2004). Utilizing such a resource requires aligning related documents at a finer level of resolution, identifying which sentences from one document align with which sentences from the other. Previous work has shown that aligning related monolingual documents is quit</context>
<context position="8595" citStr="Quirk et al., 2004" startWordPosition="1320" endWordPosition="1323">stimated to be unrelated (using human evaluation of a sample). Second, assuming that the first two sentences of related news stories should be matched, provided they have a high enough word-overlap, yields 214K sentence pairs of which about 40% are estimated to be unrelated. No recall estimates 162 are provided; however, with the release of the annotated Microsoft Research Paraphrase Corpus,1 it is apparent that Dolan et al. are seeking much more tightly related pairs of sentences than Barzilay and Elhadad, ones that are virtually semantically equivalent. In subsequent work, the same authors (Quirk et al., 2004) used such matched sentence pairs to train Giza++ (Och and Ney, 2003) on word-level alignment. The recent PASCAL “Recognizing Textual Entailment” (RTE) challenge (Dagan et al., 2005) focused on the problem of determining whether one sentence entails another. Beyond the difference in the definition of the required relation between sentences, the RTE challenge focuses on isolated sentence pairs, as opposed to sentences within a document context. The task was judged to be quite difficult, with many of the systems achieving relatively low accuracy. 3 Data The Britannica corpus, collected and annot</context>
<context position="16698" citStr="Quirk et al. (2004)" startWordPosition="2679" endWordPosition="2682">g pair of documents. We used the reciprocally best-matched pairs of the training portion of the gospels to find logistic regression parametersa 960b 2500, and p 164 a threshold,th 0250. Note that we rely on this matching only for training, but not for evaluation (see Section 5.2). Sentences in comprehensive version 4.3 Method 1: TF*IDF As a simple method for choosing sentence pairs, we just select all sentence pairs with prmatch th. We use the following additional heuristics: We unconditionally match the first sentence of one document with the first sentence of the other document. As noted by Quirk et al. (2004), these are very likely to be matched, as verified on our training set as well. We allow many-to-one matching of sentences, but limit them to at most 2-to-1 sentences in both directions (by allowing only the top two matches per sentence to be chosen), since such multiple matchings often arise due to splitting a sentence into two, or conversely, merging two sentences into one. 4.4 Method 2: TF*IDF + Global alignment Matching sentence pairs according to TF*IDF ignores sentence ordering completely. For bilingual texts, Gale and Church (1991) demonstrated the extraordinary effectiveness of a globa</context>
<context position="22537" citStr="Quirk et al. (2004)" startWordPosition="3634" endWordPosition="3637"> some lexical overlap. For instance, the following sentence pair that should have been matched was missed: Population soared, reaching 756,000 by 1903, and urban services underwent extensive modification. At the beginning of the 20th century, Warsaw had about 700,000 residents. Matching “1903” with “the beginning of the 20th century” goes beyond the scope of any method relying predominantly on word identity. The hope is, however, that such mappings could be learned by amassing a large corpus of accurately sentence-aligned documents, and then applying a word-alignment algorithm, as proposed by Quirk et al. (2004). Incidentally, examining sentence pairs with high TF*IDF similarity scores, there are some striking cases that appear to have been missed by the human judges. Of course, we faithfully and conservatively relied on the human annotation in the evaluation, ignoring such cases. 5.2 Gospels For evaluating our algorithm’s accuracy on the gospels, we again have to contend with the fact that the correct alignments are given at the resolution of pericopes, not verses. We cannot rely on the reciprocal best hit method we used for training, since it relies on the TF*IDF similarity scores, which we are att</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William B. Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 142–149, Barcelona Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<pages>523</pages>
<contexts>
<context position="3985" citStr="Salton and Buckley, 1988" startWordPosition="596" endWordPosition="599">mic programming to locally align sequences of sentences belonging to paragraphs about the same topic, yielding improved accuracy on the corpus. While powerful, such commonalities in document structure appear to be a special feature of the Britannica corpus, and therefore cannot be relied upon for other corpora. In this paper we present a novel algorithm for sentence alignment in monolingual corpora. At the core of the algorithm is a classical similar161 ity score based on differentially weighting words according to their Term Frequency-Inverse Document Frequency (TF*IDF) (Sp¨arck-Jones, 1972; Salton and Buckley, 1988). We treat sentences as documents, and the collection of sentences in the two documents being compared as the document collection, and use this score to estimate the probability that two sentences are aligned using logistic regression. Surprisingly, this approach by itself yields competitive accuracy, yielding the same level of accuracy as Barzilay and Elhadad’s algorithm, and higher than all previous approaches on the Britannica corpus. Such matching, however, is still noisy. We further improve accuracy by using a global alignment dynamic programming algorithm, which prunes many spurious matc</context>
<context position="11839" citStr="Salton and Buckley, 1988" startWordPosition="1827" endWordPosition="1830">p is a relatively effective indicator of sentence similarity and relatedness (Marcu, 1999). Unfortunately, plain word-overlap assigns all words equal importance, not even distinguishing between function and content words. Thus, once the overlap threshold is decreased to improve recall, precision degrades rapidly. For instance, if a pair of sentences has one or two words in common, this is inconclusive evidence of their similarity or difference. One way to address this problem is to differentially weight words using the TF*IDF scoring scheme, which has become standard in Information Retrieval (Salton and Buckley, 1988). IDF was also used for the similar task of directional entailment by Monz and de Rijke (2001). To apply this scheme for the task at hand we diverge from the standard IDF definition by viewing each sentence as a document, and the pair of documents as a combined collection of N single-sentence documents. For a term t in sentence s, we define TFst 5 to be a binary indicator of whether t occurs in s, and DFtto be the number of sentences in which t occurs. The TF*IDF weight is: . DFt 4The annotation of matched pericopes induces a partial segmentation of each gospel into paragraph-like segments. Si</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Chris Buckley. 1988. Termweighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513– 523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck-Jones</author>
</authors>
<title>Exhaustivity and specifi city.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<volume>28</volume>
<issue>1</issue>
<marker>Sp¨arck-Jones, 1972</marker>
<rawString>Karen Sp¨arck-Jones. 1972. Exhaustivity and specifi city. Journal of Documentation, 28(1):11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14244" citStr="Witten and Frank, 1999" startWordPosition="2256" endWordPosition="2259">w or very high values it is not as steep as for middle values. This reflects the intuition that while we always prefer a higher scoring pair over a lower scoring pair, this preference is more pronounced in the middle range than in the extremities. Indeed, Figure 1 shows a graph of this distribution on the training part of the Britannica corpus, where pointxyrepresents the fraction y of correctly matched sentences of similarity x. Overlayed on top of the points is a logistic regression model of this distribution, defined as the function eabx 1 eabx , where a and b are parameters. We used Weka (Witten and Frank, 1999) to automatically learn the parameters of the distribution on the training data. These are set to a 789 and b 2756 for the Britannica corpus. Figure 2: Reciprocal best hit example. Arrows indicate the best hit for each verse. The pairs considered correct are 2b and 4c . Logistic regression scales the similarity scores monotonically but non-linearly. In particular, it changes the density of points at different score levels. In addition, we can use this distribution to choose a threshold, th, for when a similarity score is indicative of a match. Optimizing the F-measure on the training data usin</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Ian H. Witten and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>