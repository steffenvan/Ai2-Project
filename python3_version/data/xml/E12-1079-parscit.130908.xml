<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003564">
<title confidence="0.966801">
Behind the Article: Recognizing Dialog Acts in Wikipedia Talk Pages
</title>
<author confidence="0.824041">
Oliver Ferschket, Iryna Gurevychtt and Yevgen Chebotart
</author>
<affiliation confidence="0.820856">
† Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
$ Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science
Technische Universit¨at Darmstadt
</affiliation>
<email confidence="0.951136">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.992518" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99900825">
In this paper, we propose an annota-
tion schema for the discourse analysis of
Wikipedia Talk pages aimed at the coor-
dination efforts for article improvement.
We apply the annotation schema to a cor-
pus of 100 Talk pages from the Simple
English Wikipedia and make the resulting
dataset freely available for download1. Fur-
thermore, we perform automatic dialog act
classification on Wikipedia discussions and
achieve an average F1-score of 0.82 with
our classification pipeline.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997065">
Over the past decade, the paradigm of information
sharing in the web has shifted towards participa-
tory and collaborative content production. Texts
are no longer exclusively prepared by individuals
and then shared with the community. They are in-
creasingly created collaboratively by multiple au-
thors and iteratively revised by the community.
When researchers first conducted surveys on
professional writers in the 1980s, they found that
the collaborative writing process differs consider-
ably from the way individual writing is done (Pos-
ner and Baecker, 1992). In joint writing, the writ-
ers have to externalize processes that are other-
wise not made explicit, like the planning and the
organization of the text. The authors have to com-
municate how the text should be written and what
exactly it should contain.
Today, many tools are available that support
collaborative writing. A tool that has particu-
larly taken hold is the Wiki, a web-based, asyn-
</bodyText>
<footnote confidence="0.9878975">
1http://www.ukp.tu-darmstadt.de/data/
wikidiscourse
</footnote>
<bodyText confidence="0.999904297297297">
chronous co-authoring tool. A unique character-
istic of Wikis is the documentation of the edit
history which keeps track of every change that
is made to a Wiki page. With this information,
it is possible to reconstruct the writing process
from the beginning to the end. Additionally, many
Wikis offer their users a communication platform,
the Talk pages, where they can discuss the ongo-
ing writing process with other users.
The most prominent example for a successful,
large-scale Wiki is Wikipedia, a collaboratively
created online encyclopedia, which has grown
considerably since its launch in 2001, and con-
tains a total of almost 20 million articles in 282
languages and dialects, as of Sept. 2011. As there
is no editorial body that manages Wikipedia top-
down, it is an open question how the huge on-
line community around Wikipedia regulates and
enforces standards of behavior and article qual-
ity. The user discussions on the article Talk pages
might shed light on this issue and give an insight
into the otherwise hidden processes of collabora-
tion that, until now, could only be analyzed via
interviews or group observations in experimental
settings.
The main goal of the present paper is to analyze
the content of the discussion pages of the Simple
English Wikipedia with respect to the dialog acts
aimed at the coordination efforts for article im-
provement. Dialog acts, according to the classic
speech act theory (Austin, 1962; Searle, 1969),
represent the meaning of an utterance at the level
of illocutionary force, i.e. a dialog act label con-
cisely characterizes the intention and the role of a
contribution in a dialog. We chose the Simple En-
glish Wikipedia for our initial analysis, because
we are able to obtain more representative results
</bodyText>
<page confidence="0.953945">
777
</page>
<note confidence="0.9765195">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 777–786,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999953133333333">
by covering almost 15% of all relevant Talk pages,
as opposed to the much smaller fraction we could
achieve for the English Wikipedia. The long-term
goal of this work is to identify relations between
contributions on the Talk pages and particular arti-
cle edits. We plan to analyze the relation between
article discussions and article content and identify
the edits in the article revision history that react to
the problems discussed on the Talk page. In com-
bination with article quality assessment (Yaari et
al., 2011), this opens up the possibility to iden-
tify successful patterns of collaboration which in-
crease the article quality. Furthermore, our work
will enable practical applications. By augment-
ing Wikipedia articles with the information de-
rived from automatically labeled discussions, arti-
cle readers can be made aware of particular prob-
lems that are being discussed on the Talk page
“behind the article”.
Our primary contributions in this paper are: (1)
an annotation schema for dialog acts reflecting
the efforts for coordinating the article improve-
ment; (2) the Simple English Wikipedia Dis-
cussion (SEWD) corpus, consisting of 100 seg-
mented and annotated Talk pages which we make
freely available for download; and (3) a dialog
act classification pipeline that incorporates sev-
eral state of the art machine learning algorithms
and feature selection techniques and achieves an
average Fl-score of .82 on our corpus.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998004642857143">
The analysis of speech and dialog acts has its
roots in the linguistic field of pragmatics. In
1962, John Austin shifted the focus from the mere
declarative use of language as a means for making
factual statements towards its non-declarative use
as a tool for performing actions. The speech act
theory was further systematized by Searle (1969),
whose classification of illocutionary acts (Searle,
1976) is still used as a starting point for creating
dialog act classification schemata for natural lan-
guage processing.
A well known, domain- and task-independent
annotation schema is DAMSL (Core and Allen,
1997). It was created as the standard annotation
schema for dialog tagging on the utterance level
by the Discourse Resource Initiative. It uses a
four-dimensional tagset that allows arbitrary label
combinations for each utterance. Jurafsky et al.
(1997) augmented the DAMSL schema to fit the
peculiarities of the Switchboard corpus. The re-
sulting SWDB-DAMSL schema contained more
than 220 distinct labels which have been clustered
to 42 coarse grained labels. Both schemata have
often been adapted for special purpose annotation
tasks.
With the rise of the social web, the amount of
research analyzing user generated discourse sub-
stantially increased. In addition to analyzing web
forums (Kim et al., 2010a), chats (Carpenter and
Fujioka, 2011) and emails (Cohen et al., 2004),
Wikipedia Talk pages have recently moved into
the center of attention of the research community.
Vi´egas et al. (2007) manually annotate 25
Wikipedia article discussion pages with a set of
11 labels in order to analyze how Talk pages are
used for planning the work on articles and resolv-
ing disputes among the editors. Schneider et al.
(2011) extend this schema and manually annotate
100 Talk pages with 15 labels. They confirm the
findings of Vi´egas et al. that coordination requests
occur most frequently in the discussions.
Bender et al. (2011) describe a corpus of 47
Talk pages which have been annotated for author-
ity claims and alignment moves. With this cor-
pus, the authors analyze how the participants in
Wikipedia discussions establish their credibility
and how they express agreement and disagree-
ment towards other participants or topics.
From a different perspective, Stvilia et al.
(2008) analyze 60 discussion pages in regard to
how information quality (IQ) in Wikipedia arti-
cles is assessed on the Talk pages and which types
of IQ problems are identified by the community.
They describe a Wikipedia IQ assessment model
and map it to established frameworks. Further-
more, they provide a list of IQ problems along
with related causal factors and necessary actions
which has also inspired the design of our annota-
tion schema.
Finally, Laniado et al. (2011) examine
Wikipedia discussion networks in order to
capture structural patterns of interaction. They
extract the thread structure from all Talk pages in
the English Wikipedia and create tree structures
of the discussion. The analysis of the graphs
reveals patterns that are unique to Wikipedia
discussions and might be used as a means to
characterize different types of Talk pages.
To the best of our knowledge, there is no
work yet that uses machine learning to automati-
</bodyText>
<page confidence="0.997101">
778
</page>
<figureCaption confidence="0.983379">
Figure 1: Structure of a Talk page: a) Talk page title,
b) untitled discussion topic, c) titled discussion topic,
d) unsigned turns, e) signed turns, f) topic title
</figureCaption>
<bodyText confidence="0.9994246">
cally classify user contributions in Wikipedia Talk
pages. Furthermore, there is no corpus available
that reflects the efforts of article improvement in
Wikipedia discussions. This is the subject of our
work.
</bodyText>
<sectionHeader confidence="0.952175" genericHeader="method">
3 Annotation Schema
</sectionHeader>
<bodyText confidence="0.99998292">
The main purpose of Wikipedia Talk pages is the
coordination of the editing process with the goal
of improving and sustaining the quality of the re-
spective article. The criteria for article quality in
Wikipedia are loosely defined in the guidelines for
“good articles”2 and “very good articles”3. Ac-
cording to these guidelines, distinguished articles
must be well-written in simple English, compre-
hensive, neutral, stable, accurate, verifiable and
follow the Wikipedia style guidelines4. These cri-
teria are the main points of reference in the dis-
cussions on the Talk pages.
Discourse analysis, as it is performed in this pa-
per, can be carried out on various levels, depend-
ing on what is regarded as the smallest unit of the
discourse. In this work, we focus on turns, not
on individual utterances, as we are interested in a
coarse-grained analysis of the discourse-structure
as a first step towards a finer-grained discourse
analysis. We define a turn (or contribution) as the
body of text that is added by an individual contrib-
utor in one or more revisions to a single discus-
sion topic until another contributor edits the page.
Furthermore, a topic (or discussion) is the body
of turns that revolve around a single matter. They
</bodyText>
<footnote confidence="0.999856333333333">
2http://simple.wikipedia.org/wiki/WP:RGA
3http://simple.wikipedia.org/wiki/WP:RVGA
4http://simple.wikipedia.org/wiki/WP:STYLE
</footnote>
<bodyText confidence="0.999564954545455">
are usually headed by a topic title. Finally, the
thread structure designates the sequence of turns
and their indentation levels on the Talk page. A
structural overview of a Talk page and its con-
stituents can be seen in Figure 1.
We composed an annotation schema that re-
flects the coordination efforts for article improve-
ment. Therefore, we manually analyzed a set
of thirty Talk pages from the Simple English
Wikipedia to identify the types of article defi-
ciencies that are discussed and the way article
improvement is coordinated. We furthermore
incorporated the findings from an information-
scientific analysis of information quality in
Wikipedia (Stvilia et al., 2008), which identifies
twelve types of quality problems, like e.g. Accu-
racy, Completeness or Relevance. Our resulting
tagset consists of 17 labels (cf. Table 1) which can
be subdivided into four higher level categories:
Article Criticism Denote comments that iden-
tify deficiencies in the article. The criticism
can refer to the article as a whole or to indi-
vidual parts of the article.
Explicit Performative Announce, report or sug-
gest editing activities.
Information Content Describe the direction of
the communication. A contribution can be
used to communicate new information to
others (IP), to request information (IS), or
to suggest changes to established facts (IC).
The IP label applies to most of the contri-
butions as most comments provide a certain
amount of new information.
Interpersonal Describe the attitude that is ex-
pressed towards other participants in the dis-
cussion and/or their comments.
Since a single turn may consist of several utter-
ances, it can consequently comprise multiple di-
alog acts. Therefore, we designed the annotation
study as a multi-label classification task, i.e. the
annotators can assign one or more labels to each
annotation unit. Each label is chosen indepen-
dently. Table 1 shows the labels, their respective
definitions and an example from our corpus.
</bodyText>
<sectionHeader confidence="0.902064" genericHeader="method">
4 Corpus Creation and Analysis
</sectionHeader>
<bodyText confidence="0.997526">
The SEWD corpus consists of 100 annotated Talk
pages extracted from a snapshot of the Simple En-
</bodyText>
<page confidence="0.995178">
779
</page>
<table confidence="0.995064317073171">
Label Description Example
Article Criticism
CM Content incomplete or lacking detail It should be added (1) that voters may skip prefer-
ences, but (2) that skipping preferences has no impact
on the result of the elections.
CW Lack of accuracy or correctness Kris Kringle is NOT a Germanic god, but an English
mispronunciation of Christkind, a German word that
means “the baby Jesus”.
CU Unsuitable or unnecessary content The references should be removed. The reason: The
references are too complicated for the typical reader
of simple Wikipedia.
CS Structural problems Also use sectioning, and interlinking
CL Deficiencies in language or style This section needs to be simplified further; there are a
lot of words that are too complex for this wiki.
COBJ Objectivity issues This article seems to take a clear pro-Christian, anti-
commercial view.
CO Other kind of criticism I have started an article on Google. It needs improve-
ment though.
Explicit Performative
PSR Explicit suggestion, recommendation or request This section needs to be simplified further
PREF Explicit reference or pointer Got it. The URL is http://www.dmbeatles.com/
history.php?year=1968
PFC Commitment to an action in the future Okay, I forgot to add that, I’ll do so later tonight.
PPC Report of a performed action I took and hopefully simplified the ”[[en:Prehistoric
music—Prehistoric music]]” article from EnWP
Information Content
IP Information providing “Depression” is the most basic term there is.
IS Information seeking So what kind of theory would you use for your music
composing?
IC Information correcting In linguistics and generally speaking, when Talking
about the lexicon in a language, words are usually cat-
egorized as ’nouns’, ’verbs’, ’adjectives’ and so on.
The term ’doing word’ does not exist.
Interpersonal
ATT+ Thankou.
Positive attitude towards other contributor or y
acceptance
ATTP Partial acceptance or partial rejection Okay, I can understand that, but some citations are
going to have to be included for [[WP:V]].
ATT- Negative attitude towards other contributor or Now what? You think you know so much about every-
rejection thing, and you are not even helping?!
</table>
<tableCaption confidence="0.914401">
Table 1: Annotation schema for the dialog act classification in Wikipedia discussion pages with examples from
the SEWD Corpus. Some examples have been shortened to fit the table.
</tableCaption>
<bodyText confidence="0.9943492">
glish Wikipedia from Apr 4th 2011.5 Technically
speaking, a Talk page is a normal Wiki page lo-
cated in one of the Talk namespaces. In this work,
we focus on article Talk pages and do not re-
gard User Talk pages. We selected the discussion
pages according to the number of turns they con-
tain. First, we discarded all discussion pages with
less than four contributions. We then analyzed
the distribution of turn counts per discussion page
in the remaining set of pages and defined three
classes: (i) discussion pages with 4-10 turns, (ii)
5The snapshot contains 69900 articles and 5783 Talk
pages of which 683 contained more than 3 contributions.
pages with 11-20 turns, and (iii) pages with more
than 20 turns. We then randomly extracted 50 dis-
cussion pages from class (i), 40 pages from class
(ii) and 10 pages from class (iii). This decision is
grounded in the restricted resources for the human
annotation task.
Data Preprocessing Due to a lack of discussion
structure, extracting the discussion threads from
the Talk pages requires a substantial amount of
preprocessing. Laniado et al. (2011) tackle the
thread extraction by using text indentation and in-
serted user signatures as clues. We found these
</bodyText>
<page confidence="0.969501">
780
</page>
<bodyText confidence="0.999897">
attributes to be insufficient for a reliable recon-
struction of the thread structure.6
Our preprocessing approach consists of three
steps: data retrieval, topic segmentation and turn
segmentation. For retrieving the discussion pages,
we use the Java Wikipedia Library (JWPL) (Zesch
et al., 2008), which offers efficient, database-
driven access to the contents of Wikipedia. We
segment the individual Talk pages into discus-
sions topics using the MediaWiki parser that
comes with JWPL. In our corpus, the parser man-
aged to identify all topic boundaries without any
errors. The most complex preprocessing step is
the turn segmentation.
First, we use the revision history of the Talk
page to identify the author and the creation time
of each paragraph. We use the Wikipedia Revi-
sion Toolkit (Ferschke et al., 2011) to examine the
changes between adjacent revisions of the Talk
page in order to identify the exact time a piece of
text was added as well as the author of the con-
tribution. We have to filter out malicious edits
from the history, as they would negatively affect
the segmentation process. We therefore disregard
all edits that are reverted in later later revisions.
In contrast to vandalism on article pages, this ap-
proach has proven to be sufficient to detect van-
dalism in the Talk page history.
Within each discussion topic, we aggregate all
adjacent paragraphs with the same author and the
same time stamp to one turn. In order to account
for turns that were written in multiple revisions,
we regard all time stamps within a window of 10
minutes7 as belonging to the same turn, unless the
page was edited by another user in the meantime.
Finally, the turn is marked with the indentation
level of its least indented paragraph. This infor-
mation is used to identify the relationship between
the turns, since indentation is used to indicate a
reply to an existing comment in the discussion.
A co-author of this paper evaluated the ac-
ceptability of the boundaries of each turn in the
SEWD corpus and found that 94% of the 1450
turns were correctly segmented. Turns with seg-
mentation errors were not included in the gold
standard.
</bodyText>
<footnote confidence="0.935973">
6Vi´egas et al. (2007) reported that only 67% of the con-
tributions on Wikipedia Talk pages are signed, which makes
signatures an unreliable predictor for turn boundaries.
7We experimentally tested values between 1 and 60 min-
utes.
</footnote>
<bodyText confidence="0.999135407407408">
Annotation Process For our annotation study,
we used the freely available MMAX2 annotation
tool8. Two annotators were introduced to the an-
notation schema by an instructor and trained on
an extra set of ten discussion pages. During the
annotation of the corpus, the annotators were al-
lowed to discuss difficult cases and could consult
the instructor if in doubt. They had access to the
segmented discussion pages within the MMAX2
tool as well as to the original Wikipedia articles
and discussion pages on the web.
The reconciliation of the annotations was car-
ried out by an expert annotator. In order to obtain
a consolidated gold standard, the expert decided
all cases in which the annotations of the two an-
notators did not match. Descriptive statistics for
the label assignments of each annotator and for
the gold standard can be seen in Table 2 and will
be further discussed in Section 4.2.
Corpus Format We publish our SEWD cor-
pus in two formats9, the original MMAX format,
and as XMI files for further processing with the
Apache Unstructured Information Management
Architecture10. For the latter format, we also pro-
vide the type system which defines all necessary
corpus specific types needed for using the data in
an NLP pipeline.
</bodyText>
<subsectionHeader confidence="0.581551">
4.1 Inter-Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.999905647058823">
To evaluate the reliability of our dataset, we per-
form a detailed inter-rater agreement study. For
measuring the agreement of the individual labels,
we report the observed agreement, Kappa statis-
tics (Carletta, 1996), and F1-scores. The latter are
computed by treating one annotator as the gold
standard and the other one as predictions (Hripc-
sak and Rothschild, 2005). The scores can be seen
in Table 2.
The average observed agreement across all la-
bels is �PO = .94. The individual Kappa scores
largely fall into the range that Landis and Koch
(1977) regard as substantial agreement, while
three labels are above the more strict .8 thresh-
old for reliable annotations (Artstein and Poesio,
2008). Furthermore, we obtain an overall pooled
Kappa (De Vries et al., 2008) of npool = .67,
</bodyText>
<footnote confidence="0.99492625">
8http://www.mmax2.net
9http://www.ukp.tu-darmstadt.de/data/
wikidiscourse
10http://uima.apache.org
</footnote>
<page confidence="0.97498">
781
</page>
<table confidence="0.966834590909091">
Annotator 1
N Percent
Annotator 2
N Percent
Inter-Annotator Agreement
NA,UA2 PO r, Fl
Gold Standard
N Percent
Label
Article Criticism
CM 183 13.4% 105 7.7% 193 .93 .63 .66 116 8.5%
CW 106 7.8% 57 4.2% 120 .95 .52 .55 70 5.1%
CU 69 5.0% 35 2.6% 83 .95 .38 .40 42 3.1%
CS 164 12.0% 101 7.4% 174 .94 .66 .69 136 9.9%
CL 195 14.3% 199 14.6% 244 .93 .73 .77 219 16.0%
COBJ 27 2.0% 23 1.7% 29 .99 .84 .84 27 2.0%
CO 20 1.5% 59 4.3% 71 .95 .18 .20 48 3.5%
Explicit Performative
PSR 458 33.5% 351 25.7% 503 .86 .66 .76 406 29.7%
PREF 43 3.1% 31 2.3% 51 .98 .61 .62 45 3.3%
PFC 73 5.3% 65 4.8% 86 .98 .76 .77 77 5.6%
PPC 357 26.1% 340 24.9% 371 .97 .92 .94 358 26.2%
</table>
<figure confidence="0.968618">
78.3%
1070
16.1%
220
9.5%
130
10.5%
144
2.4%
33
87
6.4%
Information Content
</figure>
<table confidence="0.953850428571429">
IP 1084 79.3% 1027 75.1% 1135 .89 .69 .93
IS 228 16.7% 208 15.2% 256 .95 .80 .83
IC 187 13.7% 109 8.0% 221 .89 .46 .51
Interpersonal
ATT+ 71 5.2% 140 10.2% 151 .94 .55 .58
ATTP 71 5.2% 30 2.2% 79 .96 .42 .44
ATT- 67 4.9% 74 5.4% 100 .96 .56 .58
</table>
<tableCaption confidence="0.955109">
Table 2: Label frequencies and inter-annotator agreement. NA,UA2 denotes the number of turns that have been
labeled with the given label by at least one annotator. PO denotes the observed agreement.
</tableCaption>
<equation confidence="0.9517405">
which is defined as
PO − PE
npool =
with
�PO = L L PEl (2)
1 l=1
l=1
L
POl , �PE = L
1
</equation>
<bodyText confidence="0.999457933333333">
where L denotes the number of labels, PEl the
expected agreement and POl the observed agree-
ment of the lth label. npool is regarded to be more
accurate than an averaged Kappa.
For assessing the overall inter-rater reliabil-
ity of the label set assignments per turn, we
chose Krippendorff’s Alpha (Krippendorff, 1980)
using MASI, a measure of agreement on set-
valued items, as the distance function (Passon-
neau, 2006). MASI accounts for partial agree-
ment if the label sets of both annotators overlap
in at least one label. We achieved an Alpha score
of α = .75. According to Krippendorff, datasets
with this score are considered reliable and allow
tentative conclusions to be drawn.
The CO label showed the lowest agreement of
only n = .18. The label was supposed to cover
any criticism that is not covered by a dedicated
label. However, the annotators reported that they
chose this label when they were unsure whether a
particular criticism label would fit a certain turn
or not.
Labels in the interpersonal category all show
agreement scores below 0.6. It turned out that the
annotators had a different understanding of these
labels. While one annotator assigned the labels
for any kind of positive or negative sentiment, the
other used the labels to express agreement and
disagreement between the participants of a dis-
cussion.
A common problem for all labels were contri-
butions with a high degree of indirectness and im-
plicitness. Indirect contributions have to be in-
terpreted in the light of conversational implica-
ture theory (Grice, 1975), which requires contex-
tual knowledge for decoding the intentions of a
speaker. For example, the message
Is population density allowed to be n/a?
has the surface form of a question. However, the
context of the discussion revealed that the author
tried to draw attention to the missing figure in the
article and requested it to be filled or removed.
The annotators rarely made use of the context,
which was a major source for disagreement in the
study.
</bodyText>
<equation confidence="0.967957">
(1)
1 − PE
</equation>
<page confidence="0.987652">
782
</page>
<bodyText confidence="0.9999916">
Another difficulty for the annotators were long
discussion turns. While the average turn consists
of 42 tokens, the largest contribution in the cor-
pus is 658 tokens long. Turns of this size can
cover multiple aspects and potentially comprise
many different dialog acts, which increases the
probability of disagreement. This issue can be ad-
dressed by going from the turn level to the utter-
ance level in future work.
A comparison of our results with the agreement
reported for other datasets shows that the reliabil-
ity of our annotations lies well within the field of
the related work. Bender et al. (2011) carried out
an annotation study of social acts in 365 discus-
sions from 47 Wikipedia Talk pages. They report
Kappa scores for thirteen labels in two categories
ranging from .13 to .66 per label. The overall
agreement for each category was .50 and .59, re-
spectively, which is considerably lower than our
nmol = .67. Kim et al. (2010b) annotate pairs of
posts taken from an online forum. They use a di-
alog act tagset with twelve labels customized for
modeling troubleshooting-oriented forum discus-
sions. For their corpus of 1334 posts, they report
an overall Kappa of .59. Kim et al. (2010a) iden-
tify unresolved discussions in student online fo-
rums by annotating 1135 posts with five different
speech acts. They report Kappa scores per speech
act between .72 and .94. Their better results might
be due to a more coarse grained label set.
</bodyText>
<subsectionHeader confidence="0.99721">
4.2 Corpus Analysis
</subsectionHeader>
<bodyText confidence="0.9999296">
The SEWD corpus contains 313 discussions con-
sisting of 1367 turns by 337 users. The average
length of a turn is 42 words. 208 of the 337
contributors are registered Wikipedia users, 129
wrote anonymously. On average, each contributor
wrote 168 words in 4 turns. However, there was a
cluster of 16 people with &gt; 20 contributions.
Table 2 shows the frequencies of all labels in
the SEWD corpus. The most frequent labels are
information providing (IP), requests (PSR) and
reports of performed edits (PPC). The IP-label
was assigned to more than 78% of all 1367 turns,
because almost every contribution provides a cer-
tain amount of information. The label was only
omitted if a turn merely consisted of a discussion
template but did not contain any text or if it exclu-
sively contained questions.
More than a quarter of the turns are labeled
with PSR and PPC, respectively. This indicates
that edit requests and reports of performed edits
are the main subject of discussion. Generally, it is
more common that edits are reported after they
have been made than to announce them before
they are carried out, as can be seen in the ratio
of PPC to PFC labels. The number of turns la-
beled with PSR is almost the same as the number
of contributions labeled with either PPC or PFC.
This allows the tentative conclusion that nearly all
requests potentially lead to an edit action. As a
matter of fact, the most common label adjacency
pair11 in the corpus is PSR—*PPC, which substan-
tiates this assumption.
Article criticism labels have been assigned to
39.4% of all turns. Almost half (241) of the labels
from this class are assigned to the first turn of a
discussion. This shows that it is common to open
a discussion in reference to a particular deficiency
of the article. The large number of CL labels com-
pared to other labels from the same category is
due to the fact that the Simple English Wikipedia
requires authors to write articles in a way that they
are understandable for non-native speakers of En-
glish. Therefore, the use of adequate language is
one of the major concerns of the Simple English
Wikipedia community.
</bodyText>
<sectionHeader confidence="0.969793" genericHeader="method">
5 Automatic Dialog Act Classification
</sectionHeader>
<bodyText confidence="0.97858115">
For the automatic classification of dialog acts in
Wikipedia Talk pages, we transform the multi-
label classification problem into a binary classi-
fication task (Tsoumakas et al., 2010). We train a
binary classifier for each label using the WEKA
data-mining software (Hall et al., 2009). We use
three learners for the classification task, a Naive
Bayes classifier, J48, an implementation of the
C4.5 decision tree algorithm (Quinlan, 1992) and
SMO, an optimization algorithm for training sup-
port vector machines (Platt, 1998). Finally, we
combine the best performing learners for each la-
bel in a UIMA-based classification pipeline (Fer-
rucci and Lally, 2004).
Features for Dialog Act Classification As fea-
tures, we use all uni-, bi- and trigrams that oc-
curred in at least three different turns. Further-
more, we include the time distance to the previ-
ous and the next turn (in seconds), the length of
the current, previous and next turn (in tokens), the
</bodyText>
<footnote confidence="0.6490485">
11A label transition A --+ B is recorded if two adjacent
turns are labeled with A and B, respectively.
</footnote>
<page confidence="0.997698">
783
</page>
<bodyText confidence="0.998137787234043">
position of the turn within the discussion, the in-
dentation level of the turn and two binary features
indicating whether a turn references or is refer-
enced by another turn.12 In order to capture the
sequential nature of the discussions, we use the
n-grams of the previous and the next turn as addi-
tional features.
Balancing Positive and Negative Instances
Since the number of positive instances for each
label is small compared to the number of nega-
tive instances, we create a balanced dataset which
contains an equal amount of positive and nega-
tive instances. Therefore, we randomly select the
appropriate number of negative instances and dis-
card the rest. This improves the classification per-
formance on every label for all three learners.
Feature Selection Using the full set of features,
we achieve the following macro/micro averaged
F1-scores: 0.29 / 0.57 for Naive Bayes, 0.42 /
0.66 for J48 and 0.43 / 0.72 for SMO. To fur-
ther improve the classification performance, we
reduce the feature space using two feature selec-
tion techniques, the x2 metric (Yang and Ped-
ersen, 1997) and the Information Gain approach
(Mitchell, 1997). For each label, we train separate
classifiers using the top 100, 200 and 300 features
obtained by each feature selection technique and
choose the best performing set for our final clas-
sification pipeline.
Indentation and temporal distance to the pre-
ceding turn proved to be the best ranked non-
lexical features overall. Additionally, the turn po-
sition within the topic was a crucial feature for
most labels in the criticism class and for PSR and
TS labels. This is not surprising, because article
criticism, suggestions and questions tend to oc-
cur in the beginning of a discussion. The two
reference features have not proven to be useful.
The relational information was better covered by
the indentation feature. The subjective quality of
the lexical features seems to be correlated with
the inter-annotator agreement of the respective la-
bels. Features for labels with low agreement con-
tain many n-grams without any recognizable se-
mantic connection to the label. For labels with
good agreement, the feature lists almost exclu-
sively contain meaningful lexical cues.
</bodyText>
<footnote confidence="0.6695415">
12A turn Y references a preceding turn X if the indenta-
tion level of Y is one level deeper than of X.
</footnote>
<table confidence="0.999876476190476">
Label Human Base Naive J48 SMO Best
Bayes
CM .66 .07 .68 .48 .66 .68
CW .55 .01 .70 .20 .56 .70
CU .40 .07 .66 .35 .59 .66
CS .69 .09 .67 .67 .75 .75
CL .77 .11 .70 .66 .73 .73
COBJ .84 .04 .78 .51 .63 .78
CO .20 .02 .61 .06 .39 .61
PSR .76 .30 .72 .70 .76 .76
PREF .62 .00 .76 .41 .64 .76
PFC .77 .04 .70 .62 .73 .73
PPC .94 .25 .74 .82 .85 .85
IP .93 .74 .83 .93 .93 .93
IS .83 .16 .79 .86 .85 .86
IC .51 .06 .67 .32 .59 .67
ATT+ .58 .10 .61 .65 .72 .72
ATTP .44 .03 .72 .25 .62 .72
ATT- .58 .07 .52 .30 .52 .52
Macro .65 .13 .70 .52 .68 .73
Micro .79 .35 .74 .75 .80 .82
</table>
<tableCaption confidence="0.92503875">
Table 3: Fl-Scores for the balanced set with feature
selection on 10-fold cross-validation. Base refers to
the baseline performance, Best to our classification
pipeline.
</tableCaption>
<bodyText confidence="0.998671592592593">
Classification Results Table 3 shows the per-
formance of all classifiers and our final classi-
fication pipeline evaluated on 10-fold cross val-
idation. Naive Bayes performed surprisingly
well and showed the best macro averaged scores
among the three learners while SMO showed the
best micro averaged performance. We compare
our results to a random baseline and to the per-
formance of the human annotators (cf. Table 3
and Figure 2). The baseline assigns the dialog act
labels at random according to their frequency dis-
tribution in the gold standard. Our classifier out-
performed the baseline significantly on all labels.
The comparison with the human performance
shows that our system is able to reach the human
performance. In most cases, the annotation agree-
ment is reliable, and so are the results of the auto-
matic classification. For the labels CU and CO,
the inter-annotator agreement is not high. The
comparably good performance of the classifiers
on these labels shows that the instances do have
shared characteristics. Human raters, however,
have difficulties recognizing these labels consis-
tently. Thus, their definitions need to be refined in
future work.
To our knowledge, none of the related work on
discourse analysis of Wikipedia Talk pages per-
</bodyText>
<page confidence="0.986455">
784
</page>
<figure confidence="0.998887625">
F1-score
0.8
0.6
0.4
0.2
0
1
Best Human Baseline
</figure>
<figureCaption confidence="0.999612">
Figure 2: Fl-Scores for our classification pipeline (Best), the human performance and baseline performance.
</figureCaption>
<bodyText confidence="0.9999716875">
formed automatic dialog act classification. How-
ever, there has been previous work on classify-
ing speech acts in other discourse types. Kim et
al. (2010a) use Support Vector Machines (SVM)
and Transformation Based Learning (TBL) for
the automatic assignment of five speech acts to
posts taken from student online forums. They re-
port individual F1-scores per label which result
in a macro average of 0.59 for SVM and 0.66
for TBL. Cohen et al. (2004) classify speech acts
in emails. They train five binary classifiers us-
ing several learners on 1375 emails and report F1
scores per speech act between .44 and .85. De-
spite the larger tagset, our classification approach
achieves an average F1-score of .82 and therefore
lies in the top ranks of the related work.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999940913043478">
In this paper, we proposed an annotation schema
for the discourse analysis of Wikipedia discus-
sions aimed at the coordination efforts for article
improvement. We applied the annotation schema
to a corpus of 100 Wikipedia Talk pages, which
we make freely available for download. A thor-
ough analysis of the inter-annotator agreement
showed that the dataset is reliable. Finally, we
performed automatic dialog act classification on
Wikipedia Talk pages. Therefore, we combined
three machine learning algorithms and two feature
selection techniques to a classification pipeline,
which we trained on our SEWD corpus. We
achieve an average F1-score of .82, which is com-
parable to the human performance of .79. The
ability to automatically classify discussion pages
will help to investigate the relations between arti-
cle discussions and article edits, which is an im-
portant step towards understanding the processes
of collaboration in large-scale Wikis. Further-
more, it will be the basis for practical applications
that bring the hidden content of Talk pages to the
attention of article readers.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999895">
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the Hessian research excellence program
“Landes-Offensive zur Entwicklung Wissen-
schaftlich-¨okonomischer Exzellenz” (LOEWE)
as part of the research center “Digital Humani-
ties”.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99944624">
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555–596, December.
John L. Austin. 1962. How to Do Things with Words.
Clarendon Press, Cambridge, UK.
Emily M. Bender, Jonathan T. Morgan, Meghan Ox-
ley, Mark Zachry, Brian Hutchinson, Alex Marin,
Bin Zhang, and Mari Ostendorf. 2011. Annotat-
ing Social Acts: Authority Claims and Alignment
Moves in Wikipedia Talk Pages. In Proceedings of
the Workshop on Language in Social Media, pages
48–57, Portland, Oregon, USA.
Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics, 22(2):249–254.
Tamitha Carpenter and Emi Fujioka. 2011. The Role
and Identification of Dialog Acts in Online Chat. In
Proceesings of the Workshop on Analyzing Micro-
text at the 25th AAAI Conference on Artificial Intel-
ligence, San Francisco, CA, USA.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to Classify Email into
”Speech Acts”. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 309–316, Barcelona, ES.
</reference>
<page confidence="0.980065">
785
</page>
<reference confidence="0.999708614035088">
Mark G. Core and James F. Allen. 1997. Cod-
ing dialogs with the DAMSL annotation scheme.
In Proceedings of the Working Notes of the AAAI
Fall Symposium on Communicative Action in Hu-
mans and Machines, pages 28–35, Cambridge, MA,
USA.
Han De Vries, Marc N. Elliott, David E. Kanouse, and
Stephanie S. Teleki. 2008. Using Pooled Kappa
to Summarize Interrater Agreement across Many
Items. Field Methods, 20(3):272–282.
David Ferrucci and Adam Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10:327–348.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently
Accessing Wikipedia’s Edit History. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. System Demonstrations, pages
97–102, Portland, OR, USA.
Paul Grice. 1975. Logic and Conversation. In Pe-
ter Cole and Jerry L. Morgan, editors, Syntax and
Semantics, volume 3. New York: Academic Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11:10–18.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the f-measure, and reliability in infor-
mation retrieval. Journal of the American Medical
Informatics Association, 12(3):296–298.
Dan Jurafsky, Liz Shriberg, and Debbra Biasca. 1997.
Switchboard SWBD-DAMSL Shallow-Discourse-
Function Annotation Coders Manual. Technical
Report Draft 13, University of Colorado, Institute
of Cognitive Science.
Jihie Kim, Jia Li, and Taehwan Kim. 2010a. To-
wards Identifying Unresolved Discussions in Stu-
dent Online Forums. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use ofNLP
for Building Educational Applications, pages 84–
91, Los Angeles, CA, USA.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. In Pro-
ceedings of the Fourteenth Conference on Compu-
tational Natural Language Learning, CoNLL ’10,
pages 192–202, Stroudsburg, PA, USA.
Klaus Krippendorff. 1980. Content Analysis: An
Introduction to Its Methodology. Thousand Oaks,
CA: Sage Publications.
J. Richard Landis and Gary G. Koch. 1977. An Appli-
cation of Hierarchical Kappa-type Statistics in the
Assessment of Majority Agreement among Multi-
ple Observers. Biometrics, 33(2):363–374, June.
David Laniado, Riccardo Tasso, Yana Volkovich, and
Andreas Kaltenbrunner. 2011. When the Wikipedi-
ans Talk: Network and Tree Structure of Wikipedia
Discussion Pages. In Proceedings of the 5th Inter-
national AAAI Conference on Weblogs and Social
Media, Dublin, IE.
Tom Mitchell. 1997. Machine Learning. McGraw-
Hill Education (ISE Editions), 1st edition.
Rebecca Passonneau. 2006. Measuring Agreement on
Set-valued Items (MASI) for Semantic and Prag-
matic Annotation. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation, Genoa, IT.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization.
In Advances in Kernel Methods: Support Vector
Learning, pages 185–208, Cambridge, MA, USA.
Ilona R. Posner and Ronald M. Baecker. 1992. How
People Write Together. In Proceedings of the 25th
Hawaii International Conference on System Sci-
ences, pages 127–138, Wailea, Maui, HI, USA.
Ross Quinlan. 1992. C4.5: Programs for Machine
Learning. Morgan Kaufmann, 1st edition.
Jodi Schneider, Alexandre Passant, and John G. Bres-
lin. 2011. Understanding and Improving Wikipedia
Article Discussion Spaces. In Proceedings of the
26th Symposium on Applied Computing, Taichung,
TW.
John R. Searle. 1969. Speech Acts. Cambridge Uni-
versity Press, Cambridge, UK.
John R. Searle. 1976. A classification of illocutionary
acts. Language in Society, 5:1–23.
Besiki Stvilia, Michael B. Twidale, Linda C. Smith,
and Les Gasser. 2008. Information Quality Work
Organization in Wikipedia. Journal of the Ameri-
can Society for Information Science, 59:983–1001.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis P.
Vlahavas. 2010. Mining multi-label data. In Data
Mining and Knowledge Discovery Handbook, pages
667–685. Springer.
Fernanda Vi´egas, Martin Wattenberg, Jesse Kriss, and
Frank Ham. 2007. Talk Before You Type: Coor-
dination in Wikipedia. In Proceedings of the 40th
Annual Hawaii International Conference on System
Sciences, Waikoloa, Big Island, HI, USA.
Eti Yaari, Shifra Baruchson-Arbib, and Judit Bar-Ilan.
2011. Information quality assessment of commu-
nity generated content: A user study of Wikipedia.
Journal of Information Science, 37:487–498.
Yiming Yang and Jan O. Pedersen. 1997. A Compara-
tive Study on Feature Selection in Text Categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, pages 412–420,
San Francisco, CA, USA.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, MA.
</reference>
<page confidence="0.998405">
786
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685117">
<title confidence="0.999315">Behind the Article: Recognizing Dialog Acts in Wikipedia Talk Pages</title>
<author confidence="0.843041">Iryna Yevgen</author>
<affiliation confidence="0.8821502">Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information Knowledge Processing Lab Department of Computer Technische Universit¨at Darmstadt</affiliation>
<web confidence="0.95458">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.999656076923077">In this paper, we propose an annotation schema for the discourse analysis of Wikipedia Talk pages aimed at the coordination efforts for article improvement. We apply the annotation schema to a corpus of 100 Talk pages from the Simple English Wikipedia and make the resulting freely available for Furthermore, we perform automatic dialog act classification on Wikipedia discussions and an average of our classification pipeline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-Coder Agreement for Computational Linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="20227" citStr="Artstein and Poesio, 2008" startWordPosition="3230" endWordPosition="3233">er-rater agreement study. For measuring the agreement of the individual labels, we report the observed agreement, Kappa statistics (Carletta, 1996), and F1-scores. The latter are computed by treating one annotator as the gold standard and the other one as predictions (Hripcsak and Rothschild, 2005). The scores can be seen in Table 2. The average observed agreement across all labels is �PO = .94. The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). Furthermore, we obtain an overall pooled Kappa (De Vries et al., 2008) of npool = .67, 8http://www.mmax2.net 9http://www.ukp.tu-darmstadt.de/data/ wikidiscourse 10http://uima.apache.org 781 Annotator 1 N Percent Annotator 2 N Percent Inter-Annotator Agreement NA,UA2 PO r, Fl Gold Standard N Percent Label Article Criticism CM 183 13.4% 105 7.7% 193 .93 .63 .66 116 8.5% CW 106 7.8% 57 4.2% 120 .95 .52 .55 70 5.1% CU 69 5.0% 35 2.6% 83 .95 .38 .40 42 3.1% CS 164 12.0% 101 7.4% 174 .94 .66 .69 136 9.9% CL 195 14.3% 199 14.6% 244 .93 .73 .77 219 16.0% COBJ 27 2.0% 23 1.7% 29 .99 .84 .84 27 2.0% C</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational Linguistics, 34(4):555–596, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John L Austin</author>
</authors>
<title>How to Do Things with Words.</title>
<date>1962</date>
<publisher>Clarendon Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="3314" citStr="Austin, 1962" startWordPosition="510" endWordPosition="511">ound Wikipedia regulates and enforces standards of behavior and article quality. The user discussions on the article Talk pages might shed light on this issue and give an insight into the otherwise hidden processes of collaboration that, until now, could only be analyzed via interviews or group observations in experimental settings. The main goal of the present paper is to analyze the content of the discussion pages of the Simple English Wikipedia with respect to the dialog acts aimed at the coordination efforts for article improvement. Dialog acts, according to the classic speech act theory (Austin, 1962; Searle, 1969), represent the meaning of an utterance at the level of illocutionary force, i.e. a dialog act label concisely characterizes the intention and the role of a contribution in a dialog. We chose the Simple English Wikipedia for our initial analysis, because we are able to obtain more representative results 777 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 777–786, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics by covering almost 15% of all relevant Talk pages, as opposed to t</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>John L. Austin. 1962. How to Do Things with Words. Clarendon Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Jonathan T Morgan</author>
<author>Meghan Oxley</author>
<author>Mark Zachry</author>
<author>Brian Hutchinson</author>
<author>Alex Marin</author>
<author>Bin Zhang</author>
<author>Mari Ostendorf</author>
</authors>
<title>Annotating Social Acts: Authority Claims and Alignment Moves in Wikipedia Talk Pages.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media,</booktitle>
<pages>48--57</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="7223" citStr="Bender et al. (2011)" startWordPosition="1127" endWordPosition="1130">), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions. Bender et al. (2011) describe a corpus of 47 Talk pages which have been annotated for authority claims and alignment moves. With this corpus, the authors analyze how the participants in Wikipedia discussions establish their credibility and how they express agreement and disagreement towards other participants or topics. From a different perspective, Stvilia et al. (2008) analyze 60 discussion pages in regard to how information quality (IQ) in Wikipedia articles is assessed on the Talk pages and which types of IQ problems are identified by the community. They describe a Wikipedia IQ assessment model and map it to </context>
<context position="24295" citStr="Bender et al. (2011)" startWordPosition="3962" endWordPosition="3965"> (1) 1 − PE 782 Another difficulty for the annotators were long discussion turns. While the average turn consists of 42 tokens, the largest contribution in the corpus is 658 tokens long. Turns of this size can cover multiple aspects and potentially comprise many different dialog acts, which increases the probability of disagreement. This issue can be addressed by going from the turn level to the utterance level in future work. A comparison of our results with the agreement reported for other datasets shows that the reliability of our annotations lies well within the field of the related work. Bender et al. (2011) carried out an annotation study of social acts in 365 discussions from 47 Wikipedia Talk pages. They report Kappa scores for thirteen labels in two categories ranging from .13 to .66 per label. The overall agreement for each category was .50 and .59, respectively, which is considerably lower than our nmol = .67. Kim et al. (2010b) annotate pairs of posts taken from an online forum. They use a dialog act tagset with twelve labels customized for modeling troubleshooting-oriented forum discussions. For their corpus of 1334 posts, they report an overall Kappa of .59. Kim et al. (2010a) identify u</context>
</contexts>
<marker>Bender, Morgan, Oxley, Zachry, Hutchinson, Marin, Zhang, Ostendorf, 2011</marker>
<rawString>Emily M. Bender, Jonathan T. Morgan, Meghan Oxley, Mark Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and Mari Ostendorf. 2011. Annotating Social Acts: Authority Claims and Alignment Moves in Wikipedia Talk Pages. In Proceedings of the Workshop on Language in Social Media, pages 48–57, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing Agreement on Classification Tasks: The Kappa Statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="19748" citStr="Carletta, 1996" startWordPosition="3151" endWordPosition="3152">discussed in Section 4.2. Corpus Format We publish our SEWD corpus in two formats9, the original MMAX format, and as XMI files for further processing with the Apache Unstructured Information Management Architecture10. For the latter format, we also provide the type system which defines all necessary corpus specific types needed for using the data in an NLP pipeline. 4.1 Inter-Annotator Agreement To evaluate the reliability of our dataset, we perform a detailed inter-rater agreement study. For measuring the agreement of the individual labels, we report the observed agreement, Kappa statistics (Carletta, 1996), and F1-scores. The latter are computed by treating one annotator as the gold standard and the other one as predictions (Hripcsak and Rothschild, 2005). The scores can be seen in Table 2. The average observed agreement across all labels is �PO = .94. The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). Furthermore, we obtain an overall pooled Kappa (De Vries et al., 2008) of npool = .67, 8http://www.mmax2.net 9http://ww</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing Agreement on Classification Tasks: The Kappa Statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamitha Carpenter</author>
<author>Emi Fujioka</author>
</authors>
<title>The Role and Identification of Dialog Acts in Online Chat.</title>
<date>2011</date>
<booktitle>In Proceesings of the Workshop on Analyzing Microtext at the 25th AAAI Conference on Artificial Intelligence,</booktitle>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="6641" citStr="Carpenter and Fujioka, 2011" startWordPosition="1030" endWordPosition="1033">esource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct labels which have been clustered to 42 coarse grained labels. Both schemata have often been adapted for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing user generated discourse substantially increased. In addition to analyzing web forums (Kim et al., 2010a), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions. Bender et al. (2011) describe a corpus</context>
</contexts>
<marker>Carpenter, Fujioka, 2011</marker>
<rawString>Tamitha Carpenter and Emi Fujioka. 2011. The Role and Identification of Dialog Acts in Online Chat. In Proceesings of the Workshop on Analyzing Microtext at the 25th AAAI Conference on Artificial Intelligence, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning to Classify Email into ”Speech Acts”.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>309--316</pages>
<location>Barcelona, ES.</location>
<contexts>
<context position="6673" citStr="Cohen et al., 2004" startWordPosition="1036" endWordPosition="1039">ional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct labels which have been clustered to 42 coarse grained labels. Both schemata have often been adapted for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing user generated discourse substantially increased. In addition to analyzing web forums (Kim et al., 2010a), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions. Bender et al. (2011) describe a corpus of 47 Talk pages which have bee</context>
<context position="33261" citStr="Cohen et al. (2004)" startWordPosition="5504" endWordPosition="5507"> pages per784 F1-score 0.8 0.6 0.4 0.2 0 1 Best Human Baseline Figure 2: Fl-Scores for our classification pipeline (Best), the human performance and baseline performance. formed automatic dialog act classification. However, there has been previous work on classifying speech acts in other discourse types. Kim et al. (2010a) use Support Vector Machines (SVM) and Transformation Based Learning (TBL) for the automatic assignment of five speech acts to posts taken from student online forums. They report individual F1-scores per label which result in a macro average of 0.59 for SVM and 0.66 for TBL. Cohen et al. (2004) classify speech acts in emails. They train five binary classifiers using several learners on 1375 emails and report F1 scores per speech act between .44 and .85. Despite the larger tagset, our classification approach achieves an average F1-score of .82 and therefore lies in the top ranks of the related work. 6 Conclusions In this paper, we proposed an annotation schema for the discourse analysis of Wikipedia discussions aimed at the coordination efforts for article improvement. We applied the annotation schema to a corpus of 100 Wikipedia Talk pages, which we make freely available for downloa</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell. 2004. Learning to Classify Email into ”Speech Acts”. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 309–316, Barcelona, ES.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James F Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>In Proceedings of the Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<pages>28--35</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="5902" citStr="Core and Allen, 1997" startWordPosition="917" endWordPosition="920">lated Work The analysis of speech and dialog acts has its roots in the linguistic field of pragmatics. In 1962, John Austin shifted the focus from the mere declarative use of language as a means for making factual statements towards its non-declarative use as a tool for performing actions. The speech act theory was further systematized by Searle (1969), whose classification of illocutionary acts (Searle, 1976) is still used as a starting point for creating dialog act classification schemata for natural language processing. A well known, domain- and task-independent annotation schema is DAMSL (Core and Allen, 1997). It was created as the standard annotation schema for dialog tagging on the utterance level by the Discourse Resource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct labels which have been clustered to 42 coarse grained labels. Both schemata have often been adapted for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing use</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James F. Allen. 1997. Coding dialogs with the DAMSL annotation scheme. In Proceedings of the Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines, pages 28–35, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han De Vries</author>
<author>Marc N Elliott</author>
<author>David E Kanouse</author>
<author>Stephanie S Teleki</author>
</authors>
<title>Using Pooled Kappa to Summarize Interrater Agreement across Many Items. Field Methods,</title>
<date>2008</date>
<marker>De Vries, Elliott, Kanouse, Teleki, 2008</marker>
<rawString>Han De Vries, Marc N. Elliott, David E. Kanouse, and Stephanie S. Teleki. 2008. Using Pooled Kappa to Summarize Interrater Agreement across Many Items. Field Methods, 20(3):272–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>UIMA: An Architectural Approach to Unstructured</title>
<date>2004</date>
<booktitle>Information Processing in the Corporate Research Environment. Natural Language Engineering,</booktitle>
<pages>10--327</pages>
<contexts>
<context position="27954" citStr="Ferrucci and Lally, 2004" startWordPosition="4583" endWordPosition="4587">ion of dialog acts in Wikipedia Talk pages, we transform the multilabel classification problem into a binary classification task (Tsoumakas et al., 2010). We train a binary classifier for each label using the WEKA data-mining software (Hall et al., 2009). We use three learners for the classification task, a Naive Bayes classifier, J48, an implementation of the C4.5 decision tree algorithm (Quinlan, 1992) and SMO, an optimization algorithm for training support vector machines (Platt, 1998). Finally, we combine the best performing learners for each label in a UIMA-based classification pipeline (Ferrucci and Lally, 2004). Features for Dialog Act Classification As features, we use all uni-, bi- and trigrams that occurred in at least three different turns. Furthermore, we include the time distance to the previous and the next turn (in seconds), the length of the current, previous and next turn (in tokens), the 11A label transition A --+ B is recorded if two adjacent turns are labeled with A and B, respectively. 783 position of the turn within the discussion, the indentation level of the turn and two binary features indicating whether a turn references or is referenced by another turn.12 In order to capture the </context>
</contexts>
<marker>Ferrucci, Lally, 2004</marker>
<rawString>David Ferrucci and Adam Lally. 2004. UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment. Natural Language Engineering, 10:327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Ferschke</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. System Demonstrations,</booktitle>
<pages>97--102</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="16710" citStr="Ferschke et al., 2011" startWordPosition="2638" endWordPosition="2641">entation. For retrieving the discussion pages, we use the Java Wikipedia Library (JWPL) (Zesch et al., 2008), which offers efficient, databasedriven access to the contents of Wikipedia. We segment the individual Talk pages into discussions topics using the MediaWiki parser that comes with JWPL. In our corpus, the parser managed to identify all topic boundaries without any errors. The most complex preprocessing step is the turn segmentation. First, we use the revision history of the Talk page to identify the author and the creation time of each paragraph. We use the Wikipedia Revision Toolkit (Ferschke et al., 2011) to examine the changes between adjacent revisions of the Talk page in order to identify the exact time a piece of text was added as well as the author of the contribution. We have to filter out malicious edits from the history, as they would negatively affect the segmentation process. We therefore disregard all edits that are reverted in later later revisions. In contrast to vandalism on article pages, this approach has proven to be sufficient to detect vandalism in the Talk page history. Within each discussion topic, we aggregate all adjacent paragraphs with the same author and the same time</context>
</contexts>
<marker>Ferschke, Zesch, Gurevych, 2011</marker>
<rawString>Oliver Ferschke, Torsten Zesch, and Iryna Gurevych. 2011. Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. System Demonstrations, pages 97–102, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Grice</author>
</authors>
<title>Logic and Conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics,</booktitle>
<volume>3</volume>
<editor>In Peter Cole and Jerry L. Morgan, editors,</editor>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="23224" citStr="Grice, 1975" startWordPosition="3780" endWordPosition="3781"> criticism label would fit a certain turn or not. Labels in the interpersonal category all show agreement scores below 0.6. It turned out that the annotators had a different understanding of these labels. While one annotator assigned the labels for any kind of positive or negative sentiment, the other used the labels to express agreement and disagreement between the participants of a discussion. A common problem for all labels were contributions with a high degree of indirectness and implicitness. Indirect contributions have to be interpreted in the light of conversational implicature theory (Grice, 1975), which requires contextual knowledge for decoding the intentions of a speaker. For example, the message Is population density allowed to be n/a? has the surface form of a question. However, the context of the discussion revealed that the author tried to draw attention to the missing figure in the article and requested it to be filled or removed. The annotators rarely made use of the context, which was a major source for disagreement in the study. (1) 1 − PE 782 Another difficulty for the annotators were long discussion turns. While the average turn consists of 42 tokens, the largest contribut</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Paul Grice. 1975. Logic and Conversation. In Peter Cole and Jerry L. Morgan, editors, Syntax and Semantics, volume 3. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<pages>11--10</pages>
<contexts>
<context position="27583" citStr="Hall et al., 2009" startWordPosition="4527" endWordPosition="4530">from the same category is due to the fact that the Simple English Wikipedia requires authors to write articles in a way that they are understandable for non-native speakers of English. Therefore, the use of adequate language is one of the major concerns of the Simple English Wikipedia community. 5 Automatic Dialog Act Classification For the automatic classification of dialog acts in Wikipedia Talk pages, we transform the multilabel classification problem into a binary classification task (Tsoumakas et al., 2010). We train a binary classifier for each label using the WEKA data-mining software (Hall et al., 2009). We use three learners for the classification task, a Naive Bayes classifier, J48, an implementation of the C4.5 decision tree algorithm (Quinlan, 1992) and SMO, an optimization algorithm for training support vector machines (Platt, 1998). Finally, we combine the best performing learners for each label in a UIMA-based classification pipeline (Ferrucci and Lally, 2004). Features for Dialog Act Classification As features, we use all uni-, bi- and trigrams that occurred in at least three different turns. Furthermore, we include the time distance to the previous and the next turn (in seconds), th</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11:10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Hripcsak</author>
<author>Adam S Rothschild</author>
</authors>
<title>Agreement, the f-measure, and reliability in information retrieval.</title>
<date>2005</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="19900" citStr="Hripcsak and Rothschild, 2005" startWordPosition="3173" endWordPosition="3177">r processing with the Apache Unstructured Information Management Architecture10. For the latter format, we also provide the type system which defines all necessary corpus specific types needed for using the data in an NLP pipeline. 4.1 Inter-Annotator Agreement To evaluate the reliability of our dataset, we perform a detailed inter-rater agreement study. For measuring the agreement of the individual labels, we report the observed agreement, Kappa statistics (Carletta, 1996), and F1-scores. The latter are computed by treating one annotator as the gold standard and the other one as predictions (Hripcsak and Rothschild, 2005). The scores can be seen in Table 2. The average observed agreement across all labels is �PO = .94. The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). Furthermore, we obtain an overall pooled Kappa (De Vries et al., 2008) of npool = .67, 8http://www.mmax2.net 9http://www.ukp.tu-darmstadt.de/data/ wikidiscourse 10http://uima.apache.org 781 Annotator 1 N Percent Annotator 2 N Percent Inter-Annotator Agreement NA,UA2 PO r</context>
</contexts>
<marker>Hripcsak, Rothschild, 2005</marker>
<rawString>George Hripcsak and Adam S. Rothschild. 2005. Agreement, the f-measure, and reliability in information retrieval. Journal of the American Medical Informatics Association, 12(3):296–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Liz Shriberg</author>
<author>Debbra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL Shallow-DiscourseFunction Annotation Coders Manual.</title>
<date>1997</date>
<tech>Technical Report Draft 13,</tech>
<institution>University of Colorado, Institute of Cognitive Science.</institution>
<contexts>
<context position="6150" citStr="Jurafsky et al. (1997)" startWordPosition="954" endWordPosition="957">ative use as a tool for performing actions. The speech act theory was further systematized by Searle (1969), whose classification of illocutionary acts (Searle, 1976) is still used as a starting point for creating dialog act classification schemata for natural language processing. A well known, domain- and task-independent annotation schema is DAMSL (Core and Allen, 1997). It was created as the standard annotation schema for dialog tagging on the utterance level by the Discourse Resource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct labels which have been clustered to 42 coarse grained labels. Both schemata have often been adapted for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing user generated discourse substantially increased. In addition to analyzing web forums (Kim et al., 2010a), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of th</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Dan Jurafsky, Liz Shriberg, and Debbra Biasca. 1997. Switchboard SWBD-DAMSL Shallow-DiscourseFunction Annotation Coders Manual. Technical Report Draft 13, University of Colorado, Institute of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jihie Kim</author>
<author>Jia Li</author>
<author>Taehwan Kim</author>
</authors>
<title>Towards Identifying Unresolved Discussions in Student Online Forums.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use ofNLP for Building Educational Applications,</booktitle>
<pages>84--91</pages>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="6602" citStr="Kim et al., 2010" startWordPosition="1025" endWordPosition="1028">ce level by the Discourse Resource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct labels which have been clustered to 42 coarse grained labels. Both schemata have often been adapted for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing user generated discourse substantially increased. In addition to analyzing web forums (Kim et al., 2010a), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions.</context>
<context position="24626" citStr="Kim et al. (2010" startWordPosition="4021" endWordPosition="4024">This issue can be addressed by going from the turn level to the utterance level in future work. A comparison of our results with the agreement reported for other datasets shows that the reliability of our annotations lies well within the field of the related work. Bender et al. (2011) carried out an annotation study of social acts in 365 discussions from 47 Wikipedia Talk pages. They report Kappa scores for thirteen labels in two categories ranging from .13 to .66 per label. The overall agreement for each category was .50 and .59, respectively, which is considerably lower than our nmol = .67. Kim et al. (2010b) annotate pairs of posts taken from an online forum. They use a dialog act tagset with twelve labels customized for modeling troubleshooting-oriented forum discussions. For their corpus of 1334 posts, they report an overall Kappa of .59. Kim et al. (2010a) identify unresolved discussions in student online forums by annotating 1135 posts with five different speech acts. They report Kappa scores per speech act between .72 and .94. Their better results might be due to a more coarse grained label set. 4.2 Corpus Analysis The SEWD corpus contains 313 discussions consisting of 1367 turns by 337 us</context>
<context position="32964" citStr="Kim et al. (2010" startWordPosition="5454" endWordPosition="5457">on these labels shows that the instances do have shared characteristics. Human raters, however, have difficulties recognizing these labels consistently. Thus, their definitions need to be refined in future work. To our knowledge, none of the related work on discourse analysis of Wikipedia Talk pages per784 F1-score 0.8 0.6 0.4 0.2 0 1 Best Human Baseline Figure 2: Fl-Scores for our classification pipeline (Best), the human performance and baseline performance. formed automatic dialog act classification. However, there has been previous work on classifying speech acts in other discourse types. Kim et al. (2010a) use Support Vector Machines (SVM) and Transformation Based Learning (TBL) for the automatic assignment of five speech acts to posts taken from student online forums. They report individual F1-scores per label which result in a macro average of 0.59 for SVM and 0.66 for TBL. Cohen et al. (2004) classify speech acts in emails. They train five binary classifiers using several learners on 1375 emails and report F1 scores per speech act between .44 and .85. Despite the larger tagset, our classification approach achieves an average F1-score of .82 and therefore lies in the top ranks of the relate</context>
</contexts>
<marker>Kim, Li, Kim, 2010</marker>
<rawString>Jihie Kim, Jia Li, and Taehwan Kim. 2010a. Towards Identifying Unresolved Discussions in Student Online Forums. In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use ofNLP for Building Educational Applications, pages 84– 91, Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Li Wang</author>
<author>Timothy Baldwin</author>
</authors>
<title>Tagging and linking web forum posts.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10,</booktitle>
<pages>192--202</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6602" citStr="Kim et al., 2010" startWordPosition="1025" endWordPosition="1028">ce level by the Discourse Resource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct labels which have been clustered to 42 coarse grained labels. Both schemata have often been adapted for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing user generated discourse substantially increased. In addition to analyzing web forums (Kim et al., 2010a), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions.</context>
<context position="24626" citStr="Kim et al. (2010" startWordPosition="4021" endWordPosition="4024">This issue can be addressed by going from the turn level to the utterance level in future work. A comparison of our results with the agreement reported for other datasets shows that the reliability of our annotations lies well within the field of the related work. Bender et al. (2011) carried out an annotation study of social acts in 365 discussions from 47 Wikipedia Talk pages. They report Kappa scores for thirteen labels in two categories ranging from .13 to .66 per label. The overall agreement for each category was .50 and .59, respectively, which is considerably lower than our nmol = .67. Kim et al. (2010b) annotate pairs of posts taken from an online forum. They use a dialog act tagset with twelve labels customized for modeling troubleshooting-oriented forum discussions. For their corpus of 1334 posts, they report an overall Kappa of .59. Kim et al. (2010a) identify unresolved discussions in student online forums by annotating 1135 posts with five different speech acts. They report Kappa scores per speech act between .72 and .94. Their better results might be due to a more coarse grained label set. 4.2 Corpus Analysis The SEWD corpus contains 313 discussions consisting of 1367 turns by 337 us</context>
<context position="32964" citStr="Kim et al. (2010" startWordPosition="5454" endWordPosition="5457">on these labels shows that the instances do have shared characteristics. Human raters, however, have difficulties recognizing these labels consistently. Thus, their definitions need to be refined in future work. To our knowledge, none of the related work on discourse analysis of Wikipedia Talk pages per784 F1-score 0.8 0.6 0.4 0.2 0 1 Best Human Baseline Figure 2: Fl-Scores for our classification pipeline (Best), the human performance and baseline performance. formed automatic dialog act classification. However, there has been previous work on classifying speech acts in other discourse types. Kim et al. (2010a) use Support Vector Machines (SVM) and Transformation Based Learning (TBL) for the automatic assignment of five speech acts to posts taken from student online forums. They report individual F1-scores per label which result in a macro average of 0.59 for SVM and 0.66 for TBL. Cohen et al. (2004) classify speech acts in emails. They train five binary classifiers using several learners on 1375 emails and report F1 scores per speech act between .44 and .85. Despite the larger tagset, our classification approach achieves an average F1-score of .82 and therefore lies in the top ranks of the relate</context>
</contexts>
<marker>Kim, Wang, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Li Wang, and Timothy Baldwin. 2010b. Tagging and linking web forum posts. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10, pages 192–202, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Thousand Oaks,</title>
<date>1980</date>
<publisher>Sage Publications.</publisher>
<location>CA:</location>
<contexts>
<context position="21999" citStr="Krippendorff, 1980" startWordPosition="3575" endWordPosition="3576">% 100 .96 .56 .58 Table 2: Label frequencies and inter-annotator agreement. NA,UA2 denotes the number of turns that have been labeled with the given label by at least one annotator. PO denotes the observed agreement. which is defined as PO − PE npool = with �PO = L L PEl (2) 1 l=1 l=1 L POl , �PE = L 1 where L denotes the number of labels, PEl the expected agreement and POl the observed agreement of the lth label. npool is regarded to be more accurate than an averaged Kappa. For assessing the overall inter-rater reliability of the label set assignments per turn, we chose Krippendorff’s Alpha (Krippendorff, 1980) using MASI, a measure of agreement on setvalued items, as the distance function (Passonneau, 2006). MASI accounts for partial agreement if the label sets of both annotators overlap in at least one label. We achieved an Alpha score of α = .75. According to Krippendorff, datasets with this score are considered reliable and allow tentative conclusions to be drawn. The CO label showed the lowest agreement of only n = .18. The label was supposed to cover any criticism that is not covered by a dedicated label. However, the annotators reported that they chose this label when they were unsure whether</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Thousand Oaks, CA: Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>An Application of Hierarchical Kappa-type Statistics in the Assessment of Majority Agreement among Multiple Observers.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="20083" citStr="Landis and Koch (1977)" startWordPosition="3208" endWordPosition="3211">d for using the data in an NLP pipeline. 4.1 Inter-Annotator Agreement To evaluate the reliability of our dataset, we perform a detailed inter-rater agreement study. For measuring the agreement of the individual labels, we report the observed agreement, Kappa statistics (Carletta, 1996), and F1-scores. The latter are computed by treating one annotator as the gold standard and the other one as predictions (Hripcsak and Rothschild, 2005). The scores can be seen in Table 2. The average observed agreement across all labels is �PO = .94. The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). Furthermore, we obtain an overall pooled Kappa (De Vries et al., 2008) of npool = .67, 8http://www.mmax2.net 9http://www.ukp.tu-darmstadt.de/data/ wikidiscourse 10http://uima.apache.org 781 Annotator 1 N Percent Annotator 2 N Percent Inter-Annotator Agreement NA,UA2 PO r, Fl Gold Standard N Percent Label Article Criticism CM 183 13.4% 105 7.7% 193 .93 .63 .66 116 8.5% CW 106 7.8% 57 4.2% 120 .95 .52 .55 70 5.1% CU 69 5.0% 35 2.6% 83 .95 .38 .40 42 3.</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. An Application of Hierarchical Kappa-type Statistics in the Assessment of Majority Agreement among Multiple Observers. Biometrics, 33(2):363–374, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Laniado</author>
<author>Riccardo Tasso</author>
<author>Yana Volkovich</author>
<author>Andreas Kaltenbrunner</author>
</authors>
<title>When the Wikipedians Talk: Network and Tree Structure of Wikipedia Discussion Pages.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International AAAI Conference on Weblogs and Social</booktitle>
<location>Media, Dublin, IE.</location>
<contexts>
<context position="8042" citStr="Laniado et al. (2011)" startWordPosition="1260" endWordPosition="1263">their credibility and how they express agreement and disagreement towards other participants or topics. From a different perspective, Stvilia et al. (2008) analyze 60 discussion pages in regard to how information quality (IQ) in Wikipedia articles is assessed on the Talk pages and which types of IQ problems are identified by the community. They describe a Wikipedia IQ assessment model and map it to established frameworks. Furthermore, they provide a list of IQ problems along with related causal factors and necessary actions which has also inspired the design of our annotation schema. Finally, Laniado et al. (2011) examine Wikipedia discussion networks in order to capture structural patterns of interaction. They extract the thread structure from all Talk pages in the English Wikipedia and create tree structures of the discussion. The analysis of the graphs reveals patterns that are unique to Wikipedia discussions and might be used as a means to characterize different types of Talk pages. To the best of our knowledge, there is no work yet that uses machine learning to automati778 Figure 1: Structure of a Talk page: a) Talk page title, b) untitled discussion topic, c) titled discussion topic, d) unsigned </context>
<context position="15788" citStr="Laniado et al. (2011)" startWordPosition="2491" endWordPosition="2494">ined three classes: (i) discussion pages with 4-10 turns, (ii) 5The snapshot contains 69900 articles and 5783 Talk pages of which 683 contained more than 3 contributions. pages with 11-20 turns, and (iii) pages with more than 20 turns. We then randomly extracted 50 discussion pages from class (i), 40 pages from class (ii) and 10 pages from class (iii). This decision is grounded in the restricted resources for the human annotation task. Data Preprocessing Due to a lack of discussion structure, extracting the discussion threads from the Talk pages requires a substantial amount of preprocessing. Laniado et al. (2011) tackle the thread extraction by using text indentation and inserted user signatures as clues. We found these 780 attributes to be insufficient for a reliable reconstruction of the thread structure.6 Our preprocessing approach consists of three steps: data retrieval, topic segmentation and turn segmentation. For retrieving the discussion pages, we use the Java Wikipedia Library (JWPL) (Zesch et al., 2008), which offers efficient, databasedriven access to the contents of Wikipedia. We segment the individual Talk pages into discussions topics using the MediaWiki parser that comes with JWPL. In o</context>
</contexts>
<marker>Laniado, Tasso, Volkovich, Kaltenbrunner, 2011</marker>
<rawString>David Laniado, Riccardo Tasso, Yana Volkovich, and Andreas Kaltenbrunner. 2011. When the Wikipedians Talk: Network and Tree Structure of Wikipedia Discussion Pages. In Proceedings of the 5th International AAAI Conference on Weblogs and Social Media, Dublin, IE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning. McGrawHill Education (ISE Editions), 1st edition.</booktitle>
<contexts>
<context position="29487" citStr="Mitchell, 1997" startWordPosition="4847" endWordPosition="4848"> amount of positive and negative instances. Therefore, we randomly select the appropriate number of negative instances and discard the rest. This improves the classification performance on every label for all three learners. Feature Selection Using the full set of features, we achieve the following macro/micro averaged F1-scores: 0.29 / 0.57 for Naive Bayes, 0.42 / 0.66 for J48 and 0.43 / 0.72 for SMO. To further improve the classification performance, we reduce the feature space using two feature selection techniques, the x2 metric (Yang and Pedersen, 1997) and the Information Gain approach (Mitchell, 1997). For each label, we train separate classifiers using the top 100, 200 and 300 features obtained by each feature selection technique and choose the best performing set for our final classification pipeline. Indentation and temporal distance to the preceding turn proved to be the best ranked nonlexical features overall. Additionally, the turn position within the topic was a crucial feature for most labels in the criticism class and for PSR and TS labels. This is not surprising, because article criticism, suggestions and questions tend to occur in the beginning of a discussion. The two reference</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom Mitchell. 1997. Machine Learning. McGrawHill Education (ISE Editions), 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
</authors>
<title>Measuring Agreement on Set-valued Items (MASI) for Semantic and Pragmatic Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation,</booktitle>
<location>Genoa, IT.</location>
<contexts>
<context position="22098" citStr="Passonneau, 2006" startWordPosition="3591" endWordPosition="3593"> of turns that have been labeled with the given label by at least one annotator. PO denotes the observed agreement. which is defined as PO − PE npool = with �PO = L L PEl (2) 1 l=1 l=1 L POl , �PE = L 1 where L denotes the number of labels, PEl the expected agreement and POl the observed agreement of the lth label. npool is regarded to be more accurate than an averaged Kappa. For assessing the overall inter-rater reliability of the label set assignments per turn, we chose Krippendorff’s Alpha (Krippendorff, 1980) using MASI, a measure of agreement on setvalued items, as the distance function (Passonneau, 2006). MASI accounts for partial agreement if the label sets of both annotators overlap in at least one label. We achieved an Alpha score of α = .75. According to Krippendorff, datasets with this score are considered reliable and allow tentative conclusions to be drawn. The CO label showed the lowest agreement of only n = .18. The label was supposed to cover any criticism that is not covered by a dedicated label. However, the annotators reported that they chose this label when they were unsure whether a particular criticism label would fit a certain turn or not. Labels in the interpersonal category</context>
</contexts>
<marker>Passonneau, 2006</marker>
<rawString>Rebecca Passonneau. 2006. Measuring Agreement on Set-valued Items (MASI) for Semantic and Pragmatic Annotation. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, Genoa, IT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1998</date>
<booktitle>In Advances in Kernel Methods: Support Vector Learning,</booktitle>
<pages>185--208</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="27822" citStr="Platt, 1998" startWordPosition="4565" endWordPosition="4566">concerns of the Simple English Wikipedia community. 5 Automatic Dialog Act Classification For the automatic classification of dialog acts in Wikipedia Talk pages, we transform the multilabel classification problem into a binary classification task (Tsoumakas et al., 2010). We train a binary classifier for each label using the WEKA data-mining software (Hall et al., 2009). We use three learners for the classification task, a Naive Bayes classifier, J48, an implementation of the C4.5 decision tree algorithm (Quinlan, 1992) and SMO, an optimization algorithm for training support vector machines (Platt, 1998). Finally, we combine the best performing learners for each label in a UIMA-based classification pipeline (Ferrucci and Lally, 2004). Features for Dialog Act Classification As features, we use all uni-, bi- and trigrams that occurred in at least three different turns. Furthermore, we include the time distance to the previous and the next turn (in seconds), the length of the current, previous and next turn (in tokens), the 11A label transition A --+ B is recorded if two adjacent turns are labeled with A and B, respectively. 783 position of the turn within the discussion, the indentation level o</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>John C. Platt. 1998. Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods: Support Vector Learning, pages 185–208, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilona R Posner</author>
<author>Ronald M Baecker</author>
</authors>
<title>How People Write Together.</title>
<date>1992</date>
<booktitle>In Proceedings of the 25th Hawaii International Conference on System Sciences,</booktitle>
<pages>127--138</pages>
<location>Wailea, Maui, HI, USA.</location>
<contexts>
<context position="1443" citStr="Posner and Baecker, 1992" startWordPosition="202" endWordPosition="206">e F1-score of 0.82 with our classification pipeline. 1 Introduction Over the past decade, the paradigm of information sharing in the web has shifted towards participatory and collaborative content production. Texts are no longer exclusively prepared by individuals and then shared with the community. They are increasingly created collaboratively by multiple authors and iteratively revised by the community. When researchers first conducted surveys on professional writers in the 1980s, they found that the collaborative writing process differs considerably from the way individual writing is done (Posner and Baecker, 1992). In joint writing, the writers have to externalize processes that are otherwise not made explicit, like the planning and the organization of the text. The authors have to communicate how the text should be written and what exactly it should contain. Today, many tools are available that support collaborative writing. A tool that has particularly taken hold is the Wiki, a web-based, asyn1http://www.ukp.tu-darmstadt.de/data/ wikidiscourse chronous co-authoring tool. A unique characteristic of Wikis is the documentation of the edit history which keeps track of every change that is made to a Wiki </context>
</contexts>
<marker>Posner, Baecker, 1992</marker>
<rawString>Ilona R. Posner and Ronald M. Baecker. 1992. How People Write Together. In Proceedings of the 25th Hawaii International Conference on System Sciences, pages 127–138, Wailea, Maui, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1992</date>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="27736" citStr="Quinlan, 1992" startWordPosition="4552" endWordPosition="4553">native speakers of English. Therefore, the use of adequate language is one of the major concerns of the Simple English Wikipedia community. 5 Automatic Dialog Act Classification For the automatic classification of dialog acts in Wikipedia Talk pages, we transform the multilabel classification problem into a binary classification task (Tsoumakas et al., 2010). We train a binary classifier for each label using the WEKA data-mining software (Hall et al., 2009). We use three learners for the classification task, a Naive Bayes classifier, J48, an implementation of the C4.5 decision tree algorithm (Quinlan, 1992) and SMO, an optimization algorithm for training support vector machines (Platt, 1998). Finally, we combine the best performing learners for each label in a UIMA-based classification pipeline (Ferrucci and Lally, 2004). Features for Dialog Act Classification As features, we use all uni-, bi- and trigrams that occurred in at least three different turns. Furthermore, we include the time distance to the previous and the next turn (in seconds), the length of the current, previous and next turn (in tokens), the 11A label transition A --+ B is recorded if two adjacent turns are labeled with A and B,</context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>Ross Quinlan. 1992. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jodi Schneider</author>
<author>Alexandre Passant</author>
<author>John G Breslin</author>
</authors>
<title>Understanding and Improving Wikipedia Article Discussion Spaces.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th Symposium on Applied Computing,</booktitle>
<location>Taichung, TW.</location>
<contexts>
<context position="7017" citStr="Schneider et al. (2011)" startWordPosition="1094" endWordPosition="1097"> for special purpose annotation tasks. With the rise of the social web, the amount of research analyzing user generated discourse substantially increased. In addition to analyzing web forums (Kim et al., 2010a), chats (Carpenter and Fujioka, 2011) and emails (Cohen et al., 2004), Wikipedia Talk pages have recently moved into the center of attention of the research community. Vi´egas et al. (2007) manually annotate 25 Wikipedia article discussion pages with a set of 11 labels in order to analyze how Talk pages are used for planning the work on articles and resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions. Bender et al. (2011) describe a corpus of 47 Talk pages which have been annotated for authority claims and alignment moves. With this corpus, the authors analyze how the participants in Wikipedia discussions establish their credibility and how they express agreement and disagreement towards other participants or topics. From a different perspective, Stvilia et al. (2008) analyze 60 discussion pages in regard to</context>
</contexts>
<marker>Schneider, Passant, Breslin, 2011</marker>
<rawString>Jodi Schneider, Alexandre Passant, and John G. Breslin. 2011. Understanding and Improving Wikipedia Article Discussion Spaces. In Proceedings of the 26th Symposium on Applied Computing, Taichung, TW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Speech Acts.</title>
<date>1969</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="3329" citStr="Searle, 1969" startWordPosition="512" endWordPosition="513"> regulates and enforces standards of behavior and article quality. The user discussions on the article Talk pages might shed light on this issue and give an insight into the otherwise hidden processes of collaboration that, until now, could only be analyzed via interviews or group observations in experimental settings. The main goal of the present paper is to analyze the content of the discussion pages of the Simple English Wikipedia with respect to the dialog acts aimed at the coordination efforts for article improvement. Dialog acts, according to the classic speech act theory (Austin, 1962; Searle, 1969), represent the meaning of an utterance at the level of illocutionary force, i.e. a dialog act label concisely characterizes the intention and the role of a contribution in a dialog. We chose the Simple English Wikipedia for our initial analysis, because we are able to obtain more representative results 777 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 777–786, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics by covering almost 15% of all relevant Talk pages, as opposed to the much smaller</context>
<context position="5635" citStr="Searle (1969)" startWordPosition="880" endWordPosition="881">pages which we make freely available for download; and (3) a dialog act classification pipeline that incorporates several state of the art machine learning algorithms and feature selection techniques and achieves an average Fl-score of .82 on our corpus. 2 Related Work The analysis of speech and dialog acts has its roots in the linguistic field of pragmatics. In 1962, John Austin shifted the focus from the mere declarative use of language as a means for making factual statements towards its non-declarative use as a tool for performing actions. The speech act theory was further systematized by Searle (1969), whose classification of illocutionary acts (Searle, 1976) is still used as a starting point for creating dialog act classification schemata for natural language processing. A well known, domain- and task-independent annotation schema is DAMSL (Core and Allen, 1997). It was created as the standard annotation schema for dialog tagging on the utterance level by the Discourse Resource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The r</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>John R. Searle. 1969. Speech Acts. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>A classification of illocutionary acts.</title>
<date>1976</date>
<booktitle>Language in Society,</booktitle>
<pages>5--1</pages>
<contexts>
<context position="5694" citStr="Searle, 1976" startWordPosition="887" endWordPosition="888">a dialog act classification pipeline that incorporates several state of the art machine learning algorithms and feature selection techniques and achieves an average Fl-score of .82 on our corpus. 2 Related Work The analysis of speech and dialog acts has its roots in the linguistic field of pragmatics. In 1962, John Austin shifted the focus from the mere declarative use of language as a means for making factual statements towards its non-declarative use as a tool for performing actions. The speech act theory was further systematized by Searle (1969), whose classification of illocutionary acts (Searle, 1976) is still used as a starting point for creating dialog act classification schemata for natural language processing. A well known, domain- and task-independent annotation schema is DAMSL (Core and Allen, 1997). It was created as the standard annotation schema for dialog tagging on the utterance level by the Discourse Resource Initiative. It uses a four-dimensional tagset that allows arbitrary label combinations for each utterance. Jurafsky et al. (1997) augmented the DAMSL schema to fit the peculiarities of the Switchboard corpus. The resulting SWDB-DAMSL schema contained more than 220 distinct</context>
</contexts>
<marker>Searle, 1976</marker>
<rawString>John R. Searle. 1976. A classification of illocutionary acts. Language in Society, 5:1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Besiki Stvilia</author>
<author>Michael B Twidale</author>
<author>Linda C Smith</author>
<author>Les Gasser</author>
</authors>
<title>Information Quality Work Organization in Wikipedia.</title>
<date>2008</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>59--983</pages>
<contexts>
<context position="7576" citStr="Stvilia et al. (2008)" startWordPosition="1182" endWordPosition="1185"> resolving disputes among the editors. Schneider et al. (2011) extend this schema and manually annotate 100 Talk pages with 15 labels. They confirm the findings of Vi´egas et al. that coordination requests occur most frequently in the discussions. Bender et al. (2011) describe a corpus of 47 Talk pages which have been annotated for authority claims and alignment moves. With this corpus, the authors analyze how the participants in Wikipedia discussions establish their credibility and how they express agreement and disagreement towards other participants or topics. From a different perspective, Stvilia et al. (2008) analyze 60 discussion pages in regard to how information quality (IQ) in Wikipedia articles is assessed on the Talk pages and which types of IQ problems are identified by the community. They describe a Wikipedia IQ assessment model and map it to established frameworks. Furthermore, they provide a list of IQ problems along with related causal factors and necessary actions which has also inspired the design of our annotation schema. Finally, Laniado et al. (2011) examine Wikipedia discussion networks in order to capture structural patterns of interaction. They extract the thread structure from </context>
<context position="10936" citStr="Stvilia et al., 2008" startWordPosition="1714" endWordPosition="1717">thread structure designates the sequence of turns and their indentation levels on the Talk page. A structural overview of a Talk page and its constituents can be seen in Figure 1. We composed an annotation schema that reflects the coordination efforts for article improvement. Therefore, we manually analyzed a set of thirty Talk pages from the Simple English Wikipedia to identify the types of article deficiencies that are discussed and the way article improvement is coordinated. We furthermore incorporated the findings from an informationscientific analysis of information quality in Wikipedia (Stvilia et al., 2008), which identifies twelve types of quality problems, like e.g. Accuracy, Completeness or Relevance. Our resulting tagset consists of 17 labels (cf. Table 1) which can be subdivided into four higher level categories: Article Criticism Denote comments that identify deficiencies in the article. The criticism can refer to the article as a whole or to individual parts of the article. Explicit Performative Announce, report or suggest editing activities. Information Content Describe the direction of the communication. A contribution can be used to communicate new information to others (IP), to reques</context>
</contexts>
<marker>Stvilia, Twidale, Smith, Gasser, 2008</marker>
<rawString>Besiki Stvilia, Michael B. Twidale, Linda C. Smith, and Les Gasser. 2008. Information Quality Work Organization in Wikipedia. Journal of the American Society for Information Science, 59:983–1001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
<author>Ioannis Katakis</author>
<author>Ioannis P Vlahavas</author>
</authors>
<title>Mining multi-label data.</title>
<date>2010</date>
<booktitle>In Data Mining and Knowledge Discovery Handbook,</booktitle>
<pages>667--685</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="27482" citStr="Tsoumakas et al., 2010" startWordPosition="4510" endWordPosition="4513">ference to a particular deficiency of the article. The large number of CL labels compared to other labels from the same category is due to the fact that the Simple English Wikipedia requires authors to write articles in a way that they are understandable for non-native speakers of English. Therefore, the use of adequate language is one of the major concerns of the Simple English Wikipedia community. 5 Automatic Dialog Act Classification For the automatic classification of dialog acts in Wikipedia Talk pages, we transform the multilabel classification problem into a binary classification task (Tsoumakas et al., 2010). We train a binary classifier for each label using the WEKA data-mining software (Hall et al., 2009). We use three learners for the classification task, a Naive Bayes classifier, J48, an implementation of the C4.5 decision tree algorithm (Quinlan, 1992) and SMO, an optimization algorithm for training support vector machines (Platt, 1998). Finally, we combine the best performing learners for each label in a UIMA-based classification pipeline (Ferrucci and Lally, 2004). Features for Dialog Act Classification As features, we use all uni-, bi- and trigrams that occurred in at least three differen</context>
</contexts>
<marker>Tsoumakas, Katakis, Vlahavas, 2010</marker>
<rawString>Grigorios Tsoumakas, Ioannis Katakis, and Ioannis P. Vlahavas. 2010. Mining multi-label data. In Data Mining and Knowledge Discovery Handbook, pages 667–685. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernanda Vi´egas</author>
<author>Martin Wattenberg</author>
<author>Jesse Kriss</author>
<author>Frank Ham</author>
</authors>
<title>Talk Before You Type: Coordination in Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 40th Annual Hawaii International Conference on System Sciences,</booktitle>
<location>Waikoloa, Big Island, HI, USA.</location>
<marker>Vi´egas, Wattenberg, Kriss, Ham, 2007</marker>
<rawString>Fernanda Vi´egas, Martin Wattenberg, Jesse Kriss, and Frank Ham. 2007. Talk Before You Type: Coordination in Wikipedia. In Proceedings of the 40th Annual Hawaii International Conference on System Sciences, Waikoloa, Big Island, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eti Yaari</author>
<author>Shifra Baruchson-Arbib</author>
<author>Judit Bar-Ilan</author>
</authors>
<title>Information quality assessment of community generated content: A user study of Wikipedia.</title>
<date>2011</date>
<journal>Journal of Information Science,</journal>
<pages>37--487</pages>
<contexts>
<context position="4367" citStr="Yaari et al., 2011" startWordPosition="677" endWordPosition="680">ages 777–786, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics by covering almost 15% of all relevant Talk pages, as opposed to the much smaller fraction we could achieve for the English Wikipedia. The long-term goal of this work is to identify relations between contributions on the Talk pages and particular article edits. We plan to analyze the relation between article discussions and article content and identify the edits in the article revision history that react to the problems discussed on the Talk page. In combination with article quality assessment (Yaari et al., 2011), this opens up the possibility to identify successful patterns of collaboration which increase the article quality. Furthermore, our work will enable practical applications. By augmenting Wikipedia articles with the information derived from automatically labeled discussions, article readers can be made aware of particular problems that are being discussed on the Talk page “behind the article”. Our primary contributions in this paper are: (1) an annotation schema for dialog acts reflecting the efforts for coordinating the article improvement; (2) the Simple English Wikipedia Discussion (SEWD) </context>
</contexts>
<marker>Yaari, Baruchson-Arbib, Bar-Ilan, 2011</marker>
<rawString>Eti Yaari, Shifra Baruchson-Arbib, and Judit Bar-Ilan. 2011. Information quality assessment of community generated content: A user study of Wikipedia. Journal of Information Science, 37:487–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A Comparative Study on Feature Selection in Text Categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning,</booktitle>
<pages>412--420</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="29436" citStr="Yang and Pedersen, 1997" startWordPosition="4837" endWordPosition="4841">tances, we create a balanced dataset which contains an equal amount of positive and negative instances. Therefore, we randomly select the appropriate number of negative instances and discard the rest. This improves the classification performance on every label for all three learners. Feature Selection Using the full set of features, we achieve the following macro/micro averaged F1-scores: 0.29 / 0.57 for Naive Bayes, 0.42 / 0.66 for J48 and 0.43 / 0.72 for SMO. To further improve the classification performance, we reduce the feature space using two feature selection techniques, the x2 metric (Yang and Pedersen, 1997) and the Information Gain approach (Mitchell, 1997). For each label, we train separate classifiers using the top 100, 200 and 300 features obtained by each feature selection technique and choose the best performing set for our final classification pipeline. Indentation and temporal distance to the preceding turn proved to be the best ranked nonlexical features overall. Additionally, the turn position within the topic was a crucial feature for most labels in the criticism class and for PSR and TS labels. This is not surprising, because article criticism, suggestions and questions tend to occur </context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A Comparative Study on Feature Selection in Text Categorization. In Proceedings of the Fourteenth International Conference on Machine Learning, pages 412–420, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<location>Marrakech, MA.</location>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Marrakech, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>