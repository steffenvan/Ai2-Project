<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000076">
<note confidence="0.7773548">
Classifying the Hungarian Web
Andras Kornai
Metacarta Inc.
875 Massachusetts Ave.
Cambridge, MA 02139
</note>
<email confidence="0.733986">
andras@kornai.com
</email>
<author confidence="0.974977">
David Twomey
</author>
<affiliation confidence="0.94428">
CEHQ, Inc.
</affiliation>
<address confidence="0.9058065">
145 Rosemary Street Ste H
Needham, MA 02494
</address>
<email confidence="0.998509">
dtwomey@theworld.com
</email>
<note confidence="0.79079025">
Marc Krellenstein
Reed-Elsevier Inc.
200 Wheeler Rd.
Burlington, MA 01803
</note>
<email confidence="0.707965">
m.krellenstein@elsevier.com
</email>
<address confidence="0.69298525">
FruasirgiVeress
TeragraraCorp.
236 Huntington Ave.
Boston, MA 02115
</address>
<email confidence="0.999229">
veress@cs.bu.edu
</email>
<note confidence="0.69772575">
Michael Mulligan
divine Inc.
1 Wayside Road
Burlington, MA 01803
</note>
<email confidence="0.94922">
mulligan@alum.mit.edu
</email>
<author confidence="0.972679">
Alec Wysoker
</author>
<affiliation confidence="0.7771605">
deNovis Inc.
One Cranberry Hill, Suite 203
</affiliation>
<address confidence="0.830073">
Lexington, MA 02421
</address>
<email confidence="0.99783">
alecw@pobox.com
</email>
<sectionHeader confidence="0.996672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993494333333333">
In this paper we present some lessons
learned from building viz s la, the
keyword search and topic classification
system used on the largest Hungarian
portal, [ origo .hu]. Based on a sim-
ple statistical language model, and the
large-scale supporting evidence from
vizsla, we argue that in topic classi-
fication only positive evidence matters.
</bodyText>
<sectionHeader confidence="0.990982" genericHeader="keywords">
0 Introduction
</sectionHeader>
<bodyText confidence="0.99996105">
Novices are often attracted to menu-based por-
tals because these are easy to navigate. As they
get more familiar with the web, users soon re-
alize that their portal covers only a tiny fraction
of the web, and move to keyword search engines.
But as their information needs and sophistication
grow, so does their frustration with simple key-
word search. As a result seemingly obscure fea-
tures, such as boolean searches, wildcards, and
topic classification become increasingly relevant
to them. To most users, the ideal system would be
one that combines the ease of navigation provided
e.g. by Yahoo with the near-exhaustive coverage
provided e.g. by Google. But topic classification
the Yahoo way, by professional editors, is expen-
sive, and the results of using amateur editors, as in
dmoz, are often highly questionable.
One way to address the problem of low edito-
rial bandwidth is to automate the topic classifi-
cation process. Section 1 of this paper describes
[origo.hu], a Hungarian portal that uses both man-
ual and automatic topic classification, and gives
a brief overview of the keyword search and au-
toclassification technology developed by Northern
Light Technology (NLT, now part of divine Inc)
that is deployed on the Hungarian web, which cur-
rently has about 20 million unique pages. As we
shall see, this is a very successful system, both
in terms of standard performance measures and in
terms of end-user satisfaction.
In Section 2 we turn to the main question of the
paper: why is this algorithm, which is in many
ways closer to classic TF-IDF than modern TREC-
style topic detection systems, performing so well?
We present a formal analysis of what we take to be
the essential part of the topic classification prob-
lem, and argue that the characteristics revealed by
this analysis justify the use of methods that are
simpler than generally thought acceptable. We of-
fer our conclusions in Section 3.
</bodyText>
<equation confidence="0.63895">
1 [origo.hu]
</equation>
<bodyText confidence="0.999751166666667">
[origo .hu] (the square brackets are part of the
branding) is owned and operated by Axelero Inc,
the largest Hungarian ISP. It is by far the most
popular web portal in Hungary: according to the
visitor number statistics published by Median Inc.
(see www . webaudit .hu for current numbers),
it enjoys the same kind of superiority, being big-
ger than the next two competitors put together,
that the British Navy had when Britannia ruled
the waves. The verb vizslazni (originally from the
noun vizs/a &apos;retriever dog&apos;, the trademark of the
Axelero search engine) entered the Hungarian lan-
</bodyText>
<page confidence="0.998114">
203
</page>
<bodyText confidence="0.999012225806452">
guage in the same sense as the verb to google is
now used in English.
An important measure of user satisfaction, the
number of pages downloaded in a single session,
is also considerably better for 1origo.hul than its
competitors. The independent auditor, Median
Inc., defines a single session as no more than
30 minutes inactivity between page downloads:
[origo.hu] users need to look at 6.9 pages until
they are satisfied, while on the two largest com-
petitors they have to download 7.9 and 8.1 pages
respectively. There is currently no obvious way to
quantify exactly how much of this effect can be at-
tributed to better search capabilities and relevance
ranking, but the conclusion that these play a sig-
nificant role seems inescapable.
The vi zsla search bar is placed promi-
nently at the center of the h.ttp://or i go .hu
start page. Upon entering a keyword such as
cement &apos;id&apos;, a results page containing three
major results areas is displayed. At the top,
we find results from the katalogus &apos;catalog&apos;, a
Yahoo-style manually filled hierarchical com-
pendium of web pages, in this case showing a
search path agriculture and industry
building and construction
construction materials
adhesives and mortars cement.
Upon clicking this last entry, the user gets 10
very high-quality pages, beginning with one
discussing the situation of the cement industry
in light of the upcoming EU ascension. Below
this, we find the URLs and abstracts for the 10
most highly ranked of the 16,684 pages that have
the keyword cement. Finally, to the left we find
a ranked list of NLT-style custom search fold-
ers, beginning with cement, elections,
and concrete) If our query is vIzzciro ce-
ment &apos;water resistant cement&apos; the katalogus is
not displayed, the number of pages found is
only 303, and the top custom search folders
are now waterproofing, drainage,
adhesives-mortars, concrete,
surface preparation, bridge con-
&apos;To understand how the elections enter the picture one
needs to know that allegations of botched and corrupt pri-
vatization of the cement industry were a prominent campaign
theme.
struction, building maintenance,
painting and stuccoing, cement,
paint industry, and waste manage-
ment in this order.
The main features of the NLT keyword search
engine that distinguish it from competitors, full
support of Boolean queries (including full support
of negation), phrase search, trailing wildcards, and
proximity search, are well known. The page rank-
ing algorithm, which uses links as one of many
factors, has been discussed elsewhere (Krellen-
stein, 2002). Here we concentrate on the topic
classification engine, which differs from its TREC
counterparts in several relevant respects. First, the
number of topics considered is very large (22,000
for the English hierarchy developed at NLT), as
opposed to the few dozen to a few hundred top-
ics considered e.g. in the Reuters work. Second,
the assumption is that the typical document has
only one dominant topic (or none, as we will dis-
cuss later). Two-topic documents are rare, three or
more topics for a single document occur seldom
enough to be negligible in the sense that we see
no practical need for returning more than two top-
ics per document (though the engine of course has
the facilities for doing so, should the need arise in
some non-web application). Finally, we assume
that training data is available only in very small
quantities, only a handful of documents per cate-
gory, as opposed to the hundreds of training docu-
ments per category used in TREC.
Axelero&apos;s katalogus system is a mature,
highly coherent work of knowledge engineering,2
with a keyword-spotting hook into the search
query system. As such, it provided an excel-
lent basis for the NLT autoclassification system,
which was trained on the basis of the high qual-
ity exemplary documents already manually classi-
fied to it. Translating the large NLT topic hierar-
chy from English to Hungarian was not feasible in
the deployment timeframe, but even if it were, we
would have been faced with the formidable chal-
lenge of finding Hungarian exemplaries for many
thousands of highly detailed NLT topics. Using
the katalogus also made sense because it was cul-
</bodyText>
<footnote confidence="0.931248666666667">
2The internal coherence of the system no doubt owes a
great deal to the fact that originally it was developed by one
person, Rudolf Ungvary, Hungarian National Library.
</footnote>
<page confidence="0.997706">
204
</page>
<bodyText confidence="0.9986696">
turally more appropriate (e.g. in the selection of
sports it has a section for table tennis but not for
American football) so the chances of finding more
Hungarian webpages on the topic are higher. Be-
sides using a native Hungarian topic hierarchy,
the system also relies on a morphological analysis
(stemming) component developed specifically for
Hungarian by Gabor Proszeky and his associates
at Morphologic Inc. We keep both the original
(inflected) and the stemmed version available for
keyword match and topic classification, since this
produces superior results to using either of them
alone.
Other than these two instances of necessary lo-
calization, there is nothing in our system that is
specifically geared toward Hungarian, and there-
fore we believe that the conclusions we draw about
this particular algorithm apply to all topic classi-
fication systems with the same broad characteris-
tics:
</bodyText>
<listItem confidence="0.9991065">
1. monolingual input
2. small amount of training data available
3. large number of topic categories
4. few documents with multiple topics
</listItem>
<bodyText confidence="0.993034471428572">
In what follows we illustrate some of our points
on a version of the old Reuters corpus, keeping
the standard (Lewis) test/train split, but removing
all articles that have more than one topic, and all
topics that have less than three training examples.
Needless to say, removal of the multitopic docu-
ments and the topics with extremely limited train-
ing makes the task easier: Bow TF-IDF (McCal-
lum, 1996) obtains 92.51% correct classification
on this set with the default settings. But our in-
tention is not to &amp;quot;report results&amp;quot; on a corpus with
21578 (or, after removal, 8998) documents: our
results are on the Hungarian web, a corpus over
three orders of magnitude larger, and displaying
all the difficulties of real language data, such as
lack of consistent style, large numbers of typos,
search engine spamming, etc. that are largely ab-
sent from Reuters.
2 The bag of words model
We assume a collection of documents D and a
system of topics T such that T partitions D into
largely disjoint subsets DI c D(t E T). We will
use a finite set of words wi , w2. , wN arranged
on order of decreasing frequency. N is generally
in the range 105 — 106 — for words not in this set
we introduce a catchall unknown word wo. By
general language we mean a probability distribu-
tion GL that assigns the appropriate frequencies
to the w, either in some large collection of topic-
less texts, or in a corpus that is appropriately rep-
resentative of all topics. By the (word unigram)
probability model of a topic t we mean a probabil-
ity distribution Gt that assigns the appropriate fre-
quencies gt(wz) to the wi in a large collection of
documents about t. Given a collection C we call
the number of documents that contain w the doc-
ument frequency of the word, denoted DF(w,C),
and we call the total number of w tokens its term
frequency in C, denoted TF(w,C).
Assume that the set of topics T =
{ti,tk,... ,tk} is arranged in order of de-
creasing probability Q(T) = ql.q2, qk. Let
qz = T &lt; 1, so that a document is topicless
with probability qo = 1 —T. The general language
probability of a word w can therefore be computed
on topicless documents to be pip = GL (w) or as
g,g,(w). In practice, it is next to impossible
to collect a large set of truly topicless documents,
so we estimate pw based on a collection D that we
assume to be representative of the distribution Q
of topics. It should be noted that this procedure,
while workable, is fraught with difficulties, since
in general the q3 are not known, and even for very
large collections it can&apos;t always be assumed that
the proportion of documents falling in topic j
estimates q3 well.
As we shall see shortly, within a given topic
t only a few dozen, or perhaps a few hundred,
words are truly characteristic (have gt(w) sig-
nificantly higher than the background probability
gaw)) and our goal will be to find these. To
this end, we need to first estimate GL: the triv-
ial method is to use the uncorrected observed fre-
quency gL(w) = TF(w,C)1L(C) where L(C)
is the length of the corpus C (total number of
word tokens in it). While this is obviously very at-
tractive, the numerical values so obtained tend to
be highly unstable. For example, the word with
makes up about 4.44% of a 55m word sample
of the Wall Street Journal (WSJ) but 5.00% of a
</bodyText>
<page confidence="0.993883">
205
</page>
<bodyText confidence="0.997842105263158">
46m word sample of the San Jose Mercury News
(Mere). For medium frequency words, the effect
is even more marked: for example uniform ap-
pears 7.65 times per million word in the WSJ and
18.7 times per million in the Merc sample. And
for low frequency words, the straightforward es-
timate very often comes out as 0, which tends to
introduce singularities in models based on the es-
timates.
The same uncorrected estimate, gt(w) =
T F (w, Dt) I L(Dt) is of course available for Gt,
but the problems discussed above are made worse
by the fact that any topic-specific collection of
documents is likely to be orders of magnitude
smaller than our overall corpus. Further, if Gt is
a Bernoulli source, the probability P(d1 t) that a
document d containing l instances of wi, /2 in-
stances of w2, etc. is produced by the source for
topic t will be given by the multinomial formula
</bodyText>
<equation confidence="0.9973005">
(io + /1 + • • • + /N)
10, 11, . • • , Tn gt(wi)li
</equation>
<bodyText confidence="0.999784666666667">
which will be zero as long as any of the gt(wz)
are zero. Therefore, we will smooth the probabil-
ities in the topic model by the (uncorrected) prob-
abilities that we obtained for general language,
since the latter are of necessity positive. Instead
of gt(w) we will therefore use
</bodyText>
<equation confidence="0.99757">
agaw) + (1 — a)gt(w) (2)
</equation>
<bodyText confidence="0.999905866666667">
where is a small but non-negligible constant,
usually between .1 and .3. In the recent litera-
ture, e.g. (Zhai and Lafferty, 2001), this is gener-
ally called Jelinek-Mercer smoothing.3 There are
two ways to justify this method: the trivial one
is to say that documents are not fully topical, but
can be expected to contain a small a portion of
general language. A more interesting justification
is to treat the general language probability as a
Bayesian prior, the topic-specific frequency as the
maximum likelihood estimate based on the obser-
vations, so that (2) will be the posterior mean of
the unknown probability. For the Reuters exper-
iment, we used the 46m Merc wordcount as our
general (background) language model.
</bodyText>
<footnote confidence="0.772765333333333">
3Actually the first to apply this technique to topic detec-
tion was Gish (1993-1994 Switchboard tasks, see (Colbath,
1998)).
</footnote>
<bodyText confidence="0.983137">
What words, if any, are specific to a few top-
ics in the sense that P(d E Dtlw E d) &gt;&gt;
P(d e Di)? This is well measured by the num-
ber of documents containing the word: for exam-
ple Fourier appears in only about 200k documents
in a large collection containing over 200m English
documents (see www. northernlight . corn),
while see occurs in 42m and book in 29m. How-
ever in a collection of 13k documents about digi-
tal signal processing Fourier appears 1100 times,
so P(d E Dt) is about 6.5 10-5 while P(d E
tr) is about 5.5 • 10-3, two orders of magni-
tude better. In general, words with low DF val-
ues, or what is the same, high IDF (inverse doc-
ument frequency) values are good candidates for
being topic-specific, though this criterion has to
be used with care: it is quite possible that a word
has high IDF because of deficiencies in the corpus,
not because it is inherently very specific. For ex-
ample, the word alternately has even higher IDF
than Fourier, yet it is hard to imagine any topic
that would call for its use more often than others.
Recall that topics are modeled by Bernoulli
(word unigram) sources: given a document with
word counts 1, and total length 71, if we make the
naive Bayesian assumption that the l are indepen-
dent, the probability that topic t emitted this doc-
ument will be obtained by substituting (2) in (1):
</bodyText>
<equation confidence="0.9966418">
(/0 /N)
ri•
(cygL(wi) + (1— ci)gt(wi))1&apos;
10, • • • ,IN i=0
(3)
</equation>
<bodyText confidence="0.995075">
For the 0th topic, general language, (1) and
(3) are the same. The log probability quotient
log P(dlt)I P(d L) of the document being emitted
by topic t vs the general language is given by
</bodyText>
<equation confidence="0.997171666666667">
i=0
E log agL(wi)± (1 — (I)gt(wi) (4)
gL (wi )
</equation>
<bodyText confidence="0.999161">
We rearrange this sum in three parts: where
gL(w,) is significantly larger than gt(w,), when
it is about the same, and when it is significantly
smaller. In the first part, the numerator is domi-
nated by agL(w,„), so we have
</bodyText>
<equation confidence="0.99889125">
log(a) Eii (5)
OL(11)1)&gt;&gt;gt(wi)
(1)
i=0
</equation>
<page confidence="0.995767">
206
</page>
<bodyText confidence="0.999933222222222">
which we can think of as the contribution of &amp;quot;nega-
tive evidence&amp;quot;, words that are significantly sparser
for this topic than for general language. In the
second part, the quotient is about 1, therefore the
logs are about 0, so this whole part can be ne-
glected — words that have about the same fre-
quency in the topic as in general language can&apos;t
help us distinguish whether the document came
from the Bernoulli source associated with the topic
t or from the one associated with general lan-
guage. Note that the summands change sign here
in the second part, and as long as the progression
of terms is roughly linear, we can extend the lim-
its in both directions without changing the overall
zero value.
Finally, the part where the probability of the
words is significantly higher than the background
probability will contribute the &amp;quot;positive evidence&amp;quot;
</bodyText>
<equation confidence="0.99808275">
tlog ( (1 — cx)gl(wz)
E , a
(wt)
9L(wi)&lt;&lt;qt(w0
</equation>
<bodyText confidence="0.995661">
Since a is a small constant, on the order of .2,
while in the interesting cases (such as Fourier in
DSP vs. in general language) gt is orders of mag-
nitude larger than gL, the first term can be ne-
glected and we have, for the positive evidence,
</bodyText>
<equation confidence="0.9923005">
E /,(100-a)+10g(gt(wo)-logoL(wo)
g (WO &lt;&lt;gt(w,)
</equation>
<bodyText confidence="0.98288125">
In every term the first summand log(1 — a) is
about —a. The other two terms log(gt (w,)) —
log(gL (w, ) measure the (base e) orders of magni-
tude in frequency over general language: we will
call this the relevance of word w to topic t and
denote it by r(w,t). Some examples of the high-
est (positive), near-zero, and the lowest (negative)
relevances follow:
</bodyText>
<table confidence="0.9990512">
rank word r(w,alum)
1 aluminium 13.4176
2 tonnes 12.9357
3 lme 12.0313
4 alumina 11.9061
1185 though 0.0079206
1186 30 0.00377953
1187 under 0.00100579
1188 second -0.0146792
1189 7 -0.0207462
1190 with -0.022297
1316 you -2.20392
1317 name -2.96474
1318 country -2.97375
1319 day -3.03341
</table>
<tableCaption confidence="0.998724">
Table 1 Samples of r for the alum topic
</tableCaption>
<bodyText confidence="0.999547333333333">
Since for the positive evidence —a is quite negli-
gible compared to the relevance, positive evidence
can be approximated by the more manageable
</bodyText>
<equation confidence="0.9983585">
E lir(w,t) (6)
gL (WO &lt;&lt;gt(Wi)
</equation>
<bodyText confidence="0.998099428571429">
Needless to say, the real interest is not in de-
termining whether a document belongs to a par-
ticular topic s as opposed to general language,
but rather in whether it belongs in topic t or
topic s. We can compute log(P(d t)/P(d1s))
as 1og((P(clIt)1P(clIE))1(P(d s)IP(d E))), and
the importance of this step is that we see that the
&amp;quot;negative evidence&amp;quot; given by (5) also disappears.
There are two reasons for this. First, the abso-
lute value of the negative evidence is small: on the
average Reuters topic, the sum of the negative rel-
evances is less than 5% of the sum of positive rele-
vances. Second, words that are below background
probability for topic t will in general be also be-
low background probability for topic s, since their
instances are concentrated in some other topic u of
which they are truly characteristic. The key contri-
bution in distinguishing topics s and t by comput-
ing log (P(t1d)1P(sId)) will therefore come from
those few words that have significantly higher than
background probabilities in at least one of these:
</bodyText>
<equation confidence="0.9979995">
E lir(w,t) — E lir(w,$) (7)
gL(wi)&lt;&lt;gt(wi) gL(wi)&lt;&lt;gs(ivi)
</equation>
<bodyText confidence="0.999108857142857">
For words w, that are significant for both top-
ics (such as Fourier would be for DSP and for
Harmonic Analysis), the contribution of gen-
eral language cancels out, and we are left with
Ei log(g1(w2)Ig8(w,)). But such words are rare
even for closely related topics, so the two sums in
(7) are largely disjoint.
</bodyText>
<page confidence="0.993959">
207
</page>
<bodyText confidence="0.999699851851852">
What (7) defines is the simplest, historically
oldest, and best understood pattern classifier, a lin-
ear machine where the decision boundaries are
simply hyperplanes (Highleynian, 1962; Duda et
al., 2001). As the above reasoning makes clear,
linearity is to some extent a matter of choice: cer-
tainly the underlying bag of words assumption,
that the words are chosen independent of one an-
other, is quite dubious. However, it is a good
first approximation, and one can extend it from
Bernoulli (0 order Markov) to first, second, third
order, etc. Once the probabilities of word pairs,
word triples, etc are explicitly modeled, much of
the criticism directed at the bag of words approach
loses its grip.4
A relevance-based linear classifier containing
for all topics all the words that appeared in its
training set gives 91.13% correct classification:
this has 2154 words in the average topic model.
If the least relevant 40% of the words is ex-
cluded from the models, average model size de-
creases to 1454 words, but accuracy actually im-
proves to 92.83% (recall that the Bow baseline was
92.51% on this set), demonstrating rather clearly
the main thesis that we derived via estimation
above, namely that negative and zero evidence is
simply noise that we can safely ignore.
</bodyText>
<figure confidence="0.638704">
Reuters 215878 (3+ train, single topic)
Accuracy by average model size
average model size (Avrords - log scale)
</figure>
<figureCaption confidence="0.9972275">
Figure 1. Models with equal number of words vs
equal cumulative TF
</figureCaption>
<bodyText confidence="0.504133">
Figure 1 shows the model-size accuracy tradeoff,
with model size plotted on the x axis on a log
scale. Note that if we keep only the top 15% of
the words (average model size 333), we lose only
</bodyText>
<footnote confidence="0.8333455">
4The NLT system directly indexes word pairs and can
match strings of arbitrary length for topic classification.
</footnote>
<bodyText confidence="0.9998845">
6.4% of our peak classification performance, since
the models still classify 87% correct. If we are pre-
pared to sacrifice another 6% in performance, av-
erage model sin can be reduced to 236, with clas-
sification accuracy still at a very acceptable 80.7%
level.
The algorithm used to obtain these numbers
simply ranks the words within each model by rel-
evance, and keeps the models balanced by cumu-
lative TF. NLT&apos;s proprietary word selection algo-
rithm gets to the 80% level with 30 words per
model. Reducing the model size even more drasti-
cally would take us out of the realm of practically
acceptable classifiers, but as an illustration of our
main point it should be noted that keeping the 5
best words in each model would give 46.8% cor-
rect classification, and keeping just one word, the
one with the greatest relevance for each topic, al-
ready gives 28.5% correct classification (on this
set, random choice would give less than 3%).
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="conclusions">
3 Conclusions
</sectionHeader>
<bodyText confidence="0.999960076923077">
In Section 2 we argued that for topic classifica-
tion only positive evidence, i.e. words with sig-
nificantly higher than background probability, will
ever matter. Though we illustrated this point on
a standard corpus, we wish to emphasize that it
is not this toy example, but rather the objectively
measurable user satisfaction with the large-scale
system described in Section 1, that provides the
empirical underpinnings of our theoretical argu-
ment.
If only the best (positive) evidence is used,
the models can be sparse, in the sense of having
nonzero coefficients r(u,, t) only for a few dozen,
or perhaps a few hundred words u) for a given
topic t, even though the number of words con-
sidered, N, is typically in the hundred thousands
to millions (Kornai and Richards, 2002). An im-
portant side effect of this approach is that many
documents, not containing a sufficient number of
keywords for any topic, will be treated as topicless
(part of the general language) i.e. they are rejected
from classification. Given the nature and quality
of many web documents, this is a desirable out-
come.
Not knowing that the parameter space is sparse,
for k = 104 topics and N = 106 words we
</bodyText>
<page confidence="0.99402">
208
</page>
<bodyText confidence="0.999910803571429">
would need to estimate kN = 1010 parameters
even for the simplest (unigram) model. This may
be (barely) within the limits of our supercomput-
ing ability, but it is definitely beyond the reliabil-
ity and representativeness of our data. Over the
years, this led to a considerable body of research
on feature selection, which tries to address the is-
sue by reducing N, and on hierarchical classifica-
tion, which addresses it by reducing k.
We can&apos;t discuss here in detail the problems in-
herent in hierarchical classification, but we note
that for a practical topic detection system higher
nodes e.g. film director are often next to
impossible to train, even though lower nodes e.g.
Spielberg, Fellini, ... will perform
well. As for feature selection, we find that much
of the literature suffers from what we will call the
once a feature, always a feature (OAFAAF) fal-
lacy: if a word w is found distinctive for topic t,
an attempt is made to estimate g8 (w) for the whole
range of s, rather than the one value gt(w) that we
really care about.
The fact that high quality working classifiers
such as vi z sla can be built using only sparse
subsets of the whole potential feature set reflects
a deep, structural property of the data: at least for
the purpose of comparing log emission probabil-
ities across topic models, the GI can be approx-
imated by sparse distributions S. In fact, this
structural property is so strong that it is possible
to build classifiers that ignore the differences be-
tween the numerical values of g8 (w) and gt (w) en-
tirely, replacing both by a uniform estimate g(w)
based on the IDF of w. Traditionally, the it multi-
pliers in (7) are known as the term frequency (TF)
factor. Such systems, where the classification load
is carried entirely by the zero-one decision of us-
ing a particular word as a keyword for a topic, are
the simplest TF-IDF classifiers, and the estimation
method used in Section 2 fits in the broad tradi-
tion of deriving IDF-like weights (Robertson and
Walker, 1997) from language modeling consider-
ations (?; Hiemstra and Kraaij, 2002; Miller et al.,
1999).
What he have done in the body of the paper was
to create a new rationale for a classical TF-IDF
system, not just for vi z s la but for any system
along the same lines. The notion of good keywords
is often used, though not always defined, in infor-
mation retrieval. We believe that this is an entirely
valid notion, and offered a simple operational def-
inition, has significantly higher than background
probability, to capture it. Our basic claim was that
only the good keywords (positive evidence) mat-
ter, and the overall performance of our classifica-
tion system largely supports this assertion.
</bodyText>
<sectionHeader confidence="0.984606" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999979684210527">
A large system such as vizsla is always the
work of many people. We would like to sin-
gle out Gabi Steinberg (divine Inc.), whose con-
tributions to the original NLT search and classi-
fication architecture are so fundamental that he
should have been a coauthor, were it not for his
insistence on staying in the background. Special
thanks to Rudolf Ungvary (National Szechenyi
Library), who created the original katalogns,
Gabor Proszeky (Morphologic), who contributed
the stemming, Andras Karpati (Axelero) and Peter
Halacsy (Axelero) for creating and managing the
training data and the Hungarian front end. Special
thanks to Herb Gish and Richard Schwartz (BBN)
for clarifying the early history of Bayesian lan-
guage modeling techniques in topic detection. The
system described here was implemented while all
authors were working at Northern Light Technol-
ogy, now divine Inc.
</bodyText>
<sectionHeader confidence="0.997178" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999112466666667">
S. Colbath Rough&apos;n&apos;Ready: A meeting recorder and
browser Perceptual User Interfaces Conference San
Francisco, CA, November 1998 220
R.O. Duda, P.E. Hart, and D.G. Stork 2001 Pattern
Classification John Wiley and Sons
D. Hiemstra and W. Kraaij 1998 TwentyOne at TREC-
7: ad-hoc and cross language track Proceedings of
TREC-7 174-185
W.H. Highleyman 1962 Linear decision functions
with application to pattern recognition. Proceedings
of the IRE, 50:1501-1514.
A. Kornai and J.M. Richards 2002 Linear discrim-
inant text classification in high dimension In: A.
Abraham and M. Koeppen (eds): Hybrid Informa-
tion Systems Physica Verlag, Heidelberg 527-538
</reference>
<page confidence="0.982553">
209
</page>
<reference confidence="0.995867842105263">
M. Krellenstein 2002 Operational aspects of the NLT
search engine Proceedings of SIGIR-02
A.K. McCallum 1996 Bow: A toolkit
for statistical language modeling, text
retrieval, classification and clustering
http: //www. cs . cmu.edu/-mccallum/bow
D.R. Miller, T. Leek, and R.M. Schwartz 1999 A
hidden Markov model information retrieval system
Proceedings of SIGIR-99 214-221
J.M. Ponte and W.B. Croft 1998 A language mod-
elling approach to information retrieval Proceedings
of SIGIR-98 275-281
S.E. Robertson and S. Walker 1997 On relevance
weights with little relevance information Proceed-
ings of SIGIR-97 16-24
Chengxiang Zhai and John Lafferty 2001 A Study of
Smoothing Methods for Language Models Applied
to Ad Hoc Information Retrieval Research and De-
velopment in Information Retrieval 334-342
</reference>
<page confidence="0.998937">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000232">
<title confidence="0.999695">Classifying the Hungarian Web</title>
<author confidence="0.999952">Andras Kornai</author>
<affiliation confidence="0.999956">Metacarta Inc.</affiliation>
<address confidence="0.999882">875 Massachusetts Ave. Cambridge, MA 02139</address>
<email confidence="0.99877">andras@kornai.com</email>
<author confidence="0.999868">David Twomey</author>
<affiliation confidence="0.999798">CEHQ, Inc.</affiliation>
<address confidence="0.9992485">145 Rosemary Street Ste H Needham, MA 02494</address>
<email confidence="0.999792">dtwomey@theworld.com</email>
<author confidence="0.999925">Marc Krellenstein</author>
<affiliation confidence="0.999759">Reed-Elsevier Inc.</affiliation>
<address confidence="0.999903">200 Wheeler Rd. Burlington, MA 01803</address>
<email confidence="0.998992">m.krellenstein@elsevier.com</email>
<note confidence="0.495591">FruasirgiVeress TeragraraCorp.</note>
<address confidence="0.9770215">236 Huntington Ave. Boston, MA 02115</address>
<email confidence="0.999593">veress@cs.bu.edu</email>
<author confidence="0.999991">Michael Mulligan</author>
<affiliation confidence="0.999478">divine Inc.</affiliation>
<address confidence="0.973302">Road Burlington, MA 01803</address>
<email confidence="0.999788">mulligan@alum.mit.edu</email>
<author confidence="0.998125">Alec Wysoker</author>
<affiliation confidence="0.999717">deNovis Inc.</affiliation>
<address confidence="0.9990705">One Cranberry Hill, Suite 203 Lexington, MA 02421</address>
<email confidence="0.99975">alecw@pobox.com</email>
<abstract confidence="0.995992028497408">In this paper we present some lessons from building s keyword search and topic classification system used on the largest Hungarian [ .hu]. on a simple statistical language model, and the large-scale supporting evidence from argue that in topic classification only positive evidence matters. Novices are often attracted to menu-based portals because these are easy to navigate. As they get more familiar with the web, users soon realize that their portal covers only a tiny fraction of the web, and move to keyword search engines. But as their information needs and sophistication grow, so does their frustration with simple keyword search. As a result seemingly obscure features, such as boolean searches, wildcards, and topic classification become increasingly relevant to them. To most users, the ideal system would be one that combines the ease of navigation provided e.g. by Yahoo with the near-exhaustive coverage provided e.g. by Google. But topic classification the Yahoo way, by professional editors, is expensive, and the results of using amateur editors, as in often highly questionable. One way to address the problem of low editorial bandwidth is to automate the topic classification process. Section 1 of this paper describes [origo.hu], a Hungarian portal that uses both manual and automatic topic classification, and gives a brief overview of the keyword search and autoclassification technology developed by Northern Light Technology (NLT, now part of divine Inc) that is deployed on the Hungarian web, which currently has about 20 million unique pages. As we shall see, this is a very successful system, both in terms of standard performance measures and in terms of end-user satisfaction. In Section 2 we turn to the main question of the paper: why is this algorithm, which is in many ways closer to classic TF-IDF than modern TRECstyle topic detection systems, performing so well? We present a formal analysis of what we take to be the essential part of the topic classification problem, and argue that the characteristics revealed by this analysis justify the use of methods that are simpler than generally thought acceptable. We offer our conclusions in Section 3. 1 [origo.hu] .hu] square brackets are part of the branding) is owned and operated by Axelero Inc, the largest Hungarian ISP. It is by far the most popular web portal in Hungary: according to the visitor number statistics published by Median Inc. www . .hu current numbers), it enjoys the same kind of superiority, being bigger than the next two competitors put together, that the British Navy had when Britannia ruled waves. The verb from the dog&apos;, the trademark of the search engine) entered the Hungarian lan- 203 in the same sense as the verb google now used in English. An important measure of user satisfaction, the number of pages downloaded in a single session, is also considerably better for 1origo.hul than its competitors. The independent auditor, Median Inc., defines a single session as no more than 30 minutes inactivity between page downloads: [origo.hu] users need to look at 6.9 pages until they are satisfied, while on the two largest competitors they have to download 7.9 and 8.1 pages respectively. There is currently no obvious way to quantify exactly how much of this effect can be attributed to better search capabilities and relevance ranking, but the conclusion that these play a significant role seems inescapable. The vi zsla search bar is placed promiat the center of the .hu start page. Upon entering a keyword such as a results page containing three major results areas is displayed. At the top, find results from the a Yahoo-style manually filled hierarchical compendium of web pages, in this case showing a path and industry building and construction construction materials adhesives and mortars cement. Upon clicking this last entry, the user gets 10 very high-quality pages, beginning with one discussing the situation of the cement industry in light of the upcoming EU ascension. Below this, we find the URLs and abstracts for the 10 most highly ranked of the 16,684 pages that have keyword to the left we find a ranked list of NLT-style custom search foldbeginning with elections, our query is ceresistant cement&apos; the katalogus is not displayed, the number of pages found is only 303, and the top custom search folders now drainage, adhesives-mortars, concrete, preparation, bridge con- &apos;To understand how the elections enter the picture one needs to know that allegations of botched and corrupt privatization of the cement industry were a prominent campaign theme. struction, building maintenance, painting and stuccoing, cement, industry, managethis order. The main features of the NLT keyword search engine that distinguish it from competitors, full support of Boolean queries (including full support of negation), phrase search, trailing wildcards, and proximity search, are well known. The page ranking algorithm, which uses links as one of many factors, has been discussed elsewhere (Krellenstein, 2002). Here we concentrate on the topic classification engine, which differs from its TREC counterparts in several relevant respects. First, the number of topics considered is very large (22,000 for the English hierarchy developed at NLT), as opposed to the few dozen to a few hundred topics considered e.g. in the Reuters work. Second, the assumption is that the typical document has only one dominant topic (or none, as we will discuss later). Two-topic documents are rare, three or more topics for a single document occur seldom enough to be negligible in the sense that we see no practical need for returning more than two topics per document (though the engine of course has the facilities for doing so, should the need arise in some non-web application). Finally, we assume that training data is available only in very small quantities, only a handful of documents per category, as opposed to the hundreds of training documents per category used in TREC. is a mature, coherent work of knowledge with a keyword-spotting hook into the search query system. As such, it provided an excellent basis for the NLT autoclassification system, which was trained on the basis of the high qualalready manually classified to it. Translating the large NLT topic hierarchy from English to Hungarian was not feasible in the deployment timeframe, but even if it were, we would have been faced with the formidable challenge of finding Hungarian exemplaries for many thousands of highly detailed NLT topics. Using katalogus also made sense because it was culinternal coherence of the system no doubt owes a great deal to the fact that originally it was developed by one person, Rudolf Ungvary, Hungarian National Library. 204 turally more appropriate (e.g. in the selection of sports it has a section for table tennis but not for American football) so the chances of finding more Hungarian webpages on the topic are higher. Besides using a native Hungarian topic hierarchy, the system also relies on a morphological analysis (stemming) component developed specifically for Hungarian by Gabor Proszeky and his associates at Morphologic Inc. We keep both the original (inflected) and the stemmed version available for keyword match and topic classification, since this produces superior results to using either of them alone. Other than these two instances of necessary localization, there is nothing in our system that is specifically geared toward Hungarian, and therefore we believe that the conclusions we draw about this particular algorithm apply to all topic classification systems with the same broad characteristics: 1. monolingual input 2. small amount of training data available 3. large number of topic categories 4. few documents with multiple topics In what follows we illustrate some of our points on a version of the old Reuters corpus, keeping the standard (Lewis) test/train split, but removing all articles that have more than one topic, and all topics that have less than three training examples. Needless to say, removal of the multitopic documents and the topics with extremely limited training makes the task easier: Bow TF-IDF (McCallum, 1996) obtains 92.51% correct classification on this set with the default settings. But our intention is not to &amp;quot;report results&amp;quot; on a corpus with 21578 (or, after removal, 8998) documents: our results are on the Hungarian web, a corpus over three orders of magnitude larger, and displaying all the difficulties of real language data, such as lack of consistent style, large numbers of typos, search engine spamming, etc. that are largely absent from Reuters. 2 The bag of words model assume a collection of documents a of topics that disjoint subsets c D(t will a finite set of words wi , w2. , order of decreasing frequency. generally the range — — for words not in this set introduce a catchall word wo. language mean a probability distribuassigns the appropriate frequencies to the w, either in some large collection of topicless texts, or in a corpus that is appropriately representative of all topics. By the (word unigram) model of a topic mean a probabildistribution that assigns the appropriate frethe in a large collection of about a collection C we call number of documents that contain w the docfrequency the word, denoted we call the total number of w tokens its that the set of topics = ,tk} arranged in order of deprobability qk. = T &lt; 1, that a document is topicless probability = 1 general language probability of a word w can therefore be computed topicless documents to be = or as practice, it is next to impossible to collect a large set of truly topicless documents, we estimate based on a collection we to be representative of the distribution of topics. It should be noted that this procedure, while workable, is fraught with difficulties, since general the are not known, and even for very large collections it can&apos;t always be assumed that proportion of documents falling in topic well. As we shall see shortly, within a given topic a few dozen, or perhaps a few hundred, are truly characteristic (have significantly higher than the background probability our goal will be to find these. To end, we need to first estimate trivmethod is to use the observed fregL(w) = TF(w,C)1L(C) is the length of the corpus C (total number of word tokens in it). While this is obviously very attractive, the numerical values so obtained tend to highly unstable. For example, the word makes up about 4.44% of a 55m word sample the Street Journal but 5.00% of a 205 word sample of the Jose Mercury News (Mere). For medium frequency words, the effect even more marked: for example appears 7.65 times per million word in the WSJ and 18.7 times per million in the Merc sample. And for low frequency words, the straightforward estimate very often comes out as 0, which tends to introduce singularities in models based on the estimates. same uncorrected estimate, = F (w, I of course available for but the problems discussed above are made worse by the fact that any topic-specific collection of documents is likely to be orders of magnitude than our overall corpus. Further, if is Bernoulli source, the probability t) a l instances of wi, /2 instances of w2, etc. is produced by the source for be given by the multinomial formula + /1 + • • • + 11, . • • , will be zero as long as any of the zero. Therefore, we will probabilities in the topic model by the (uncorrected) probabilities that we obtained for general language, since the latter are of necessity positive. Instead will therefore use + — where is a small but non-negligible constant, usually between .1 and .3. In the recent literature, e.g. (Zhai and Lafferty, 2001), this is genercalled There are two ways to justify this method: the trivial one is to say that documents are not fully topical, but be expected to contain a small of general language. A more interesting justification is to treat the general language probability as a Bayesian prior, the topic-specific frequency as the maximum likelihood estimate based on the observations, so that (2) will be the posterior mean of the unknown probability. For the Reuters experiment, we used the 46m Merc wordcount as our general (background) language model. the first to apply this technique to topic detection was Gish (1993-1994 Switchboard tasks, see (Colbath, 1998)). words, if any, are specific to a few topin the sense that &gt;&gt; is well measured by the number of documents containing the word: for examin only about 200k documents in a large collection containing over 200m English documents (see www. northernlight . corn), in 42m and 29m. However in a collection of 13k documents about digisignal processing 1100 times, about 6.5 while is about 5.5 • two orders of magnitude better. In general, words with low DF values, or what is the same, high IDF (inverse document frequency) values are good candidates for being topic-specific, though this criterion has to be used with care: it is quite possible that a word has high IDF because of deficiencies in the corpus, not because it is inherently very specific. For exthe word even higher IDF it is hard to imagine any topic that would call for its use more often than others. Recall that topics are modeled by Bernoulli (word unigram) sources: given a document with counts total length we make the naive Bayesian assumption that the l are indepenthe probability that topic this document will be obtained by substituting (2) in (1): + (1— • • • ,IN (3) For the 0th topic, general language, (1) and (3) are the same. The log probability quotient P(d L) the document being emitted topic the general language is given by log — (I)gt(wi)(4) ) We rearrange this sum in three parts: where significantly larger than it is about the same, and when it is significantly smaller. In the first part, the numerator is domiby we have OL(11)1)&gt;&gt;gt(wi) (1) i=0 206 which we can think of as the contribution of &amp;quot;negative evidence&amp;quot;, words that are significantly sparser for this topic than for general language. In the second part, the quotient is about 1, therefore the logs are about 0, so this whole part can be neglected — words that have about the same frequency in the topic as in general language can&apos;t help us distinguish whether the document came from the Bernoulli source associated with the topic from the one associated with general language. Note that the summands change sign here in the second part, and as long as the progression of terms is roughly linear, we can extend the limits in both directions without changing the overall zero value. Finally, the part where the probability of the words is significantly higher than the background probability will contribute the &amp;quot;positive evidence&amp;quot; ( (1 — (wt) 9L(wi)&lt;&lt;qt(w0 Since a is a small constant, on the order of .2, in the interesting cases (such as vs. in general language) is orders of maglarger than first term can be neglected and we have, for the positive evidence, g (WO &lt;&lt;gt(w,) In every term the first summand log(1 — a) is other two terms (w,)) — (w, ) measure the (base of magnitude in frequency over general language: we will this the word topic it by examples of the highest (positive), near-zero, and the lowest (negative) relevances follow: rank word r(w,alum) 1 aluminium 13.4176 2 tonnes 12.9357 3 lme 12.0313</abstract>
<address confidence="0.776687181818182">4 alumina 11.9061 1185 though 0.0079206 1186 30 0.00377953 1187 under 0.00100579 1188 second -0.0146792 1189 7 -0.0207462 1190 with -0.022297 1316 you -2.20392 1317 name -2.96474 1318 country -2.97375 1319 day -3.03341</address>
<abstract confidence="0.998686624365482">1 of the for the positive evidence quite negligible compared to the relevance, positive evidence can be approximated by the more manageable gL (WO &lt;&lt;gt(Wi) Needless to say, the real interest is not in determining whether a document belongs to a partopic opposed to general language, rather in whether it belongs in topic can compute log(P(d s)IP(d E))), the importance of this step is that we see that the &amp;quot;negative evidence&amp;quot; given by (5) also disappears. There are two reasons for this. First, the absolute value of the negative evidence is small: on the average Reuters topic, the sum of the negative relevances is less than 5% of the sum of positive relevances. Second, words that are below background for topic in general be also bebackground probability for topic their are concentrated in some other topic which they are truly characteristic. The key contriin distinguishing topics computlog therefore come from those few words that have significantly higher than background probabilities in at least one of these: — For words w, that are significant for both top- (such as be for DSP and for Harmonic Analysis), the contribution of general language cancels out, and we are left with But such words are rare even for closely related topics, so the two sums in (7) are largely disjoint. 207 What (7) defines is the simplest, historically and best understood pattern classifier, a linmachine the decision boundaries are simply hyperplanes (Highleynian, 1962; Duda et al., 2001). As the above reasoning makes clear, linearity is to some extent a matter of choice: certainly the underlying bag of words assumption, that the words are chosen independent of one another, is quite dubious. However, it is a good first approximation, and one can extend it from Bernoulli (0 order Markov) to first, second, third order, etc. Once the probabilities of word pairs, word triples, etc are explicitly modeled, much of the criticism directed at the bag of words approach its A relevance-based linear classifier containing for all topics all the words that appeared in its training set gives 91.13% correct classification: this has 2154 words in the average topic model. If the least relevant 40% of the words is excluded from the models, average model size deto 1454 words, but accuracy actually im- 92.83% (recall that the Bow baseline was 92.51% on this set), demonstrating rather clearly the main thesis that we derived via estimation above, namely that negative and zero evidence is simply noise that we can safely ignore. Reuters 215878 (3+ train, single topic) Accuracy by average model size average model size (Avrords log scale) Figure 1. Models with equal number of words vs equal cumulative TF Figure 1 shows the model-size accuracy tradeoff, with model size plotted on the x axis on a log scale. Note that if we keep only the top 15% of the words (average model size 333), we lose only NLT system directly indexes word pairs and can match strings of arbitrary length for topic classification. 6.4% of our peak classification performance, since the models still classify 87% correct. If we are prepared to sacrifice another 6% in performance, average model sin can be reduced to 236, with classification accuracy still at a very acceptable 80.7% level. The algorithm used to obtain these numbers simply ranks the words within each model by relevance, and keeps the models balanced by cumulative TF. NLT&apos;s proprietary word selection algorithm gets to the 80% level with 30 words per model. Reducing the model size even more drastically would take us out of the realm of practically acceptable classifiers, but as an illustration of our main point it should be noted that keeping the 5 best words in each model would give 46.8% corclassification, and keeping just the one with the greatest relevance for each topic, already gives 28.5% correct classification (on this set, random choice would give less than 3%). 3 Conclusions In Section 2 we argued that for topic classification only positive evidence, i.e. words with significantly higher than background probability, will ever matter. Though we illustrated this point on a standard corpus, we wish to emphasize that it is not this toy example, but rather the objectively measurable user satisfaction with the large-scale system described in Section 1, that provides the empirical underpinnings of our theoretical argument. If only the best (positive) evidence is used, models can be the sense of having coefficients t) for a few dozen, or perhaps a few hundred words u) for a given though the number of words contypically in the hundred thousands to millions (Kornai and Richards, 2002). An important side effect of this approach is that many documents, not containing a sufficient number of keywords for any topic, will be treated as topicless (part of the general language) i.e. they are rejected from classification. Given the nature and quality of many web documents, this is a desirable outcome. Not knowing that the parameter space is sparse, = topics and = words we 208 need to estimate = parameters even for the simplest (unigram) model. This may be (barely) within the limits of our supercomputing ability, but it is definitely beyond the reliability and representativeness of our data. Over the years, this led to a considerable body of research selection, tries to address the isby reducing on classificaaddresses it by reducing We can&apos;t discuss here in detail the problems inherent in hierarchical classification, but we note that for a practical topic detection system higher e.g. director often next to impossible to train, even though lower nodes e.g. Fellini, ... perform well. As for feature selection, we find that much of the literature suffers from what we will call the a feature, always a feature falif a word w is found distinctive for topic attempt is made to estimate (w) the whole of than the one value we really care about. The fact that high quality working classifiers as vi z be built using only sparse subsets of the whole potential feature set reflects a deep, structural property of the data: at least for the purpose of comparing log emission probabilacross topic models, the can be approxby sparse distributions fact, this structural property is so strong that it is possible to build classifiers that ignore the differences bethe numerical values of (w) (w) enreplacing both by a uniform estimate based on the IDF of w. Traditionally, the it multipliers in (7) are known as the term frequency (TF) factor. Such systems, where the classification load is carried entirely by the zero-one decision of using a particular word as a keyword for a topic, are the simplest TF-IDF classifiers, and the estimation method used in Section 2 fits in the broad tradition of deriving IDF-like weights (Robertson and Walker, 1997) from language modeling considerations (?; Hiemstra and Kraaij, 2002; Miller et al., 1999). What he have done in the body of the paper was to create a new rationale for a classical TF-IDF not just for vi z s for any system the same lines. The notion of keywords is often used, though not always defined, in information retrieval. We believe that this is an entirely valid notion, and offered a simple operational defsignificantly higher than background capture it. Our basic claim was that only the good keywords (positive evidence) matter, and the overall performance of our classification system largely supports this assertion. Acknowledgements large system such as always the work of many people. We would like to single out Gabi Steinberg (divine Inc.), whose contributions to the original NLT search and classification architecture are so fundamental that he should have been a coauthor, were it not for his insistence on staying in the background. Special thanks to Rudolf Ungvary (National Szechenyi who created the original Gabor Proszeky (Morphologic), who contributed the stemming, Andras Karpati (Axelero) and Peter Halacsy (Axelero) for creating and managing the training data and the Hungarian front end. Special thanks to Herb Gish and Richard Schwartz (BBN) for clarifying the early history of Bayesian language modeling techniques in topic detection. The described here while all authors were working at Northern Light Technology, now divine Inc.</abstract>
<title confidence="0.582396">References</title>
<author confidence="0.605899">S Colbath Rough&apos;n&apos;Ready A meeting recorder</author>
<affiliation confidence="0.410825">browser Perceptual User Interfaces Conference San</affiliation>
<address confidence="0.4325255">Francisco, CA, November 1998 220 Duda, P.E. Hart, and D.G. Stork 2001</address>
<title confidence="0.866701">Wiley and Sons</title>
<abstract confidence="0.902738125">D. Hiemstra and W. Kraaij 1998 TwentyOne at TRECad-hoc and cross language track of W.H. Highleyman 1962 Linear decision functions application to pattern recognition. the IRE, A. Kornai and J.M. Richards 2002 Linear discriminant text classification in high dimension In: A. and M. Koeppen (eds): Informa- Systems Verlag, Heidelberg 527-538 209 M. Krellenstein 2002 Operational aspects of the NLT engine of SIGIR-02 A.K. McCallum 1996 Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering //www. cs . T. Leek, and R.M. Schwartz 1999 A hidden Markov model information retrieval system of SIGIR-99 J.M. Ponte and W.B. Croft 1998 A language modapproach to information retrieval SIGIR-98 S.E. Robertson and S. Walker 1997 On relevance with little relevance information Proceed-</abstract>
<note confidence="0.653206">of SIGIR-97 Chengxiang Zhai and John Lafferty 2001 A Study of</note>
<title confidence="0.970587">Smoothing Methods for Language Models Applied</title>
<author confidence="0.611494">Ad Hoc Information Retrieval</author>
<author confidence="0.611494">De-</author>
<affiliation confidence="0.593477">in Information Retrieval</affiliation>
<address confidence="0.733083">210</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Colbath</author>
</authors>
<title>Rough&apos;n&apos;Ready: A meeting recorder and browser</title>
<date>1998</date>
<booktitle>Perceptual User Interfaces Conference</booktitle>
<pages>220</pages>
<location>San Francisco, CA,</location>
<contexts>
<context position="14149" citStr="Colbath, 1998" startWordPosition="2388" endWordPosition="2389">he trivial one is to say that documents are not fully topical, but can be expected to contain a small a portion of general language. A more interesting justification is to treat the general language probability as a Bayesian prior, the topic-specific frequency as the maximum likelihood estimate based on the observations, so that (2) will be the posterior mean of the unknown probability. For the Reuters experiment, we used the 46m Merc wordcount as our general (background) language model. 3Actually the first to apply this technique to topic detection was Gish (1993-1994 Switchboard tasks, see (Colbath, 1998)). What words, if any, are specific to a few topics in the sense that P(d E Dtlw E d) &gt;&gt; P(d e Di)? This is well measured by the number of documents containing the word: for example Fourier appears in only about 200k documents in a large collection containing over 200m English documents (see www. northernlight . corn), while see occurs in 42m and book in 29m. However in a collection of 13k documents about digital signal processing Fourier appears 1100 times, so P(d E Dt) is about 6.5 10-5 while P(d E tr) is about 5.5 • 10-3, two orders of magnitude better. In general, words with low DF values,</context>
</contexts>
<marker>Colbath, 1998</marker>
<rawString>S. Colbath Rough&apos;n&apos;Ready: A meeting recorder and browser Perceptual User Interfaces Conference San Francisco, CA, November 1998 220</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
<author>D G Stork</author>
</authors>
<date>2001</date>
<publisher>Pattern Classification John Wiley and Sons</publisher>
<contexts>
<context position="19703" citStr="Duda et al., 2001" startWordPosition="3374" endWordPosition="3377">und probabilities in at least one of these: E lir(w,t) — E lir(w,$) (7) gL(wi)&lt;&lt;gt(wi) gL(wi)&lt;&lt;gs(ivi) For words w, that are significant for both topics (such as Fourier would be for DSP and for Harmonic Analysis), the contribution of general language cancels out, and we are left with Ei log(g1(w2)Ig8(w,)). But such words are rare even for closely related topics, so the two sums in (7) are largely disjoint. 207 What (7) defines is the simplest, historically oldest, and best understood pattern classifier, a linear machine where the decision boundaries are simply hyperplanes (Highleynian, 1962; Duda et al., 2001). As the above reasoning makes clear, linearity is to some extent a matter of choice: certainly the underlying bag of words assumption, that the words are chosen independent of one another, is quite dubious. However, it is a good first approximation, and one can extend it from Bernoulli (0 order Markov) to first, second, third order, etc. Once the probabilities of word pairs, word triples, etc are explicitly modeled, much of the criticism directed at the bag of words approach loses its grip.4 A relevance-based linear classifier containing for all topics all the words that appeared in its train</context>
</contexts>
<marker>Duda, Hart, Stork, 2001</marker>
<rawString>R.O. Duda, P.E. Hart, and D.G. Stork 2001 Pattern Classification John Wiley and Sons</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hiemstra</author>
<author>W Kraaij</author>
</authors>
<title>TwentyOne at TREC7: ad-hoc and cross language track</title>
<date>1998</date>
<booktitle>Proceedings of TREC-7</booktitle>
<pages>174--185</pages>
<marker>Hiemstra, Kraaij, 1998</marker>
<rawString>D. Hiemstra and W. Kraaij 1998 TwentyOne at TREC7: ad-hoc and cross language track Proceedings of TREC-7 174-185</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Highleyman</author>
</authors>
<title>Linear decision functions with application to pattern recognition.</title>
<date>1962</date>
<booktitle>Proceedings of the IRE,</booktitle>
<pages>50--1501</pages>
<marker>Highleyman, 1962</marker>
<rawString>W.H. Highleyman 1962 Linear decision functions with application to pattern recognition. Proceedings of the IRE, 50:1501-1514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kornai</author>
<author>J M Richards</author>
</authors>
<title>Linear discriminant text classification in high dimension In: A. Abraham and M. Koeppen (eds): Hybrid Information Systems Physica Verlag,</title>
<date>2002</date>
<pages>527--538</pages>
<location>Heidelberg</location>
<contexts>
<context position="22974" citStr="Kornai and Richards, 2002" startWordPosition="3933" endWordPosition="3936">ver matter. Though we illustrated this point on a standard corpus, we wish to emphasize that it is not this toy example, but rather the objectively measurable user satisfaction with the large-scale system described in Section 1, that provides the empirical underpinnings of our theoretical argument. If only the best (positive) evidence is used, the models can be sparse, in the sense of having nonzero coefficients r(u,, t) only for a few dozen, or perhaps a few hundred words u) for a given topic t, even though the number of words considered, N, is typically in the hundred thousands to millions (Kornai and Richards, 2002). An important side effect of this approach is that many documents, not containing a sufficient number of keywords for any topic, will be treated as topicless (part of the general language) i.e. they are rejected from classification. Given the nature and quality of many web documents, this is a desirable outcome. Not knowing that the parameter space is sparse, for k = 104 topics and N = 106 words we 208 would need to estimate kN = 1010 parameters even for the simplest (unigram) model. This may be (barely) within the limits of our supercomputing ability, but it is definitely beyond the reliabil</context>
</contexts>
<marker>Kornai, Richards, 2002</marker>
<rawString>A. Kornai and J.M. Richards 2002 Linear discriminant text classification in high dimension In: A. Abraham and M. Koeppen (eds): Hybrid Information Systems Physica Verlag, Heidelberg 527-538</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Krellenstein</author>
</authors>
<date>2002</date>
<booktitle>Operational aspects of the NLT search engine Proceedings of SIGIR-02</booktitle>
<contexts>
<context position="5988" citStr="Krellenstein, 2002" startWordPosition="957" endWordPosition="959">tions enter the picture one needs to know that allegations of botched and corrupt privatization of the cement industry were a prominent campaign theme. struction, building maintenance, painting and stuccoing, cement, paint industry, and waste management in this order. The main features of the NLT keyword search engine that distinguish it from competitors, full support of Boolean queries (including full support of negation), phrase search, trailing wildcards, and proximity search, are well known. The page ranking algorithm, which uses links as one of many factors, has been discussed elsewhere (Krellenstein, 2002). Here we concentrate on the topic classification engine, which differs from its TREC counterparts in several relevant respects. First, the number of topics considered is very large (22,000 for the English hierarchy developed at NLT), as opposed to the few dozen to a few hundred topics considered e.g. in the Reuters work. Second, the assumption is that the typical document has only one dominant topic (or none, as we will discuss later). Two-topic documents are rare, three or more topics for a single document occur seldom enough to be negligible in the sense that we see no practical need for re</context>
</contexts>
<marker>Krellenstein, 2002</marker>
<rawString>M. Krellenstein 2002 Operational aspects of the NLT search engine Proceedings of SIGIR-02</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering http: //www. cs . cmu.edu/-mccallum/bow</title>
<date>1996</date>
<contexts>
<context position="9215" citStr="McCallum, 1996" startWordPosition="1490" endWordPosition="1492"> to all topic classification systems with the same broad characteristics: 1. monolingual input 2. small amount of training data available 3. large number of topic categories 4. few documents with multiple topics In what follows we illustrate some of our points on a version of the old Reuters corpus, keeping the standard (Lewis) test/train split, but removing all articles that have more than one topic, and all topics that have less than three training examples. Needless to say, removal of the multitopic documents and the topics with extremely limited training makes the task easier: Bow TF-IDF (McCallum, 1996) obtains 92.51% correct classification on this set with the default settings. But our intention is not to &amp;quot;report results&amp;quot; on a corpus with 21578 (or, after removal, 8998) documents: our results are on the Hungarian web, a corpus over three orders of magnitude larger, and displaying all the difficulties of real language data, such as lack of consistent style, large numbers of typos, search engine spamming, etc. that are largely absent from Reuters. 2 The bag of words model We assume a collection of documents D and a system of topics T such that T partitions D into largely disjoint subsets DI c</context>
</contexts>
<marker>McCallum, 1996</marker>
<rawString>A.K. McCallum 1996 Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering http: //www. cs . cmu.edu/-mccallum/bow</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Miller</author>
<author>T Leek</author>
<author>R M Schwartz</author>
</authors>
<title>A hidden Markov model information retrieval system</title>
<date>1999</date>
<booktitle>Proceedings of SIGIR-99</booktitle>
<pages>214--221</pages>
<contexts>
<context position="25469" citStr="Miller et al., 1999" startWordPosition="4371" endWordPosition="4374">ifferences between the numerical values of g8 (w) and gt (w) entirely, replacing both by a uniform estimate g(w) based on the IDF of w. Traditionally, the it multipliers in (7) are known as the term frequency (TF) factor. Such systems, where the classification load is carried entirely by the zero-one decision of using a particular word as a keyword for a topic, are the simplest TF-IDF classifiers, and the estimation method used in Section 2 fits in the broad tradition of deriving IDF-like weights (Robertson and Walker, 1997) from language modeling considerations (?; Hiemstra and Kraaij, 2002; Miller et al., 1999). What he have done in the body of the paper was to create a new rationale for a classical TF-IDF system, not just for vi z s la but for any system along the same lines. The notion of good keywords is often used, though not always defined, in information retrieval. We believe that this is an entirely valid notion, and offered a simple operational definition, has significantly higher than background probability, to capture it. Our basic claim was that only the good keywords (positive evidence) matter, and the overall performance of our classification system largely supports this assertion. Ackn</context>
</contexts>
<marker>Miller, Leek, Schwartz, 1999</marker>
<rawString>D.R. Miller, T. Leek, and R.M. Schwartz 1999 A hidden Markov model information retrieval system Proceedings of SIGIR-99 214-221</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A language modelling approach to information retrieval</title>
<date>1998</date>
<booktitle>Proceedings of SIGIR-98</booktitle>
<pages>275--281</pages>
<marker>Ponte, Croft, 1998</marker>
<rawString>J.M. Ponte and W.B. Croft 1998 A language modelling approach to information retrieval Proceedings of SIGIR-98 275-281</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
</authors>
<title>On relevance weights with little relevance information</title>
<date>1997</date>
<booktitle>Proceedings of SIGIR-97</booktitle>
<pages>16--24</pages>
<contexts>
<context position="25379" citStr="Robertson and Walker, 1997" startWordPosition="4357" endWordPosition="4360"> this structural property is so strong that it is possible to build classifiers that ignore the differences between the numerical values of g8 (w) and gt (w) entirely, replacing both by a uniform estimate g(w) based on the IDF of w. Traditionally, the it multipliers in (7) are known as the term frequency (TF) factor. Such systems, where the classification load is carried entirely by the zero-one decision of using a particular word as a keyword for a topic, are the simplest TF-IDF classifiers, and the estimation method used in Section 2 fits in the broad tradition of deriving IDF-like weights (Robertson and Walker, 1997) from language modeling considerations (?; Hiemstra and Kraaij, 2002; Miller et al., 1999). What he have done in the body of the paper was to create a new rationale for a classical TF-IDF system, not just for vi z s la but for any system along the same lines. The notion of good keywords is often used, though not always defined, in information retrieval. We believe that this is an entirely valid notion, and offered a simple operational definition, has significantly higher than background probability, to capture it. Our basic claim was that only the good keywords (positive evidence) matter, and </context>
</contexts>
<marker>Robertson, Walker, 1997</marker>
<rawString>S.E. Robertson and S. Walker 1997 On relevance weights with little relevance information Proceedings of SIGIR-97 16-24</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A Study of Smoothing Methods for Language Models Applied to Ad Hoc</title>
<date>2001</date>
<booktitle>Information Retrieval Research and Development in Information Retrieval</booktitle>
<pages>334--342</pages>
<contexts>
<context position="13437" citStr="Zhai and Lafferty, 2001" startWordPosition="2270" endWordPosition="2273">ment d containing l instances of wi, /2 instances of w2, etc. is produced by the source for topic t will be given by the multinomial formula (io + /1 + • • • + /N) 10, 11, . • • , Tn gt(wi)li which will be zero as long as any of the gt(wz) are zero. Therefore, we will smooth the probabilities in the topic model by the (uncorrected) probabilities that we obtained for general language, since the latter are of necessity positive. Instead of gt(w) we will therefore use agaw) + (1 — a)gt(w) (2) where is a small but non-negligible constant, usually between .1 and .3. In the recent literature, e.g. (Zhai and Lafferty, 2001), this is generally called Jelinek-Mercer smoothing.3 There are two ways to justify this method: the trivial one is to say that documents are not fully topical, but can be expected to contain a small a portion of general language. A more interesting justification is to treat the general language probability as a Bayesian prior, the topic-specific frequency as the maximum likelihood estimate based on the observations, so that (2) will be the posterior mean of the unknown probability. For the Reuters experiment, we used the 46m Merc wordcount as our general (background) language model. 3Actually</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty 2001 A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval Research and Development in Information Retrieval 334-342</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>