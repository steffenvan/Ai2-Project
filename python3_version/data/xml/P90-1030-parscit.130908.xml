<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001672">
<note confidence="0.813061666666667">
Computational structure of generative phonology
and its relation to language comprehension.
Eric Sven Ristad*
MIT Artificial Intelligence Lab
545 Technology Square
Cambridge, MA 02139
</note>
<sectionHeader confidence="0.982249" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999639625">
We analyze the computational complexity of
phonological models as they have developed over
the past twenty years. The major results are
that generation and recognition are nndecidable
for segmental models, and that recognition is NP-
hard for that portion of segmental phonology sub-
sumed by modern autosegmental models. Formal
restrictions are evaluated.
</bodyText>
<sectionHeader confidence="0.980916" genericHeader="method">
I. Introduction
</sectionHeader>
<bodyText confidence="0.999903">
Generative linguistic theory and human language
comprehension may both be thought of as com-
putations. The goal of language comprehension
is to construct structural descriptions of linguistic
sensations, while the goal of generative theory is
to enumerate all and only the possible (grammat-
ical) structural descriptions. These computations
are only indirectly related. For one, the input to
the two computations is not the same. As we shall
see below, the most we might say is that generative
theory provides an extensional characterization of
language comprehension, which is a function from
surface forms to complete representations, includ-
ing underlying forms. The goal of this article is
to reveal exactly what generative linguistic theory
says about language comprehension in the domain
of phonology.
The article is organized as follows. In the next
section, we provide a brief overview of the com-
putational structure of generative phonology. In
section 3, we introduce the segmental model of
phonology, discuss its computational complexity,
and prove that even restricted segmental mod-
els are extremely powerful (undecidable). Subse-
quently, we consider various proposed and plausi-
ble restrictions on the model, and conclude that
even the maximally restricted segmental model is
likely to be intractable. The fourth section in-
troduces the modern autosegmental (nonlinear)
model and discusses its computational complexity.
</bodyText>
<note confidence="0.5912908">
The author is supported by a IBM graduate
fellowship and eternally indebted to Morris Halle
and Michael Kenstowicz for teaching him phonol-
ogy. Thanks to Noam Chomsky, Sandiway Fong, and
Michael Kashket for their comments and assistance.
</note>
<bodyText confidence="0.957786882352941">
We prove that the natural problem of construct-
ing an autosegmental representation of an under-
specified surface form is NP-hard. The article
concludes by arguing that the complexity proofs
are unnatural despite being true of the phonolog-
ical models, because the formalism of generative
phonology is itself unnatural.
The central contributions of this article are:
(i) to explicate the relation between generative
theory and language processing, and argue that
generative theories are not models of language
users primarily because they do not consider the
inputs naturally available to language users; and
(ii) to analyze the computational complexity of
generative phonological theory, as it has developed
over the past twenty years, including segmental
and autosegmental models.
</bodyText>
<sectionHeader confidence="0.717444" genericHeader="method">
2 Computational structure
of generative phonology
</sectionHeader>
<bodyText confidence="0.980743066666667">
The structure of a computation may be described
at many levels of abstraction, principally includ-
ing: (i) the goal of the computation; (ii) its in-
put/output specification (the problem statement),
(iii) the algorithm and representation for achiev-
ing that specification, and (iv) the primitive opera-
tions in which terms the algorithm is implemented
(the machine architecture).
Using this framework, the computational struc-
ture of generative phonology may be described as
follows:
• The computational goal of generative phonol-
ogy (as distinct from it&apos;s research goals) is to
enumerate the phonological dictionaries of all
and only the possible human languages.
</bodyText>
<listItem confidence="0.9601451">
• The problem statement is to enumerate the
observed phonological dictionary of a particu-
lar language from some underlying dictionary
of morphemes (roots and affixes) and phono-
logical processes that apply to combinations
of underlying morphemes.
• The algorithm by which this is accomplished
is a derivational process g (&apos;the grammar&apos;)
from underlying forms r to surface forms
y = g(m). Underlying forms are constructed
</listItem>
<page confidence="0.997701">
235
</page>
<bodyText confidence="0.999858433333333">
by combining (typically, with concatenation
or substitution) the forms stored in the under-
lying dictionary of morphemes. Linguistic re-
lations are represented both in the structural
descriptions and the derivational process.
The structural descriptions of phonology are
representations of perceivable distinctions be-
tween linguistic sounds, such as stress lev-
els, syllable structure, tone, and articula-
tory gestures. The underlying and surface
forms are both drawn from the same class
of structural descriptions, which consist of
both segmental strings and autosegmental re-
lations. A segmental string is a string of
segments with some representation of con-
stituent structur. In the SPE theory of Chom-
sky and Halle (1968) concrete boundary sym-
bols are used; in Lexical Phonology, abstract
brackets are used. Each segment is a set of
phonological features, which are abstract as
compared with phonetic representations, al-
though both are given in terms of phonetic
features. Suprasegmental relations are rela-
tions among segments, rather than properties
of individual segments. For example, a syl-
lable is a hierarchical relation between a se-
quence of segments (the nucleus of the syl-
lable) and the less sonorous segments that
immediately preceed and follow it (the onset
and coda, respectively). Syllables must sat-
isfy certain universal constraints, such as the
sonority sequencing constraint, as well as lan-
guage particular ones.
• The derivational process is implemented by
an ordered sequence of unrestricted rewriting
rules that are applied to the current deriva-
tion string to obtain surface forms.
According to generative phonology, comprehen-
sion consists of finding a structural description for
a given surface form. In effect, the logical prob-
lem of language comprehension is reduced to the
problem of searching for the underlying form that
generates a given surface form. When the sur-
face form does not transparently identify its cor-
responding underlying form, when the space of
possible underlying forms is large, or when the
grammar g is computationally complex, the logical
problem of language comprehension can quickly
become very difficult.
In fact, the language comprehension problem
is intractable for all segmental theories. For ex-
ample, in the formal system of The Sound Pat-
tern of English (SPE) the comprehension prob-
lem is undecidable. Even if we replace the seg-
mental representation of cyclic boundaries with
the abstract constituents of Lexical Phonology,
and prohibit derivational rules from readjusting
constituent boundaries, comprehension remains
PSPACE-complete. Let us now turn to the tech-
nical details.
</bodyText>
<sectionHeader confidence="0.973604" genericHeader="method">
3 Segmental Phonology
</sectionHeader>
<bodyText confidence="0.993098490909091">
The essential components of the segmental model
may be briefly described as follows. The set of
features includes both phonological features and
diacritics and the distinguished feature segment
that marks boundaries. (An example diacritic is
ablaut, a feature that marks stems that must
undergo a change vowel quality, such as tense-
conditioned ablaut in the English sing, sang, sung
alternation.) As noted in SPE, &amp;quot;technically speak-
ing, the number of diacritic features should be at
least as large as the number of rules in the phonol-
ogy. Hence, unless there is a bound on the length
of a phonology, the set [of features] should be un-
limited.&amp;quot; (f.n.1, p.390) Features may be specified
+ or - or by an integral value 1, 2, N where N
is the maximal degree of differentiation permitted
for any linguistic feature. Note that N may vary
from language to language, because languages ad-
mit different degrees of differentiation in such fea-
tures as vowel height, stress, and tone. A set of
feature specifications is called a unit or sometimes
a segment. A string of units is called a matrix or
a segmental string.
A elementary rule is of the form Z X AY W
ZXBYW where A and B may be •;6 or any unit,
A 0 B; X and Y may be matrices (strings of
units), and Z and W may be thought of a brack-
ets labelled with syntactic categories such as
or &apos;N&apos; and so forth. A complez rule is a finite
schema for generating a (potentially infinite) set
of elementary rules.&apos; The rules are organised into
&apos;Following Johnson (1974 we may define schema
as follows. The empty string and each unit is a schema;
schema may be combined by the operations of union,
intersection, negation, kleene star, and exponentiation
over the set of units. Johnson also introduces variables
and Boolean conditions into the schema. This &amp;quot;schema
language&amp;quot; is a extremely powerful characterisation of
the class of regular languages over the alphabet of
units; it is not used by practicing phonologists. Be-
cause a given complex rule can represent an infinite set
of elementary rules, Johnson shows how the iterated,
exhaustive application of one complex rule to a given
segmental string can &amp;quot;effect virtually any computable
mapping,&amp;quot; (p.10) ie., can simulate any TM computa-
tion. Next, he proposes a more restricted &amp;quot;simultane-
ous&amp;quot; mode of application for a complex rule, which is
only capable of performing a finite-state mapping in
any application. This article considers the indepen-
dent question of what computations can be performed
by a set of elementary rules, and hence provides loose
lower bounds for Johnson&apos;s model. We note in pass-
ing, however, that the problem of simply determining
whether a given rule is subsumed by one of Johnson&apos;s
schema is itself intractable, requiring at least exponen-
</bodyText>
<page confidence="0.987945">
236
</page>
<bodyText confidence="0.999986">
linear sequence R1, 1?2, ... R,i, and they are ap-
plied in order to an underlying matrix to obtain a
surface matrix.
Ignoring a great many issues that are important
for linguistic reasons but irrelevant for our pur-
poses, we may think of the derivational process as
follows. The input to the derivation, or &amp;quot;underly-
ing form,&amp;quot; is a bracketed string of morphemes, the
output of the syntax. The output of the derivation
is the &amp;quot;surface form,&amp;quot; a string of phonetic units.
The derivation consists of a series of cycles. On
each cycle, the ordered sequence of rules is ap-
plied to every maximal string of units containing
no internal brackets, where each Ri+i applies (or
doesn&apos;t apply) to the result of applying the imme-
diately preceding rule 14, and so forth. Each rule
applies simultaneously to all units in the current
derivational string. For example, if we apply the
rule A —■ B to the string AA, the result is the
string BE. At the end of the cycle, the last rule
erases the innermost brackets, and then the
next cycle begins with the rule R1. The deriva-
tion terminates when all the brackets are erased.
Some phonological processes, such as the as-
similation of voicing across morpheme boundaries,
are very common across the world&apos;s languages.
Other processes, such as the arbitrary insertion
of consonants or the substitution of one unit for
another entirely distinct unit, are extremely rare
or entirely unattested. For this reason, all ade-
quate phonological theories must include an ex-
plicit measure of the naturalness of a phonologi-
cal process. A phonological theory must also de-
fine a criterion to decide what constitutes two in-
dependent phonological processes and what con-
stitutes a legitimate phonological generalization.
Two central hypotheses of segmental phonology
are (i) that the most natural grammaas contain
the fewest symbols and (ii) a set of rules rep-
resent independent phonological processes when
they cannot be combined into a single rule schema
according to the intricate notational system first
described in SPE. (Chapter 9 of Kenstowicz and
Kisseberth (1979) contains a less technical sum-
mary of the SPE system and a discussion of sub-
sequent modifications and emendations to it.)
</bodyText>
<subsectionHeader confidence="0.990053">
3.1 Complexity of segmental
recognition and generation.
</subsectionHeader>
<bodyText confidence="0.97854605">
Let us say a dictionary D is a finite set of the
underlying phonological forms (matrices) of mor-
phemes. These morphemes may be combined by
concatenation and simple substitution (a syntactic
category is replaced by a morpheme of that cate-
gory) to form a possibly infinite set of underlying
forms. Then we may characterize the two central
computations of phonology as follows.
tial space.
The phonological generation problem (PGP) is:
Given a completely specified phonological matrix
and a segmental grammar g, compute the sur-
face form y = g(z) of.
The phonological recognition problem (PRP) is:
Given a (partially specified) surface form y, a dic-
tionary D of underlying forms, and a segmental
grammar g, decide if the surface form y = g(a)
can be derived from some underlying form z ac-
cording to the grammar g, where z constructed
from the forms in D.
</bodyText>
<construct confidence="0.98710675">
Lemma 3.1 The segmental model can directly
simulate the computation of any deterministic-
Turing machine M on any input w, using only
elementary rules.
</construct>
<bodyText confidence="0.995697761904762">
Proof. We sketch the simulation. The underlying
form a will represent the TM input w, while the
surface form y will represent the halted state of M
on w. The immediate description of the machine
(tape contents, head position, state symbol) is rep-
resented in the string of units. Each unit repre-
sents the contents of a tape square. The unit rep-
resenting the currently scanned tape square will
also be specified for two additional features, to
represent the state symbol of the machine and the
direction in which the head will move. Therefore,
three features are needed, with a number of spec-
ifications determined by the finite control of the
machine M. Each transition of M is simulated by
a phonological rule. A few rules are also needed to
move the head position around, and to erase the
entire derivation string when the simulated ma-
chine halts.
There are only two key observations, which do
not appear to have been noticed before. The first
is that contrary to popular misstatement, phono-
logical rules are not context-sensitive. Rather,
they are unrestricted rewriting rules because they
can perform deletions as well as insertions. (This
is essential to the reduction, because it allows
the derivation string to become arbitarily long.)
The second observation is that segmental rules
can freely manipulate (insert and delete) bound-
ary symbols, and thus it is possible to prolong the
derivation indefinitely: we need only employ a rule
Rn.4 at the end of the cycle that adds an extra
boundary symbol to each end of the derivation
string, unless the simulated machine has halted.
The remaining details are omitted, but may be
found in Ristad (1990. fl
The immediate consequences are:
Theorem 1 POP is undecidabk.
Proof. By reduction to the unclecidable prob-
lem to e L(M)? of deciding whether a given TM
M accepts an input to. The input to the gen-
eration problem consists of an underlying form
z that represents to and a segmental grammar
</bodyText>
<page confidence="0.982185">
237
</page>
<bodyText confidence="0.997067538461539">
g that simulates the computations of M accord-
ing to lemma 3.1. The output is a surface form
y = g(x) that represents the halted configuration
of the TM, with all but the accepting unit erased.
Theorem 2 PRP is undecidable.
Proof. By reduction to the undecidable prob-
lem L(M) =?O of deciding whether a given TM
M accepts any inputs. The input to the recog-
nition problem consists of a surface form y that
represents the halted accepting state of the TM,
a trivial dictionary capable of generating E*, and
a segmental grammar g that simulates the com-
putations of the TM according to lemma 3.1. The
output is an underlying form x that represents the
input that M accepts. The only trick is to con-
struct a (trivial) dictionary capable of generating
all possible underlying forms E*.
An important corollary to lemma 3.1 is that we
can encode a universal Turing machine in a seg-
mental grammar. If we use the four-symbol seven-
state &amp;quot;smallest UTM&amp;quot; of Minsky (1969), then the
resulting segmental model contains no more than
three features, eight specifications, and 36 very
simple rules (exact details in Ristad, 1990). As
mentioned above, a central component of the seg-
mental theory is an evaluation metric that favors
simpler (ie., shorter) grammars. This segmental
grammar of universal computation appears to con-
tain significantly fewer symbols than a segmental
grammar for any natural language. Therefore, this
corollary presents severe conceptual and empirical
problems for the segmental theory.
Let us now turn to consider the range of plau-
sible restrictions on the segmental model. At
first glance, it may seem that the single most
important computational restriction is to prevent
rules from inserting boundaries. Rules that ma-
nipulate boundaries are called readjustment rules.
They are needed for two reasons. The first is to
reduce the number of cycles in a given deriva-
tion by deleting boundaries and flattening syntac-
tic structure, for example to prevent the phonol-
ogy from assigning too many degrees of stress
to a highly-embedded sentence. The second is
to rearrange the boundaries given by the syn-
tax when the intonational phrasing of an utter-
ance does not correspond to its syntactic phras-
ing (so-called &amp;quot;bracketing paradoxes&amp;quot;). In this
case, boundaries are merely moved around, while
preserving the total number of boundaries in the
string. The only way to accomplish this kind of
bracket readjustment in the segmental model is
with rules that delete brackets and rules that in-
sert brackets. Therefore, if we wish to exclude
rules that insert boundaries, we must provide an
alternate mechanism for boundary readjustment.
For the sake of argument—and because it is not
too hard to construct such a boundary readjust-
ment mechanism—let us henceforth adopt this re-
striction. Now how powerful is the segmental
model?
Although the generation problem is now cer-
tainly decidable, the recognition problem remains
undecidable, because the dictionary and syntax
are both potentially infinite sources of bound-
aries: the underlying form n needed to generate
any given surface form according to the grammar
g could be arbitrarily long and contain an arbi-
trary number of boundaries. Therefore, the com-
plexity of the recognition problem is unaffected
by the proposed restriction on boundary readjust-
ments. The obvious restriction then is to addi-
tionally limit the depth of embeddings by some
fixed constant. (Chomsky and Halle flirt with
this restriction for the linguistic reasons mentioned
above, but view it as a performance limitation,
and hence choose not to adopt it in their theory
of linguistic competence.)
</bodyText>
<construct confidence="0.789492333333333">
Lemma 3.2 Each derivational cycle can directly
simulate any polynomial time alternating Turing
machine (ATM) M computation.
</construct>
<bodyText confidence="0.999632205882353">
Proof. By reduction from a polynomial-depth
ATM computation. The input to the reduction is
an ATM M on input to. The output is a segmen-
tal grammar g and underlying form z s.t. the sur-
face form y = g(z) represents a halted accepting
computation if M accepts to in polynomial time.
The major change from lemma 3.1 is to encode
the entire instantaneous description of the ATM
state (ie., tape contents, machine state, head po-
sition) in the features of a single unit. To do this
requires a polynomial number of features, one for
each possible tape square, plus one feature for the
machine state and another for the head position.
Now each derivation string represents a level of
the ATM computation tree. The transitions of the
ATM computation are encoded in a block B as fol-
lows. An AND-transition is simulated by a triple
of rules, one to insert a copy of the current state,
and two to implement the two transitions. An OR-
transition is simulated by a pair of disjunctively-
ordered rules, one for each of the possible succes-
sor states. The complete rule sequence consists
of a polynomial number of copies of the block B.
The last rules in the cycle delete halting states,
so that the surface form is the empty string (or
reasonably-sized string of &apos;accepting&apos; units) when
the ATM computation halts and accepts. If, on
the other hand, the surface form contains any non-
halting or nonaccepting units, then the ATM does
not accept its input to in polynomial time. The
reduction may clearly be performed in time poly-
nomial in the size of the ATM and its input.
Because we have restricted the number of em-
beddings in an underlying form to be no more than
</bodyText>
<page confidence="0.994431">
238
</page>
<bodyText confidence="0.993812987951807">
a fixed language-universal constant, no derivation
can consist of more than a constant number of
cycles. Therefore, lemma 3.2 establishes the fol-
lowing theorems:
Theorem 3 PGP with bounded embeddings is
P5PACE-hard.
Proof. The proof is an immediate consequence of
lemma 3.2 and a corollary to the Chandra-Kozen-
Stockmeyer theorem (1981) that equates polyno-
mial time ATM computations and PSPACE DTM
computations. p
Theorem 4 PRP with bounded embedding. is
PSPACE-hard.
Proof. The proof follows from lemma 3.2 and
the Chandra-Kozen-Stockmeyer result. The dic-
tionary consists of the lone unit that encodes the
ATM starting configuration (ie., input w, start
state, head on leftmost square). The surface string
is either the empty string or a unit that represents
the halted accepting ATM configuration. 0
There is some evidence that this is the most
we can do, at least for the PGP. The requirement
that the reduction be polynomial time limits us
to specifying a polynomial number of features and
a polynomial number of rules. Since each feature
corresponds to a tape square, ie., the ATM space
resource, we are limited to PSPACE ATM compu-
tations. Since each phonological rule corresponds
to a next-move relation, ie., one time step of the
ATM, we are thereby limited to specifying PTIME
ATM computations.
For the PRP, the dictionary (or syntax-
interface) provides the additional ability to
nondeterministically guess an arbitrarily long,
boundary-free underlying form x with which to
generate a given surface form g(a). This ability
remains unused in the preceeding proof, and it is
not too hard to see how it might lead to undecid-
ability.
We conclude this section by summarizing the
range of linguistically plausible formal restrictions
on the derivational process:
Feature system. As Chomsky and Halle noted,
the SPE formal system is most naturally seen
as having a variable (unbounded) set of fea-
tures and specifications. This is because lan-
guages differ in the diacritics they employ, as
well as differing in the degrees of vowel height,
tone, and stress they allow. Therefore, the set
of features must be allowed to vary from lan-
guage to language, and in principle is limited
only by the number of rules in the phonol-
ogy; the set of specifications must likewise be
allowed to vary from language to language.
It is possible, however, to postulate the ex-
istence of a large, fixed, language-universal
set of phonological features and a fixed upper
limit to the number N of perceivable distinc-
tions any one feature is capable of supporting.
If we take these upper limits seriously, then
the class of reductions described in lemma 3.2
would no longer be allowed. (It will be pos-
sible to simulate any NI&apos; computation in a
single cycle, however.)
Rule format. Rules that delete, change, ex-
change, or insert segments—as well as rules
that manipulate boundaries—are crucial to
phonological theorizing, and therefore cannot
be crudely constrained. More subtle and in-
direct restrictions are needed. One approach
is to formulate language-universal constraints
on phonological representations, and to allow
a segment to be altered only when it violates
some constraint.
McCarthy (1981:405) proposes a morpheme
rule constraint (MRC) that requires all mor-
phological rules to be of the form A BIX
where A is a unit or 0, and B and X are
(possibly null) strings of units. (X is the im-
mediate context of A, to the right or left.)
It should be obvious that the MRC does
not constrain the computational complexity
of segmental phonology.
</bodyText>
<sectionHeader confidence="0.976758" genericHeader="method">
4 Autosegmental Phonology
</sectionHeader>
<bodyText confidence="0.99997162962963">
In the past decade, generative phonology has
seen a revolution in the linguistic treatment of
suprasegmental phenomena such as tone, har-
mony, infixation, and stress assignment. Although
these autosegmental models have yet to be for-
malised, they may be briefly described as follows.
Rather than one-dimensional strings of segments,
representations may be thought of as &amp;quot;a three-
dimensional object that for concreteness one might
picture as a spiral-bound notebook,&amp;quot; whose spine
is the segmental string and whose pages contain
simple constituent structures that are indendent
of the spine (Halle 1985). One page represents the
sequence of tones associated with a given articu-
lation. By decoupling the representation of tonal
sequences from the articulation sequence, it is pos-
sible for segmental sequences of different lengths
to nonetheless be associated to the same tone se-
quence. For example, the tonal sequence Low-
High-High, which is used by English speakers to
express surprise when answering a question, might
be associated to a word containing any number
of syllables, from two (Brazil) to twelve (floccin-
auccinihilipiNication) and beyond. Other pages
(called &amp;quot;planes&amp;quot;) represent morphemes, syllable
structure, vowels and consonants, and the tree of
articulatory (ie., phonetic) features.
</bodyText>
<page confidence="0.994527">
239
</page>
<subsectionHeader confidence="0.883297">
4.1 Complexity of autosegmental
recognition.
</subsectionHeader>
<bodyText confidence="0.999833391304348">
In this section, we prove that the PRP for an-
tosegmental models is NP-hard, a significant re-
duction in complexity from the undecidable and
PSPACE-hard computations of segmental theo-
ries. (Note however that autos egmental repre-
sentations have augmented—but not replaced—
portions of the segmental model, and therefore,
unless something can be done to simplify segmen-
tal derivations, modern phonology inherits the in-
tractability of purely segmental approaches.)
Let us begin by thinking of the NP-complete
3-Satisfiability problem (3SAT) as a set of inter-
acting constraints. In particular, every satisfiable
Boolean formula in 3-CNF is a string of clauses
Ci,Cs,...,C, in the variables xi, zz, , Zn that
satisfies the following three constraints: (i) nega-
tion: a variable z1 and its negation &amp;quot;i7 have op-
posite truth values; (ii) clausal satisfaction: every
clause Ci = (NA Vci) contains a true literal (a lit-
eral is a variable or its negation); (iii) consistency
of truth assignments: every unnegated literal of
a given variable is assigned the same truth value,
either 1 or 0.
</bodyText>
<subsectionHeader confidence="0.641561">
Lemma 4.1 Autosegmental representations can
enforce the MAT constraints.
</subsectionHeader>
<bodyText confidence="0.989008505617978">
Proof. The idea of the proof is to encode negation
and the truth values of variables in features; to
enforce clausal satisfication with a local autoseg-
mental process, such as syllable structure; and
to ensure consistency of truth assignments with
a nonlocal autosegmental process, such as a non-
concatenative morphology or long-distance assim-
ilation (harmony). To implement these ideas we
must examine morphology, harmony, and syllable
structure.
Morphology. In the more familiar languages
of the world, such as Romance languages, mor-
phemes are concatenated to form words. In other
languages, such as Semitic languages, a morpheme
may appear more that once inside another mor-
pheme (this is called infixation). For example, the
Arabic word katab, meaning he wrote&apos;, is formed
from the active perfective morpheme a doubly in-
fixed to the ktb morpheme. In the a.utosegmental
model, each morpheme is assigned its own plane.
We can use this system of representation to ensure
consistency of truth assigments. Each Boolean
variable zi is represented by a separate morpheme
pi, and every literal of zi in the string of formula
literals is associated to the one underlying mor-
pheme pi.
Harmony. Assimilation is the common phono-
logical process whereby some segment comes to
share properties of an adjacent segment. In En-
glish, consonant nasality assimilates to immedi-
ately preceding vowels; assimilation also occurs
across morpheme boundaries, as the varied surface
forms of the prefix in- demonstrate: in+togleal
illogical and in+probable --• improbable. In other
languages, assimilation is unbounded and can af-
fect nonadjacent segments: these assimilation pro-
cesses are called harmony systems. In the Turkic
languages all suffix vowels assimilate the backnesss
feature of the last stem vowel; in Capanahua, vow-
els and glides that precede a word-final deleted
nasal (an underlying nasal segment absent from
the surface form) are all nasalized. In the autoseg-
mental model, each harmonic feature is assigned
its own plane. As with morpheme-infixation, we
can represent each Boolean variable by a harmonic
feature, and thereby ensure consistency of truth
assignments.
Syllable structure. Words are partitioned into
syllables. Each syllable contains one or more vow-
els V (its nucleus) that may be preceded or fol-
lowed by consonants C. For example, the Ara-
bic word ka.tab consists of two syllables, the two-
segment syllable CV and the three-segment closed
syllable CVC. Every segment is assigned a sonor-
ity value, which (intuitively) is proportional to the
openness of the vocal cavity. For example, vowels
are the most sonorous segments, while stops such
as p or b are the least sonorous. Syllables obey a
language-universal sonority sequencing constraint
(SSC), which states that the nucleus is the sonor-
ity peak of a syllable, and that the sonority of
adjacent segments swiftly and monotonically de-
creases. We can use the SSC to ensure that every
clause Ci contains a true literal as follows. The
central idea is to make literal truth correspond to
the stricture feature, so that a true literal (repre-
sented as a vowel) is more sonorous than a false
literal (represented as a consonant). Each clause
= (a.i V bi V ci) is encoded as a segmental string
C— za zb ze, where C is a consonant of sonor-
ity 1. Segment z. has sonority 10 when literal
ai is true, 2 otherwise; segment zb has sonority 9
when literal bi is true, 5 otherwise; and segment zc
has sonority 8 when literal ci is true, 2 otherwise.
Of the eight possible truth values of the three lit-
erals and the corresponding syllabifications, only
the syllabification corresponding to three false lit-
erals is excluded by the SSC. In that case, the
corresponding string of four consonants C-C-C-C
has the sonority sequence 1-2-5-2. No immediately
preceeding or following segment of any sonority
can result in a syllabification that obeys the SSC.
Therefore, all Boolean clauses must contain a true
literal. (Complete proof in Ristad, 1990) f=1
The direct consequence of this lemma 4.1 is:
Theorem 5 PRP for the autosegmental model is
NP-hard.
Proof. By reduction to 3SAT. The idea is to
construct a surface form that completely identi-
</bodyText>
<page confidence="0.982537">
240
</page>
<bodyText confidence="0.9993385">
fies the variables and their negation or lack of
it, but does not specify the truth values of those
variables. The dictionary will generate all possi-
ble underlying forms (infixed morphemes or har-
monic strings), one for each possible truth as-
signment, and the autosegmental representation
of lemma 4.1 will ensure that generated formulas
are in fact satisfiable. 0
</bodyText>
<sectionHeader confidence="0.997258" genericHeader="method">
5 Conclusion.
</sectionHeader>
<bodyText confidence="0.998937976470589">
In my opinion, the preceding proofs are unnatural,
despite being true of the phonological models, be-
cause the phonological models themselves are un-
natural. Regarding segmental models, the uncle-
eidability results tell us that the empirical content
of the SPE theory is primarily in the particular
rules postulated for English, and not in the ex-
tremely powerful and opaque formal system. We
have also seen that symbol-minimization is a poor
metric for naturalness, and that the complex no-
tational system of SPE (not discussed here) is an
inadequate characterization of the notion of &amp;quot;ap-
propriate phonological generalization.&amp;quot;2
Because not every segmental grammar g gener-
ates a natural set of sound patterns, why should
we have any faith or interest in the formal system?
The only justification for these formal systems
then is that they are good programming languages
for phonological processes, that clearly capture
our intuitions about human phonology. But seg-
mental theories are not such good programming
languages. They are notationally-constrained and
highly-articulated, which limits their expressive
power; they obscurely represent phonological re-
lations in rules and in the derivation process it-
self, and hide the dependency relations and inter-
actions among phonological processes in rule or-
dering, disjunctive ordering, blocks, and cyclicity.3
&apos;The explication of what constitutes a &amp;quot;natural
rule&amp;quot; is significantly more elusive than the symbol-
minimization metric suggests. Explicit symbol-
counting is rarely performed by practicing phonolo-
gists, and when it is, it results in unnatural rules.
Moreover, the goal of constructing the smallest gram-
mar for a given (infinite) set is not attainable in prin-
ciple, because it requires us to solve the undecid-
able TM equivalence problem. Nor does the symbol-
counting metric constrain the generative or computa-
tional power of the formalism. Worst of all, the UTM
simulation suggested above shows that symbol count
does not correspond to &amp;quot;naturalness.&amp;quot; In fact, two
of the simplest grammars generate 43 and Es, both of
which are extremely unnatural.
A further difficulty for autosegmental models (not
brought out by the proof) is that the interactions
among planes is obscured by the current practice of
imposing an absolute order on the construction of
planes in the derivation process. For example, in En-
glish phonology, syllable structure is constructed be-
Yet, despite all these opaque notational con-
straints, it is possible to write a segmental gram-
mar for any decidable set.
A third unnatural feature is that the goal of
enumerating structural descriptions has an indi-
rect and computationally costly connection to the
goal of language comprehension, which is to con-
struct a structural description of a given utter-
ance. When information is missing from the sur-
face form, the generative model obligates itself
to enumerate all possible underlying forms that
might generate the surface form. When the gen-
erative process is lengthy, capable of deletions, or
capable of enforcing complex interactions between
nonlocal and local relations, then the logical prob-
lem of language comprehension will be intractable.
Natural phonological processes seem to avoid
complexity and simplify interactions. It is hard
to find an phonological constraint that is absolute
and inviolable. There are always exceptions, ex-
ceptions to the exceptions, and so forth. Deletion
processes like apocope, syncopy, cluster simplica-
tion and stray erasure, as well as insertions, seem
to be motivated by the necessity of modifying a
representation to satisfy a phonological constraint,
not to exclude representations or to generate com-
plex sets, as we have used them here.
Finally, the goal of enumerating structural de-
scriptions might not be appropriate for phonology
and morphology, because the set of phonological
words is only finite and phrase-level phonology is
computationally simple. There is no need or ra-
tional for employing such a powerful derivational
system when all we are trying to do is capture
the relatively little systematicity in a finite set of
representations.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="method">
6 References.
</sectionHeader>
<reference confidence="0.998743928571429">
Chandra, A., D. Kozen, and L. Stockmeyer, 1981.
Alternation. J. ACM 28(1):114-133.
Chomsky, Noam and Morris Halle, 1968. The
Sound Pattern of English. New York: Harper
&amp; Row.
Halle, Morris. 1985. &amp;quot;Speculations about the rep-
resentation of words in memory.&amp;quot; In Phonetic
Linguistics, Essays in Honor of Peter Lade-
foged, V. Fromkin, ed. Academic Press.
Johnson, C. Douglas. 1972. Formal Aspects of
Phonological Description. The Hague: Mou-
ton.
Kenstowicz, Michael and Charles Kisseberth,
1979. Generative Phonology. New York:
</reference>
<bodyText confidence="0.745576">
fore stress is assigned, and then recomputed on the ba-
sis of the resulting stress assignment. A more natural
approach would be to let stress and syllable structure
computations intermingle in a nondirectional process.
</bodyText>
<page confidence="0.98975">
241
</page>
<reference confidence="0.995692636363636">
Academic Press.
McCarthy, John. 1981. &amp;quot;A prosodic theory of
nonconcatenative morphology.&amp;quot; Linguistic
Inquiry 12, 3T3-418.
Minsky, Marvin. 1969. Computation: finite and
infinite machines, Englewood Cliffs: Prentice
Hall.
Ristad, Eric S. 1990. Computational structure of
human language. Ph.D dissertation, MIT De-
partment of Electrical Engineering and Com-
puter Science.
</reference>
<page confidence="0.997718">
242
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.833145">
<title confidence="0.8819375">Computational structure of generative phonology and its relation to language comprehension.</title>
<author confidence="0.999256">Eric Sven Ristad</author>
<affiliation confidence="0.999705">MIT Artificial Intelligence Lab</affiliation>
<address confidence="0.9980585">545 Technology Square Cambridge, MA 02139</address>
<abstract confidence="0.999433666666667">We analyze the computational complexity of phonological models as they have developed over the past twenty years. The major results are that generation and recognition are nndecidable for segmental models, and that recognition is NPhard for that portion of segmental phonology subsumed by modern autosegmental models. Formal restrictions are evaluated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Chandra</author>
<author>D Kozen</author>
<author>L Stockmeyer</author>
</authors>
<date>1981</date>
<journal>Alternation. J. ACM</journal>
<pages>28--1</pages>
<marker>Chandra, Kozen, Stockmeyer, 1981</marker>
<rawString>Chandra, A., D. Kozen, and L. Stockmeyer, 1981. Alternation. J. ACM 28(1):114-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern of English.</title>
<date>1968</date>
<location>New York: Harper &amp; Row.</location>
<contexts>
<context position="4855" citStr="Chomsky and Halle (1968)" startWordPosition="716" endWordPosition="720"> dictionary of morphemes. Linguistic relations are represented both in the structural descriptions and the derivational process. The structural descriptions of phonology are representations of perceivable distinctions between linguistic sounds, such as stress levels, syllable structure, tone, and articulatory gestures. The underlying and surface forms are both drawn from the same class of structural descriptions, which consist of both segmental strings and autosegmental relations. A segmental string is a string of segments with some representation of constituent structur. In the SPE theory of Chomsky and Halle (1968) concrete boundary symbols are used; in Lexical Phonology, abstract brackets are used. Each segment is a set of phonological features, which are abstract as compared with phonetic representations, although both are given in terms of phonetic features. Suprasegmental relations are relations among segments, rather than properties of individual segments. For example, a syllable is a hierarchical relation between a sequence of segments (the nucleus of the syllable) and the less sonorous segments that immediately preceed and follow it (the onset and coda, respectively). Syllables must satisfy certa</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Chomsky, Noam and Morris Halle, 1968. The Sound Pattern of English. New York: Harper &amp; Row.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morris Halle</author>
</authors>
<title>Speculations about the representation of words in memory.&amp;quot;</title>
<date>1985</date>
<booktitle>In Phonetic Linguistics, Essays in Honor of</booktitle>
<editor>Peter Ladefoged, V. Fromkin, ed.</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="24259" citStr="Halle 1985" startWordPosition="3930" endWordPosition="3931">al Phonology In the past decade, generative phonology has seen a revolution in the linguistic treatment of suprasegmental phenomena such as tone, harmony, infixation, and stress assignment. Although these autosegmental models have yet to be formalised, they may be briefly described as follows. Rather than one-dimensional strings of segments, representations may be thought of as &amp;quot;a threedimensional object that for concreteness one might picture as a spiral-bound notebook,&amp;quot; whose spine is the segmental string and whose pages contain simple constituent structures that are indendent of the spine (Halle 1985). One page represents the sequence of tones associated with a given articulation. By decoupling the representation of tonal sequences from the articulation sequence, it is possible for segmental sequences of different lengths to nonetheless be associated to the same tone sequence. For example, the tonal sequence LowHigh-High, which is used by English speakers to express surprise when answering a question, might be associated to a word containing any number of syllables, from two (Brazil) to twelve (floccinauccinihilipiNication) and beyond. Other pages (called &amp;quot;planes&amp;quot;) represent morphemes, syl</context>
</contexts>
<marker>Halle, 1985</marker>
<rawString>Halle, Morris. 1985. &amp;quot;Speculations about the representation of words in memory.&amp;quot; In Phonetic Linguistics, Essays in Honor of Peter Ladefoged, V. Fromkin, ed. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Douglas Johnson</author>
</authors>
<title>Formal Aspects of Phonological Description.</title>
<date>1972</date>
<publisher>The Hague: Mouton.</publisher>
<marker>Johnson, 1972</marker>
<rawString>Johnson, C. Douglas. 1972. Formal Aspects of Phonological Description. The Hague: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kenstowicz</author>
<author>Charles Kisseberth</author>
</authors>
<title>Generative Phonology.</title>
<date>1979</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="11613" citStr="Kenstowicz and Kisseberth (1979)" startWordPosition="1828" endWordPosition="1831">phonological theories must include an explicit measure of the naturalness of a phonological process. A phonological theory must also define a criterion to decide what constitutes two independent phonological processes and what constitutes a legitimate phonological generalization. Two central hypotheses of segmental phonology are (i) that the most natural grammaas contain the fewest symbols and (ii) a set of rules represent independent phonological processes when they cannot be combined into a single rule schema according to the intricate notational system first described in SPE. (Chapter 9 of Kenstowicz and Kisseberth (1979) contains a less technical summary of the SPE system and a discussion of subsequent modifications and emendations to it.) 3.1 Complexity of segmental recognition and generation. Let us say a dictionary D is a finite set of the underlying phonological forms (matrices) of morphemes. These morphemes may be combined by concatenation and simple substitution (a syntactic category is replaced by a morpheme of that category) to form a possibly infinite set of underlying forms. Then we may characterize the two central computations of phonology as follows. tial space. The phonological generation problem</context>
</contexts>
<marker>Kenstowicz, Kisseberth, 1979</marker>
<rawString>Kenstowicz, Michael and Charles Kisseberth, 1979. Generative Phonology. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John McCarthy</author>
</authors>
<title>A prosodic theory of nonconcatenative morphology.&amp;quot;</title>
<date>1981</date>
<journal>Linguistic Inquiry</journal>
<volume>12</volume>
<pages>3--3</pages>
<contexts>
<context position="23288" citStr="McCarthy (1981" startWordPosition="3772" endWordPosition="3773">er limits seriously, then the class of reductions described in lemma 3.2 would no longer be allowed. (It will be possible to simulate any NI&apos; computation in a single cycle, however.) Rule format. Rules that delete, change, exchange, or insert segments—as well as rules that manipulate boundaries—are crucial to phonological theorizing, and therefore cannot be crudely constrained. More subtle and indirect restrictions are needed. One approach is to formulate language-universal constraints on phonological representations, and to allow a segment to be altered only when it violates some constraint. McCarthy (1981:405) proposes a morpheme rule constraint (MRC) that requires all morphological rules to be of the form A BIX where A is a unit or 0, and B and X are (possibly null) strings of units. (X is the immediate context of A, to the right or left.) It should be obvious that the MRC does not constrain the computational complexity of segmental phonology. 4 Autosegmental Phonology In the past decade, generative phonology has seen a revolution in the linguistic treatment of suprasegmental phenomena such as tone, harmony, infixation, and stress assignment. Although these autosegmental models have yet to be</context>
</contexts>
<marker>McCarthy, 1981</marker>
<rawString>McCarthy, John. 1981. &amp;quot;A prosodic theory of nonconcatenative morphology.&amp;quot; Linguistic Inquiry 12, 3T3-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marvin Minsky</author>
</authors>
<title>Computation: finite and infinite machines,</title>
<date>1969</date>
<publisher>Prentice Hall.</publisher>
<location>Englewood Cliffs:</location>
<contexts>
<context position="15691" citStr="Minsky (1969)" startWordPosition="2529" endWordPosition="2530">to the recognition problem consists of a surface form y that represents the halted accepting state of the TM, a trivial dictionary capable of generating E*, and a segmental grammar g that simulates the computations of the TM according to lemma 3.1. The output is an underlying form x that represents the input that M accepts. The only trick is to construct a (trivial) dictionary capable of generating all possible underlying forms E*. An important corollary to lemma 3.1 is that we can encode a universal Turing machine in a segmental grammar. If we use the four-symbol sevenstate &amp;quot;smallest UTM&amp;quot; of Minsky (1969), then the resulting segmental model contains no more than three features, eight specifications, and 36 very simple rules (exact details in Ristad, 1990). As mentioned above, a central component of the segmental theory is an evaluation metric that favors simpler (ie., shorter) grammars. This segmental grammar of universal computation appears to contain significantly fewer symbols than a segmental grammar for any natural language. Therefore, this corollary presents severe conceptual and empirical problems for the segmental theory. Let us now turn to consider the range of plausible restrictions </context>
</contexts>
<marker>Minsky, 1969</marker>
<rawString>Minsky, Marvin. 1969. Computation: finite and infinite machines, Englewood Cliffs: Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric S Ristad</author>
</authors>
<title>Computational structure of human language.</title>
<date>1990</date>
<institution>MIT Department of Electrical Engineering and Computer Science.</institution>
<note>Ph.D dissertation,</note>
<contexts>
<context position="14441" citStr="Ristad (1990" startWordPosition="2305" endWordPosition="2306">r, they are unrestricted rewriting rules because they can perform deletions as well as insertions. (This is essential to the reduction, because it allows the derivation string to become arbitarily long.) The second observation is that segmental rules can freely manipulate (insert and delete) boundary symbols, and thus it is possible to prolong the derivation indefinitely: we need only employ a rule Rn.4 at the end of the cycle that adds an extra boundary symbol to each end of the derivation string, unless the simulated machine has halted. The remaining details are omitted, but may be found in Ristad (1990. fl The immediate consequences are: Theorem 1 POP is undecidabk. Proof. By reduction to the unclecidable problem to e L(M)? of deciding whether a given TM M accepts an input to. The input to the generation problem consists of an underlying form z that represents to and a segmental grammar 237 g that simulates the computations of M according to lemma 3.1. The output is a surface form y = g(x) that represents the halted configuration of the TM, with all but the accepting unit erased. Theorem 2 PRP is undecidable. Proof. By reduction to the undecidable problem L(M) =?O of deciding whether a give</context>
<context position="15844" citStr="Ristad, 1990" startWordPosition="2552" endWordPosition="2553">E*, and a segmental grammar g that simulates the computations of the TM according to lemma 3.1. The output is an underlying form x that represents the input that M accepts. The only trick is to construct a (trivial) dictionary capable of generating all possible underlying forms E*. An important corollary to lemma 3.1 is that we can encode a universal Turing machine in a segmental grammar. If we use the four-symbol sevenstate &amp;quot;smallest UTM&amp;quot; of Minsky (1969), then the resulting segmental model contains no more than three features, eight specifications, and 36 very simple rules (exact details in Ristad, 1990). As mentioned above, a central component of the segmental theory is an evaluation metric that favors simpler (ie., shorter) grammars. This segmental grammar of universal computation appears to contain significantly fewer symbols than a segmental grammar for any natural language. Therefore, this corollary presents severe conceptual and empirical problems for the segmental theory. Let us now turn to consider the range of plausible restrictions on the segmental model. At first glance, it may seem that the single most important computational restriction is to prevent rules from inserting boundari</context>
<context position="30130" citStr="Ristad, 1990" startWordPosition="4865" endWordPosition="4866">has sonority 9 when literal bi is true, 5 otherwise; and segment zc has sonority 8 when literal ci is true, 2 otherwise. Of the eight possible truth values of the three literals and the corresponding syllabifications, only the syllabification corresponding to three false literals is excluded by the SSC. In that case, the corresponding string of four consonants C-C-C-C has the sonority sequence 1-2-5-2. No immediately preceeding or following segment of any sonority can result in a syllabification that obeys the SSC. Therefore, all Boolean clauses must contain a true literal. (Complete proof in Ristad, 1990) f=1 The direct consequence of this lemma 4.1 is: Theorem 5 PRP for the autosegmental model is NP-hard. Proof. By reduction to 3SAT. The idea is to construct a surface form that completely identi240 fies the variables and their negation or lack of it, but does not specify the truth values of those variables. The dictionary will generate all possible underlying forms (infixed morphemes or harmonic strings), one for each possible truth assignment, and the autosegmental representation of lemma 4.1 will ensure that generated formulas are in fact satisfiable. 0 5 Conclusion. In my opinion, the prec</context>
</contexts>
<marker>Ristad, 1990</marker>
<rawString>Ristad, Eric S. 1990. Computational structure of human language. Ph.D dissertation, MIT Department of Electrical Engineering and Computer Science.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>