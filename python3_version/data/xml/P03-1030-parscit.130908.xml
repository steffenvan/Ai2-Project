<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.992808">
Optimizing Story Link Detection is not Equivalent to
Optimizing New Event Detection
</title>
<author confidence="0.90102">
Ayman Farahat
</author>
<affiliation confidence="0.807258">
PARC
</affiliation>
<address confidence="0.921864">
3333 Coyote Hill Rd
Palo Alto, CA 94304
</address>
<email confidence="0.997491">
farahat@parc.com
</email>
<author confidence="0.744293">
Francine Chen
</author>
<affiliation confidence="0.669851">
PARC
</affiliation>
<address confidence="0.9196085">
3333 Coyote Hill Rd
Palo Alto, CA 94304
</address>
<email confidence="0.998203">
fchen@parc.com
</email>
<author confidence="0.954613">
Thorsten Brants
</author>
<affiliation confidence="0.850877">
PARC
</affiliation>
<address confidence="0.9291815">
3333 Coyote Hill Rd
Palo Alto, CA 94304
</address>
<email confidence="0.998548">
thorsten@brants.net
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945384615385">
Link detection has been regarded as a core
technology for the Topic Detection and
Tracking tasks of new event detection. In
this paper we formulate story link detec-
tion and new event detection as informa-
tion retrieval task and hypothesize on the
impact of precision and recall on both sys-
tems. Motivated by these arguments, we
introduce a number of new performance
enhancing techniques including part of
speech tagging, new similarity measures
and expanded stop lists. Experimental re-
sults validate our hypothesis.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999809044444444">
Topic Detection and Tracking (TDT) research is
sponsored by the DARPA Translingual Information
Detection, Extraction, and Summarization (TIDES)
program. The research has five tasks related to
organizing streams of data such as newswire and
broadcast news (Wayne, 2000): story segmentation,
topic tracking, topic detection, new event detection
(NED), and link detection (LNK). A link detection
system detects whether two stories are “linked”, or
discuss the same event. A story about a plane crash
and another story about the funeral of the crash vic-
tims are considered to be linked. In contrast, a story
about hurricane Andrew and a story about hurricane
Agnes are not linked because they are two different
events. A new event detection system detects when
a story discusses a previously unseen or “not linked”
event. Link detection is considered to be a core tech-
nology for new event detection and the other tasks.
Several groups are performing research in the
TDT tasks of link detection and new event detection.
Based on their findings, we incorporated a number
of their ideas into our baseline system. CMU (Yang
et al., 1998) and UMass (Allan et al., 2000a) found
that for new event detection it was better to com-
pare a new story against all previously seen stories
than to cluster previously seen stories and compare
a new story against the clusters. CMU (Carbonell
et al., 2001) found that NED results could be im-
proved by developing separate models for different
news sources to that could capture idiosyncrasies of
different sources, which we also extended to link de-
tection. UMass reported on adapting a tracking sys-
tem for NED detection (Allan et al., 2000b). Allan
et. al , (Allan et al., 2000b) developed a NED system
based upon a tracking technology and showed that
to achieve high-quality first story detection, tracking
effectiveness must improve to a degree that experi-
ence suggests is unlikely. In this paper, while we
reach a similar conclusion as (Allan et al., 2000b) for
LNK and NED systems , we give specific directions
for improving each system separately. We compare
the link detection and new event detection tasks and
discuss ways in which we have observed that tech-
niques developed for one task do not always perform
similarly for the other task.
</bodyText>
<sectionHeader confidence="0.899799" genericHeader="method">
2 Common Processing and Models
</sectionHeader>
<bodyText confidence="0.999649">
This section describes those parts of the process-
ing steps and the models that are the same for New
Event Detection and for Link Detection.
</bodyText>
<subsectionHeader confidence="0.999187">
2.1 Pre-Processing
</subsectionHeader>
<bodyText confidence="0.999992">
For pre-processing, we tokenize the data, recog-
nize abbreviations, normalize abbreviations, remove
stop-words, replace spelled-out numbers by digits,
add part-of-speech tags, replace the tokens by their
stems, and then generate term-frequency vectors.
</bodyText>
<subsectionHeader confidence="0.982269">
2.2 Incremental TF-IDF Model
</subsectionHeader>
<bodyText confidence="0.99961055">
Our similarity calculations of documents are based
on an incremental TF-IDF model. In a TF-IDF
model, the frequency of a term in a document (TF) is
weighted by the inverse document frequency (IDF).
In the incremental model, document frequencies
are not static but change in time steps. At
time, a new set of test documents is added to
the model by updating the frequencies
where denote the document frequencies in the
newly added set of documents . The initial docu-
ment frequencies are generated from a (pos-
sibly emtpy) training set. In a static TF-IDF model,
new words (i.e., those words, that did not occur in
the training set) are ignored in further computations.
An incremental TF-IDF model uses the new vocab-
ulary in similarity calculations. This is an advantage
because new events often contain new vocabulary.
Very low frequency terms tend to be uninfor-
mative. We therefore set a threshold . Only terms
with are used at time. We use .
</bodyText>
<subsectionHeader confidence="0.999331">
2.3 Term Weighting
</subsectionHeader>
<bodyText confidence="0.99857875">
The document frequencies as described in the pre-
vious section are used to calculate weights for the
terms in the documents. At time, we use
where is the total number of documents at time
. is a normalization value such that either
the weights sum to 1 (if we use Hellinger distance,
KL-divergence, or Clarity-based distance), or their
squares sum to 1 (if we use cosine distance).
</bodyText>
<subsectionHeader confidence="0.99101">
2.4 Similarity Calculation
</subsectionHeader>
<bodyText confidence="0.999622227272727">
The vectors consisting of normalized term weights
are used to calculate the similarity between
two documents and. In our current implementa-
tion, we use the the Clarity metric which was intro-
duced by (Croft et al., 2001; Lavrenko et al., 2002)
and gets its name from the distance to general En-
glish, which is called Clarity. We used a symmetric
version that is computed as:
where “ ” is the Kullback-Leibler divergence,
is the probability distribution of words for “gen-
eral English” as derived from the training corpus.
The idea behind this metric is that we want to give
credit to similar pairs of documents that are very
different from general English, and we want to dis-
count similar pairs of documents that are close to
general English (which can be interpreted as being
the noise). The motivation for using the clarity met-
ric will given in section 6.1.
Another metric is Hellinger distance
Other possible similarity metrics are the cosine dis-
tance, the Kullback-Leibler divergence, or the sym-
metric form of it, Jensen-Shannon distance.
</bodyText>
<subsectionHeader confidence="0.91801">
2.5 Source-Specific TF-IDF Model
</subsectionHeader>
<bodyText confidence="0.999849851851852">
Documents in the stream of news stories may stem
from different sources, e.g., there are 20 different
sources in the data for TDT 2002 (ABC News, As-
sociated Press, New York Times, etc). Each source
might use the vocabulary differently. For example,
the names of the sources, names of shows, or names
of news anchors are much more frequent in their
own source than in the other ones. In order to re-
flect the source-specific differences we do not build
one incremental TF-IDF model, but as many as we
have different sources and use frequencies
for source at time. The frequencies are updated
according to equation (1), but only using those doc-
uments in that are from the same source. As
a consequence, a term like “CNN” receives a high
document frequency (thus low weight) in the model
for the source CNN and a low document frequency
(thus high weight) in the New York Times model.
Instead of the overall document frequencies
, we now use the source specific
when calculating the term weights in equation (2).
Sources for which no training data is available
(i.e., no data to generate is available) might
be initialized in two different ways:
using labeled training data and calculating similar-
ity scores for document pairs on the same event and
on different events.
</bodyText>
<sectionHeader confidence="0.958473" genericHeader="method">
4 Link Detection
</sectionHeader>
<bodyText confidence="0.9997075">
In order to decide whether a pair of stories and
are linked, we identify a set of similarity metrics
that capture the similarity between the two docu-
ments using Clarity and Hellinger metrics:
</bodyText>
<figure confidence="0.879261666666667">
1. Use an empty model: for all ;
(9)
(7)
</figure>
<subsectionHeader confidence="0.9315">
2.6 Source-Pair-Specific Normalization
</subsectionHeader>
<bodyText confidence="0.9999642">
Due to stylistic differences between various sources,
e.g., news paper vs. broadcast news, translation er-
rors, and automatic speech recognition errors (Allan
et al., 1999), the similarity measures for both on-
topic and off-topic pairs will in general depend on
the source pair. Errors due to these differences can
be reduced by using thresholds conditioned on the
sources (Carbonell et al., 2001), or, as we do, by
normalizing the similarity values based on similari-
ties for the source pairs found in the story history.
</bodyText>
<sectionHeader confidence="0.989995" genericHeader="method">
3 New Event Detection
</sectionHeader>
<bodyText confidence="0.967842954545455">
In order to decide whether a new document that
is added to the collection at time describes a new
event, it is individually compared to all previous
documents using the steps described in section 2.
We identify the document with highest similarity:
(8)
The value is used to de-
termine whether a document is about a new event
and at the same time is an indication of the confi-
dence in our decision. If the score exceeds a thresh-
old, then there is no sufficiently similar previous
document, thus describes a new event (decision
YES). If the score is smaller than, then is suf-
ficiently similar, thus describes an old event (de-
cision NO). The threshold can be determined by
The value is used to determine whether sto-
ries “q” and “d” are linked. If the similarity exceeds
a threshold we the two stories are sufficiently
similar (decision YES). If the similarity is smaller
than we the two stories are sufficiently differ-
ent (decision NO). The Threshold can be deter-
mined using labeled training data.
</bodyText>
<sectionHeader confidence="0.972144" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.74654">
All TDT systems are evaluated by calculating a De-
tection Cost:
</bodyText>
<equation confidence="0.736488">
(10)
</equation>
<bodyText confidence="0.9614363">
where and are the costs of a miss and
a false alarm. They are set to 1 and 0.1, respec-
tively, for all tasks. and are the condi-
tional probabilities of a miss and a false alarm in the
system output. and a the a priori
target and non-target probabilities. They are set to
0.02 and 0.98 for LNK and NED. The detection cost
is normalized such that a perfect system scores 0,
and a random baseline scores 1:
TDT evaluates all systems with a topic-weighted
method: error probabilities are accumulated sepa-
rately for each topic and then averaged. This is mo-
tivated by the different sizes of the topics.
The evaluation yields two costs: the detection cost
is the cost when using the actual decisions made by
the system; the minimum detection cost is the cost
when using the confidence scores that each system
min
2. Identify one or more other but similar sources
for which training data is available and use
</bodyText>
<equation confidence="0.580224">
(11)
</equation>
<bodyText confidence="0.977110125">
has to emit with each decision and selecting the op-
timal threshold based on the score.
In the TDT-2002 evaluation, our Link Detec-
tion system was the best of three systems, yield-
ing and
. Our New Event Detection system was
ranked second of four with costs of
and .
</bodyText>
<figure confidence="0.9860586875">
6 Differences between LNK and NED
LNK − Clarity — Hellinger
1
0.9
0.8
0.7
0.6
CDF
0.5
0.4
0.3
0.2
Clarity on−topic
Clarity off−topic
Hellinger on−topic
Hellinger off−topic
</figure>
<bodyText confidence="0.998914">
In this section, we draw on Information retrieval
tools to analyze LNK and NED tasks. Motivated by
the results of this analysis, we compare a number of
techniques in the LNK and NED tasks in particular
we compare the utility of two similarity measures,
part-of-speech tagging, stop wording, and normal-
izing abbreviations and numerals. The comparisons
were performed on corpora developed for TDT, in-
cluding TDT2 and TDT3.
</bodyText>
<subsectionHeader confidence="0.998371">
6.1 Information Retrieval and TDT
</subsectionHeader>
<bodyText confidence="0.999561192307692">
The conditions for false alarms and misses are re-
versed for LNK and NED tasks. In the LNK task,
incorrectly flagging two stories as being on the same
event is considered a false alarm. In contrast in the
NED task, incorrectly flagging two stories as being
on the same event will cause the true first story to
be missed. Conversely, in LNK incorrectly labeling
two stories that are on the same event as not linked is
a miss, but in the NED task, incorrectly labeling two
stories on the same event as not linked can result in
a false alarm where a story is incorrectly identified
as a new event.
The detection cost in Eqn.10 which assigns a
higher cost to false alarm
. A LNK system
wants to minimize false alarms and to do this it
should identify stories as being linked only if they
are linked, which translates to high precision. In
contrast a NED system, will minimize false alarms
by identifying all stories that are linked which trans-
lates to high recall. Motivated by this discussion, we
investigated the use of number of precision and re-
call enhancing techniques with the LNK and NED
system. We investigated the use of the Clarity met-
ric (Lavrenko et al., 2002) which was shown to cor-
relate positively with precision. We investigated the
</bodyText>
<figure confidence="0.973527666666667">
0.1
00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Similarity
</figure>
<figureCaption confidence="0.9256475">
Figure 1: CDF for Clarity and Hellinger similarity
on the LNK task for on-topic and off-topic pairs.
</figureCaption>
<bodyText confidence="0.999977466666667">
use of part-of-speech tagging which was shown by
Allan and Raghavan (Allan and Raghavan, 2002)
to improve query clarity. In section 6.2.1 we will
show how POS helps recall. We also investigated the
use of expanded stop-list which improves precision.
We also investigated normalizing abbreviations and
transforming spelled out numbers into numbers. On
the one hand the enhanced processing list includes
most of the term in the ASR stop-list and remov-
ing these terms will improve precision. On the other
hand normalizing these terms will have the same ef-
fect as stemming a recall enhancing device (Xu and
Croft, 1998) , (Kraaij and Pohlmann, 1996). In ad-
dition to these techniques, we also investigated the
use of different similarity measures.
</bodyText>
<subsectionHeader confidence="0.998301">
6.2 Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999962142857143">
The systems developed for TDT primarily use co-
sine similarity as the similarity measure. We have
developed systems based on cosine similarity (Chen
et al., 2003). In work on text segmentation, (Brants
et al., 2002) observed that the system performance
was much better when the Hellinger measure was
used instead. In this work, we decided to use the
clarity metric, a precision enhancing device (Croft et
al., 2001). For both our LNK and NED systems, we
compared the performance of the systems using each
of the similarity measures separately. Table 1 shows
that for LNK, the system based on Clarity similar-
ity performed better the system based on Hellinger
similarity; in contrast, for NED, the system based on
</bodyText>
<figure confidence="0.988868666666667">
1
NED Hellinger vs. Clarity
Similarity
</figure>
<figureCaption confidence="0.695823833333333">
Figure 2: CDF for Clarity and Hellinger similarity
on the NED task for on-topic and off-topic pairs.
Table 1: Effect of different similarity measures
on topic-weighted minimum normalized detection
costs for LNK and NED on the TDT 2002 dry run
data.
</figureCaption>
<bodyText confidence="0.886134423076923">
System Clarity Hellinger % Chg
LNK 0.3054 0.3777 -0.0597 -19.2
NED 0.8419 0.5873 +0.2546 +30.24
Hellinger similarity performed better.
Figure 1 shows the cumulative density function
for the Hellinger and Clarity similarities for on-topic
(about the same event) and off-topic (about different
events) pairs for the LNK task. While there are a
number of statistics to measure the overall difference
between tow cumulative distribution functions, we
used the Kolmogorov-Smirnov distance (K-S dis-
tance; the largest difference between two cumula-
tive distributions) for two reasons. First, the K-S
distance is invariant under re-parametrization. Sec-
ond, the significance of the K-S distance in case of
the null hypothesis (data sets are drawn from same
distribution) can be calculated (Press et al., 1993).
The K-S distance between the on-topic and off-topic
similarities is larger for Clarity similarity (cf. table
2), indicating that it is the better metric for LNK.
Figure 2 shows the cumulative distribution func-
tions for Hellinger and Clarity similarities in the
NED task. The plot is based on pairs that contain the
current story and its most similar story in the story
history. When the most similar story is on the same
event (approx. 75% of the cases), its similarity is part
</bodyText>
<tableCaption confidence="0.9798695">
Table 2: K-S distance between on-topic and off-
topic story pairs.
</tableCaption>
<table confidence="0.994137666666667">
Clarity Hellinger Change (%)
LNK 0.7680 0.7251 ( )
NED 0.5353 0.6055 ( )
</table>
<tableCaption confidence="0.665513333333333">
Table 3: Effect of using part-of-speech on minimum
normalized detection costs for LNK and NED on the
TDT 2002 dry run data.
</tableCaption>
<table confidence="0.986440333333333">
System PoS PoS Change (%)
LNK 0.3054 0.4224 -0.117 ( %)
NED 0.6403 0.5873 +0.0530 ( %)
</table>
<bodyText confidence="0.99882805882353">
of the on-topic distribution, otherwise (approx. 25%
of the cases) it is plotted as off-topic. The K-S dis-
tance between the Hellinger on-topic and off-topic
CDFs is larger than those for Clarity (cf. table 2).
For both NED and LNK, we can reject the null hy-
pothesis for both metrics with over 99.99 % confi-
dence.
To get the high precision required for LNK sys-
tem, we need to have a large separation between the
on-topic and off-topic distributions. Examining Fig-
ure 1 and Table 2 , indicates that the Clarity metric
has a larger separation than the Hellinger metric. At
high recall required by NED system (low CDF val-
ues for on-topic), there is a greater separation with
the Hellinger metric. For example, at 10% recall, the
Hellinger metric has 71 % false alarm rate as com-
pared to 75 % for the Clarity metric.
</bodyText>
<subsectionHeader confidence="0.691398">
6.2.1 Part-of-Speech (PoS) Tagging
</subsectionHeader>
<bodyText confidence="0.999957071428571">
We explored the idea that noting the part-of-
speech of the terms in a document may help to re-
duce confusion among some of the senses of a word.
During pre-processing, we tagged the terms as one
of five categories: adjective, noun, proper nouns,
verb, or other. A “tagged term” was then created
by combining the stem and part-of-speech. For ex-
ample, ‘N train’ represents the term ‘train’ when
used as a noun, and ‘V train’ represents the term
‘train’ when used as a verb. We then ran our NED
and LNK systems using the tagged terms. The sys-
tems were tested in the Dry Run 2002 TDT data.
A comparison of the performance of the systems
when part-of-speech is used against a baseline sys-
</bodyText>
<figure confidence="0.985372588235294">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
CDF
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Hellinger on topic
Hellinger off topic
Clarity on topic
Clarity off topic
</figure>
<tableCaption confidence="0.7726075">
Table 4: Comparison of using an “ASR stop-list”
and “enhanced preprocessing” for handling ASR
</tableCaption>
<table confidence="0.7172175">
differences.
No ASR stop
Std Preproc
0.3054
</table>
<tableCaption confidence="0.875082">
Table 5: Impact of recall and precision enhancing
devices.
</tableCaption>
<table confidence="0.910723272727273">
Device Impact LNK NED
ASR stop precision +3.1% -5.5 %
POS recall -38.8 % 8.3 %
Clarity precision +19 % -30 %
LNK
NED
0.3153
0.6062
ASR stop
Std Preproc
0.6407
</table>
<bodyText confidence="0.999950882352941">
tem when part-of-speech is not used is shown in Ta-
ble 3. For Story Link Detection, performance de-
creases by 38.3%, while for New Event Detection,
performance improves by 8.3%. Since POS tagging
helps differentiates between the different senses of
the same root, it also reduces the number of match-
ing terms between two documents. In the LNK task
for example, the total number of matches drops from
177,550 to 151,132. This has the effect of placing a
higher weight on terms that match, i.e. terms that
have the same sense and for the TDT corpus will
increase recall and decrease. Consider for example
matching “food server to “food service” and “java
server”. When using POS both terms will have the
same similarity to the query and the use of POS will
retrieve the relevant documents but will also retrieve
other documents that share the same sense.
</bodyText>
<subsectionHeader confidence="0.808227">
6.2.2 Stop Words
</subsectionHeader>
<bodyText confidence="0.999940805555556">
A large portion of the documents in the TDT col-
lection has been automatically transcribed using Au-
tomatic Speech Recognition (ASR) systems which
can achieve over 95% accuracies. However, some
of the words not recognized by the ASR tend to be
very informative words that can significantly impact
the detection performance (Allan et al., 1999). Fur-
thermore, there are systematic differences between
ASR and manually transcribed text, e.g., numbers
are often spelled out thus “30” will be spelled out
“thirty”. Another situation where ASR is different
from transcribed text is abbreviations, e.g. ASR sys-
tem will recognize ‘CNN” as three separate tokens
“C”, “N”, and “N”.
In order to account for these differences, we iden-
tified the set of tokens that are problematic for ASR.
Our approach was to identify a parallel corpus of
manually and automatically transcribed documents,
the TDT2 corpus, and then use a statistical approach
(Dunning, 1993) to identify tokens with significantly
different distributions in the two corpora. We com-
piled the problematic ASR terms into an “ASR stop-
list”. This list was primarily composed of spelled-
out numbers, numerals and a few other terms. Ta-
ble 4 shows the topic-weighted minimum detection
costs for LNK and NED on the TDT 2002 dry run
data. The table shows results for standard pre-
processing without an ASR stop-list and with and
ASR stop-list. For Link Detection, the ASR stop-
list improves results, while the same list decreases
performance for New Event Detection.
In (Chen et al., 2003) we investigated normalizing
abbreviations and transforming spelled-out numbers
into numerals, “enhanced preprocessing”, and then
compared this approach with using an “ASR stop-
list”.
</bodyText>
<subsectionHeader confidence="0.96809">
6.2.3 Impact of Recall and Precision
</subsectionHeader>
<bodyText confidence="0.9999451">
The previous two sections examined the impact
of four different techniques on the performance of
LNK and NED systems. The Part-of-speech is a re-
call enhancing devices while the ASR stop-list is a
precision enhancing device. The enhanced prepro-
cessing improves precision and recall. The results
which are summarized in Table 5 indicate that pre-
cision enhancing devices improved the performance
of the LNK task while recall enhancing devices im-
proved the NED task.
</bodyText>
<subsectionHeader confidence="0.992614">
6.3 Final Remarks on Differences
</subsectionHeader>
<bodyText confidence="0.999484222222222">
In the extreme case, a perfect link detection system
performs perfectly on the NED task. We gave em-
pirical evidence that there is not necessarily such a
correlation at lower accuracies. These findings are in
accordance with the results reported in (Allan et al.,
2000b) for topic tracking and first story detection.
To test the impact of the cost function on the per-
formance of LNK and NED systems, we repeated
the evaluation with and both set to 1,
and we found that the difference between the two re-
Table 6: Topic-weighted minimum normalized de-
tection cost for NED when using parameter settings
that are best for NED (1) and those that are best
for LNK (2). Columns (3) and (4) show the detec-
tion costs using uniform costs for misses and false
alarms.
sults decreases from 30.24% to 14.73%. The result
indicates that the setting (Hel, PoS, ASRstop)
is better at recall (identifying same-event stories),
while (Clarity, PoS, ASRstop) is better at pre-
cision (identifying different-event stories).
In addition to the different costs assigned to
misses and false alarms, there is a difference in the
number of positives and negatives in the data set (the
TDT cost function uses ). This might
explain part of the remaining difference of 14.73%.
Another view on the differences is that a NED
system must perform very well on the higher penal-
ized first stories when it does not have any training
data for the new event, event though it may perform
worse on follow-up stories. A LNK system, how-
ever, can afford to perform worse on the first story if
it compensates by performing well on follow-up sto-
ries (because here not flagged follow-up stories are
considered misses and thus higher penalized than in
NED). This view explains the benefits of using part-
of-speech information and the negative effect of the
ASR stop-list on NED : different part-of-speech tags
help discriminate new events from old events; re-
moving words by using the ASR stoplist makes it
harder to discriminate new events. We conjecture
that the Hellinger metric helps improve recall, and
in a study similar to (Allan et al., 2000b) we plan to
further evaluate the impact of the Hellinger metric
on a closed collection e.g. TREC.
</bodyText>
<sectionHeader confidence="0.991295" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999985047619047">
We have compared the effect of several techniques
on the performance of a story link detection system
and a new event detection system. Although many
of the processing techniques used by our systems are
the same, a number of core technologies affect the
performance of the LNK and NED systems differ-
ently. The Clarity similarity measure was more ef-
fective for LNK, Hellinger similarity measure was
more effective for NED, part-of-speech was more
useful for NED, and stop-list adjustment was more
useful for LNK. These differences may be due in
part to a reversal in the tasks: a miss in LNK means
the system does not flag two stories as being on the
same event when they actually are, while a miss in
NED means the system does flag two stories as be-
ing on the same event when actually they are not.
In future work, we plan to evaluate the impact of
the Hellinger metric on recall. In addition, we plan
to use Anaphora resolution which was shown to im-
prove recall (Pirkola and Jrvelin, 1996) to enhance
the NED system.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999371454545454">
James Allan and Hema Raghavan. 2002. Using part-of-
speech patterns to reduce query ambiguity. In ACM
SIGIR2002, Tampere, Finland.
James Allan, Hubert Jin, Martin Rajman, Charles Wayne,
and et. al. 1999. Topic-based novelty detection. Sum-
mer workshop final report, Center for Language and
Speech Processing, Johns Hopkins University.
J. Allan, V. Lavrenko, D. Malin, and R. Swan. 2000a.
Detections, bounds, and timelines: Umass and tdt-3.
In Proceedings of Topic Detection and Tracking Work-
shop (TDT-3), Vienna, VA.
James Allan, Victor Lavrenko, and Hubert Jin. 2000b.
First story detection in TDT is hard. In CIKM, pages
374–381.
Thorsten Brants, Francine Chen, and Ioannis Tsochan-
taridis. 2002. Topic-based document segmentation
with probabilistic latent semantic analysis. In Inter-
national Conference on Information and Knowledge
Management (CIKM), McLean, VA.
Jaime Carbonell, Yiming Yang, Ralf Brown, Chun Jin,
and Jian Zhang. 2001. Cmu tdt report. Slides at the
TDT-2001 meeting, CMU.
</reference>
<figure confidence="0.585606428571429">
(1) (2) (3) (4)
Metric Hel Cla Hel Cla
POS
ASR stop
0.1 0.1 1 1
0.5873 0.8419 0.8268 0.9498
% change – +30.24% – +14.73%
</figure>
<reference confidence="0.998506243243243">
Francine Chen, Ayman Farahat, and Thorsten Brants.
2003. Story link detection and new event detection
are asymmetric. In Proceedings ofNAACL-HLT-2002,
Edmonton, AL.
W. Bruce Croft, Stephen Cronen-Townsend, and Victor
Larvrenko. 2001. Relevance feedback and person-
alization: A language modeling perspective. In DE-
LOS Workshop: Personalisation and Recommender
Systems in Digital Libraries.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61–74.
Wessel Kraaij and Renee Pohlmann. 1996. Viewing
stemming as recall enhancement. InACMSIGIR1996.
Victor Lavrenko, James Allan, Edward DeGuzman,
Daniel LaFlamme, Veera Pollard, and Stephen
Thomas. 2002. Relevance models for topic detection
and tracking. In Proceedings ofHLT-2002, San Diego,
CA.
A. Pirkola and K. Jrvelin. 1996. The effect of anaphora
and ellipsis resolution on proximity searching in a text
database. Information Processing and Management,
32(2):199–216.
William H. Press, Saul A. Teukolsky, William Vetterling,
and Brian Flannery. 1993. Numerical Recipes. Cam-
bridge Unv. Press.
Charles Wayne. 2000. Multilingual topic detection
and tracking: Successful research enabled by corpora
and evaluation. In Language Resources and Evalu-
ation Conference (LREC), pages 1487–1494, Athens,
Greece.
Jinxi Xu and W. Bruce Croft. 1998. Corpus-based
stemming using cooccurrence of word variants. ACM
Transactions on Information Systems, 16(1):61–81.
Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998.
A study on retrospective and on-line event detection.
In Proceedings of SIGIR-98, Melbourne, Australia.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.840853">
<title confidence="0.999943">Optimizing Story Link Detection is not Equivalent to Optimizing New Event Detection</title>
<author confidence="0.949087">Ayman Farahat</author>
<affiliation confidence="0.952646">PARC</affiliation>
<address confidence="0.996199">3333 Coyote Hill Rd Palo Alto, CA 94304</address>
<email confidence="0.999725">farahat@parc.com</email>
<author confidence="0.999117">Francine Chen</author>
<affiliation confidence="0.987296">PARC</affiliation>
<address confidence="0.9973275">3333 Coyote Hill Rd Palo Alto, CA 94304</address>
<email confidence="0.999782">fchen@parc.com</email>
<author confidence="0.998998">Thorsten Brants</author>
<affiliation confidence="0.962763">PARC</affiliation>
<address confidence="0.997184">3333 Coyote Hill Rd Palo Alto, CA 94304</address>
<email confidence="0.982511">thorsten@brants.net</email>
<abstract confidence="0.999201285714286">Link detection has been regarded as a core technology for the Topic Detection and Tracking tasks of new event detection. In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of precision and recall on both systems. Motivated by these arguments, we introduce a number of new performance enhancing techniques including part of speech tagging, new similarity measures and expanded stop lists. Experimental results validate our hypothesis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Hema Raghavan</author>
</authors>
<title>Using part-ofspeech patterns to reduce query ambiguity.</title>
<date>2002</date>
<booktitle>In ACM SIGIR2002,</booktitle>
<location>Tampere, Finland.</location>
<contexts>
<context position="12499" citStr="Allan and Raghavan, 2002" startWordPosition="2115" endWordPosition="2118">ze false alarms by identifying all stories that are linked which translates to high recall. Motivated by this discussion, we investigated the use of number of precision and recall enhancing techniques with the LNK and NED system. We investigated the use of the Clarity metric (Lavrenko et al., 2002) which was shown to correlate positively with precision. We investigated the 0.1 00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Similarity Figure 1: CDF for Clarity and Hellinger similarity on the LNK task for on-topic and off-topic pairs. use of part-of-speech tagging which was shown by Allan and Raghavan (Allan and Raghavan, 2002) to improve query clarity. In section 6.2.1 we will show how POS helps recall. We also investigated the use of expanded stop-list which improves precision. We also investigated normalizing abbreviations and transforming spelled out numbers into numbers. On the one hand the enhanced processing list includes most of the term in the ASR stop-list and removing these terms will improve precision. On the other hand normalizing these terms will have the same effect as stemming a recall enhancing device (Xu and Croft, 1998) , (Kraaij and Pohlmann, 1996). In addition to these techniques, we also invest</context>
</contexts>
<marker>Allan, Raghavan, 2002</marker>
<rawString>James Allan and Hema Raghavan. 2002. Using part-ofspeech patterns to reduce query ambiguity. In ACM SIGIR2002, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Hubert Jin</author>
<author>Martin Rajman</author>
<author>Charles Wayne</author>
</authors>
<title>Topic-based novelty detection. Summer workshop final report,</title>
<date>1999</date>
<institution>Center for Language and Speech Processing, Johns Hopkins University.</institution>
<contexts>
<context position="7766" citStr="Allan et al., 1999" startWordPosition="1280" endWordPosition="1283">initialized in two different ways: using labeled training data and calculating similarity scores for document pairs on the same event and on different events. 4 Link Detection In order to decide whether a pair of stories and are linked, we identify a set of similarity metrics that capture the similarity between the two documents using Clarity and Hellinger metrics: 1. Use an empty model: for all ; (9) (7) 2.6 Source-Pair-Specific Normalization Due to stylistic differences between various sources, e.g., news paper vs. broadcast news, translation errors, and automatic speech recognition errors (Allan et al., 1999), the similarity measures for both ontopic and off-topic pairs will in general depend on the source pair. Errors due to these differences can be reduced by using thresholds conditioned on the sources (Carbonell et al., 2001), or, as we do, by normalizing the similarity values based on similarities for the source pairs found in the story history. 3 New Event Detection In order to decide whether a new document that is added to the collection at time describes a new event, it is individually compared to all previous documents using the steps described in section 2. We identify the document with h</context>
<context position="19050" citStr="Allan et al., 1999" startWordPosition="3230" endWordPosition="3233">or example matching “food server to “food service” and “java server”. When using POS both terms will have the same similarity to the query and the use of POS will retrieve the relevant documents but will also retrieve other documents that share the same sense. 6.2.2 Stop Words A large portion of the documents in the TDT collection has been automatically transcribed using Automatic Speech Recognition (ASR) systems which can achieve over 95% accuracies. However, some of the words not recognized by the ASR tend to be very informative words that can significantly impact the detection performance (Allan et al., 1999). Furthermore, there are systematic differences between ASR and manually transcribed text, e.g., numbers are often spelled out thus “30” will be spelled out “thirty”. Another situation where ASR is different from transcribed text is abbreviations, e.g. ASR system will recognize ‘CNN” as three separate tokens “C”, “N”, and “N”. In order to account for these differences, we identified the set of tokens that are problematic for ASR. Our approach was to identify a parallel corpus of manually and automatically transcribed documents, the TDT2 corpus, and then use a statistical approach (Dunning, 199</context>
</contexts>
<marker>Allan, Jin, Rajman, Wayne, 1999</marker>
<rawString>James Allan, Hubert Jin, Martin Rajman, Charles Wayne, and et. al. 1999. Topic-based novelty detection. Summer workshop final report, Center for Language and Speech Processing, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>V Lavrenko</author>
<author>D Malin</author>
<author>R Swan</author>
</authors>
<title>Detections, bounds, and timelines: Umass and tdt-3.</title>
<date>2000</date>
<booktitle>In Proceedings of Topic Detection and Tracking Workshop (TDT-3),</booktitle>
<location>Vienna, VA.</location>
<contexts>
<context position="2013" citStr="Allan et al., 2000" startWordPosition="317" endWordPosition="320">rash victims are considered to be linked. In contrast, a story about hurricane Andrew and a story about hurricane Agnes are not linked because they are two different events. A new event detection system detects when a story discusses a previously unseen or “not linked” event. Link detection is considered to be a core technology for new event detection and the other tasks. Several groups are performing research in the TDT tasks of link detection and new event detection. Based on their findings, we incorporated a number of their ideas into our baseline system. CMU (Yang et al., 1998) and UMass (Allan et al., 2000a) found that for new event detection it was better to compare a new story against all previously seen stories than to cluster previously seen stories and compare a new story against the clusters. CMU (Carbonell et al., 2001) found that NED results could be improved by developing separate models for different news sources to that could capture idiosyncrasies of different sources, which we also extended to link detection. UMass reported on adapting a tracking system for NED detection (Allan et al., 2000b). Allan et. al , (Allan et al., 2000b) developed a NED system based upon a tracking technol</context>
<context position="21219" citStr="Allan et al., 2000" startWordPosition="3577" endWordPosition="3580">all enhancing devices while the ASR stop-list is a precision enhancing device. The enhanced preprocessing improves precision and recall. The results which are summarized in Table 5 indicate that precision enhancing devices improved the performance of the LNK task while recall enhancing devices improved the NED task. 6.3 Final Remarks on Differences In the extreme case, a perfect link detection system performs perfectly on the NED task. We gave empirical evidence that there is not necessarily such a correlation at lower accuracies. These findings are in accordance with the results reported in (Allan et al., 2000b) for topic tracking and first story detection. To test the impact of the cost function on the performance of LNK and NED systems, we repeated the evaluation with and both set to 1, and we found that the difference between the two reTable 6: Topic-weighted minimum normalized detection cost for NED when using parameter settings that are best for NED (1) and those that are best for LNK (2). Columns (3) and (4) show the detection costs using uniform costs for misses and false alarms. sults decreases from 30.24% to 14.73%. The result indicates that the setting (Hel, PoS, ASRstop) is better at rec</context>
<context position="23046" citStr="Allan et al., 2000" startWordPosition="3890" endWordPosition="3893"> system, however, can afford to perform worse on the first story if it compensates by performing well on follow-up stories (because here not flagged follow-up stories are considered misses and thus higher penalized than in NED). This view explains the benefits of using partof-speech information and the negative effect of the ASR stop-list on NED : different part-of-speech tags help discriminate new events from old events; removing words by using the ASR stoplist makes it harder to discriminate new events. We conjecture that the Hellinger metric helps improve recall, and in a study similar to (Allan et al., 2000b) we plan to further evaluate the impact of the Hellinger metric on a closed collection e.g. TREC. 7 Conclusions and Future Work We have compared the effect of several techniques on the performance of a story link detection system and a new event detection system. Although many of the processing techniques used by our systems are the same, a number of core technologies affect the performance of the LNK and NED systems differently. The Clarity similarity measure was more effective for LNK, Hellinger similarity measure was more effective for NED, part-of-speech was more useful for NED, and stop</context>
</contexts>
<marker>Allan, Lavrenko, Malin, Swan, 2000</marker>
<rawString>J. Allan, V. Lavrenko, D. Malin, and R. Swan. 2000a. Detections, bounds, and timelines: Umass and tdt-3. In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Victor Lavrenko</author>
<author>Hubert Jin</author>
</authors>
<title>First story detection in TDT is hard.</title>
<date>2000</date>
<booktitle>In CIKM,</booktitle>
<pages>374--381</pages>
<contexts>
<context position="2013" citStr="Allan et al., 2000" startWordPosition="317" endWordPosition="320">rash victims are considered to be linked. In contrast, a story about hurricane Andrew and a story about hurricane Agnes are not linked because they are two different events. A new event detection system detects when a story discusses a previously unseen or “not linked” event. Link detection is considered to be a core technology for new event detection and the other tasks. Several groups are performing research in the TDT tasks of link detection and new event detection. Based on their findings, we incorporated a number of their ideas into our baseline system. CMU (Yang et al., 1998) and UMass (Allan et al., 2000a) found that for new event detection it was better to compare a new story against all previously seen stories than to cluster previously seen stories and compare a new story against the clusters. CMU (Carbonell et al., 2001) found that NED results could be improved by developing separate models for different news sources to that could capture idiosyncrasies of different sources, which we also extended to link detection. UMass reported on adapting a tracking system for NED detection (Allan et al., 2000b). Allan et. al , (Allan et al., 2000b) developed a NED system based upon a tracking technol</context>
<context position="21219" citStr="Allan et al., 2000" startWordPosition="3577" endWordPosition="3580">all enhancing devices while the ASR stop-list is a precision enhancing device. The enhanced preprocessing improves precision and recall. The results which are summarized in Table 5 indicate that precision enhancing devices improved the performance of the LNK task while recall enhancing devices improved the NED task. 6.3 Final Remarks on Differences In the extreme case, a perfect link detection system performs perfectly on the NED task. We gave empirical evidence that there is not necessarily such a correlation at lower accuracies. These findings are in accordance with the results reported in (Allan et al., 2000b) for topic tracking and first story detection. To test the impact of the cost function on the performance of LNK and NED systems, we repeated the evaluation with and both set to 1, and we found that the difference between the two reTable 6: Topic-weighted minimum normalized detection cost for NED when using parameter settings that are best for NED (1) and those that are best for LNK (2). Columns (3) and (4) show the detection costs using uniform costs for misses and false alarms. sults decreases from 30.24% to 14.73%. The result indicates that the setting (Hel, PoS, ASRstop) is better at rec</context>
<context position="23046" citStr="Allan et al., 2000" startWordPosition="3890" endWordPosition="3893"> system, however, can afford to perform worse on the first story if it compensates by performing well on follow-up stories (because here not flagged follow-up stories are considered misses and thus higher penalized than in NED). This view explains the benefits of using partof-speech information and the negative effect of the ASR stop-list on NED : different part-of-speech tags help discriminate new events from old events; removing words by using the ASR stoplist makes it harder to discriminate new events. We conjecture that the Hellinger metric helps improve recall, and in a study similar to (Allan et al., 2000b) we plan to further evaluate the impact of the Hellinger metric on a closed collection e.g. TREC. 7 Conclusions and Future Work We have compared the effect of several techniques on the performance of a story link detection system and a new event detection system. Although many of the processing techniques used by our systems are the same, a number of core technologies affect the performance of the LNK and NED systems differently. The Clarity similarity measure was more effective for LNK, Hellinger similarity measure was more effective for NED, part-of-speech was more useful for NED, and stop</context>
</contexts>
<marker>Allan, Lavrenko, Jin, 2000</marker>
<rawString>James Allan, Victor Lavrenko, and Hubert Jin. 2000b. First story detection in TDT is hard. In CIKM, pages 374–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Francine Chen</author>
<author>Ioannis Tsochantaridis</author>
</authors>
<title>Topic-based document segmentation with probabilistic latent semantic analysis.</title>
<date>2002</date>
<booktitle>In International Conference on Information and Knowledge Management (CIKM),</booktitle>
<location>McLean, VA.</location>
<contexts>
<context position="13386" citStr="Brants et al., 2002" startWordPosition="2258" endWordPosition="2261">nd the enhanced processing list includes most of the term in the ASR stop-list and removing these terms will improve precision. On the other hand normalizing these terms will have the same effect as stemming a recall enhancing device (Xu and Croft, 1998) , (Kraaij and Pohlmann, 1996). In addition to these techniques, we also investigated the use of different similarity measures. 6.2 Similarity Measures The systems developed for TDT primarily use cosine similarity as the similarity measure. We have developed systems based on cosine similarity (Chen et al., 2003). In work on text segmentation, (Brants et al., 2002) observed that the system performance was much better when the Hellinger measure was used instead. In this work, we decided to use the clarity metric, a precision enhancing device (Croft et al., 2001). For both our LNK and NED systems, we compared the performance of the systems using each of the similarity measures separately. Table 1 shows that for LNK, the system based on Clarity similarity performed better the system based on Hellinger similarity; in contrast, for NED, the system based on 1 NED Hellinger vs. Clarity Similarity Figure 2: CDF for Clarity and Hellinger similarity on the NED ta</context>
</contexts>
<marker>Brants, Chen, Tsochantaridis, 2002</marker>
<rawString>Thorsten Brants, Francine Chen, and Ioannis Tsochantaridis. 2002. Topic-based document segmentation with probabilistic latent semantic analysis. In International Conference on Information and Knowledge Management (CIKM), McLean, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Yiming Yang</author>
<author>Ralf Brown</author>
<author>Chun Jin</author>
<author>Jian Zhang</author>
</authors>
<date>2001</date>
<booktitle>Cmu tdt report. Slides at the TDT-2001 meeting, CMU.</booktitle>
<contexts>
<context position="2238" citStr="Carbonell et al., 2001" startWordPosition="356" endWordPosition="359">ory discusses a previously unseen or “not linked” event. Link detection is considered to be a core technology for new event detection and the other tasks. Several groups are performing research in the TDT tasks of link detection and new event detection. Based on their findings, we incorporated a number of their ideas into our baseline system. CMU (Yang et al., 1998) and UMass (Allan et al., 2000a) found that for new event detection it was better to compare a new story against all previously seen stories than to cluster previously seen stories and compare a new story against the clusters. CMU (Carbonell et al., 2001) found that NED results could be improved by developing separate models for different news sources to that could capture idiosyncrasies of different sources, which we also extended to link detection. UMass reported on adapting a tracking system for NED detection (Allan et al., 2000b). Allan et. al , (Allan et al., 2000b) developed a NED system based upon a tracking technology and showed that to achieve high-quality first story detection, tracking effectiveness must improve to a degree that experience suggests is unlikely. In this paper, while we reach a similar conclusion as (Allan et al., 200</context>
<context position="7990" citStr="Carbonell et al., 2001" startWordPosition="1317" endWordPosition="1320"> and are linked, we identify a set of similarity metrics that capture the similarity between the two documents using Clarity and Hellinger metrics: 1. Use an empty model: for all ; (9) (7) 2.6 Source-Pair-Specific Normalization Due to stylistic differences between various sources, e.g., news paper vs. broadcast news, translation errors, and automatic speech recognition errors (Allan et al., 1999), the similarity measures for both ontopic and off-topic pairs will in general depend on the source pair. Errors due to these differences can be reduced by using thresholds conditioned on the sources (Carbonell et al., 2001), or, as we do, by normalizing the similarity values based on similarities for the source pairs found in the story history. 3 New Event Detection In order to decide whether a new document that is added to the collection at time describes a new event, it is individually compared to all previous documents using the steps described in section 2. We identify the document with highest similarity: (8) The value is used to determine whether a document is about a new event and at the same time is an indication of the confidence in our decision. If the score exceeds a threshold, then there is no suffic</context>
</contexts>
<marker>Carbonell, Yang, Brown, Jin, Zhang, 2001</marker>
<rawString>Jaime Carbonell, Yiming Yang, Ralf Brown, Chun Jin, and Jian Zhang. 2001. Cmu tdt report. Slides at the TDT-2001 meeting, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francine Chen</author>
<author>Ayman Farahat</author>
<author>Thorsten Brants</author>
</authors>
<title>Story link detection and new event detection are asymmetric.</title>
<date>2003</date>
<booktitle>In Proceedings ofNAACL-HLT-2002,</booktitle>
<location>Edmonton, AL.</location>
<contexts>
<context position="13333" citStr="Chen et al., 2003" startWordPosition="2249" endWordPosition="2252">ing spelled out numbers into numbers. On the one hand the enhanced processing list includes most of the term in the ASR stop-list and removing these terms will improve precision. On the other hand normalizing these terms will have the same effect as stemming a recall enhancing device (Xu and Croft, 1998) , (Kraaij and Pohlmann, 1996). In addition to these techniques, we also investigated the use of different similarity measures. 6.2 Similarity Measures The systems developed for TDT primarily use cosine similarity as the similarity measure. We have developed systems based on cosine similarity (Chen et al., 2003). In work on text segmentation, (Brants et al., 2002) observed that the system performance was much better when the Hellinger measure was used instead. In this work, we decided to use the clarity metric, a precision enhancing device (Croft et al., 2001). For both our LNK and NED systems, we compared the performance of the systems using each of the similarity measures separately. Table 1 shows that for LNK, the system based on Clarity similarity performed better the system based on Hellinger similarity; in contrast, for NED, the system based on 1 NED Hellinger vs. Clarity Similarity Figure 2: C</context>
<context position="20236" citStr="Chen et al., 2003" startWordPosition="3424" endWordPosition="3427">atistical approach (Dunning, 1993) to identify tokens with significantly different distributions in the two corpora. We compiled the problematic ASR terms into an “ASR stoplist”. This list was primarily composed of spelledout numbers, numerals and a few other terms. Table 4 shows the topic-weighted minimum detection costs for LNK and NED on the TDT 2002 dry run data. The table shows results for standard preprocessing without an ASR stop-list and with and ASR stop-list. For Link Detection, the ASR stoplist improves results, while the same list decreases performance for New Event Detection. In (Chen et al., 2003) we investigated normalizing abbreviations and transforming spelled-out numbers into numerals, “enhanced preprocessing”, and then compared this approach with using an “ASR stoplist”. 6.2.3 Impact of Recall and Precision The previous two sections examined the impact of four different techniques on the performance of LNK and NED systems. The Part-of-speech is a recall enhancing devices while the ASR stop-list is a precision enhancing device. The enhanced preprocessing improves precision and recall. The results which are summarized in Table 5 indicate that precision enhancing devices improved the</context>
</contexts>
<marker>Chen, Farahat, Brants, 2003</marker>
<rawString>Francine Chen, Ayman Farahat, and Thorsten Brants. 2003. Story link detection and new event detection are asymmetric. In Proceedings ofNAACL-HLT-2002, Edmonton, AL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Bruce Croft</author>
<author>Stephen Cronen-Townsend</author>
<author>Victor Larvrenko</author>
</authors>
<title>Relevance feedback and personalization: A language modeling perspective.</title>
<date>2001</date>
<booktitle>In DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries.</booktitle>
<contexts>
<context position="5186" citStr="Croft et al., 2001" startWordPosition="845" endWordPosition="848">ment frequencies as described in the previous section are used to calculate weights for the terms in the documents. At time, we use where is the total number of documents at time . is a normalization value such that either the weights sum to 1 (if we use Hellinger distance, KL-divergence, or Clarity-based distance), or their squares sum to 1 (if we use cosine distance). 2.4 Similarity Calculation The vectors consisting of normalized term weights are used to calculate the similarity between two documents and. In our current implementation, we use the the Clarity metric which was introduced by (Croft et al., 2001; Lavrenko et al., 2002) and gets its name from the distance to general English, which is called Clarity. We used a symmetric version that is computed as: where “ ” is the Kullback-Leibler divergence, is the probability distribution of words for “general English” as derived from the training corpus. The idea behind this metric is that we want to give credit to similar pairs of documents that are very different from general English, and we want to discount similar pairs of documents that are close to general English (which can be interpreted as being the noise). The motivation for using the cla</context>
<context position="13586" citStr="Croft et al., 2001" startWordPosition="2291" endWordPosition="2294">emming a recall enhancing device (Xu and Croft, 1998) , (Kraaij and Pohlmann, 1996). In addition to these techniques, we also investigated the use of different similarity measures. 6.2 Similarity Measures The systems developed for TDT primarily use cosine similarity as the similarity measure. We have developed systems based on cosine similarity (Chen et al., 2003). In work on text segmentation, (Brants et al., 2002) observed that the system performance was much better when the Hellinger measure was used instead. In this work, we decided to use the clarity metric, a precision enhancing device (Croft et al., 2001). For both our LNK and NED systems, we compared the performance of the systems using each of the similarity measures separately. Table 1 shows that for LNK, the system based on Clarity similarity performed better the system based on Hellinger similarity; in contrast, for NED, the system based on 1 NED Hellinger vs. Clarity Similarity Figure 2: CDF for Clarity and Hellinger similarity on the NED task for on-topic and off-topic pairs. Table 1: Effect of different similarity measures on topic-weighted minimum normalized detection costs for LNK and NED on the TDT 2002 dry run data. System Clarity </context>
</contexts>
<marker>Croft, Cronen-Townsend, Larvrenko, 2001</marker>
<rawString>W. Bruce Croft, Stephen Cronen-Townsend, and Victor Larvrenko. 2001. Relevance feedback and personalization: A language modeling perspective. In DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted E Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="19652" citStr="Dunning, 1993" startWordPosition="3327" endWordPosition="3328">t al., 1999). Furthermore, there are systematic differences between ASR and manually transcribed text, e.g., numbers are often spelled out thus “30” will be spelled out “thirty”. Another situation where ASR is different from transcribed text is abbreviations, e.g. ASR system will recognize ‘CNN” as three separate tokens “C”, “N”, and “N”. In order to account for these differences, we identified the set of tokens that are problematic for ASR. Our approach was to identify a parallel corpus of manually and automatically transcribed documents, the TDT2 corpus, and then use a statistical approach (Dunning, 1993) to identify tokens with significantly different distributions in the two corpora. We compiled the problematic ASR terms into an “ASR stoplist”. This list was primarily composed of spelledout numbers, numerals and a few other terms. Table 4 shows the topic-weighted minimum detection costs for LNK and NED on the TDT 2002 dry run data. The table shows results for standard preprocessing without an ASR stop-list and with and ASR stop-list. For Link Detection, the ASR stoplist improves results, while the same list decreases performance for New Event Detection. In (Chen et al., 2003) we investigated</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted E. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wessel Kraaij</author>
<author>Renee Pohlmann</author>
</authors>
<title>Viewing stemming as recall enhancement.</title>
<date>1996</date>
<tech>InACMSIGIR1996.</tech>
<contexts>
<context position="13050" citStr="Kraaij and Pohlmann, 1996" startWordPosition="2205" endWordPosition="2208">ch tagging which was shown by Allan and Raghavan (Allan and Raghavan, 2002) to improve query clarity. In section 6.2.1 we will show how POS helps recall. We also investigated the use of expanded stop-list which improves precision. We also investigated normalizing abbreviations and transforming spelled out numbers into numbers. On the one hand the enhanced processing list includes most of the term in the ASR stop-list and removing these terms will improve precision. On the other hand normalizing these terms will have the same effect as stemming a recall enhancing device (Xu and Croft, 1998) , (Kraaij and Pohlmann, 1996). In addition to these techniques, we also investigated the use of different similarity measures. 6.2 Similarity Measures The systems developed for TDT primarily use cosine similarity as the similarity measure. We have developed systems based on cosine similarity (Chen et al., 2003). In work on text segmentation, (Brants et al., 2002) observed that the system performance was much better when the Hellinger measure was used instead. In this work, we decided to use the clarity metric, a precision enhancing device (Croft et al., 2001). For both our LNK and NED systems, we compared the performance </context>
</contexts>
<marker>Kraaij, Pohlmann, 1996</marker>
<rawString>Wessel Kraaij and Renee Pohlmann. 1996. Viewing stemming as recall enhancement. InACMSIGIR1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>James Allan</author>
<author>Edward DeGuzman</author>
<author>Daniel LaFlamme</author>
<author>Veera Pollard</author>
<author>Stephen Thomas</author>
</authors>
<title>Relevance models for topic detection and tracking.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT-2002,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="5210" citStr="Lavrenko et al., 2002" startWordPosition="849" endWordPosition="852">described in the previous section are used to calculate weights for the terms in the documents. At time, we use where is the total number of documents at time . is a normalization value such that either the weights sum to 1 (if we use Hellinger distance, KL-divergence, or Clarity-based distance), or their squares sum to 1 (if we use cosine distance). 2.4 Similarity Calculation The vectors consisting of normalized term weights are used to calculate the similarity between two documents and. In our current implementation, we use the the Clarity metric which was introduced by (Croft et al., 2001; Lavrenko et al., 2002) and gets its name from the distance to general English, which is called Clarity. We used a symmetric version that is computed as: where “ ” is the Kullback-Leibler divergence, is the probability distribution of words for “general English” as derived from the training corpus. The idea behind this metric is that we want to give credit to similar pairs of documents that are very different from general English, and we want to discount similar pairs of documents that are close to general English (which can be interpreted as being the noise). The motivation for using the clarity metric will given i</context>
<context position="12173" citStr="Lavrenko et al., 2002" startWordPosition="2059" endWordPosition="2062"> story is incorrectly identified as a new event. The detection cost in Eqn.10 which assigns a higher cost to false alarm . A LNK system wants to minimize false alarms and to do this it should identify stories as being linked only if they are linked, which translates to high precision. In contrast a NED system, will minimize false alarms by identifying all stories that are linked which translates to high recall. Motivated by this discussion, we investigated the use of number of precision and recall enhancing techniques with the LNK and NED system. We investigated the use of the Clarity metric (Lavrenko et al., 2002) which was shown to correlate positively with precision. We investigated the 0.1 00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Similarity Figure 1: CDF for Clarity and Hellinger similarity on the LNK task for on-topic and off-topic pairs. use of part-of-speech tagging which was shown by Allan and Raghavan (Allan and Raghavan, 2002) to improve query clarity. In section 6.2.1 we will show how POS helps recall. We also investigated the use of expanded stop-list which improves precision. We also investigated normalizing abbreviations and transforming spelled out numbers into numbers. On the one hand the </context>
</contexts>
<marker>Lavrenko, Allan, DeGuzman, LaFlamme, Pollard, Thomas, 2002</marker>
<rawString>Victor Lavrenko, James Allan, Edward DeGuzman, Daniel LaFlamme, Veera Pollard, and Stephen Thomas. 2002. Relevance models for topic detection and tracking. In Proceedings ofHLT-2002, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pirkola</author>
<author>K Jrvelin</author>
</authors>
<title>The effect of anaphora and ellipsis resolution on proximity searching in a text database.</title>
<date>1996</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>32--2</pages>
<marker>Pirkola, Jrvelin, 1996</marker>
<rawString>A. Pirkola and K. Jrvelin. 1996. The effect of anaphora and ellipsis resolution on proximity searching in a text database. Information Processing and Management, 32(2):199–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William Vetterling</author>
<author>Brian Flannery</author>
</authors>
<title>Numerical Recipes.</title>
<date>1993</date>
<publisher>Cambridge Unv. Press.</publisher>
<contexts>
<context position="14970" citStr="Press et al., 1993" startWordPosition="2508" endWordPosition="2511">n for the Hellinger and Clarity similarities for on-topic (about the same event) and off-topic (about different events) pairs for the LNK task. While there are a number of statistics to measure the overall difference between tow cumulative distribution functions, we used the Kolmogorov-Smirnov distance (K-S distance; the largest difference between two cumulative distributions) for two reasons. First, the K-S distance is invariant under re-parametrization. Second, the significance of the K-S distance in case of the null hypothesis (data sets are drawn from same distribution) can be calculated (Press et al., 1993). The K-S distance between the on-topic and off-topic similarities is larger for Clarity similarity (cf. table 2), indicating that it is the better metric for LNK. Figure 2 shows the cumulative distribution functions for Hellinger and Clarity similarities in the NED task. The plot is based on pairs that contain the current story and its most similar story in the story history. When the most similar story is on the same event (approx. 75% of the cases), its similarity is part Table 2: K-S distance between on-topic and offtopic story pairs. Clarity Hellinger Change (%) LNK 0.7680 0.7251 ( ) NED </context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1993</marker>
<rawString>William H. Press, Saul A. Teukolsky, William Vetterling, and Brian Flannery. 1993. Numerical Recipes. Cambridge Unv. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Wayne</author>
</authors>
<title>Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.</title>
<date>2000</date>
<booktitle>In Language Resources and Evaluation Conference (LREC),</booktitle>
<pages>1487--1494</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1122" citStr="Wayne, 2000" startWordPosition="169" endWordPosition="170">ection as information retrieval task and hypothesize on the impact of precision and recall on both systems. Motivated by these arguments, we introduce a number of new performance enhancing techniques including part of speech tagging, new similarity measures and expanded stop lists. Experimental results validate our hypothesis. 1 Introduction Topic Detection and Tracking (TDT) research is sponsored by the DARPA Translingual Information Detection, Extraction, and Summarization (TIDES) program. The research has five tasks related to organizing streams of data such as newswire and broadcast news (Wayne, 2000): story segmentation, topic tracking, topic detection, new event detection (NED), and link detection (LNK). A link detection system detects whether two stories are “linked”, or discuss the same event. A story about a plane crash and another story about the funeral of the crash victims are considered to be linked. In contrast, a story about hurricane Andrew and a story about hurricane Agnes are not linked because they are two different events. A new event detection system detects when a story discusses a previously unseen or “not linked” event. Link detection is considered to be a core technolo</context>
</contexts>
<marker>Wayne, 2000</marker>
<rawString>Charles Wayne. 2000. Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation. In Language Resources and Evaluation Conference (LREC), pages 1487–1494, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Corpus-based stemming using cooccurrence of word variants.</title>
<date>1998</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="13020" citStr="Xu and Croft, 1998" startWordPosition="2200" endWordPosition="2203">rs. use of part-of-speech tagging which was shown by Allan and Raghavan (Allan and Raghavan, 2002) to improve query clarity. In section 6.2.1 we will show how POS helps recall. We also investigated the use of expanded stop-list which improves precision. We also investigated normalizing abbreviations and transforming spelled out numbers into numbers. On the one hand the enhanced processing list includes most of the term in the ASR stop-list and removing these terms will improve precision. On the other hand normalizing these terms will have the same effect as stemming a recall enhancing device (Xu and Croft, 1998) , (Kraaij and Pohlmann, 1996). In addition to these techniques, we also investigated the use of different similarity measures. 6.2 Similarity Measures The systems developed for TDT primarily use cosine similarity as the similarity measure. We have developed systems based on cosine similarity (Chen et al., 2003). In work on text segmentation, (Brants et al., 2002) observed that the system performance was much better when the Hellinger measure was used instead. In this work, we decided to use the clarity metric, a precision enhancing device (Croft et al., 2001). For both our LNK and NED systems</context>
</contexts>
<marker>Xu, Croft, 1998</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 1998. Corpus-based stemming using cooccurrence of word variants. ACM Transactions on Information Systems, 16(1):61–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Tom Pierce</author>
<author>Jaime Carbonell</author>
</authors>
<title>A study on retrospective and on-line event detection.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR-98,</booktitle>
<location>Melbourne, Australia.</location>
<contexts>
<context position="1983" citStr="Yang et al., 1998" startWordPosition="311" endWordPosition="314">ory about the funeral of the crash victims are considered to be linked. In contrast, a story about hurricane Andrew and a story about hurricane Agnes are not linked because they are two different events. A new event detection system detects when a story discusses a previously unseen or “not linked” event. Link detection is considered to be a core technology for new event detection and the other tasks. Several groups are performing research in the TDT tasks of link detection and new event detection. Based on their findings, we incorporated a number of their ideas into our baseline system. CMU (Yang et al., 1998) and UMass (Allan et al., 2000a) found that for new event detection it was better to compare a new story against all previously seen stories than to cluster previously seen stories and compare a new story against the clusters. CMU (Carbonell et al., 2001) found that NED results could be improved by developing separate models for different news sources to that could capture idiosyncrasies of different sources, which we also extended to link detection. UMass reported on adapting a tracking system for NED detection (Allan et al., 2000b). Allan et. al , (Allan et al., 2000b) developed a NED system</context>
</contexts>
<marker>Yang, Pierce, Carbonell, 1998</marker>
<rawString>Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998. A study on retrospective and on-line event detection. In Proceedings of SIGIR-98, Melbourne, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>