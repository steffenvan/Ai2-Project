<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.481227">
Error Mining on Dependency Trees
</title>
<author confidence="0.413695">
Claire Gardent Shashi Narayan
</author>
<address confidence="0.835902">
CNRS, LORIA, UMR 7503 Universit´e de Lorraine, LORIA, UMR 7503
Vandoeuvre-l`es-Nancy, F-54500, France Villers-l`es-Nancy, F-54600, France
</address>
<email confidence="0.998401">
claire.gardent@loria.fr shashi.narayan@loria.fr
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958176470588">
In recent years, error mining approaches were
developed to help identify the most likely
sources of parsing failures in parsing sys-
tems using handcrafted grammars and lexi-
cons. However the techniques they use to enu-
merate and count n-grams builds on the se-
quential nature of a text corpus and do not eas-
ily extend to structured data. In this paper, we
propose an algorithm for mining trees and ap-
ply it to detect the most likely sources of gen-
eration failure. We show that this tree mining
algorithm permits identifying not only errors
in the generation system (grammar, lexicon)
but also mismatches between the structures
contained in the input and the input structures
expected by our generator as well as a few id-
iosyncrasies/error in the input data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985895833334">
In recent years, error mining techniques have been
developed to help identify the most likely sources
of parsing failure (van Noord, 2004; Sagot and de la
Clergerie, 2006; de Kok et al., 2009). First, the input
data (text) is separated into two subcorpora, a corpus
of sentences that could be parsed (PASS) and a cor-
pus of sentences that failed to be parsed (FAIL). For
each n-gram of words (and/or part of speech tag) oc-
curring in the corpus to be parsed, a suspicion rate is
then computed which, in essence, captures the like-
lihood that this n-gram causes parsing to fail.
These error mining techniques have been applied
with good results on parsing output and shown to
help improve the large scale symbolic grammars and
lexicons used by the parser. However the techniques
they use (e.g., suffix arrays) to enumerate and count
n-grams builds on the sequential nature of a text cor-
pus and cannot easily extend to structured data.
There are some NLP applications though where
the processed data is structured data such as trees
or graphs and which would benefit from error min-
ing. For instance, when generating sentences from
dependency trees, as was proposed recently in the
Generation Challenge Surface Realisation Task (SR
Task, (Belz et al., 2011)), it would be useful to be
able to apply error mining on the input trees to find
the most likely causes of generation failure.
In this paper, we address this issue and propose
an approach that supports error mining on trees. We
adapt an existing algorithm for tree mining which we
then use to mine the Generation Challenge depen-
dency trees and identify the most likely causes of
generation failure. We show in particular, that this
tree mining algorithm permits identifying not only
errors in the grammar and the lexicon used by gener-
ation but also a few idiosyncrasies/error in the input
data as well as mismatches between the structures
contained in the SR input and the input structures
expected by our generator. The latter is an impor-
tant point since, for symbolic approaches, a major
hurdle to participation in the SR challenge is known
to be precisely these mismatches i.e., the fact that
the input provided by the SR task fails to match the
input expected by the symbolic generation systems
(Belz et al., 2011).
The paper is structured as follows. Section 2
presents the HybridTreeMiner algorithm, a complete
and computationally efficient algorithm developed
</bodyText>
<page confidence="0.926559">
592
</page>
<note confidence="0.9926435">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 592–600,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.992812">
Figure 1: Four unordered labelled trees. The right-
most is in Breadth-First Canonical Form
</figureCaption>
<bodyText confidence="0.999586888888889">
by (Chi et al., 2004) for discovering frequently oc-
curring subtrees in a database of labelled unordered
trees. Section 3 shows how to adapt this algorithm
to mine the SR dependency trees for subtrees with
high suspicion rate. Section 4 presents an experi-
ment we made using the resulting tree mining algo-
rithm on SR dependency trees and summarises the
results. Section 5 discusses related work. Section 6
concludes.
</bodyText>
<sectionHeader confidence="0.990299" genericHeader="method">
2 Mining Trees
</sectionHeader>
<bodyText confidence="0.999919153846154">
Mining for frequent subtrees is an important prob-
lem that has many applications such as XML data
mining, web usage analysis and RNA classification.
The HybridTreeMiner (HTM) algorithm presented
in (Chi et al., 2004) provides a complete and com-
putationally efficient method for discovering fre-
quently occurring subtrees in a database of labelled
unordered trees and counting them. We now sketch
the intuition underlying this algorithm1. In the next
section, we will show how to modify this algorithm
to mine for errors in dependency trees.
Given a set of trees T, the HybridTreeMiner al-
gorithm proceeds in two steps. First, the unordered
labelled trees contained in T are converted to a
canonical form called BFCF (Breadth-First Canoni-
cal Form). In that way, distinct instantiations of the
same unordered trees have a unique representation.
Second, the subtrees of the BFCF trees are enumer-
ated in increasing size order using two tree opera-
tions called join and extension and their support (the
number of trees in the database that contains each
subtree) is recorded. In effect, the algorithm builds
an enumeration tree whose nodes are the possible
subtrees of T and such that, at depth d of this enu-
meration tree, all possible frequent subtrees consist-
ing of d nodes are listed.
</bodyText>
<footnote confidence="0.616574">
1For a more complete definition see (Chi et al., 2004).
</footnote>
<bodyText confidence="0.999766625">
The BFCF canonical form of an unordered tree
is an ordered tree t such that t has the smallest
breath-first canonical string (BFCS) encoding ac-
cording to lexicographic order. The BFCS encod-
ing of a tree is obtained by breadth-first traver-
sal of the tree, recording the string labelling each
node, “$” to separate siblings with distinct parents
and “#” to represent the end of the tree2. For in-
stance, the BFCS encodings of the four trees shown
in Figure 1 are ’A$BB$C$DC#’, ’A$BB$C$CD#’,
’A$BB$DC$C#’ and ’A$BB$CD$C#’ respectively.
Hence, the rightmost tree is the BFCF of all four
trees.
The join and extension operations used to itera-
tively enumerate subtrees are depicted in Figure 2
and can be defined as follows.
</bodyText>
<listItem confidence="0.998777083333333">
• A leg is a leaf of maximal depth.
• Extension: Given a tree t of height ht and a
node n, extending t with n yields a tree t&apos; (a
child of t in the enumeration tree) with height
ht, such that n is a child of one of t’s legs and
ht, is ht + 1.
• Join: Given two trees t1 and t2 of same height
h differing only in their rightmost leg and such
that t1 sorts lower than t2, joining t1 and t2
yields a tree t&apos; (a child of t1 in the enumeration
tree) of same height h by adding the rightmost
leg of t2 to t1 at level h − 1.
</listItem>
<figure confidence="0.649832">
A
</figure>
<figureCaption confidence="0.997165">
Figure 2: Join and Extension Operations
</figureCaption>
<bodyText confidence="0.9966065">
To support counting, the algorithm additionally
records for each subtree a list (called occurrence list)
</bodyText>
<footnote confidence="0.8453785">
2Assuming “#” sorts greater than “$” and both sort greater
than any other alphabets in node labels.
</footnote>
<figure confidence="0.999567717391304">
B
C
A
B
D
C
B
C
A
B
C
D
A
B
C
C
A
B
D
B
C
B
C
D
B
D
E
C
A
C
D + E —4Extension
B
D +
B
A
C
A
B
B
D
—4Join
A
C
E
C
E
</figure>
<page confidence="0.998643">
593
</page>
<bodyText confidence="0.9998635">
of all trees in which this subtree occurs and of its po-
sition in the tree (represented by the list of tree nodes
mapped onto by the subtree). Thus for a given sub-
tree t, the support of t is the number of elements
in that list. Occurrence lists are also used to check
that trees that are combined occur in the data. For
the join operation, the subtrees being combined must
occur in the same tree at the same position (the inter-
section of their occurrence lists must be non empty
and the tree nodes must match except the last node).
For the extension operation, the extension of a tree
t is licensed for any given occurrence in the occur-
rence list only if the planned extension maps onto
the tree identified by the occurrence.
</bodyText>
<sectionHeader confidence="0.942219" genericHeader="method">
3 Mining Dependency Trees
</sectionHeader>
<bodyText confidence="0.999983222222222">
We develop an algorithm (called ErrorTreeMiner,
ETM) which adapts the HybridTreeMiner algorithm
to mine sources of generation errors in the Gener-
ation Challenge SR shallow input data. The main
modification is that instead of simply counting trees,
we want to compute their suspicion rate. Following
(de Kok et al., 2009), we take the suspicion rate of a
given subtree t to be the proportion of cases where t
occurs in an input tree for which generation fails:
</bodyText>
<equation confidence="0.980402">
Sus(t) _
count (t|FAIL) count(t)
</equation>
<bodyText confidence="0.999972210526316">
where count(t) is the number of occurrences of
t in all input trees and count(t|FAIL) is the number
of occurrences of t in input trees for which no output
was produced.
Since we work with subtrees of arbitrary length,
we also need to check whether constructing a longer
subtree is useful that is, whether its suspicion rate
is equal or higher than the suspicion rate of any of
the subtrees it contains. In that way, we avoid com-
puting all subtrees (thus saving time and space). As
noted in (de Kok et al., 2009), this also permits by-
passing suspicion sharing that is the fact that, if n2
is the cause of a generation failure, and if n2 is con-
tained in larger trees n3 and n4, then all three trees
will have high suspicion rate making it difficult to
identify the actual source of failure namely n2. Be-
cause we use a milder condition however (we accept
bigger trees whose suspicion rate is equal to the sus-
picion rate of any of their subtrees), some amount of
</bodyText>
<equation confidence="0.5354765">
Algorithm 1 ErrorTreeMiner(D, minsup)
Note: D consists of Dfail and Dpass
F1 &lt;-- {Frequent 1-trees}
F2 &lt;-- 0
for i &lt;-- 1,..., |F1 |do
for j &lt;-- 1, ..., |F1 |do
q &lt;-- fi plus legfj
if Noord-Validation(q, minsup) then
F2 &lt;-- F2 U q
end if
end for
end for
F &lt;-- F1 U F2
PUSH: sort(F2) -4 LQueue
Enum-Grow(LQueue, F, minsup)
return F
Algorithm 2 Enum-Grow(LQueue, F, minsup)
while LQueue # empty do
POP: pop(LQueue) -4 C
for i &lt;-- 1, ..., |C |do
*The join operation
J &lt; --0
for j &lt;-- i, ..., |C |do
p &lt;-- join(ci, cj)
if Noord-Validation(p, minsup) then
J &lt;-- J U p
end if
end for
F &lt;-- F U J
PUSH: sort(J) -4 LQueue
*The extension operation
E &lt; --0
</equation>
<bodyText confidence="0.896343">
for possible leg lm of ci do
for possible new leg ln(C F1) do
q &lt;-- extend ci with ln at position lm
</bodyText>
<construct confidence="0.572085714285714">
if Noord-Validation(q, minsup) then
E &lt;-- E U q
end if
end for
end for
F &lt;-- F U E
PUSH: sort(E) -4 LQueue
</construct>
<listItem confidence="0.6867405">
end for
end while
</listItem>
<page confidence="0.981634">
594
</page>
<figure confidence="0.592709125">
Algorithm 3 Noord-Validation(tn, minsup)
Note: tn, tree with n nodes
if Sup(tn) &gt; minsup then
if Sus(tn) &gt; Sus(tn_1),Vtn_1 in tn then
return true
end if
end if
return false
</figure>
<bodyText confidence="0.9996626">
suspicion sharing remains. As we shall see in Sec-
tion 4.3.2, relaxing this check though allows us to
extract frequent larger tree patterns and thereby get
a more precise picture of the context in which highly
suspicious items occur.
Finally, we only keep subtrees whose support is
above a given threshold where the support Sup(t)
of a tree t is defined as the ratio between the number
of times it occurs in an input for which generation
fails and the total number of generation failures:
</bodyText>
<equation confidence="0.8997065">
count FAIL)
Sup(t) = (tcount(FAIL)
</equation>
<bodyText confidence="0.999850318181818">
The modified algorithm we use for error mining is
given in Algorithm 1, 2 and 3. It can be summarised
as follows.
First, dependency trees are converted to Breadth-
First Canonical Form whereby lexicographic order
can apply to the word forms labelling tree nodes, to
their part of speech, to their dependency relation or
to any combination thereof3.
Next, the algorithm iteratively enumerates the
subtrees occurring in the input data in increasing
size order and associating each subtree t with two
occurrence lists namely, the list of input trees in
which t occurs and for which generation was suc-
cessful (PASS(t)); and the list of input trees in which
t occurs and for which generation failed (FAIL(t)).
This process is initiated by building trees of size
one (i.e., one-node tree) and extending them to trees
of size two. It is then continued by extending the
trees using the join and extension operations. As
explained in Section 2 above, join and extension
only apply provided the resulting trees occur in the
data (this is checked by looking up occurrence lists).
</bodyText>
<footnote confidence="0.622746666666667">
3For convenience, the dependency relation labelling the
edges of dependency trees is brought down to the daughter node
of the edge.
</footnote>
<bodyText confidence="0.999849541666667">
Each time an n-node tree tn, is built, it is checked
that (i) its support is above the set threshold and (ii)
its suspicion rate is higher than or equal to the sus-
picion rate of all (n − 1)-node subtrees of tn.
In sum, the ETM algorithm differs from the HTM
algorithm in two main ways. First, while HTM ex-
plores the enumeration tree depth-first, ETM pro-
ceeds breadth-first to ensure that the suspicion rate
of (n-1)-node trees is always available when check-
ing whether an n-node tree should be introduced.
Second, while the HTM algorithm uses support to
prune the search space (only trees with a minimum
support bigger than the set threshold are stored), the
ETM algorithm drastically prunes the search space
by additionally checking that the suspicion rate of
all subtrees contained in a new tree t is smaller or
equal to the suspicion rate of t . As a result, while
ETM looses the space advantage of HTM by a small
margin4, it benefits from a much stronger pruning of
the search space than HTM through suspicion rate
checking. In practice, the ETM algorithm allows us
to process e.g., all NP chunks of size 4 and 6 present
in the SR data (roughly 60 000 trees) in roughly 20
minutes on a PC.
</bodyText>
<sectionHeader confidence="0.997351" genericHeader="method">
4 Experiment and Results
</sectionHeader>
<bodyText confidence="0.9996866">
Using the input data provided by the Generation
Challenge SR Task, we applied the error mining al-
gorithm described in the preceding Section to debug
and extend a symbolic surface realiser developed for
this task.
</bodyText>
<subsectionHeader confidence="0.998278">
4.1 Input Data and Surface Realisation System
</subsectionHeader>
<bodyText confidence="0.999930333333333">
The shallow input data provided by the SR Task
was obtained from the Penn Treebank using the
LTH Constituent-to-Dependency Conversion Tool
for Penn-style Treebanks (Pennconverter, (Johans-
son and Nugues, 2007)). It consists of a set
of unordered labelled syntactic dependency trees
whose nodes are labelled with word forms, part of
speech categories, partial morphosyntactic informa-
tion such as tense and number and, in some cases, a
sense tag identifier. The edges are labelled with the
syntactic labels provided by the Pennconverter. All
words (including punctuation) of the original sen-
</bodyText>
<footnote confidence="0.982404">
4ETM needs to store all (n-1)-node trees in queues before
producing n-node trees.
</footnote>
<page confidence="0.996452">
595
</page>
<bodyText confidence="0.999838882352941">
tence are represented by a node in the tree and the
alignment between nodes and word forms was pro-
vided by the organisers.
The surface realiser used is a system based on
a Feature-Based Lexicalised Tree Adjoining Gram-
mar (FB-LTAG) for English extended with a unifica-
tion based compositional semantics. Both the gram-
mars and the lexicon were developed in view of the
Generation Challenge and the data provided by this
challenge was used as a means to debug and extend
the system. Unknown words are assigned a default
TAG family/tree based on the part of speech they
are associated with in the SR data. The surface real-
isation algorithm extends the algorithm proposed in
(Gardent and Perez-Beltrachini, 2010) and adapts it
to work on the SR dependency input rather than on
flat semantic representations.
</bodyText>
<subsectionHeader confidence="0.940991">
4.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999848529411765">
To facilitate interpretation, we first chunked the in-
put data in NPs, PPs and Clauses and performed er-
ror mining on the resulting sets of data. The chunk-
ing was performed by retrieving from the Penn Tree-
bank (PTB), for each phrase type, the yields of the
constituents of that type and by using the alignment
between words and dependency tree nodes provided
by the organisers of the SR Task. For instance, given
the sentence “The most troublesome report may be
the August merchandise trade deficit due out tomor-
row”, the NPs “The most troublesome report” and
“the August merchandise trade deficit due out to-
morrow” will be extracted from the PTB and the
corresponding dependency structures from the SR
Task data.
Using this chunked data, we then ran the genera-
tor on the corresponding SR Task dependency trees
and stored separately, the input dependency trees for
which generation succeeded and the input depen-
dency trees for which generation failed. Using infor-
mation provided by the generator, we then removed
from the failed data, those cases where generation
failed either because a word was missing in the lex-
icon or because a TAG tree/family was missing in
the grammar but required by the lexicon and the in-
put data. These cases can easily be detected using
the generation system and thus do not need to be
handled by error mining.
Finally, we performed error mining on the data
using different minimal support thresholds, differ-
ent display modes (sorted first by size and second by
suspicion rate vs sorted by suspicion rate) and differ-
ent labels (part of speech, words and part of speech,
dependency, dependency and part of speech).
</bodyText>
<subsectionHeader confidence="0.921131">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.982399324324324">
One feature of our approach is that it permits min-
ing the data for tree patterns of arbitrary size us-
ing different types of labelling information (POS
tags, dependencies, word forms and any combina-
tion thereof). In what follows, we focus on the NP
chunk data and illustrate by means of examples how
these features can be exploited to extract comple-
mentary debugging information from the data.
4.3.1 Mining on single labels (word form, POS
tag or dependency)
Mining on a single label permits (i) assessing the
relative impact of each category in a given label cat-
egory and (ii) identifying different sources of errors
depending on the type of label considered (POS tag,
dependency or word form).
Mining on POS tags Table 1 illustrates how min-
ing on a single label (in this case, POS tags) gives
a good overview of how the different categories in
that label type impact generation: two POS tags
(POS and CC) have a suspicion rate of 0.99 indicat-
ing that these categories always lead generation to
fail. Other POS tag with much lower suspicion rate
indicate that there are unresolved issues with, in de-
creasing order of suspicion rate, cardinal numbers
(CD), proper names (NNP), nouns (NN), prepositions
(IN) and determiners (DT).
The highest ranking category (POS5) points to
a mismatch between the representation of geni-
tive NPs (e.g., John’s father) in the SR Task data
and in the grammar. While our generator ex-
pects the representation of ‘John’s father’ to be FA-
THER(“S”(JOHN)), the structure provided by the SR
Task is FATHER(JOHN(“S”)). Hence whenever a
possessive appears in the input data, generation fails.
This is in line with (Rajkumar et al., 2011)’s finding
that the logical forms expected by their system for
possessives differed from the shared task inputs.
</bodyText>
<footnote confidence="0.982168">
5In the Penn Treebank, the POS tag is the category assigned
to possessive ’s.
</footnote>
<page confidence="0.993756">
596
</page>
<table confidence="0.998280125">
POS Sus Sup Fail Pass
POS 0.99 0.38 3237 1
CC 0.99 0.21 1774 9
CD 0.39 0.16 1419 2148
NNP 0.35 0.32 2749 5014
NN 0.30 0.81 6798 15663
IN 0.30 0.16 1355 3128
DT 0.09 0.12 1079 10254
</table>
<tableCaption confidence="0.998507">
Table 1: Error Mining on POS tags with frequency
</tableCaption>
<bodyText confidence="0.985691">
cutoff 0.1 and displaying only trees of size 1 sorted
by decreasing suspicion rate (Sus)
The second highest ranked category is CC for co-
ordinations. In this case, error mining unveils a
bug in the grammar trees associated with conjunc-
tion which made all sentences containing a conjunc-
tion fail. Because the grammar is compiled out of
a strongly factorised description, errors in this de-
scription can propagate to a large number of trees
in the grammar. It turned out that an error occurred
in a class inherited by all conjunction trees thereby
blocking the generation of any sentence requiring
the use of a conjunction.
Next but with a much lower suspicion rate come
cardinal numbers (CD), proper names (NNP), nouns
(NN), prepositions (IN) and determiners (DT). We
will see below how the richer information provided
by mining for larger tree patterns with mixed la-
belling information permits identifying the contexts
in which these POS tags lead to generation failure.
Mining on Word Forms Because we remove
from the failure set all cases of errors due to a miss-
ing word form in the lexicon, a high suspicion rate
for a word form usually indicates a missing or incor-
rect lexical entry: the word is present in the lexicon
but associated with either the wrong POS tag and/or
the wrong TAG tree/family. To capture such cases,
we therefore mine not on word forms alone but on
pairs of word forms and POS tag. In this way, we
found for instance, that cardinal numbers induced
many generation failures whenever they were cate-
gorised as determiners but not as nouns in our lexi-
con. As we will see below, larger tree patterns help
identify the specific contexts inducing such failures.
One interesting case stood out which pointed to
idiosyncrasies in the input data: The word form $
(Sus=1) was assigned the POS tag $ in the input
data, a POS tag which is unknown to our system and
not documented in the SR Task guidelines. The SR
guidelines specify that the Penn Treebank tagset is
used modulo the modifications which are explicitly
listed. However for the $ symbol, the Penn treebank
used SYM as a POS tag and the SR Task $, but the
modification is not listed. Similarly, while in the
Penn treebank, punctuations are assigned the SYM
POS tag, in the SR data “,” is used for the comma,
“(“ for an opening bracket and so on.
Mining on Dependencies When mining on de-
pendencies, suspects can point to syntactic construc-
tions (rather than words or word categories) that are
not easily spotted when mining on words or parts
of speech. Thus, while problems with coordination
could easily be spotted through a high suspicion rate
for the CC POS tag, some constructions are linked
neither to a specific POS tag nor to a specific word.
This is the case, for instance, for apposition which
a suspicion rate of 0.19 (286F/1148P) identified as
problematic. Similarly, a high suspicion rate (0.54,
183F/155P) on the TMP dependency indicates that
temporal modifiers are not correctly handled either
because of missing or erroneous information in the
grammar or because of a mismatch between the in-
put data and the fomat expected by the surface re-
aliser.
Interestingly, the underspecified dependency rela-
tion DEP which is typically used in cases for which
no obvious syntactic dependency comes to mind
shows a suspicion rate of 0.61 (595F/371P).
</bodyText>
<subsectionHeader confidence="0.999001">
4.3.2 Mining on trees of arbitrary size and
complex labelling patterns
</subsectionHeader>
<bodyText confidence="0.999923444444444">
While error mining with tree patterns of size one
permits ranking and qualifying the various sources
of errors, larger patterns often provide more detailed
contextual information about these errors. For in-
stance, Table 1 shows that the CD POS tag has a
suspicion rate of 0.39 (1419F/2148P). The larger
tree patterns identified below permits a more specific
characterization of the context in which this POS tag
co-occurs with generation failure:
</bodyText>
<note confidence="0.651885">
TP1 CD(IN,RBR) more than 10
TP2 IN(CD) of 1991
TP3 NNP(CD) November 1
TP4 CD(NNP(CD)) Nov. 1, 1997
</note>
<page confidence="0.995963">
597
</page>
<bodyText confidence="0.961356466666667">
Two patterns clearly emerge: a pattern where car-
dinal numbers are parts of a date (tree patterns TP2-
TP4) and a more specific pattern (TP1) involving
the comparative construction (e.g., more than 10).
All these patterns in fact point to a missing category
for cardinals in the lexicon: they are only associated
with determiner TAG trees, not nouns, and therefore
fail to combine with prepositions (e.g., of 1991, than
10) and with proper names (e.g., November 1).
For proper names (NNP), dates also show up be-
cause months are tagged as proper names (TP3,TP4)
as well as addresses TP5:
TP5 NNP(“,”,“,”) Brooklyn, n.y.,
For prepositions (IN), we find, in addition to the
TP1-TP2, the following two main patterns:
</bodyText>
<equation confidence="0.699913">
TP6 DT(IN) those with, some of
TP7 RB(IN) just under, little more
</equation>
<bodyText confidence="0.999561285714286">
Pattern TP6 points to a missing entry for words
such as those and some which are categorised in the
lexicon as determiners but not as nouns. TP7 points
to a mismatch between the SR data and the format
expected by the generator: while the latter expects
the structure IN(RB), the input format provided by
the SR Task is RB(IN).
</bodyText>
<subsectionHeader confidence="0.999892">
4.4 Improving Generation Using the Results of
Error Mining
</subsectionHeader>
<bodyText confidence="0.999923421052632">
Table 2 shows how implementing some of the cor-
rections suggested by error mining impacts the num-
ber of NP chunks (size 4) that can be generated. In
this experiment, the total number of input (NP) de-
pendency trees is 24995. Before error mining, gen-
eration failed on 33% of these input. Correcting
the erroneous class inherited by all conjunction trees
mentioned in Section 4.3.1 brings generation failure
down to 26%. Converting the input data to the cor-
rect input format to resolve the mismatch induced
by possessive ’s (cf. Section 4.3.1) reduce gener-
ation failure to 21%6 and combining both correc-
tions results in a failure rate of 13%. In other words,
error mining permits quickly identifying two issues
which, once corrected, reduces generation failure by
20 points.
When mining on clause size chunks, other mis-
matches were identified such as in particular, mis-
matches introduced by subjects and auxiliaries:
</bodyText>
<footnote confidence="0.9864775">
6For NP of size 4, 3264 structures with possessive ’s were
rewritten.
</footnote>
<table confidence="0.986598666666667">
NP 4 Before After
SR Data 8361 6511
Rewritten SR Data 5255 3401
</table>
<tableCaption confidence="0.919046">
Table 2: Diminishing the number of errors using in-
</tableCaption>
<bodyText confidence="0.984790636363636">
formation from error mining. The table compares
the number of failures on NP chunks of size 4 be-
fore (first row) and after (second row) rewriting the
SR data to the format expected by our generator and
before (second column) and after (third column) cor-
recting the grammar and lexicon errors discussed in
Section 4.3.1
while our generator expects both the subject and the
auxiliary to be children of the verb, the SR data rep-
resent the subject and the verb as children of the aux-
iliary.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99486675">
We now relate our proposal (i) to previous proposals
on error mining and (ii) to the use of error mining in
natural language generation.
Previous work on error mining. (van Noord,
2004) initiated error mining on parsing results with
a very simple approach computing the parsability
rate of each n-gram in a very large corpus. The
parsability rate of an n-gram wi ... wn is the ratio
</bodyText>
<equation confidence="0.982324">
C(wi ... wn |OK)
R(wi ... wn) = C(wi...wn) with C(wi ... wn)
</equation>
<bodyText confidence="0.999875368421053">
the number of sentences in which the n-gram
wi ... wn occurs and C(wi ... wn I OK) the num-
ber of sentences containing wi ... wn which could
be parsed. The corpus is stored in a suffix array
and the sorted suffixes are used to compute the fre-
quency of each n-grams in the total corpus and in the
corpus of parsed sentences. The approach was later
extended and refined in (Sagot and de la Clergerie,
2006) and (de Kok et al., 2009) whereby (Sagot and
de la Clergerie, 2006) defines a suspicion rate for n-
grams which takes into account the number of occur-
rences of a given word form and iteratively defines
the suspicion rate of each word form in a sentence
based on the suspicion rate of this word form in the
corpus; (de Kok et al., 2009) combined the iterative
error mining proposed by (Sagot and de la Clergerie,
2006) with expansion of forms to n-grams of words
and POS tags of arbitrary length.
Our approach differs from these previous ap-
</bodyText>
<page confidence="0.993928">
598
</page>
<bodyText confidence="0.998185244897959">
proaches in several ways. First, error mining is per-
formed on trees. Second, it can be parameterised to
use any combination of POS tag, dependency and/or
word form information. Third, it is applied to gener-
ation input rather than parsing output. Typically, the
input to surface realisation is a structured represen-
tation (i.e., a flat semantic representation, a first or-
der logic formula or a dependency tree) rather than a
string. Mining these structured representations thus
permits identifying causes of undergeneration in sur-
face realisation systems.
Error Mining for Generation Not much work
has been done on mining the results of surface re-
alisers. Nonetheless, (Gardent and Kow, 2007) de-
scribes an error mining approach which works on
the output of surface realisation (the generated sen-
tences), manually separates correct from incorrect
output and looks for derivation items which system-
atically occur in incorrect output but not in correct
ones. In contrast, our approach works on the input
to surface realisation, automatically separates cor-
rect from incorrect items using surface realisation
and targets the most likely sources of errors rather
than the absolute ones.
More generally, our approach is the first to our
knowledge, which mines a surface realiser for un-
dergeneration. Indeed, apart from (Gardent and
Kow, 2007), most previous work on surface reali-
sation evaluation has focused on evaluating the per-
formance and the coverage of surface realisers. Ap-
proaches based on reversible grammars (Carroll et
al., 1999) have used the semantic formulae output
by parsing to evaluate the coverage and performance
of their realiser; similarly, (Gardent et al., 2010) de-
veloped a tool called GenSem which traverses the
grammar to produce flat semantic representations
and thereby provide a benchmark for performance
and coverage evaluation. In both cases however, be-
cause it is produced using the grammar exploited by
the surface realiser, the input produced can only be
used to test for overgeneration (and performance) .
(Callaway, 2003) avoids this shortcoming by con-
verting the Penn Treebank to the format expected by
his realiser. However, this involves manually iden-
tifying the mismatches between two formats much
like symbolic systems did in the Generation Chal-
lenge SR Task. The error mining approach we pro-
pose helps identifying such mismatches automati-
cally.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982257142857">
Previous work on error mining has focused on appli-
cations (parsing) where the input data is sequential
working mainly on words and part of speech tags.
In this paper, we proposed a novel approach to error
mining which permits mining trees. We applied it
to the input data provided by the Generation Chal-
lenge SR Task. And we showed that this supports
the identification of gaps and errors in the grammar
and in the lexicon; and of mismatches between the
input data format and the format expected by our re-
aliser.
We applied our error mining approach to the in-
put of a surface realiser to identify the most likely
sources of undergeneration. We plan to also ex-
plore how it can be used to detect the most likely
sources of overgeneration based on the output of
this surface realiser on the SR Task data. Using the
Penn Treebank sentences associated with each SR
Task dependency tree, we will create the two tree
sets necessary to support error mining by dividing
the set of trees output by the surface realiser into a
set of trees (FAIL) associated with overgeneration
(the generated sentences do not match the original
sentences) and a set of trees (SUCCESS) associated
with success (the generated sentence matches the
original sentences). Exactly which tree should popu-
late the SUCCESS and FAIL set is an open question.
The various evaluation metrics used by the SR Task
(BLEU, NIST, METEOR and TER) could be used
to determine a threshold under which an output is
considered incorrect (and thus classificed as FAIL).
Alternatively, a strict matching might be required.
Similarly, since the surface realiser is non determin-
istic, the number of output trees to be kept will need
to be experimented with.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999451833333333">
We would like to thank Cl´ement Jacq for useful dis-
cussions on the hybrid tree miner algorithm. The
research presented in this paper was partially sup-
ported by the European Fund for Regional Develop-
ment within the framework of the INTERREG IV A
Allegro Project.
</bodyText>
<page confidence="0.998125">
599
</page>
<sectionHeader confidence="0.99834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999269380952381">
Anja Belz, Michael White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proceedings of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. In Proceedings of the
18th International Joint Conference on Artificial Intel-
ligence, pages 811–817, Acapulco, Mexico.
John Carroll, Ann Copestake, Dan Flickinger, and Vik-
tor Pazna´nski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of the
7th European Workshop on Natural Language Gener-
ation, pages 86–95, Toulouse, France.
Yun Chi, Yirong Yang, and Richard R. Muntz. 2004.
Hybridtreeminer: An efficient algorithm for mining
frequent rooted trees and free trees using canonical
form. In Proceedings of the 16th International Con-
ference on and Statistical Database Management (SS-
DBM), pages 11–20, Santorini Island, Greece. IEEE
Computer Society.
Dani¨el de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error mining
in parsing results. In Proceedings of the 2009 Work-
shop on Grammar Engineering Across Frameworks
(GEAF 2009), pages 71–79, Suntec, Singapore. As-
sociation for Computational Linguistics.
Claire Gardent and Eric Kow. 2007. Spotting overgen-
eration suspect. In Proceedings of the 11th European
Workshop on Natural Language Generation (ENLG),
pages 41–48, Schloss Dagstuhl, Germany.
Claire Gardent and Laura Perez-Beltrachini. 2010. Rtg
based surface realisation for tag. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING), pages 367–375, Beijing, China.
Claire Gardent, Benjamin Gottesman, and Laura Perez-
Beltrachini. 2010. Comparing the performance of
two TAG-based Surface Realisers using controlled
Grammar Traversal. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING - Poster session), pages 338–346, Beijing,
China.
Richert Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of the 16th Nordic Conference of Com-
putational Linguistics (NODALIDA), pages 105–112,
Tartu, Estonia.
Rajakrishnan Rajkumar, Dominic Espinosa, and Michael
White. 2011. The osu system for surface realization
at generation challenges 2011. In Proceedings of the
13th European Workshop on Natural Language Gen-
eration (ENLG), pages 236–238, Nancy, France.
Benoit Sagot and ´Eric de la Clergerie. 2006. Error min-
ing in parsing results. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 329–336, Sydney,
Australia.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL), pages 446–453, Barcelona, Spain.
</reference>
<page confidence="0.996639">
600
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.793844">
<title confidence="0.999063">Error Mining on Dependency Trees</title>
<author confidence="0.998237">Claire Gardent Shashi Narayan</author>
<address confidence="0.9385615">CNRS, LORIA, UMR 7503 Universit´e de Lorraine, LORIA, UMR 7503 Vandoeuvre-l`es-Nancy, F-54500, France Villers-l`es-Nancy, F-54600, France</address>
<email confidence="0.942287">claire.gardent@loria.frshashi.narayan@loria.fr</email>
<abstract confidence="0.9956645">In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Michael White</author>
<author>Dominic Espinosa</author>
<author>Eric Kow</author>
<author>Deirdre Hogan</author>
<author>Amanda Stent</author>
</authors>
<title>The first surface realisation shared task: Overview and evaluation results.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="2278" citStr="Belz et al., 2011" startWordPosition="367" endWordPosition="370">results on parsing output and shown to help improve the large scale symbolic grammars and lexicons used by the parser. However the techniques they use (e.g., suffix arrays) to enumerate and count n-grams builds on the sequential nature of a text corpus and cannot easily extend to structured data. There are some NLP applications though where the processed data is structured data such as trees or graphs and which would benefit from error mining. For instance, when generating sentences from dependency trees, as was proposed recently in the Generation Challenge Surface Realisation Task (SR Task, (Belz et al., 2011)), it would be useful to be able to apply error mining on the input trees to find the most likely causes of generation failure. In this paper, we address this issue and propose an approach that supports error mining on trees. We adapt an existing algorithm for tree mining which we then use to mine the Generation Challenge dependency trees and identify the most likely causes of generation failure. We show in particular, that this tree mining algorithm permits identifying not only errors in the grammar and the lexicon used by generation but also a few idiosyncrasies/error in the input data as we</context>
</contexts>
<marker>Belz, White, Espinosa, Kow, Hogan, Stent, 2011</marker>
<rawString>Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>811--817</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="28932" citStr="Callaway, 2003" startWordPosition="4996" endWordPosition="4997">d the coverage of surface realisers. Approaches based on reversible grammars (Carroll et al., 1999) have used the semantic formulae output by parsing to evaluate the coverage and performance of their realiser; similarly, (Gardent et al., 2010) developed a tool called GenSem which traverses the grammar to produce flat semantic representations and thereby provide a benchmark for performance and coverage evaluation. In both cases however, because it is produced using the grammar exploited by the surface realiser, the input produced can only be used to test for overgeneration (and performance) . (Callaway, 2003) avoids this shortcoming by converting the Penn Treebank to the format expected by his realiser. However, this involves manually identifying the mismatches between two formats much like symbolic systems did in the Generation Challenge SR Task. The error mining approach we propose helps identifying such mismatches automatically. 6 Conclusion Previous work on error mining has focused on applications (parsing) where the input data is sequential working mainly on words and part of speech tags. In this paper, we proposed a novel approach to error mining which permits mining trees. We applied it to </context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>Charles B. Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, pages 811–817, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Viktor Pazna´nski</author>
</authors>
<title>An efficient chart generator for (semi-)lexicalist grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th European Workshop on Natural Language Generation,</booktitle>
<pages>86--95</pages>
<location>Toulouse, France.</location>
<marker>Carroll, Copestake, Flickinger, Pazna´nski, 1999</marker>
<rawString>John Carroll, Ann Copestake, Dan Flickinger, and Viktor Pazna´nski. 1999. An efficient chart generator for (semi-)lexicalist grammars. In Proceedings of the 7th European Workshop on Natural Language Generation, pages 86–95, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Chi</author>
<author>Yirong Yang</author>
<author>Richard R Muntz</author>
</authors>
<title>Hybridtreeminer: An efficient algorithm for mining frequent rooted trees and free trees using canonical form.</title>
<date>2004</date>
<booktitle>In Proceedings of the 16th International Conference on and Statistical Database Management (SSDBM),</booktitle>
<pages>11--20</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Santorini Island, Greece.</location>
<contexts>
<context position="3755" citStr="Chi et al., 2004" startWordPosition="608" endWordPosition="611">these mismatches i.e., the fact that the input provided by the SR task fails to match the input expected by the symbolic generation systems (Belz et al., 2011). The paper is structured as follows. Section 2 presents the HybridTreeMiner algorithm, a complete and computationally efficient algorithm developed 592 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 592–600, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Four unordered labelled trees. The rightmost is in Breadth-First Canonical Form by (Chi et al., 2004) for discovering frequently occurring subtrees in a database of labelled unordered trees. Section 3 shows how to adapt this algorithm to mine the SR dependency trees for subtrees with high suspicion rate. Section 4 presents an experiment we made using the resulting tree mining algorithm on SR dependency trees and summarises the results. Section 5 discusses related work. Section 6 concludes. 2 Mining Trees Mining for frequent subtrees is an important problem that has many applications such as XML data mining, web usage analysis and RNA classification. The HybridTreeMiner (HTM) algorithm present</context>
<context position="5497" citStr="Chi et al., 2004" startWordPosition="895" endWordPosition="898">eadth-First Canonical Form). In that way, distinct instantiations of the same unordered trees have a unique representation. Second, the subtrees of the BFCF trees are enumerated in increasing size order using two tree operations called join and extension and their support (the number of trees in the database that contains each subtree) is recorded. In effect, the algorithm builds an enumeration tree whose nodes are the possible subtrees of T and such that, at depth d of this enumeration tree, all possible frequent subtrees consisting of d nodes are listed. 1For a more complete definition see (Chi et al., 2004). The BFCF canonical form of an unordered tree is an ordered tree t such that t has the smallest breath-first canonical string (BFCS) encoding according to lexicographic order. The BFCS encoding of a tree is obtained by breadth-first traversal of the tree, recording the string labelling each node, “$” to separate siblings with distinct parents and “#” to represent the end of the tree2. For instance, the BFCS encodings of the four trees shown in Figure 1 are ’A$BB$C$DC#’, ’A$BB$C$CD#’, ’A$BB$DC$C#’ and ’A$BB$CD$C#’ respectively. Hence, the rightmost tree is the BFCF of all four trees. The join </context>
</contexts>
<marker>Chi, Yang, Muntz, 2004</marker>
<rawString>Yun Chi, Yirong Yang, and Richard R. Muntz. 2004. Hybridtreeminer: An efficient algorithm for mining frequent rooted trees and free trees using canonical form. In Proceedings of the 16th International Conference on and Statistical Database Management (SSDBM), pages 11–20, Santorini Island, Greece. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianqiang Ma Dani¨el de Kok</author>
<author>Gertjan van Noord</author>
</authors>
<title>A generalized method for iterative error mining in parsing results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks (GEAF</booktitle>
<pages>71--79</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<marker>Dani¨el de Kok, van Noord, 2009</marker>
<rawString>Dani¨el de Kok, Jianqiang Ma, and Gertjan van Noord. 2009. A generalized method for iterative error mining in parsing results. In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks (GEAF 2009), pages 71–79, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Eric Kow</author>
</authors>
<title>Spotting overgeneration suspect.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>41--48</pages>
<location>Schloss Dagstuhl, Germany.</location>
<contexts>
<context position="27575" citStr="Gardent and Kow, 2007" startWordPosition="4783" endWordPosition="4786">Second, it can be parameterised to use any combination of POS tag, dependency and/or word form information. Third, it is applied to generation input rather than parsing output. Typically, the input to surface realisation is a structured representation (i.e., a flat semantic representation, a first order logic formula or a dependency tree) rather than a string. Mining these structured representations thus permits identifying causes of undergeneration in surface realisation systems. Error Mining for Generation Not much work has been done on mining the results of surface realisers. Nonetheless, (Gardent and Kow, 2007) describes an error mining approach which works on the output of surface realisation (the generated sentences), manually separates correct from incorrect output and looks for derivation items which systematically occur in incorrect output but not in correct ones. In contrast, our approach works on the input to surface realisation, automatically separates correct from incorrect items using surface realisation and targets the most likely sources of errors rather than the absolute ones. More generally, our approach is the first to our knowledge, which mines a surface realiser for undergeneration.</context>
</contexts>
<marker>Gardent, Kow, 2007</marker>
<rawString>Claire Gardent and Eric Kow. 2007. Spotting overgeneration suspect. In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG), pages 41–48, Schloss Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Laura Perez-Beltrachini</author>
</authors>
<title>Rtg based surface realisation for tag.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>367--375</pages>
<location>Beijing, China.</location>
<contexts>
<context position="14933" citStr="Gardent and Perez-Beltrachini, 2010" startWordPosition="2613" endWordPosition="2616">nodes and word forms was provided by the organisers. The surface realiser used is a system based on a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG) for English extended with a unification based compositional semantics. Both the grammars and the lexicon were developed in view of the Generation Challenge and the data provided by this challenge was used as a means to debug and extend the system. Unknown words are assigned a default TAG family/tree based on the part of speech they are associated with in the SR data. The surface realisation algorithm extends the algorithm proposed in (Gardent and Perez-Beltrachini, 2010) and adapts it to work on the SR dependency input rather than on flat semantic representations. 4.2 Experimental Setup To facilitate interpretation, we first chunked the input data in NPs, PPs and Clauses and performed error mining on the resulting sets of data. The chunking was performed by retrieving from the Penn Treebank (PTB), for each phrase type, the yields of the constituents of that type and by using the alignment between words and dependency tree nodes provided by the organisers of the SR Task. For instance, given the sentence “The most troublesome report may be the August merchandis</context>
</contexts>
<marker>Gardent, Perez-Beltrachini, 2010</marker>
<rawString>Claire Gardent and Laura Perez-Beltrachini. 2010. Rtg based surface realisation for tag. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 367–375, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Benjamin Gottesman</author>
<author>Laura PerezBeltrachini</author>
</authors>
<title>Comparing the performance of two TAG-based Surface Realisers using controlled Grammar Traversal.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING - Poster session),</booktitle>
<pages>338--346</pages>
<location>Beijing, China.</location>
<contexts>
<context position="28560" citStr="Gardent et al., 2010" startWordPosition="4936" endWordPosition="4939"> from incorrect items using surface realisation and targets the most likely sources of errors rather than the absolute ones. More generally, our approach is the first to our knowledge, which mines a surface realiser for undergeneration. Indeed, apart from (Gardent and Kow, 2007), most previous work on surface realisation evaluation has focused on evaluating the performance and the coverage of surface realisers. Approaches based on reversible grammars (Carroll et al., 1999) have used the semantic formulae output by parsing to evaluate the coverage and performance of their realiser; similarly, (Gardent et al., 2010) developed a tool called GenSem which traverses the grammar to produce flat semantic representations and thereby provide a benchmark for performance and coverage evaluation. In both cases however, because it is produced using the grammar exploited by the surface realiser, the input produced can only be used to test for overgeneration (and performance) . (Callaway, 2003) avoids this shortcoming by converting the Penn Treebank to the format expected by his realiser. However, this involves manually identifying the mismatches between two formats much like symbolic systems did in the Generation Cha</context>
</contexts>
<marker>Gardent, Gottesman, PerezBeltrachini, 2010</marker>
<rawString>Claire Gardent, Benjamin Gottesman, and Laura PerezBeltrachini. 2010. Comparing the performance of two TAG-based Surface Realisers using controlled Grammar Traversal. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING - Poster session), pages 338–346, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richert Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA),</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia.</location>
<contexts>
<context position="13761" citStr="Johansson and Nugues, 2007" startWordPosition="2420" endWordPosition="2424"> allows us to process e.g., all NP chunks of size 4 and 6 present in the SR data (roughly 60 000 trees) in roughly 20 minutes on a PC. 4 Experiment and Results Using the input data provided by the Generation Challenge SR Task, we applied the error mining algorithm described in the preceding Section to debug and extend a symbolic surface realiser developed for this task. 4.1 Input Data and Surface Realisation System The shallow input data provided by the SR Task was obtained from the Penn Treebank using the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter, (Johansson and Nugues, 2007)). It consists of a set of unordered labelled syntactic dependency trees whose nodes are labelled with word forms, part of speech categories, partial morphosyntactic information such as tense and number and, in some cases, a sense tag identifier. The edges are labelled with the syntactic labels provided by the Pennconverter. All words (including punctuation) of the original sen4ETM needs to store all (n-1)-node trees in queues before producing n-node trees. 595 tence are represented by a node in the tree and the alignment between nodes and word forms was provided by the organisers. The surface</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richert Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA), pages 105–112, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajakrishnan Rajkumar</author>
<author>Dominic Espinosa</author>
<author>Michael White</author>
</authors>
<title>The osu system for surface realization at generation challenges 2011.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>236--238</pages>
<location>Nancy, France.</location>
<contexts>
<context position="18362" citStr="Rajkumar et al., 2011" startWordPosition="3193" endWordPosition="3196">cion rate indicate that there are unresolved issues with, in decreasing order of suspicion rate, cardinal numbers (CD), proper names (NNP), nouns (NN), prepositions (IN) and determiners (DT). The highest ranking category (POS5) points to a mismatch between the representation of genitive NPs (e.g., John’s father) in the SR Task data and in the grammar. While our generator expects the representation of ‘John’s father’ to be FATHER(“S”(JOHN)), the structure provided by the SR Task is FATHER(JOHN(“S”)). Hence whenever a possessive appears in the input data, generation fails. This is in line with (Rajkumar et al., 2011)’s finding that the logical forms expected by their system for possessives differed from the shared task inputs. 5In the Penn Treebank, the POS tag is the category assigned to possessive ’s. 596 POS Sus Sup Fail Pass POS 0.99 0.38 3237 1 CC 0.99 0.21 1774 9 CD 0.39 0.16 1419 2148 NNP 0.35 0.32 2749 5014 NN 0.30 0.81 6798 15663 IN 0.30 0.16 1355 3128 DT 0.09 0.12 1079 10254 Table 1: Error Mining on POS tags with frequency cutoff 0.1 and displaying only trees of size 1 sorted by decreasing suspicion rate (Sus) The second highest ranked category is CC for coordinations. In this case, error mining</context>
</contexts>
<marker>Rajkumar, Espinosa, White, 2011</marker>
<rawString>Rajakrishnan Rajkumar, Dominic Espinosa, and Michael White. 2011. The osu system for surface realization at generation challenges 2011. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 236–238, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
<author>´Eric de la Clergerie</author>
</authors>
<title>Error mining in parsing results.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>329--336</pages>
<location>Sydney, Australia.</location>
<marker>Sagot, Clergerie, 2006</marker>
<rawString>Benoit Sagot and ´Eric de la Clergerie. 2006. Error mining in parsing results. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL), pages 329–336, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Error mining for widecoverage grammar engineering.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>446--453</pages>
<location>Barcelona,</location>
<marker>van Noord, 2004</marker>
<rawString>Gertjan van Noord. 2004. Error mining for widecoverage grammar engineering. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL), pages 446–453, Barcelona, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>