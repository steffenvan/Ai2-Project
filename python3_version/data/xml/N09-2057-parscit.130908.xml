<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001488">
<title confidence="0.998442">
Tree Linearization in English:
Improving Language Model Based Approaches
</title>
<author confidence="0.742995">
Katja Filippova and Michael Strube
</author>
<affiliation confidence="0.575844">
EML Research gGmbH
</affiliation>
<address confidence="0.855983">
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
</address>
<email confidence="0.978085">
http://www.eml-research.de/nlp
</email>
<sectionHeader confidence="0.99522" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957692307692">
We compare two approaches to dependency
tree linearization, a task which arises in many
NLP applications. The first one is the widely
used ’overgenerate and rank’ approach which
relies exclusively on a trigram language model
(LM); the second one combines language
modeling with a maximum entropy classifier
trained on a range of linguistic features. The
results provide strong support for the com-
bined method and show that trigram LMs are
appropriate for phrase linearization while on
the clause level a richer representation is nec-
essary to achieve comparable performance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999861833333333">
To date, many natural language processing appli-
cations rely on syntactic representations and also
modify them by compressing, fusing, or translating
into a different language. A syntactic tree emerg-
ing as a result of such operations has to be lin-
earized to a string of words before it can be out-
put to the end-user. The simple and most widely
used trigram LM has become a standard tool for
tree linearization in English (Langkilde &amp; Knight,
1998). For languages with less rigid word order,
LM-based approaches have been shown to perform
poorly (e.g., Marsi &amp; Krahmer (2005) for Dutch),
and methods relying on a range of linguistic fea-
tures have been successfully applied instead (see
Uchimoto et al. (2000) and Ringger et al. (2004),
Filippova &amp; Strube (2007) for Japanese and German
resp.). To our knowledge, none of the linearization
studies have compared a LM-based method with
an alternative. Thus, it would be of interest to
draw such a comparison, especially on English data,
where LMs are usually expected to work well.
As an improvement to the LM-based approach,
we propose a combined method which distinguishes
between the phrase and the clause levels:
</bodyText>
<listItem confidence="0.973710666666667">
• it relies on a trigram LM to order words within
phrases;
• it finds the order of clause constituents (i.e.,
constituents dependent on a finite verb) with a
maximum entropy classifier trained on a range
of linguistic features.
</listItem>
<bodyText confidence="0.999211875">
We show that such a differentiated approach is ben-
eficial and that the proposed combination outper-
forms the method which relies solely on a LM.
Hence, our results challenge the widespread attitude
that trigram LMs provide an appropriate way to lin-
earize syntactic trees in English but also indicate
that they perform well in linearizing subtrees cor-
responding to phrases.
</bodyText>
<sectionHeader confidence="0.973885" genericHeader="method">
2 LM-based Approach
</sectionHeader>
<bodyText confidence="0.999935">
Trigram models are easy to build and use, and it has
been shown that more sophisticated n-gram models
(e.g., with higher n, complex smoothing techniques,
skipping, clustering or caching) are often not worth
the effort of implementing them due to data sparse-
ness and other issues (Goodman, 2001). This ex-
plains the popularity of trigram LMs in a variety
of NLP tasks (Jurafsky &amp; Martin, 2008), in partic-
ular, in tree linearization where they have become
</bodyText>
<page confidence="0.982504">
225
</page>
<note confidence="0.506646">
Proceedings of NAACL HLT 2009: Short Papers, pages 225–228,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
brothers
</note>
<figureCaption confidence="0.897817">
Figure 1: A tree of the noun phrase all the brothers of my
neighbor
</figureCaption>
<bodyText confidence="0.999920607142857">
de facto the standard tree linearization tool in ac-
cordance with the ‘overgenerate and rank’ principle:
given a syntactic tree, one needs to consider all pos-
sible linearizations and then choose the one with the
lowest entropy. Given aprojective dependency tree1,
all linearizations can be found recursively by gener-
ating permutations of a node and its children. Unfor-
tunately, the number of possible permutations grows
factorially with the branching factor. Hence it is
highly desirable to prohibit generation of clearly un-
acceptable permutations by putting hard constraints
encoded in the English grammar. The constraints
which we implement in our study are the following:
determiners, possessives, quantifiers and noun or ad-
jective modifiers always precede their heads. Con-
junctions, coordinated elements, prepositional ob-
jects always follow their heads. These constraints
allow us to limit, e.g., the total of 96 (2 × 2 × 4!)
possibilities for the tree corresponding to the phrase
all the brothers ofmy neighbor (see Figure 1) to only
two (all the brothers of my neighbor, the all brothers
of my neighbor).
Still, even with such constraints, in some cases the
list of possible linearizations is too long and has to
be reduced to the first N, where N is supposed to be
sufficiently large. In our experiments we break the
permutation generation process if the limit of 20,000
variants is reached.
</bodyText>
<sectionHeader confidence="0.985283" genericHeader="method">
3 Combined Approach
</sectionHeader>
<bodyText confidence="0.9939595">
The LM approach described above has at least two
disadvantages: (1) long distance dependencies are
not captured, and (2) the list of all possible lineariza-
tions can be huge which makes the search for the
</bodyText>
<footnote confidence="0.5475415">
1Note that a phrase structure tree can be converted into a
dependency tree, and some PCFG parsers provide this option.
</footnote>
<bodyText confidence="0.995881051282051">
best string unfeasible. However, our combined ap-
proach is based on the premise that trigram LMs are
well-suited for finding the order within NPs, PPs and
other phrases where the head is not a finite verb.
E.g., given a noun modified by the words big, red
and the, a LM can reliably rank the correct order
higher than incorrect ones ( the big red N vs. the red
big N, etc.).
Next, on the clause level, for every finite verb in
the tree we find the order of its dependents using the
method which we originally developed for German
(Filippova &amp; Strube, 2007), which utilizes a range
of such linguistic features as PoS tag, syntactic role,
length in words, pronominalization, semantic class,
etc.2 For the experiments presented in this paper, we
train two maximum entropy classifiers on all but the
semantic features:
1. The first classifier determines the best starting
point for a sentence: for each constituent de-
pendent on the verb it returns the probability of
this constituent being the first one in a sentence.
The subject and also adjuncts (e.g. temporal ad-
juncts like yesterday) are usually found in the
beginning of the sentence.
2. The second classifier is trained to determine
whether the precedence relation holds between
two adjacent constituents and is applied to all
constituents but the one selected by the first
classifier. The precedence relation defined by
this classifier has been shown to be transitive
and thus can be used to sort randomly ordered
constituents. Note that we do not need to con-
sider all possible orders to find the best one.
Once the order within clause constituents as well as
the order among them is found, the verb is placed
right after the subject. The verb placing step com-
pletes the linearization process.
The need for two distinct classifiers can be illus-
trated with the following example:
</bodyText>
<listItem confidence="0.5764">
(1) a [Earlier today] [she] sent [him] [an email].
</listItem>
<bodyText confidence="0.2824985">
b [She] sent [him] [an email] [earlier today].
c *[She] sent [earlier today] [him] [an email].
</bodyText>
<footnote confidence="0.8100575">
2See the cited paper for the full list of features and imple-
mentation details.
</footnote>
<figure confidence="0.980057625">
predet det
prep
the of
pobj
neighbor
poss
my
all
</figure>
<page confidence="0.99477">
226
</page>
<bodyText confidence="0.999947357142857">
(1a,b) are grammatical while (1c) is hardly accept-
able, and no simple precedence rule can be learned
from pairs of constituents in (1a) and (1b): the tem-
poral adjunct earlier today can precede or follow
each of the other constituents dependent on the verb
(she, him, an email). Thus, the classifier which
determines the precedence relation is not enough.
However, an adequate rule can be inferred with
an additional classifier trained to find good starting
points: a temporal adjunct may appear as the first
constituent in a sentence; if it is not chosen for this
position, it should be preceded by the pronominal-
ized subject (she), the indirect object (him) and the
short non-pronominalized object (an email).
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.994076">
The goal of our experiments is to check the follow-
ing hypotheses:
</bodyText>
<listItem confidence="0.968088857142857">
1. That trigram LMs are well-suited for phrase
linearization.
2. That there is a considerable drop in perfor-
mance when one uses them for linearization on
the clause level.
3. That an approach which uses a richer represen-
tation on the clause level is more appropriate.
</listItem>
<subsectionHeader confidence="0.96484">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999980416666666">
We take a subset of the TIPSTER3 corpus – all Wall
Street Journal articles from the period of 1987-92
(approx. 72 mill. words) – and automatically anno-
tate them with sentence boundaries, part of speech
tags and dependency relations using the Stanford
parser (Klein &amp; Manning, 2003). We reserve a
small subset of about 600 articles (340,000 words)
for testing and use the rest to build a trigram LM
with the CMU toolkit (Clarkson &amp; Rosenfeld, 1997,
with Good-Turing smoothing and vocabulary size of
30,000). To train the maximum entropy classifiers
we use about 41,000 sentences.
</bodyText>
<subsectionHeader confidence="0.915253">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9991665">
To test the trigram-based approach, we generate all
possible permutations of clause constituents, place
</bodyText>
<footnote confidence="0.986209333333333">
3Description at http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC93T3A.
</footnote>
<bodyText confidence="0.999468666666667">
the verb right after the subject and then rank the re-
sulting strings with the LM taking the information
on sentence boundaries into account. To test the
combined approach, we find the best candidate for
the first position in the clause, then put the remain-
ing constituents in a random order, and finally sort
them by consulting the second classifier.
The purpose of the evaluation is to assess how
good a method is at reproducing the input from its
dependency tree. We separately evaluate the perfor-
mance on the phrase and the clause levels. When
comparing the two methods on the clause level, we
take the clause constituents as they are presented
in the input sentence. Although English allows for
some minor variation in word order and it might
happen that the generated order is not necessarily
wrong if different from the original one, we do not
expect this to happen often and evaluate the perfor-
mance rigorously: only the original order counts as
the correct one. The default evaluation metric is per-
phrase/per-clause accuracy:
</bodyText>
<equation confidence="0.945195333333333">
|correct|
acc =
|total|
</equation>
<bodyText confidence="0.992481">
Other metrics we use to measure how different a
generated order of N elements is from the correct
one are:
</bodyText>
<listItem confidence="0.788555222222222">
1. Kendall’s τ, τ = 1 − 4t
,(,−1) where t is
the minimum number of interchanges of con-
secutive elements to achieve the right order
(Kendall, 1938; Lapata, 2006).
2. Edit distance related di, di = 1 − &apos; where m
is the minimum number of deletions combined
with insertions to get to the right order (Ringger
et al., 2004).
</listItem>
<bodyText confidence="0.997254625">
E.g., on the phrase level, the incorrectly generated
phrase the all brothers of my neighbor (&apos;1-0-2-3-4-
5&apos;)
&apos;1-0-2-3-4-
5&apos;) gets τ = 0.87, di = 0.83. Likewise, given the
input sentence from (1a), the incorrectly generated
order of the four clause constituents in (1c) – ’1-0-
2-3’ – gets τ of 0.67 and di of 0.75.
</bodyText>
<subsectionHeader confidence="0.921105">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999932">
The results of the experiments on the phrase and the
clause levels are presented in Tables 1 and 2 respec-
tively. From the total of 5,000 phrases, 55 (about
</bodyText>
<page confidence="0.992587">
227
</page>
<bodyText confidence="0.999919">
1%) were discarded because the number of admis-
sible linearizations exceeded the limit of 20,000. In
the first row of Table 1 we give the results for cases
where, with all constraints applied, there were still
several possible linearizations (non-triv; 1,797); the
second row is for all phrases which were longer than
one word (&gt; 1; 2,791); the bottom row presents the
results for the total of 4,945 phrases (all).
</bodyText>
<table confidence="0.940931">
acc r di
non-triv 76% 0.85 0.94
&gt; 1 85% 0.90 0.96
all 91% 0.94 0.98
</table>
<tableCaption confidence="0.99986">
Table 1: Results of the trigram method on the phrase level
</tableCaption>
<bodyText confidence="0.992294833333333">
Table 2 presents the results of the trigram-based
(TRIGRAM) and combined (COMBINED) methods on
the clause level. Here, we filtered out trivial cases
and considered only clauses which had at least two
constituents dependent on the verb (approx. 5,000
clauses in total).
</bodyText>
<table confidence="0.957487">
acc r di
TRIGRAM 49% 0.49 0.81
COMBINED 67% 0.71 0.88
</table>
<tableCaption confidence="0.999437">
Table 2: Results of the two methods on the clause level
</tableCaption>
<subsectionHeader confidence="0.869891">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999979176470588">
The difference in accuracy between the performance
of the trigram model on the phrase and the clause
level is considerable – 76% vs. 49%. The accuracy
of 76% is remarkable given that the average length
of phrases which counted as non-triv is 6.2 words,
whereas the average clause length in constituents is
3.3. This statistically significant difference in per-
formance supports our hypothesis that the ’overgen-
erate and rank’ approach advocated in earlier studies
is more adequate for finding the optimal order within
phrases. The r value of 0.85 also indicates that many
of the wrong phrase linearizations were near misses.
On the clause level, where long distance dependen-
cies are frequent, an approach which takes a range
of grammatical features into account is more appro-
priate – this is confirmed by the significantly better
results of the combined method (67%).
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999225">
We investigated two tree linearization methods in
English: the mainstream trigram-based approach
and the one which combines a trigram LM on the
phrase level with two classifiers trained on a range
of linguistic features on the clause level. The results
demonstrate (1) that the combined approach repro-
duces the word order more accurately, and (2) that
the performance of the trigram LM-based method on
phrases is significantly better than on clauses.
Acknowledgments: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). We would like to thank the
anonymous reviewers for their feedback.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888967741936">
Clarkson, P. &amp; R. Rosenfeld (1997). Statistical language
modeling using the CMU-Cambridge toolkit. In Proc.
ofEUROSPEECH-97, pp. 2707–2710.
Filippova, K. &amp; M. Strube (2007). Generating constituent
order in German clauses. In Proc. ofACL-07, pp. 320–
327.
Goodman, J. T. (2001). A bit of progress in language
modeling. Computer Speech and Language, pp. 403–
434.
Jurafsky, D. &amp; J. H. Martin (2008). Speech and Language
Processing. Upper Saddle River, N.J.: Prentice Hall.
Kendall, M. G. (1938). A new measure of rank correla-
tion. Biometrika, 30:81–93.
Klein, D. &amp; C. D. Manning (2003). Accurate unlexical-
ized parsing. In Proc. ofACL-03, pp. 423–430.
Langkilde, I. &amp; K. Knight (1998). Generation that ex-
ploits corpus-based statistical knowledge. In Proc. of
COLING-ACL-98, pp. 704–710.
Lapata, M. (2006). Automatic evaluation of information
ordering: Kendall’s tau. Computational Linguistics,
32(4):471–484.
Marsi, E. &amp; E. Krahmer (2005). Explorations in sentence
fusion. In Proc. of ENLG-05, pp. 109–117.
Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets
&amp; S. Corston-Oliver (2004). Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In Proc. of COLING-04, pp.
673–679.
Uchimoto, K., M. Murata, Q. Ma, S. Sekine &amp; H. Isahara
(2000). Word order acquisition from corpora. In Proc.
of COLING-00, pp. 871–877.
</reference>
<page confidence="0.99767">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497133">
<title confidence="0.9991325">Tree Linearization in Improving Language Model Based Approaches</title>
<author confidence="0.87292">Filippova</author>
<affiliation confidence="0.8335885">EML Research Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.991655">69118 Heidelberg,</address>
<web confidence="0.876891">http://www.eml-research.de/nlp</web>
<abstract confidence="0.9939535">We compare two approaches to dependency tree linearization, a task which arises in many NLP applications. The first one is the widely used ’overgenerate and rank’ approach which relies exclusively on a trigram language model (LM); the second one combines language modeling with a maximum entropy classifier trained on a range of linguistic features. The results provide strong support for the combined method and show that trigram LMs are appropriate for phrase linearization while on the clause level a richer representation is necessary to achieve comparable performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proc. ofEUROSPEECH-97,</booktitle>
<pages>2707--2710</pages>
<contexts>
<context position="8629" citStr="Clarkson &amp; Rosenfeld, 1997" startWordPosition="1417" endWordPosition="1420">p in performance when one uses them for linearization on the clause level. 3. That an approach which uses a richer representation on the clause level is more appropriate. 4.1 Data We take a subset of the TIPSTER3 corpus – all Wall Street Journal articles from the period of 1987-92 (approx. 72 mill. words) – and automatically annotate them with sentence boundaries, part of speech tags and dependency relations using the Stanford parser (Klein &amp; Manning, 2003). We reserve a small subset of about 600 articles (340,000 words) for testing and use the rest to build a trigram LM with the CMU toolkit (Clarkson &amp; Rosenfeld, 1997, with Good-Turing smoothing and vocabulary size of 30,000). To train the maximum entropy classifiers we use about 41,000 sentences. 4.2 Evaluation To test the trigram-based approach, we generate all possible permutations of clause constituents, place 3Description at http://www.ldc.upenn.edu/ Catalog/CatalogEntry.jsp?catalogId= LDC93T3A. the verb right after the subject and then rank the resulting strings with the LM taking the information on sentence boundaries into account. To test the combined approach, we find the best candidate for the first position in the clause, then put the remaining </context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Clarkson, P. &amp; R. Rosenfeld (1997). Statistical language modeling using the CMU-Cambridge toolkit. In Proc. ofEUROSPEECH-97, pp. 2707–2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Filippova</author>
<author>M Strube</author>
</authors>
<title>Generating constituent order in German clauses.</title>
<date>2007</date>
<booktitle>In Proc. ofACL-07,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="1568" citStr="Filippova &amp; Strube (2007)" startWordPosition="239" endWordPosition="242">or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde &amp; Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi &amp; Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova &amp; Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with an alternative. Thus, it would be of interest to draw such a comparison, especially on English data, where LMs are usually expected to work well. As an improvement to the LM-based approach, we propose a combined method which distinguishes between the phrase and the clause levels: • it relies on a trigram LM to order words within phrases; • it finds the order of clause constituents (i.e., constituents dependent on a finite verb) with a maximum entropy classifier trained on a</context>
<context position="5560" citStr="Filippova &amp; Strube, 2007" startWordPosition="901" endWordPosition="904">ed into a dependency tree, and some PCFG parsers provide this option. best string unfeasible. However, our combined approach is based on the premise that trigram LMs are well-suited for finding the order within NPs, PPs and other phrases where the head is not a finite verb. E.g., given a noun modified by the words big, red and the, a LM can reliably rank the correct order higher than incorrect ones ( the big red N vs. the red big N, etc.). Next, on the clause level, for every finite verb in the tree we find the order of its dependents using the method which we originally developed for German (Filippova &amp; Strube, 2007), which utilizes a range of such linguistic features as PoS tag, syntactic role, length in words, pronominalization, semantic class, etc.2 For the experiments presented in this paper, we train two maximum entropy classifiers on all but the semantic features: 1. The first classifier determines the best starting point for a sentence: for each constituent dependent on the verb it returns the probability of this constituent being the first one in a sentence. The subject and also adjuncts (e.g. temporal adjuncts like yesterday) are usually found in the beginning of the sentence. 2. The second class</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Filippova, K. &amp; M. Strube (2007). Generating constituent order in German clauses. In Proc. ofACL-07, pp. 320– 327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<pages>403--434</pages>
<contexts>
<context position="2885" citStr="Goodman, 2001" startWordPosition="456" endWordPosition="457">roposed combination outperforms the method which relies solely on a LM. Hence, our results challenge the widespread attitude that trigram LMs provide an appropriate way to linearize syntactic trees in English but also indicate that they perform well in linearizing subtrees corresponding to phrases. 2 LM-based Approach Trigram models are easy to build and use, and it has been shown that more sophisticated n-gram models (e.g., with higher n, complex smoothing techniques, skipping, clustering or caching) are often not worth the effort of implementing them due to data sparseness and other issues (Goodman, 2001). This explains the popularity of trigram LMs in a variety of NLP tasks (Jurafsky &amp; Martin, 2008), in particular, in tree linearization where they have become 225 Proceedings of NAACL HLT 2009: Short Papers, pages 225–228, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics brothers Figure 1: A tree of the noun phrase all the brothers of my neighbor de facto the standard tree linearization tool in accordance with the ‘overgenerate and rank’ principle: given a syntactic tree, one needs to consider all possible linearizations and then choose the one with the lowest ent</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Goodman, J. T. (2001). A bit of progress in language modeling. Computer Speech and Language, pp. 403– 434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing. Upper Saddle River,</title>
<date>2008</date>
<publisher>Prentice Hall.</publisher>
<location>N.J.:</location>
<contexts>
<context position="2982" citStr="Jurafsky &amp; Martin, 2008" startWordPosition="472" endWordPosition="475">lts challenge the widespread attitude that trigram LMs provide an appropriate way to linearize syntactic trees in English but also indicate that they perform well in linearizing subtrees corresponding to phrases. 2 LM-based Approach Trigram models are easy to build and use, and it has been shown that more sophisticated n-gram models (e.g., with higher n, complex smoothing techniques, skipping, clustering or caching) are often not worth the effort of implementing them due to data sparseness and other issues (Goodman, 2001). This explains the popularity of trigram LMs in a variety of NLP tasks (Jurafsky &amp; Martin, 2008), in particular, in tree linearization where they have become 225 Proceedings of NAACL HLT 2009: Short Papers, pages 225–228, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics brothers Figure 1: A tree of the noun phrase all the brothers of my neighbor de facto the standard tree linearization tool in accordance with the ‘overgenerate and rank’ principle: given a syntactic tree, one needs to consider all possible linearizations and then choose the one with the lowest entropy. Given aprojective dependency tree1, all linearizations can be found recursively by generati</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Jurafsky, D. &amp; J. H. Martin (2008). Speech and Language Processing. Upper Saddle River, N.J.: Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--81</pages>
<contexts>
<context position="10279" citStr="Kendall, 1938" startWordPosition="1686" endWordPosition="1687">r some minor variation in word order and it might happen that the generated order is not necessarily wrong if different from the original one, we do not expect this to happen often and evaluate the performance rigorously: only the original order counts as the correct one. The default evaluation metric is perphrase/per-clause accuracy: |correct| acc = |total| Other metrics we use to measure how different a generated order of N elements is from the correct one are: 1. Kendall’s τ, τ = 1 − 4t ,(,−1) where t is the minimum number of interchanges of consecutive elements to achieve the right order (Kendall, 1938; Lapata, 2006). 2. Edit distance related di, di = 1 − &apos; where m is the minimum number of deletions combined with insertions to get to the right order (Ringger et al., 2004). E.g., on the phrase level, the incorrectly generated phrase the all brothers of my neighbor (&apos;1-0-2-3-4- 5&apos;) &apos;1-0-2-3-4- 5&apos;) gets τ = 0.87, di = 0.83. Likewise, given the input sentence from (1a), the incorrectly generated order of the four clause constituents in (1c) – ’1-0- 2-3’ – gets τ of 0.67 and di of 0.75. 4.3 Results The results of the experiments on the phrase and the clause levels are presented in Tables 1 and 2</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Kendall, M. G. (1938). A new measure of rank correlation. Biometrika, 30:81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ofACL-03,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="8464" citStr="Klein &amp; Manning, 2003" startWordPosition="1387" endWordPosition="1390">e goal of our experiments is to check the following hypotheses: 1. That trigram LMs are well-suited for phrase linearization. 2. That there is a considerable drop in performance when one uses them for linearization on the clause level. 3. That an approach which uses a richer representation on the clause level is more appropriate. 4.1 Data We take a subset of the TIPSTER3 corpus – all Wall Street Journal articles from the period of 1987-92 (approx. 72 mill. words) – and automatically annotate them with sentence boundaries, part of speech tags and dependency relations using the Stanford parser (Klein &amp; Manning, 2003). We reserve a small subset of about 600 articles (340,000 words) for testing and use the rest to build a trigram LM with the CMU toolkit (Clarkson &amp; Rosenfeld, 1997, with Good-Turing smoothing and vocabulary size of 30,000). To train the maximum entropy classifiers we use about 41,000 sentences. 4.2 Evaluation To test the trigram-based approach, we generate all possible permutations of clause constituents, place 3Description at http://www.ldc.upenn.edu/ Catalog/CatalogEntry.jsp?catalogId= LDC93T3A. the verb right after the subject and then rank the resulting strings with the LM taking the inf</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. &amp; C. D. Manning (2003). Accurate unlexicalized parsing. In Proc. ofACL-03, pp. 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL-98,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="1255" citStr="Langkilde &amp; Knight, 1998" startWordPosition="188" endWordPosition="191">nd show that trigram LMs are appropriate for phrase linearization while on the clause level a richer representation is necessary to achieve comparable performance. 1 Introduction To date, many natural language processing applications rely on syntactic representations and also modify them by compressing, fusing, or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde &amp; Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi &amp; Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova &amp; Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with an alternative. Thus, it would be of interest to draw such a comparison, especially on English data, where LMs are usually expected to work well. As an improvement t</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, I. &amp; K. Knight (1998). Generation that exploits corpus-based statistical knowledge. In Proc. of COLING-ACL-98, pp. 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendall’s tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="10294" citStr="Lapata, 2006" startWordPosition="1688" endWordPosition="1689">riation in word order and it might happen that the generated order is not necessarily wrong if different from the original one, we do not expect this to happen often and evaluate the performance rigorously: only the original order counts as the correct one. The default evaluation metric is perphrase/per-clause accuracy: |correct| acc = |total| Other metrics we use to measure how different a generated order of N elements is from the correct one are: 1. Kendall’s τ, τ = 1 − 4t ,(,−1) where t is the minimum number of interchanges of consecutive elements to achieve the right order (Kendall, 1938; Lapata, 2006). 2. Edit distance related di, di = 1 − &apos; where m is the minimum number of deletions combined with insertions to get to the right order (Ringger et al., 2004). E.g., on the phrase level, the incorrectly generated phrase the all brothers of my neighbor (&apos;1-0-2-3-4- 5&apos;) &apos;1-0-2-3-4- 5&apos;) gets τ = 0.87, di = 0.83. Likewise, given the input sentence from (1a), the incorrectly generated order of the four clause constituents in (1c) – ’1-0- 2-3’ – gets τ of 0.67 and di of 0.75. 4.3 Results The results of the experiments on the phrase and the clause levels are presented in Tables 1 and 2 respectively. </context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Lapata, M. (2006). Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4):471–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsi</author>
<author>E Krahmer</author>
</authors>
<title>Explorations in sentence fusion.</title>
<date>2005</date>
<booktitle>In Proc. of ENLG-05,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="1382" citStr="Marsi &amp; Krahmer (2005)" startWordPosition="208" endWordPosition="211">o achieve comparable performance. 1 Introduction To date, many natural language processing applications rely on syntactic representations and also modify them by compressing, fusing, or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde &amp; Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi &amp; Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova &amp; Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with an alternative. Thus, it would be of interest to draw such a comparison, especially on English data, where LMs are usually expected to work well. As an improvement to the LM-based approach, we propose a combined method which distinguishes between the phrase and the clause levels: • it relies</context>
</contexts>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>Marsi, E. &amp; E. Krahmer (2005). Explorations in sentence fusion. In Proc. of ENLG-05, pp. 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>M Gamon</author>
<author>R C Moore</author>
<author>D Rojas</author>
<author>M Smets</author>
<author>S Corston-Oliver</author>
</authors>
<title>Linguistically informed statistical models of constituent structure for ordering in sentence realization.</title>
<date>2004</date>
<booktitle>In Proc. of COLING-04,</booktitle>
<pages>673--679</pages>
<contexts>
<context position="1541" citStr="Ringger et al. (2004)" startWordPosition="235" endWordPosition="238">y compressing, fusing, or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde &amp; Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi &amp; Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova &amp; Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with an alternative. Thus, it would be of interest to draw such a comparison, especially on English data, where LMs are usually expected to work well. As an improvement to the LM-based approach, we propose a combined method which distinguishes between the phrase and the clause levels: • it relies on a trigram LM to order words within phrases; • it finds the order of clause constituents (i.e., constituents dependent on a finite verb) with a maximum entr</context>
<context position="10452" citStr="Ringger et al., 2004" startWordPosition="1717" endWordPosition="1720">to happen often and evaluate the performance rigorously: only the original order counts as the correct one. The default evaluation metric is perphrase/per-clause accuracy: |correct| acc = |total| Other metrics we use to measure how different a generated order of N elements is from the correct one are: 1. Kendall’s τ, τ = 1 − 4t ,(,−1) where t is the minimum number of interchanges of consecutive elements to achieve the right order (Kendall, 1938; Lapata, 2006). 2. Edit distance related di, di = 1 − &apos; where m is the minimum number of deletions combined with insertions to get to the right order (Ringger et al., 2004). E.g., on the phrase level, the incorrectly generated phrase the all brothers of my neighbor (&apos;1-0-2-3-4- 5&apos;) &apos;1-0-2-3-4- 5&apos;) gets τ = 0.87, di = 0.83. Likewise, given the input sentence from (1a), the incorrectly generated order of the four clause constituents in (1c) – ’1-0- 2-3’ – gets τ of 0.67 and di of 0.75. 4.3 Results The results of the experiments on the phrase and the clause levels are presented in Tables 1 and 2 respectively. From the total of 5,000 phrases, 55 (about 227 1%) were discarded because the number of admissible linearizations exceeded the limit of 20,000. In the first r</context>
</contexts>
<marker>Ringger, Gamon, Moore, Rojas, Smets, Corston-Oliver, 2004</marker>
<rawString>Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets &amp; S. Corston-Oliver (2004). Linguistically informed statistical models of constituent structure for ordering in sentence realization. In Proc. of COLING-04, pp. 673–679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
<author>M Murata</author>
<author>Q Ma</author>
<author>S Sekine</author>
<author>H Isahara</author>
</authors>
<title>Word order acquisition from corpora.</title>
<date>2000</date>
<booktitle>In Proc. of COLING-00,</booktitle>
<pages>871--877</pages>
<contexts>
<context position="1515" citStr="Uchimoto et al. (2000)" startWordPosition="230" endWordPosition="233">ions and also modify them by compressing, fusing, or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde &amp; Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi &amp; Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova &amp; Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with an alternative. Thus, it would be of interest to draw such a comparison, especially on English data, where LMs are usually expected to work well. As an improvement to the LM-based approach, we propose a combined method which distinguishes between the phrase and the clause levels: • it relies on a trigram LM to order words within phrases; • it finds the order of clause constituents (i.e., constituents dependent on a finite</context>
</contexts>
<marker>Uchimoto, Murata, Ma, Sekine, Isahara, 2000</marker>
<rawString>Uchimoto, K., M. Murata, Q. Ma, S. Sekine &amp; H. Isahara (2000). Word order acquisition from corpora. In Proc. of COLING-00, pp. 871–877.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>