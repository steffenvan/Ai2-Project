<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.999182">
Error Detection for Statistical Machine
Translation Using Linguistic Features
</title>
<author confidence="0.963584">
Deyi Xiong, Min Zhang, Haizhou Li
</author>
<affiliation confidence="0.837592">
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632.
</affiliation>
<email confidence="0.955977">
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.994705" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984333333333">
Automatic error detection is desired in
the post-processing to improve machine
translation quality. The previous work is
largely based on confidence estimation us-
ing system-based features, such as word
posterior probabilities calculated from N-
best lists or word lattices. We propose to
incorporate two groups of linguistic fea-
tures, which convey information from out-
side machine translation systems, into er-
ror detection: lexical and syntactic fea-
tures. We use a maximum entropy clas-
sifier to predict translation errors by inte-
grating word posterior probability feature
and linguistic features. The experimen-
tal results show that 1) linguistic features
alone outperform word posterior probabil-
ity based confidence estimation in error
detection; and 2) linguistic features can
further provide complementary informa-
tion when combined with word confidence
scores, which collectively reduce the clas-
sification error rate by 18.52% and im-
prove the F measure by 16.37%.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965142857143">
Translation hypotheses generated by a statistical
machine translation (SMT) system always contain
both correct parts (e.g. words, n-grams, phrases
matched with reference translations) and incor-
rect parts. Automatically distinguishing incorrect
parts from correct parts is therefore very desir-
able not only for post-editing and interactive ma-
chine translation (Ueffing and Ney, 2007) but also
for SMT itself: either by rescoring hypotheses in
the N-best list using the probability of correct-
ness calculated for each hypothesis (Zens and Ney,
2006) or by generating new hypotheses using N-
best lists from one SMT system or multiple sys-
tems (Akibay et al., 2004; Jayaraman and Lavie,
2005).
In this paper we restrict the “parts” to words.
That is, we detect errors at the word level for SMT.
A common approach to SMT error detection at the
word level is calculating the confidence at which a
word is correct. The majority of word confidence
estimation methods follows three steps:
</bodyText>
<listItem confidence="0.947705428571429">
1) Calculate features that express the correct-
ness of words either based on SMT model
(e.g. translation/language model) or based on
SMT system output (e.g. N-best lists, word
lattices) (Blatz et al., 2003; Ueffing and Ney,
2007).
2) Combine these features together with a clas-
sification model such as multi-layer percep-
tron (Blatz et al., 2003), Naive Bayes (Blatz
et al., 2003; Sanchis et al., 2007), or log-
linear model (Ueffing and Ney, 2007).
3) Divide words into two groups (correct trans-
lations and errors) by using a classification
threshold optimized on a development set.
</listItem>
<bodyText confidence="0.999642647058824">
Sometimes the step 2) is not necessary if only one
effective feature is used (Ueffing and Ney, 2007);
and sometimes the step 2) and 3) can be merged
into a single step if we directly output predicting
results from binary classifiers instead of making
thresholding decision.
Various features from different SMT models
and system outputs are investigated (Blatz et al.,
2003; Ueffing and Ney, 2007; Sanchis et al., 2007;
Raybaud et al., 2009). Experimental results show
that they are useful for error detection. However,
it is not adequate to just use these features as dis-
cussed in (Shi and Zhou, 2005) because the infor-
mation that they carry is either from the inner com-
ponents of SMT systems or from system outputs.
To some extent, it has already been considered by
SMT systems. Hence finding external information
</bodyText>
<page confidence="0.455201">
604
</page>
<note confidence="0.990142">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604–611,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999370916666667">
sources from outside SMT systems is desired for
error detection.
Linguistic knowledge is exactly such a good
choice as an external information source. It has al-
ready been proven effective in error detection for
speech recognition (Shi and Zhou, 2005). How-
ever, it is not widely used in SMT error detection.
The reason is probably that people have yet to find
effective linguistic features that outperform non-
linguistic features such as word posterior proba-
bility features (Blatz et al., 2003; Raybaud et al.,
2009). In this paper, we would like to show an
effective use of linguistic features in SMT error
detection.
We integrate two sets of linguistic features into
a maximum entropy (MaxEnt) model and develop
a MaxEnt-based binary classifier to predict the cat-
egory (correct or incorrect) for each word in a
generated target sentence. Our experimental re-
sults show that linguistic features substantially im-
prove error detection and even outperform word
posterior probability features. Further, they can
produce additional improvements when combined
with word posterior probability features.
The rest of the paper is organized as follows. In
Section 2, we review the previous work on word-
level confidence estimation which is used for error
detection. In Section 3, we introduce our linguistic
features as well as the word posterior probability
feature. In Section 4, we elaborate our MaxEnt-
based error detection model which combine lin-
guistic features and word posterior probability fea-
ture together. In Section 5, we describe the SMT
system which we use to generate translation hy-
potheses. We report our experimental results in
Section 6 and conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999697923076923">
In this section, we present an overview of confi-
dence estimation (CE) for machine translation at
the word level. As we are only interested in error
detection, we focus on work that uses confidence
estimation approaches to detect translation errors.
Of course, confidence estimation is not limited to
the application of error detection, it can also be
used in other scenarios, such as translation predic-
tion in an interactive environment (Grandrabur and
Foster, 2003) .
In a JHU workshop, Blatz et al. (2003) investi-
gate using neural networks and a naive Bayes clas-
sifier to combine various confidence features for
confidence estimation at the word level as well as
at the sentence level. The features they use for
word level CE include word posterior probabil-
ities estimated from N-best lists, features based
on SMT models, semantic features extracted from
WordNet as well as simple syntactic features, i.e.
parentheses and quotation mark check. Among all
these features, the word posterior probability is the
most effective feature, which is much better than
linguistic features such as semantic features, ac-
cording to their final results.
Ueffing and Ney (2007) exhaustively explore
various word-level confidence measures to label
each word in a generated translation hypothe-
sis as correct or incorrect. All their measures
are based on word posterior probabilities, which
are estimated from 1) system output, such as
word lattices or N-best lists and 2) word or
phrase translation table. Their experimental re-
sults show that word posterior probabilities di-
rectly estimated from phrase translation table are
better than those from system output except for the
Chinese-English language pair.
Sanchis et al. (2007) adopt a smoothed naive
Bayes model to combine different word posterior
probability based confidence features which are
estimated from N-best lists, similar to (Ueffing
and Ney, 2007).
Raybaud et al. (2009) study several confi-
dence features based on mutual information be-
tween words and n-gram and backward n-gram
language model for word-level and sentence-level
CE. They also explore linguistic features using in-
formation from syntactic category, tense, gender
and so on. Unfortunately, such linguistic features
neither improve performance at the word level nor
at the sentence level.
Our work departs from the previous work in two
major respects.
</bodyText>
<listItem confidence="0.951502818181818">
• We exploit various linguistic features and
show that they are able to produce larger im-
provements than widely used system-related
features such as word posterior probabilities.
This is in contrast to some previous work. Yet
another advantage of using linguistic features
is that they are system-independent, which
therefore can be used across different sys-
tems.
• We treat error detection as a complete bi-
nary classification problem. Hence we di-
</listItem>
<page confidence="0.818158">
605
</page>
<bodyText confidence="0.997975333333333">
rectly output prediction results from our dis-
criminatively trained classifier without opti-
mizing a classification threshold on a distinct
development set beforehand.1 Most previous
approaches make decisions based on a pre-
tuned classification threshold τ as follows
</bodyText>
<equation confidence="0.93596">
�
correct, Φ(correct, 0) &gt; τ
class =
incorrect, otherwise
</equation>
<bodyText confidence="0.9999265">
where Φ is a classifier or a confidence mea-
sure and 0 is the parameter set of Φ. The per-
formance of these approaches is strongly de-
pendent on the classification threshold.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999983555555556">
We explore two sets of linguistic features for each
word in a machine generated translation hypoth-
esis. The first set of linguistic features are sim-
ple lexical features. The second set of linguistic
features are syntactic features which are extracted
from link grammar parse. To compare with the
previously widely used features, we also investi-
gate features based on word posterior probabili-
ties.
</bodyText>
<subsectionHeader confidence="0.999738">
3.1 Lexical Features
</subsectionHeader>
<bodyText confidence="0.9042035">
We use the following lexical features.
• wd: word itself
</bodyText>
<listItem confidence="0.9436735">
• pos: part-of-speech tag from a tagger trained
on WSJ corpus. 2
</listItem>
<bodyText confidence="0.999829">
For each word, we look at previous n
words/tags and next n words/tags. They together
form a word/tag sequence pattern. The basic idea
of using these features is that words in rare pat-
terns are more likely to be incorrect than words
in frequently occurring patterns. To some extent,
these two features have similar function to a tar-
get language model or pos-based target language
model.
</bodyText>
<subsectionHeader confidence="0.999755">
3.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.993755142857143">
High-level linguistic knowledge such as syntac-
tic information about a word is a very natural and
promising indicator to decide whether this word is
syntactically correct or not. Words occurring in an
1This does not mean we do not need a development set.
We do validate our feature selection and other experimental
settings on the development set.
</bodyText>
<footnote confidence="0.4612915">
2Available via http://www-tsujii.is.s.u-tokyo.ac.jp/
—tsuruoka/postagger/
</footnote>
<bodyText confidence="0.999888310344828">
ungrammatical part of a target sentence are prone
to be incorrect. The challenge of using syntac-
tic knowledge for error detection is that machine-
generated hypotheses are rarely fully grammati-
cal. They are mixed with grammatical and un-
grammatical parts, which hence are not friendly
to traditional parsers trained on grammatical sen-
tences because ungrammatical parts of a machine-
generated sentence could lead to a parsing failure.
To overcome this challenge, we select the Link
Grammar (LG) parser 3 as our syntactic parser to
generate syntactic features. The LG parser pro-
duces a set of labeled links which connect pairs of
words with a link grammar (Sleator and Temper-
ley, 1993).
The main reason why we choose the LG parser
is that it provides a robustness feature: null-link
scheme. The null-link scheme allows the parser to
parse a sentence even when the parser can not fully
interpret the entire sentence (e.g. including un-
grammatical parts). When the parser fail to parse
the entire sentence, it ignores one word each time
until it finds linkages for remaining words. After
parsing, those ignored words are not connected to
any other words. We call them null-linked words.
Our hypothesis is that null-linked words are
prone to be syntactically incorrect. We hence
straightforwardly define a syntactic feature for a
word w according to its links as follows
</bodyText>
<equation confidence="0.86434925">
�
yes, w has links
link(w) =
no, otherwise
</equation>
<bodyText confidence="0.999974">
In Figure 1 we show an example of a generated
translation hypothesis with its link parse. Here
links are denoted with dotted lines which are an-
notated with link types (e.g., Jp, Op). Bracketed
words, namely “,” and “including”, are null-linked
words.
</bodyText>
<subsectionHeader confidence="0.999591">
3.3 Word Posterior Probability Features
</subsectionHeader>
<bodyText confidence="0.9999787">
Our word posterior probability is calculated on N-
best list, which is first proposed by (Ueffing et al.,
2003) and widely used in (Blatz et al., 2003; Ueff-
ing and Ney, 2007; Sanchis et al., 2007).
Given a source sentence f, let {en}N1 be the N-
best list generated by an SMT system, and let ein is
the i-th word in en. The major work of calculating
word posterior probabilities is to find the Leven-
shtein alignment (Levenshtein, 1966) between the
best hypothesis el and its competing hypothesis
</bodyText>
<footnote confidence="0.955347">
3Available at http://www.link.cs.cmu.edu/link/
</footnote>
<page confidence="0.659877">
606
</page>
<figureCaption confidence="0.999349">
Figure 1: An example of Link Grammar parsing results.
</figureCaption>
<bodyText confidence="0.99995525">
en in the N-best list {en}N1 . We denote the align-
ment between them as ℓ(e1, en). The word in the
hypothesis en which ei1 is Levenshtein aligned to
is denoted as ℓi(e1, en).
The word posterior probability of ei1 is then cal-
culated by summing up the probabilities over all
hypotheses containing ei1 in a position which is
Levenshtein aligned to ei1.
</bodyText>
<equation confidence="0.9970985">
i Ee�: ℓj(e1,e�)=ej1 p(en)
pwpp( e1) = EN1 p(en)
</equation>
<bodyText confidence="0.999659">
To use the word posterior probability in our er-
ror detection model, we need to make it discrete.
We introduce a feature for a word w based on its
word posterior probability as follows
</bodyText>
<equation confidence="0.943557">
dwpp(w) = ⌊−log(pwpp(w))/df⌋
</equation>
<bodyText confidence="0.999893">
where df is the discrete factor which can be set to
1, 0.1, 0.01 and so on. “⌊ ⌋” is a rounding oper-
ator which takes the largest integer that does not
exceed −log(pwpp(w))/df. We optimize the dis-
crete factor on our development set and find the
optimal value is 1. Therefore a feature “dwpp =
2” represents that the logarithm of the word poste-
rior probability is between -3 and -2;
</bodyText>
<sectionHeader confidence="0.8638665" genericHeader="method">
4 Error Detection with a Maximum
Entropy Model
</sectionHeader>
<bodyText confidence="0.999984157894737">
As mentioned before, we consider error detec-
tion as a binary classification task. To formal-
ize this task, we use a feature vector ψ to rep-
resent a word w in question, and a binary vari-
able c to indicate whether this word is correct or
not. In the feature vector, we look at 2 words
before and 2 words after the current word posi-
tion (w_2, w_1, w, w1, w2). We collect features
{wd, pos, link, dwpp} for each word among these
words and combine them into the feature vector
ψ for w. As such, we want the feature vector to
capture the contextual environment, e.g., pos se-
quence pattern, syntactic pattern, where the word
w occurs.
For classification, we employ the maximum
entropy model (Berger et al., 1996) to predict
whether a word w is correct or incorrect given its
feature vector ψ.
where fi is a binary model feature defined on c
and the feature vector ψ. θi is the weight of fi.
Table 1 shows some examples of our binary model
features.
In order to learn the model feature weights θ for
probability estimation, we need a training set of
m samples {ψi, ci}m1 . The challenge of collect-
ing training instances is that the correctness of a
word in a generated translation hypothesis is not
intuitively clear (Ueffing and Ney, 2007). We will
describe the method to determine the correctness
of a word in Section 6.1, which is broadly adopted
in previous work.
We tune our model feature weights using an
off-the-shelf MaxEnt toolkit (Zhang, 2004). To
avoid overfitting, we optimize the Gaussian prior
on the development set. During test, if the proba-
bility p(correct|ψ) is larger than p(incorrect|ψ)
according the trained MaxEnt model, the word is
labeled as correct otherwise incorrect.
</bodyText>
<sectionHeader confidence="0.991309" genericHeader="method">
5 SMT System
</sectionHeader>
<bodyText confidence="0.998457769230769">
To obtain machine-generated translation hypothe-
ses for our error detection, we use a state-of-the-art
phrase-based machine translation system MOSES
(Koehn et al., 2003; Koehn et al., 2007). The
translation task is on the official NIST Chinese-
to-English evaluation data. The training data con-
sists of more than 4 million pairs of sentences (in-
cluding 101.93M Chinese words and 112.78M En-
glish words) from LDC distributed corpora. Table
2 shows the corpora that we use for the translation
task.
We build a four-gram language model using the
SRILM toolkit (Stolcke, 2002), which is trained
</bodyText>
<equation confidence="0.811457714285714">
p(c|ψ) = Ec′ exp(Ei θifi(c′, ψ))
exp(Ei θifi(c, ψ))
607
Feature
Example
wd
pos
link
dwpp
� 1, O.w.wd = �.�, c = correct
f(c, O) =
0, otherwise
� 1, O.w2.pos = �NN�, c = incorrect
f(c, O) =
0, otherwise
� 1, O.w.link = no, c = incorrect
f(c, O) =
0, otherwise
� 1, O.w−2.dwpp = 2, c = correct
f(c, O) =
0, otherwise
</equation>
<tableCaption confidence="0.995357">
Table 1: Examples of model features.
</tableCaption>
<figure confidence="0.95628248">
LDC ID
LDC2004E12
LDC2004T08
LDC2005T10
LDC2003E14
LDC2002E18
LDC2005T06
LDC2003E07
LDC2004T07
Description
United Nations
Hong Kong News
Sinorama Magazine
FBIS
Xinhua News V1 beta
Chinese News Translation
Chinese Treebank
Multiple Translation Chinese
Training
Development
Test
Corpus Sentences Words
MT-02 878 24,225
MT-05 1082 31,321
MT-03 919 25,619
</figure>
<tableCaption confidence="0.990785333333333">
Table 3: Corpus statistics (number of sentences
and words) for the error detection task.
Table 2: Training corpora for the translation task.
</tableCaption>
<bodyText confidence="0.998787375">
on Xinhua section of the English Gigaword cor-
pus (181.1M words). For minimum error rate tun-
ing (Och, 2003), we use NIST MT-02 as the de-
velopment set for the translation task. In order
to calculate word posterior probabilities, we gen-
erate 10,000 best lists for NIST MT-02/03/05 re-
spectively. The performance, in terms of BLEU
(Papineni et al., 2002) score, is shown in Table 4.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999996222222222">
We conducted our experiments at several levels.
Starting with MaxEnt models with single linguis-
tic feature or word posterior probability based fea-
ture, we incorporated additional features incre-
mentally by combining features together. In do-
ing so, we would like the experimental results not
only to display the effectiveness of linguistic fea-
tures for error detection but also to identify the ad-
ditional contribution of each feature to the task.
</bodyText>
<subsectionHeader confidence="0.993893">
6.1 Data Corpus
</subsectionHeader>
<bodyText confidence="0.999991972972973">
For the error detection task, we use the best trans-
lation hypotheses of NIST MT-02/05/03 generated
by MOSES as our training, development, and test
corpus respectively. The statistics about these cor-
pora is shown in Table 3. Each translation hypoth-
esis has four reference translations.
To obtain the linkage information, we run the
LG parser on all translation hypotheses. We find
that the LG parser can not fully parse 560 sen-
tences (63.8%) in the training set (MT-02), 731
sentences (67.6%) in the development set (MT-05)
and 660 sentences (71.8%) in the test set (MT-03).
For these sentences, the LG parser will use the the
null-link scheme to generate null-linked words.
To determine the true class of a word in a gen-
erated translation hypothesis, we follow (Blatz et
al., 2003) to use the word error rate (WER). We
tag a word as correct if it is aligned to itself in
the Levenshtein alignment between the hypothesis
and the nearest reference translation that has min-
imum edit distance to the hypothesis among four
reference translations. Figure 2 shows the Lev-
enshtein alignment between a machine-generated
hypothesis and its nearest reference translation.
The “Class” row shows the label of each word ac-
cording to the alignment, where “c” and “i” repre-
sent correct and incorrect respectively.
There are several other metrics to tag single
words in a translation hypothesis as correct or in-
correct, such as PER where a word is tagged as
correct if it occurs in one of reference translations
with the same number of occurrences, Set which is
a less strict variant of PER, ignoring the number of
occurrences per word. In Figure 2, the two words
“last year” in the hypothesis will be tagged as cor-
rect if we use the PER or Set metric since they do
not consider the occurring positions of words. Our
</bodyText>
<equation confidence="0.90396125">
608
H❑y❑ ❑b❑t❑h❑e❑s❑�s❑
C�h�i�n�a� U�n�i�c�o�m�
l�a�s�t� y❑eE]a❑r❑ n❑e❑t❑ p❑r❑o❑f�it� r❑ods❑e❑ u❑p❑ 3❑8❑%❑
R❑e❑f❑e❑r❑e❑n❑�❑e❑
C�h�i�n�a� UFn[]i�c❑o❑m❑ n❑e❑t❑ p❑r❑o❑f❑i�tI r❑ods❑e❑ u❑p❑ 3❑8❑%❑ lCb
C❑�❑a❑s❑s❑ CEh1�n❑a[/❑c❑ U�n�i�c�om�/�c�
l�a�s�t�/�i� y�e�a�r�/�i� n❑e❑t❑/❑c❑ p❑r❑o❑f❑i�t❑/❑c❑ r❑ods❑e❑/❑c❑ u❑pXc❑ 3
</equation>
<figureCaption confidence="0.814878">
Figure 2: Tagging a word as correct/incorrect according to the Levenshtein alignment.
</figureCaption>
<table confidence="0.94874175">
Corpus BLEU (%) RCW (%)
MT-02 33.24 47.76
MT-05 32.03 47.85
MT-03 32.86 47.57
</table>
<tableCaption confidence="0.988764">
Table 4: Case-insensitive BLEU score and ratio
</tableCaption>
<bodyText confidence="0.921936">
of correct words (RCW) on the training, develop-
ment and test corpus.
metric corresponds to the m-WER used in (Ueff-
ing and Ney, 2007), which is stricter than PER and
Set. It is also stricter than normal WER metric
which compares each hypothesis to all references,
rather than the nearest reference.
Table 4 shows the case-insensitive BLEU score
and the percentage of words that are labeled as cor-
rect according to the method described above on
the training, development and test corpus.
</bodyText>
<subsectionHeader confidence="0.999055">
6.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.936733833333333">
To evaluate the overall performance of the error
detection, we use the commonly used metric, clas-
sification error rate (CER) to evaluate our classi-
fiers. CER is defined as the percentage of words
that are wrongly tagged as follows
# of wrongly tagged words
</bodyText>
<equation confidence="0.695099">
CER =
</equation>
<bodyText confidence="0.96212305882353">
Total # of words
The baseline CER is determined by assuming
the most frequent class for all words. Since the ra-
tio of correct words in both the development and
test set is lower than 50%, the most frequent class
is “incorrect”. Hence the baseline CER in our ex-
periments is equal to the ratio of correct words as
these words are wrongly tagged as incorrect.
We also use precision and recall on errors to
evaluate the performance of error detection. Let
ng be the number of words of which the true class
is incorrect, nt be the number of words which are
tagged as incorrect by classifiers, and nm be the
number of words tagged as incorrect that are in-
deed translation errors. The precision Pre is the
percentage of words correctly tagged as transla-
tion errors.
</bodyText>
<equation confidence="0.8289129">
Pre = nm
nt
The recall Rec is the proportion of actual transla-
tion errors that are found by classifiers.
Rec = nm
ng
F measure, the trade-off between precision and re-
call, is also used.
2 × Pre × Rec
Pre + Rec
</equation>
<subsectionHeader confidence="0.997808">
6.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999758321428571">
Table 5 shows the performance of our experiments
on the error detection task. To compare with pre-
vious work using word posterior probabilities for
confidence estimation, we carried out experiments
using wpp estimated from N-best lists with the
classification threshold T, which was optimized on
our development set to minimize CER. A relative
improvement of 9.27% is achieved over the base-
line CER, which reconfirms the effectiveness of
word posterior probabilities for error detection.
We conducted three groups of experiments us-
ing the MaxEnt based error detection model with
various feature combinations.
• The first group of experiments uses single
feature, such as dwpp, pos. We find the
most effective feature is pos, which achieves
a 16.12% relative improvement over the base-
line CER and 7.55% relative improvement
over the CER of word posterior probabil-
ity thresholding. Using discrete word pos-
terior probabilities as features in the Max-
Ent based error detection model is marginally
better than word posterior probability thresh-
olding in terms of CER, but obtains a 13.79%
relative improvement in F measure. The syn-
tactic feature link also improves the error de-
tection in terms of CER and particularly re-
call.
</bodyText>
<table confidence="0.919458357142857">
F=
609
Combination Features CER (%) Pre (%) Rec (%) F (%)
Baseline - 47.57 - - -
Thresholding wpp - 43.16 58.98 58.07 58.52
MaxEnt (dwpp) 44 43.07 56.12 81.86 66.59
MaxEnt (wd) 19,164 41.57 58.25 73.11 64.84
MaxEnt (pos) 199 39.90 58.88 79.23 67.55
MaxEnt (link) 19 44.31 54.72 89.72 67.98
MaxEnt (wd + pos) 19,363 39.43 59.36 78.60 67.64
MaxEnt (wd + pos + link) 19,382 39.79 58.74 80.97 68.08
MaxEnt (dwpp + wd) 19,208 41.04 57.18 83.75 67.96
MaxEnt (dwpp + wd + pos) 19,407 38.88 59.87 78.38 67.88
MaxEnt (dwpp + wd + pos + link) 19,426 38.76 59.89 78.94 68.10
</table>
<tableCaption confidence="0.999664">
Table 5: Performance of the error detection task.
</tableCaption>
<bodyText confidence="0.999548914285714">
• The second group of experiments concerns
with the combination of linguistic features
without word posterior probability feature.
The combination of lexical features improves
both CER and precision over single lexical
feature (wd, pos). The addition of syntactic
feature link marginally undermines CER but
improves recall by a lot.
• The last group of experiments concerns about
the additional contribution of linguistic fea-
tures to error detection with word posterior
probability. We added linguistic features in-
crementally into the feature pool. The best
performance was achieved by using all fea-
tures, which has a relative of improvement of
18.52% over the baseline CER.
The first two groups of experiments show that
linguistic features, individually (except for link)
or by combination, are able to produce much better
performance than word posterior probability fea-
tures in both CER and F measure. The best com-
bination of linguistic features achieves a relative
improvement of 8.64% and 15.58% in CER and
F measure respectively over word posterior prob-
ability thresholding.
The Table 5 also reveals how linguistic fea-
tures improve error detection. The lexical features
(pos, wd) improve precision when they are used.
This suggests that lexical features can help the sys-
tem find errors more accurately. Syntactic features
(link), on the other hand, improve recall whenever
they are used, which indicates that they can help
the system find more errors.
We also show the number of features in each
combination in Table 5. Except for the wd feature,
</bodyText>
<figure confidence="0.663922">
E
</figure>
<figureCaption confidence="0.889183">
Figure 3: CER vs. the number of training sen-
tences.
</figureCaption>
<bodyText confidence="0.999784818181818">
the pos has the largest number of features, 199,
which is a small set of features. This suggests that
our error detection model can be learned from a
rather small training set.
Figure 3 shows CERs for the feature combina-
tion MaxEnt (dwpp + wd + pos + link) when
the number of training sentences is enlarged incre-
mentally. CERs drop significantly when the num-
ber of training sentences is increased from 100 to
500. After 500 sentences are used, CERs change
marginally and tend to converge.
</bodyText>
<sectionHeader confidence="0.995841" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998753">
In this paper, we have presented a maximum en-
tropy based approach to automatically detect er-
rors in translation hypotheses generated by SMT
</bodyText>
<figure confidence="0.9830722">
40.6E
40.4E
40.2E
40.0E
39.8E
39.6E
39.4E
39.2E
39.0E
38.8E
38.6E
CER (%)
0E 200E 400E 600E 800E 1000E
Number of Training SentencesEl
610
</figure>
<bodyText confidence="0.999892904761905">
systems. We incorporate two sets of linguistic
features together with word posterior probability
based features into error detection.
Our experiments validate that linguistic features
are very useful for error detection: 1) they by
themselves achieve a higher improvement in terms
of both CER and F measure than word posterior
probability features; 2) the performance is further
improved when they are combined with word pos-
terior probability features.
The extracted linguistic features are quite com-
pact, which can be learned from a small train-
ing set. Furthermore, The learned linguistic fea-
tures are system-independent. Therefore our ap-
proach can be used for other machine translation
systems, such as rule-based or example-based sys-
tem, which generally do not produce N-best lists.
Future work in this direction involve detect-
ing particular error types such as incorrect po-
sitions, inappropriate/unnecessary words (Elliott,
2006) and automatically correcting errors.
</bodyText>
<sectionHeader confidence="0.998462" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998166573333334">
Yasuhiro Akibay, Eiichiro Sumitay, Hiromi Nakaiway,
Seiichi Yamamotoy, and Hiroshi G. Okunoz. 2004.
Using a Mixture of N-best Lists from Multiple MT
Systems in Rank-sum-based Confidence Measure
for MT Outputs. In Proceedings of COLING.
Adam L. Berger, Stephen A. Della Pietra andVincent
J. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1): 39-71.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, Nicola Ueffing. 2003. Confidence estima-
tion for machine translation. final report, jhu/clsp
summer workshop.
Debra Elliott. 2006 Corpus-based Machine Transla-
tion Evaluation via Automated Error Detection in
Output Texts. Phd Thesis, University of Leeds.
Simona Gandrabur and George Foster. 2003. Confi-
dence Estimation for Translation Prediction. In Pro-
ceedings of HLT-NAACL.
S. Jayaraman and A. Lavie. 2005. Multi-engine Ma-
chine Translation Guided by Explicit Word Match-
ing. In Proceedings of EAMT.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of HLT-NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constrantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings ofACL, Demonstration Session.
V. I. Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet
Physics Doklady, Feb.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatically
Evaluation of Machine Translation. In Proceedings
of ACL 2002.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
Kamel Smaili. 2009. Word- and Sentence-level
Confidence Measures for Machine Translation. In
Proceedings of EAMT 2009.
Alberto Sanchis, Alfons Juan and Enrique Vidal. 2007.
Estimation of Confidence Measures for Machine
Translation. In Procedings of Machine Translation
Summit XI.
Daniel Sleator and Davy Temperley. 1993. Parsing En-
glish with a Link Grammar. In Proceedings of Third
International Workshop on Parsing Technologies.
Yongmei Shi and Lina Zhou. 2005. Error Detec-
tion Using Linguistic Features. In Proceedings of
HLT/EMNLP 2005.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence Measures for Statistical Machine
Translation. In Proceedings. of MT Summit IX.
Nicola Ueffing and Hermann Ney. 2007. Word-
Level Confidence Estimation for Machine Transla-
tion. Computational Linguistics, 33(1):9-40.
Richard Zens and Hermann Ney. 2006. N-gram Pos-
terior Probabilities for Statistical Machine Transla-
tion. In HLT/NAACL: Proceedings of the Workshop
on Statistical Machine Translation.
Le Zhang. 2004. Maximum Entropy Model-
ing Tooklkit for Python and C++. Available at
http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
</reference>
<page confidence="0.859689">
611
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.404777">
<title confidence="0.9986035">Error Detection for Statistical Machine Translation Using Linguistic Features</title>
<author confidence="0.981095">Deyi Xiong</author>
<author confidence="0.981095">Min Zhang</author>
<author confidence="0.981095">Haizhou Li</author>
<affiliation confidence="0.979507">Human Language Technology Institute for Infocomm Research</affiliation>
<note confidence="0.70382">1 Fusionopolis Way, #21-01 Connexis, Singapore 138632.</note>
<email confidence="0.756711">mzhang,</email>
<abstract confidence="0.99554508">Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word probabilities calculated from best lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasuhiro Akibay</author>
<author>Eiichiro Sumitay</author>
<author>Hiromi Nakaiway</author>
<author>Seiichi Yamamotoy</author>
<author>Hiroshi G Okunoz</author>
</authors>
<title>Using a Mixture of N-best Lists from Multiple MT Systems in Rank-sum-based Confidence Measure for MT Outputs.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1911" citStr="Akibay et al., 2004" startWordPosition="274" endWordPosition="277">statistical machine translation (SMT) system always contain both correct parts (e.g. words, n-grams, phrases matched with reference translations) and incorrect parts. Automatically distinguishing incorrect parts from correct parts is therefore very desirable not only for post-editing and interactive machine translation (Ueffing and Ney, 2007) but also for SMT itself: either by rescoring hypotheses in the N-best list using the probability of correctness calculated for each hypothesis (Zens and Ney, 2006) or by generating new hypotheses using Nbest lists from one SMT system or multiple systems (Akibay et al., 2004; Jayaraman and Lavie, 2005). In this paper we restrict the “parts” to words. That is, we detect errors at the word level for SMT. A common approach to SMT error detection at the word level is calculating the confidence at which a word is correct. The majority of word confidence estimation methods follows three steps: 1) Calculate features that express the correctness of words either based on SMT model (e.g. translation/language model) or based on SMT system output (e.g. N-best lists, word lattices) (Blatz et al., 2003; Ueffing and Ney, 2007). 2) Combine these features together with a classifi</context>
</contexts>
<marker>Akibay, Sumitay, Nakaiway, Yamamotoy, Okunoz, 2004</marker>
<rawString>Yasuhiro Akibay, Eiichiro Sumitay, Hiromi Nakaiway, Seiichi Yamamotoy, and Hiroshi G. Okunoz. 2004. Using a Mixture of N-best Lists from Multiple MT Systems in Rank-sum-based Confidence Measure for MT Outputs. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra andVincent J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>39--71</pages>
<marker>Berger, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra andVincent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1): 39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
</authors>
<title>Alex Kulesza, Alberto Sanchis, Nicola Ueffing.</title>
<date>2003</date>
<contexts>
<context position="2435" citStr="Blatz et al., 2003" startWordPosition="362" endWordPosition="365"> new hypotheses using Nbest lists from one SMT system or multiple systems (Akibay et al., 2004; Jayaraman and Lavie, 2005). In this paper we restrict the “parts” to words. That is, we detect errors at the word level for SMT. A common approach to SMT error detection at the word level is calculating the confidence at which a word is correct. The majority of word confidence estimation methods follows three steps: 1) Calculate features that express the correctness of words either based on SMT model (e.g. translation/language model) or based on SMT system output (e.g. N-best lists, word lattices) (Blatz et al., 2003; Ueffing and Ney, 2007). 2) Combine these features together with a classification model such as multi-layer perceptron (Blatz et al., 2003), Naive Bayes (Blatz et al., 2003; Sanchis et al., 2007), or loglinear model (Ueffing and Ney, 2007). 3) Divide words into two groups (correct translations and errors) by using a classification threshold optimized on a development set. Sometimes the step 2) is not necessary if only one effective feature is used (Ueffing and Ney, 2007); and sometimes the step 2) and 3) can be merged into a single step if we directly output predicting results from binary cla</context>
<context position="4308" citStr="Blatz et al., 2003" startWordPosition="666" endWordPosition="669">r Computational Linguistics, pages 604–611, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sources from outside SMT systems is desired for error detection. Linguistic knowledge is exactly such a good choice as an external information source. It has already been proven effective in error detection for speech recognition (Shi and Zhou, 2005). However, it is not widely used in SMT error detection. The reason is probably that people have yet to find effective linguistic features that outperform nonlinguistic features such as word posterior probability features (Blatz et al., 2003; Raybaud et al., 2009). In this paper, we would like to show an effective use of linguistic features in SMT error detection. We integrate two sets of linguistic features into a maximum entropy (MaxEnt) model and develop a MaxEnt-based binary classifier to predict the category (correct or incorrect) for each word in a generated target sentence. Our experimental results show that linguistic features substantially improve error detection and even outperform word posterior probability features. Further, they can produce additional improvements when combined with word posterior probability feature</context>
<context position="6007" citStr="Blatz et al. (2003)" startWordPosition="938" endWordPosition="941">anslation hypotheses. We report our experimental results in Section 6 and conclude in Section 7. 2 Related Work In this section, we present an overview of confidence estimation (CE) for machine translation at the word level. As we are only interested in error detection, we focus on work that uses confidence estimation approaches to detect translation errors. Of course, confidence estimation is not limited to the application of error detection, it can also be used in other scenarios, such as translation prediction in an interactive environment (Grandrabur and Foster, 2003) . In a JHU workshop, Blatz et al. (2003) investigate using neural networks and a naive Bayes classifier to combine various confidence features for confidence estimation at the word level as well as at the sentence level. The features they use for word level CE include word posterior probabilities estimated from N-best lists, features based on SMT models, semantic features extracted from WordNet as well as simple syntactic features, i.e. parentheses and quotation mark check. Among all these features, the word posterior probability is the most effective feature, which is much better than linguistic features such as semantic features, </context>
<context position="12032" citStr="Blatz et al., 2003" startWordPosition="1899" endWordPosition="1902">one to be syntactically incorrect. We hence straightforwardly define a syntactic feature for a word w according to its links as follows � yes, w has links link(w) = no, otherwise In Figure 1 we show an example of a generated translation hypothesis with its link parse. Here links are denoted with dotted lines which are annotated with link types (e.g., Jp, Op). Bracketed words, namely “,” and “including”, are null-linked words. 3.3 Word Posterior Probability Features Our word posterior probability is calculated on Nbest list, which is first proposed by (Ueffing et al., 2003) and widely used in (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007). Given a source sentence f, let {en}N1 be the Nbest list generated by an SMT system, and let ein is the i-th word in en. The major work of calculating word posterior probabilities is to find the Levenshtein alignment (Levenshtein, 1966) between the best hypothesis el and its competing hypothesis 3Available at http://www.link.cs.cmu.edu/link/ 606 Figure 1: An example of Link Grammar parsing results. en in the N-best list {en}N1 . We denote the alignment between them as ℓ(e1, en). The word in the hypothesis en which ei1 is Levenshtein aligned to is </context>
<context position="18285" citStr="Blatz et al., 2003" startWordPosition="2969" endWordPosition="2972">vely. The statistics about these corpora is shown in Table 3. Each translation hypothesis has four reference translations. To obtain the linkage information, we run the LG parser on all translation hypotheses. We find that the LG parser can not fully parse 560 sentences (63.8%) in the training set (MT-02), 731 sentences (67.6%) in the development set (MT-05) and 660 sentences (71.8%) in the test set (MT-03). For these sentences, the LG parser will use the the null-link scheme to generate null-linked words. To determine the true class of a word in a generated translation hypothesis, we follow (Blatz et al., 2003) to use the word error rate (WER). We tag a word as correct if it is aligned to itself in the Levenshtein alignment between the hypothesis and the nearest reference translation that has minimum edit distance to the hypothesis among four reference translations. Figure 2 shows the Levenshtein alignment between a machine-generated hypothesis and its nearest reference translation. The “Class” row shows the label of each word according to the alignment, where “c” and “i” represent correct and incorrect respectively. There are several other metrics to tag single words in a translation hypothesis as </context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, Nicola Ueffing. 2003. Confidence estimation for machine translation. final report, jhu/clsp summer workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Debra Elliott</author>
</authors>
<title>Corpus-based Machine Translation Evaluation via Automated Error Detection in Output Texts. Phd Thesis,</title>
<date>2006</date>
<institution>University of Leeds.</institution>
<marker>Elliott, 2006</marker>
<rawString>Debra Elliott. 2006 Corpus-based Machine Translation Evaluation via Automated Error Detection in Output Texts. Phd Thesis, University of Leeds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simona Gandrabur</author>
<author>George Foster</author>
</authors>
<title>Confidence Estimation for Translation Prediction.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<marker>Gandrabur, Foster, 2003</marker>
<rawString>Simona Gandrabur and George Foster. 2003. Confidence Estimation for Translation Prediction. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jayaraman</author>
<author>A Lavie</author>
</authors>
<title>Multi-engine Machine Translation Guided by Explicit Word Matching.</title>
<date>2005</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="1939" citStr="Jayaraman and Lavie, 2005" startWordPosition="278" endWordPosition="281">ranslation (SMT) system always contain both correct parts (e.g. words, n-grams, phrases matched with reference translations) and incorrect parts. Automatically distinguishing incorrect parts from correct parts is therefore very desirable not only for post-editing and interactive machine translation (Ueffing and Ney, 2007) but also for SMT itself: either by rescoring hypotheses in the N-best list using the probability of correctness calculated for each hypothesis (Zens and Ney, 2006) or by generating new hypotheses using Nbest lists from one SMT system or multiple systems (Akibay et al., 2004; Jayaraman and Lavie, 2005). In this paper we restrict the “parts” to words. That is, we detect errors at the word level for SMT. A common approach to SMT error detection at the word level is calculating the confidence at which a word is correct. The majority of word confidence estimation methods follows three steps: 1) Calculate features that express the correctness of words either based on SMT model (e.g. translation/language model) or based on SMT system output (e.g. N-best lists, word lattices) (Blatz et al., 2003; Ueffing and Ney, 2007). 2) Combine these features together with a classification model such as multi-l</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>S. Jayaraman and A. Lavie. 2005. Multi-engine Machine Translation Guided by Explicit Word Matching. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="15386" citStr="Koehn et al., 2003" startWordPosition="2485" endWordPosition="2488">e the method to determine the correctness of a word in Section 6.1, which is broadly adopted in previous work. We tune our model feature weights using an off-the-shelf MaxEnt toolkit (Zhang, 2004). To avoid overfitting, we optimize the Gaussian prior on the development set. During test, if the probability p(correct|ψ) is larger than p(incorrect|ψ) according the trained MaxEnt model, the word is labeled as correct otherwise incorrect. 5 SMT System To obtain machine-generated translation hypotheses for our error detection, we use a state-of-the-art phrase-based machine translation system MOSES (Koehn et al., 2003; Koehn et al., 2007). The translation task is on the official NIST Chineseto-English evaluation data. The training data consists of more than 4 million pairs of sentences (including 101.93M Chinese words and 112.78M English words) from LDC distributed corpora. Table 2 shows the corpora that we use for the translation task. We build a four-gram language model using the SRILM toolkit (Stolcke, 2002), which is trained p(c|ψ) = Ec′ exp(Ei θifi(c′, ψ)) exp(Ei θifi(c, ψ)) 607 Feature Example wd pos link dwpp � 1, O.w.wd = �.�, c = correct f(c, O) = 0, otherwise � 1, O.w2.pos = �NN�, c = incorrect f</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL, Demonstration Session.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and</location>
<contexts>
<context position="15407" citStr="Koehn et al., 2007" startWordPosition="2489" endWordPosition="2492">rmine the correctness of a word in Section 6.1, which is broadly adopted in previous work. We tune our model feature weights using an off-the-shelf MaxEnt toolkit (Zhang, 2004). To avoid overfitting, we optimize the Gaussian prior on the development set. During test, if the probability p(correct|ψ) is larger than p(incorrect|ψ) according the trained MaxEnt model, the word is labeled as correct otherwise incorrect. 5 SMT System To obtain machine-generated translation hypotheses for our error detection, we use a state-of-the-art phrase-based machine translation system MOSES (Koehn et al., 2003; Koehn et al., 2007). The translation task is on the official NIST Chineseto-English evaluation data. The training data consists of more than 4 million pairs of sentences (including 101.93M Chinese words and 112.78M English words) from LDC distributed corpora. Table 2 shows the corpora that we use for the translation task. We build a four-gram language model using the SRILM toolkit (Stolcke, 2002), which is trained p(c|ψ) = Ec′ exp(Ei θifi(c′, ψ)) exp(Ei θifi(c, ψ)) 607 Feature Example wd pos link dwpp � 1, O.w.wd = �.�, c = correct f(c, O) = 0, otherwise � 1, O.w2.pos = �NN�, c = incorrect f(c, O) = 0, otherwise</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<contexts>
<context position="12315" citStr="Levenshtein, 1966" startWordPosition="1953" endWordPosition="1954"> denoted with dotted lines which are annotated with link types (e.g., Jp, Op). Bracketed words, namely “,” and “including”, are null-linked words. 3.3 Word Posterior Probability Features Our word posterior probability is calculated on Nbest list, which is first proposed by (Ueffing et al., 2003) and widely used in (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007). Given a source sentence f, let {en}N1 be the Nbest list generated by an SMT system, and let ein is the i-th word in en. The major work of calculating word posterior probabilities is to find the Levenshtein alignment (Levenshtein, 1966) between the best hypothesis el and its competing hypothesis 3Available at http://www.link.cs.cmu.edu/link/ 606 Figure 1: An example of Link Grammar parsing results. en in the N-best list {en}N1 . We denote the alignment between them as ℓ(e1, en). The word in the hypothesis en which ei1 is Levenshtein aligned to is denoted as ℓi(e1, en). The word posterior probability of ei1 is then calculated by summing up the probabilities over all hypotheses containing ei1 in a position which is Levenshtein aligned to ei1. i Ee�: ℓj(e1,e�)=ej1 p(en) pwpp( e1) = EN1 p(en) To use the word posterior probabilit</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady, Feb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="16757" citStr="Och, 2003" startWordPosition="2718" endWordPosition="2719"> features. LDC ID LDC2004E12 LDC2004T08 LDC2005T10 LDC2003E14 LDC2002E18 LDC2005T06 LDC2003E07 LDC2004T07 Description United Nations Hong Kong News Sinorama Magazine FBIS Xinhua News V1 beta Chinese News Translation Chinese Treebank Multiple Translation Chinese Training Development Test Corpus Sentences Words MT-02 878 24,225 MT-05 1082 31,321 MT-03 919 25,619 Table 3: Corpus statistics (number of sentences and words) for the error detection task. Table 2: Training corpora for the translation task. on Xinhua section of the English Gigaword corpus (181.1M words). For minimum error rate tuning (Och, 2003), we use NIST MT-02 as the development set for the translation task. In order to calculate word posterior probabilities, we generate 10,000 best lists for NIST MT-02/03/05 respectively. The performance, in terms of BLEU (Papineni et al., 2002) score, is shown in Table 4. 6 Experiments We conducted our experiments at several levels. Starting with MaxEnt models with single linguistic feature or word posterior probability based feature, we incorporated additional features incrementally by combining features together. In doing so, we would like the experimental results not only to display the effe</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatically Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="17000" citStr="Papineni et al., 2002" startWordPosition="2757" endWordPosition="2760">k Multiple Translation Chinese Training Development Test Corpus Sentences Words MT-02 878 24,225 MT-05 1082 31,321 MT-03 919 25,619 Table 3: Corpus statistics (number of sentences and words) for the error detection task. Table 2: Training corpora for the translation task. on Xinhua section of the English Gigaword corpus (181.1M words). For minimum error rate tuning (Och, 2003), we use NIST MT-02 as the development set for the translation task. In order to calculate word posterior probabilities, we generate 10,000 best lists for NIST MT-02/03/05 respectively. The performance, in terms of BLEU (Papineni et al., 2002) score, is shown in Table 4. 6 Experiments We conducted our experiments at several levels. Starting with MaxEnt models with single linguistic feature or word posterior probability based feature, we incorporated additional features incrementally by combining features together. In doing so, we would like the experimental results not only to display the effectiveness of linguistic features for error detection but also to identify the additional contribution of each feature to the task. 6.1 Data Corpus For the error detection task, we use the best translation hypotheses of NIST MT-02/05/03 generat</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and WeiJing Zhu. 2002. BLEU: a Method for Automatically Evaluation of Machine Translation. In Proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Raybaud</author>
<author>Caroline Lavecchia</author>
<author>David Langlois</author>
<author>Kamel Smaili</author>
</authors>
<title>Word- and Sentence-level Confidence Measures for Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EAMT</booktitle>
<contexts>
<context position="3251" citStr="Raybaud et al., 2009" startWordPosition="497" endWordPosition="500">), or loglinear model (Ueffing and Ney, 2007). 3) Divide words into two groups (correct translations and errors) by using a classification threshold optimized on a development set. Sometimes the step 2) is not necessary if only one effective feature is used (Ueffing and Ney, 2007); and sometimes the step 2) and 3) can be merged into a single step if we directly output predicting results from binary classifiers instead of making thresholding decision. Various features from different SMT models and system outputs are investigated (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007; Raybaud et al., 2009). Experimental results show that they are useful for error detection. However, it is not adequate to just use these features as discussed in (Shi and Zhou, 2005) because the information that they carry is either from the inner components of SMT systems or from system outputs. To some extent, it has already been considered by SMT systems. Hence finding external information 604 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604–611, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sources from outside SMT systems i</context>
<context position="7419" citStr="Raybaud et al. (2009)" startWordPosition="1155" endWordPosition="1158">orrect. All their measures are based on word posterior probabilities, which are estimated from 1) system output, such as word lattices or N-best lists and 2) word or phrase translation table. Their experimental results show that word posterior probabilities directly estimated from phrase translation table are better than those from system output except for the Chinese-English language pair. Sanchis et al. (2007) adopt a smoothed naive Bayes model to combine different word posterior probability based confidence features which are estimated from N-best lists, similar to (Ueffing and Ney, 2007). Raybaud et al. (2009) study several confidence features based on mutual information between words and n-gram and backward n-gram language model for word-level and sentence-level CE. They also explore linguistic features using information from syntactic category, tense, gender and so on. Unfortunately, such linguistic features neither improve performance at the word level nor at the sentence level. Our work departs from the previous work in two major respects. • We exploit various linguistic features and show that they are able to produce larger improvements than widely used system-related features such as word pos</context>
</contexts>
<marker>Raybaud, Lavecchia, Langlois, Smaili, 2009</marker>
<rawString>Sylvain Raybaud, Caroline Lavecchia, David Langlois, Kamel Smaili. 2009. Word- and Sentence-level Confidence Measures for Machine Translation. In Proceedings of EAMT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Sanchis</author>
<author>Alfons Juan</author>
<author>Enrique Vidal</author>
</authors>
<title>Estimation of Confidence Measures for Machine Translation.</title>
<date>2007</date>
<booktitle>In Procedings of Machine Translation</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="2631" citStr="Sanchis et al., 2007" startWordPosition="395" endWordPosition="398">rors at the word level for SMT. A common approach to SMT error detection at the word level is calculating the confidence at which a word is correct. The majority of word confidence estimation methods follows three steps: 1) Calculate features that express the correctness of words either based on SMT model (e.g. translation/language model) or based on SMT system output (e.g. N-best lists, word lattices) (Blatz et al., 2003; Ueffing and Ney, 2007). 2) Combine these features together with a classification model such as multi-layer perceptron (Blatz et al., 2003), Naive Bayes (Blatz et al., 2003; Sanchis et al., 2007), or loglinear model (Ueffing and Ney, 2007). 3) Divide words into two groups (correct translations and errors) by using a classification threshold optimized on a development set. Sometimes the step 2) is not necessary if only one effective feature is used (Ueffing and Ney, 2007); and sometimes the step 2) and 3) can be merged into a single step if we directly output predicting results from binary classifiers instead of making thresholding decision. Various features from different SMT models and system outputs are investigated (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007; R</context>
<context position="7213" citStr="Sanchis et al. (2007)" startWordPosition="1124" endWordPosition="1127">ntic features, according to their final results. Ueffing and Ney (2007) exhaustively explore various word-level confidence measures to label each word in a generated translation hypothesis as correct or incorrect. All their measures are based on word posterior probabilities, which are estimated from 1) system output, such as word lattices or N-best lists and 2) word or phrase translation table. Their experimental results show that word posterior probabilities directly estimated from phrase translation table are better than those from system output except for the Chinese-English language pair. Sanchis et al. (2007) adopt a smoothed naive Bayes model to combine different word posterior probability based confidence features which are estimated from N-best lists, similar to (Ueffing and Ney, 2007). Raybaud et al. (2009) study several confidence features based on mutual information between words and n-gram and backward n-gram language model for word-level and sentence-level CE. They also explore linguistic features using information from syntactic category, tense, gender and so on. Unfortunately, such linguistic features neither improve performance at the word level nor at the sentence level. Our work depar</context>
<context position="12078" citStr="Sanchis et al., 2007" startWordPosition="1908" endWordPosition="1911"> straightforwardly define a syntactic feature for a word w according to its links as follows � yes, w has links link(w) = no, otherwise In Figure 1 we show an example of a generated translation hypothesis with its link parse. Here links are denoted with dotted lines which are annotated with link types (e.g., Jp, Op). Bracketed words, namely “,” and “including”, are null-linked words. 3.3 Word Posterior Probability Features Our word posterior probability is calculated on Nbest list, which is first proposed by (Ueffing et al., 2003) and widely used in (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007). Given a source sentence f, let {en}N1 be the Nbest list generated by an SMT system, and let ein is the i-th word in en. The major work of calculating word posterior probabilities is to find the Levenshtein alignment (Levenshtein, 1966) between the best hypothesis el and its competing hypothesis 3Available at http://www.link.cs.cmu.edu/link/ 606 Figure 1: An example of Link Grammar parsing results. en in the N-best list {en}N1 . We denote the alignment between them as ℓ(e1, en). The word in the hypothesis en which ei1 is Levenshtein aligned to is denoted as ℓi(e1, en). The word posterior prob</context>
</contexts>
<marker>Sanchis, Juan, Vidal, 2007</marker>
<rawString>Alberto Sanchis, Alfons Juan and Enrique Vidal. 2007. Estimation of Confidence Measures for Machine Translation. In Procedings of Machine Translation Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a Link Grammar.</title>
<date>1993</date>
<booktitle>In Proceedings of Third International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="10868" citStr="Sleator and Temperley, 1993" startWordPosition="1703" endWordPosition="1707">o be incorrect. The challenge of using syntactic knowledge for error detection is that machinegenerated hypotheses are rarely fully grammatical. They are mixed with grammatical and ungrammatical parts, which hence are not friendly to traditional parsers trained on grammatical sentences because ungrammatical parts of a machinegenerated sentence could lead to a parsing failure. To overcome this challenge, we select the Link Grammar (LG) parser 3 as our syntactic parser to generate syntactic features. The LG parser produces a set of labeled links which connect pairs of words with a link grammar (Sleator and Temperley, 1993). The main reason why we choose the LG parser is that it provides a robustness feature: null-link scheme. The null-link scheme allows the parser to parse a sentence even when the parser can not fully interpret the entire sentence (e.g. including ungrammatical parts). When the parser fail to parse the entire sentence, it ignores one word each time until it finds linkages for remaining words. After parsing, those ignored words are not connected to any other words. We call them null-linked words. Our hypothesis is that null-linked words are prone to be syntactically incorrect. We hence straightfo</context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Daniel Sleator and Davy Temperley. 1993. Parsing English with a Link Grammar. In Proceedings of Third International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongmei Shi</author>
<author>Lina Zhou</author>
</authors>
<title>Error Detection Using Linguistic Features.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<contexts>
<context position="3412" citStr="Shi and Zhou, 2005" startWordPosition="525" endWordPosition="528"> development set. Sometimes the step 2) is not necessary if only one effective feature is used (Ueffing and Ney, 2007); and sometimes the step 2) and 3) can be merged into a single step if we directly output predicting results from binary classifiers instead of making thresholding decision. Various features from different SMT models and system outputs are investigated (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007; Raybaud et al., 2009). Experimental results show that they are useful for error detection. However, it is not adequate to just use these features as discussed in (Shi and Zhou, 2005) because the information that they carry is either from the inner components of SMT systems or from system outputs. To some extent, it has already been considered by SMT systems. Hence finding external information 604 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604–611, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics sources from outside SMT systems is desired for error detection. Linguistic knowledge is exactly such a good choice as an external information source. It has already been proven effective in erro</context>
</contexts>
<marker>Shi, Zhou, 2005</marker>
<rawString>Yongmei Shi and Lina Zhou. 2005. Error Detection Using Linguistic Features. In Proceedings of HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an Extensible Language Modeling Toolkit. In</title>
<date>2002</date>
<booktitle>Proceedings of International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="15787" citStr="Stolcke, 2002" startWordPosition="2554" endWordPosition="2555"> as correct otherwise incorrect. 5 SMT System To obtain machine-generated translation hypotheses for our error detection, we use a state-of-the-art phrase-based machine translation system MOSES (Koehn et al., 2003; Koehn et al., 2007). The translation task is on the official NIST Chineseto-English evaluation data. The training data consists of more than 4 million pairs of sentences (including 101.93M Chinese words and 112.78M English words) from LDC distributed corpora. Table 2 shows the corpora that we use for the translation task. We build a four-gram language model using the SRILM toolkit (Stolcke, 2002), which is trained p(c|ψ) = Ec′ exp(Ei θifi(c′, ψ)) exp(Ei θifi(c, ψ)) 607 Feature Example wd pos link dwpp � 1, O.w.wd = �.�, c = correct f(c, O) = 0, otherwise � 1, O.w2.pos = �NN�, c = incorrect f(c, O) = 0, otherwise � 1, O.w.link = no, c = incorrect f(c, O) = 0, otherwise � 1, O.w−2.dwpp = 2, c = correct f(c, O) = 0, otherwise Table 1: Examples of model features. LDC ID LDC2004E12 LDC2004T08 LDC2005T10 LDC2003E14 LDC2002E18 LDC2005T06 LDC2003E07 LDC2004T07 Description United Nations Hong Kong News Sinorama Magazine FBIS Xinhua News V1 beta Chinese News Translation Chinese Treebank Multipl</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an Extensible Language Modeling Toolkit. In Proceedings of International Conference on Spoken Language Processing, volume 2, pages 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Klaus Macherey</author>
<author>Hermann Ney</author>
</authors>
<title>Confidence Measures for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings. of MT Summit IX.</booktitle>
<contexts>
<context position="11993" citStr="Ueffing et al., 2003" startWordPosition="1891" endWordPosition="1894">ypothesis is that null-linked words are prone to be syntactically incorrect. We hence straightforwardly define a syntactic feature for a word w according to its links as follows � yes, w has links link(w) = no, otherwise In Figure 1 we show an example of a generated translation hypothesis with its link parse. Here links are denoted with dotted lines which are annotated with link types (e.g., Jp, Op). Bracketed words, namely “,” and “including”, are null-linked words. 3.3 Word Posterior Probability Features Our word posterior probability is calculated on Nbest list, which is first proposed by (Ueffing et al., 2003) and widely used in (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007). Given a source sentence f, let {en}N1 be the Nbest list generated by an SMT system, and let ein is the i-th word in en. The major work of calculating word posterior probabilities is to find the Levenshtein alignment (Levenshtein, 1966) between the best hypothesis el and its competing hypothesis 3Available at http://www.link.cs.cmu.edu/link/ 606 Figure 1: An example of Link Grammar parsing results. en in the N-best list {en}N1 . We denote the alignment between them as ℓ(e1, en). The word in the hypothesis en </context>
</contexts>
<marker>Ueffing, Macherey, Ney, 2003</marker>
<rawString>Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003. Confidence Measures for Statistical Machine Translation. In Proceedings. of MT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<date>2007</date>
<booktitle>WordLevel Confidence Estimation for Machine Translation. Computational Linguistics,</booktitle>
<pages>33--1</pages>
<contexts>
<context position="1636" citStr="Ueffing and Ney, 2007" startWordPosition="226" endWordPosition="229">tion; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%. 1 Introduction Translation hypotheses generated by a statistical machine translation (SMT) system always contain both correct parts (e.g. words, n-grams, phrases matched with reference translations) and incorrect parts. Automatically distinguishing incorrect parts from correct parts is therefore very desirable not only for post-editing and interactive machine translation (Ueffing and Ney, 2007) but also for SMT itself: either by rescoring hypotheses in the N-best list using the probability of correctness calculated for each hypothesis (Zens and Ney, 2006) or by generating new hypotheses using Nbest lists from one SMT system or multiple systems (Akibay et al., 2004; Jayaraman and Lavie, 2005). In this paper we restrict the “parts” to words. That is, we detect errors at the word level for SMT. A common approach to SMT error detection at the word level is calculating the confidence at which a word is correct. The majority of word confidence estimation methods follows three steps: 1) Ca</context>
<context position="2911" citStr="Ueffing and Ney, 2007" startWordPosition="442" endWordPosition="445">s either based on SMT model (e.g. translation/language model) or based on SMT system output (e.g. N-best lists, word lattices) (Blatz et al., 2003; Ueffing and Ney, 2007). 2) Combine these features together with a classification model such as multi-layer perceptron (Blatz et al., 2003), Naive Bayes (Blatz et al., 2003; Sanchis et al., 2007), or loglinear model (Ueffing and Ney, 2007). 3) Divide words into two groups (correct translations and errors) by using a classification threshold optimized on a development set. Sometimes the step 2) is not necessary if only one effective feature is used (Ueffing and Ney, 2007); and sometimes the step 2) and 3) can be merged into a single step if we directly output predicting results from binary classifiers instead of making thresholding decision. Various features from different SMT models and system outputs are investigated (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007; Raybaud et al., 2009). Experimental results show that they are useful for error detection. However, it is not adequate to just use these features as discussed in (Shi and Zhou, 2005) because the information that they carry is either from the inner components of SMT systems or from</context>
<context position="6663" citStr="Ueffing and Ney (2007)" startWordPosition="1041" endWordPosition="1044"> and a naive Bayes classifier to combine various confidence features for confidence estimation at the word level as well as at the sentence level. The features they use for word level CE include word posterior probabilities estimated from N-best lists, features based on SMT models, semantic features extracted from WordNet as well as simple syntactic features, i.e. parentheses and quotation mark check. Among all these features, the word posterior probability is the most effective feature, which is much better than linguistic features such as semantic features, according to their final results. Ueffing and Ney (2007) exhaustively explore various word-level confidence measures to label each word in a generated translation hypothesis as correct or incorrect. All their measures are based on word posterior probabilities, which are estimated from 1) system output, such as word lattices or N-best lists and 2) word or phrase translation table. Their experimental results show that word posterior probabilities directly estimated from phrase translation table are better than those from system output except for the Chinese-English language pair. Sanchis et al. (2007) adopt a smoothed naive Bayes model to combine dif</context>
<context position="12055" citStr="Ueffing and Ney, 2007" startWordPosition="1903" endWordPosition="1907">lly incorrect. We hence straightforwardly define a syntactic feature for a word w according to its links as follows � yes, w has links link(w) = no, otherwise In Figure 1 we show an example of a generated translation hypothesis with its link parse. Here links are denoted with dotted lines which are annotated with link types (e.g., Jp, Op). Bracketed words, namely “,” and “including”, are null-linked words. 3.3 Word Posterior Probability Features Our word posterior probability is calculated on Nbest list, which is first proposed by (Ueffing et al., 2003) and widely used in (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007). Given a source sentence f, let {en}N1 be the Nbest list generated by an SMT system, and let ein is the i-th word in en. The major work of calculating word posterior probabilities is to find the Levenshtein alignment (Levenshtein, 1966) between the best hypothesis el and its competing hypothesis 3Available at http://www.link.cs.cmu.edu/link/ 606 Figure 1: An example of Link Grammar parsing results. en in the N-best list {en}N1 . We denote the alignment between them as ℓ(e1, en). The word in the hypothesis en which ei1 is Levenshtein aligned to is denoted as ℓi(e1, en). </context>
<context position="14751" citStr="Ueffing and Ney, 2007" startWordPosition="2388" endWordPosition="2391">he word w occurs. For classification, we employ the maximum entropy model (Berger et al., 1996) to predict whether a word w is correct or incorrect given its feature vector ψ. where fi is a binary model feature defined on c and the feature vector ψ. θi is the weight of fi. Table 1 shows some examples of our binary model features. In order to learn the model feature weights θ for probability estimation, we need a training set of m samples {ψi, ci}m1 . The challenge of collecting training instances is that the correctness of a word in a generated translation hypothesis is not intuitively clear (Ueffing and Ney, 2007). We will describe the method to determine the correctness of a word in Section 6.1, which is broadly adopted in previous work. We tune our model feature weights using an off-the-shelf MaxEnt toolkit (Zhang, 2004). To avoid overfitting, we optimize the Gaussian prior on the development set. During test, if the probability p(correct|ψ) is larger than p(incorrect|ψ) according the trained MaxEnt model, the word is labeled as correct otherwise incorrect. 5 SMT System To obtain machine-generated translation hypotheses for our error detection, we use a state-of-the-art phrase-based machine translati</context>
<context position="19960" citStr="Ueffing and Ney, 2007" startWordPosition="3234" endWordPosition="3238"> U�n�i�c�o�m� l�a�s�t� y❑eE]a❑r❑ n❑e❑t❑ p❑r❑o❑f�it� r❑ods❑e❑ u❑p❑ 3❑8❑%❑ R❑e❑f❑e❑r❑e❑n❑�❑e❑ C�h�i�n�a� UFn[]i�c❑o❑m❑ n❑e❑t❑ p❑r❑o❑f❑i�tI r❑ods❑e❑ u❑p❑ 3❑8❑%❑ lCb C❑�❑a❑s❑s❑ CEh1�n❑a[/❑c❑ U�n�i�c�om�/�c� l�a�s�t�/�i� y�e�a�r�/�i� n❑e❑t❑/❑c❑ p❑r❑o❑f❑i�t❑/❑c❑ r❑ods❑e❑/❑c❑ u❑pXc❑ 3 Figure 2: Tagging a word as correct/incorrect according to the Levenshtein alignment. Corpus BLEU (%) RCW (%) MT-02 33.24 47.76 MT-05 32.03 47.85 MT-03 32.86 47.57 Table 4: Case-insensitive BLEU score and ratio of correct words (RCW) on the training, development and test corpus. metric corresponds to the m-WER used in (Ueffing and Ney, 2007), which is stricter than PER and Set. It is also stricter than normal WER metric which compares each hypothesis to all references, rather than the nearest reference. Table 4 shows the case-insensitive BLEU score and the percentage of words that are labeled as correct according to the method described above on the training, development and test corpus. 6.2 Evaluation Metrics To evaluate the overall performance of the error detection, we use the commonly used metric, classification error rate (CER) to evaluate our classifiers. CER is defined as the percentage of words that are wrongly tagged as </context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2007. WordLevel Confidence Estimation for Machine Translation. Computational Linguistics, 33(1):9-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>N-gram Posterior Probabilities for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>HLT/NAACL: Proceedings of the Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1800" citStr="Zens and Ney, 2006" startWordPosition="253" endWordPosition="256">error rate by 18.52% and improve the F measure by 16.37%. 1 Introduction Translation hypotheses generated by a statistical machine translation (SMT) system always contain both correct parts (e.g. words, n-grams, phrases matched with reference translations) and incorrect parts. Automatically distinguishing incorrect parts from correct parts is therefore very desirable not only for post-editing and interactive machine translation (Ueffing and Ney, 2007) but also for SMT itself: either by rescoring hypotheses in the N-best list using the probability of correctness calculated for each hypothesis (Zens and Ney, 2006) or by generating new hypotheses using Nbest lists from one SMT system or multiple systems (Akibay et al., 2004; Jayaraman and Lavie, 2005). In this paper we restrict the “parts” to words. That is, we detect errors at the word level for SMT. A common approach to SMT error detection at the word level is calculating the confidence at which a word is correct. The majority of word confidence estimation methods follows three steps: 1) Calculate features that express the correctness of words either based on SMT model (e.g. translation/language model) or based on SMT system output (e.g. N-best lists,</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. N-gram Posterior Probabilities for Statistical Machine Translation. In HLT/NAACL: Proceedings of the Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<date>2004</date>
<booktitle>Maximum Entropy Modeling Tooklkit for Python and C++. Available at http://homepages.inf.ed.ac.uk/s0450736</booktitle>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum Entropy Modeling Tooklkit for Python and C++. Available at http://homepages.inf.ed.ac.uk/s0450736</rawString>
</citation>
<citation valid="false">
<note>maxent toolkit.html.</note>
<marker></marker>
<rawString>/maxent toolkit.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>