<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000328">
<title confidence="0.988121">
A Bayesian Approach to Unsupervised Semantic Role Induction
</title>
<author confidence="0.997536">
Ivan Titov Alexandre Klementiev
</author>
<affiliation confidence="0.997277">
Saarland University
</affiliation>
<address confidence="0.661665">
Saarbr¨ucken, Germany
</address>
<email confidence="0.994551">
{titov|aklement}@mmci.uni-saarland.de
</email>
<sectionHeader confidence="0.994686" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999848347826087">
We introduce two Bayesian models for un-
supervised semantic role labeling (SRL)
task. The models treat SRL as clustering
of syntactic signatures of arguments with
clusters corresponding to semantic roles.
The first model induces these clusterings
independently for each predicate, exploit-
ing the Chinese Restaurant Process (CRP)
as a prior. In a more refined hierarchical
model, we inject the intuition that the clus-
terings are similar across different predi-
cates, even though they are not necessar-
ily identical. This intuition is encoded as
a distance-dependent CRP with a distance
between two syntactic signatures indicating
how likely they are to correspond to a single
semantic role. These distances are automat-
ically induced within the model and shared
across predicates. Both models achieve
state-of-the-art results when evaluated on
PropBank, with the coupled model consis-
tently outperforming the factored counter-
part in all experimental set-ups.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999178">
Semantic role labeling (SRL) (Gildea and Juraf-
sky, 2002), a shallow semantic parsing task, has
recently attracted a lot of attention in the com-
putational linguistic community (Carreras and
M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et
al., 2009). The task involves prediction of predi-
cate argument structure, i.e. both identification of
arguments as well as assignment of labels accord-
ing to their underlying semantic role. For exam-
ple, in the following sentences:
</bodyText>
<listItem confidence="0.95971275">
(a) [A0 Mary] opened [A1 the door].
(b) [A0 Mary] is expected to open [A1 the door].
(c) [A1 The door] opened.
(d) [A1 The door] was opened [A0 by Mary].
</listItem>
<bodyText confidence="0.999609625">
Mary always takes an agent role (A0) for the pred-
icate open, and door is always a patient (A1).
SRL representations have many potential appli-
cations in natural language processing and have
recently been shown to be beneficial in question
answering (Shen and Lapata, 2007; Kaisser and
Webber, 2007), textual entailment (Sammons et
al., 2009), machine translation (Wu and Fung,
2009; Liu and Gildea, 2010; Wu et al., 2011; Gao
and Vogel, 2011), and dialogue systems (Basili et
al., 2009; van der Plas et al., 2011), among others.
Though syntactic representations are often predic-
tive of semantic roles (Levin, 1993), the interface
between syntactic and semantic representations is
far from trivial. The lack of simple determinis-
tic rules for mapping syntax to shallow semantics
motivates the use of statistical methods.
Although current statistical approaches have
been successful in predicting shallow seman-
tic representations, they typically require large
amounts of annotated data to estimate model pa-
rameters. These resources are scarce and ex-
pensive to create, and even the largest of them
have low coverage (Palmer and Sporleder, 2010).
Moreover, these models are domain-specific, and
their performance drops substantially when they
are used in a new domain (Pradhan et al., 2008).
Such domain specificity is arguably unavoidable
for a semantic analyzer, as even the definitions
of semantic roles are typically predicate specific,
and different domains can have radically different
distributions of predicates (and their senses). The
necessity for a large amounts of human-annotated
data for every language and domain is one of the
major obstacles to the wide-spread adoption of se-
mantic role representations.
These challenges motivate the need for unsu-
pervised methods which, instead of relying on la-
beled data, can exploit large amounts of unlabeled
texts. In this paper, we propose simple and effi-
</bodyText>
<page confidence="0.984484">
12
</page>
<note confidence="0.9767135">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999756479452055">
cient hierarchical Bayesian models for this task.
It is natural to split the SRL task into two
stages: the identification of arguments (the iden-
tification stage) and the assignment of semantic
roles (the labeling stage). In this and in much
of the previous work on unsupervised techniques,
the focus is on the labeling stage. Identification,
though an important problem, can be tackled with
heuristics (Lang and Lapata, 2011a; Grenager and
Manning, 2006) or, potentially, by using a super-
vised classifier trained on a small amount of data.
We follow (Lang and Lapata, 2011a), and regard
the labeling stage as clustering of syntactic sig-
natures of argument realizations for every predi-
cate. In our first model, as in most of the previous
work on unsupervised SRL, we define an indepen-
dent model for each predicate. We use the Chi-
nese Restaurant Process (CRP) (Ferguson, 1973)
as a prior for the clustering of syntactic signatures.
The resulting model achieves state-of-the-art re-
sults, substantially outperforming previous meth-
ods evaluated in the same setting.
In the first model, for each predicate we inde-
pendently induce a linking between syntax and se-
mantics, encoded as a clustering of syntactic sig-
natures. The clustering implicitly defines the set
of permissible alternations, or changes in the syn-
tactic realization of the argument structure of the
verb. Though different verbs admit different alter-
nations, some alternations are shared across mul-
tiple verbs and are very frequent (e.g., passiviza-
tion, example sentences (a) vs. (d), or dativiza-
tion: John gave a book to Mary vs. John gave
Mary a book) (Levin, 1993). Therefore, it is nat-
ural to assume that the clusterings should be sim-
ilar, though not identical, across verbs.
Our second model encodes this intuition by re-
placing the CRP prior for each predicate with
a distance-dependent CRP (dd-CRP) prior (Blei
and Frazier, 2011) shared across predicates. The
distance between two syntactic signatures en-
codes how likely they are to correspond to a sin-
gle semantic role. Unlike most of the previous
work exploiting distance-dependent CRPs (Blei
and Frazier, 2011; Socher et al., 2011; Duan et al.,
2007), we do not encode prior or external knowl-
edge in the distance function but rather induce it
automatically within our Bayesian model. The
coupled dd-CRP model consistently outperforms
the factored CRP counterpart across all the experi-
mental settings (with gold and predicted syntactic
parses, and with gold and automatically identified
arguments).
Both models admit efficient inference: the es-
timation time on the Penn Treebank WSJ corpus
does not exceed 30 minutes on a single proces-
sor and the inference algorithm is highly paral-
lelizable, reducing inference time down to sev-
eral minutes on multiple processors. This sug-
gests that the models scale to much larger corpora,
which is an important property for a successful
unsupervised learning method, as unlabeled data
is abundant.
The rest of the paper is structured as follows.
Section 2 begins with a definition of the seman-
tic role labeling task and discuss some specifics
of the unsupervised setting. In Section 3, we de-
scribe CRPs and dd-CRPs, the key components
of our models. In Sections 4 – 6, we describe
our factored and coupled models and the infer-
ence method. Section 7 provides both evaluation
and analysis. Finally, additional related work is
presented in Section 8.
</bodyText>
<sectionHeader confidence="0.973103" genericHeader="introduction">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.999948166666667">
In this work, instead of assuming the availabil-
ity of role annotated data, we rely only on auto-
matically generated syntactic dependency graphs.
While we cannot expect that syntactic structure
can trivially map to a semantic representation
(Palmer et al., 2005)1, we can use syntactic cues
to help us in both stages of unsupervised SRL.
Before defining our task, let us consider the two
stages separately.
In the argument identification stage, we imple-
ment a heuristic proposed in (Lang and Lapata,
2011a) comprised of a list of 8 rules, which use
nonlexicalized properties of syntactic paths be-
tween a predicate and a candidate argument to it-
eratively discard non-arguments from the list of
all words in a sentence. Note that inducing these
rules for a new language would require some lin-
guistic expertise. One alternative may be to an-
notate a small number of arguments and train a
classifier with nonlexicalized features instead.
In the argument labeling stage, semantic roles
are represented by clusters of arguments, and la-
beling a particular argument corresponds to decid-
ing on its role cluster. However, instead of deal-
</bodyText>
<footnote confidence="0.995877666666667">
1Although it provides a strong baseline which is diffi-
cult to beat (Grenager and Manning, 2006; Lang and Lapata,
2010; Lang and Lapata, 2011a).
</footnote>
<page confidence="0.999582">
13
</page>
<bodyText confidence="0.999908">
ing with argument occurrences directly, we rep-
resent them as predicate specific syntactic signa-
tures, and refer to them as argument keys. This
representation aids our models in inducing high
purity clusters (of argument keys) while reducing
their granularity. We follow (Lang and Lapata,
2011a) and use the following syntactic features to
form the argument key representation:
</bodyText>
<listItem confidence="0.9995926">
• Active or passive verb voice (ACT/PASS).
• Argument position relative to predicate
(LEFT/RIGHT).
• Syntactic relation to its governor.
• Preposition used for argument realization.
</listItem>
<bodyText confidence="0.867917923076923">
In the example sentences in Section 1, the argu-
ment keys for candidate arguments Mary for sen-
tences (a) and (d) would be ACT:LEFT:SBJ and
PASS:RIGHT:LGS-&gt;by,2 respectively. While
aiming to increase the purity of argument key
clusters, this particular representation will not al-
ways produce a good match: e.g. the door in
sentence (c) will have the same key as Mary in
sentence (a). Increasing the expressiveness of the
argument key representation by flagging intransi-
tive constructions would distinguish that pair of
arguments. However, we keep this particular rep-
resentation, in part to compare with the previous
work.
In this work, we treat the unsupervised seman-
tic role labeling task as clustering of argument
keys. Thus, argument occurrences in the corpus
whose keys are clustered together are assigned the
same semantic role. Note that some adjunct-like
modifier arguments are already explicitly repre-
sented in syntax and thus do not need to be clus-
tered (modifiers AM-TMP, AM-MNR, AM-LOC, and
AM-DIR are encoded as ‘syntactic’ relations TMP,
MNR, LOC, and DIR, respectively (Surdeanu et al.,
2008)); instead we directly use the syntactic labels
as semantic roles.
</bodyText>
<sectionHeader confidence="0.979977" genericHeader="method">
3 Traditional and Distance-dependent
CRPs
</sectionHeader>
<bodyText confidence="0.994719166666667">
The central components of our non-parametric
Bayesian models are the Chinese Restaurant Pro-
cesses (CRPs) and the closely related Dirichlet
Processes (DPs) (Ferguson, 1973).
CRPs define probability distributions over par-
titions of a set of objects. An intuitive metaphor
</bodyText>
<footnote confidence="0.983801">
2LGS denotes a logical subject in a passive construction
(Surdeanu et al., 2008).
</footnote>
<bodyText confidence="0.995766909090909">
for describing CRPs is assignment of tables to
restaurant customers. Assume a restaurant with a
sequence of tables, and customers who walk into
the restaurant one at a time and choose a table to
join. The first customer to enter is assigned the
first table. Suppose that when a client number i
enters the restaurant, i − 1 customers are sitting
at each of the k E (1, ... , K) tables occupied so
far. The new customer is then either seated at one
of the K tables with probability ��
i−1+α, where Nk
is the number customers already sitting at table
k, or assigned to a new table with the probability
i−1+α. The concentration parameter α encodes
α
the granularity of the drawn partitions: the larger
α, the larger the expected number of occupied ta-
bles. Though it is convenient to describe CRP in a
sequential manner, the probability of a seating ar-
rangement is invariant of the order of customers’
arrival, i.e. the process is exchangeable. In our
factored model, we use CRPs as a prior for clus-
tering argument keys, as we explain in Section 4.
Often CRP is used as a part of the Dirich-
let Process mixture model where each subset in
the partition (each table) selects a parameter (a
meal) from some base distribution over parame-
ters. This parameter is then used to generate all
data points corresponding to customers assigned
to the table. The Dirichlet processes (DP) are
closely connected to CRPs: instead of choosing
meals for customers through the described gener-
ative story, one can equivalently draw a distribu-
tion G over meals from DP and then draw a meal
for every customer from G. We refer the reader
to Teh (2010) for details on CRPs and DPs. In
our method, we use DPs to model distributions of
arguments for every role.
In order to clarify how similarities between
customers can be integrated in the generative pro-
cess, we start by reformulating the traditional
CRP in an equivalent form so that distance-
dependent CRP (dd-CRP) can be seen as its gen-
eralization. Instead of selecting a table for each
customer as described above, one can equiva-
lently assume that a customer i chooses one of
the previous customers ci as a partner with prob-
ability 1
i−1+α and sits at the same table, or occu-
pies a new table with the probability α
i−1+α. The
transitive closure of this seating-with relation de-
termines the partition.
A generalization of this view leads to the defini-
tion of the distance-dependent CRP. In dd-CRPs,
</bodyText>
<page confidence="0.994371">
14
</page>
<bodyText confidence="0.99972975">
a customer i chooses a partner ci = j with
the probability proportional to some non-negative
score di,j (di,j = dj,i) which encodes a similarity
between the two customers.3 More formally,
</bodyText>
<equation confidence="0.992507666666667">
� ( )
p(ci = j |D, α) a di,j, i 7� j 1
α, i = j
</equation>
<bodyText confidence="0.99819975">
where D is the entire similarity graph. This pro-
cess lacks the exchangeability property of the tra-
ditional CRP but efficient approximate inference
with dd-CRP is possible with Gibbs sampling.
For more details on inference with dd-CRPs, we
refer the reader to Blei and Frazier (2011).
Though in previous work dd-CRP was used ei-
ther to encode prior knowledge (Blei and Fra-
zier, 2011) or other external information (Socher
et al., 2011), we treat D as a latent variable
drawn from some prior distribution over weighted
graphs. This view provides a powerful approach
for coupling a family of distinct but similar clus-
terings: the family of clusterings can be drawn by
first choosing a similarity graph D for the entire
family and then re-using D to generate each of the
clusterings independently of each other as defined
by equation (1). In Section 5, we explain how we
use this formalism to encode relatedness between
argument key clusterings for different predicates.
</bodyText>
<sectionHeader confidence="0.998916" genericHeader="method">
4 Factored Model
</sectionHeader>
<bodyText confidence="0.987177253333334">
In this section we describe the factored method
which models each predicate independently. In
Section 2 we defined our task as clustering of ar-
gument keys, where each cluster corresponds to a
semantic role. If an argument key k is assigned
to a role r (k E r), all of its occurrences are la-
beled r.
Our Bayesian model encodes two common as-
sumptions about semantic roles. First, we enforce
the selectional restriction assumption: we assume
that the distribution over potential argument fillers
is sparse for every role, implying that ‘peaky’ dis-
tributions of arguments for each role r are pre-
ferred to flat distributions. Second, each role nor-
mally appears at most once per predicate occur-
rence. Our inference will search for a clustering
which meets the above requirements to the maxi-
mal extent.
3It may be more standard to use a decay function f :
R --+ R and choose a partner with the probability propor-
tional to f(−dz,j). However, the two forms are equivalent
and using scores d,,j directly is more convenient for our in-
duction purposes.
Our model associates two distributions with
each predicate: one governs the selection of argu-
ment fillers for each semantic role, and the other
models (and penalizes) duplicate occurrence of
roles. Each predicate occurrence is generated in-
dependently given these distributions. Let us de-
scribe the model by first defining how the set of
model parameters and an argument key clustering
are drawn, and then explaining the generation of
individual predicate and argument instances. The
generative story is formally presented in Figure 1.
We start by generating a partition of argument
keys Bp with each subset r E Bp representing
a single semantic role. The partitions are drawn
from CRP(α) (see the Factored model section of
Figure 1) independently for each predicate. The
crucial part of the model is the set of selectional
preference parameters Op,r, the distributions of ar-
guments x for each role r of predicate p. We
represent arguments by their syntactic heads,4 or
more specifically, by either their lemmas or word
clusters assigned to the head by an external clus-
tering algorithm, as we will discuss in more detail
in Section 7.5 For the agent role A0 of the pred-
icate open, for example, this distribution would
assign most of the probability mass to arguments
denoting sentient beings, whereas the distribution
for the patient role A1 would concentrate on ar-
guments representing “openable” things (doors,
boxes, books, etc).
In order to encode the assumption about sparse-
ness of the distributions Op,r, we draw them from
the DP prior DP(Q, H(A)) with a small concen-
tration parameter Q, the base probability distribu-
tion H(A) is just the normalized frequencies of ar-
guments in the corpus. The geometric distribution
Op,r is used to model the number of times a role
r appears with a given predicate occurrence. The
decision whether to generate at least one role r is
drawn from the uniform Bernoulli distribution. If
0 is drawn then the semantic role is not realized
for the given occurrence, otherwise the number
of additional roles r is drawn from the geometric
distribution Geom(op,r). The Beta priors over 0
4For prepositional phrases, we take as head the head noun
of the object noun phrase as it encodes crucial lexical infor-
mation. However, the preposition is not ignored but rather
encoded in the corresponding argument key, as explained
in Section 2.
5Alternatively, the clustering of arguments could be in-
duced within the model, as done in (Titov and Klementiev,
2011).
</bodyText>
<page confidence="0.96728">
15
</page>
<table confidence="0.999757166666667">
Clustering of argument keys:
Factored model:
for each predicate p = 1, 2, ... :
Bp ∼ CRP(a) [partition of arg keys]
Coupled model:
D ∼ NonInform [similarity graph]
for each predicate p = 1, 2, ... :
Bp ∼ dd-CRP(a, D) [partition of arg keys]
Parameters:
for each predicate p = 1, 2,... :
for each role r ∈ Bp:
Bp,r ∼ DP(,Q, H(A)) [distrib of arg fillers]
4&apos;p,r ∼Beta(770, 771) [geom distr for dup roles]
Data Generation:
for each predicate p = 1, 2,... :
for each occurrence l of p:
for every role r ∈ Bp:
if [n ∼ Unif(0, 1)] = 1: [role appears at least once]
GenArgument(p, r) [draw one arg]
while [n ∼ �p,r] = 1: [continue generation]
GenArgument(p, r) [draw more args]
GenArgument(p, r):
kp,r ∼ Unif(1, ... ,|r|) [draw arg key]
xp,r ∼ Bp,r [draw arg filler]
</table>
<figureCaption confidence="0.998424">
Figure 1: Generative stories for the factored and cou-
pled models.
</figureCaption>
<bodyText confidence="0.999888555555555">
can indicate the preference towards generating at
most one argument for each role. For example,
it would express the preference that a predicate
open typically appears with a single agent and a
single patient arguments.
Now, when parameters and argument key clus-
terings are chosen, we can summarize the re-
mainder of the generative story as follows. We
begin by independently drawing occurrences for
each predicate. For each predicate role we in-
dependently decide on the number of role occur-
rences. Then we generate each of the arguments
(see GenArgument) by generating an argument
key kp,r uniformly from the set of argument keys
assigned to the cluster r, and finally choosing its
filler xp,r, where the filler is either a lemma or a
word cluster corresponding to the syntactic head
of the argument.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="method">
5 Coupled Model
</sectionHeader>
<bodyText confidence="0.999309882352941">
As we argued in Section 1, clusterings of argu-
ment keys implicitly encode the pattern of alter-
nations for a predicate. E.g., passivization can be
roughly represented with the clustering of the key
ACT:LEFT:SBJ with PASS:RIGHT:LGS-&gt;by
and ACT:RIGHT:OBJ with PASS:LEFT:SBJ.
The set of permissible alternations is predicate-
specific,6 but nevertheless they arguably repre-
sent a small subset of all clusterings of argu-
ment keys. Also, some alternations are more
likely to be applicable to a verb than others: for
example, passivization and dativization alterna-
tions are both fairly frequent, whereas, locative-
preposition-drop alternation (Mary climbed up the
mountain vs. Mary climbed the mountain) is less
common and applicable only to several classes
of predicates representing motion (Levin, 1993).
We represent this observation by quantifying how
likely a pair of keys is to be clustered. These
scores (di,� for every pair of argument keys i and
j) are induced automatically within the model,
and treated as latent variables shared across pred-
icates. Intuitively, if data for several predicates
strongly suggests that two argument keys should
be clustered (e.g., there is a large overlap be-
tween argument fillers for the two keys) then the
posterior will indicate that di,� is expected to be
greater for the pair {i, j} than for some other pair
{i&apos;, j&apos;} for which the evidence is less clear. Con-
sequently, argument keys i and j will be clustered
even for predicates without strong evidence for
such a clustering, whereas i&apos; and j&apos; will not.
One argument against coupling predicates may
stem from the fact that we are using unlabeled
data and may be able to obtain sufficient amount
of learning material even for less frequent pred-
icates. This may be a valid observation, but an-
other rationale for sharing this similarity structure
is the hypothesis that alternations may be easier
to detect for some predicates than for others. For
example, argument key clustering of predicates
with very restrictive selectional restrictions on ar-
gument fillers is presumably easier than clustering
for predicates with less restrictive and overlap-
ping selectional restriction, as compactness of se-
lectional preferences is a central assumption driv-
ing unsupervised learning of semantic roles. E.g.,
predicates change and defrost belong to the same
Levin class (change-of-state verbs) and therefore
admit similar alternations. However, the set of po-
tential patients of defrost is sufficiently restricted,
</bodyText>
<footnote confidence="0.893421">
6Or, at least specific to a class of predicates (Levin,
1993).
</footnote>
<page confidence="0.996589">
16
</page>
<bodyText confidence="0.999991333333333">
whereas the selectional restrictions for the patient
of change are far less specific and they overlap
with selectional restrictions for the agent role, fur-
ther complicating the clustering induction task.
This observation suggests that sharing clustering
preferences across verbs is likely to help even if
the unlabeled data is plentiful for every predicate.
More formally, we generate scores di,j, or
equivalently, the full labeled graph D with ver-
tices corresponding to argument keys and edges
weighted with the similarity scores, from a prior.
In our experiments we use a non-informative prior
which factorizes over pairs (i.e. edges of the
graph D), though more powerful alternatives can
be considered. Then we use it, in a dd-CRP(α,
D), to generate clusterings of argument keys for
every predicate. The rest of the generative story is
the same as for the factored model. The part rele-
vant to this model is shown in the Coupled model
section of Figure 1.
Note that this approach does not assume that
the frequencies of syntactic patterns correspond-
ing to alternations are similar, and a large value
for di,j does not necessarily mean that the corre-
sponding syntactic frames i and j are very fre-
quent in a corpus. What it indicates is that a large
number of different predicates undergo the corre-
sponding alternation; the frequency of the alterna-
tion is a different matter. We believe that this is an
important point, as we do not make a restricting
assumption that an alternation has the same dis-
tributional properties for all verbs which undergo
this alternation.
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="method">
6 Inference
</sectionHeader>
<bodyText confidence="0.999984888888889">
An inference algorithm for an unsupervised
model should be efficient enough to handle vast
amounts of unlabeled data, as it can easily be ob-
tained and is likely to improve results. We use
a simple approximate inference algorithm based
on greedy MAP search. We start by discussing
MAP search for argument key clustering with the
factored model and then discuss its extension ap-
plicable to the coupled model.
</bodyText>
<subsectionHeader confidence="0.983743">
6.1 Role Induction
</subsectionHeader>
<bodyText confidence="0.999988130434783">
For the factored model, semantic roles for every
predicate are induced independently. Neverthe-
less, search for a MAP clustering can be expen-
sive, as even a move involving a single argument
key implies some computations for all its occur-
rences in the corpus. Instead of more complex
MAP search algorithms (see, e.g., (Daume III,
2007)), we use a greedy procedure where we start
with each argument key assigned to an individual
cluster, and then iteratively try to merge clusters.
Each move involves (1) choosing an argument key
and (2) deciding on a cluster to reassign it to. This
is done by considering all clusters (including cre-
ating a new one) and choosing the most probable
one.
Instead of choosing argument keys randomly at
the first stage, we order them by corpus frequency.
This ordering is beneficial as getting clustering
right for frequent argument keys is more impor-
tant and the corresponding decisions should be
made earlier.7 We used a single iteration in our
experiments, as we have not noticed any benefit
from using multiple iterations.
</bodyText>
<subsectionHeader confidence="0.999503">
6.2 Similarity Graph Induction
</subsectionHeader>
<bodyText confidence="0.999254882352941">
In the coupled model, clusterings for different
predicates are statistically dependent, as the simi-
larity structure D is latent and shared across pred-
icates. Consequently, a more complex inference
procedure is needed. For simplicity here and in
our experiments, we use the non-informative prior
distribution over D which assigns the same prior
probability to every possible weight di,j for every
pair {i, j}.
Recall that the dd-CRP prior is defined in terms
of customers choosing other customers to sit with.
For the moment, let us assume that this relation
among argument keys is known, that is, every ar-
gument key k for predicate p has chosen an argu-
ment key cp,k to ‘sit’ with. We can compute the
MAP estimate for all di,j by maximizing the ob-
jective:
</bodyText>
<equation confidence="0.899098666666667">
1: 1: dk,c�,k
arg max log
dzj, i7�j p kEK Ek&apos;EKI, dk,k&apos;
</equation>
<bodyText confidence="0.999971285714286">
where Kp is the set of all argument keys for the
predicate p. We slightly abuse the notation by us-
ing di,i to denote the concentration parameter α
in the previous expression. Note that we also as-
sume that similarities are symmetric, di,j = dj,i.
If the set of argument keys Kp would be the same
for every predicate, then the optimal di,j would
</bodyText>
<footnote confidence="0.983446666666667">
7This idea has been explored before for shallow semantic
representations (Lang and Lapata, 2011a; Titov and Klemen-
tiev, 2011).
</footnote>
<page confidence="0.6641795">
,
17
</page>
<bodyText confidence="0.999860791666667">
be proportional to the number of times either i se-
lects j as a partner, or j chooses i as a partner.8
This no longer holds if the sets are different, but
the solution can be found efficiently using a nu-
meric optimization strategy; we use the gradient
descent algorithm.
We do not learn the concentration parameter
α, as it is used in our model to indicate the de-
sired granularity of semantic roles, but instead
only learn di,j (i =� j). However, just learning
the concentration parameter would not be suffi-
cient as the effective concentration can be reduced
or increased arbitrarily by scaling all the similar-
ities di,j (i =� j) at once, as follows from expres-
sion (1). Instead, we enforce the normalization
constraint on the similarities di,j. We ensure that
the prior probability of choosing itself as a part-
ner, averaged over predicates, is the same as it
would be with uniform di,j (di,j = 1 for every
key pair {i, j}, i =� j). This roughly says that
we want to preserve the same granularity of clus-
tering as it was with the uniform similarities. We
accomplish this normalization in a post-hoc fash-
ion by dividing the weights after optimization by
</bodyText>
<equation confidence="0.7402965">
E Ek,k&apos;�K,, k&apos;��k dk,k&apos;/ Ep |Kp|(|Kp |− 1).
p
</equation>
<bodyText confidence="0.998322772727273">
If D is fixed, partners for every predicate p and
every k can be found using virtually the same al-
gorithm as in Section 6.1: the only difference is
that, instead of a cluster, each argument key itera-
tively chooses a partner.
Though, in practice, both the choice of partners
and the similarity graphs are latent, we can use an
iterative approach to obtain a joint MAP estimate
of ck (for every k) and the similarity graph D by
alternating the two steps.9
Notice that the resulting algorithm is again
highly parallelizable: the graph induction stage
is fast, and induction of the seat-with relation
(i.e. clustering argument keys) is factorizable over
predicates.
One shortcoming of this approach is typical
for generative models with multiple ‘features’:
when such a model predicts a latent variable, it
tends to ignore the prior class distribution and re-
lies solely on features. This behavior is due to
the over-simplifying independence assumptions.
It is well known, for instance, that the poste-
</bodyText>
<footnote confidence="0.9809005">
8Note that weights dzj are invariant under rescaling
when the rescaling is also applied to the concentration pa-
rameter α.
9In practice, two iterations were sufficient.
</footnote>
<bodyText confidence="0.9970681">
rior with Naive Bayes tends to be overconfident
due to violated conditional independence assump-
tions (Rennie, 2001). The same behavior is ob-
served here: the shared prior does not have suf-
ficient effect on frequent predicates.10 Though
different techniques have been developed to dis-
count the over-confidence (Kolcz and Chowdhury,
2005), we use the most basic one: we raise the
likelihood term in power T, where the parameter
T is chosen empirically.
</bodyText>
<sectionHeader confidence="0.942777" genericHeader="method">
7 Empirical Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998494">
7.1 Data and Evaluation
</subsectionHeader>
<bodyText confidence="0.9998821">
We keep the general setup of (Lang and Lapata,
2011a), to evaluate our models and compare them
to the current state of the art. We run all of our
experiments on the standard CoNLL 2008 shared
task (Surdeanu et al., 2008) version of Penn Tree-
bank WSJ and PropBank. In addition to gold
dependency analyses and gold PropBank annota-
tions, it has dependency structures generated au-
tomatically by the MaltParser (Nivre et al., 2007).
We vary our experimental setup as follows:
</bodyText>
<listItem confidence="0.96216375">
• We evaluate our models on gold and auto-
matically generated parses, and use either
gold PropBank annotations or the heuristic
from Section 2 to identify arguments, result-
ing in four experimental regimes.
• In order to reduce the sparsity of predicate
argument fillers we consider replacing lem-
mas of their syntactic heads with word clus-
</listItem>
<bodyText confidence="0.984649416666667">
ters induced by a clustering algorithm as a
preprocessing step. In particular, we use
Brown (Br) clustering (Brown et al., 1992)
induced over RCV1 corpus (Turian et al.,
2010). Although the clustering is hierarchi-
cal, we only use a cluster at the lowest level
of the hierarchy for each word.
We use the purity (PU) and collocation (CO) met-
rics as well as their harmonic mean (F1) to mea-
sure the quality of the resulting clusters. Purity
measures the degree to which each cluster con-
tains arguments sharing the same gold role:
</bodyText>
<equation confidence="0.989943666666667">
1 �
P U = �
i
</equation>
<bodyText confidence="0.9959395">
where if Ci is the set of arguments in the i-th in-
duced cluster, Gj is the set of arguments in the jth
</bodyText>
<footnote confidence="0.7333945">
10The coupled model without discounting still outper-
forms the factored counterpart in our experiments.
max |Gj n Ci|
j
</footnote>
<page confidence="0.995431">
18
</page>
<bodyText confidence="0.999553">
gold cluster, and N is the total number of argu-
ments. Collocation evaluates the degree to which
arguments with the same gold roles are assigned
to a single cluster. It is computed as follows:
</bodyText>
<equation confidence="0.975071">
1 �
CO = N
9
</equation>
<bodyText confidence="0.99986094117647">
We compute the aggregate PU, CO, and F1
scores over all predicates in the same way as
(Lang and Lapata, 2011a) by weighting the scores
of each predicate by the number of its argument
occurrences. Note that since our goal is to evalu-
ate the clustering algorithms, we do not include
incorrectly identified arguments (i.e. mistakes
made by the heuristic defined in Section 2) when
computing these metrics.
We evaluate both factored and coupled models
proposed in this work with and without Brown
word clustering of argument fillers (Factored,
Coupled, Factored+Br, Coupled+Br). Our mod-
els are robust to parameter settings, they were
tuned (to an order of magnitude) on the develop-
ment set and were the same for all model variants:
α = 1.e-3, β = 1.e-3, η0 = 1.e-3, η1 = 1.e-10,
T = 5. Although they can be induced within the
model, we set them by hand to indicate granular-
ity preferences. We compare our results with the
following alternative approaches. The syntactic
function baseline (SyntF) simply clusters predi-
cate arguments according to the dependency re-
lation to their head. Following (Lang and Lapata,
2010), we allocate a cluster for each of 20 most
frequent relations in the CoNLL dataset and one
cluster for all other relations. We also compare
our performance with the Latent Logistic classifi-
cation (Lang and Lapata, 2010), Split-Merge clus-
tering (Lang and Lapata, 2011a), and Graph Parti-
tioning (Lang and Lapata, 2011b) approaches (la-
beled LLogistic, SplitMerge, and GraphPart, re-
spectively) which achieve the current best unsu-
pervised SRL results in this setting.
</bodyText>
<subsectionHeader confidence="0.805084">
7.2 Results
7.2.1 Gold Arguments
</subsectionHeader>
<bodyText confidence="0.999314142857143">
Experimental results are summarized in Ta-
ble 1. We begin by comparing our models to the
three existing clustering approaches on gold syn-
tactic parses, and using gold PropBank annota-
tions to identify predicate arguments. In this set of
experiments we measure the relative performance
of argument clustering, removing the identifica-
</bodyText>
<table confidence="0.9984863">
gold parses auto parses
PU CO F1 PU CO F1
LLogistic 79.5 76.5 78.0 77.9 74.4 76.2
SplitMerge 88.7 73.0 80.1 86.5 69.8 77.3
GraphPart 88.6 70.7 78.6 87.4 65.9 75.2
Factored 88.1 77.1 82.2 85.1 71.8 77.9
Coupled 89.3 76.6 82.5 86.7 71.2 78.2
Factored+Br 86.8 78.8 82.6 83.8 74.1 78.6
Coupled+Br 88.7 78.1 83.0 86.2 72.7 78.8
SyntF 81.6 77.5 79.5 77.1 70.9 73.9
</table>
<tableCaption confidence="0.974981333333333">
Table 1: Argument clustering performance with gold
argument identification. Bold-face is used to highlight
the best F1 scores.
</tableCaption>
<bodyText confidence="0.99996905">
tion stage, and minimize the noise due to auto-
matic syntactic annotations. All four variants of
the models we propose substantially outperform
other models: the coupled model with Brown
clustering of argument fillers (Coupled+Br) beats
the previous best model SplitMerge by 2.9% F1
score. As mentioned in Section 2, our approach
specifically does not cluster some of the modifier
arguments. In order to verify that this and argu-
ment filler clustering were not the only aspects
of our approach contributing to performance im-
provements, we also evaluated our coupled model
without Brown clustering and treating modifiers
as regular arguments. The model achieves 89.2%
purity, 74.0% collocation, and 80.9% F1 scores,
still substantially outperforming all of the alter-
native approaches. Replacing gold parses with
MaltParser analyses we see a similar trend, where
Coupled+Br outperforms the best alternative ap-
proach SplitMerge by 1.5%.
</bodyText>
<subsectionHeader confidence="0.810274">
7.2.2 Automatic Arguments
</subsectionHeader>
<bodyText confidence="0.956611882352941">
Results are summarized in Table 2.11 The
precision and recall of our re-implementation of
the argument identification heuristic described in
Section 2 on gold parses were 87.7% and 88.0%,
respectively, and do not quite match 88.1% and
87.9% reported in (Lang and Lapata, 2011a).
Since we could not reproduce their argument
identification stage exactly, we are omitting their
results for the two regimes, instead including the
results for our two best models Factored+Br and
Coupled+Br. We see a similar trend, where the
coupled system consistently outperforms its fac-
tored counterpart, achieving 85.8% and 83.9% F1
11Note, that the scores are computed on correctly iden-
tified arguments only, and tend to be higher in these ex-
periments probably because the complex arguments get dis-
carded by the heuristic.
</bodyText>
<equation confidence="0.581487">
max
i
|G; \ Ci|
</equation>
<page confidence="0.960902">
19
</page>
<table confidence="0.9980418">
gold parses auto parses
PU CO F1 PU CO F1
Factored+Br 87.8 82.9 85.3 85.8 81.1 83.4
Coupled+Br 89.2 82.6 85.8 87.4 80.7 83.9
SyntF 83.5 81.4 82.4 81.4 79.1 80.2
</table>
<tableCaption confidence="0.883585">
Table 2: Argument clustering performance with auto-
matic argument identification.
</tableCaption>
<bodyText confidence="0.997910166666667">
for gold and MaltParser analyses, respectively.
We observe that consistently through the four
regimes, sharing of alternations between predi-
cates captured by the coupled model outperforms
the factored version, and that reducing the argu-
ment filler sparsity with clustering also has a sub-
stantial positive effect. Due to the space con-
straints we are not able to present detailed anal-
ysis of the induced similarity graph D, however,
argument-key pairs with the highest induced sim-
ilarity encode, among other things, passivization,
benefactive alternations, near-interchangeability
of some subordinating conjunctions and preposi-
tions (e.g., if and whether), as well as, restoring
some of the unnecessary splits introduced by the
argument key definition (e.g., semantic roles for
adverbials do not normally depend on whether the
construction is passive or active).
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999962226415095">
Most of SRL research has focused on the super-
vised setting (Carreras and M`arquez, 2005; Sur-
deanu et al., 2008), however, lack of annotated re-
sources for most languages and insufficient cover-
age provided by the existing resources motivates
the need for using unlabeled data or other forms
of weak supervision. This work includes methods
based on graph alignment between labeled and
unlabeled data (F¨urstenau and Lapata, 2009), us-
ing unlabeled data to improve lexical generaliza-
tion (Deschacht and Moens, 2009), and projection
of annotation across languages (Pado and Lapata,
2009; van der Plas et al., 2011). Semi-supervised
and weakly-supervised techniques have also been
explored for other types of semantic representa-
tions but these studies have mostly focused on re-
stricted domains (Kate and Mooney, 2007; Liang
et al., 2009; Titov and Kozhevnikov, 2010; Gold-
wasser et al., 2011; Liang et al., 2011).
Unsupervised learning has been one of the cen-
tral paradigms for the closely-related area of re-
lation extraction, where several techniques have
been proposed to cluster semantically similar ver-
balizations of relations (Lin and Pantel, 2001;
Banko et al., 2007). Early unsupervised ap-
proaches to the SRL problem include the work
by Swier and Stevenson (2004), where the Verb-
Net verb lexicon was used to guide unsupervised
learning, and a generative model of Grenager and
Manning (2006) which exploits linguistic priors
on syntactic-semantic interface.
More recently, the role induction problem has
been studied in Lang and Lapata (2010) where
it has been reformulated as a problem of detect-
ing alterations and mapping non-standard link-
ings to the canonical ones. Later, Lang and La-
pata (2011a) proposed an algorithmic approach
to clustering argument signatures which achieves
higher accuracy and outperforms the syntactic
baseline. In Lang and Lapata (2011b), the role
induction problem is formulated as a graph parti-
tioning problem: each vertex in the graph corre-
sponds to a predicate occurrence and edges repre-
sent lexical and syntactic similarities between the
occurrences. Unsupervised induction of seman-
tics has also been studied in Poon and Domin-
gos (2009) and Titov and Klementiev (2010) but
the induced representations are not entirely com-
patible with the PropBank-style annotations and
they have been evaluated only on a question an-
swering task for the biomedical domain. Also, the
related task of unsupervised argument identifica-
tion was considered in Abend et al. (2009).
</bodyText>
<sectionHeader confidence="0.997276" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999965363636364">
In this work we introduced two Bayesian models
for unsupervised role induction. They treat the
task as a family of related clustering problems,
one for each predicate. The first factored model
induces each clustering independently, whereas
the second model couples them by exploiting a
novel technique for sharing clustering preferences
across a family of clusterings. Both methods
achieve state-of-the-art results with the coupled
model outperforming the factored counterpart in
all regimes.
</bodyText>
<sectionHeader confidence="0.996841" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<footnote confidence="0.415543833333333">
The authors acknowledge the support of the MMCI
Cluster of Excellence, and thank Hagen F¨urstenau,
Mikhail Kozhevnikov, Alexis Palmer, Manfred Pinkal,
Caroline Sporleder and the anonymous reviewers for
their suggestions, and Joel Lang for answering ques-
tions about their methods and data.
</footnote>
<page confidence="0.993574">
20
</page>
<sectionHeader confidence="0.982154" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999885362831858">
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In ACL-IJCNLP.
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI.
Roberto Basili, Diego De Cao, Danilo Croce,
Bonaventura Coppola, and Alessandro Moschitti.
2009. Cross-language frame semantics transfer in
bilingual corpora. In CICLING.
David M. Blei and Peter Frazier. 2011. Distance de-
pendent chinese restaurant processes. Journal of
Machine Learning Research, 12:2461–2488.
Peter F. Brown, Vincent Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models for natural language. Compu-
tational Linguistics, 18(4):467–479.
Xavier Carreras and Llu´ıs M`arquez. 2005. Intro-
duction to the CoNLL-2005 Shared Task: Semantic
Role Labeling. In CoNLL.
Hal Daume III. 2007. Fast search for dirichlet process
mixture models. In AISTATS.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the
Latent Words Language Model. In EMNLP.
Jason Duan, Michele Guindani, and Alan Gelfand.
2007. Generalized spatial dirichlet process models.
Biometrika, 94:809–825.
Thomas S. Ferguson. 1973. A Bayesian analysis
of some nonparametric problems. The Annals of
Statistics, 1(2):209–230.
Hagen F¨urstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In EMNLP.
Qin Gao and Stephan Vogel. 2011. Corpus expansion
for statistical machine translation with semantic role
label substitution rules. In ACL:HLT.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labelling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In ACL.
Trond Grenager and Christoph Manning. 2006. Unsu-
pervised discovery of a statistical verb lexicon. In
EMNLP.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings
of the 13th Conference on Computational Natural
Language Learning (CoNLL-2009), June 4-5.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL Work-
shop on Deep Linguistic Processing.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambigous supervision.
In AAAI.
Aleksander Kolcz and Abdur Chowdhury. 2005. Dis-
counting over-confidence of naive bayes in high-
recall text classification. In ECML.
Joel Lang and Mirella Lapata. 2010. Unsupervised
induction of semantic roles. In ACL.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In ACL.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In ACL-IJCNLP.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL: HLT.
Dekang Lin and Patrick Pantel. 2001. DIRT – discov-
ery of inference rules from text. In KDD.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Coling.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In EMNLP-
CoNLL.
Sebastian Pado and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307–
340.
Alexis Palmer and Caroline Sporleder. 2010. Evalu-
ating FrameNet-style semantic parsing: the role of
coverage gaps in FrameNet. In COLING.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Com-
putational Linguistics, 34:289–310.
Jason Rennie. 2001. Improving multi-class text
classification with Naive bayes. Technical Report
AITR-2001-004, MIT.
M. Sammons, V. Vydiswaran, T. Vieira, N. Johri,
M. Chang, D. Goldwasser, V. Srikumar, G. Kundu,
Y. Tu, K. Small, J. Rule, Q. Do, and D. Roth. 2009.
Relation alignment for textual entailment recogni-
tion. In Text Analysis Conference (TAC).
</reference>
<page confidence="0.983145">
21
</page>
<reference confidence="0.999411606060606">
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP.
Richard Socher, Andrew Maas, and Christopher Man-
ning. 2011. Spectral chinese restaurant processes:
Nonparametric clustering based on similarities. In
AISTATS.
Mihai Surdeanu, Adam Meyers Richard Johansson,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Shared Task.
Richard Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In EMNLP.
Yee Whye Teh. 2010. Dirichlet processes. In Ency-
clopedia of Machine Learning. Springer.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In ACL.
Ivan Titov and Mikhail Kozhevnikov. 2010.
Bootstrapping semantic analyzers from non-
contradictory texts. In ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In ACL.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In ACL.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
SMT: A hybrid two-pass model. In NAACL.
Dekai Wu, Marianna Apidianaki, Marine Carpuat, and
Lucia Specia, editors. 2011. Proc. of Fifth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation. ACL.
</reference>
<page confidence="0.999029">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.835617">
<title confidence="0.999982">A Bayesian Approach to Unsupervised Semantic Role Induction</title>
<author confidence="0.998084">Ivan Titov Alexandre Klementiev</author>
<affiliation confidence="0.910759">Saarland</affiliation>
<address confidence="0.964962">Saarbr¨ucken, Germany</address>
<abstract confidence="0.99753175">We introduce two Bayesian models for unsupervised semantic role labeling (SRL) task. The models treat SRL as clustering of syntactic signatures of arguments with clusters corresponding to semantic roles. The first model induces these clusterings independently for each predicate, exploiting the Chinese Restaurant Process (CRP) as a prior. In a more refined hierarchical model, we inject the intuition that the clusterings are similar across different predicates, even though they are not necessarily identical. This intuition is encoded as a distance-dependent CRP with a distance between two syntactic signatures indicating how likely they are to correspond to a single semantic role. These distances are automatically induced within the model and shared across predicates. Both models achieve state-of-the-art results when evaluated on PropBank, with the coupled model consistently outperforming the factored counterpart in all experimental set-ups.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised argument identification for semantic role labeling.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="38893" citStr="Abend et al. (2009)" startWordPosition="6377" endWordPosition="6380">e role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2010) but the induced representations are not entirely compatible with the PropBank-style annotations and they have been evaluated only on a question answering task for the biomedical domain. Also, the related task of unsupervised argument identification was considered in Abend et al. (2009). 9 Conclusions In this work we introduced two Bayesian models for unsupervised role induction. They treat the task as a family of related clustering problems, one for each predicate. The first factored model induces each clustering independently, whereas the second model couples them by exploiting a novel technique for sharing clustering preferences across a family of clusterings. Both methods achieve state-of-the-art results with the coupled model outperforming the factored counterpart in all regimes. Acknowledgements The authors acknowledge the support of the MMCI Cluster of Excellence, and</context>
</contexts>
<marker>Abend, Reichart, Rappoport, 2009</marker>
<rawString>Omri Abend, Roi Reichart, and Ari Rappoport. 2009. Unsupervised argument identification for semantic role labeling. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web. In IJCAI.</title>
<date>2007</date>
<contexts>
<context position="37573" citStr="Banko et al., 2007" startWordPosition="6171" endWordPosition="6174">s languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures </context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Diego De Cao</author>
<author>Danilo Croce</author>
<author>Bonaventura Coppola</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Cross-language frame semantics transfer in bilingual corpora.</title>
<date>2009</date>
<booktitle>In CICLING.</booktitle>
<marker>Basili, De Cao, Croce, Coppola, Moschitti, 2009</marker>
<rawString>Roberto Basili, Diego De Cao, Danilo Croce, Bonaventura Coppola, and Alessandro Moschitti. 2009. Cross-language frame semantics transfer in bilingual corpora. In CICLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Peter Frazier</author>
</authors>
<title>Distance dependent chinese restaurant processes.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2461</pages>
<contexts>
<context position="5778" citStr="Blei and Frazier, 2011" startWordPosition="894" endWordPosition="897"> alternations, or changes in the syntactic realization of the argument structure of the verb. Though different verbs admit different alternations, some alternations are shared across multiple verbs and are very frequent (e.g., passivization, example sentences (a) vs. (d), or dativization: John gave a book to Mary vs. John gave Mary a book) (Levin, 1993). Therefore, it is natural to assume that the clusterings should be similar, though not identical, across verbs. Our second model encodes this intuition by replacing the CRP prior for each predicate with a distance-dependent CRP (dd-CRP) prior (Blei and Frazier, 2011) shared across predicates. The distance between two syntactic signatures encodes how likely they are to correspond to a single semantic role. Unlike most of the previous work exploiting distance-dependent CRPs (Blei and Frazier, 2011; Socher et al., 2011; Duan et al., 2007), we do not encode prior or external knowledge in the distance function but rather induce it automatically within our Bayesian model. The coupled dd-CRP model consistently outperforms the factored CRP counterpart across all the experimental settings (with gold and predicted syntactic parses, and with gold and automatically i</context>
<context position="13641" citStr="Blei and Frazier (2011)" startWordPosition="2200" endWordPosition="2203">ion. A generalization of this view leads to the definition of the distance-dependent CRP. In dd-CRPs, 14 a customer i chooses a partner ci = j with the probability proportional to some non-negative score di,j (di,j = dj,i) which encodes a similarity between the two customers.3 More formally, � ( ) p(ci = j |D, α) a di,j, i 7� j 1 α, i = j where D is the entire similarity graph. This process lacks the exchangeability property of the traditional CRP but efficient approximate inference with dd-CRP is possible with Gibbs sampling. For more details on inference with dd-CRPs, we refer the reader to Blei and Frazier (2011). Though in previous work dd-CRP was used either to encode prior knowledge (Blei and Frazier, 2011) or other external information (Socher et al., 2011), we treat D as a latent variable drawn from some prior distribution over weighted graphs. This view provides a powerful approach for coupling a family of distinct but similar clusterings: the family of clusterings can be drawn by first choosing a similarity graph D for the entire family and then re-using D to generate each of the clusterings independently of each other as defined by equation (1). In Section 5, we explain how we use this formali</context>
</contexts>
<marker>Blei, Frazier, 2011</marker>
<rawString>David M. Blei and Peter Frazier. 2011. Distance dependent chinese restaurant processes. Journal of Machine Learning Research, 12:2461–2488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models for natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="30238" citStr="Brown et al., 1992" startWordPosition="4979" endWordPosition="4982">Bank annotations, it has dependency structures generated automatically by the MaltParser (Nivre et al., 2007). We vary our experimental setup as follows: • We evaluate our models on gold and automatically generated parses, and use either gold PropBank annotations or the heuristic from Section 2 to identify arguments, resulting in four experimental regimes. • In order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by a clustering algorithm as a preprocessing step. In particular, we use Brown (Br) clustering (Brown et al., 1992) induced over RCV1 corpus (Turian et al., 2010). Although the clustering is hierarchical, we only use a cluster at the lowest level of the hierarchy for each word. We use the purity (PU) and collocation (CO) metrics as well as their harmonic mean (F1) to measure the quality of the resulting clusters. Purity measures the degree to which each cluster contains arguments sharing the same gold role: 1 � P U = � i where if Ci is the set of arguments in the i-th induced cluster, Gj is the set of arguments in the jth 10The coupled model without discounting still outperforms the factored counterpart in</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models for natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In CoNLL.</title>
<date>2005</date>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Fast search for dirichlet process mixture models.</title>
<date>2007</date>
<booktitle>In AISTATS.</booktitle>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Fast search for dirichlet process mixture models. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Semi-supervised semantic role labeling using the Latent Words Language Model. In EMNLP.</title>
<date>2009</date>
<contexts>
<context position="36918" citStr="Deschacht and Moens, 2009" startWordPosition="6069" endWordPosition="6072">antic roles for adverbials do not normally depend on whether the construction is passive or active). 8 Related Work Most of SRL research has focused on the supervised setting (Carreras and M`arquez, 2005; Surdeanu et al., 2008), however, lack of annotated resources for most languages and insufficient coverage provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations </context>
</contexts>
<marker>Deschacht, Moens, 2009</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2009. Semi-supervised semantic role labeling using the Latent Words Language Model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Duan</author>
<author>Michele Guindani</author>
<author>Alan Gelfand</author>
</authors>
<title>Generalized spatial dirichlet process models.</title>
<date>2007</date>
<journal>Biometrika,</journal>
<pages>94--809</pages>
<contexts>
<context position="6052" citStr="Duan et al., 2007" startWordPosition="938" endWordPosition="941">ivization: John gave a book to Mary vs. John gave Mary a book) (Levin, 1993). Therefore, it is natural to assume that the clusterings should be similar, though not identical, across verbs. Our second model encodes this intuition by replacing the CRP prior for each predicate with a distance-dependent CRP (dd-CRP) prior (Blei and Frazier, 2011) shared across predicates. The distance between two syntactic signatures encodes how likely they are to correspond to a single semantic role. Unlike most of the previous work exploiting distance-dependent CRPs (Blei and Frazier, 2011; Socher et al., 2011; Duan et al., 2007), we do not encode prior or external knowledge in the distance function but rather induce it automatically within our Bayesian model. The coupled dd-CRP model consistently outperforms the factored CRP counterpart across all the experimental settings (with gold and predicted syntactic parses, and with gold and automatically identified arguments). Both models admit efficient inference: the estimation time on the Penn Treebank WSJ corpus does not exceed 30 minutes on a single processor and the inference algorithm is highly parallelizable, reducing inference time down to several minutes on multipl</context>
</contexts>
<marker>Duan, Guindani, Gelfand, 2007</marker>
<rawString>Jason Duan, Michele Guindani, and Alan Gelfand. 2007. Generalized spatial dirichlet process models. Biometrika, 94:809–825.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A Bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>The Annals of Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="4760" citStr="Ferguson, 1973" startWordPosition="733" endWordPosition="734">rk on unsupervised techniques, the focus is on the labeling stage. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006) or, potentially, by using a supervised classifier trained on a small amount of data. We follow (Lang and Lapata, 2011a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predicate. In our first model, as in most of the previous work on unsupervised SRL, we define an independent model for each predicate. We use the Chinese Restaurant Process (CRP) (Ferguson, 1973) as a prior for the clustering of syntactic signatures. The resulting model achieves state-of-the-art results, substantially outperforming previous methods evaluated in the same setting. In the first model, for each predicate we independently induce a linking between syntax and semantics, encoded as a clustering of syntactic signatures. The clustering implicitly defines the set of permissible alternations, or changes in the syntactic realization of the argument structure of the verb. Though different verbs admit different alternations, some alternations are shared across multiple verbs and are</context>
<context position="10517" citStr="Ferguson, 1973" startWordPosition="1649" endWordPosition="1650">ys are clustered together are assigned the same semantic role. Note that some adjunct-like modifier arguments are already explicitly represented in syntax and thus do not need to be clustered (modifiers AM-TMP, AM-MNR, AM-LOC, and AM-DIR are encoded as ‘syntactic’ relations TMP, MNR, LOC, and DIR, respectively (Surdeanu et al., 2008)); instead we directly use the syntactic labels as semantic roles. 3 Traditional and Distance-dependent CRPs The central components of our non-parametric Bayesian models are the Chinese Restaurant Processes (CRPs) and the closely related Dirichlet Processes (DPs) (Ferguson, 1973). CRPs define probability distributions over partitions of a set of objects. An intuitive metaphor 2LGS denotes a logical subject in a passive construction (Surdeanu et al., 2008). for describing CRPs is assignment of tables to restaurant customers. Assume a restaurant with a sequence of tables, and customers who walk into the restaurant one at a time and choose a table to join. The first customer to enter is assigned the first table. Suppose that when a client number i enters the restaurant, i − 1 customers are sitting at each of the k E (1, ... , K) tables occupied so far. The new customer i</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S. Ferguson. 1973. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen F¨urstenau</author>
<author>Mirella Lapata</author>
</authors>
<title>Graph alignment for semi-supervised semantic role labeling.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<marker>F¨urstenau, Lapata, 2009</marker>
<rawString>Hagen F¨urstenau and Mirella Lapata. 2009. Graph alignment for semi-supervised semantic role labeling. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Corpus expansion for statistical machine translation with semantic role label substitution rules.</title>
<date>2011</date>
<booktitle>In ACL:HLT.</booktitle>
<contexts>
<context position="2212" citStr="Gao and Vogel, 2011" startWordPosition="335" endWordPosition="338">the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters. These resources are scarce an</context>
</contexts>
<marker>Gao, Vogel, 2011</marker>
<rawString>Qin Gao and Stephan Vogel. 2011. Corpus expansion for statistical machine translation with semantic role label substitution rules. In ACL:HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labelling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1205" citStr="Gildea and Jurafsky, 2002" startWordPosition="166" endWordPosition="170">ion that the clusterings are similar across different predicates, even though they are not necessarily identical. This intuition is encoded as a distance-dependent CRP with a distance between two syntactic signatures indicating how likely they are to correspond to a single semantic role. These distances are automatically induced within the model and shared across predicates. Both models achieve state-of-the-art results when evaluated on PropBank, with the coupled model consistently outperforming the factored counterpart in all experimental set-ups. 1 Introduction Semantic role labeling (SRL) (Gildea and Jurafsky, 2002), a shallow semantic parsing task, has recently attracted a lot of attention in the computational linguistic community (Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). The task involves prediction of predicate argument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labelling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="37291" citStr="Goldwasser et al., 2011" startWordPosition="6126" endWordPosition="6130">nlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role ind</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Christoph Manning</author>
</authors>
<title>Unsupervised discovery of a statistical verb lexicon.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4340" citStr="Grenager and Manning, 2006" startWordPosition="659" endWordPosition="662">of the European Chapter of the Association for Computational Linguistics, pages 12–22, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics cient hierarchical Bayesian models for this task. It is natural to split the SRL task into two stages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage). In this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006) or, potentially, by using a supervised classifier trained on a small amount of data. We follow (Lang and Lapata, 2011a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predicate. In our first model, as in most of the previous work on unsupervised SRL, we define an independent model for each predicate. We use the Chinese Restaurant Process (CRP) (Ferguson, 1973) as a prior for the clustering of syntactic signatures. The resulting model achieves state-of-the-art results, substantially outperforming previous methods evaluated in the same se</context>
<context position="8521" citStr="Grenager and Manning, 2006" startWordPosition="1341" endWordPosition="1344">between a predicate and a candidate argument to iteratively discard non-arguments from the list of all words in a sentence. Note that inducing these rules for a new language would require some linguistic expertise. One alternative may be to annotate a small number of arguments and train a classifier with nonlexicalized features instead. In the argument labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of deal1Although it provides a strong baseline which is difficult to beat (Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a). 13 ing with argument occurrences directly, we represent them as predicate specific syntactic signatures, and refer to them as argument keys. This representation aids our models in inducing high purity clusters (of argument keys) while reducing their granularity. We follow (Lang and Lapata, 2011a) and use the following syntactic features to form the argument key representation: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relation to its governor. • Preposition used for argument r</context>
<context position="37797" citStr="Grenager and Manning (2006)" startWordPosition="6208" endWordPosition="6211">ed on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline. In Lang and Lapata (2011b), the role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate</context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>Trond Grenager and Christoph Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In EMNLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>4--5</pages>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>Bonnie Webber</author>
</authors>
<title>Question answering based on semantic roles.</title>
<date>2007</date>
<booktitle>In ACL Workshop on Deep Linguistic Processing.</booktitle>
<contexts>
<context position="2068" citStr="Kaisser and Webber, 2007" startWordPosition="311" endWordPosition="314">gument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shal</context>
</contexts>
<marker>Kaisser, Webber, 2007</marker>
<rawString>Michael Kaisser and Bonnie Webber. 2007. Question answering based on semantic roles. In ACL Workshop on Deep Linguistic Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language semantics from ambigous supervision.</title>
<date>2007</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="37217" citStr="Kate and Mooney, 2007" startWordPosition="6114" endWordPosition="6117">verage provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits ling</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007. Learning language semantics from ambigous supervision. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aleksander Kolcz</author>
<author>Abdur Chowdhury</author>
</authors>
<title>Discounting over-confidence of naive bayes in highrecall text classification.</title>
<date>2005</date>
<booktitle>In ECML.</booktitle>
<contexts>
<context position="29140" citStr="Kolcz and Chowdhury, 2005" startWordPosition="4793" endWordPosition="4796">ures. This behavior is due to the over-simplifying independence assumptions. It is well known, for instance, that the poste8Note that weights dzj are invariant under rescaling when the rescaling is also applied to the concentration parameter α. 9In practice, two iterations were sufficient. rior with Naive Bayes tends to be overconfident due to violated conditional independence assumptions (Rennie, 2001). The same behavior is observed here: the shared prior does not have sufficient effect on frequent predicates.10 Though different techniques have been developed to discount the over-confidence (Kolcz and Chowdhury, 2005), we use the most basic one: we raise the likelihood term in power T, where the parameter T is chosen empirically. 7 Empirical Evaluation 7.1 Data and Evaluation We keep the general setup of (Lang and Lapata, 2011a), to evaluate our models and compare them to the current state of the art. We run all of our experiments on the standard CoNLL 2008 shared task (Surdeanu et al., 2008) version of Penn Treebank WSJ and PropBank. In addition to gold dependency analyses and gold PropBank annotations, it has dependency structures generated automatically by the MaltParser (Nivre et al., 2007). We vary ou</context>
</contexts>
<marker>Kolcz, Chowdhury, 2005</marker>
<rawString>Aleksander Kolcz and Abdur Chowdhury. 2005. Discounting over-confidence of naive bayes in highrecall text classification. In ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised induction of semantic roles.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8544" citStr="Lang and Lapata, 2010" startWordPosition="1345" endWordPosition="1348">ndidate argument to iteratively discard non-arguments from the list of all words in a sentence. Note that inducing these rules for a new language would require some linguistic expertise. One alternative may be to annotate a small number of arguments and train a classifier with nonlexicalized features instead. In the argument labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. However, instead of deal1Although it provides a strong baseline which is difficult to beat (Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011a). 13 ing with argument occurrences directly, we represent them as predicate specific syntactic signatures, and refer to them as argument keys. This representation aids our models in inducing high purity clusters (of argument keys) while reducing their granularity. We follow (Lang and Lapata, 2011a) and use the following syntactic features to form the argument key representation: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relation to its governor. • Preposition used for argument realization. In the exam</context>
<context position="32193" citStr="Lang and Lapata, 2010" startWordPosition="5327" endWordPosition="5330">n word clustering of argument fillers (Factored, Coupled, Factored+Br, Coupled+Br). Our models are robust to parameter settings, they were tuned (to an order of magnitude) on the development set and were the same for all model variants: α = 1.e-3, β = 1.e-3, η0 = 1.e-3, η1 = 1.e-10, T = 5. Although they can be induced within the model, we set them by hand to indicate granularity preferences. We compare our results with the following alternative approaches. The syntactic function baseline (SyntF) simply clusters predicate arguments according to the dependency relation to their head. Following (Lang and Lapata, 2010), we allocate a cluster for each of 20 most frequent relations in the CoNLL dataset and one cluster for all other relations. We also compare our performance with the Latent Logistic classification (Lang and Lapata, 2010), Split-Merge clustering (Lang and Lapata, 2011a), and Graph Partitioning (Lang and Lapata, 2011b) approaches (labeled LLogistic, SplitMerge, and GraphPart, respectively) which achieve the current best unsupervised SRL results in this setting. 7.2 Results 7.2.1 Gold Arguments Experimental results are summarized in Table 1. We begin by comparing our models to the three existing </context>
<context position="37948" citStr="Lang and Lapata (2010)" startWordPosition="6229" endWordPosition="6232"> learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline. In Lang and Lapata (2011b), the role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied i</context>
</contexts>
<marker>Lang, Lapata, 2010</marker>
<rawString>Joel Lang and Mirella Lapata. 2010. Unsupervised induction of semantic roles. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction via split-merge clustering.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4310" citStr="Lang and Lapata, 2011" startWordPosition="655" endWordPosition="658"> of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics cient hierarchical Bayesian models for this task. It is natural to split the SRL task into two stages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage). In this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006) or, potentially, by using a supervised classifier trained on a small amount of data. We follow (Lang and Lapata, 2011a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predicate. In our first model, as in most of the previous work on unsupervised SRL, we define an independent model for each predicate. We use the Chinese Restaurant Process (CRP) (Ferguson, 1973) as a prior for the clustering of syntactic signatures. The resulting model achieves state-of-the-art results, substantially outperforming previous me</context>
<context position="7805" citStr="Lang and Lapata, 2011" startWordPosition="1223" endWordPosition="1226">7 provides both evaluation and analysis. Finally, additional related work is presented in Section 8. 2 Task Definition In this work, instead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs. While we cannot expect that syntactic structure can trivially map to a semantic representation (Palmer et al., 2005)1, we can use syntactic cues to help us in both stages of unsupervised SRL. Before defining our task, let us consider the two stages separately. In the argument identification stage, we implement a heuristic proposed in (Lang and Lapata, 2011a) comprised of a list of 8 rules, which use nonlexicalized properties of syntactic paths between a predicate and a candidate argument to iteratively discard non-arguments from the list of all words in a sentence. Note that inducing these rules for a new language would require some linguistic expertise. One alternative may be to annotate a small number of arguments and train a classifier with nonlexicalized features instead. In the argument labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. Ho</context>
<context position="26406" citStr="Lang and Lapata, 2011" startWordPosition="4329" endWordPosition="4332">te p has chosen an argument key cp,k to ‘sit’ with. We can compute the MAP estimate for all di,j by maximizing the objective: 1: 1: dk,c�,k arg max log dzj, i7�j p kEK Ek&apos;EKI, dk,k&apos; where Kp is the set of all argument keys for the predicate p. We slightly abuse the notation by using di,i to denote the concentration parameter α in the previous expression. Note that we also assume that similarities are symmetric, di,j = dj,i. If the set of argument keys Kp would be the same for every predicate, then the optimal di,j would 7This idea has been explored before for shallow semantic representations (Lang and Lapata, 2011a; Titov and Klementiev, 2011). , 17 be proportional to the number of times either i selects j as a partner, or j chooses i as a partner.8 This no longer holds if the sets are different, but the solution can be found efficiently using a numeric optimization strategy; we use the gradient descent algorithm. We do not learn the concentration parameter α, as it is used in our model to indicate the desired granularity of semantic roles, but instead only learn di,j (i =� j). However, just learning the concentration parameter would not be sufficient as the effective concentration can be reduced or in</context>
<context position="29353" citStr="Lang and Lapata, 2011" startWordPosition="4831" endWordPosition="4834">concentration parameter α. 9In practice, two iterations were sufficient. rior with Naive Bayes tends to be overconfident due to violated conditional independence assumptions (Rennie, 2001). The same behavior is observed here: the shared prior does not have sufficient effect on frequent predicates.10 Though different techniques have been developed to discount the over-confidence (Kolcz and Chowdhury, 2005), we use the most basic one: we raise the likelihood term in power T, where the parameter T is chosen empirically. 7 Empirical Evaluation 7.1 Data and Evaluation We keep the general setup of (Lang and Lapata, 2011a), to evaluate our models and compare them to the current state of the art. We run all of our experiments on the standard CoNLL 2008 shared task (Surdeanu et al., 2008) version of Penn Treebank WSJ and PropBank. In addition to gold dependency analyses and gold PropBank annotations, it has dependency structures generated automatically by the MaltParser (Nivre et al., 2007). We vary our experimental setup as follows: • We evaluate our models on gold and automatically generated parses, and use either gold PropBank annotations or the heuristic from Section 2 to identify arguments, resulting in fo</context>
<context position="31188" citStr="Lang and Lapata, 2011" startWordPosition="5161" endWordPosition="5164">to which each cluster contains arguments sharing the same gold role: 1 � P U = � i where if Ci is the set of arguments in the i-th induced cluster, Gj is the set of arguments in the jth 10The coupled model without discounting still outperforms the factored counterpart in our experiments. max |Gj n Ci| j 18 gold cluster, and N is the total number of arguments. Collocation evaluates the degree to which arguments with the same gold roles are assigned to a single cluster. It is computed as follows: 1 � CO = N 9 We compute the aggregate PU, CO, and F1 scores over all predicates in the same way as (Lang and Lapata, 2011a) by weighting the scores of each predicate by the number of its argument occurrences. Note that since our goal is to evaluate the clustering algorithms, we do not include incorrectly identified arguments (i.e. mistakes made by the heuristic defined in Section 2) when computing these metrics. We evaluate both factored and coupled models proposed in this work with and without Brown word clustering of argument fillers (Factored, Coupled, Factored+Br, Coupled+Br). Our models are robust to parameter settings, they were tuned (to an order of magnitude) on the development set and were the same for </context>
<context position="32460" citStr="Lang and Lapata, 2011" startWordPosition="5371" endWordPosition="5374">3, η1 = 1.e-10, T = 5. Although they can be induced within the model, we set them by hand to indicate granularity preferences. We compare our results with the following alternative approaches. The syntactic function baseline (SyntF) simply clusters predicate arguments according to the dependency relation to their head. Following (Lang and Lapata, 2010), we allocate a cluster for each of 20 most frequent relations in the CoNLL dataset and one cluster for all other relations. We also compare our performance with the Latent Logistic classification (Lang and Lapata, 2010), Split-Merge clustering (Lang and Lapata, 2011a), and Graph Partitioning (Lang and Lapata, 2011b) approaches (labeled LLogistic, SplitMerge, and GraphPart, respectively) which achieve the current best unsupervised SRL results in this setting. 7.2 Results 7.2.1 Gold Arguments Experimental results are summarized in Table 1. We begin by comparing our models to the three existing clustering approaches on gold syntactic parses, and using gold PropBank annotations to identify predicate arguments. In this set of experiments we measure the relative performance of argument clustering, removing the identificagold parses auto parses PU CO F1 PU CO F</context>
<context position="34740" citStr="Lang and Lapata, 2011" startWordPosition="5728" endWordPosition="5731">fiers as regular arguments. The model achieves 89.2% purity, 74.0% collocation, and 80.9% F1 scores, still substantially outperforming all of the alternative approaches. Replacing gold parses with MaltParser analyses we see a similar trend, where Coupled+Br outperforms the best alternative approach SplitMerge by 1.5%. 7.2.2 Automatic Arguments Results are summarized in Table 2.11 The precision and recall of our re-implementation of the argument identification heuristic described in Section 2 on gold parses were 87.7% and 88.0%, respectively, and do not quite match 88.1% and 87.9% reported in (Lang and Lapata, 2011a). Since we could not reproduce their argument identification stage exactly, we are omitting their results for the two regimes, instead including the results for our two best models Factored+Br and Coupled+Br. We see a similar trend, where the coupled system consistently outperforms its factored counterpart, achieving 85.8% and 83.9% F1 11Note, that the scores are computed on correctly identified arguments only, and tend to be higher in these experiments probably because the complex arguments get discarded by the heuristic. max i |G; \ Ci| 19 gold parses auto parses PU CO F1 PU CO F1 Factored</context>
<context position="38103" citStr="Lang and Lapata (2011" startWordPosition="6255" endWordPosition="6259">semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline. In Lang and Lapata (2011b), the role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2010) but the induced representations are not entirely compatible with the PropBank-style annotations </context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011a. Unsupervised semantic role induction via split-merge clustering. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction with graph partitioning.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4310" citStr="Lang and Lapata, 2011" startWordPosition="655" endWordPosition="658"> of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics cient hierarchical Bayesian models for this task. It is natural to split the SRL task into two stages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage). In this and in much of the previous work on unsupervised techniques, the focus is on the labeling stage. Identification, though an important problem, can be tackled with heuristics (Lang and Lapata, 2011a; Grenager and Manning, 2006) or, potentially, by using a supervised classifier trained on a small amount of data. We follow (Lang and Lapata, 2011a), and regard the labeling stage as clustering of syntactic signatures of argument realizations for every predicate. In our first model, as in most of the previous work on unsupervised SRL, we define an independent model for each predicate. We use the Chinese Restaurant Process (CRP) (Ferguson, 1973) as a prior for the clustering of syntactic signatures. The resulting model achieves state-of-the-art results, substantially outperforming previous me</context>
<context position="7805" citStr="Lang and Lapata, 2011" startWordPosition="1223" endWordPosition="1226">7 provides both evaluation and analysis. Finally, additional related work is presented in Section 8. 2 Task Definition In this work, instead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs. While we cannot expect that syntactic structure can trivially map to a semantic representation (Palmer et al., 2005)1, we can use syntactic cues to help us in both stages of unsupervised SRL. Before defining our task, let us consider the two stages separately. In the argument identification stage, we implement a heuristic proposed in (Lang and Lapata, 2011a) comprised of a list of 8 rules, which use nonlexicalized properties of syntactic paths between a predicate and a candidate argument to iteratively discard non-arguments from the list of all words in a sentence. Note that inducing these rules for a new language would require some linguistic expertise. One alternative may be to annotate a small number of arguments and train a classifier with nonlexicalized features instead. In the argument labeling stage, semantic roles are represented by clusters of arguments, and labeling a particular argument corresponds to deciding on its role cluster. Ho</context>
<context position="26406" citStr="Lang and Lapata, 2011" startWordPosition="4329" endWordPosition="4332">te p has chosen an argument key cp,k to ‘sit’ with. We can compute the MAP estimate for all di,j by maximizing the objective: 1: 1: dk,c�,k arg max log dzj, i7�j p kEK Ek&apos;EKI, dk,k&apos; where Kp is the set of all argument keys for the predicate p. We slightly abuse the notation by using di,i to denote the concentration parameter α in the previous expression. Note that we also assume that similarities are symmetric, di,j = dj,i. If the set of argument keys Kp would be the same for every predicate, then the optimal di,j would 7This idea has been explored before for shallow semantic representations (Lang and Lapata, 2011a; Titov and Klementiev, 2011). , 17 be proportional to the number of times either i selects j as a partner, or j chooses i as a partner.8 This no longer holds if the sets are different, but the solution can be found efficiently using a numeric optimization strategy; we use the gradient descent algorithm. We do not learn the concentration parameter α, as it is used in our model to indicate the desired granularity of semantic roles, but instead only learn di,j (i =� j). However, just learning the concentration parameter would not be sufficient as the effective concentration can be reduced or in</context>
<context position="29353" citStr="Lang and Lapata, 2011" startWordPosition="4831" endWordPosition="4834">concentration parameter α. 9In practice, two iterations were sufficient. rior with Naive Bayes tends to be overconfident due to violated conditional independence assumptions (Rennie, 2001). The same behavior is observed here: the shared prior does not have sufficient effect on frequent predicates.10 Though different techniques have been developed to discount the over-confidence (Kolcz and Chowdhury, 2005), we use the most basic one: we raise the likelihood term in power T, where the parameter T is chosen empirically. 7 Empirical Evaluation 7.1 Data and Evaluation We keep the general setup of (Lang and Lapata, 2011a), to evaluate our models and compare them to the current state of the art. We run all of our experiments on the standard CoNLL 2008 shared task (Surdeanu et al., 2008) version of Penn Treebank WSJ and PropBank. In addition to gold dependency analyses and gold PropBank annotations, it has dependency structures generated automatically by the MaltParser (Nivre et al., 2007). We vary our experimental setup as follows: • We evaluate our models on gold and automatically generated parses, and use either gold PropBank annotations or the heuristic from Section 2 to identify arguments, resulting in fo</context>
<context position="31188" citStr="Lang and Lapata, 2011" startWordPosition="5161" endWordPosition="5164">to which each cluster contains arguments sharing the same gold role: 1 � P U = � i where if Ci is the set of arguments in the i-th induced cluster, Gj is the set of arguments in the jth 10The coupled model without discounting still outperforms the factored counterpart in our experiments. max |Gj n Ci| j 18 gold cluster, and N is the total number of arguments. Collocation evaluates the degree to which arguments with the same gold roles are assigned to a single cluster. It is computed as follows: 1 � CO = N 9 We compute the aggregate PU, CO, and F1 scores over all predicates in the same way as (Lang and Lapata, 2011a) by weighting the scores of each predicate by the number of its argument occurrences. Note that since our goal is to evaluate the clustering algorithms, we do not include incorrectly identified arguments (i.e. mistakes made by the heuristic defined in Section 2) when computing these metrics. We evaluate both factored and coupled models proposed in this work with and without Brown word clustering of argument fillers (Factored, Coupled, Factored+Br, Coupled+Br). Our models are robust to parameter settings, they were tuned (to an order of magnitude) on the development set and were the same for </context>
<context position="32460" citStr="Lang and Lapata, 2011" startWordPosition="5371" endWordPosition="5374">3, η1 = 1.e-10, T = 5. Although they can be induced within the model, we set them by hand to indicate granularity preferences. We compare our results with the following alternative approaches. The syntactic function baseline (SyntF) simply clusters predicate arguments according to the dependency relation to their head. Following (Lang and Lapata, 2010), we allocate a cluster for each of 20 most frequent relations in the CoNLL dataset and one cluster for all other relations. We also compare our performance with the Latent Logistic classification (Lang and Lapata, 2010), Split-Merge clustering (Lang and Lapata, 2011a), and Graph Partitioning (Lang and Lapata, 2011b) approaches (labeled LLogistic, SplitMerge, and GraphPart, respectively) which achieve the current best unsupervised SRL results in this setting. 7.2 Results 7.2.1 Gold Arguments Experimental results are summarized in Table 1. We begin by comparing our models to the three existing clustering approaches on gold syntactic parses, and using gold PropBank annotations to identify predicate arguments. In this set of experiments we measure the relative performance of argument clustering, removing the identificagold parses auto parses PU CO F1 PU CO F</context>
<context position="34740" citStr="Lang and Lapata, 2011" startWordPosition="5728" endWordPosition="5731">fiers as regular arguments. The model achieves 89.2% purity, 74.0% collocation, and 80.9% F1 scores, still substantially outperforming all of the alternative approaches. Replacing gold parses with MaltParser analyses we see a similar trend, where Coupled+Br outperforms the best alternative approach SplitMerge by 1.5%. 7.2.2 Automatic Arguments Results are summarized in Table 2.11 The precision and recall of our re-implementation of the argument identification heuristic described in Section 2 on gold parses were 87.7% and 88.0%, respectively, and do not quite match 88.1% and 87.9% reported in (Lang and Lapata, 2011a). Since we could not reproduce their argument identification stage exactly, we are omitting their results for the two regimes, instead including the results for our two best models Factored+Br and Coupled+Br. We see a similar trend, where the coupled system consistently outperforms its factored counterpart, achieving 85.8% and 83.9% F1 11Note, that the scores are computed on correctly identified arguments only, and tend to be higher in these experiments probably because the complex arguments get discarded by the heuristic. max i |G; \ Ci| 19 gold parses auto parses PU CO F1 PU CO F1 Factored</context>
<context position="38103" citStr="Lang and Lapata (2011" startWordPosition="6255" endWordPosition="6259">semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline. In Lang and Lapata (2011b), the role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2010) but the induced representations are not entirely compatible with the PropBank-style annotations </context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011b. Unsupervised semantic role induction with graph partitioning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="2384" citStr="Levin, 1993" startWordPosition="364" endWordPosition="365">ary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters. These resources are scarce and expensive to create, and even the largest of them have low coverage (Palmer and Sporleder, 2010). Moreover, these models are domain-specific, and their performance drops </context>
<context position="5510" citStr="Levin, 1993" startWordPosition="852" endWordPosition="853"> previous methods evaluated in the same setting. In the first model, for each predicate we independently induce a linking between syntax and semantics, encoded as a clustering of syntactic signatures. The clustering implicitly defines the set of permissible alternations, or changes in the syntactic realization of the argument structure of the verb. Though different verbs admit different alternations, some alternations are shared across multiple verbs and are very frequent (e.g., passivization, example sentences (a) vs. (d), or dativization: John gave a book to Mary vs. John gave Mary a book) (Levin, 1993). Therefore, it is natural to assume that the clusterings should be similar, though not identical, across verbs. Our second model encodes this intuition by replacing the CRP prior for each predicate with a distance-dependent CRP (dd-CRP) prior (Blei and Frazier, 2011) shared across predicates. The distance between two syntactic signatures encodes how likely they are to correspond to a single semantic role. Unlike most of the previous work exploiting distance-dependent CRPs (Blei and Frazier, 2011; Socher et al., 2011; Duan et al., 2007), we do not encode prior or external knowledge in the dist</context>
<context position="20308" citStr="Levin, 1993" startWordPosition="3319" endWordPosition="3320">ng of the key ACT:LEFT:SBJ with PASS:RIGHT:LGS-&gt;by and ACT:RIGHT:OBJ with PASS:LEFT:SBJ. The set of permissible alternations is predicatespecific,6 but nevertheless they arguably represent a small subset of all clusterings of argument keys. Also, some alternations are more likely to be applicable to a verb than others: for example, passivization and dativization alternations are both fairly frequent, whereas, locativepreposition-drop alternation (Mary climbed up the mountain vs. Mary climbed the mountain) is less common and applicable only to several classes of predicates representing motion (Levin, 1993). We represent this observation by quantifying how likely a pair of keys is to be clustered. These scores (di,� for every pair of argument keys i and j) are induced automatically within the model, and treated as latent variables shared across predicates. Intuitively, if data for several predicates strongly suggests that two argument keys should be clustered (e.g., there is a large overlap between argument fillers for the two keys) then the posterior will indicate that di,� is expected to be greater for the pair {i, j} than for some other pair {i&apos;, j&apos;} for which the evidence is less clear. Cons</context>
<context position="22060" citStr="Levin, 1993" startWordPosition="3601" endWordPosition="3602">ple, argument key clustering of predicates with very restrictive selectional restrictions on argument fillers is presumably easier than clustering for predicates with less restrictive and overlapping selectional restriction, as compactness of selectional preferences is a central assumption driving unsupervised learning of semantic roles. E.g., predicates change and defrost belong to the same Levin class (change-of-state verbs) and therefore admit similar alternations. However, the set of potential patients of defrost is sufficiently restricted, 6Or, at least specific to a class of predicates (Levin, 1993). 16 whereas the selectional restrictions for the patient of change are far less specific and they overlap with selectional restrictions for the agent role, further complicating the clustering induction task. This observation suggests that sharing clustering preferences across verbs is likely to help even if the unlabeled data is plentiful for every predicate. More formally, we generate scores di,j, or equivalently, the full labeled graph D with vertices corresponding to argument keys and edges weighted with the similarity scores, from a prior. In our experiments we use a non-informative prior</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="37237" citStr="Liang et al., 2009" startWordPosition="6118" endWordPosition="6121">existing resources motivates the need for using unlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syn</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In ACL: HLT.</booktitle>
<contexts>
<context position="37312" citStr="Liang et al., 2011" startWordPosition="6131" endWordPosition="6134">rms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has be</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In ACL: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT – discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="37552" citStr="Lin and Pantel, 2001" startWordPosition="6167" endWordPosition="6170">on of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT – discovery of inference rules from text. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Semantic role features for machine translation. In Coling.</title>
<date>2010</date>
<contexts>
<context position="2173" citStr="Liu and Gildea, 2010" startWordPosition="327" endWordPosition="330">erlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model pa</context>
</contexts>
<marker>Liu, Gildea, 2010</marker>
<rawString>Ding Liu and Daniel Gildea. 2010. Semantic role features for machine translation. In Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Crosslingual annotation projection for semantic roles.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<pages>340</pages>
<contexts>
<context position="36988" citStr="Pado and Lapata, 2009" startWordPosition="6079" endWordPosition="6082">on is passive or active). 8 Related Work Most of SRL research has focused on the supervised setting (Carreras and M`arquez, 2005; Surdeanu et al., 2008), however, lack of annotated resources for most languages and insufficient coverage provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsuper</context>
</contexts>
<marker>Pado, Lapata, 2009</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2009. Crosslingual annotation projection for semantic roles. Journal of Artificial Intelligence Research, 36:307– 340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Palmer</author>
<author>Caroline Sporleder</author>
</authors>
<title>Evaluating FrameNet-style semantic parsing: the role of coverage gaps in FrameNet.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2910" citStr="Palmer and Sporleder, 2010" startWordPosition="440" endWordPosition="443">, among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters. These resources are scarce and expensive to create, and even the largest of them have low coverage (Palmer and Sporleder, 2010). Moreover, these models are domain-specific, and their performance drops substantially when they are used in a new domain (Pradhan et al., 2008). Such domain specificity is arguably unavoidable for a semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses). The necessity for a large amounts of human-annotated data for every language and domain is one of the major obstacles to the wide-spread adoption of semantic role representations. These challenges motivate</context>
</contexts>
<marker>Palmer, Sporleder, 2010</marker>
<rawString>Alexis Palmer and Caroline Sporleder. 2010. Evaluating FrameNet-style semantic parsing: the role of coverage gaps in FrameNet. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="7563" citStr="Palmer et al., 2005" startWordPosition="1182" endWordPosition="1185">ling task and discuss some specifics of the unsupervised setting. In Section 3, we describe CRPs and dd-CRPs, the key components of our models. In Sections 4 – 6, we describe our factored and coupled models and the inference method. Section 7 provides both evaluation and analysis. Finally, additional related work is presented in Section 8. 2 Task Definition In this work, instead of assuming the availability of role annotated data, we rely only on automatically generated syntactic dependency graphs. While we cannot expect that syntactic structure can trivially map to a semantic representation (Palmer et al., 2005)1, we can use syntactic cues to help us in both stages of unsupervised SRL. Before defining our task, let us consider the two stages separately. In the argument identification stage, we implement a heuristic proposed in (Lang and Lapata, 2011a) comprised of a list of 8 rules, which use nonlexicalized properties of syntactic paths between a predicate and a candidate argument to iteratively discard non-arguments from the list of all words in a sentence. Note that inducing these rules for a new language would require some linguistic expertise. One alternative may be to annotate a small number of </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="38574" citStr="Poon and Domingos (2009)" startWordPosition="6326" endWordPosition="6330">here it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline. In Lang and Lapata (2011b), the role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between the occurrences. Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2010) but the induced representations are not entirely compatible with the PropBank-style annotations and they have been evaluated only on a question answering task for the biomedical domain. Also, the related task of unsupervised argument identification was considered in Abend et al. (2009). 9 Conclusions In this work we introduced two Bayesian models for unsupervised role induction. They treat the task as a family of related clustering problems, one for each predicate. The first factored model induces each clustering independently, whereas the second model couples </context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--289</pages>
<contexts>
<context position="3055" citStr="Pradhan et al., 2008" startWordPosition="462" endWordPosition="465">resentations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters. These resources are scarce and expensive to create, and even the largest of them have low coverage (Palmer and Sporleder, 2010). Moreover, these models are domain-specific, and their performance drops substantially when they are used in a new domain (Pradhan et al., 2008). Such domain specificity is arguably unavoidable for a semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses). The necessity for a large amounts of human-annotated data for every language and domain is one of the major obstacles to the wide-spread adoption of semantic role representations. These challenges motivate the need for unsupervised methods which, instead of relying on labeled data, can exploit large amounts of unlabeled texts. In this paper, we pro</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2008</marker>
<rawString>Sameer Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. Computational Linguistics, 34:289–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Rennie</author>
</authors>
<title>Improving multi-class text classification with Naive bayes.</title>
<date>2001</date>
<tech>Technical Report AITR-2001-004, MIT.</tech>
<contexts>
<context position="28920" citStr="Rennie, 2001" startWordPosition="4761" endWordPosition="4762">shortcoming of this approach is typical for generative models with multiple ‘features’: when such a model predicts a latent variable, it tends to ignore the prior class distribution and relies solely on features. This behavior is due to the over-simplifying independence assumptions. It is well known, for instance, that the poste8Note that weights dzj are invariant under rescaling when the rescaling is also applied to the concentration parameter α. 9In practice, two iterations were sufficient. rior with Naive Bayes tends to be overconfident due to violated conditional independence assumptions (Rennie, 2001). The same behavior is observed here: the shared prior does not have sufficient effect on frequent predicates.10 Though different techniques have been developed to discount the over-confidence (Kolcz and Chowdhury, 2005), we use the most basic one: we raise the likelihood term in power T, where the parameter T is chosen empirically. 7 Empirical Evaluation 7.1 Data and Evaluation We keep the general setup of (Lang and Lapata, 2011a), to evaluate our models and compare them to the current state of the art. We run all of our experiments on the standard CoNLL 2008 shared task (Surdeanu et al., 200</context>
</contexts>
<marker>Rennie, 2001</marker>
<rawString>Jason Rennie. 2001. Improving multi-class text classification with Naive bayes. Technical Report AITR-2001-004, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sammons</author>
<author>V Vydiswaran</author>
<author>T Vieira</author>
<author>N Johri</author>
<author>M Chang</author>
<author>D Goldwasser</author>
<author>V Srikumar</author>
<author>G Kundu</author>
<author>Y Tu</author>
<author>K Small</author>
<author>J Rule</author>
<author>Q Do</author>
<author>D Roth</author>
</authors>
<title>Relation alignment for textual entailment recognition.</title>
<date>2009</date>
<booktitle>In Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="2111" citStr="Sammons et al., 2009" startWordPosition="317" endWordPosition="320">rguments as well as assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typicall</context>
</contexts>
<marker>Sammons, Vydiswaran, Vieira, Johri, Chang, Goldwasser, Srikumar, Kundu, Tu, Small, Rule, Do, Roth, 2009</marker>
<rawString>M. Sammons, V. Vydiswaran, T. Vieira, N. Johri, M. Chang, D. Goldwasser, V. Srikumar, G. Kundu, Y. Tu, K. Small, J. Rule, Q. Do, and D. Roth. 2009. Relation alignment for textual entailment recognition. In Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2041" citStr="Shen and Lapata, 2007" startWordPosition="307" endWordPosition="310">diction of predicate argument structure, i.e. both identification of arguments as well as assignment of labels according to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been su</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrew Maas</author>
<author>Christopher Manning</author>
</authors>
<title>Spectral chinese restaurant processes: Nonparametric clustering based on similarities.</title>
<date>2011</date>
<booktitle>In AISTATS.</booktitle>
<contexts>
<context position="6032" citStr="Socher et al., 2011" startWordPosition="934" endWordPosition="937">s (a) vs. (d), or dativization: John gave a book to Mary vs. John gave Mary a book) (Levin, 1993). Therefore, it is natural to assume that the clusterings should be similar, though not identical, across verbs. Our second model encodes this intuition by replacing the CRP prior for each predicate with a distance-dependent CRP (dd-CRP) prior (Blei and Frazier, 2011) shared across predicates. The distance between two syntactic signatures encodes how likely they are to correspond to a single semantic role. Unlike most of the previous work exploiting distance-dependent CRPs (Blei and Frazier, 2011; Socher et al., 2011; Duan et al., 2007), we do not encode prior or external knowledge in the distance function but rather induce it automatically within our Bayesian model. The coupled dd-CRP model consistently outperforms the factored CRP counterpart across all the experimental settings (with gold and predicted syntactic parses, and with gold and automatically identified arguments). Both models admit efficient inference: the estimation time on the Penn Treebank WSJ corpus does not exceed 30 minutes on a single processor and the inference algorithm is highly parallelizable, reducing inference time down to severa</context>
<context position="13792" citStr="Socher et al., 2011" startWordPosition="2226" endWordPosition="2229">obability proportional to some non-negative score di,j (di,j = dj,i) which encodes a similarity between the two customers.3 More formally, � ( ) p(ci = j |D, α) a di,j, i 7� j 1 α, i = j where D is the entire similarity graph. This process lacks the exchangeability property of the traditional CRP but efficient approximate inference with dd-CRP is possible with Gibbs sampling. For more details on inference with dd-CRPs, we refer the reader to Blei and Frazier (2011). Though in previous work dd-CRP was used either to encode prior knowledge (Blei and Frazier, 2011) or other external information (Socher et al., 2011), we treat D as a latent variable drawn from some prior distribution over weighted graphs. This view provides a powerful approach for coupling a family of distinct but similar clusterings: the family of clusterings can be drawn by first choosing a similarity graph D for the entire family and then re-using D to generate each of the clusterings independently of each other as defined by equation (1). In Section 5, we explain how we use this formalism to encode relatedness between argument key clusterings for different predicates. 4 Factored Model In this section we describe the factored method wh</context>
</contexts>
<marker>Socher, Maas, Manning, 2011</marker>
<rawString>Richard Socher, Andrew Maas, and Christopher Manning. 2011. Spectral chinese restaurant processes: Nonparametric clustering based on similarities. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Adam Meyers Richard Johansson</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Shared Task.</booktitle>
<marker>Surdeanu, Johansson, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Adam Meyers Richard Johansson, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="37670" citStr="Swier and Stevenson (2004)" startWordPosition="6187" endWordPosition="6190">-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. More recently, the role induction problem has been studied in Lang and Lapata (2010) where it has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones. Later, Lang and Lapata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline. In Lang and Lapata (2011b)</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>Richard Swier and Suzanne Stevenson. 2004. Unsupervised semantic role labelling. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>Dirichlet processes.</title>
<date>2010</date>
<booktitle>In Encyclopedia of Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="12320" citStr="Teh (2010)" startWordPosition="1967" endWordPosition="1968">ys, as we explain in Section 4. Often CRP is used as a part of the Dirichlet Process mixture model where each subset in the partition (each table) selects a parameter (a meal) from some base distribution over parameters. This parameter is then used to generate all data points corresponding to customers assigned to the table. The Dirichlet processes (DP) are closely connected to CRPs: instead of choosing meals for customers through the described generative story, one can equivalently draw a distribution G over meals from DP and then draw a meal for every customer from G. We refer the reader to Teh (2010) for details on CRPs and DPs. In our method, we use DPs to model distributions of arguments for every role. In order to clarify how similarities between customers can be integrated in the generative process, we start by reformulating the traditional CRP in an equivalent form so that distancedependent CRP (dd-CRP) can be seen as its generalization. Instead of selecting a table for each customer as described above, one can equivalently assume that a customer i chooses one of the previous customers ci as a partner with probability 1 i−1+α and sits at the same table, or occupies a new table with t</context>
</contexts>
<marker>Teh, 2010</marker>
<rawString>Yee Whye Teh. 2010. Dirichlet processes. In Encyclopedia of Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A Bayesian model for unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="17865" citStr="Titov and Klementiev, 2011" startWordPosition="2906" endWordPosition="2909">t one role r is drawn from the uniform Bernoulli distribution. If 0 is drawn then the semantic role is not realized for the given occurrence, otherwise the number of additional roles r is drawn from the geometric distribution Geom(op,r). The Beta priors over 0 4For prepositional phrases, we take as head the head noun of the object noun phrase as it encodes crucial lexical information. However, the preposition is not ignored but rather encoded in the corresponding argument key, as explained in Section 2. 5Alternatively, the clustering of arguments could be induced within the model, as done in (Titov and Klementiev, 2011). 15 Clustering of argument keys: Factored model: for each predicate p = 1, 2, ... : Bp ∼ CRP(a) [partition of arg keys] Coupled model: D ∼ NonInform [similarity graph] for each predicate p = 1, 2, ... : Bp ∼ dd-CRP(a, D) [partition of arg keys] Parameters: for each predicate p = 1, 2,... : for each role r ∈ Bp: Bp,r ∼ DP(,Q, H(A)) [distrib of arg fillers] 4&apos;p,r ∼Beta(770, 771) [geom distr for dup roles] Data Generation: for each predicate p = 1, 2,... : for each occurrence l of p: for every role r ∈ Bp: if [n ∼ Unif(0, 1)] = 1: [role appears at least once] GenArgument(p, r) [draw one arg] whi</context>
<context position="26436" citStr="Titov and Klementiev, 2011" startWordPosition="4333" endWordPosition="4337">ent key cp,k to ‘sit’ with. We can compute the MAP estimate for all di,j by maximizing the objective: 1: 1: dk,c�,k arg max log dzj, i7�j p kEK Ek&apos;EKI, dk,k&apos; where Kp is the set of all argument keys for the predicate p. We slightly abuse the notation by using di,i to denote the concentration parameter α in the previous expression. Note that we also assume that similarities are symmetric, di,j = dj,i. If the set of argument keys Kp would be the same for every predicate, then the optimal di,j would 7This idea has been explored before for shallow semantic representations (Lang and Lapata, 2011a; Titov and Klementiev, 2011). , 17 be proportional to the number of times either i selects j as a partner, or j chooses i as a partner.8 This no longer holds if the sets are different, but the solution can be found efficiently using a numeric optimization strategy; we use the gradient descent algorithm. We do not learn the concentration parameter α, as it is used in our model to indicate the desired granularity of semantic roles, but instead only learn di,j (i =� j). However, just learning the concentration parameter would not be sufficient as the effective concentration can be reduced or increased arbitrarily by scaling</context>
</contexts>
<marker>Titov, Klementiev, 2011</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2011. A Bayesian model for unsupervised semantic parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Mikhail Kozhevnikov</author>
</authors>
<title>Bootstrapping semantic analyzers from noncontradictory texts.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="37266" citStr="Titov and Kozhevnikov, 2010" startWordPosition="6122" endWordPosition="6125">otivates the need for using unlabeled data or other forms of weak supervision. This work includes methods based on graph alignment between labeled and unlabeled data (F¨urstenau and Lapata, 2009), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009), and projection of annotation across languages (Pado and Lapata, 2009; van der Plas et al., 2011). Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Goldwasser et al., 2011; Liang et al., 2011). Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar verbalizations of relations (Lin and Pantel, 2001; Banko et al., 2007). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the VerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. Mo</context>
</contexts>
<marker>Titov, Kozhevnikov, 2010</marker>
<rawString>Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrapping semantic analyzers from noncontradictory texts. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30285" citStr="Turian et al., 2010" startWordPosition="4987" endWordPosition="4990"> generated automatically by the MaltParser (Nivre et al., 2007). We vary our experimental setup as follows: • We evaluate our models on gold and automatically generated parses, and use either gold PropBank annotations or the heuristic from Section 2 to identify arguments, resulting in four experimental regimes. • In order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by a clustering algorithm as a preprocessing step. In particular, we use Brown (Br) clustering (Brown et al., 1992) induced over RCV1 corpus (Turian et al., 2010). Although the clustering is hierarchical, we only use a cluster at the lowest level of the hierarchy for each word. We use the purity (PU) and collocation (CO) metrics as well as their harmonic mean (F1) to measure the quality of the resulting clusters. Purity measures the degree to which each cluster contains arguments sharing the same gold role: 1 � P U = � i where if Ci is the set of arguments in the i-th induced cluster, Gj is the set of arguments in the jth 10The coupled model without discounting still outperforms the factored counterpart in our experiments. max |Gj n Ci| j 18 gold clust</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Paola Merlo</author>
<author>James Henderson</author>
</authors>
<title>Scaling up automatic cross-lingual semantic role annotation.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>van der Plas, Merlo, Henderson, 2011</marker>
<rawString>Lonneke van der Plas, Paola Merlo, and James Henderson. 2011. Scaling up automatic cross-lingual semantic role annotation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Semantic roles for SMT: A hybrid two-pass model.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2151" citStr="Wu and Fung, 2009" startWordPosition="323" endWordPosition="326">ording to their underlying semantic role. For example, in the following sentences: (a) [A0 Mary] opened [A1 the door]. (b) [A0 Mary] is expected to open [A1 the door]. (c) [A1 The door] opened. (d) [A1 The door] was opened [A0 by Mary]. Mary always takes an agent role (A0) for the predicate open, and door is always a patient (A1). SRL representations have many potential applications in natural language processing and have recently been shown to be beneficial in question answering (Shen and Lapata, 2007; Kaisser and Webber, 2007), textual entailment (Sammons et al., 2009), machine translation (Wu and Fung, 2009; Liu and Gildea, 2010; Wu et al., 2011; Gao and Vogel, 2011), and dialogue systems (Basili et al., 2009; van der Plas et al., 2011), among others. Though syntactic representations are often predictive of semantic roles (Levin, 1993), the interface between syntactic and semantic representations is far from trivial. The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods. Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated dat</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. 2009. Semantic roles for SMT: A hybrid two-pass model. In NAACL.</rawString>
</citation>
<citation valid="true">
<date>2011</date>
<booktitle>Proc. of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation. ACL.</booktitle>
<editor>Dekai Wu, Marianna Apidianaki, Marine Carpuat, and Lucia Specia, editors.</editor>
<contexts>
<context position="13641" citStr="(2011)" startWordPosition="2203" endWordPosition="2203">tion of this view leads to the definition of the distance-dependent CRP. In dd-CRPs, 14 a customer i chooses a partner ci = j with the probability proportional to some non-negative score di,j (di,j = dj,i) which encodes a similarity between the two customers.3 More formally, � ( ) p(ci = j |D, α) a di,j, i 7� j 1 α, i = j where D is the entire similarity graph. This process lacks the exchangeability property of the traditional CRP but efficient approximate inference with dd-CRP is possible with Gibbs sampling. For more details on inference with dd-CRPs, we refer the reader to Blei and Frazier (2011). Though in previous work dd-CRP was used either to encode prior knowledge (Blei and Frazier, 2011) or other external information (Socher et al., 2011), we treat D as a latent variable drawn from some prior distribution over weighted graphs. This view provides a powerful approach for coupling a family of distinct but similar clusterings: the family of clusterings can be drawn by first choosing a similarity graph D for the entire family and then re-using D to generate each of the clusterings independently of each other as defined by equation (1). In Section 5, we explain how we use this formali</context>
</contexts>
<marker>2011</marker>
<rawString>Dekai Wu, Marianna Apidianaki, Marine Carpuat, and Lucia Specia, editors. 2011. Proc. of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>