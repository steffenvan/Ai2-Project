<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.976262">
Supersense Tagging of Unknown Nouns in WordNet
</title>
<author confidence="0.999281">
Massimiliano Ciaramita Mark Johnson
</author>
<affiliation confidence="0.999233">
Brown University Brown University
</affiliation>
<email confidence="0.999573">
massi@brown.edu markjohnson@brown.edu
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999723923076923">
We present a new framework for classify-
ing common nouns that extends named-
entity classification. We used a fixed set
of 26 semantic labels, which we called su-
persenses. These are the labels used by
lexicographers developing WordNet. This
framework has a number of practical ad-
vantages. We show how information con-
tained in the dictionary can be used as ad-
ditional training data that improves accu-
racy in learning new nouns. We also de-
fine a more realistic evaluation procedure
than cross-validation.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998208745098039">
Lexical semantic information is useful in many nat-
ural language processing and information retrieval
applications, particularly tasks that require com-
plex inferences involving world knowledge, such
as question answering or the identification of co-
referential entities (Pasca and Harabagiu, 2001;
Pustejovsky et al., 2002).
However, even large lexical databases such as
WordNet (Fellbaum, 1998) do not include all of
the words encountered in broad-coverage NLP ap-
plications. Ideally, we would like a system that
automatically extends existing lexical resources by
✁We would like to thank Thomas Hofmann, Brian Roark,
and our colleagues in the Brown Laboratory for Linguistic In-
formation Processing (BLLIP), as well as Jesse Hochstadt for
his editing advice. This material is based upon work supported
by the National Science Foundation under Grant No. 0085940.
identifying the syntactic and semantic properties of
unknown words. In terms of the WordNet lexical
database, one would like to automatically assign un-
known words a position in the synset hierarchy, in-
troducing new synsets and extending the synset hier-
archy where appropriate. Doing this accurately is a
difficult problem, and in this paper we address a sim-
pler problem: automatically determining the broad
semantic class, or supersense, to which unknown
words belong.
Systems for thesaurus extension (Hearst, 1992;
Roark and Charniak, 1998), information extrac-
tion (Riloff and Jones, 1999) or named-entity recog-
nition (Collins and Singer, 1999) each partially ad-
dress this problem in different ways. The goal
in these tasks is automatically tagging words with
semantic labels such as “vehicle”, “organization”,
“person”, etc.
In this paper we extend the named-entity recogni-
tion approach to the classification of common nouns
into 26 different supersenses. Rather than define
these ourselves, we adopted the 26 “lexicographer
class” labels used in WordNet, which include labels
such as person, location, event, quantity, etc. We be-
lieve our general approach should generalize to other
definitions of supersenses.
Using the WordNet lexicographer classes as su-
persenses has a number of practical advantages.
First, we show how information contained in the dic-
tionary can be used as additional training data that
improves the system’s accuracy. Secondly, it is pos-
sible to use a very natural evaluation procedure. A
system can be trained on an earlier release of Word-
Net and tested on the words added in a later release,
</bodyText>
<figure confidence="0.970917666666667">
1 person 7 cognition 13 attribute 19 quantity 25 plant
2 communication 8 possession 14 object 20 motive 26 relation
3 artifact 9 location 15 process 21 animal
4 act 10 substance 16 Tops 22 body
5 group 11 state 17 phenomenon 23 feeling
6 food 12 time 18 event 24 shape
</figure>
<tableCaption confidence="0.992568">
Table 1. Lexicographer class labels, or supersenses.
</tableCaption>
<bodyText confidence="0.999944185185185">
since these labels are constant across different re-
leases. This new evaluation defines a realistic lexi-
cal acquisition task which is well defined, well mo-
tivated and easily standardizable.
The heart of our system is a multiclass perceptron
classifier (Crammer and Singer, 2002). The features
used are the standard ones used in word-sense classi-
fication and named-entity extraction tasks, i.e., col-
location, spelling and syntactic context features.
The experiments presented below show that when
the classifier also uses the data contained in the dic-
tionary its accuracy improves over that of a tradition-
ally trained classifier. Finally, we show that there are
both similarities and differences in the results ob-
tained with the new evaluation and standard cross-
validation. This might suggest that in fact that the
new evaluation defines a more realistic task.
The paper is organized as follows. In Section 2
we discuss the problem of unknown words and the
task of semantic classification. In Section 3 we de-
scribe the WordNet lexicographer classes, how to
extract training data from WordNet, the new evalu-
ation method and the relation of this task to named-
entity classification. In Section 4 we describe the
experimental setup, and in Section 5 we explain the
averaged perceptron classifier used. In Section 6 and
7 we discuss the results and the two evaluations.
</bodyText>
<sectionHeader confidence="0.956133" genericHeader="introduction">
2 Unknown Words and Semantic
Classification
</sectionHeader>
<bodyText confidence="0.999938791666667">
Language processing systems make use of “dictio-
naries”, i.e., lists that associate words with useful
information such as the word’s frequency or syn-
tactic category. In tasks that also involve inferences
about world knowledge, it is useful to know some-
thing about the meaning of the word. This lexical
semantic information is often modeled on what is
found in normal dictionaries, e.g., that “irises” are
flowers or that “exane” is a solvent.
This information can be crucial in tasks such
as question answering - e.g., to answer a ques-
tion such as “What kind of flowers did Van Gogh
paint?” (Pasca and Harabagiu, 2001) - or the indi-
viduation of co-referential expressions, as in the pas-
sage “... the prerun can be performed with
... this can be considered ...” (Pustejovsky
et al., 2002).
Lexical semantic information can be extracted
from existing dictionaries such as WordNet. How-
ever, these resources are incomplete and systems
that rely on them often encounter unknown words,
even if the dictionary is large. As an example, in the
Bllip corpus (a very large corpus of Wall Street Jour-
nal text) the relative frequency of common nouns
that are unknown to WordNet 1.6 is approximately
0.0054; an unknown noun occurs, on average, ev-
ery eight sentences. WordNet 1.6 lists 95,000 noun
types. For this reason the importance of issues such
as automatically building, extending or customizing
lexical resources has been recognized for some time
in computational linguistics (Zernik, 1991).
Solutions to this problem were first proposed
in AI in the context of story understanding, cf.
(Granger, 1977). The goal is to label words using
a set of semantic labels specified by the dictionary.
Several studies have addressed the problem of ex-
panding one semantic category at a time, such as
“vehicle” or “organization”, that are relevant to a
particular task (Hearst, 1992; Roark and Charniak,
1998; Riloff and Jones, 1999). In named-entity clas-
sification a large set of named entities (proper nouns)
are classified using a comprehensive set of semantic
labels such as “organization”, “person”, “location”
or “other” (Collins and Singer, 1999). This latter
approach assigns all named entities in the data set a
semantic label. We extend this approach to the clas-
sification of common nouns using a suitable set of
semantic classes.
</bodyText>
<sectionHeader confidence="0.9944265" genericHeader="method">
3 Lexicographer Classes for Noun
Classification
</sectionHeader>
<subsectionHeader confidence="0.999977">
3.1 WordNet Lexicographer Labels
</subsectionHeader>
<bodyText confidence="0.999989805555555">
WordNet (Fellbaum, 1998) is a broad-coverage
machine-readable dictionary. Release 1.71 of the
English version lists about 150,000 entries for all
open-class words, mostly nouns (109,000 types), but
also verbs, adjectives, and adverbs. WordNet is or-
ganized as a network of lexicalized concepts, sets of
synonyms called synsets; e.g., the nounschairman,
chairwoman, chair, chairperson form a synset. A
word that belongs to several synsets is ambiguous.
To facilitate the development of WordNet, lexi-
cographers organize synsets into several domains,
based on syntactic category and semantic coherence.
Each noun synset is assigned one out of 26 broad
categories1. Since these broad categories group to-
gether very many synsets, i.e., word senses, we call
them supersenses. The supersense labels that Word-
Net lexicographers use to organize nouns are listed
in Table 12. Notice that since the lexicographer la-
bels are assigned to synsets, often ambiguity is pre-
served even at this level. For example, chair has
three supersenses: “person”, “artifact”, and “act”.
This set of labels has a number of attractive fea-
tures for the purposes of lexical acquisition. It is
fairly general and therefore small. The reasonable
size of the label set makes it possible to apply state-
of-the-art machine learning methods. Otherwise,
classifying new words at the synset level defines a
multiclass problem with a huge class space - more
than 66,000 noun synsets in WordNet 1.6, more than
75,000 in the newest release, 1.71 (cf. also (Cia-
ramita, 2002) on this problem). At the same time
the labels are not too abstract or vague. Most of the
classes seem natural and easily recognizable. That
is probably why they were chosen by the lexicog-
raphers to facilitate their task. But there are more
important practical and methodological advantages.
</bodyText>
<subsectionHeader confidence="0.999769">
3.2 Extra Training Data from WordNet
</subsectionHeader>
<bodyText confidence="0.993889">
WordNet contains a great deal of information about
words and word senses.The information contained
</bodyText>
<footnote confidence="0.8719375">
1There are also 15 lexicographer classes for verbs, 3 for ad-
jectives and 1 for adverbs.
2The label “Tops” refers to about 40 very general synsets,
such as “phenomenon” “entity” “object” etc.
</footnote>
<bodyText confidence="0.99997796969697">
in the dictionary’s glosses is very similar to what
is typically listed in normal dictionaries: synonyms,
definitions and example sentences. This suggests a
very simple way in which it can be put into use: it
can be compiled into training data for supersense la-
bels. This data can then be added to the data ex-
tracted from the training corpus.
For several thousand concepts WordNet’s glosses
are very informative. The synset “chair” for example
looks as follows:
: president, chairman, chairwoman,
chair, chairperson – (the officer who presides at
the meetings of an organization); “address your
remarks to the chairperson”.
In WordNet 1.6, 66,841 synsets contain definitions
(in parentheses above), and 6,147 synsets contain
example sentences (in quotation marks). As we
show below, this information about word senses is
useful for supersense tagging. Presumably this is be-
cause if it can be said of a “chairperson” that she can
“preside at meetings” or that “a remark” can be “ad-
dressed to her”, then logically speaking these things
can be said of the superordinates of “chairperson”,
like “person”, as well.
Therefore information at the synset level is rele-
vant also at the supersense level. Furthermore, while
individually each gloss doesn’t say too much about
the narrow concept it is attached to (at least from
a machine learning perspective) at the supersense
level this information accumulates. In fact it forms
a small corpus of supersense-annotated data that can
be used to train a classifier for supersense tagging of
words or for other semantic classification tasks.
</bodyText>
<subsectionHeader confidence="0.943984">
3.3 Evaluation Methods
</subsectionHeader>
<bodyText confidence="0.999895416666667">
Formulating the problem in this fashion makes it
possible to define also a very natural evaluation pro-
cedure. Systems can be trained on nouns listed in
a given release of WordNet and tested on the nouns
introduced in a later version. The set of lexicogra-
pher labels remains constant and can be used across
different versions.
In this way systems can be tested on a more real-
istic lexical acquisition task - the same task that lex-
icographers carried out to extend the database. The
task is then well defined and motivated, and easily
standardizable.
</bodyText>
<subsectionHeader confidence="0.476601">
Supersense Labels
</subsectionHeader>
<figureCaption confidence="0.9759645">
Figure 1. Cumulative distribution of supersense labels in
Bllip.
</figureCaption>
<subsectionHeader confidence="0.971776">
3.4 Relation to Named-Entity Tasks
</subsectionHeader>
<bodyText confidence="0.999976583333333">
The categories typically used in named-entity recog-
nition tasks are a subset of the noun supersense la-
bels: “person”, “location”, and “group”. Small la-
bel sets like these can be sufficient in named-entity
recognition. Collins and Singer (1999) for exam-
ple report that 88% of the named entities occur-
ring in their data set belong to these three cate-
gories (Collins and Singer, 1999).
The distribution of common nouns, however, is
more uniform. We estimated this distribution by
counting the occurrences of 744 unambiguous com-
mon nouns newly introduced in WordNet 1.71. Fig-
ure 1 plots the cumulative frequency distribution of
supersense tokens; the labels are ordered by decreas-
ing relative frequency as in Table 1.
The most frequent supersenses are “person”,
“communication”, “artifact” etc. The three most fre-
quent supersenses account for a little more of 50%
of all tokens, and 9 supersenses account for 90% of
all tokens. A larger number of labels is needed for
supersense tagging than for named-entity recogni-
tion. The figure also shows the distribution of labels
for all unambiguous tokens in WordNet 1.6; the two
distributions are quite similar.
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9986696">
The “new” nouns in WordNet 1.71 and the “old”
ones in WordNet 1.6 constitute the test and training
data that we used in our word classification exper-
iments. Here we describe the experimental setup:
training and test data, and features used.
</bodyText>
<subsectionHeader confidence="0.997126">
4.1 Training data
</subsectionHeader>
<bodyText confidence="0.999894166666667">
We extracted from the Bllip corpus all occur-
rences of nouns that have an entry in WordNet 1.6.
Bllip (BLLIP, 2000) is a 40-million-word syntac-
tically parsed corpus. We used the parses to ex-
tract the syntactic features described below. We then
removed all ambiguous nouns, i.e., nouns that are
tagged with more than one supersense label (72%
of the tokens, 28.9% of the types). In this way we
avoided dealing with the problem of ambi
We extracted a feature vector for each noun in-
stance using the feature set described below. Each
vector is a training instance. In addition we com-
piled another training set from the example sen-
tences and from the definitions in the noun database
of WordNet 1.6. Overall this procedure produced
787,186 training instances from Bllip, 66,841 train-
ing instances from WordNet’s definitions, and 6,147
training instances from the example sentences.
</bodyText>
<subsectionHeader confidence="0.657718">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.9953682">
We used a mix of standard features used in word
sense disambiguation, named-entity classification
and lexical acquisition. The following sentence il-
lustrates them: “The art-students, nine teen-agers,
read the book”, art-students is the tagged noun:
</bodyText>
<listItem confidence="0.926710583333333">
1. part of speech of the neighboring words: ,
, , ...
2. single words in the surrounding context: ,
, , , ...
3. bigrams and trigrams: ,
, , ...
4. syntactically governed elements under a given phrase:
5. syntactically governing elements under a given phrase:
6. coordinates/appositives:
7. spelling/morphological features: prefixes, suffixes, com-
plex morphology: , ... ,
... , ...
</listItem>
<footnote confidence="0.8487842">
3A simple option to deal with ambiguous words would be
to distribute an ambiguous noun’s counts to all its senses. How-
ever, in preliminary experiments we found that a better accuracy
is achieved using only non-ambiguous nouns. We will investi-
gate this issue in future research.
</footnote>
<figure confidence="0.9897853125">
0.1
5 10 15
20 25
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
Test: new nouns in WN 1.71
Train: nouns in WN 1.6
Cumulative Relative Frequency
guity3.
</figure>
<bodyText confidence="0.998658136363636">
Open class words were morphologically simpli-
fied with the “morph” function included in Word-
Net. We parsed the WordNet definitions and exam-
ple sentences with the same syntactic parser used for
Bllip (Charniak, 2000).
It is not always possible to identify the noun that
represents the synset in the WordNet glosses. For
example, in the gloss for the synset relegation the
example sentence is “He has been relegated to a post
in Siberia”, where a verb is used instead of the noun.
When it was possible to identify the target noun the
complete feature set was used; otherwise only the
surrounding-word features (2) and the spelling fea-
tures (7) of all synonyms were used. With the def-
initions it is much harder to individuate the target;
consider the definition “a member of the genus Ca-
nis” for dog. For all definitions we used only the
reduced feature set. One training instance per synset
was extracted from the example sentences and one
training instance from the definitions. Overall, in
the experiments we performed we used around 1.5
million features.
</bodyText>
<subsectionHeader confidence="0.990832">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999934428571429">
In a similar way to how we produced the training
data we compiled a test set from the Bllip corpus.
We found all instances of nouns that are not in Word-
Net 1.6 but are listed in WordNet 1.71 with only
one supersense. The majority of the novel nouns in
WordNet 1.71 are unambiguous (more than 90%).
There were 744 new noun types, with a total fre-
quency of 9,537 occurrences. We refer to this test
set as Test .
We also randomly removed 755 noun types
(20,394 tokens) from the training data and used them
as an alternative test set. We refer to this other test
set as Test . We then ran experiments using the
averaged multiclass perceptron.
</bodyText>
<sectionHeader confidence="0.995473" genericHeader="method">
5 The Multiclass Averaged Perceptron
</sectionHeader>
<bodyText confidence="0.998389138888889">
We used a multiclass averaged perceptron classifier,
which is an “ultraconservative” on-line learning al-
gorithm (Crammer and Singer, 2002), that is a mul-
ticlass extension of the standard perceptron learning
to the multiclass case. It takes as input a training set
, where each instance rep-
resents an instance of a noun and . Here
Algorithm 1 Multiclass Perceptron
is the set of supersenses defined by WordNet. Since
for training and testing we used only unambiguous
words there is always exactly one label per instance.
Thus summarizes word tokens that belong to the
dictionary, where each instance is represented as a
vector of features extracted from the context in
which the noun occurred; is the total number of
features; and is the true label of .
In general, a multiclass classifier for the dictio-
nary is a function that maps fea-
ture vectors to one of the possible supersenses of
WordNet. In the multiclass perceptron, one intro-
duces a weight vector for every and
defines implicitly by the so-called winner-take-all
rule
Here refers to the matrix of weights,
with every column corresponding to one of the
weight vectors .
The learning algorithm works as follows: Train-
ing patterns are presented one at a time in
the standard on-line learning setting. Whenever
an update step is performed; oth-
erwise the weight vectors remain unchanged. To
perform the update, one first computes the error set
containing those class labels that have received a
higher score than the correct class:
An ultraconservative update scheme in its most gen-
eral form is then defined as follows: Update
</bodyText>
<figure confidence="0.995757111111111">
1: input training data ,
2: repeat
3: for do
4: if then
7: for do
9: end for
10: end if
11: end for
12: until no more mistakes
</figure>
<bodyText confidence="0.997166037037037">
with learning rates fulfilling the con-
straints , ,and
for . Hence changes are limited to
for . The sum constraint ensures
that the update is balanced, which is crucial to guar-
anteeing the convergence of the learning procedure
(cf. (Crammer and Singer, 2002)). We have focused
on the simplest case of uniform update weights,
for . The algorithm is summa-
rized in Algorithm 1.
Notice that the multiclass perceptron algorithm
learns all weight vectors in a coupled manner, in
contrast to methods that perform multiclass classifi-
cation by combining binary classifiers, for example,
training a classifier for each class in a one-against-
the-rest manner.
The averaged version of the perceptron (Collins,
2002), like the voted perceptron (Freund and
Schapire, 1999), reduces the effect of over-training.
In addition to the matrix of weight vectors the
model keeps track for each feature of each value
it assumed during training, , and the number of
consecutive training instance presentations during
which this weight was not changed, or “life span”,
. When training is done these weights are av-
eraged and the final averaged weight of feature
is computed as
</bodyText>
<equation confidence="0.406552">
(3)
</equation>
<bodyText confidence="0.999909454545454">
For example, if there is a feature weight that is
not updated until example 500, at which point it is
incremented to value 1, and is not touched again
until after example 1000, then the average weight
of that feature in the averaged perceptron at ex-
ample 750 will be: , or 1/3. At ex-
ample 1000 it will be 1/2, etc. We used the av-
eraged model for evaluation and parameter setting;
see below. Figure 2 plots the results on test data of
both models. The average model produces a better-
performing and smoother output.
</bodyText>
<subsectionHeader confidence="0.995326">
5.1 Parameters Setting
</subsectionHeader>
<bodyText confidence="0.999638666666667">
We used an implementation with full, i.e., not
sparse, representation of the matrix for the percep-
tron. Training and test are fast, at the expense of a
</bodyText>
<page confidence="0.9516024">
38
37
36
35
34
33
32
31
30
29
</page>
<figure confidence="0.9888268">
Averaged perceptron
Basic perceptron
27
0 100 200 300 400 500 600 700 800 900 1000
Epochs
</figure>
<figureCaption confidence="0.9940795">
Figure 2. Results on test of the normal and averaged
perceptron
</figureCaption>
<bodyText confidence="0.99955405882353">
slightly greater memory load. Given the great num-
ber of features, we couldn’t use the full training set
from the Bllip corpus. Instead we randomly sam-
pled from roughly half of the available training data,
yielding around 400,000 instances, the size of the
training is close to 500,000 instances with also the
WordNet data. When training to test on Test , we
removed from the WordNet training set the synsets
relative to the nouns in Test .
The only adjustable parameter to set is the number
of passes on the training data, or epochs. While test-
ing on Test we set this parameter using Test ,
and vice versa for Test . The estimated values for
the stopping iterations were very close at roughly ten
passes. As Figure 2 shows, the great amount of data
requires many passes over the data, around 1,000,
before reaching convergence (on Test ).
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.962549333333333">
The classifier outputs the estimated supersense label
of each instance of each unknown noun type. The
label of a noun type is obtained by voting4:
</bodyText>
<equation confidence="0.288657">
(4)
</equation>
<bodyText confidence="0.496608">
where is the indicator function and means
that is a token of type . The score on is 1 if
</bodyText>
<footnote confidence="0.996772666666667">
4During preliminary experiments we tried also creating one
single aggregate pattern for each test noun type but this method
produced worse results.
</footnote>
<table confidence="0.998482363636364">
Accuracy on Test1.71
28
Method Token Type Test set
Baseline 20.0 27.8
AP-B-55 35.9 50.7 Test
AP-B-65 36.1 50.8
AP-B-55+WN 36.9 52.9
Baseline 24.1 21.0
AP-B-55 47.4 47.7 Test
AP-B-65 47.9 48.3
AP-B-55+WN 52.3 53.4
</table>
<tableCaption confidence="0.999928">
Table 2. Experimental results.
</tableCaption>
<bodyText confidence="0.961152861111111">
, where is the correct label for
, and 0 otherwise.
Table 2 summarizes the results of the experiments
on Test (upper half) and on Test (bottom half).
A baseline was computed that always selected the
most frequent label in the training data, “person”,
which is also the most frequent in both Test and
Test . The baseline performances are in the low
twenties. The first and second columns report per-
formance on tokens and types respectively.
The classifiers’ results are averages over 50 trials
in which a fraction of the Bllip data was randomly
selected. One classifier was trained on 55% of the
Bllip data (AP-B-55). An identical one was trained
on the same data and, additionally, on the WordNet
data (AP-B-55+WN). We also trained a classifier on
65% of the Bliip data (AP-B-65). Adding the Word-
Net data to this training set was not possible because
of memory limitations. The model also trained on
WordNet outperforms on both test sets those trained
only on the Bllip data. A paired t-test proved the
difference between models with and without Word-
Net data to be statistically significant. The “least”
significant difference is between AP-B-65 and AP-
B-55+WN (token) on Test : . In all
other cases the -level is much smaller.
These results seem to show that the positive im-
pact of the WordNet data is not simply due to the
fact that there is more training data5. Adding the
WordNet data seems more effective than adding an
equivalent amount of standard training data. Fig-
ure 3 plots the results of the last set of (single trial)
experiments we performed, in which we varied the
5Notice that 10% of the Bllip data is approximately the size
of the WordNet data and therefore AP-B-65 and AP-B-55+WN
are trained on roughly the same amount of data.
</bodyText>
<subsubsectionHeader confidence="0.210431">
Percentage of Training Data Used
</subsubsectionHeader>
<figureCaption confidence="0.9285195">
Figure 3. Results on Test incrementing the amount of
training data.
</figureCaption>
<bodyText confidence="0.999919">
amount of Bllip data to be added to the WordNet
one. The model with WordNet data often performs
better than the model trained only on Bllip data even
when the latter training set is much larger.
Two important reasons why the WordNet data is
particularly good are, in our opinion, the following.
The data is less noisy because it is extracted from
sentences and definitions that are always “pertinent”
to the class label. The data also contains instances
of disambiguated polysemous nouns, which instead
were excluded from the Bllip training. This means
that disambiguating the training data is important;
unfortunately this is not a trivial task. Using the
WordNet data provides a simple way of getting at
least some information from ambiguous nouns.
</bodyText>
<sectionHeader confidence="0.991258" genericHeader="method">
7 Differences Between Test Sets
</sectionHeader>
<bodyText confidence="0.999950916666667">
The type scores on both evaluations produced simi-
lar results. This finding supports the hypothesis that
the two evaluations are similar in difficulty, and that
the two versions of WordNet are not inconsistent in
the way they assign supersenses to nouns.
The evaluations show, however, very different
patterns at the token level. This might be due to the
fact that the label distribution of the training data is
more similar to Test than to Test . In particular,
there are many new nouns in Test that belong to
“abstract” classes6, which seem harder to learn. Ab-
stract classes are also more confusable; i.e., mem-
</bodyText>
<footnote confidence="0.9874665">
6Such as “communication” (e.g., reaffirmation) or “cogni-
tion” (e.g., mind set).
</footnote>
<page confidence="0.889977">
48
</page>
<figure confidence="0.935352823529412">
47
46
45
5 10 15 20 25 30 35 40 45 50 55
AP−B−55+WN
AP−B−55
55
54
53
52
51
50
49
Accuracy on Types (Tesi.71)
70
Test1.6
Test1.71
</figure>
<figureCaption confidence="0.8051905">
Figure 4. Results on types for Test and Test ranked
by the frequency of the test words.
</figureCaption>
<bodyText confidence="0.9997595">
bers of these classes are frequently mis-classified
with the same wrong label. A few very frequently
mis-classified pairs are communication/act, commu-
nication/person and communication/artifact.
As a result of the fact that abstract nouns are more
frequent in Test than in Test the accuracy on
tokens is much worse in the new evaluation than in
the more standard one. This has an impact also on
the type scores. Figure 4 plots the results on types
for Test and Test grouped in bins of test noun
types ranked by decreasing frequency. It shows that
the first bin is harder in Test than in Test .
Overall, then, it seems that there are similarities
but also important differences between the evalua-
tions. Therefore the new evaluation might define a
more realistic task than cross-validation.
</bodyText>
<sectionHeader confidence="0.998288" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999996">
We presented a new framework for word sense
classification, based on the WordNet lexicographer
classes, that extends named-entity classification.
Within this framework it is possible to use the in-
formation contained in WordNet to improve classi-
fication and define a more realistic evaluation than
standard cross-validation. Directions for future re-
search include the following topics: disambiguation
of the training data, e.g. during training as in co-
training; learning unknown ambiguous nouns, e.g.,
studying the distribution of the labels the classifier
guessed for the individual tokens of the new word.
</bodyText>
<sectionHeader confidence="0.996451" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852923076923">
BLLIP. 2000. 1987-1989 WSJ Corpus Release 1. Linguistic
Data Consortium.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 38th Annual Meeting of the Association
for Computational Linguistics.
M. Ciaramita. 2002. Boosting Automatic Lexical Acquisi-
tion with Morphological Information. In Proceedings of the
Workshop on Unsupervised Lexical Acquisition, ACL-02.
M. Collins and Y. Singer. 1999. Unsupervised Models for
Named Entity Classification. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora.
M. Collins. 2002. Discriminative Training Methods for Hidden
Markov Models: Theory and Experiments with Perceptron
Algorithms. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP-02),
pages 1–8.
K. Crammer and Y. Singer. 2002. Ultraconservative Online Al-
gorithms for Multiclass Problems. Technical Report [2001-
18], School of Computer Science and Engineering, Hebrew
University, Jerusalem, Israel.
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA.
Y. Freund and R. Schapire. 1999. Large Margin Classification
Using the Perceptron Algorithm. Machine Learning, 37.
R. Granger. 1977. FOUL-UP: A Program that Figures Out
Meanings of Words from Context. In Proceedings of the
Fifth International Joint Conference on Artificial Intelli-
gence.
M. Hearst. 1992. Automatic Acquisition of Hyponyms from
Large Text Corpora. In Proceedings of the 14th Interna-
tional Conference on Computational Linguistics.
M. Pasca and S.H. Harabagiu. 2001. The Informative Role of
WordNet in Open-Domain Question Answering. In NAACL
2001 Workshop on WordNet and Other Lexical Resources:
Applications, Extensions and Customizations.
J. Pustejovsky, A. Rumshisky, and J. Casta o. 2002. Rerender-
ing Semantic Ontologies: Automatic Extensions to UMLS
through Corpus Analytics. In In Proceedings of REC 2002
Workshop on Ontologies and Lexical Knowledge Bases.
E. Riloff and R. Jones. 1999. Learning Dictionaries for In-
formation Extraction by Multi-Level Bootstrapping. In Pro-
ceedings of the Sixteenth National Conference on Artificial
Intelligence.
B. Roark and E. Charniak. 1998. Noun-Phrase Co-Occurrence
Statistics for Semi-Automatic Semantic Lexicon Construc-
tion. In Proceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and 17th International
Conference on Computational Linguistics.
U. Zernik. 1991. Introduction. In U. Zernik, editor, Lexical Ac-
quisition: Exploiting On-line Resources to Build a Lexicon.
Lawrence Erlbaum Associates.
</reference>
<page confidence="0.568828">
65
</page>
<figure confidence="0.993988428571429">
60
55
50
45
1 2 3 4 5 6 7
Test Nouns Frequency Bins
Accuracy
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981192">
<title confidence="0.999438">Supersense Tagging of Unknown Nouns in WordNet</title>
<author confidence="0.999984">Massimiliano Ciaramita Mark Johnson</author>
<affiliation confidence="0.999992">Brown University Brown University</affiliation>
<email confidence="0.999215">massi@brown.edumarkjohnson@brown.edu</email>
<abstract confidence="0.998744928571429">We present a new framework for classifying common nouns that extends namedentity classification. We used a fixed set 26 semantic labels, which we called su- These are the labels used by lexicographers developing WordNet. This framework has a number of practical advantages. We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. We also define a more realistic evaluation procedure than cross-validation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>BLLIP</author>
</authors>
<title>1987-1989 WSJ Corpus Release 1. Linguistic Data Consortium.</title>
<date>2000</date>
<contexts>
<context position="13240" citStr="BLLIP, 2000" startWordPosition="2113" endWordPosition="2114">larger number of labels is needed for supersense tagging than for named-entity recognition. The figure also shows the distribution of labels for all unambiguous tokens in WordNet 1.6; the two distributions are quite similar. 4 Experiments The “new” nouns in WordNet 1.71 and the “old” ones in WordNet 1.6 constitute the test and training data that we used in our word classification experiments. Here we describe the experimental setup: training and test data, and features used. 4.1 Training data We extracted from the Bllip corpus all occurrences of nouns that have an entry in WordNet 1.6. Bllip (BLLIP, 2000) is a 40-million-word syntactically parsed corpus. We used the parses to extract the syntactic features described below. We then removed all ambiguous nouns, i.e., nouns that are tagged with more than one supersense label (72% of the tokens, 28.9% of the types). In this way we avoided dealing with the problem of ambi We extracted a feature vector for each noun instance using the feature set described below. Each vector is a training instance. In addition we compiled another training set from the example sentences and from the definitions in the noun database of WordNet 1.6. Overall this proced</context>
</contexts>
<marker>BLLIP, 2000</marker>
<rawString>BLLIP. 2000. 1987-1989 WSJ Corpus Release 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="15279" citStr="Charniak, 2000" startWordPosition="2449" endWordPosition="2450">to deal with ambiguous words would be to distribute an ambiguous noun’s counts to all its senses. However, in preliminary experiments we found that a better accuracy is achieved using only non-ambiguous nouns. We will investigate this issue in future research. 0.1 5 10 15 20 25 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 Test: new nouns in WN 1.71 Train: nouns in WN 1.6 Cumulative Relative Frequency guity3. Open class words were morphologically simplified with the “morph” function included in WordNet. We parsed the WordNet definitions and example sentences with the same syntactic parser used for Bllip (Charniak, 2000). It is not always possible to identify the noun that represents the synset in the WordNet glosses. For example, in the gloss for the synset relegation the example sentence is “He has been relegated to a post in Siberia”, where a verb is used instead of the noun. When it was possible to identify the target noun the complete feature set was used; otherwise only the surrounding-word features (2) and the spelling features (7) of all synonyms were used. With the definitions it is much harder to individuate the target; consider the definition “a member of the genus Canis” for dog. For all definitio</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
</authors>
<title>Boosting Automatic Lexical Acquisition with Morphological Information.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Unsupervised Lexical Acquisition, ACL-02.</booktitle>
<contexts>
<context position="8815" citStr="Ciaramita, 2002" startWordPosition="1392" endWordPosition="1394"> assigned to synsets, often ambiguity is preserved even at this level. For example, chair has three supersenses: “person”, “artifact”, and “act”. This set of labels has a number of attractive features for the purposes of lexical acquisition. It is fairly general and therefore small. The reasonable size of the label set makes it possible to apply stateof-the-art machine learning methods. Otherwise, classifying new words at the synset level defines a multiclass problem with a huge class space - more than 66,000 noun synsets in WordNet 1.6, more than 75,000 in the newest release, 1.71 (cf. also (Ciaramita, 2002) on this problem). At the same time the labels are not too abstract or vague. Most of the classes seem natural and easily recognizable. That is probably why they were chosen by the lexicographers to facilitate their task. But there are more important practical and methodological advantages. 3.2 Extra Training Data from WordNet WordNet contains a great deal of information about words and word senses.The information contained 1There are also 15 lexicographer classes for verbs, 3 for adjectives and 1 for adverbs. 2The label “Tops” refers to about 40 very general synsets, such as “phenomenon” “ent</context>
</contexts>
<marker>Ciaramita, 2002</marker>
<rawString>M. Ciaramita. 2002. Boosting Automatic Lexical Acquisition with Morphological Information. In Proceedings of the Workshop on Unsupervised Lexical Acquisition, ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="2184" citStr="Collins and Singer, 1999" startWordPosition="324" endWordPosition="327">actic and semantic properties of unknown words. In terms of the WordNet lexical database, one would like to automatically assign unknown words a position in the synset hierarchy, introducing new synsets and extending the synset hierarchy where appropriate. Doing this accurately is a difficult problem, and in this paper we address a simpler problem: automatically determining the broad semantic class, or supersense, to which unknown words belong. Systems for thesaurus extension (Hearst, 1992; Roark and Charniak, 1998), information extraction (Riloff and Jones, 1999) or named-entity recognition (Collins and Singer, 1999) each partially address this problem in different ways. The goal in these tasks is automatically tagging words with semantic labels such as “vehicle”, “organization”, “person”, etc. In this paper we extend the named-entity recognition approach to the classification of common nouns into 26 different supersenses. Rather than define these ourselves, we adopted the 26 “lexicographer class” labels used in WordNet, which include labels such as person, location, event, quantity, etc. We believe our general approach should generalize to other definitions of supersenses. Using the WordNet lexicographer</context>
<context position="7019" citStr="Collins and Singer, 1999" startWordPosition="1110" endWordPosition="1113">em were first proposed in AI in the context of story understanding, cf. (Granger, 1977). The goal is to label words using a set of semantic labels specified by the dictionary. Several studies have addressed the problem of expanding one semantic category at a time, such as “vehicle” or “organization”, that are relevant to a particular task (Hearst, 1992; Roark and Charniak, 1998; Riloff and Jones, 1999). In named-entity classification a large set of named entities (proper nouns) are classified using a comprehensive set of semantic labels such as “organization”, “person”, “location” or “other” (Collins and Singer, 1999). This latter approach assigns all named entities in the data set a semantic label. We extend this approach to the classification of common nouns using a suitable set of semantic classes. 3 Lexicographer Classes for Noun Classification 3.1 WordNet Lexicographer Labels WordNet (Fellbaum, 1998) is a broad-coverage machine-readable dictionary. Release 1.71 of the English version lists about 150,000 entries for all open-class words, mostly nouns (109,000 types), but also verbs, adjectives, and adverbs. WordNet is organized as a network of lexicalized concepts, sets of synonyms called synsets; e.g.</context>
<context position="11945" citStr="Collins and Singer (1999)" startWordPosition="1896" endWordPosition="1899">d can be used across different versions. In this way systems can be tested on a more realistic lexical acquisition task - the same task that lexicographers carried out to extend the database. The task is then well defined and motivated, and easily standardizable. Supersense Labels Figure 1. Cumulative distribution of supersense labels in Bllip. 3.4 Relation to Named-Entity Tasks The categories typically used in named-entity recognition tasks are a subset of the noun supersense labels: “person”, “location”, and “group”. Small label sets like these can be sufficient in named-entity recognition. Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999). The distribution of common nouns, however, is more uniform. We estimated this distribution by counting the occurrences of 744 unambiguous common nouns newly introduced in WordNet 1.71. Figure 1 plots the cumulative frequency distribution of supersense tokens; the labels are ordered by decreasing relative frequency as in Table 1. The most frequent supersenses are “person”, “communication”, “artifact” etc. The three most frequent supersenses account for a li</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-02),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="19220" citStr="Collins, 2002" startWordPosition="3126" endWordPosition="3127">o for . The sum constraint ensures that the update is balanced, which is crucial to guaranteeing the convergence of the learning procedure (cf. (Crammer and Singer, 2002)). We have focused on the simplest case of uniform update weights, for . The algorithm is summarized in Algorithm 1. Notice that the multiclass perceptron algorithm learns all weight vectors in a coupled manner, in contrast to methods that perform multiclass classification by combining binary classifiers, for example, training a classifier for each class in a one-againstthe-rest manner. The averaged version of the perceptron (Collins, 2002), like the voted perceptron (Freund and Schapire, 1999), reduces the effect of over-training. In addition to the matrix of weight vectors the model keeps track for each feature of each value it assumed during training, , and the number of consecutive training instance presentations during which this weight was not changed, or “life span”, . When training is done these weights are averaged and the final averaged weight of feature is computed as (3) For example, if there is a feature weight that is not updated until example 500, at which point it is incremented to value 1, and is not touched aga</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-02), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative Online Algorithms for Multiclass Problems.</title>
<date>2002</date>
<tech>Technical Report [2001-18],</tech>
<institution>School of Computer Science and Engineering, Hebrew University,</institution>
<location>Jerusalem,</location>
<contexts>
<context position="3757" citStr="Crammer and Singer, 2002" startWordPosition="581" endWordPosition="584">added in a later release, 1 person 7 cognition 13 attribute 19 quantity 25 plant 2 communication 8 possession 14 object 20 motive 26 relation 3 artifact 9 location 15 process 21 animal 4 act 10 substance 16 Tops 22 body 5 group 11 state 17 phenomenon 23 feeling 6 food 12 time 18 event 24 shape Table 1. Lexicographer class labels, or supersenses. since these labels are constant across different releases. This new evaluation defines a realistic lexical acquisition task which is well defined, well motivated and easily standardizable. The heart of our system is a multiclass perceptron classifier (Crammer and Singer, 2002). The features used are the standard ones used in word-sense classification and named-entity extraction tasks, i.e., collocation, spelling and syntactic context features. The experiments presented below show that when the classifier also uses the data contained in the dictionary its accuracy improves over that of a traditionally trained classifier. Finally, we show that there are both similarities and differences in the results obtained with the new evaluation and standard crossvalidation. This might suggest that in fact that the new evaluation defines a more realistic task. The paper is organ</context>
<context position="16949" citStr="Crammer and Singer, 2002" startWordPosition="2737" endWordPosition="2740">1 with only one supersense. The majority of the novel nouns in WordNet 1.71 are unambiguous (more than 90%). There were 744 new noun types, with a total frequency of 9,537 occurrences. We refer to this test set as Test . We also randomly removed 755 noun types (20,394 tokens) from the training data and used them as an alternative test set. We refer to this other test set as Test . We then ran experiments using the averaged multiclass perceptron. 5 The Multiclass Averaged Perceptron We used a multiclass averaged perceptron classifier, which is an “ultraconservative” on-line learning algorithm (Crammer and Singer, 2002), that is a multiclass extension of the standard perceptron learning to the multiclass case. It takes as input a training set , where each instance represents an instance of a noun and . Here Algorithm 1 Multiclass Perceptron is the set of supersenses defined by WordNet. Since for training and testing we used only unambiguous words there is always exactly one label per instance. Thus summarizes word tokens that belong to the dictionary, where each instance is represented as a vector of features extracted from the context in which the noun occurred; is the total number of features; and is the t</context>
<context position="18776" citStr="Crammer and Singer, 2002" startWordPosition="3055" endWordPosition="3058">. To perform the update, one first computes the error set containing those class labels that have received a higher score than the correct class: An ultraconservative update scheme in its most general form is then defined as follows: Update 1: input training data , 2: repeat 3: for do 4: if then 7: for do 9: end for 10: end if 11: end for 12: until no more mistakes with learning rates fulfilling the constraints , ,and for . Hence changes are limited to for . The sum constraint ensures that the update is balanced, which is crucial to guaranteeing the convergence of the learning procedure (cf. (Crammer and Singer, 2002)). We have focused on the simplest case of uniform update weights, for . The algorithm is summarized in Algorithm 1. Notice that the multiclass perceptron algorithm learns all weight vectors in a coupled manner, in contrast to methods that perform multiclass classification by combining binary classifiers, for example, training a classifier for each class in a one-againstthe-rest manner. The averaged version of the perceptron (Collins, 2002), like the voted perceptron (Freund and Schapire, 1999), reduces the effect of over-training. In addition to the matrix of weight vectors the model keeps tr</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>K. Crammer and Y. Singer. 2002. Ultraconservative Online Algorithms for Multiclass Problems. Technical Report [2001-18], School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1072" citStr="Fellbaum, 1998" startWordPosition="155" endWordPosition="156"> how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. We also define a more realistic evaluation procedure than cross-validation. 1 Introduction Lexical semantic information is useful in many natural language processing and information retrieval applications, particularly tasks that require complex inferences involving world knowledge, such as question answering or the identification of coreferential entities (Pasca and Harabagiu, 2001; Pustejovsky et al., 2002). However, even large lexical databases such as WordNet (Fellbaum, 1998) do not include all of the words encountered in broad-coverage NLP applications. Ideally, we would like a system that automatically extends existing lexical resources by ✁We would like to thank Thomas Hofmann, Brian Roark, and our colleagues in the Brown Laboratory for Linguistic Information Processing (BLLIP), as well as Jesse Hochstadt for his editing advice. This material is based upon work supported by the National Science Foundation under Grant No. 0085940. identifying the syntactic and semantic properties of unknown words. In terms of the WordNet lexical database, one would like to autom</context>
<context position="7312" citStr="Fellbaum, 1998" startWordPosition="1157" endWordPosition="1158">hat are relevant to a particular task (Hearst, 1992; Roark and Charniak, 1998; Riloff and Jones, 1999). In named-entity classification a large set of named entities (proper nouns) are classified using a comprehensive set of semantic labels such as “organization”, “person”, “location” or “other” (Collins and Singer, 1999). This latter approach assigns all named entities in the data set a semantic label. We extend this approach to the classification of common nouns using a suitable set of semantic classes. 3 Lexicographer Classes for Noun Classification 3.1 WordNet Lexicographer Labels WordNet (Fellbaum, 1998) is a broad-coverage machine-readable dictionary. Release 1.71 of the English version lists about 150,000 entries for all open-class words, mostly nouns (109,000 types), but also verbs, adjectives, and adverbs. WordNet is organized as a network of lexicalized concepts, sets of synonyms called synsets; e.g., the nounschairman, chairwoman, chair, chairperson form a synset. A word that belongs to several synsets is ambiguous. To facilitate the development of WordNet, lexicographers organize synsets into several domains, based on syntactic category and semantic coherence. Each noun synset is assig</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<contexts>
<context position="19275" citStr="Freund and Schapire, 1999" startWordPosition="3132" endWordPosition="3135"> update is balanced, which is crucial to guaranteeing the convergence of the learning procedure (cf. (Crammer and Singer, 2002)). We have focused on the simplest case of uniform update weights, for . The algorithm is summarized in Algorithm 1. Notice that the multiclass perceptron algorithm learns all weight vectors in a coupled manner, in contrast to methods that perform multiclass classification by combining binary classifiers, for example, training a classifier for each class in a one-againstthe-rest manner. The averaged version of the perceptron (Collins, 2002), like the voted perceptron (Freund and Schapire, 1999), reduces the effect of over-training. In addition to the matrix of weight vectors the model keeps track for each feature of each value it assumed during training, , and the number of consecutive training instance presentations during which this weight was not changed, or “life span”, . When training is done these weights are averaged and the final averaged weight of feature is computed as (3) For example, if there is a feature weight that is not updated until example 500, at which point it is incremented to value 1, and is not touched again until after example 1000, then the average weight of</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Granger</author>
</authors>
<title>FOUL-UP: A Program that Figures Out Meanings of Words from Context.</title>
<date>1977</date>
<booktitle>In Proceedings of the Fifth International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6481" citStr="Granger, 1977" startWordPosition="1027" endWordPosition="1028">ven if the dictionary is large. As an example, in the Bllip corpus (a very large corpus of Wall Street Journal text) the relative frequency of common nouns that are unknown to WordNet 1.6 is approximately 0.0054; an unknown noun occurs, on average, every eight sentences. WordNet 1.6 lists 95,000 noun types. For this reason the importance of issues such as automatically building, extending or customizing lexical resources has been recognized for some time in computational linguistics (Zernik, 1991). Solutions to this problem were first proposed in AI in the context of story understanding, cf. (Granger, 1977). The goal is to label words using a set of semantic labels specified by the dictionary. Several studies have addressed the problem of expanding one semantic category at a time, such as “vehicle” or “organization”, that are relevant to a particular task (Hearst, 1992; Roark and Charniak, 1998; Riloff and Jones, 1999). In named-entity classification a large set of named entities (proper nouns) are classified using a comprehensive set of semantic labels such as “organization”, “person”, “location” or “other” (Collins and Singer, 1999). This latter approach assigns all named entities in the data </context>
</contexts>
<marker>Granger, 1977</marker>
<rawString>R. Granger. 1977. FOUL-UP: A Program that Figures Out Meanings of Words from Context. In Proceedings of the Fifth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2053" citStr="Hearst, 1992" startWordPosition="307" endWordPosition="308">material is based upon work supported by the National Science Foundation under Grant No. 0085940. identifying the syntactic and semantic properties of unknown words. In terms of the WordNet lexical database, one would like to automatically assign unknown words a position in the synset hierarchy, introducing new synsets and extending the synset hierarchy where appropriate. Doing this accurately is a difficult problem, and in this paper we address a simpler problem: automatically determining the broad semantic class, or supersense, to which unknown words belong. Systems for thesaurus extension (Hearst, 1992; Roark and Charniak, 1998), information extraction (Riloff and Jones, 1999) or named-entity recognition (Collins and Singer, 1999) each partially address this problem in different ways. The goal in these tasks is automatically tagging words with semantic labels such as “vehicle”, “organization”, “person”, etc. In this paper we extend the named-entity recognition approach to the classification of common nouns into 26 different supersenses. Rather than define these ourselves, we adopted the 26 “lexicographer class” labels used in WordNet, which include labels such as person, location, event, qu</context>
<context position="6748" citStr="Hearst, 1992" startWordPosition="1072" endWordPosition="1073">s. WordNet 1.6 lists 95,000 noun types. For this reason the importance of issues such as automatically building, extending or customizing lexical resources has been recognized for some time in computational linguistics (Zernik, 1991). Solutions to this problem were first proposed in AI in the context of story understanding, cf. (Granger, 1977). The goal is to label words using a set of semantic labels specified by the dictionary. Several studies have addressed the problem of expanding one semantic category at a time, such as “vehicle” or “organization”, that are relevant to a particular task (Hearst, 1992; Roark and Charniak, 1998; Riloff and Jones, 1999). In named-entity classification a large set of named entities (proper nouns) are classified using a comprehensive set of semantic labels such as “organization”, “person”, “location” or “other” (Collins and Singer, 1999). This latter approach assigns all named entities in the data set a semantic label. We extend this approach to the classification of common nouns using a suitable set of semantic classes. 3 Lexicographer Classes for Noun Classification 3.1 WordNet Lexicographer Labels WordNet (Fellbaum, 1998) is a broad-coverage machine-readabl</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Proceedings of the 14th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>S H Harabagiu</author>
</authors>
<title>The Informative Role of WordNet in Open-Domain Question Answering.</title>
<date>2001</date>
<booktitle>In NAACL 2001 Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations.</booktitle>
<contexts>
<context position="973" citStr="Pasca and Harabagiu, 2001" startWordPosition="139" endWordPosition="142">abels used by lexicographers developing WordNet. This framework has a number of practical advantages. We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. We also define a more realistic evaluation procedure than cross-validation. 1 Introduction Lexical semantic information is useful in many natural language processing and information retrieval applications, particularly tasks that require complex inferences involving world knowledge, such as question answering or the identification of coreferential entities (Pasca and Harabagiu, 2001; Pustejovsky et al., 2002). However, even large lexical databases such as WordNet (Fellbaum, 1998) do not include all of the words encountered in broad-coverage NLP applications. Ideally, we would like a system that automatically extends existing lexical resources by ✁We would like to thank Thomas Hofmann, Brian Roark, and our colleagues in the Brown Laboratory for Linguistic Information Processing (BLLIP), as well as Jesse Hochstadt for his editing advice. This material is based upon work supported by the National Science Foundation under Grant No. 0085940. identifying the syntactic and sema</context>
<context position="5504" citStr="Pasca and Harabagiu, 2001" startWordPosition="868" endWordPosition="871">fication Language processing systems make use of “dictionaries”, i.e., lists that associate words with useful information such as the word’s frequency or syntactic category. In tasks that also involve inferences about world knowledge, it is useful to know something about the meaning of the word. This lexical semantic information is often modeled on what is found in normal dictionaries, e.g., that “irises” are flowers or that “exane” is a solvent. This information can be crucial in tasks such as question answering - e.g., to answer a question such as “What kind of flowers did Van Gogh paint?” (Pasca and Harabagiu, 2001) - or the individuation of co-referential expressions, as in the passage “... the prerun can be performed with ... this can be considered ...” (Pustejovsky et al., 2002). Lexical semantic information can be extracted from existing dictionaries such as WordNet. However, these resources are incomplete and systems that rely on them often encounter unknown words, even if the dictionary is large. As an example, in the Bllip corpus (a very large corpus of Wall Street Journal text) the relative frequency of common nouns that are unknown to WordNet 1.6 is approximately 0.0054; an unknown noun occurs, </context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>M. Pasca and S.H. Harabagiu. 2001. The Informative Role of WordNet in Open-Domain Question Answering. In NAACL 2001 Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>A Rumshisky</author>
<author>J Casta o</author>
</authors>
<title>Rerendering Semantic Ontologies: Automatic Extensions to UMLS through Corpus Analytics. In</title>
<date>2002</date>
<booktitle>In Proceedings of REC 2002 Workshop on Ontologies and Lexical Knowledge Bases.</booktitle>
<contexts>
<context position="1000" citStr="Pustejovsky et al., 2002" startWordPosition="143" endWordPosition="146">s developing WordNet. This framework has a number of practical advantages. We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. We also define a more realistic evaluation procedure than cross-validation. 1 Introduction Lexical semantic information is useful in many natural language processing and information retrieval applications, particularly tasks that require complex inferences involving world knowledge, such as question answering or the identification of coreferential entities (Pasca and Harabagiu, 2001; Pustejovsky et al., 2002). However, even large lexical databases such as WordNet (Fellbaum, 1998) do not include all of the words encountered in broad-coverage NLP applications. Ideally, we would like a system that automatically extends existing lexical resources by ✁We would like to thank Thomas Hofmann, Brian Roark, and our colleagues in the Brown Laboratory for Linguistic Information Processing (BLLIP), as well as Jesse Hochstadt for his editing advice. This material is based upon work supported by the National Science Foundation under Grant No. 0085940. identifying the syntactic and semantic properties of unknown </context>
<context position="5673" citStr="Pustejovsky et al., 2002" startWordPosition="898" endWordPosition="901">ry. In tasks that also involve inferences about world knowledge, it is useful to know something about the meaning of the word. This lexical semantic information is often modeled on what is found in normal dictionaries, e.g., that “irises” are flowers or that “exane” is a solvent. This information can be crucial in tasks such as question answering - e.g., to answer a question such as “What kind of flowers did Van Gogh paint?” (Pasca and Harabagiu, 2001) - or the individuation of co-referential expressions, as in the passage “... the prerun can be performed with ... this can be considered ...” (Pustejovsky et al., 2002). Lexical semantic information can be extracted from existing dictionaries such as WordNet. However, these resources are incomplete and systems that rely on them often encounter unknown words, even if the dictionary is large. As an example, in the Bllip corpus (a very large corpus of Wall Street Journal text) the relative frequency of common nouns that are unknown to WordNet 1.6 is approximately 0.0054; an unknown noun occurs, on average, every eight sentences. WordNet 1.6 lists 95,000 noun types. For this reason the importance of issues such as automatically building, extending or customizing</context>
</contexts>
<marker>Pustejovsky, Rumshisky, o, 2002</marker>
<rawString>J. Pustejovsky, A. Rumshisky, and J. Casta o. 2002. Rerendering Semantic Ontologies: Automatic Extensions to UMLS through Corpus Analytics. In In Proceedings of REC 2002 Workshop on Ontologies and Lexical Knowledge Bases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2129" citStr="Riloff and Jones, 1999" startWordPosition="316" endWordPosition="319">ndation under Grant No. 0085940. identifying the syntactic and semantic properties of unknown words. In terms of the WordNet lexical database, one would like to automatically assign unknown words a position in the synset hierarchy, introducing new synsets and extending the synset hierarchy where appropriate. Doing this accurately is a difficult problem, and in this paper we address a simpler problem: automatically determining the broad semantic class, or supersense, to which unknown words belong. Systems for thesaurus extension (Hearst, 1992; Roark and Charniak, 1998), information extraction (Riloff and Jones, 1999) or named-entity recognition (Collins and Singer, 1999) each partially address this problem in different ways. The goal in these tasks is automatically tagging words with semantic labels such as “vehicle”, “organization”, “person”, etc. In this paper we extend the named-entity recognition approach to the classification of common nouns into 26 different supersenses. Rather than define these ourselves, we adopted the 26 “lexicographer class” labels used in WordNet, which include labels such as person, location, event, quantity, etc. We believe our general approach should generalize to other defi</context>
<context position="6799" citStr="Riloff and Jones, 1999" startWordPosition="1078" endWordPosition="1081">For this reason the importance of issues such as automatically building, extending or customizing lexical resources has been recognized for some time in computational linguistics (Zernik, 1991). Solutions to this problem were first proposed in AI in the context of story understanding, cf. (Granger, 1977). The goal is to label words using a set of semantic labels specified by the dictionary. Several studies have addressed the problem of expanding one semantic category at a time, such as “vehicle” or “organization”, that are relevant to a particular task (Hearst, 1992; Roark and Charniak, 1998; Riloff and Jones, 1999). In named-entity classification a large set of named entities (proper nouns) are classified using a comprehensive set of semantic labels such as “organization”, “person”, “location” or “other” (Collins and Singer, 1999). This latter approach assigns all named entities in the data set a semantic label. We extend this approach to the classification of common nouns using a suitable set of semantic classes. 3 Lexicographer Classes for Noun Classification 3.1 WordNet Lexicographer Labels WordNet (Fellbaum, 1998) is a broad-coverage machine-readable dictionary. Release 1.71 of the English version l</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-Phrase Co-Occurrence Statistics for Semi-Automatic Semantic Lexicon Construction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2080" citStr="Roark and Charniak, 1998" startWordPosition="309" endWordPosition="312">sed upon work supported by the National Science Foundation under Grant No. 0085940. identifying the syntactic and semantic properties of unknown words. In terms of the WordNet lexical database, one would like to automatically assign unknown words a position in the synset hierarchy, introducing new synsets and extending the synset hierarchy where appropriate. Doing this accurately is a difficult problem, and in this paper we address a simpler problem: automatically determining the broad semantic class, or supersense, to which unknown words belong. Systems for thesaurus extension (Hearst, 1992; Roark and Charniak, 1998), information extraction (Riloff and Jones, 1999) or named-entity recognition (Collins and Singer, 1999) each partially address this problem in different ways. The goal in these tasks is automatically tagging words with semantic labels such as “vehicle”, “organization”, “person”, etc. In this paper we extend the named-entity recognition approach to the classification of common nouns into 26 different supersenses. Rather than define these ourselves, we adopted the 26 “lexicographer class” labels used in WordNet, which include labels such as person, location, event, quantity, etc. We believe our</context>
<context position="6774" citStr="Roark and Charniak, 1998" startWordPosition="1074" endWordPosition="1077"> lists 95,000 noun types. For this reason the importance of issues such as automatically building, extending or customizing lexical resources has been recognized for some time in computational linguistics (Zernik, 1991). Solutions to this problem were first proposed in AI in the context of story understanding, cf. (Granger, 1977). The goal is to label words using a set of semantic labels specified by the dictionary. Several studies have addressed the problem of expanding one semantic category at a time, such as “vehicle” or “organization”, that are relevant to a particular task (Hearst, 1992; Roark and Charniak, 1998; Riloff and Jones, 1999). In named-entity classification a large set of named entities (proper nouns) are classified using a comprehensive set of semantic labels such as “organization”, “person”, “location” or “other” (Collins and Singer, 1999). This latter approach assigns all named entities in the data set a semantic label. We extend this approach to the classification of common nouns using a suitable set of semantic classes. 3 Lexicographer Classes for Noun Classification 3.1 WordNet Lexicographer Labels WordNet (Fellbaum, 1998) is a broad-coverage machine-readable dictionary. Release 1.71</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-Phrase Co-Occurrence Statistics for Semi-Automatic Semantic Lexicon Construction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Zernik</author>
</authors>
<title>Introduction. In</title>
<date>1991</date>
<editor>U. Zernik, editor, Lexical</editor>
<contexts>
<context position="6369" citStr="Zernik, 1991" startWordPosition="1009" endWordPosition="1010">WordNet. However, these resources are incomplete and systems that rely on them often encounter unknown words, even if the dictionary is large. As an example, in the Bllip corpus (a very large corpus of Wall Street Journal text) the relative frequency of common nouns that are unknown to WordNet 1.6 is approximately 0.0054; an unknown noun occurs, on average, every eight sentences. WordNet 1.6 lists 95,000 noun types. For this reason the importance of issues such as automatically building, extending or customizing lexical resources has been recognized for some time in computational linguistics (Zernik, 1991). Solutions to this problem were first proposed in AI in the context of story understanding, cf. (Granger, 1977). The goal is to label words using a set of semantic labels specified by the dictionary. Several studies have addressed the problem of expanding one semantic category at a time, such as “vehicle” or “organization”, that are relevant to a particular task (Hearst, 1992; Roark and Charniak, 1998; Riloff and Jones, 1999). In named-entity classification a large set of named entities (proper nouns) are classified using a comprehensive set of semantic labels such as “organization”, “person”</context>
</contexts>
<marker>Zernik, 1991</marker>
<rawString>U. Zernik. 1991. Introduction. In U. Zernik, editor, Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon. Lawrence Erlbaum Associates.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>