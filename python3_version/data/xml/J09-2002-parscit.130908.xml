<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998944">
Exploiting Semantic Role Resources
for Preposition Disambiguation
</title>
<author confidence="0.999017">
Tom O’Hara*
</author>
<affiliation confidence="0.999611">
University of Maryland, Baltimore County
</affiliation>
<author confidence="0.991218">
Janyce Wiebe**
</author>
<affiliation confidence="0.990841">
University of Pittsburgh
</affiliation>
<bodyText confidence="0.969891647058823">
This article describes how semantic role resources can be exploited for preposition disambigua-
tion. The main resources include the semantic role annotations provided by the Penn Treebank
and FrameNet tagged corpora. The resources also include the assertions contained in the Fac-
totum knowledge base, as well as information from Cyc and Conceptual Graphs. A common
inventory is derived from these in support of definition analysis, which is the motivation for this
work.
The disambiguation concentrates on relations indicated by prepositional phrases, and is
framed as word-sense disambiguation for the preposition in question. A new type offeature for
word-sense disambiguation is introduced, using WordNet hypernyms as collocations rather than
just words. Various experiments over the Penn Treebank and FrameNet data are presented, in-
cluding prepositions classified separately versus together, and illustrating the effects offiltering.
Similar experimentation is done over the Factotum data, including a method for inferring likely
preposition usage from corpora, as knowledge bases do not generally indicate how relationships
are expressed in English (in contrast to the explicit annotations on this in the Penn Treebank and
FrameNet). Other experiments are included with the FrameNet data mapped into the common
relation inventory developed for definition analysis, illustrating how preposition disambiguation
might be applied in lexical acquisition.
</bodyText>
<sectionHeader confidence="0.997203" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999601">
English prepositions convey important relations in text. When used as verbal adjuncts,
they are the principal means of conveying semantic roles for the supporting entities
described by the predicate. Preposition disambiguation is a challenging problem. First,
prepositions are highly polysemous. A typical collegiate dictionary has dozens of
senses for each of the common prepositions. Second, the senses of prepositions tend
to be closely related to one another. For instance, there are three duplicate role assign-
ments among the twenty senses for of in The Preposition Project (Litkowski and
Hargraves 2006), a resource containing semantic annotations for common prepositions.
</bodyText>
<footnote confidence="0.393002">
* Institute for Language and Information Technologies, Baltimore, MD 21250. E-mail:
tomohara@umbc.edu.
** Department of Computer Science, Pittsburgh, PA 15260. E-mail: wiebe@cs.pitt.edu.
</footnote>
<note confidence="0.871338333333333">
Submission received: 7 August 2006; accepted for publication: 21 February 2007.
© 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 2
</note>
<bodyText confidence="0.800489">
Consider the disambiguation of the usages of on in the following sentences:
</bodyText>
<listItem confidence="0.732005">
(1) The cut should be blocked on procedural grounds.
(2) The industry already operates on very thin margins.
</listItem>
<bodyText confidence="0.999772545454546">
The choice between the purpose and manner meanings for on in these sentences is
difficult. The purpose meaning seems preferred for sentence 1, as grounds is a type of
justification. For sentence 2, the choice is even less clear, though the manner meaning
seems preferred.
This article presents a new method for disambiguating prepositions using infor-
mation learned from annotated corpora as well as knowledge stored in declarative
lexical resources. The approach allows for better coverage and finer distinctions than
in previous work in preposition disambiguation. For instance, a traditional approach
would involve manually developing rules for on that specify the semantic type of objects
associated with the different senses (e.g., time for temporal). Instead, we infer this based
on lexical associations learned from annotated corpora.
The motivation for preposition disambiguation is to support a system for lexical
acquisition (O’Hara 2005). The focus of the system is to acquire distinguishing infor-
mation for the concepts serving to define words. Large-scale semantic lexicons mainly
emphasize the taxonomic relations among the underlying concepts (e.g., is-a and part-
of), and often lack sufficient differentiation among similar concepts (e.g., via attributes
or functional relations such as is-used-for). For example, in WordNet (Miller et al. 1990),
the standard lexical resource for natural language processing, the only relations for
beagle and Afghan are that they are both a type of hound. Although the size difference can
be inferred from the definitions, it is not represented in the WordNet semantic network.
In WordNet, words are grouped into synonym sets called synsets, which represent
the underlying concepts and serve as nodes in a semantic network. Synsets are ordered
into a hierarchy using the hypernym relation (i.e., is-a). There are several other semantic
relations, such as part-whole, is-similar-to, and domain-of. Nonetheless, in version 2.1 of
WordNet, about 30% of the synsets for noun entries are not explicitly distinguished from
sibling synsets via semantic relations.
To address such coverage problems in lexicons, we have developed an empirical
approach to lexical acquisition, building upon earlier knowledge-based approaches in
dictionary definition analysis (Wilks, Slator, and Guthrie 1996). This involves a two-step
process: Definitions are first analyzed with a broad-coverage parser, and then the result-
ing syntactic relationships are disambiguated using statistical classification. A crucial
part of this process is the disambiguation of prepositions, exploiting online resources
with semantic role usage information. The main resources are the Penn Treebank
(PTB; Marcus et al. 1994) and FrameNet (Fillmore, Wooters, and Baker 2001), two
popular corpora providing rich annotations on English text, such as the semantic roles
associated with prepositional phrases in context. In addition to the semantic role annota-
tions from PTB and FrameNet, traditional knowledge bases (KBs) are utilized to provide
training data for the relation classification. In particular, the Factotum KB (Cassidy 2000)
is used to provide additional training data for prepositions that are used to convey
particular relationships. Information on preposition usage is not explicitly encoded in
Factotum, so a new corpus analysis technique is employed to infer the associations.
Details on the lexical acquisition process, including application and evaluation, can
be found in O’Hara (2005). This article focuses on the aspects of this method relevant
to the processing of prepositions. In particular, here we specifically address preposition
</bodyText>
<page confidence="0.988727">
152
</page>
<note confidence="0.743404">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<bodyText confidence="0.999706125">
disambiguation using semantic role annotations from PTB, FrameNet, and Factotum.
In each case, classification experiments are presented using the respective resources as
training data with evaluation via 10-fold cross validation.
This article is organized as follows. Section 2 presents background information on
the relation inventories used during classification, including one developed specifically
for definition analysis. Section 3 discusses the relation classifiers in depth with results
given for four different inventories. Section 4 discusses related work in relation disam-
biguation, and Section 5 presents our conclusions.
</bodyText>
<sectionHeader confidence="0.964241" genericHeader="keywords">
2. Semantic Relation Inventories
</sectionHeader>
<bodyText confidence="0.999963153846154">
The representation of natural language utterances often incorporates the notion of
semantic roles, which are analogous to the slots in a frame-based representation. In
particular, there is an emphasis on the analysis of thematic roles, which serve to tie
the grammatical constituents of a sentence to the underlying semantic representation.
Thematic roles are also called case roles, because in some languages the grammatical
constituents are indicated by case inflections (e.g., ablative in Latin). As used here, the
term “semantic role” refers to an arbitrary semantic relation, and the term “thematic
role” refers to a relation intended to capture the semantics of sentences (e.g., event
participation).
Which semantic roles are used varies widely in Natural Language Processing
(NLP). Some systems use just a small number of very general roles, such as beneficiary.
At the other extreme, some systems use quite specific roles tailored to a particular
domain, such as catalyst in the chemical sense.
</bodyText>
<subsectionHeader confidence="0.991633">
2.1 Background on Semantic Roles
</subsectionHeader>
<bodyText confidence="0.999952260869565">
Bruce (1975) presents an account of early case systems in NLP. For the most part,
those systems had limited case role inventories, along the lines of the cases defined by
Fillmore (1968). Palmer (1990) discusses some of the more contentious issues regarding
case systems, including adequacy for representation, such as reliance solely upon case
information to determine semantics versus the use of additional inference mechanisms.
Barker (1998) provides a comprehensive summary of case inventories in NLP, along
with criteria for the qualitative evaluation of case systems (generality, completeness, and
uniqueness). Linguistic work on thematic roles tends to use a limited number of roles.
Frawley (1992) presents a detailed discussion of twelve thematic roles and discusses
how they are realized in different languages.
During the shift in emphasis away from systems that work in small, self-contained
domains to those that can handle open-ended domains, there has been a trend towards
the use of larger sets of semantic primitives (Wilks, Slator, and Guthrie 1996). The
WordNet lexicon (Miller et al. 1990) serves as one example of this. A synset is defined
in terms of its relations with any of the other 100,000+ synsets, rather than in terms of a
set of features like [±ANIMATE]. There has also been a shift in focus from deep under-
standing (e.g., story comprehension) facilitated by specially constructed KBs to shallow
surface-level analysis (e.g., text extraction) facilitated by corpus analysis. Both trends
seem to be behind the increase in case inventories in two relatively recent resources,
namely FrameNet (Fillmore, Wooters, and Baker 2001) and OpenCyc (OpenCyc 2002),
both of which define well over a hundred case roles. However, provided that the case
roles are well structured in an inheritance hierarchy, both paraphrasability and coverage
can be addressed by the same inventory.
</bodyText>
<page confidence="0.99773">
153
</page>
<note confidence="0.781121">
Computational Linguistics Volume 35, Number 2
</note>
<subsectionHeader confidence="0.871017">
2.2 Inventories Developed for Corpus Annotation
</subsectionHeader>
<bodyText confidence="0.971381862068965">
With the emphasis on corpus analysis in computational linguistics, there has been a
shift away from relying on explicitly-coded knowledge towards the use of knowledge
inferred from naturally occurring text, in particular text that has been annotated by
humans to indicate phenomena of interest. For example, rather than manually devel-
oping rules for preferring one sense of a word over another based on context, the
most successful approaches have automatically learned the rules based on word-sense
annotations, as evidenced by the Senseval competitions (Kilgarriff 1998; Edmonds and
Cotton 2001).
The Penn Treebank version II (Marcus et al. 1994) provided the first large-scale set
of case annotations for general-purpose text. These are very general roles, following
Fillmore (1968). The Berkeley FrameNet (Fillmore, Wooters, and Baker 2001) project
currently provides the most comprehensive set of semantic roles annotations. These are
at a much finer granularity than those in PTB, making them quite useful for applications
learning semantics from corpora. Relation disambiguation experiments for both of these
role inventories are presented subsequently.
2.2.1 Penn Treebank. The original PTB (Marcus, Santorini, and Marcinkiewicz 1993) pro-
vided syntactic annotations in the form of parse trees for text from the Wall Street Journal.
This resource is very popular in computational linguistics, particularly for inducing
part-of-speech taggers and parsers. PTB version II (Marcus et al. 1994) added 20 func-
tional tags, including a few thematic roles such as temporal, direction, and purpose. These
can be attached to any verb complement but normally occur with clauses, adverbs, and
prepositions.
For example, Figure 1 shows a parse tree using the extended annotation format.
In addition to the usual syntactic constituents such as NP and VP, function tags are
included. For example, the second NP gives the subject. This also shows that the first
prepositional phrase (PP) indicates the time frame, whereas the last PP indicates the
Sentence:
In 1982, Sports &amp; Recreation’s managers and certain passive investors purchased the
company from Brunswick Corp. of Skokie, Ill.
</bodyText>
<equation confidence="0.9091018">
Parse:
(S (PP-TMP In (NP 1982)), temporal extent
(NP-SBJ grammatical subject
(NP (NP (NP Sports) &amp; (NP Recreation) ’s)
managers)
</equation>
<bodyText confidence="0.825171375">
and (NP certain passive investors))
(VP purchased
(NP the company)
(PP-CLR from closely related
(NP (NP Brunswick Corp.)
(PP-LOC of locative
(NP (NP Skokie) , (NP Ill)))
))) .)
</bodyText>
<figureCaption confidence="0.825944">
Figure 1
</figureCaption>
<bodyText confidence="0.313148">
Penn Treebank II parse tree annotation sample. The functional tags are shown in boldface.
</bodyText>
<page confidence="0.999115">
154
</page>
<note confidence="0.932959">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<tableCaption confidence="0.993387">
Table 1
</tableCaption>
<bodyText confidence="0.7618104">
Frequency of Penn Treebank II semantic role annotations. Relative frequencies estimated over the
counts for unique assignments given in the PTB documentation (bkt tags.lst), and descriptions
based on Bies et al. (1995). Omits low-frequency benefactive role. The syntactic role annotations
generally have higher frequencies; for example, the subject role occurs 49% of the time (out of
about 240,000 total annotations).
</bodyText>
<table confidence="0.90031475">
Role Freq.
temporal .113
locative .075
direction .026
manner .021
purpose .017
extent .010
Description
</table>
<bodyText confidence="0.960240476190476">
indicates when, how often, or how long
place/setting of the event
starting or ending location (trajectory)
indicates manner, including instrument
purpose or reason
spatial extent
location. The second PP is tagged as closely-related, which is one of the miscellaneous
PTB function tags that are more syntactic in nature: “[CLR] occupy some middle ground
between arguments and adjunct” (Bies et al. 1995). Frequency information for the
semantic role annotations is shown in Table 1.
2.2.2 FrameNet. FrameNet (Fillmore, Wooters, and Baker 2001) is striving to develop an
English lexicon with rich case structure information for the various contexts that words
can occur in. Each of these contexts is called a frame, and the semantic relations that
occur in each frame are called frame elements (FE). For example, in the communica-
tion frame, there are frame elements for communicator, message, medium, and so forth.
FrameNet annotations occur at the phrase level instead of the grammatical constituent
level as in PTB. Figure 2 shows an example.
Table 2 displays the top 25 semantic roles by frequency of annotation. This shows
that the semantic roles in FrameNet can be quite specific, as with the roles cognizer,
evaluee, and addressee. In all, there are over 780 roles annotated with over 288,000 tagged
instances.
</bodyText>
<figure confidence="0.609234375">
Sentence:
Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling stand-
alone workstations to communicate over public or private ISDN networks.
Annotation:
Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling
(C FE=“Communicator” PT=“NP”)standalone workstations(/C)
to (C TARGET=“y”)communicate(/C)
(C FE=“Medium” PT=“PP”)over public or private ISDN networks(/C) .
</figure>
<figureCaption confidence="0.937139">
Figure 2
</figureCaption>
<bodyText confidence="0.3632318">
FrameNet annotation sample. The constituent (C) tags identify the phrases that have been
annotated. The frame element (FE) attributes indicate the semantic roles, and the phrase type
(PT) attributes indicate the traditional grammatical category for the phrase. For simplicity, this
example is formatted in the earlier FrameNet format, but the information is taken from the
latest annotations (lu5.xml).
</bodyText>
<page confidence="0.992982">
155
</page>
<table confidence="0.44558">
Computational Linguistics Volume 35, Number 2
</table>
<tableCaption confidence="0.994562">
Table 2
</tableCaption>
<table confidence="0.9106408">
Common FrameNet semantic roles. The top 25 of 773 roles are shown, representing nearly half of
the total annotations (about 290,000). Descriptions based on FrameNet 1.3 frame documentation.
Role Freq.
agent .037
theme .031
experiencer .029
goal .028
speaker .028
stimulus .026
manner .025
degree .024
self-mover .023
message .021
path .020
cognizer .018
source .017
time .016
evaluee .016
descriptor .015
body-part .014
content .014
topic .014
item .012
target .011
garment .011
addressee .011
protagonist .011
communicator .010
2.3 Other
Description
</table>
<bodyText confidence="0.982500295454545">
person performing the intentional act
object being acted on, affected, etc.
being who has a physical experience, etc.
endpoint of the path
individual that communicates the message
entity that evokes response
manner of performing an action, etc.
degree to which event occurs
volitional agent that moves
the content that is communicated
the trajectory of motion, etc.
person who perceives the event
the beginning of the path
the time at which the situation occurs
thing about which a judgment has been made
attributes, traits, etc. of the entity
location on the body of the experiencer
situation or state-of-affairs that attention is focused on
subject matter of the communicated message, etc.
entity whose scalar property is specified
entity which is hit by a projectile
clothing worn
entity that receives a message from the communicator
person to whom a mental property is attributed
the person who communicates a message
A recent semantic role resource that is starting to attract interest is the Proposition Bank
(PropBank), developed at the University of Pennsylvania (Palmer, Gildea, and Kings-
bury 2005). It extends the Penn Treebank with information on verb subcategorization.
The focus is on annotating all verb occurrences and all their argument realizations that
occur in the Wall Street Journal, rather than select corpus examples as in FrameNet.
Therefore, the role inventory is heavily verb-centric, for example, with the generic labels
arg0 through arg4 denoting the main verbal arguments to avoid misinterpretations.
Verbal adjuncts are assigned roles based on PTB version II (e.g., argM-LOC and argM-
TMP). PropBank has been used as the training data in recent semantic role labeling
competitions as part of the Conferences on Computational Natural Language Learn-
ing (Carreras and M`arquez 2004, 2005). Thus, it is likely to become as influential as
FrameNet in computational semantics.
The Preposition Project similarly adds information to an existing semantic role
resource, namely FrameNet. It is being developed by CL Research (Litkowski and
Hargraves 2006) and endeavors to provide comprehensive syntactic and semantic in-
formation on various usages of prepositions, which often are not represented well
in semantic lexicons (e.g., they are not included at all in WordNet). The Preposition
Project uses the sense distinctions from the Oxford Dictionary of English and integrates
syntactic information about prepositions from comprehensive grammar references.
</bodyText>
<page confidence="0.993176">
156
</page>
<bodyText confidence="0.375667">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</bodyText>
<subsectionHeader confidence="0.67216">
2.4 Inventories for Knowledge Representation
</subsectionHeader>
<bodyText confidence="0.998438807692308">
This section describes three case inventories: one developed for the Cyc KB (Lenat
1995), one used to define Conceptual Graphs (Sowa 1984), and one for the Factotum
KB (Cassidy 2000). The first two are based on a traditional knowledge representation
paradigm. With respect to natural language processing, these approaches are more
representative of the earlier approaches in which deep understanding is the chief goal.
Factotum is also based on a knowledge representation paradigm, but in a sense also
reflects the empirical aspect of the corpus annotation approach, because the annotations
were developed to address the relations implicit in Roget’s Thesaurus.
In this article, relation disambiguation experiments are only presented for Facto-
tum, given that the others do not readily provide sufficient training data. However, the
other inventories are discussed because each provides relation types incorporated into
the inventory used below for the definition analysis (see Section 3.5).
2.4.1 Cyc. The Cyc system (Lenat 1995) is the most ambitious knowledge representation
project undertaken to date, in development since 1984. The full Cyc KB is propri-
etary, which has hindered its adoption in natural language processing. However, to
encourage broader usage, portions of the KB have been made freely available to the
public. For instance, there is an open-source version of the system called OpenCyc
(www.opencyc.org), which covers the upper part of the KB and also includes the Cyc
inference engine, KB browser, and other tools. In addition, researchers can obtain access
to ResearchCyc, which contains most of the KB except for proprietary information (e.g.,
internal bookkeeping assertions).
Cyc uses a wide range of role types: very general roles (e.g., beneficiary); commonly
occurring situational roles (e.g., victim); and highly specialized roles (e.g., catalyst). Of
the 8,756 concepts in OpenCyc, 130 are for event-based roles (i.e., instances of actor-
slot) with 51 other semantic roles (i.e., other instances of role). Table 3 shows the most
commonly used event-based roles in the KB.
</bodyText>
<subsubsectionHeader confidence="0.712706">
2.4.2 Conceptual Graphs. The Conceptual Graphs (CG) mechanism was introduced by
</subsubsectionHeader>
<bodyText confidence="0.997738642857143">
Sowa (1984) for knowledge representation as part of his Conceptual Structures theory.
The original text listed two dozen or so thematic relations, such as destination and
initiator. In all, 37 conceptual relations were defined. This inventory formed the basis
for most work in Conceptual Graphs. Recently, Sowa (1999) updated the inventory to
allow for better hierarchical structuring and to incorporate the important thematic roles
identified by Somers (1987). Table 4 shows a sample of these roles, along with usage
estimates based on corpus analysis (O’Hara 2005).
2.4.3 Factotum. The Factotum semantic network (Cassidy 2000) developed by Micra,
Inc., makes explicit many of the relations in Roget’s Thesaurus.1 Outside of proprietary
resources such as Cyc, Factotum is the most comprehensive KB with respect to functional
relations, which are taken here to be non-hierarchical relations, excluding attributes.
OpenCyc does include definitions of many non-hierarchical relations. However, there
are not many instantiations (i.e., relationship assertions), because it concentrates on the
higher level of the ontology.
</bodyText>
<footnote confidence="0.9995365">
1 Factotum is based on the public domain version of Roget’s Thesaurus. The latter is freely available via
Project Gutenberg (http://promo.net/pg), thanks to Micra, Inc.
</footnote>
<page confidence="0.987951">
157
</page>
<table confidence="0.444654">
Computational Linguistics Volume 35, Number 2
</table>
<tableCaption confidence="0.99511">
Table 3
</tableCaption>
<table confidence="0.829532538461538">
Most common event-based roles in OpenCyc. Descriptions based on comments from the
OpenCyc knowledge base (version 0.7). Relative frequencies based on counts obtained
via Cyc’s utility functions.
Role Freq. Description
done-by .178 relates an event to its “doer”
performed-by .119 doer deliberately does act
object-of-state-change .081 object undergoes some kind of intrinsic change of state
object-acted-on .057 object is altered or affected in event
outputs-created .051 object comes into existence sometime during event
transporter .044 object facilitating conveyance of transportees
transportees .044 object being moved
to-location .041 where the moving object is found when event ends
object-removed .036 object removed from its previous location
</table>
<bodyText confidence="0.878258361111111">
inputs .036 pre-existing event participant destroyed or incorporated
into a new entity
products .035 object is one of the intended outputs of event
inputs-destroyed .035 object exists before event and is destroyed during event
from-location .034 where some moving-object in the move is found at the
beginning
primary-object-moving .033 object is in motion at some point during the event, and
this movement is focal
seller .030 agent sells something in the exchange
object-of-possession-transfer .030 rights to use object transferred from one agent to another
transferred-thing .030 object is being moved, transferred, or exchanged in the
event transfer
sender-of-info .030 sender is an agent who is the source of information
transferred
inputs-committed .028 object exists before event and continues to exist
afterwards, and as a result of event, object becomes
incorporated into something created during event
object-emitted .026 object is emitted from the emitter during the emission
event
The Factotum knowledge base is based on the 1911 version of Roget’s Thesaurus
and specifies the relations that hold between the Roget categories and the words listed
in each entry. Factotum incorporates information from other resources as well. For
instance, the Unified Medical Language System (UMLS) formed the basis for the initial
inventory of semantic relations, which was later revised during tagging.
Figure 3 shows a sample from Factotum. This illustrates that the basic Roget or-
ganization is still used, although additional hierarchical levels have been added. The
relations are contained within double braces (e.g., “{{has subtype}}”) and generally
apply from the category to each word in the synonym list on the same line. For example,
the line with “{{result of}}” indicates that conversion is the result of transforming,
as shown in the semantic relation listing that would be extracted. There are over 400
different relations instantiated in the knowledge base, which has over 93,000 assertions.
Some of these are quite specialized (e.g., has-brandname). In addition, there are quite a
few inverse relations, because most of the relations are not symmetrical. Certain features
of the knowledge representation are ignored during the relation extraction used later.
For example, relation specifications can have qualifier prefixes, such as an ampersand
to indicate that the relationship only sometimes holds.
</bodyText>
<page confidence="0.997007">
158
</page>
<note confidence="0.654902">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<tableCaption confidence="0.990994">
Table 4
</tableCaption>
<bodyText confidence="0.66166575">
Common semantic roles used in Conceptual Graphs. Inventory and descriptions based on
Sowa (1999, pages 502–510). The term situation is used in place of Sowa’s nexus (i.e., “fact of
togetherness”), which also covers spatial structures. Freq. gives estimated relative frequencies
from O’Hara (2005).
</bodyText>
<construct confidence="0.347822766666667">
Role Freq.
agent .267
attribute .155
characteristic .080
theme .064
patient .061
location .053
possession .035
part .035
origin .035
experiencer .035
result .032
instrument .027
recipient .019
destination .013
point-in-time .011
path .011
accompaniment .011
effector .008
beneficiary .008
matter .005
manner .005
source .003
resource .003
product .003
medium .003
goal .003
duration .003
because .003
amount .003
</construct>
<subsectionHeader confidence="0.426996">
Description
</subsectionHeader>
<bodyText confidence="0.968982906976744">
entity voluntarily initiating an action
entity that is a property of some object
types of properties of entities
participant involved with but not changed
participant undergoing structural change
participant of a spatial situation
entity owned by some animate being
object that is a component of some object
source of a spatial or ambient situation
animate goal of an experience
inanimate goal of an act
resource used but not changed
animate goal of an act
goal of a spatial process
participant of a temporal situation
resource of a spatial or ambient situation
object participating with another
source involuntarily initiating an action
entity benefiting from event completion
resource that is changed by the event
entity that is a property of some process
present at beginning of activity
material necessary for situation
present at end of activity
resource for transmitting information
final cause which is purpose or benefit
resource of a temporal process
situation causing another situation
a measure of some characteristic
Table 5 shows the most common relations in terms of usage in the semantic network,
and includes others that are used in the experiments discussed later.2 The relative
frequencies just reflect relationships explicitly labeled in the KB data file. For instance,
this does not account for implicit has-subtype relationships based on the hierarchical
organization of the thesaural groups (e.g., (simple-change, has-subtype, conversion)).
The functional relations are shown in boldface. This excludes the meronym or part-
whole relations (e.g., is-conceptual-part-of), in line with their classification by Cruse
(1986) as hierarchical relations. The reason for concentrating on the functional relations
is that these are more akin to the roles tagged in PTB and FrameNet.
The information in Factotum complements WordNet through the inclusion of more
functional relations (e.g., non-hierarchical relations such as uses and is-function-of). For
comparison purposes, Table 6 shows the semantic relation usage in WordNet version
2 The database files and documentation for the semantic network are available from Micra, Inc., via
ftp://micra.com/factotum.
</bodyText>
<page confidence="0.973033">
159
</page>
<figure confidence="0.994869363636364">
Computational Linguistics Volume 35, Number 2
Original data:
A. ABSTRACT RELATION
...
A6 CHANGE (R140 TO R152)
...
A6.1 SIMPLE CHANGE (R140)
...
A6.1.4 CONVERSION (R144)
#144. Conversion.
N. {{has subtype(change, R140)}} conversion, transformation.
</figure>
<figureCaption confidence="0.6009115">
{{has case: @R7, initial state, final state}}.
{{has patient: @R3a, object, entity}}.
</figureCaption>
<bodyText confidence="0.611824333333333">
{{result of}} {{has subtype(process, A7.7)}} converting, transforming.
{{has subtype}} processing.
transition.
</bodyText>
<equation confidence="0.484464571428571">
Extracted relationships:
(change, has-subtype, conversion) (change, has-subtype, transformation)
(conversion, has-case, initial state) (conversion, has-case, final state)
(conversion, has-patient, object) (conversion, has-patient, entity)
(conversion, is-result-of, converting) (conversion, is-result-of, transforming)
(process, has-subtype, converting) (process, has-subtype, transforming)
(conversion, has-subtype, processing)
</equation>
<figureCaption confidence="0.450822">
Figure 3
</figureCaption>
<bodyText confidence="0.93632025">
Sample data from Factotum. Based on version 0.56 of Factotum.
2.1. As can be seen from the table, the majority of the relations are hierarchical.3
WordNet 2.1 averages just about 1.1 non-taxonomic properties per concept (includ-
ing inverses but excluding hierarchical relations such as has-hypernym and is-member-
meronym-of). OpenCyc provides a much higher average at 3.7 properties per concept,
although with an emphasis on argument constraints and other usage restrictions. Fac-
totum averages 1.8 properties per concept, thus complementing WordNet in terms of
information content.4
</bodyText>
<subsectionHeader confidence="0.996854">
2.5 Combining the Different Semantic Role Inventories
</subsectionHeader>
<bodyText confidence="0.8406461">
It is difficult to provide precise comparisons of the five inventories just discussed. This is
due both to the different nature of the inventories (e.g., developed for knowledge bases
as opposed to being derived from natural language annotations) and due to the way the
3 In WordNet, the is-similar-to relation for adjectives can be considered as hierarchical, as it links satellite
synsets to heads of adjective clusters (Miller 1998). For example, the satellite synsets for “thirsty” and
“rainless” are both linked to the head synset for “dry (vs. wet).”
4 These figures are derived by counting the number of relations excluding the instance and subset ones
and then dividing by the number of concepts (i.e., ratio of non-hierarchical relations to concepts). Cyc’s
comments and lexical assertions are also excluded, as these are implicit in Factotum and WordNet.
WordNet’s is-derived-from relations are omitted as lexical in nature (the figure otherwise would be 1.6).
</bodyText>
<page confidence="0.980402">
160
</page>
<note confidence="0.659168">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<tableCaption confidence="0.976691">
Table 5
</tableCaption>
<bodyText confidence="0.967918980769231">
Common Factotum semantic roles. These account for 80% of the instances. Boldface relations are
used in the experiments (Section 3.4.2).
Relation Freq. Description
has-subtype .401 inverse of is-a relation
is-property-of .077 object with given salient character
is-caused-by .034 force that is the origin of something
has-property .028 salient property of an object
has-part .022 a part of a physical object
has-high-intensity .018 intensifier for property or characteristic
has-high-level .017 implication of activity (e.g., intelligence)
is-antonym-of .016 generally used for lexical opposition
is-conceptual-part-of .015 parts of other entities (e.g., case relations)
has-metaphor .014 non-literal reference to the word
causesmental .013 motivation (causation in the mental realm)
uses .012 a tool needing active manipulation
is-performed-by .012 human actor for the event
performshuman .011 human role in performing some activity
is-function-of .011 artifact passively performing the function
has-result .010 more specific type of causes
has-conceptual-part .010 generalization of has-part
is-used-in .010 activity or desired effect for the entity
is-part-of .010 distinguishes part from group membership
causes .009 inverse of is-caused-by
has-method .009 method used to achieve some goal
is-caused-bymental .009 inverse of causesmental
has-consequence .008 causation due to a natural association
has-commencement .007 state that commences with the action
is-location-of .007 absolute location of an object
requires .004 object or sub-action needed for an action
is-studied-in .004 inquires into any field of study
is-topic-of .002 communication dealing with given subject
produces .002 what an action yields, generates, etc.
is-measured-by .002 instrument for measuring something
is-job-of .001 occupation title for a job function
is-patient-of .001 action that the object participates in
is-facilitated-by .001 object or sub-action aiding an action
is-biofunction-of .0003 biological function of parts of living things
was-performed-by .0002 is-performed-by occurring in the past
has-consequenceobject .0002 consequence for the patient of an action
is-facilitated-bymental .0001 trait that facilitates some human action
relation listings were extracted (e.g., just including event-based roles from OpenCyc).
As can be seen from Tables 2 and 3, FrameNet tends to refine the roles for agents (e.g.,
communicator) compared to OpenCyc, which in contrast has more refinements of the
object role (e.g., object-removed). The Concept Graphs inventory includes more emphasis
on specialization relations than the others, as can be seen from the top entries in Table 4
(e.g., attribute).
In the next section, we show how classifiers can be automatically developed for
the semantic role inventories just discussed. For the application to dictionary defin-
ition analysis, we need to combine the classifiers learned over PTB, FrameNet, and
Factotum. This can be done readily in a cascaded fashion with the classifier for the
most specific relation inventory (i.e., FrameNet) being used first and then the other
classifiers being applied in turn whenever the classification is inconclusive. This would
</bodyText>
<page confidence="0.990659">
161
</page>
<table confidence="0.429028">
Computational Linguistics Volume 35, Number 2
</table>
<tableCaption confidence="0.992257">
Table 6
</tableCaption>
<table confidence="0.5124307">
Semantic relation usage in WordNet. Relative frequencies for semantic relations in WordNet
(173,570 total instances). This table omits lexical relations, such as the is-derived-from relation
(71,914 instances). Frequencies based on analysis of database files for WordNet 2.1.
Relation Freq. Description
has-hypernym .558 superset relation
is-similar-to .130 similar adjective synset
is-member-meronym-of .071 constituent member
is-part-meronym-of .051 constituent part
is-pertainym-of .046 noun that adjective pertains to
is-antonym-of .046 opposing concept
has-topic-domain .038 topic domain for the synset
also-see .019 related entry (for adjectives and verbs)
has-verb-group .010 verb senses grouped by similarity
has-region-domain .008 region domain for the synset
has-attribute .007 related attribute category or value
has-usage-domain .007 usage domain for the synset
is-substance-meronym-of .004 constituent substance
entails .002 action entailed by the verb
causes .001 action caused by the verb
has-participle .001 verb participle
</table>
<bodyText confidence="0.99996225">
have the advantage that new resources could be integrated into the combined relation
classifier with minimal effort. However, the resulting role inventory would likely be
heterogeneous and might be prone to inconsistent classifications. In addition, the role
inventory could change whenever new annotation resources are incorporated, making
the overall definition analysis system somewhat unpredictable.
Alternatively, the annotations can be converted into a common inventory, and a
separate relation classifier induced over the resulting data. This has the advantage
that the target relation-type inventory remains stable whenever new sources of relation
annotations are introduced. In addition, the classifier will likely be more accurate as
there are more examples per relation type on average. The drawback, however, is that
annotations from new resources must first be mapped into the common inventory
before incorporation.
The latter approach is employed here. The common inventory incorporates some of
the general relation types defined by Gildea and Jurafsky (2002) for their experiments
in classifying semantic relations in FrameNet using a reduced relation inventory. They
defined 18 relations (including a special-case null role for expletives), as shown in
Table 7. These roles served as the starting point for the common relation inventory
we developed to support definition analysis (O’Hara 2005), with half of the roles used
as is and a few others mapped into similar roles. In total, twenty-six relations are
defined, including a few roles based on the PTB, Cyc, and Conceptual Graphs inven-
</bodyText>
<tableCaption confidence="0.796711">
Table 7
</tableCaption>
<bodyText confidence="0.9530962">
Abstract roles defined by Gildea and Jurafsky based on FrameNet. Taken from Gildea and
Jurafsky (2002).
agent cause degree experiencer force goal
instrument location manner null path patient
percept proposition result source state topic
</bodyText>
<page confidence="0.927686">
162
</page>
<note confidence="0.632885">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<tableCaption confidence="0.913151470588235">
Table 8
Inventory of semantic relations for definition analysis. This inventory is inspired by the roles in
Table 7 and is primarily based on FrameNet (Fillmore, Wooters, and Baker 2001) and Conceptual
Graphs (Sowa 1999); it also includes roles based on the PTB and Cyc inventories.
Relation Description
accompaniment entity that participates with another entity
agent entity voluntarily performing an action
amount quantity used as a measure of some characteristic
area region in which the action takes place
category general type or class of which the item is an instance
cause non-agentive entity that produces an effect
characteristic general properties of entities
context background for situation or predication
direction either spatial source or goal (same as in PTB)
distance spatial extent of motion
duration period of time that the situation applies within
experiencer entity undergoing some (non-voluntary) experience
</tableCaption>
<bodyText confidence="0.959503838709678">
goal location that an affected entity ends up in
instrument entity or resource facilitating event occurrence
location reference spatial location for situation
manner property of the underlying process
means action taken to affect something
medium setting in which an affected entity is conveyed
part component of entity or situation
path trajectory which is neither a source nor a goal
product entity present at end of event (same as Cyc products)
recipient recipient of the resource(s)
resource entity utilized during event (same as Cyc inputs)
source initial position of an affected entity
theme entity somehow affected by the event
time reference time for situation
tories. Table 8 shows this role inventory along with a description of each case. In
addition to traditional thematic relations, this includes a few specialization relations,
which are relevant to definition analysis. For example, characteristic corresponds to the
general relation from Conceptual Graphs for properties of entities; and category gen-
eralizes the corresponding FrameNet role, which indicates category type, to subsume
other FrameNet roles related to categorization (e.g., topic). Note that this inventory is
not meant to be definitive and has been developed primarily to address mappings from
FrameNet for the experiments discussed in Section 3.5. Thus, it is likely that additional
roles will be required when additional sources of semantic relations are incorporated
(e.g., Cyc). The mappings were produced manually by reviewing the role descriptions in
the FrameNet documentation and checking prepositional usages for each to determine
which of the common inventory roles might be most relevant. As some of the roles with
the same name have frame-specific meanings, in a few cases this involved conflicting
usages (e.g., body-part associated with both area and instrument), which were resolved in
favor of the more common usage.5
5 See www.cs.nmsu.edu/~tomohara/cl-prep-article/relation-mapping.html for the mapping,
covering cases occurring at least 50 times in FrameNet.
</bodyText>
<page confidence="0.991506">
163
</page>
<note confidence="0.303977">
Computational Linguistics Volume 35, Number 2
</note>
<sectionHeader confidence="0.635651" genericHeader="introduction">
3. Preposition Disambiguation
</sectionHeader>
<bodyText confidence="0.998508777777778">
This section presents the results of our experiments on the disambiguation of relations
indicated by prepositional phrases. Results are given for PTB, FrameNet, and Factotum.
The PTB roles are general: For example, for the preposition for, there are six distinctions
(four, with low-frequency pruning). The PTB role disambiguation experiments thus
address a coarse form of sense distinction. In contrast, the FrameNet distinctions are
quite specific: there are 192 distinctions associated with for (21 with low-frequency prun-
ing); and, there are 17 distinctions in Factotum (15 with low-frequency pruning). Our
FrameNet and Factotum role disambiguation experiments thus address fine-grained
sense distinctions.
</bodyText>
<subsectionHeader confidence="0.961277">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.997437">
A straightforward approach for preposition disambiguation would be to use typical
word-sense disambiguation features, such as the parts-of-speech of surrounding words
and, more importantly, collocations (e.g., lexical associations). Although this can be
highly accurate, it tends to overfit the data and to generalize poorly. The latter is of
particular concern here as the training data is taken from a different genre than the
application data. For example, the PTB data is from newspaper text (specifically, Wall
Street Journal), but the lexical acquisition is based on dictionary definitions. We first
discuss how class-based collocations address this problem and then present the features
used in the experiments.
Before getting into technical details, an informal example will be used to motivate
the use of hypernym collocations. Consider the following purpose role examples, which
are similar to the first example from the introduction.
</bodyText>
<listItem confidence="0.998953333333333">
(3) This contention would justify dismissal of these actions onpurpose
prudential grounds.
(4) Ramada’s stock rose 87.5 cents onpurpose the news.
</listItem>
<bodyText confidence="0.9999275">
It turns out that grounds and news are often used as the prepositional object in PTB
when the sense for on is purpose (or reason). Thus, these words would likely be chosen as
collocations for this sense. However, for the sake of generalization, it would be better to
choose the WordNet hypernym subject matter, as that subsumes both words. This would
then allow the following sentence to be recognized as indicating purpose even though
censure was not contained in the training data.
</bodyText>
<listItem confidence="0.947873">
(5) Senator sets hearing onpurpose censure of Bush.
</listItem>
<subsubsectionHeader confidence="0.432042">
3.1.1 Class-Based Collocations via Hypernyms. To overcome data sparseness problems, a
</subsubsectionHeader>
<bodyText confidence="0.991047">
class-based approach is used for the collocations, with WordNet synsets as the source
of the word classes. (Part-of-speech tags are a popular type of class-based feature used
in word sense disambiguation (WSD) to capture syntactic generalizations.) Recall that
the WordNet synset hierarchy can be viewed as a taxonomy of concepts. Therefore, in
addition to using collocations in the form of other words, we use collocations in the
form of semantic concepts.
Word collocation features are derived by making two passes over the training
data (e.g., “on” sentences with correct role indicated). The first pass tabulates the
</bodyText>
<page confidence="0.972457">
164
</page>
<bodyText confidence="0.943485888888889">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
co-occurrence counts for each of the context words (i.e., those in a window around the
target word) paired with the classification value for the given training instance (e.g.,
the preposition sense from the annotation). These counts are used to derive conditional
probability estimates of each class value given co-occurrence of the various potential
collocates. The words exceeding a certain threshold are collected into a list associated
with the class value, making this a “bag of words” approach. In the experiments dis-
cussed below, a potential collocate (coll) is selected whenever the conditional probability
for the class (C) value exceeds the prior probability by a factor greater than 20%:6
</bodyText>
<equation confidence="0.9909795">
≥ .20 (1)
P(C)
</equation>
<bodyText confidence="0.9989658125">
That is, for a given potential collocation word (coll) to be treated as one of the ac-
tual collocation words, the relative percent change of the class conditional probability
(P(C|coll)) versus the prior probability for the class value (P(C)) must be 20% or higher.
The second pass over the training data determines the value for the collocational feature
of each classification category by checking whether the current context window has any
of the associated collocation words. Note that for the test data, only the second pass is
made, using the collocation lists derived from the training data.
In generalizing this to a class-based approach, the potential collocational words are
replaced with each of their hypernym ancestors from WordNet. The adjective hierarchy
is relatively shallow, so it is augmented by treating is-similar-to as has-hypernym. For
example, the synset for “arid” and “waterless” is linked to the synset for “dry (vs.
wet).” Adverbs would be included, but there is no hierarchy for them. Because the co-
occurring words are not sense-tagged, this is done for each synset serving as a different
sense of the word. Likewise, in the case of multiple inheritance, each parent synset is
used. For example, given the co-occurring word money, the counts would be updated as
if each of the following tokens were seen (grouped by sense).
</bodyText>
<listItem confidence="0.9104019">
1. { medium of exchange#1, monetary system#1, standard#1, criterion#1,
measure#2, touchstone#1, reference point#1, point of reference#1, ref-
erence#3, indicator#2, signal#1, signaling#1, sign#3, communication#2,
social relation#1, relation#1, abstraction#6 }
2. { wealth#4, property#2, belongings#1, holding#2, material possession#1,
possession#2 }
3. { currency#1, medium of exchange#1, monetary system#1, standard#1,
criterion#1, measure#2, touchstone#1, reference point#1, point of -
reference#1, reference#3, indicator#2, signal#1, signaling#1, sign#3,
communication#2, social relation#1, relation#1, abstraction#6 }
</listItem>
<bodyText confidence="0.9995992">
Thus, the word token money is replaced by 41 synset tokens. Then, the same two-pass
process just described is performed over the text consisting of the replacement tokens.
Although this introduces noise due to ambiguity, the conditional-probability selection
scheme (Wiebe, McKeever, and Bruce 1998) compensates by selecting hypernym synsets
that tend to co-occur with specific roles.
</bodyText>
<footnote confidence="0.907492">
6 The 20% threshold is a heuristic that is fixed for all experiments. We tested automatic threshold derivation
for Senseval-3 and found that the optimal percentage differed across training sets. As values near 20%
were common, it is left fixed rather than adding an additional feature-threshold refinement step.
</footnote>
<equation confidence="0.867095">
P(C|coll) − P(C)
</equation>
<page confidence="0.960726">
165
</page>
<note confidence="0.468812">
Computational Linguistics Volume 35, Number 2
</note>
<bodyText confidence="0.9998522">
Note that there is no preference in the system for choosing either specific or general
hypernyms. Instead, they are inferred automatically based on the word to be disam-
biguated (i.e., preposition for these experiments). Hypernyms at the top levels of the
hierarchy are less likely to be chosen, as they most likely occur with different senses for
the same word (as with relation#1 previously). However, hypernyms at lower levels
tend not to be chosen, as there might not be enough occurrences due to other co-
occurring words. For example, wealth#4 is unlikely to be chosen as a collocation for
the second sense of money, as only a few words map into it, unlike property#2. The
conditional-probability selection scheme (i.e., Equation (1)) handles this automatically
without having to encode heuristics about hypernym rank, and so on.
</bodyText>
<subsubsectionHeader confidence="0.810957">
3.1.2 Classification Experiments. A supervised approach for word-sense disambiguation
</subsubsectionHeader>
<bodyText confidence="0.994281875">
is used following Bruce and Wiebe (1999).
For each experiment, stratified 10-fold cross validation is used: The classifiers are
repeatedly trained on 90% of the data and tested on the remainder, with the test sets
randomly selected to form a partition. The results described here were obtained using
the settings in Figure 4, which are similar to the settings used by O’Hara et al. (2004)
in the third Senseval competition. The top systems from recent Senseval competitions
(Mihalcea 2002; Grozea 2004) use a variety of lexical features for WSD. Words in the im-
mediate context (Word±i) and their parts of speech (POS±i) are standard features. Word
collocations are also common, but there are various ways of organizing collocations into
features (Wiebe, McKeever, and Bruce 1998). We use the simple approach of having a
single binary feature per sense (e.g., role) that is set true whenever any of the associated
collocation words for that sense are encountered (i.e., per-class-binary).
The main difference of our approach from more typical WSD systems (Mihalcea,
Chklovski, and Kilgarriff 2004) concerns the hypernym collocations. The collocation
context section of Figure 4 shows that word collocations can occur anywhere in the
sentence, whereas hypernym collocations must occur within five words of the target
</bodyText>
<table confidence="0.763946133333333">
Features:
Prep: preposition being classified
POS±i: part-of-speech of word at offset i
Word±i: stem of word at offset i
WordCollr: context has word collocation for role r
HypernymCollr: context has hypernym collocation for role r
Collocation context:
Word: anywhere in the sentence
Hypernym: within 5 words of target preposition
Collocation selection:
Frequency: f (word) &gt; 1
Conditional probability: P(C|coll) ≥ .50
Relative percent change: (P(C|coll) − P(C))/P(C) ≥ .20
Organization: per-class-binary
Model selection:
</table>
<footnote confidence="0.38116">
C4.5 Decision tree via Weka’s J4.8 classifier (Quinlan 1993; Witten and Frank 1999)
</footnote>
<figureCaption confidence="0.831082">
Figure 4
</figureCaption>
<bodyText confidence="0.536417">
Feature settings used in preposition classification experiments. Aspects that differ from a typical WSD
system are italicized.
</bodyText>
<page confidence="0.978894">
166
</page>
<bodyText confidence="0.971474444444445">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
prepositions (i.e., a five-word context window).7 This reduced window size is used
to make the hypernym collocations more related to the prepositional object and the
modified term.
The feature settings in Figure 4 are used in three different configurations: word-
based collocations alone, hypernym collocations alone, and both collocations together.
Combining the two types generally produces the best results, because this balances the
specific clues provided by the word collocations with the generalized clues provided by
the hypernym collocations.
Unlike the general case for WSD, the sense inventory is the same for all the words
being disambiguated; therefore, a single classifier can be produced rather than indi-
vidual classifiers. This has the advantage of allowing more training data to be used
in the derivation of the clues indicative of each semantic role. However, if there were
sufficient annotations for particular preposition, then it would be advantageous to have
a dedicated classifier. For example, the prior probabilities for the roles would be based
on the usages for the given preposition. Therefore, we perform experiments illustrating
the difference when disambiguating prepositions with a single classifier versus the use
of separate classifiers.
</bodyText>
<subsectionHeader confidence="0.999226">
3.2 Penn Treebank Classification Experiments
</subsectionHeader>
<bodyText confidence="0.99997775">
The first set of experiments deals with preposition disambiguation using PTB. When
deriving training data from PTB via the parse tree annotations, the functional tags as-
sociated with prepositional phrases are converted into preposition sense tags. Consider
the following excerpt from the sample annotation for PTB shown earlier:
</bodyText>
<equation confidence="0.97684525">
(S (PP-TMP In (NP 1982)), temporal extent
(NP-SBJ grammatical subject
(NP (NP (NP Sports) &amp; (NP Recreation) ’s)
managers) ...
</equation>
<bodyText confidence="0.9333166">
Treating temporal as the preposition sense yields the following annotation:
(7) InTMP 1982, Sports &amp; Recreation’s managers ...
The relative frequencies of the roles in the PTB annotations for PPs are shown in Ta-
ble 9. As can be seen, several of the roles do not occur often with PPs (e.g., extent). This
somewhat skewed distribution makes for an easier classification task than the one for
FrameNet.
3.2.1 Illustration with “at.” As an illustration of the probabilities associated with class-
based collocations, consider the differences in the prior versus class-based conditional
probabilities for the semantic roles of the preposition at in the Penn Treebank (ver-
sion II). Table 10 shows the global probabilities for the roles assigned to at, along with
</bodyText>
<footnote confidence="0.934996">
7 This window size was chosen after estimating that on average the prepositional objects occur within
2.3 ± 1.26 words of the preposition and that the average attachment site is within 3.0 ± 2.98 words. These
figures were produced by analyzing the parse trees for the semantic role annotations in the PTB.
</footnote>
<page confidence="0.952465">
167
</page>
<table confidence="0.582545">
Computational Linguistics Volume 35, Number 2
</table>
<tableCaption confidence="0.989177">
Table 9
</tableCaption>
<table confidence="0.968445454545455">
Penn Treebank semantic roles for PPs. Omits low-frequency benefactive relation. Freq. is the relative
frequency of the role occurrence (36,476 total instances). Example usages are taken from
the corpus.
Role Freq.
locative .472
temporal .290
direction .149
manner .050
purpose .030
extent .008
Example
</table>
<tableCaption confidence="0.6583745">
workers at a factory
expired at midnight Tuesday
has grown at a sluggish pace
CDs aimed at individual investors
opened for trading
declined by 14%
</tableCaption>
<bodyText confidence="0.98570875">
conditional probabilities for these roles given that certain high-level WordNet synsets
occur in the context. In a context referring to a concrete concept (i.e., entity#1), the
difference in the probability distributions for the locative and temporal roles shows that
the locative interpretation becomes even more likely. In contrast, in a context referring
to an abstract concept (i.e., abstraction#6), the difference in the probability distributions
for the same roles shows that the temporal interpretation becomes more likely. Therefore,
these class-based lexical associations capture commonsense usages of the preposition at.
3.2.2 Results. The classification results for these prepositions in the Penn Treebank show
that this approach is very effective. Table 11 shows the accuracy when disambiguating
the 14 prepositions using a single classifier with 6 roles. Table 11 also shows the per-
class statistics, showing that there are difficulties tagging the manner role (e.g., lowest
F-score). For the single-classifier case, the overall accuracy is 89.3%, using Weka’s J4.8
classifier (Witten and Frank 1999), which is an implementation of Quinlan’s (1993) C4.5
decision tree learner.
For comparison, Table 12 shows the results for individual classifiers created for the
prepositions annotated in PTB. A few prepositions only have small data sets, such as
of which is used more for specialization relations (e.g., category) than thematic ones.
This table is ordered by entropy, which measures the inherent ambiguity in the classes
as given by the annotations. Note that the Baseline column is the probability of the most
frequent sense, which is a common estimate of the lower bound for classification
</bodyText>
<tableCaption confidence="0.842875">
Table 10
</tableCaption>
<bodyText confidence="0.890721333333333">
Prior and posterior probabilities of roles for “at” in the Penn Treebank. P(R) is the relative frequency.
P(R|S) is the probability of the relation given that the synset occurs in the immediate context of
at. RPCR,S is the relative percentage change: (P(R|S) − P(R))/P(R).
</bodyText>
<table confidence="0.894890666666667">
Synset
entity#1 abstraction#6
Relation P(R) P(R|S) RPCR,S P(R|S) RPCR,S
locative 73.5 75.5 0.03 67.0 −0.09
temporal 23.9 22.5 −0.06 30.6 0.28
manner 2.0 1.5 −0.25 2.0 0.00
direction 0.6 0.4 −0.33 0.4 −0.33
168
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</table>
<tableCaption confidence="0.995088">
Table 11
</tableCaption>
<bodyText confidence="0.562815">
Overall preposition disambiguation results over Penn Treebank roles. A single classifier is used for all
the prepositions. # Instances is the number of role annotations. # Classes is the number of distinct
roles. Entropy measures non-uniformity of the role distributions. Baseline is estimated by the
most-frequent role. The Word Only experiment uses just word collocations, Hypernym Only just
uses hypernym collocations, and Both uses both types of collocations. Accuracy is average for
percent correct over ten trials in cross validation. STDEV is the standard deviation over the trials.
</bodyText>
<table confidence="0.998934461538462">
Experiment Accuracy STDEV Data Set Characteristics
Word Collocations Only 88.1 0.88 # Instances: 27,308
Hypernym Collocations Only 88.2 0.43 # Classes: 6
Both Collocations 89.3 0.33 Entropy: 1.831
Baseline: 49.2
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
direction .953 .969 .960 .952 .967 .959 .956 .965 .961
extent .817 .839 .826 .854 .819 .834 .817 .846 .829
locative .879 .967 .921 .889 .953 .920 .908 .932 .920
manner .797 .607 .687 .790 .599 .680 .826 .558 .661
purpose .854 .591 .695 .774 .712 .740 .793 .701 .744
temporal .897 .776 .832 .879 .794 .834 .845 .852 .848
</table>
<tableCaption confidence="0.999177">
Table 12
</tableCaption>
<bodyText confidence="0.848328142857143">
Per-preposition disambiguation results over Penn Treebank roles. A separate classifier is used for each
preposition, excluding roles with less than 1% relative frequency. Freq gives the preposition
frequency, and Roles the number of senses. Entropy measures data set uniformity, and Baseline
selects most common role. The Word and Hypernym columns show results when including just
word and hypernym collocations respectively, whereas Both includes both types. Each column
shows averages for percent correct over ten trials. The Mean row averages the values of the
individual experiments.
</bodyText>
<table confidence="0.9997045">
Prep Freq. Roles Entropy Baseline Word Hypernym Both
through 331 4 1.668 0.438 59.795 62.861 58.592
by 1290 7 1.575 0.479 87.736 88.231 86.655
as 220 3 1.565 0.405 95.113 96.377 96.165
between 87 4 1.506 0.483 77.421 81.032 70.456
of 30 3 1.325 0.567 63.182 82.424 65.606
out 76 4 1.247 0.711 70.238 76.250 63.988
for 1401 6 1.189 0.657 82.444 85.795 80.158
on 1915 5 1.181 0.679 85.998 88.720 79.428
in 14321 7 1.054 0.686 86.404 92.647 86.523
throughout 59 2 0.998 0.525 61.487 35.949 63.923
at 2825 5 0.981 0.735 84.178 90.265 85.561
across 78 2 0.706 0.808 75.000 78.750 77.857
from 1521 5 0.517 0.917 91.649 91.650 91.650
to 3074 5 0.133 0.985 98.732 98.537 98.829
Mean 1944.8 4.43 1.12 0.648 80.0 82.1 78.9
</table>
<page confidence="0.919563">
169
</page>
<note confidence="0.451232">
Computational Linguistics Volume 35, Number 2
</note>
<bodyText confidence="0.995884333333333">
experiments. When using preposition-specific classifiers, the hypernym collocations
surprisingly outperform the other configurations, most likely due to overfitting with
word-based clues: 82.1% versus 80.0% for the word-only case.
</bodyText>
<subsectionHeader confidence="0.996313">
3.3 FrameNet Classification Experiments
</subsectionHeader>
<bodyText confidence="0.99997025">
The second set of experiments perform preposition disambiguation using FrameNet.
A similar preposition word-sense disambiguation experiment is carried out over the
FrameNet semantic role annotations involving prepositional phrases. Consider the sam-
ple annotation shown earlier:
</bodyText>
<listItem confidence="0.95991575">
(8) Hewlett-Packard Co has rolled out a new range of ISDN connectivity
enabling (C FE=“Communicator” PT=“NP”)standalone workstations(/C)
to (C TARGET=“y”)communicate(/C) (C FE=“Medium” PT=“PP”)over
public or private ISDN networks(/C).
</listItem>
<bodyText confidence="0.9835285">
The prepositional phrase annotation is isolated and treated as the sense of the preposi-
tion. This yields the following sense annotation:
</bodyText>
<listItem confidence="0.879476666666667">
(9) Hewlett-Packard Co has rolled out a new range of ISDN connectivity
enabling standalone workstations to communicate overMedium public or
private ISDN networks.
</listItem>
<bodyText confidence="0.908968166666666">
Table 13 shows the distribution of common roles assigned to prepositional phrases. The
topic role is the most frequent case not directly covered in PTB.
3.3.1 Illustration with “at.” See Table 14 for the most frequent roles out of the 124 cases
that were assigned to at, along with the conditional probabilities for these roles given
that certain high-level WordNet synsets occur in the context. In a context referring
to concrete entities, the role place becomes more prominent. However, in an abstract
context, the role time becomes more prominent. Thus, similar behavior to that noted for
PTB in Section 3.2.1 occurs with FrameNet.
3.3.2 Results. Table 15 shows the results of classification when all of the prepositions
are classified together. Due to the exorbitant number of roles (641), the overall results
are low. However, the combined collocation approach still shows slight improvement
(23.3% versus 23.1%). The FrameNet inventory contains many low-frequency relations
</bodyText>
<tableCaption confidence="0.995377">
Table 13
</tableCaption>
<table confidence="0.955271555555556">
Most common FrameNet semantic roles for PPs. Relative frequencies for roles assigned to
prepositional phrases in version 1.3 (66,038 instances), omitting cases below 0.01.
Role Freq. Role Freq. Role Freq.
goal .092 theme .022 whole .015
path .071 manner .021 individuals .013
source .043 area .018 location .012
topic .040 reason .018 ground .012
time .037 addressee .017 means .011
place .033 stimulus .017 content .011
</table>
<page confidence="0.697858">
170
</page>
<note confidence="0.555224">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<tableCaption confidence="0.992761">
Table 14
</tableCaption>
<bodyText confidence="0.9783175">
Prior and posterior probabilities of roles for “at” in FrameNet. Only the top 5 of 641 applicable roles
are shown. P(R) is the relative frequency for relation. P(R|S) is the probability of the relation given
that the synset occurs in the immediate context of at. RPCR,S is the relative percentage change:
(P(R|S) − P(R))/P(R).
</bodyText>
<table confidence="0.998624">
Synset
entity#1 abstraction#6
Relation P(R) P(R|S) RPCR,S P(R|S) RPCR,S
place 15.6 19.0 21.8 16.8 7.7
time 12.0 11.5 −4.2 15.1 25.8
stimulus 6.6 5.0 −24.2 6.6 0.0
addressee 6.1 4.4 −27.9 3.3 −45.9
goal 5.5 6.3 14.5 6.0 9.1
</table>
<tableCaption confidence="0.991893">
Table 15
</tableCaption>
<table confidence="0.988175714285714">
Preposition disambiguation with all FrameNet roles. All 641 roles are considered. Entropy measures
data set uniformity, and Baseline selects most common role.
Experiment Accuracy STDEV Data Set Characteristics
Word Collocations Only 23.078 0.472 # Instances: 65,550
Hypernym Collocations Only 23.206 0.467 # Classes: 641
Both Collocations 23.317 0.556 Entropy: 6.785
Baseline: 9.3
</table>
<bodyText confidence="0.999537444444444">
that complicate this type of classification. By filtering out relations that occur in less than
1% of the role occurrences for prepositional phrases, substantial improvement results,
as shown in Table 16. Even with filtering, the classification is challenging (e.g., 18 classes
with entropy 3.82). Table 16 also shows the per-class statistics, indicating that the means
and place roles are posing difficulties for classification.
Table 17 shows the results when using individual classifiers, ordered by entropy.
This illustrates that the role distributions are more complicated than those for PTB,
yielding higher entropy values on average. In all, there are over 360 prepositions
with annotations, 92 with ten or more instances each. (Several of the low-frequency
cases are actually adverbs, such as anywhere, but are treated as prepositions during the
annotation extraction.) The results show that the word collocations produce slightly
better results: 67.8 versus 66.0 for combined collocations. Unlike the case with PTB,
the single-classifier performance is below that of the individual classifiers. This is
due to the fine-grained nature of the role inventory. When all the roles are considered
together, prepositions are sometimes being incorrectly classified using roles that have
not been assigned to them in the training data. This occurs when contextual clues are
stronger for a commonly used role than for the appropriate one. Given PTB’s small role
inventory, this problem does not occur in the corresponding experiments.
</bodyText>
<subsectionHeader confidence="0.8353">
3.4 Factotum Classification Experiments
</subsectionHeader>
<bodyText confidence="0.998638">
The third set of experiments deals with preposition disambiguation using Factotum.
Note that Factotum does not indicate the way the relationships are expressed in English.
</bodyText>
<page confidence="0.995193">
171
</page>
<table confidence="0.414873">
Computational Linguistics Volume 35, Number 2
</table>
<tableCaption confidence="0.87882975">
Table 16
Overall results for preposition disambiguation with common FrameNet roles. Excludes roles with less
than 1% relative frequency. Entropy measures data set uniformity, and Baseline selects most
common role. Detailed per-class statistics are also included, averaged over the 10 folds.
</tableCaption>
<table confidence="0.936246551724138">
Experiment Accuracy STDEV Data Set Characteristics
Word Collocations Only 73.339 0.865 # Instances: 32974
Hypernym Collocations Only 73.437 0.594 # Classes: 18
Both Collocations 73.544 0.856 Entropy: 3.822
Baseline: 18.4
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
addressee .785 .332 .443 .818 .263 .386 .903 .298 .447
area .618 .546 .578 .607 .533 .566 .640 .591 .613
content .874 .618 .722 .895 .624 .734 .892 .639 .744
goal .715 .766 .739 .704 .778 .739 .703 .790 .743
ground .667 .386 .487 .684 .389 .494 .689 .449 .541
individuals .972 .947 .959 .961 .945 .953 .938 .935 .936
location .736 .524 .610 .741 .526 .612 .815 .557 .660
manner .738 .484 .584 .748 .481 .584 .734 .497 .591
means .487 .449 .464 .562 .361 .435 .524 .386 .441
path .778 .851 .812 .777 .848 .811 .788 .849 .817
place .475 .551 .510 .483 .549 .513 .474 .576 .519
reason .803 .767 .784 .777 .773 .774 .769 .714 .738
source .864 .980 .918 .865 .981 .919 .860 .978 .915
stimulus .798 .798 .797 .795 .809 .802 .751 .752 .750
theme .787 .811 .798 .725 .847 .779 .780 .865 .820
time .585 .665 .622 .623 .687 .653 .643 .690 .664
topic .831 .836 .833 .829 .842 .835 .856 .863 .859
whole .818 .932 .871 .807 .932 .865 .819 .941 .875
Similarly, WordNet does not indicate this, but it does include definition glosses. For
example,
Factotum:
(drying, is-function-of, drier)
</table>
<listItem confidence="0.299923">
(10) WordNet:
</listItem>
<bodyText confidence="0.946328142857143">
dryalter remove the moisture from and make dry
dryerappliance an appliance that removes moisture
These definition glosses might be useful in certain cases for inferring the relation markers
(i.e., generalized case markers). As is, Factotum cannot be used to provide training data
for learning how the relations are expressed in English. This contrasts with corpus-
based annotations, such as PTB (Marcus et al. 1994) and FrameNet (Fillmore, Wooters,
and Baker 2001), where the relationships are marked in context.
</bodyText>
<footnote confidence="0.571849333333333">
3.4.1 Inferring Semantic Role Markers. To overcome the lack of context in Factotum, the
relation markers are inferred through corpus checks, in particular through proximity
searches involving the source and target terms from the relationship (i.e., (source,
</footnote>
<page confidence="0.993308">
172
</page>
<note confidence="0.793959">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<tableCaption confidence="0.65007175">
Table 17
Per-preposition disambiguation results over FrameNet roles. A separate classifier is used for each
preposition, excluding roles with less than 1% relative frequency. Freq gives the preposition
frequency, and Roles the number of senses. Entropy measures data set uniformity, and Baseline
selects most common role. The Word and Hypernym columns show results when including just
word and hypernym collocations, respectively, whereas Both includes both types. Each column
shows averages for percent correct over ten trials. The Mean row averages the values of the
individual experiments.
</tableCaption>
<table confidence="0.999775735294117">
Prep Freq. Roles Entropy Baseline Word Hypernym Both
with 3758 25 4.201 19.6 59.970 57.809 61.924
of 7339 22 4.188 12.8 85.747 84.663 85.965
between 675 23 4.166 11.4 61.495 56.215 53.311
under 286 26 4.045 25.5 29.567 33.040 33.691
against 557 26 4.028 21.2 53.540 58.885 31.892
for 2678 22 3.988 22.6 58.135 58.839 39.809
by 3348 18 3.929 13.6 62.618 60.854 61.152
on 3579 22 3.877 18.1 61.011 57.671 60.838
at 2685 21 3.790 21.2 61.814 58.501 57.630
in 6071 18 3.717 18.7 54.253 49.953 53.880
as 1123 17 3.346 27.1 53.585 47.186 42.722
to 4741 17 3.225 36.6 71.963 77.751 72.448
behind 254 13 3.222 22.8 47.560 41.045 43.519
over 1157 16 3.190 27.8 47.911 48.548 50.337
after 349 16 2.837 45.8 62.230 65.395 61.944
around 772 15 2.829 45.1 52.463 52.582 49.357
from 3251 14 2.710 51.2 73.268 71.934 75.423
round 389 12 2.633 34.7 46.531 50.733 49.393
into 1923 14 2.208 62.9 79.175 77.366 80.846
during 242 10 2.004 63.6 71.067 75.200 68.233
like 570 9 1.938 62.3 82.554 79.784 85.666
through 1358 10 1.905 66.0 77.800 77.798 79.963
up 745 10 1.880 60.3 76.328 76.328 74.869
off 647 9 1.830 63.8 90.545 86.854 90.423
out 966 8 1.773 60.7 77.383 79.722 78.671
across 894 11 1.763 67.6 80.291 80.095 80.099
towards 673 10 1.754 67.9 65.681 71.171 65.517
down 965 7 1.600 63.2 81.256 81.466 79.141
along 723 9 1.597 72.5 87.281 86.862 86.590
about 1894 8 1.488 72.2 83.214 76.663 83.899
back 405 7 1.462 64.7 88.103 91.149 86.183
past 275 9 1.268 78.9 85.683 86.423 85.573
Mean 1727.9 14.8 2.762 43.8 67.813 67.453 65.966
</table>
<bodyText confidence="0.9949354">
relation, target)). For example, using AltaVista’s Boolean search,8 this can be done via
“source NEAR target.”
Unfortunately, this technique would require detailed post-processing of the Web
search results, possibly including parsing, in order to extract the patterns. As an ex-
pedient, common prepositions9 are included in a series of proximity searches to find
</bodyText>
<footnote confidence="0.99401">
8 AltaVista’s Boolean search is available at www.altavista.com/sites/search/adv.
9 The common prepositions are determined from the prepositional phrases assigned functional
annotations in the Penn Treebank (Marcus et al. 1994).
</footnote>
<page confidence="0.994938">
173
</page>
<note confidence="0.592103">
Computational Linguistics Volume 35, Number 2
</note>
<bodyText confidence="0.982372522727273">
the preposition occurring most frequently with the given terms. For instance, given the
relationship (drying, is-function-of, drier), the following searches would be performed.
(11) drying NEAR drier NEAR in
drying NEAR drier NEAR to
...
drying NEAR drier NEAR “around”
To account for prepositions that occur frequently (e.g., of ), pointwise mutual infor-
mation (MI) statistics (Manning and Sch¨utze 1999, pages 66–68) are used in place of the
raw frequency when rating the potential markers. These are calculated as follows:
P(X,Y) f (source NEAR target NEAR prep) MIprep = log2 P(X) x P(Y) — log2 (2)
f (source NEAR target) x f (prep)
Such checks are done for the 25 most common prepositions to find the preposition
yielding the highest mutual information score. For example, the top three markers for
the (drying, is-function-of, drier) relationship based on this metric are during, after, and
with.
3.4.2 Method for Classifying Functional Relations. Given the functional relationships in
Factotum along with the inferred relation markers, machine-learning algorithms can
be used to infer what relation most likely applies to terms occurring together with a
particular marker. Note that the main purpose of including the relation markers is to
provide clues for the particular type of relation. Because the source term and target
terms might occur in other relationships, associations based on them alone might not
be as accurate. In addition, the inclusion of these clue words (e.g., the prepositions)
makes the task closer to what would be done in inferring the relations from free text.
The task thus approximates preposition disambiguation, using the Factotum relations
as senses.
Figure 5 gives the feature settings used in the experiments. This is a version of
the feature set used in the PTB and FrameNet experiments (see Figure 4), simplified to
account for the lack of sentential context. Figure 6 contains sample feature specifications
from the experiments discussed in the next section. The top part shows the original
relationships from Factotum; the first example indicates that connaturalize causes simi-
larity. Also included is the most likely relation marker inferred for each instance. This
shows that “n/a” is used whenever a preposition for a particular relationship cannot be
inferred. This happens in the first example because connaturalize is a rare term.
The remaining parts of Figure 6 illustrate the feature values that would be derived
for the three different experiment configurations, based on the inclusion of word and/or
hypernym collocations. In each case, the classification variable is given by relation.
For brevity, the feature specification only includes collocation features for the most
frequent relations. Sample collocations are also shown for the relations (e.g., vulgar-
ity for is-caused-by). In the word collocation case, the occurrence of similarity is used
to determine that the is-caused-by feature (WC1) should be positive (i.e., “1”) for the
first two instances. Note that there is no corresponding hypernym collocation due to
conditional probability filtering. In addition, although new is not included as a word
collocation, one of its hypernyms, namely Adj:early#2, is used to determine that the
has-consequence feature (HC3) should be positive in the last instance.
</bodyText>
<page confidence="0.993047">
174
</page>
<note confidence="0.6255835">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Context:
</note>
<bodyText confidence="0.598555857142857">
Source and target terms from relationship ((source, relation, target))
Features:
POSsource: part-of-speech of the source term
POStarget: part-of-speech of the target term
Prep: preposition serving as relation marker (“n/a” if not inferable)
WordCollr: 1 iff context contains any word collocation for relation r
HypernymCollr: 1 iff context contains any hypernym collocation for relation r
</bodyText>
<figure confidence="0.515721666666667">
Collocation selection:
Frequency: f (word) &gt; 1
Relative percent change: (P(C|coll) − P(C))/P(C) &gt; .20
Organization: per-class-binary grouping
Model selection:
Decision tree using Weka’s J4.8 classifier (Witten and Frank 1999)
</figure>
<figureCaption confidence="0.605100333333333">
Figure 5
Features used in Factotum role classification experiments. Simplified version of Figure 4: Context
only consists of the source and target terms.
</figureCaption>
<bodyText confidence="0.996623045454546">
3.4.3 Results. To make the task more similar to the PTB and FrameNet cases covered
previously, only the functional relations in Factotum are used. These are determined
by removing the hierarchical relations (e.g., has-subtype and has-part) along with the
attribute relations (e.g., is-property-of). In addition, in cases where there are inverse
functions (e.g., causes and is-caused-by), the most frequently occurring relation of each
inverse pair is used. This is done because the relation marker inference approach does
not account for argument order. The boldface relations in the listing shown earlier in
Table 5 are those used in the experiment. Only single-word source and target terms are
considered to simplify the WordNet hypernym lookup (i.e., no phrasals). The resulting
data set has 5,959 training instances. The data set also includes the inferred relation
markers (e.g., one preposition per training instance), thus introducing some noise.
Figure 6 includes a few examples from this data set. This shows that the original
relationship (similarity, is-caused-by, rhyme) from Factotum is augmented with the
by marker prior to classification. Again, these markers are inferred via Web searches
involving the terms from the original relationship.
Table 18 shows the results of the classification. The combined use of both collocation
types achieves the best overall accuracy at 71.2%, which is good considering that the
baseline of always choosing the most common relation (is-caused-by) is 24.2%. This com-
bination generalizes well by using hypernym collocations, while retaining specificity
via word collocations. The classification task is difficult, as suggested by the number
of classes, entropy, and baseline values all being comparable to the filtered FrameNet
experiment (see Table 16).
</bodyText>
<subsectionHeader confidence="0.987434">
3.5 Common Relation Inventory Classification Experiments
</subsectionHeader>
<bodyText confidence="0.9999796">
The last set of experiments investigate preposition disambiguation using FrameNet
mapped into a reduced semantic role inventory. For the application to lexical acqui-
sition, the semantic role annotations are converted into the common relation inventory
discussed in Section 2.5. To apply the common inventory to the FrameNet data, anno-
tations using the 641 FrameNet relations (see Table 2) need to be mapped into those
</bodyText>
<page confidence="0.994588">
175
</page>
<table confidence="0.751284257142857">
Computational Linguistics Volume 35, Number 2
Relationships from Factotum with inferred markers:
Relationship Marker
(similarity, is-caused-by, connaturalize) n/a
(similarity, is-caused-by, rhyme) by
(approximate, has-consequence, imprecise) because
(new, has-consequence, patented) with
Word collocations only:
Relation POSs POSt Prep WC1 WC2
is-caused-by NN VB n/a 1 0
is-caused-by NN NN by 1 0
has-consequence NN JJ because 0 0
has-consequence JJ VBN with 0 0
WC3 WC4 WC5 WC6 WC7
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
Sample collocations:
is-caused-by {bitterness, evildoing, monochrome, similarity, vulgarity}
has-consequence {abrogate, frequently, insufficiency, nonplus, ornament}
POSs POSt Prep HC1 HC2 HC3 HC4 HC5 HC6 HC7
NN VB n/a 0 0 0 0 0 0 0
NN NN by 0 0 0 0 0 0 0
NN JJ because 0 0 0 0 0 0 0
JJ VBN with 0 0 1 0 0 0 0
Hypernym collocations only:
Relation
is-caused-by
is-caused-by
has-consequence
has-consequence
Sample collocations:
is-caused-by {N:hostility#3, N:inelegance#1, N:humorist#1}
has-consequence {V:abolish#1, Adj:early#2, N:inability#1, V:write#2}
</table>
<bodyText confidence="0.9385635">
Both collocations:
Relation
is-caused-by
is-caused-by
has-consequence
has-consequence
</bodyText>
<table confidence="0.999324">
POSs POSt Prep WC1 ... WC7 HC1 HC2 HC3 ...
NN VB n/a 1 ... 0 0 0 0 ...
NN NN by 1 ... 0 0 0 0 ...
NN JJ because 0 ... 0 0 0 0 ...
JJ VBN with 0 ... 0 0 0 1 ...
</table>
<sectionHeader confidence="0.232505" genericHeader="background">
Legend:
</sectionHeader>
<bodyText confidence="0.5888015">
POSs &amp; POSt are the parts of speech for the source and target terms; and
WCr &amp; HCr are the word and hypernym collocations as follows:
</bodyText>
<listItem confidence="0.6967515">
1. is-caused-by 2. is-function-of 3. has-consequence 4. has-result
5. is-caused-bymental 6. is-performed-by 7. uses
</listItem>
<footnote confidence="0.6847046">
Figure 6
Sample feature specifications for Factotum experiments. Each relationship from Factotum is
augmented with one relational marker inferred via Web searches, as shown at top of figure.
Three distinct sets of feature vectors are shown based on the type of collocation included,
omitting features for low-frequency relations.
</footnote>
<page confidence="0.988142">
176
</page>
<note confidence="0.636343">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<tableCaption confidence="0.8882154">
Table 18
Functional relation classification over Factotum. This uses the relational source and target terms
with inferred prepositions. The accuracy figures are averages based on 10-fold cross validation.
The gain in accuracy for the combined experiment versus the word experiment is statistically
significant at p &lt; .01 (via a paired t-test).
</tableCaption>
<table confidence="0.9977214">
Experiment Accuracy STDEV Data Set Characteristics
Word Collocations Only 68.4 1.28 # Instances: 5,959
Hypernym Collocations Only 53.9 1.66 # Classes: 21
Both Collocations 71.2 1.78 Entropy: 3.504
Baseline: 24.2
</table>
<bodyText confidence="0.999471162162162">
using the 26 common relations shown in Table 8. Results for the classification of the
FrameNet data mapped into the common inventory are shown in Table 19. As can
be seen, the performance is well above that of the full classification over FrameNet
without filtering (see Table 15). Although the low-frequency role filtering yields the
highest performance (see Table 16), this comes at the expense of having half of the
training instances discarded. Corpus annotations are a costly resource, so such waste
is undesirable. Table 19 also shows the per-class statistics, indicating that the means,
direction, and part roles are handled poorly by the classifier. The latter two are due to the
relatively small training examples for the roles in question, which can be addressed
partly by refining the mapping from FrameNet. However, problems classifying the
means role occur with all classifiers discussed in this article, suggesting that that role
is too subtle to be classified with the feature set currently used.
The results in Table 19 also illustrate that the reduced, common-role inventory has
an additional advantage of improving performance in the classification, compared to a
cascaded approach. This occurs because several of the miscellaneous roles in FrameNet
cover subtle distinctions that are not relevant for definition analysis (e.g., cognizer and
addressee). The common inventory therefore strikes a balance between the overly general
roles in PTB, which are easy to classify, and the overly specialized roles in FrameNet,
which are quite difficult to classify. Nonetheless, a certain degree of classification diffi-
culty is inevitable in order for the inventory to provide adequate coverage of the dif-
ferent distinctions present in dictionary definitions. Note that, by using the annotations
from PTB and FrameNet, the end result is a general-purpose classifier, not one tied into
dictionary text. Thus, it is useful for other tasks besides definition analysis.
This classifier was used to disambiguate prepositions in the lexical acquisition
system we developed at NMSU (O’Hara 2005). Evaluation of the resulting distinctions
was performed by having the output of the system rated by human judges. Manu-
ally corrected results were also evaluated by the same judges. The overall ratings are
not high in both cases, suggesting that some of the distinctions being made are subtle.
For instance, for “counterintelligence achieved by deleting any information of value”
from the definition of censoring, means is the preferred role for by, but manner is ac-
ceptable. Likewise, characteristic is the preferred role for of, but category is interpretable.
Thus, the judges differed considerably on these cases. However, as the ratings for
the uncorrected output were close to those for the corrected output, the approach is
promising to use for lexical acquisition. If desired, the per-role accuracy results shown
in Table 19 could be incorporated as confidence values assigned to particular relation-
ships extracted from definitions (e.g., 81% for those with source but only 21% when
means used).
</bodyText>
<page confidence="0.991263">
177
</page>
<note confidence="0.693291">
Computational Linguistics Volume 35, Number 2
</note>
<sectionHeader confidence="0.994245" genericHeader="related work">
4. Related Work
</sectionHeader>
<bodyText confidence="0.9998975">
The main contribution of this article concerns the classification methodology (rather
than the inventories for semantic roles), so we will only review other work related
to this aspect. First, we discuss similar work involving hypernyms. Then, we address
preposition classification proper.
Scott and Matwin (1998) use WordNet hypernyms for text classification. They
include a numeric density feature for any synset that subsumes words appearing in
the document, potentially yielding hundreds of features. In contrast, the hypernym
collocations discussed in Section 3.1.1 involve a binary feature for each of the relations
being classified, using indicative synsets based on the conditional probability test. This
test alleviates the need for their maximum height parameter to avoid overly general
hypernyms. Their approach, as well as ours, considers all senses of a word, distrib-
uting the alternative readings throughout the set of features. In comparison, Gildea
</bodyText>
<tableCaption confidence="0.9456168">
Table 19
Results for preposition disambiguation with common roles. The FrameNet annotations are mapped
into the common inventory from Table 8. Entropy measures data set uniformity, and Baseline
selects most common role. Detailed per-class statistics are also included, averaged over the
10 folds.
</tableCaption>
<table confidence="0.999940032258064">
Experiment Accuracy STDEV Data Set Characteristics
Word Collocations Only 62.9 0.345 # Instances: 59,615
Hypernym Collocations Only 62.6 0.487 # Classes: 24
Both Collocations 63.1 0.639 Entropy: 4.191
Baseline: 12.2
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
accompaniment .630 .611 .619 .671 .605 .636 .628 .625 .626
agent .623 .720 .667 .639 .726 .677 .616 .731 .668
area .546 .475 .508 .541 .490 .514 .545 .501 .522
category .694 .706 .699 .695 .700 .697 .714 .718 .716
cause .554 .493 .521 .569 .498 .531 .540 .482 .509
characteristic .595 .468 .523 .607 .474 .530 .584 .490 .532
context .569 .404 .472 .577 .388 .463 .568 .423 .485
direction .695 .171 .272 .701 .189 .294 .605 .169 .260
duration .601 .465 .522 .589 .445 .503 .596 .429 .497
experiencer .623 .354 .449 .606 .342 .435 .640 .378 .474
goal .664 .683 .673 .662 .674 .668 .657 .680 .668
instrument .406 .339 .367 .393 .337 .360 .405 .370 .385
location .433 .557 .487 .427 .557 .483 .417 .553 .475
manner .493 .489 .490 .483 .478 .479 .490 .481 .485
means .235 .183 .205 .250 .183 .210 .254 .184 .212
medium .519 .306 .382 .559 .328 .412 .529 .330 .403
part .539 .289 .368 .582 .236 .323 .526 .301 .380
path .705 .810 .753 .712 .813 .759 .706 .795 .748
product .837 .750 .785 .868 .739 .788 .769 .783 .770
recipient .661 .486 .559 .661 .493 .563 .642 .482 .549
resource .613 .471 .530 .614 .458 .524 .618 .479 .539
source .703 .936 .802 .697 .936 .799 .707 .937 .806
theme .545 .660 .596 .511 .661 .576 .567 .637 .600
time .619 .624 .621 .626 .612 .619 .628 .611 .619
</table>
<page confidence="0.979367">
178
</page>
<note confidence="0.475705">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<bodyText confidence="0.99996438">
and Jurafsky (2002) instead just select the first sense for their hypernym features for
relation classification. They report marginal improvements using the features, whereas
configurations with hypernym collocations usually perform best in our preposition
disambiguation experiments.
Mohit and Narayanan (2003) use WordNet hypernyms to generalize patterns for
information extraction inferred from FrameNet annotations by distributing support
from terms co-occurring in annotations for frame elements to the terms for hypernyms.
However, they do not incorporate a filtering stage, as with our conditional probability
test. Mihalcea (2002) shows how hypernym information can be useful in deriving clues
for unsupervised WSD. Patterns for co-occurring words of a given sense are induced
from sense-tagged corpora. Each pattern specifies templates for the co-occurring words
in the immediate context window of the target word, as well as their corresponding
synsets if known (e.g., sense tagged or unambiguous), and similarly the hypernym
synsets if known. To disambiguate a word, the patterns for each of its senses are
evaluated in the context, and the sense with the most support is chosen.
The work here addresses relation disambiguation specifically with respect to those
indicated by prepositional phrases (i.e., preposition word-sense disambiguation). Until
recently, there has been little work on general-purpose preposition disambiguation.
Litkowski (2002) and Srihari, Niu, and Li (2001) present approaches using manually
derived rules. Both approaches account for only a handful of prepositions; in contrast,
for FrameNet we disambiguate 32 prepositions via individual classifiers and over 100
prepositions via the combined classifier. Liu and Soo (1993) present a heuristic approach
for relation disambiguation relying upon syntactic clues as well as occurrence of specific
prepositions. They assign roles to constituents of a sentence from corpus data provided
that sufficient instances are available. Otherwise, a human trainer is used to answer
questions needed by the system for the assignment. They report an 86% accuracy rate
for the assignment of roles to verbal arguments in about 5,000 processed sentences.
Alam (2004) sketches out how the preposition over might be disambiguated into one
of a dozen roles using features based on the head and complement, such as whether the
head is a movement verb or whether the complement refers to a duration. These features
form the basis for a manually-constructed decision tree, which is interpreted by hand in
an evaluation over sentences from the British National Corpus (BNC), giving a precision
of 93.5%. Boonthum, Toida, and Levinstein (2006), building upon the work of Alam,
show how WordNet can be used to automate the determination of similar head and
complement properties. For example, if both the head and complement refer to people,
with should be interpreted as accompaniment. These features form the basis for a
disambiguation system using manually constructed rules accounting for ten commonly
occurring prepositions. They report a precision of 79% with a recall of 76% over an
inventory of seven roles in a post hoc evaluation that allows for partial correctness.
There have been a few machine-learning approaches that are more similar to the ap-
proach used here. Gildea and Jurafsky (2002) perform relation disambiguation using the
FrameNet annotations as training data. They include lexical features for the headword
of the phrase and the predicating word for the entire annotated frame (e.g., the verb
corresponding to the frame under which the annotations are grouped). They also use
several features derived from the output of a parser, such as the constituent type of the
phrase (e.g., NP), the grammatical function (e.g., subject), and a path feature listing part-
of-speech tags from the target word to the phrase being tagged. They report an accuracy
of 78.5% with a baseline of 40.6% over the FrameNet semantic roles. However, by
conditioning the classification on the predicating word, the range of roles for a particular
classification instance is more limited than in the experiments presented in this article.
</bodyText>
<page confidence="0.991893">
179
</page>
<note confidence="0.561511">
Computational Linguistics Volume 35, Number 2
</note>
<bodyText confidence="0.999858588235294">
Blaheta and Charniak (2000) use the PTB annotations for relation disambiguation. They
use a few parser-derived features, such as the constituent labels for nearby nodes and
part-of-speech for parent and grandparent nodes. They also include lexical features for
the head and alternative head (because prepositions are considered as the head by their
parser). As their classifier tags all adjuncts, they include the nominal and adverbial roles,
which are syntactic and more predictable than the roles occurring with prepositional
phrases.
There have been recent workshops featuring competitions for semantic role tagging
(Carreras and M`arquez 2004, 2005; Litkowski 2004). A common approach is to tag all
the semantic roles in a sentence at the same time to account for dependencies, such
as via Hidden Markov Models. To take advantage of accurate Support Vector Machine
classification, Pradhan et al. (2005) instead use a postprocessing phrase based on trigram
models of roles. Their system incorporates a large variety of features, building upon sev-
eral different preceding approaches, such as including extensions to the path features
from Gildea and Jurafsky (2002). Their lexical features include the predicate root word,
headwords for the sentence constituents and PPs, as well as their first and last words.
Koomen et al. (2005) likewise use a large feature set. They use an optimization phase to
maximize satisfaction of the constraints imposed by the PropBank data set, such as the
number of arguments for particular predicates (e.g., just two for stalk, arg0 and arg1).
Lastly, Ye and Baldwin (2006) show how filtering can be used to constrain the
hypernyms selected to serve as collocations, building upon our earlier work (O’Hara
and Wiebe 2003). They report 87.7% accuracy in a setup similar to ours over PTB
(i.e., a gain of 2 percentage points). They use a different type of collocation feature
than ours: having a binary feature for each potential collocation rather than a single
feature per class. That is, they use Over-Range Binary rather than Per-Class Binary (Wiebe,
McKeever, and Bruce 1998). Moreover, they include several hundred of these features,
rather than our seven (benefactive previously included), which is likely the main source
of improvement. Again, the per-class binary organization is a bag of words approach,
so it works well only with a limited number of potential collocations. Follow-up work
of theirs (Ye and Baldwin 2007) fared well in the recent preposition disambiguation
competition, held as part of SemEval-2007 (Litkowski and Hargraves 2007). Thus, an
immediate area for future work will be to incorporate such improved feature sets. We
will also investigate addressing sentential role constraints as in general semantic role
tagging.
</bodyText>
<sectionHeader confidence="0.997842" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.999938692307692">
This article shows how to exploit semantic role resources for preposition disambigua-
tion. Information about two different types of semantic role resources is provided. The
emphasis is on corpus-based resources providing annotations of naturally occurring
text. The Penn Treebank (Marcus et al. 1994) covers general roles for verbal adjuncts and
FrameNet (Fillmore, Wooters, and Baker 2001) includes a wide range of domain-specific
roles for all verbal arguments. In addition, semantic role inventories from knowledge
bases are investigated. Cyc (Lehmann 1996) provides fine-grained role distinctions,
Factotum (Cassidy 2000) includes a variety of functional relations, and work in Concep-
tual Graphs (Sowa 1999) emphasizes roles for attributes. Relations from both types of
resources are considered when developing the inventory of relations used for definition
analysis, as shown in Table 8.
The disambiguation concentrates on relations indicated by prepositional phrases,
and is framed as word-sense disambiguation for the preposition in question. A new
</bodyText>
<page confidence="0.955348">
180
</page>
<note confidence="0.464279">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<bodyText confidence="0.999930647058824">
type of feature for word-sense disambiguation is introduced, using WordNet hyper-
nyms as collocations rather than just words, as is typically done. The full feature set is
shown in Figure 4. Various experiments over the PTB and FrameNet data are presented,
including prepositions classified separately versus together, and illustrating the effects
of filtering. The main results in Tables 11 and 16 show that the combined use of word
and hypernym collocations generally achieves the best performance. For relationships
derived from knowledge bases, the prepositions and other relational markers need to
be inferred from corpora. A method for doing this is demonstrated using Factotum,
with results shown in Table 18. In addition, to account for granularity differences in the
semantic role inventories, the relations are mapped into a common inventory that was
developed based on the inventories discussed in the article. This allows for improved
classification in cases where inventories provide overly specialized relations, such as
those in FrameNet. Classification results are shown in Table 19.
The recent competitions on semantic relation labeling have highlighted the useful-
ness of incorporating a variety of clues for general-purpose relation disambiguation
(Carreras and M`arquez 2005). Some of the techniques developed here for preposition
disambiguation can likely help with relation disambiguation in general. For instance,
there are quite a few lexical features, such as in Pradhan et al. (2005), which could be
extended to use semantic classes as with our hypernym collocations. In general it seems
that, when lexical features are used in supervised machine learning, it is likely that
corresponding class-based features based on hypernyms can be beneficial for improved
coverage.
Other aspects of this approach are geared specifically to our goal of supporting
lexical acquisition from dictionaries, which was the motivation for the emphasis on
preposition disambiguation. Isolating the preposition annotations allows the classifiers
to be more readily tailored to definition analysis, especially because predicate frames
are not assumed as with other FrameNet relation disambiguation. Future work will
investigate combining the general relation classifiers with preposition disambiguation
classifiers, such as is done in Ye and Baldwin (2006). Future work will also investigate
improvements to the application to definition analysis. Currently, FrameNet roles are
always mapped to the same common inventory role (e.g., place to location). However, this
should account for the frame of the annotation and perhaps other context information.
Lastly, we will also look for more resources to exploit for preposition disambiguation
(e.g., ResearchCyc).
</bodyText>
<sectionHeader confidence="0.997664" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999888666666667">
The experimentation for this article was
greatly facilitated though the use of
computing resources at New Mexico State
University. We are also grateful for the
extremely helpful comments provided
by the anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.999139" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989083958333333">
Alam, Yukiko Sasaki. 2004. Decision trees
for sense disambiguation of prepositions:
Case of over. In Proceedings of the
Computational Lexical Semantics Workshop,
pages 52–59, Boston, MA.
Barker, Ken. 1998. Semi-Automatic Recognition
of Semantic Relationships in English Technical
Texts. Ph.D. thesis, Department of
Computer Science, University of Ottawa.
Bies, Ann, Mark Ferguson, Karen Katz,
Robert MacIntyre, Victoria Tredinnick,
Grace Kim, Mary Ann Marcinkiewicz,
and Britta Schasberger.1995. Bracketing
guidelines for Treebank II style:
Penn Treebank project. Technical
Report MS-CIS-95-06, University of
Pennsylvania.
Blaheta, Don and Eugene Charniak. 2000.
Assigning function tags to parsed text.
In Proceedings of the 1st Annual Meeting
of the North American Chapter of the
American Association for Computational
Linguistics (NAACL-2000), pages
234–240,Seattle, WA.
</reference>
<page confidence="0.989856">
181
</page>
<note confidence="0.545064">
Computational Linguistics Volume 35, Number 2
</note>
<reference confidence="0.999404855932203">
Boonthum, Chutima, Shunichi Toida, and
Irwin B. Levinstein. 2006. Preposition
senses: Generalized disambiguation
model. In Proceedings of the Seventh
International Conference on Computational
Linguistics and Intelligent Text Processing
(CICLing-2006), pages 196–207,
Mexico City.
Bruce, Bertram. 1975. Case systems for
natural language. Artificial Intelligence,
6:327–360.
Bruce, Rebecca and Janyce Wiebe. 1999.
Decomposable modeling in natural
language processing. Computational
Linguistics, 25(2):195–208.
Carreras, Xavier and Llu´ıs M`arquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Natural Language
Learning (CoNLL-2004), pages 89–97,
Boston, MA.
Carreras, Xavier and Llu´ıs M`arquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Natural Language
Learning (CoNLL-2005), pages 152–164,
Ann Arbor, MI.
Cassidy, Patrick J. 2000. An investigation
of the semantic relations in the Roget’s
Thesaurus: Preliminary results. In
Proceedings of the First International
Conference on Intelligent Text Processing
and Computational Linguistics
(CICLing-2000), pages 181–204,
Mexico City.
Cruse, David A. 1986. Lexical Semantics.
Cambridge University Press, Cambridge.
Edmonds, Phil and Scott Cotton, editors.
2001. Proceedings of the Senseval 2 Workshop.
Association for Computational Linguistics,
Toulouse.
Fillmore, Charles.1968. The case for case.
In Emmon Bach and Rovert T. Harms,
editors, Universals in Linguistic Theory.
Holt, Rinehart and Winston, New York,
pages 1–88.
Fillmore, Charles J., Charles Wooters, and
Collin F. Baker. 2001. Building a large
lexical databank which provides
deep semantics. In Proceedings of the
Pacific Asian Conference on Language,
Information and Computation, pages 3–25,
Hong Kong.
Frawley, William. 1992. Linguistic Semantics.
Lawrence Erlbaum Associates,
Hillsdale, NJ.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245–288.
Grozea, Cristian. 2004. Finding optimal
parameter settings for high performance
word sense disambiguation. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 125–128,
Barcelona.
Kilgarriff, Adam. 1998. Senseval: An exercise
in evaluating word sense disambiguation
programs. In Proceedings of the First
International Conference on Language
Resources and Evaluation (LREC ’98),
pages 581–588, Granada.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 181–184,
Ann Arbor, MI.
Lehmann, Fritz. 1996. Big posets of
participatings and thematic roles. In
Peter W. Eklund, Gerard Ellis, and
Graham Mann, editors, Conceptual
Structures: Knowledge Representation as
Interlingua, Springer-Verlag, Berlin,
pages 50–74.
Lenat, Douglas B. 1995. Cyc: A large-scale
investment in knowledge infrastructure.
Communications of the ACM, 38(11):33–38.
Litkowski, Kenneth C. 2002. Digraph
analysis of dictionary preposition
definitions. In Proceedings of the
SIGLEX/SENSEVAL Workshop on Word
Sense Disambiguation: Recent Successes
and Future Directions, pages 9–16,
Philadelphia, PA.
Litkowski, Kenneth C. 2004. Senseval-3 task:
Automatic labeling of semantic roles.
In Proceedings of Senseval-3: The Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text,
pages 9–12, Barcelona.
Litkowski, Kenneth C. and Orin Hargraves.
2006. Coverage and inheritance in The
Preposition Project. In Third ACL-SIGSEM
Workshop on Prepositions, pages 37–44,
Trento.
Litkowski, Kenneth C. and Orin Hargraves.
2007. SemEval-2007 task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24–29, Prague.
Liu, Rey-Long and Von-Wun Soo. 1993. An
empirical study on thematic knowledge
acquisition based on syntactic clues and
heuristics. In Proceedings of the 31st Annual
Meeting of the Association for Computational
</reference>
<page confidence="0.967227">
182
</page>
<note confidence="0.511159">
O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation
</note>
<reference confidence="0.999614347457627">
Linguistics (ACL-93), pages 243–250,
Columbus, OH.
Manning, Christopher D. and Hinrich
Sch¨utze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings
of the ARPA Human Language
Technology Workshop, pages 110–115,
Plainsboro, NJ.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz.1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313–330.
Mihalcea, Rada. 2002. Instance based
learning with automatic feature
selection applied to word sense
disambiguation. In Proceedings of
the 19th International Conference on
Computational Linguistics (COLING-2002),
pages 1–7, Taiwan.
Mihalcea, Rada, Timothy Chklovski, and
Adam Kilgarriff. 2004. The Senseval-3
English lexical sample task. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 25–28,
Barcelona.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine Miller. 1990. WordNet: An
on-line lexical database. International
Journal of Lexicography, 3(4): Special
Issue on WordNet.
Miller, Katherine. 1998. Modifiers in
WordNet. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA,
pages 47–67.
Mohit, Behrang and Srini Narayanan.
2003. Semantic extraction with
wide-coverage lexical resources. In
Companion Volume of the Proceedings
of HLT-NAACL 2003 - Short Papers,
pages 64–66, Edmonton.
O’Hara, Thomas P. 2005. Empirical acquisition
of conceptual distinctions via dictionary
definitions. Ph.D. thesis, Department
of Computer Science, New Mexico
State University.
O’Hara, Tom, Rebecca Bruce, Jeff Donner,
and Janyce Wiebe. 2004. Class-based
collocations for word-sense
disambiguation. In Proceedings of
Senseval-3: The Third International
Workshop on the Evaluation of Systems
for the Semantic Analysis of Text,
pages 199–202, Barcelona.
O’Hara, Tom and Janyce Wiebe. 2003.
Classifying functional relations in
Factotum via WordNet hypernym
associations. In Proceedings of the
Fourth International Conference on
Intelligent Text Processing and
Computational Linguistics (CICLing-2003),
pages 347–359, Mexico City.
OpenCyc. 2002.OpenCyc release 0.6b.
Available at www.opencyc.org.
Palmer, Martha, Dan Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
A corpus annotated with semantic roles.
Computational Linguistics, 31(1):71–106.
Palmer, Martha Stone. 1990. Semantic
Processing for Finite Domains. Cambridge
University Press, Cambridge.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Daniel Jurafsky. 2005. Support
vector learning for semantic argument
classification. Machine Learning,
60(1–3):11–39.
Quinlan, J. Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
San Mateo, CA.
Scott, Sam and Stan Matwin.1998. Text
classification using WordNet hypernyms.
In Proceedings of the COLING/ACL
Workshop on Usage of WordNet in Natural
Language Processing Systems, pages 38–44,
Montreal.
Somers, Harold L. 1987. Valency and Case in
Computational Linguistics. Edinburgh
University Press, Scotland.
Sowa, John F. 1984. Conceptual Structures in
Mind and Machines. Addison-Wesley,
Reading, MA.
Sowa, John F. 1999. Knowledge Representation:
Logical, Philosophical, and Computational
Foundations. Brooks Cole Publishing,
Pacific Grove, CA.
Srihari, Rohini, Cheng Niu, and Wei Li.
2001. A hybrid approach for named
entity and sub-type tagging. In
Proceedings of the 6th Applied Natural
Language Processing Conference,
pages 247–254, Seattle.
Wiebe, Janyce, Kenneth McKeever, and
Rebecca F. Bruce. 1998. Mapping
collocational properties into machine
learning features. In Proceedings of the
6th Workshop on Very Large Corpora
(WVLC-98), pages 225–233, Montreal.
</reference>
<page confidence="0.975855">
183
</page>
<reference confidence="0.970751333333333">
Computational Linguistics Volume 35, Number 2
Wilks, Yorick, Brian M. Slator, and Louise
Guthrie. 1996. Electric Words. MIT Press,
Cambridge, MA.
Witten, Ian H. and Eibe Frank. 1999.
Data Mining: Practical Machine
Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann,
San Francisco, CA.
Ye, Patrick and Timothy Baldwin. 2006.
Semantic role labeling of prepositional
phrases. ACM Transactions on Asian
Language Information Processing (TALIP),
5(3):228–244.
Ye, Patrick and Timothy Baldwin.
2007. MELB-YB: Preposition sense
disambiguation using rich semantic
features. In Proceedings of the Fourth
International Workshop on Semantic
Evaluations (SemEval-2007),
pages 241–244, Prague.
</reference>
<page confidence="0.994713">
184
</page>
<note confidence="0.517621">
This article has been cited by:
</note>
<reference confidence="0.910118">
1. Timothy Baldwin, Valia Kordoni, Aline Villavicencio. 2009. Prepositions in Applications:
A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and
Introduction to the Special Issue. Computational Linguistics 35:2, 119-149. [Citation] [PDF]
[PDF Plus]
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.702930">
<title confidence="0.904986">Exploiting Semantic Role Resources for Preposition Disambiguation</title>
<affiliation confidence="0.994949">University of Maryland, Baltimore County University of Pittsburgh</affiliation>
<abstract confidence="0.989495">This article describes how semantic role resources can be exploited for preposition disambiguation. The main resources include the semantic role annotations provided by the Penn Treebank and FrameNet tagged corpora. The resources also include the assertions contained in the Factotum knowledge base, as well as information from Cyc and Conceptual Graphs. A common inventory is derived from these in support of definition analysis, which is the motivation for this work. The disambiguation concentrates on relations indicated by prepositional phrases, and is framed as word-sense disambiguation for the preposition in question. A new type offeature for word-sense disambiguation is introduced, using WordNet hypernyms as collocations rather than just words. Various experiments over the Penn Treebank and FrameNet data are presented, including prepositions classified separately versus together, and illustrating the effects offiltering. Similar experimentation is done over the Factotum data, including a method for inferring likely preposition usage from corpora, as knowledge bases do not generally indicate how relationships are expressed in English (in contrast to the explicit annotations on this in the Penn Treebank and FrameNet). Other experiments are included with the FrameNet data mapped into the common relation inventory developed for definition analysis, illustrating how preposition disambiguation might be applied in lexical acquisition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yukiko Sasaki Alam</author>
</authors>
<title>Decision trees for sense disambiguation of prepositions: Case of over.</title>
<date>2004</date>
<booktitle>In Proceedings of the Computational Lexical Semantics Workshop,</booktitle>
<pages>52--59</pages>
<location>Boston, MA.</location>
<contexts>
<context position="86017" citStr="Alam (2004)" startWordPosition="12906" endWordPosition="12907">eNet we disambiguate 32 prepositions via individual classifiers and over 100 prepositions via the combined classifier. Liu and Soo (1993) present a heuristic approach for relation disambiguation relying upon syntactic clues as well as occurrence of specific prepositions. They assign roles to constituents of a sentence from corpus data provided that sufficient instances are available. Otherwise, a human trainer is used to answer questions needed by the system for the assignment. They report an 86% accuracy rate for the assignment of roles to verbal arguments in about 5,000 processed sentences. Alam (2004) sketches out how the preposition over might be disambiguated into one of a dozen roles using features based on the head and complement, such as whether the head is a movement verb or whether the complement refers to a duration. These features form the basis for a manually-constructed decision tree, which is interpreted by hand in an evaluation over sentences from the British National Corpus (BNC), giving a precision of 93.5%. Boonthum, Toida, and Levinstein (2006), building upon the work of Alam, show how WordNet can be used to automate the determination of similar head and complement propert</context>
</contexts>
<marker>Alam, 2004</marker>
<rawString>Alam, Yukiko Sasaki. 2004. Decision trees for sense disambiguation of prepositions: Case of over. In Proceedings of the Computational Lexical Semantics Workshop, pages 52–59, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Barker</author>
</authors>
<title>Semi-Automatic Recognition of Semantic Relationships in English Technical Texts.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Ottawa.</institution>
<contexts>
<context position="8738" citStr="Barker (1998)" startWordPosition="1258" endWordPosition="1259">y. At the other extreme, some systems use quite specific roles tailored to a particular domain, such as catalyst in the chemical sense. 2.1 Background on Semantic Roles Bruce (1975) presents an account of early case systems in NLP. For the most part, those systems had limited case role inventories, along the lines of the cases defined by Fillmore (1968). Palmer (1990) discusses some of the more contentious issues regarding case systems, including adequacy for representation, such as reliance solely upon case information to determine semantics versus the use of additional inference mechanisms. Barker (1998) provides a comprehensive summary of case inventories in NLP, along with criteria for the qualitative evaluation of case systems (generality, completeness, and uniqueness). Linguistic work on thematic roles tends to use a limited number of roles. Frawley (1992) presents a detailed discussion of twelve thematic roles and discusses how they are realized in different languages. During the shift in emphasis away from systems that work in small, self-contained domains to those that can handle open-ended domains, there has been a trend towards the use of larger sets of semantic primitives (Wilks, Sl</context>
</contexts>
<marker>Barker, 1998</marker>
<rawString>Barker, Ken. 1998. Semi-Automatic Recognition of Semantic Relationships in English Technical Texts. Ph.D. thesis, Department of Computer Science, University of Ottawa.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
<author>Victoria Tredinnick</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Britta Schasberger 1995</author>
</authors>
<title>Bracketing guidelines for Treebank II style: Penn Treebank project.</title>
<tech>Technical Report MS-CIS-95-06,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="13174" citStr="Bies et al. (1995)" startWordPosition="1931" endWordPosition="1934">NP Sports) &amp; (NP Recreation) ’s) managers) and (NP certain passive investors)) (VP purchased (NP the company) (PP-CLR from closely related (NP (NP Brunswick Corp.) (PP-LOC of locative (NP (NP Skokie) , (NP Ill))) ))) .) Figure 1 Penn Treebank II parse tree annotation sample. The functional tags are shown in boldface. 154 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation Table 1 Frequency of Penn Treebank II semantic role annotations. Relative frequencies estimated over the counts for unique assignments given in the PTB documentation (bkt tags.lst), and descriptions based on Bies et al. (1995). Omits low-frequency benefactive role. The syntactic role annotations generally have higher frequencies; for example, the subject role occurs 49% of the time (out of about 240,000 total annotations). Role Freq. temporal .113 locative .075 direction .026 manner .021 purpose .017 extent .010 Description indicates when, how often, or how long place/setting of the event starting or ending location (trajectory) indicates manner, including instrument purpose or reason spatial extent location. The second PP is tagged as closely-related, which is one of the miscellaneous PTB function tags that are mo</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, Tredinnick, Kim, Marcinkiewicz, 1995, </marker>
<rawString>Bies, Ann, Mark Ferguson, Karen Katz, Robert MacIntyre, Victoria Tredinnick, Grace Kim, Mary Ann Marcinkiewicz, and Britta Schasberger.1995. Bracketing guidelines for Treebank II style: Penn Treebank project. Technical Report MS-CIS-95-06, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Blaheta</author>
<author>Eugene Charniak</author>
</authors>
<title>Assigning function tags to parsed text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of the American Association for Computational Linguistics (NAACL-2000),</booktitle>
<pages>234--240</pages>
<location>WA.</location>
<contexts>
<context position="88053" citStr="Blaheta and Charniak (2000)" startWordPosition="13228" endWordPosition="13231">. They also use several features derived from the output of a parser, such as the constituent type of the phrase (e.g., NP), the grammatical function (e.g., subject), and a path feature listing partof-speech tags from the target word to the phrase being tagged. They report an accuracy of 78.5% with a baseline of 40.6% over the FrameNet semantic roles. However, by conditioning the classification on the predicating word, the range of roles for a particular classification instance is more limited than in the experiments presented in this article. 179 Computational Linguistics Volume 35, Number 2 Blaheta and Charniak (2000) use the PTB annotations for relation disambiguation. They use a few parser-derived features, such as the constituent labels for nearby nodes and part-of-speech for parent and grandparent nodes. They also include lexical features for the head and alternative head (because prepositions are considered as the head by their parser). As their classifier tags all adjuncts, they include the nominal and adverbial roles, which are syntactic and more predictable than the roles occurring with prepositional phrases. There have been recent workshops featuring competitions for semantic role tagging (Carrera</context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>Blaheta, Don and Eugene Charniak. 2000. Assigning function tags to parsed text. In Proceedings of the 1st Annual Meeting of the North American Chapter of the American Association for Computational Linguistics (NAACL-2000), pages 234–240,Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chutima Boonthum</author>
<author>Shunichi Toida</author>
<author>Irwin B Levinstein</author>
</authors>
<title>Preposition senses: Generalized disambiguation model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Seventh International Conference on Computational Linguistics and Intelligent Text Processing (CICLing-2006),</booktitle>
<pages>196--207</pages>
<location>Mexico City.</location>
<marker>Boonthum, Toida, Levinstein, 2006</marker>
<rawString>Boonthum, Chutima, Shunichi Toida, and Irwin B. Levinstein. 2006. Preposition senses: Generalized disambiguation model. In Proceedings of the Seventh International Conference on Computational Linguistics and Intelligent Text Processing (CICLing-2006), pages 196–207, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bertram Bruce</author>
</authors>
<title>Case systems for natural language.</title>
<date>1975</date>
<journal>Artificial Intelligence,</journal>
<pages>6--327</pages>
<contexts>
<context position="8306" citStr="Bruce (1975)" startWordPosition="1193" endWordPosition="1194">ts are indicated by case inflections (e.g., ablative in Latin). As used here, the term “semantic role” refers to an arbitrary semantic relation, and the term “thematic role” refers to a relation intended to capture the semantics of sentences (e.g., event participation). Which semantic roles are used varies widely in Natural Language Processing (NLP). Some systems use just a small number of very general roles, such as beneficiary. At the other extreme, some systems use quite specific roles tailored to a particular domain, such as catalyst in the chemical sense. 2.1 Background on Semantic Roles Bruce (1975) presents an account of early case systems in NLP. For the most part, those systems had limited case role inventories, along the lines of the cases defined by Fillmore (1968). Palmer (1990) discusses some of the more contentious issues regarding case systems, including adequacy for representation, such as reliance solely upon case information to determine semantics versus the use of additional inference mechanisms. Barker (1998) provides a comprehensive summary of case inventories in NLP, along with criteria for the qualitative evaluation of case systems (generality, completeness, and uniquene</context>
</contexts>
<marker>Bruce, 1975</marker>
<rawString>Bruce, Bertram. 1975. Case systems for natural language. Artificial Intelligence, 6:327–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>Decomposable modeling in natural language processing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="47880" citStr="Bruce and Wiebe (1999)" startWordPosition="7012" endWordPosition="7015">enses for the same word (as with relation#1 previously). However, hypernyms at lower levels tend not to be chosen, as there might not be enough occurrences due to other cooccurring words. For example, wealth#4 is unlikely to be chosen as a collocation for the second sense of money, as only a few words map into it, unlike property#2. The conditional-probability selection scheme (i.e., Equation (1)) handles this automatically without having to encode heuristics about hypernym rank, and so on. 3.1.2 Classification Experiments. A supervised approach for word-sense disambiguation is used following Bruce and Wiebe (1999). For each experiment, stratified 10-fold cross validation is used: The classifiers are repeatedly trained on 90% of the data and tested on the remainder, with the test sets randomly selected to form a partition. The results described here were obtained using the settings in Figure 4, which are similar to the settings used by O’Hara et al. (2004) in the third Senseval competition. The top systems from recent Senseval competitions (Mihalcea 2002; Grozea 2004) use a variety of lexical features for WSD. Words in the immediate context (Word±i) and their parts of speech (POS±i) are standard feature</context>
</contexts>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>Bruce, Rebecca and Janyce Wiebe. 1999. Decomposable modeling in natural language processing. Computational Linguistics, 25(2):195–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Conference on Natural Language Learning (CoNLL-2004),</booktitle>
<pages>89--97</pages>
<location>Boston, MA.</location>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>Carreras, Xavier and Llu´ıs M`arquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of the Eighth Conference on Natural Language Learning (CoNLL-2004), pages 89–97, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Natural Language Learning (CoNLL-2005),</booktitle>
<pages>152--164</pages>
<location>Ann Arbor, MI.</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Carreras, Xavier and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Natural Language Learning (CoNLL-2005), pages 152–164, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick J Cassidy</author>
</authors>
<title>An investigation of the semantic relations in the Roget’s Thesaurus: Preliminary results.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2000),</booktitle>
<pages>181--204</pages>
<location>Mexico City.</location>
<contexts>
<context position="5996" citStr="Cassidy 2000" startWordPosition="859" endWordPosition="860"> A crucial part of this process is the disambiguation of prepositions, exploiting online resources with semantic role usage information. The main resources are the Penn Treebank (PTB; Marcus et al. 1994) and FrameNet (Fillmore, Wooters, and Baker 2001), two popular corpora providing rich annotations on English text, such as the semantic roles associated with prepositional phrases in context. In addition to the semantic role annotations from PTB and FrameNet, traditional knowledge bases (KBs) are utilized to provide training data for the relation classification. In particular, the Factotum KB (Cassidy 2000) is used to provide additional training data for prepositions that are used to convey particular relationships. Information on preposition usage is not explicitly encoded in Factotum, so a new corpus analysis technique is employed to infer the associations. Details on the lexical acquisition process, including application and evaluation, can be found in O’Hara (2005). This article focuses on the aspects of this method relevant to the processing of prepositions. In particular, here we specifically address preposition 152 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation disam</context>
<context position="18995" citStr="Cassidy 2000" startWordPosition="2808" endWordPosition="2809">ges of prepositions, which often are not represented well in semantic lexicons (e.g., they are not included at all in WordNet). The Preposition Project uses the sense distinctions from the Oxford Dictionary of English and integrates syntactic information about prepositions from comprehensive grammar references. 156 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation 2.4 Inventories for Knowledge Representation This section describes three case inventories: one developed for the Cyc KB (Lenat 1995), one used to define Conceptual Graphs (Sowa 1984), and one for the Factotum KB (Cassidy 2000). The first two are based on a traditional knowledge representation paradigm. With respect to natural language processing, these approaches are more representative of the earlier approaches in which deep understanding is the chief goal. Factotum is also based on a knowledge representation paradigm, but in a sense also reflects the empirical aspect of the corpus annotation approach, because the annotations were developed to address the relations implicit in Roget’s Thesaurus. In this article, relation disambiguation experiments are only presented for Factotum, given that the others do not readi</context>
<context position="21622" citStr="Cassidy 2000" startWordPosition="3202" endWordPosition="3203"> knowledge representation as part of his Conceptual Structures theory. The original text listed two dozen or so thematic relations, such as destination and initiator. In all, 37 conceptual relations were defined. This inventory formed the basis for most work in Conceptual Graphs. Recently, Sowa (1999) updated the inventory to allow for better hierarchical structuring and to incorporate the important thematic roles identified by Somers (1987). Table 4 shows a sample of these roles, along with usage estimates based on corpus analysis (O’Hara 2005). 2.4.3 Factotum. The Factotum semantic network (Cassidy 2000) developed by Micra, Inc., makes explicit many of the relations in Roget’s Thesaurus.1 Outside of proprietary resources such as Cyc, Factotum is the most comprehensive KB with respect to functional relations, which are taken here to be non-hierarchical relations, excluding attributes. OpenCyc does include definitions of many non-hierarchical relations. However, there are not many instantiations (i.e., relationship assertions), because it concentrates on the higher level of the ontology. 1 Factotum is based on the public domain version of Roget’s Thesaurus. The latter is freely available via Pr</context>
<context position="91446" citStr="Cassidy 2000" startWordPosition="13746" endWordPosition="13747">e shows how to exploit semantic role resources for preposition disambiguation. Information about two different types of semantic role resources is provided. The emphasis is on corpus-based resources providing annotations of naturally occurring text. The Penn Treebank (Marcus et al. 1994) covers general roles for verbal adjuncts and FrameNet (Fillmore, Wooters, and Baker 2001) includes a wide range of domain-specific roles for all verbal arguments. In addition, semantic role inventories from knowledge bases are investigated. Cyc (Lehmann 1996) provides fine-grained role distinctions, Factotum (Cassidy 2000) includes a variety of functional relations, and work in Conceptual Graphs (Sowa 1999) emphasizes roles for attributes. Relations from both types of resources are considered when developing the inventory of relations used for definition analysis, as shown in Table 8. The disambiguation concentrates on relations indicated by prepositional phrases, and is framed as word-sense disambiguation for the preposition in question. A new 180 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation type of feature for word-sense disambiguation is introduced, using WordNet hypernyms as collocat</context>
</contexts>
<marker>Cassidy, 2000</marker>
<rawString>Cassidy, Patrick J. 2000. An investigation of the semantic relations in the Roget’s Thesaurus: Preliminary results. In Proceedings of the First International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2000), pages 181–204, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="27949" citStr="Cruse (1986)" startWordPosition="4131" endWordPosition="4132"> Table 5 shows the most common relations in terms of usage in the semantic network, and includes others that are used in the experiments discussed later.2 The relative frequencies just reflect relationships explicitly labeled in the KB data file. For instance, this does not account for implicit has-subtype relationships based on the hierarchical organization of the thesaural groups (e.g., (simple-change, has-subtype, conversion)). The functional relations are shown in boldface. This excludes the meronym or partwhole relations (e.g., is-conceptual-part-of), in line with their classification by Cruse (1986) as hierarchical relations. The reason for concentrating on the functional relations is that these are more akin to the roles tagged in PTB and FrameNet. The information in Factotum complements WordNet through the inclusion of more functional relations (e.g., non-hierarchical relations such as uses and is-function-of). For comparison purposes, Table 6 shows the semantic relation usage in WordNet version 2 The database files and documentation for the semantic network are available from Micra, Inc., via ftp://micra.com/factotum. 159 Computational Linguistics Volume 35, Number 2 Original data: A.</context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>Cruse, David A. 1986. Lexical Semantics. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Edmonds</author>
<author>Scott Cotton</author>
<author>editors</author>
</authors>
<date>2001</date>
<booktitle>Proceedings of the Senseval 2 Workshop. Association for Computational Linguistics,</booktitle>
<location>Toulouse.</location>
<marker>Edmonds, Cotton, editors, 2001</marker>
<rawString>Edmonds, Phil and Scott Cotton, editors. 2001. Proceedings of the Senseval 2 Workshop. Association for Computational Linguistics, Toulouse.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Charles 1968 Fillmore</author>
</authors>
<title>The case for case.</title>
<booktitle>Universals in Linguistic Theory.</booktitle>
<pages>1--88</pages>
<editor>In Emmon Bach and Rovert T. Harms, editors,</editor>
<location>Holt, Rinehart and Winston, New York,</location>
<marker>Fillmore, </marker>
<rawString>Fillmore, Charles.1968. The case for case. In Emmon Bach and Rovert T. Harms, editors, Universals in Linguistic Theory. Holt, Rinehart and Winston, New York, pages 1–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Charles Wooters</author>
<author>Collin F Baker</author>
</authors>
<title>Building a large lexical databank which provides deep semantics.</title>
<date>2001</date>
<booktitle>In Proceedings of the Pacific Asian Conference on Language, Information and Computation,</booktitle>
<pages>3--25</pages>
<location>Hong Kong.</location>
<marker>Fillmore, Wooters, Baker, 2001</marker>
<rawString>Fillmore, Charles J., Charles Wooters, and Collin F. Baker. 2001. Building a large lexical databank which provides deep semantics. In Proceedings of the Pacific Asian Conference on Language, Information and Computation, pages 3–25, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Frawley</author>
</authors>
<title>Linguistic Semantics. Lawrence Erlbaum Associates,</title>
<date>1992</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="8999" citStr="Frawley (1992)" startWordPosition="1296" endWordPosition="1297">tems had limited case role inventories, along the lines of the cases defined by Fillmore (1968). Palmer (1990) discusses some of the more contentious issues regarding case systems, including adequacy for representation, such as reliance solely upon case information to determine semantics versus the use of additional inference mechanisms. Barker (1998) provides a comprehensive summary of case inventories in NLP, along with criteria for the qualitative evaluation of case systems (generality, completeness, and uniqueness). Linguistic work on thematic roles tends to use a limited number of roles. Frawley (1992) presents a detailed discussion of twelve thematic roles and discusses how they are realized in different languages. During the shift in emphasis away from systems that work in small, self-contained domains to those that can handle open-ended domains, there has been a trend towards the use of larger sets of semantic primitives (Wilks, Slator, and Guthrie 1996). The WordNet lexicon (Miller et al. 1990) serves as one example of this. A synset is defined in terms of its relations with any of the other 100,000+ synsets, rather than in terms of a set of features like [±ANIMATE]. There has also been</context>
</contexts>
<marker>Frawley, 1992</marker>
<rawString>Frawley, William. 1992. Linguistic Semantics. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="36420" citStr="Gildea and Jurafsky (2002)" startWordPosition="5299" endWordPosition="5302">rted into a common inventory, and a separate relation classifier induced over the resulting data. This has the advantage that the target relation-type inventory remains stable whenever new sources of relation annotations are introduced. In addition, the classifier will likely be more accurate as there are more examples per relation type on average. The drawback, however, is that annotations from new resources must first be mapped into the common inventory before incorporation. The latter approach is employed here. The common inventory incorporates some of the general relation types defined by Gildea and Jurafsky (2002) for their experiments in classifying semantic relations in FrameNet using a reduced relation inventory. They defined 18 relations (including a special-case null role for expletives), as shown in Table 7. These roles served as the starting point for the common relation inventory we developed to support definition analysis (O’Hara 2005), with half of the roles used as is and a few others mapped into similar roles. In total, twenty-six relations are defined, including a few roles based on the PTB, Cyc, and Conceptual Graphs invenTable 7 Abstract roles defined by Gildea and Jurafsky based on Fram</context>
<context position="87144" citStr="Gildea and Jurafsky (2002)" startWordPosition="13086" endWordPosition="13089">, show how WordNet can be used to automate the determination of similar head and complement properties. For example, if both the head and complement refer to people, with should be interpreted as accompaniment. These features form the basis for a disambiguation system using manually constructed rules accounting for ten commonly occurring prepositions. They report a precision of 79% with a recall of 76% over an inventory of seven roles in a post hoc evaluation that allows for partial correctness. There have been a few machine-learning approaches that are more similar to the approach used here. Gildea and Jurafsky (2002) perform relation disambiguation using the FrameNet annotations as training data. They include lexical features for the headword of the phrase and the predicating word for the entire annotated frame (e.g., the verb corresponding to the frame under which the annotations are grouped). They also use several features derived from the output of a parser, such as the constituent type of the phrase (e.g., NP), the grammatical function (e.g., subject), and a path feature listing partof-speech tags from the target word to the phrase being tagged. They report an accuracy of 78.5% with a baseline of 40.6</context>
<context position="89193" citStr="Gildea and Jurafsky (2002)" startWordPosition="13400" endWordPosition="13403">e have been recent workshops featuring competitions for semantic role tagging (Carreras and M`arquez 2004, 2005; Litkowski 2004). A common approach is to tag all the semantic roles in a sentence at the same time to account for dependencies, such as via Hidden Markov Models. To take advantage of accurate Support Vector Machine classification, Pradhan et al. (2005) instead use a postprocessing phrase based on trigram models of roles. Their system incorporates a large variety of features, building upon several different preceding approaches, such as including extensions to the path features from Gildea and Jurafsky (2002). Their lexical features include the predicate root word, headwords for the sentence constituents and PPs, as well as their first and last words. Koomen et al. (2005) likewise use a large feature set. They use an optimization phase to maximize satisfaction of the constraints imposed by the PropBank data set, such as the number of arguments for particular predicates (e.g., just two for stalk, arg0 and arg1). Lastly, Ye and Baldwin (2006) show how filtering can be used to constrain the hypernyms selected to serve as collocations, building upon our earlier work (O’Hara and Wiebe 2003). They repor</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Grozea</author>
</authors>
<title>Finding optimal parameter settings for high performance word sense disambiguation. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>125--128</pages>
<location>Barcelona.</location>
<contexts>
<context position="48342" citStr="Grozea 2004" startWordPosition="7088" endWordPosition="7089">pernym rank, and so on. 3.1.2 Classification Experiments. A supervised approach for word-sense disambiguation is used following Bruce and Wiebe (1999). For each experiment, stratified 10-fold cross validation is used: The classifiers are repeatedly trained on 90% of the data and tested on the remainder, with the test sets randomly selected to form a partition. The results described here were obtained using the settings in Figure 4, which are similar to the settings used by O’Hara et al. (2004) in the third Senseval competition. The top systems from recent Senseval competitions (Mihalcea 2002; Grozea 2004) use a variety of lexical features for WSD. Words in the immediate context (Word±i) and their parts of speech (POS±i) are standard features. Word collocations are also common, but there are various ways of organizing collocations into features (Wiebe, McKeever, and Bruce 1998). We use the simple approach of having a single binary feature per sense (e.g., role) that is set true whenever any of the associated collocation words for that sense are encountered (i.e., per-class-binary). The main difference of our approach from more typical WSD systems (Mihalcea, Chklovski, and Kilgarriff 2004) conce</context>
</contexts>
<marker>Grozea, 2004</marker>
<rawString>Grozea, Cristian. 2004. Finding optimal parameter settings for high performance word sense disambiguation. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 125–128, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Senseval: An exercise in evaluating word sense disambiguation programs.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation (LREC ’98),</booktitle>
<pages>581--588</pages>
<location>Granada.</location>
<contexts>
<context position="10863" citStr="Kilgarriff 1998" startWordPosition="1583" endWordPosition="1584">Inventories Developed for Corpus Annotation With the emphasis on corpus analysis in computational linguistics, there has been a shift away from relying on explicitly-coded knowledge towards the use of knowledge inferred from naturally occurring text, in particular text that has been annotated by humans to indicate phenomena of interest. For example, rather than manually developing rules for preferring one sense of a word over another based on context, the most successful approaches have automatically learned the rules based on word-sense annotations, as evidenced by the Senseval competitions (Kilgarriff 1998; Edmonds and Cotton 2001). The Penn Treebank version II (Marcus et al. 1994) provided the first large-scale set of case annotations for general-purpose text. These are very general roles, following Fillmore (1968). The Berkeley FrameNet (Fillmore, Wooters, and Baker 2001) project currently provides the most comprehensive set of semantic roles annotations. These are at a much finer granularity than those in PTB, making them quite useful for applications learning semantics from corpora. Relation disambiguation experiments for both of these role inventories are presented subsequently. 2.2.1 Penn</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Kilgarriff, Adam. 1998. Senseval: An exercise in evaluating word sense disambiguation programs. In Proceedings of the First International Conference on Language Resources and Evaluation (LREC ’98), pages 581–588, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Koomen</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Generalized inference with multiple semantic role labeling systems.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>181--184</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="89359" citStr="Koomen et al. (2005)" startWordPosition="13427" endWordPosition="13430">ic roles in a sentence at the same time to account for dependencies, such as via Hidden Markov Models. To take advantage of accurate Support Vector Machine classification, Pradhan et al. (2005) instead use a postprocessing phrase based on trigram models of roles. Their system incorporates a large variety of features, building upon several different preceding approaches, such as including extensions to the path features from Gildea and Jurafsky (2002). Their lexical features include the predicate root word, headwords for the sentence constituents and PPs, as well as their first and last words. Koomen et al. (2005) likewise use a large feature set. They use an optimization phase to maximize satisfaction of the constraints imposed by the PropBank data set, such as the number of arguments for particular predicates (e.g., just two for stalk, arg0 and arg1). Lastly, Ye and Baldwin (2006) show how filtering can be used to constrain the hypernyms selected to serve as collocations, building upon our earlier work (O’Hara and Wiebe 2003). They report 87.7% accuracy in a setup similar to ours over PTB (i.e., a gain of 2 percentage points). They use a different type of collocation feature than ours: having a binar</context>
</contexts>
<marker>Koomen, Punyakanok, Roth, Yih, 2005</marker>
<rawString>Koomen, Peter, Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2005. Generalized inference with multiple semantic role labeling systems. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL), pages 181–184, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fritz Lehmann</author>
</authors>
<title>Big posets of participatings and thematic roles.</title>
<date>1996</date>
<booktitle>Conceptual Structures: Knowledge Representation as Interlingua,</booktitle>
<pages>50--74</pages>
<editor>In Peter W. Eklund, Gerard Ellis, and Graham Mann, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<contexts>
<context position="91381" citStr="Lehmann 1996" startWordPosition="13739" endWordPosition="13740">ts as in general semantic role tagging. 5. Conclusion This article shows how to exploit semantic role resources for preposition disambiguation. Information about two different types of semantic role resources is provided. The emphasis is on corpus-based resources providing annotations of naturally occurring text. The Penn Treebank (Marcus et al. 1994) covers general roles for verbal adjuncts and FrameNet (Fillmore, Wooters, and Baker 2001) includes a wide range of domain-specific roles for all verbal arguments. In addition, semantic role inventories from knowledge bases are investigated. Cyc (Lehmann 1996) provides fine-grained role distinctions, Factotum (Cassidy 2000) includes a variety of functional relations, and work in Conceptual Graphs (Sowa 1999) emphasizes roles for attributes. Relations from both types of resources are considered when developing the inventory of relations used for definition analysis, as shown in Table 8. The disambiguation concentrates on relations indicated by prepositional phrases, and is framed as word-sense disambiguation for the preposition in question. A new 180 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation type of feature for word-sense </context>
</contexts>
<marker>Lehmann, 1996</marker>
<rawString>Lehmann, Fritz. 1996. Big posets of participatings and thematic roles. In Peter W. Eklund, Gerard Ellis, and Graham Mann, editors, Conceptual Structures: Knowledge Representation as Interlingua, Springer-Verlag, Berlin, pages 50–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Lenat</author>
</authors>
<title>Cyc: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="18901" citStr="Lenat 1995" startWordPosition="2792" endWordPosition="2793">06) and endeavors to provide comprehensive syntactic and semantic information on various usages of prepositions, which often are not represented well in semantic lexicons (e.g., they are not included at all in WordNet). The Preposition Project uses the sense distinctions from the Oxford Dictionary of English and integrates syntactic information about prepositions from comprehensive grammar references. 156 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation 2.4 Inventories for Knowledge Representation This section describes three case inventories: one developed for the Cyc KB (Lenat 1995), one used to define Conceptual Graphs (Sowa 1984), and one for the Factotum KB (Cassidy 2000). The first two are based on a traditional knowledge representation paradigm. With respect to natural language processing, these approaches are more representative of the earlier approaches in which deep understanding is the chief goal. Factotum is also based on a knowledge representation paradigm, but in a sense also reflects the empirical aspect of the corpus annotation approach, because the annotations were developed to address the relations implicit in Roget’s Thesaurus. In this article, relation </context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Lenat, Douglas B. 1995. Cyc: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
</authors>
<title>Digraph analysis of dictionary preposition definitions.</title>
<date>2002</date>
<booktitle>In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>9--16</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="85243" citStr="Litkowski (2002)" startWordPosition="12790" endWordPosition="12791">or the co-occurring words in the immediate context window of the target word, as well as their corresponding synsets if known (e.g., sense tagged or unambiguous), and similarly the hypernym synsets if known. To disambiguate a word, the patterns for each of its senses are evaluated in the context, and the sense with the most support is chosen. The work here addresses relation disambiguation specifically with respect to those indicated by prepositional phrases (i.e., preposition word-sense disambiguation). Until recently, there has been little work on general-purpose preposition disambiguation. Litkowski (2002) and Srihari, Niu, and Li (2001) present approaches using manually derived rules. Both approaches account for only a handful of prepositions; in contrast, for FrameNet we disambiguate 32 prepositions via individual classifiers and over 100 prepositions via the combined classifier. Liu and Soo (1993) present a heuristic approach for relation disambiguation relying upon syntactic clues as well as occurrence of specific prepositions. They assign roles to constituents of a sentence from corpus data provided that sufficient instances are available. Otherwise, a human trainer is used to answer quest</context>
</contexts>
<marker>Litkowski, 2002</marker>
<rawString>Litkowski, Kenneth C. 2002. Digraph analysis of dictionary preposition definitions. In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 9–16, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
</authors>
<title>Senseval-3 task: Automatic labeling of semantic roles.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>9--12</pages>
<location>Barcelona.</location>
<contexts>
<context position="88695" citStr="Litkowski 2004" startWordPosition="13323" endWordPosition="13324">r relation disambiguation. They use a few parser-derived features, such as the constituent labels for nearby nodes and part-of-speech for parent and grandparent nodes. They also include lexical features for the head and alternative head (because prepositions are considered as the head by their parser). As their classifier tags all adjuncts, they include the nominal and adverbial roles, which are syntactic and more predictable than the roles occurring with prepositional phrases. There have been recent workshops featuring competitions for semantic role tagging (Carreras and M`arquez 2004, 2005; Litkowski 2004). A common approach is to tag all the semantic roles in a sentence at the same time to account for dependencies, such as via Hidden Markov Models. To take advantage of accurate Support Vector Machine classification, Pradhan et al. (2005) instead use a postprocessing phrase based on trigram models of roles. Their system incorporates a large variety of features, building upon several different preceding approaches, such as including extensions to the path features from Gildea and Jurafsky (2002). Their lexical features include the predicate root word, headwords for the sentence constituents and </context>
</contexts>
<marker>Litkowski, 2004</marker>
<rawString>Litkowski, Kenneth C. 2004. Senseval-3 task: Automatic labeling of semantic roles. In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 9–12, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>Coverage and inheritance in The Preposition Project.</title>
<date>2006</date>
<booktitle>In Third ACL-SIGSEM Workshop on Prepositions,</booktitle>
<pages>37--44</pages>
<location>Trento.</location>
<contexts>
<context position="2238" citStr="Litkowski and Hargraves 2006" startWordPosition="313" endWordPosition="316">on. 1. Introduction English prepositions convey important relations in text. When used as verbal adjuncts, they are the principal means of conveying semantic roles for the supporting entities described by the predicate. Preposition disambiguation is a challenging problem. First, prepositions are highly polysemous. A typical collegiate dictionary has dozens of senses for each of the common prepositions. Second, the senses of prepositions tend to be closely related to one another. For instance, there are three duplicate role assignments among the twenty senses for of in The Preposition Project (Litkowski and Hargraves 2006), a resource containing semantic annotations for common prepositions. * Institute for Language and Information Technologies, Baltimore, MD 21250. E-mail: tomohara@umbc.edu. ** Department of Computer Science, Pittsburgh, PA 15260. E-mail: wiebe@cs.pitt.edu. Submission received: 7 August 2006; accepted for publication: 21 February 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 35, Number 2 Consider the disambiguation of the usages of on in the following sentences: (1) The cut should be blocked on procedural grounds. (2) The industry already operates on ve</context>
<context position="18293" citStr="Litkowski and Hargraves 2006" startWordPosition="2706" endWordPosition="2709">els arg0 through arg4 denoting the main verbal arguments to avoid misinterpretations. Verbal adjuncts are assigned roles based on PTB version II (e.g., argM-LOC and argMTMP). PropBank has been used as the training data in recent semantic role labeling competitions as part of the Conferences on Computational Natural Language Learning (Carreras and M`arquez 2004, 2005). Thus, it is likely to become as influential as FrameNet in computational semantics. The Preposition Project similarly adds information to an existing semantic role resource, namely FrameNet. It is being developed by CL Research (Litkowski and Hargraves 2006) and endeavors to provide comprehensive syntactic and semantic information on various usages of prepositions, which often are not represented well in semantic lexicons (e.g., they are not included at all in WordNet). The Preposition Project uses the sense distinctions from the Oxford Dictionary of English and integrates syntactic information about prepositions from comprehensive grammar references. 156 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation 2.4 Inventories for Knowledge Representation This section describes three case inventories: one developed for the Cyc KB (Len</context>
</contexts>
<marker>Litkowski, Hargraves, 2006</marker>
<rawString>Litkowski, Kenneth C. and Orin Hargraves. 2006. Coverage and inheritance in The Preposition Project. In Third ACL-SIGSEM Workshop on Prepositions, pages 37–44, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval-2007 task 06: Word-sense disambiguation of prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>24--29</pages>
<location>Prague.</location>
<contexts>
<context position="90614" citStr="Litkowski and Hargraves 2007" startWordPosition="13626" endWordPosition="13629">al collocation rather than a single feature per class. That is, they use Over-Range Binary rather than Per-Class Binary (Wiebe, McKeever, and Bruce 1998). Moreover, they include several hundred of these features, rather than our seven (benefactive previously included), which is likely the main source of improvement. Again, the per-class binary organization is a bag of words approach, so it works well only with a limited number of potential collocations. Follow-up work of theirs (Ye and Baldwin 2007) fared well in the recent preposition disambiguation competition, held as part of SemEval-2007 (Litkowski and Hargraves 2007). Thus, an immediate area for future work will be to incorporate such improved feature sets. We will also investigate addressing sentential role constraints as in general semantic role tagging. 5. Conclusion This article shows how to exploit semantic role resources for preposition disambiguation. Information about two different types of semantic role resources is provided. The emphasis is on corpus-based resources providing annotations of naturally occurring text. The Penn Treebank (Marcus et al. 1994) covers general roles for verbal adjuncts and FrameNet (Fillmore, Wooters, and Baker 2001) in</context>
</contexts>
<marker>Litkowski, Hargraves, 2007</marker>
<rawString>Litkowski, Kenneth C. and Orin Hargraves. 2007. SemEval-2007 task 06: Word-sense disambiguation of prepositions. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 24–29, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rey-Long Liu</author>
<author>Von-Wun Soo</author>
</authors>
<title>An empirical study on thematic knowledge acquisition based on syntactic clues and heuristics.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL-93),</booktitle>
<pages>243--250</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="85543" citStr="Liu and Soo (1993)" startWordPosition="12832" endWordPosition="12835">and the sense with the most support is chosen. The work here addresses relation disambiguation specifically with respect to those indicated by prepositional phrases (i.e., preposition word-sense disambiguation). Until recently, there has been little work on general-purpose preposition disambiguation. Litkowski (2002) and Srihari, Niu, and Li (2001) present approaches using manually derived rules. Both approaches account for only a handful of prepositions; in contrast, for FrameNet we disambiguate 32 prepositions via individual classifiers and over 100 prepositions via the combined classifier. Liu and Soo (1993) present a heuristic approach for relation disambiguation relying upon syntactic clues as well as occurrence of specific prepositions. They assign roles to constituents of a sentence from corpus data provided that sufficient instances are available. Otherwise, a human trainer is used to answer questions needed by the system for the assignment. They report an 86% accuracy rate for the assignment of roles to verbal arguments in about 5,000 processed sentences. Alam (2004) sketches out how the preposition over might be disambiguated into one of a dozen roles using features based on the head and c</context>
</contexts>
<marker>Liu, Soo, 1993</marker>
<rawString>Liu, Rey-Long and Von-Wun Soo. 1993. An empirical study on thematic knowledge acquisition based on syntactic clues and heuristics. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL-93), pages 243–250, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, Christopher D. and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language</booktitle>
<contexts>
<context position="5586" citStr="Marcus et al. 1994" startWordPosition="797" endWordPosition="800">ress such coverage problems in lexicons, we have developed an empirical approach to lexical acquisition, building upon earlier knowledge-based approaches in dictionary definition analysis (Wilks, Slator, and Guthrie 1996). This involves a two-step process: Definitions are first analyzed with a broad-coverage parser, and then the resulting syntactic relationships are disambiguated using statistical classification. A crucial part of this process is the disambiguation of prepositions, exploiting online resources with semantic role usage information. The main resources are the Penn Treebank (PTB; Marcus et al. 1994) and FrameNet (Fillmore, Wooters, and Baker 2001), two popular corpora providing rich annotations on English text, such as the semantic roles associated with prepositional phrases in context. In addition to the semantic role annotations from PTB and FrameNet, traditional knowledge bases (KBs) are utilized to provide training data for the relation classification. In particular, the Factotum KB (Cassidy 2000) is used to provide additional training data for prepositions that are used to convey particular relationships. Information on preposition usage is not explicitly encoded in Factotum, so a n</context>
<context position="10940" citStr="Marcus et al. 1994" startWordPosition="1594" endWordPosition="1597">nalysis in computational linguistics, there has been a shift away from relying on explicitly-coded knowledge towards the use of knowledge inferred from naturally occurring text, in particular text that has been annotated by humans to indicate phenomena of interest. For example, rather than manually developing rules for preferring one sense of a word over another based on context, the most successful approaches have automatically learned the rules based on word-sense annotations, as evidenced by the Senseval competitions (Kilgarriff 1998; Edmonds and Cotton 2001). The Penn Treebank version II (Marcus et al. 1994) provided the first large-scale set of case annotations for general-purpose text. These are very general roles, following Fillmore (1968). The Berkeley FrameNet (Fillmore, Wooters, and Baker 2001) project currently provides the most comprehensive set of semantic roles annotations. These are at a much finer granularity than those in PTB, making them quite useful for applications learning semantics from corpora. Relation disambiguation experiments for both of these role inventories are presented subsequently. 2.2.1 Penn Treebank. The original PTB (Marcus, Santorini, and Marcinkiewicz 1993) provi</context>
<context position="65539" citStr="Marcus et al. 1994" startWordPosition="9731" endWordPosition="9734">9 whole .818 .932 .871 .807 .932 .865 .819 .941 .875 Similarly, WordNet does not indicate this, but it does include definition glosses. For example, Factotum: (drying, is-function-of, drier) (10) WordNet: dryalter remove the moisture from and make dry dryerappliance an appliance that removes moisture These definition glosses might be useful in certain cases for inferring the relation markers (i.e., generalized case markers). As is, Factotum cannot be used to provide training data for learning how the relations are expressed in English. This contrasts with corpusbased annotations, such as PTB (Marcus et al. 1994) and FrameNet (Fillmore, Wooters, and Baker 2001), where the relationships are marked in context. 3.4.1 Inferring Semantic Role Markers. To overcome the lack of context in Factotum, the relation markers are inferred through corpus checks, in particular through proximity searches involving the source and target terms from the relationship (i.e., (source, 172 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation Table 17 Per-preposition disambiguation results over FrameNet roles. A separate classifier is used for each preposition, excluding roles with less than 1% relative frequen</context>
<context position="68671" citStr="Marcus et al. 1994" startWordPosition="10231" endWordPosition="10234">.762 43.8 67.813 67.453 65.966 relation, target)). For example, using AltaVista’s Boolean search,8 this can be done via “source NEAR target.” Unfortunately, this technique would require detailed post-processing of the Web search results, possibly including parsing, in order to extract the patterns. As an expedient, common prepositions9 are included in a series of proximity searches to find 8 AltaVista’s Boolean search is available at www.altavista.com/sites/search/adv. 9 The common prepositions are determined from the prepositional phrases assigned functional annotations in the Penn Treebank (Marcus et al. 1994). 173 Computational Linguistics Volume 35, Number 2 the preposition occurring most frequently with the given terms. For instance, given the relationship (drying, is-function-of, drier), the following searches would be performed. (11) drying NEAR drier NEAR in drying NEAR drier NEAR to ... drying NEAR drier NEAR “around” To account for prepositions that occur frequently (e.g., of ), pointwise mutual information (MI) statistics (Manning and Sch¨utze 1999, pages 66–68) are used in place of the raw frequency when rating the potential markers. These are calculated as follows: P(X,Y) f (source NEAR </context>
<context position="91121" citStr="Marcus et al. 1994" startWordPosition="13700" endWordPosition="13703">n the recent preposition disambiguation competition, held as part of SemEval-2007 (Litkowski and Hargraves 2007). Thus, an immediate area for future work will be to incorporate such improved feature sets. We will also investigate addressing sentential role constraints as in general semantic role tagging. 5. Conclusion This article shows how to exploit semantic role resources for preposition disambiguation. Information about two different types of semantic role resources is provided. The emphasis is on corpus-based resources providing annotations of naturally occurring text. The Penn Treebank (Marcus et al. 1994) covers general roles for verbal adjuncts and FrameNet (Fillmore, Wooters, and Baker 2001) includes a wide range of domain-specific roles for all verbal arguments. In addition, semantic role inventories from knowledge bases are investigated. Cyc (Lehmann 1996) provides fine-grained role distinctions, Factotum (Cassidy 2000) includes a variety of functional relations, and work in Conceptual Graphs (Sowa 1999) emphasizes roles for attributes. Relations from both types of resources are considered when developing the inventory of relations used for definition analysis, as shown in Table 8. The dis</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the ARPA Human Language</rawString>
</citation>
<citation valid="false">
<authors>
<author>Technology Workshop</author>
</authors>
<pages>110--115</pages>
<location>Plainsboro, NJ.</location>
<marker>Workshop, </marker>
<rawString>Technology Workshop, pages 110–115, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann</author>
</authors>
<title>Marcinkiewicz.1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date></date>
<marker>Marcus, Santorini, Ann, </marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz.1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING-2002),</booktitle>
<pages>1--7</pages>
<contexts>
<context position="48328" citStr="Mihalcea 2002" startWordPosition="7086" endWordPosition="7087">istics about hypernym rank, and so on. 3.1.2 Classification Experiments. A supervised approach for word-sense disambiguation is used following Bruce and Wiebe (1999). For each experiment, stratified 10-fold cross validation is used: The classifiers are repeatedly trained on 90% of the data and tested on the remainder, with the test sets randomly selected to form a partition. The results described here were obtained using the settings in Figure 4, which are similar to the settings used by O’Hara et al. (2004) in the third Senseval competition. The top systems from recent Senseval competitions (Mihalcea 2002; Grozea 2004) use a variety of lexical features for WSD. Words in the immediate context (Word±i) and their parts of speech (POS±i) are standard features. Word collocations are also common, but there are various ways of organizing collocations into features (Wiebe, McKeever, and Bruce 1998). We use the simple approach of having a single binary feature per sense (e.g., role) that is set true whenever any of the associated collocation words for that sense are encountered (i.e., per-class-binary). The main difference of our approach from more typical WSD systems (Mihalcea, Chklovski, and Kilgarri</context>
<context position="84419" citStr="Mihalcea (2002)" startWordPosition="12670" endWordPosition="12671">tead just select the first sense for their hypernym features for relation classification. They report marginal improvements using the features, whereas configurations with hypernym collocations usually perform best in our preposition disambiguation experiments. Mohit and Narayanan (2003) use WordNet hypernyms to generalize patterns for information extraction inferred from FrameNet annotations by distributing support from terms co-occurring in annotations for frame elements to the terms for hypernyms. However, they do not incorporate a filtering stage, as with our conditional probability test. Mihalcea (2002) shows how hypernym information can be useful in deriving clues for unsupervised WSD. Patterns for co-occurring words of a given sense are induced from sense-tagged corpora. Each pattern specifies templates for the co-occurring words in the immediate context window of the target word, as well as their corresponding synsets if known (e.g., sense tagged or unambiguous), and similarly the hypernym synsets if known. To disambiguate a word, the patterns for each of its senses are evaluated in the context, and the sense with the most support is chosen. The work here addresses relation disambiguation</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Mihalcea, Rada. 2002. Instance based learning with automatic feature selection applied to word sense disambiguation. In Proceedings of the 19th International Conference on Computational Linguistics (COLING-2002), pages 1–7, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>25--28</pages>
<location>Barcelona.</location>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Mihalcea, Rada, Timothy Chklovski, and Adam Kilgarriff. 2004. The Senseval-3 English lexical sample task. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 25–28, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<note>Special Issue on WordNet.</note>
<contexts>
<context position="4214" citStr="Miller et al. 1990" startWordPosition="597" endWordPosition="600"> for temporal). Instead, we infer this based on lexical associations learned from annotated corpora. The motivation for preposition disambiguation is to support a system for lexical acquisition (O’Hara 2005). The focus of the system is to acquire distinguishing information for the concepts serving to define words. Large-scale semantic lexicons mainly emphasize the taxonomic relations among the underlying concepts (e.g., is-a and partof), and often lack sufficient differentiation among similar concepts (e.g., via attributes or functional relations such as is-used-for). For example, in WordNet (Miller et al. 1990), the standard lexical resource for natural language processing, the only relations for beagle and Afghan are that they are both a type of hound. Although the size difference can be inferred from the definitions, it is not represented in the WordNet semantic network. In WordNet, words are grouped into synonym sets called synsets, which represent the underlying concepts and serve as nodes in a semantic network. Synsets are ordered into a hierarchy using the hypernym relation (i.e., is-a). There are several other semantic relations, such as part-whole, is-similar-to, and domain-of. Nonetheless, </context>
<context position="9403" citStr="Miller et al. 1990" startWordPosition="1358" endWordPosition="1361">ntories in NLP, along with criteria for the qualitative evaluation of case systems (generality, completeness, and uniqueness). Linguistic work on thematic roles tends to use a limited number of roles. Frawley (1992) presents a detailed discussion of twelve thematic roles and discusses how they are realized in different languages. During the shift in emphasis away from systems that work in small, self-contained domains to those that can handle open-ended domains, there has been a trend towards the use of larger sets of semantic primitives (Wilks, Slator, and Guthrie 1996). The WordNet lexicon (Miller et al. 1990) serves as one example of this. A synset is defined in terms of its relations with any of the other 100,000+ synsets, rather than in terms of a set of features like [±ANIMATE]. There has also been a shift in focus from deep understanding (e.g., story comprehension) facilitated by specially constructed KBs to shallow surface-level analysis (e.g., text extraction) facilitated by corpus analysis. Both trends seem to be behind the increase in case inventories in two relatively recent resources, namely FrameNet (Fillmore, Wooters, and Baker 2001) and OpenCyc (OpenCyc 2002), both of which define wel</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, George A., Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4): Special Issue on WordNet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine Miller</author>
</authors>
<title>Modifiers in WordNet.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database.</booktitle>
<pages>47--67</pages>
<editor>In Christiane Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="30440" citStr="Miller 1998" startWordPosition="4465" endWordPosition="4466">e restrictions. Factotum averages 1.8 properties per concept, thus complementing WordNet in terms of information content.4 2.5 Combining the Different Semantic Role Inventories It is difficult to provide precise comparisons of the five inventories just discussed. This is due both to the different nature of the inventories (e.g., developed for knowledge bases as opposed to being derived from natural language annotations) and due to the way the 3 In WordNet, the is-similar-to relation for adjectives can be considered as hierarchical, as it links satellite synsets to heads of adjective clusters (Miller 1998). For example, the satellite synsets for “thirsty” and “rainless” are both linked to the head synset for “dry (vs. wet).” 4 These figures are derived by counting the number of relations excluding the instance and subset ones and then dividing by the number of concepts (i.e., ratio of non-hierarchical relations to concepts). Cyc’s comments and lexical assertions are also excluded, as these are implicit in Factotum and WordNet. WordNet’s is-derived-from relations are omitted as lexical in nature (the figure otherwise would be 1.6). 160 O’Hara and Wiebe Exploiting Resources for Preposition Disamb</context>
</contexts>
<marker>Miller, 1998</marker>
<rawString>Miller, Katherine. 1998. Modifiers in WordNet. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, pages 47–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Srini Narayanan</author>
</authors>
<title>Semantic extraction with wide-coverage lexical resources.</title>
<date>2003</date>
<booktitle>In Companion Volume of the Proceedings of HLT-NAACL</booktitle>
<pages>64--66</pages>
<location>Edmonton.</location>
<contexts>
<context position="84092" citStr="Mohit and Narayanan (2003)" startWordPosition="12623" endWordPosition="12626">.559 .661 .493 .563 .642 .482 .549 resource .613 .471 .530 .614 .458 .524 .618 .479 .539 source .703 .936 .802 .697 .936 .799 .707 .937 .806 theme .545 .660 .596 .511 .661 .576 .567 .637 .600 time .619 .624 .621 .626 .612 .619 .628 .611 .619 178 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation and Jurafsky (2002) instead just select the first sense for their hypernym features for relation classification. They report marginal improvements using the features, whereas configurations with hypernym collocations usually perform best in our preposition disambiguation experiments. Mohit and Narayanan (2003) use WordNet hypernyms to generalize patterns for information extraction inferred from FrameNet annotations by distributing support from terms co-occurring in annotations for frame elements to the terms for hypernyms. However, they do not incorporate a filtering stage, as with our conditional probability test. Mihalcea (2002) shows how hypernym information can be useful in deriving clues for unsupervised WSD. Patterns for co-occurring words of a given sense are induced from sense-tagged corpora. Each pattern specifies templates for the co-occurring words in the immediate context window of the </context>
</contexts>
<marker>Mohit, Narayanan, 2003</marker>
<rawString>Mohit, Behrang and Srini Narayanan. 2003. Semantic extraction with wide-coverage lexical resources. In Companion Volume of the Proceedings of HLT-NAACL 2003 - Short Papers, pages 64–66, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P O’Hara</author>
</authors>
<title>Empirical acquisition of conceptual distinctions via dictionary definitions.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, New Mexico State University.</institution>
<marker>O’Hara, 2005</marker>
<rawString>O’Hara, Thomas P. 2005. Empirical acquisition of conceptual distinctions via dictionary definitions. Ph.D. thesis, Department of Computer Science, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom O’Hara</author>
<author>Rebecca Bruce</author>
<author>Jeff Donner</author>
<author>Janyce Wiebe</author>
</authors>
<title>Class-based collocations for word-sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>199--202</pages>
<location>Barcelona.</location>
<marker>O’Hara, Bruce, Donner, Wiebe, 2004</marker>
<rawString>O’Hara, Tom, Rebecca Bruce, Jeff Donner, and Janyce Wiebe. 2004. Class-based collocations for word-sense disambiguation. In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 199–202, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom O’Hara</author>
<author>Janyce Wiebe</author>
</authors>
<title>Classifying functional relations in Factotum via WordNet hypernym associations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2003),</booktitle>
<pages>347--359</pages>
<location>Mexico City.</location>
<marker>O’Hara, Wiebe, 2003</marker>
<rawString>O’Hara, Tom and Janyce Wiebe. 2003. Classifying functional relations in Factotum via WordNet hypernym associations. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2003), pages 347–359, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>OpenCyc</author>
</authors>
<title>release 0.6b. Available at www.opencyc.org.</title>
<date>2002</date>
<contexts>
<context position="9977" citStr="OpenCyc 2002" startWordPosition="1451" endWordPosition="1452">he WordNet lexicon (Miller et al. 1990) serves as one example of this. A synset is defined in terms of its relations with any of the other 100,000+ synsets, rather than in terms of a set of features like [±ANIMATE]. There has also been a shift in focus from deep understanding (e.g., story comprehension) facilitated by specially constructed KBs to shallow surface-level analysis (e.g., text extraction) facilitated by corpus analysis. Both trends seem to be behind the increase in case inventories in two relatively recent resources, namely FrameNet (Fillmore, Wooters, and Baker 2001) and OpenCyc (OpenCyc 2002), both of which define well over a hundred case roles. However, provided that the case roles are well structured in an inheritance hierarchy, both paraphrasability and coverage can be addressed by the same inventory. 153 Computational Linguistics Volume 35, Number 2 2.2 Inventories Developed for Corpus Annotation With the emphasis on corpus analysis in computational linguistics, there has been a shift away from relying on explicitly-coded knowledge towards the use of knowledge inferred from naturally occurring text, in particular text that has been annotated by humans to indicate phenomena of </context>
</contexts>
<marker>OpenCyc, 2002</marker>
<rawString>OpenCyc. 2002.OpenCyc release 0.6b. Available at www.opencyc.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, Martha, Dan Gildea, and Paul Kingsbury. 2005. The Proposition Bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Stone Palmer</author>
</authors>
<title>Semantic Processing for Finite Domains.</title>
<date>1990</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="8495" citStr="Palmer (1990)" startWordPosition="1225" endWordPosition="1226">lation intended to capture the semantics of sentences (e.g., event participation). Which semantic roles are used varies widely in Natural Language Processing (NLP). Some systems use just a small number of very general roles, such as beneficiary. At the other extreme, some systems use quite specific roles tailored to a particular domain, such as catalyst in the chemical sense. 2.1 Background on Semantic Roles Bruce (1975) presents an account of early case systems in NLP. For the most part, those systems had limited case role inventories, along the lines of the cases defined by Fillmore (1968). Palmer (1990) discusses some of the more contentious issues regarding case systems, including adequacy for representation, such as reliance solely upon case information to determine semantics versus the use of additional inference mechanisms. Barker (1998) provides a comprehensive summary of case inventories in NLP, along with criteria for the qualitative evaluation of case systems (generality, completeness, and uniqueness). Linguistic work on thematic roles tends to use a limited number of roles. Frawley (1992) presents a detailed discussion of twelve thematic roles and discusses how they are realized in </context>
</contexts>
<marker>Palmer, 1990</marker>
<rawString>Palmer, Martha Stone. 1990. Semantic Processing for Finite Domains. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="88932" citStr="Pradhan et al. (2005)" startWordPosition="13361" endWordPosition="13364"> head (because prepositions are considered as the head by their parser). As their classifier tags all adjuncts, they include the nominal and adverbial roles, which are syntactic and more predictable than the roles occurring with prepositional phrases. There have been recent workshops featuring competitions for semantic role tagging (Carreras and M`arquez 2004, 2005; Litkowski 2004). A common approach is to tag all the semantic roles in a sentence at the same time to account for dependencies, such as via Hidden Markov Models. To take advantage of accurate Support Vector Machine classification, Pradhan et al. (2005) instead use a postprocessing phrase based on trigram models of roles. Their system incorporates a large variety of features, building upon several different preceding approaches, such as including extensions to the path features from Gildea and Jurafsky (2002). Their lexical features include the predicate root word, headwords for the sentence constituents and PPs, as well as their first and last words. Koomen et al. (2005) likewise use a large feature set. They use an optimization phase to maximize satisfaction of the constraints imposed by the PropBank data set, such as the number of argumen</context>
<context position="93457" citStr="Pradhan et al. (2005)" startWordPosition="14040" endWordPosition="14043">entories discussed in the article. This allows for improved classification in cases where inventories provide overly specialized relations, such as those in FrameNet. Classification results are shown in Table 19. The recent competitions on semantic relation labeling have highlighted the usefulness of incorporating a variety of clues for general-purpose relation disambiguation (Carreras and M`arquez 2005). Some of the techniques developed here for preposition disambiguation can likely help with relation disambiguation in general. For instance, there are quite a few lexical features, such as in Pradhan et al. (2005), which could be extended to use semantic classes as with our hypernym collocations. In general it seems that, when lexical features are used in supervised machine learning, it is likely that corresponding class-based features based on hypernyms can be beneficial for improved coverage. Other aspects of this approach are geared specifically to our goal of supporting lexical acquisition from dictionaries, which was the motivation for the emphasis on preposition disambiguation. Isolating the preposition annotations allows the classifiers to be more readily tailored to definition analysis, especia</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning, 60(1–3):11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="49734" citStr="Quinlan 1993" startWordPosition="7295" endWordPosition="7296"> within five words of the target Features: Prep: preposition being classified POS±i: part-of-speech of word at offset i Word±i: stem of word at offset i WordCollr: context has word collocation for role r HypernymCollr: context has hypernym collocation for role r Collocation context: Word: anywhere in the sentence Hypernym: within 5 words of target preposition Collocation selection: Frequency: f (word) &gt; 1 Conditional probability: P(C|coll) ≥ .50 Relative percent change: (P(C|coll) − P(C))/P(C) ≥ .20 Organization: per-class-binary Model selection: C4.5 Decision tree via Weka’s J4.8 classifier (Quinlan 1993; Witten and Frank 1999) Figure 4 Feature settings used in preposition classification experiments. Aspects that differ from a typical WSD system are italicized. 166 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation prepositions (i.e., a five-word context window).7 This reduced window size is used to make the hypernym collocations more related to the prepositional object and the modified term. The feature settings in Figure 4 are used in three different configurations: wordbased collocations alone, hypernym collocations alone, and both collocations together. Combining the two</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J. Ross. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sam Scott</author>
<author>Stan Matwin 1998</author>
</authors>
<title>Text classification using WordNet hypernyms.</title>
<booktitle>In Proceedings of the COLING/ACL Workshop on Usage of WordNet in Natural Language Processing Systems,</booktitle>
<pages>38--44</pages>
<location>Montreal.</location>
<marker>Scott, 1998, </marker>
<rawString>Scott, Sam and Stan Matwin.1998. Text classification using WordNet hypernyms. In Proceedings of the COLING/ACL Workshop on Usage of WordNet in Natural Language Processing Systems, pages 38–44, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold L Somers</author>
</authors>
<title>Valency and Case in Computational Linguistics.</title>
<date>1987</date>
<publisher>Edinburgh University Press,</publisher>
<location>Scotland.</location>
<contexts>
<context position="21454" citStr="Somers (1987)" startWordPosition="3176" endWordPosition="3177">role). Table 3 shows the most commonly used event-based roles in the KB. 2.4.2 Conceptual Graphs. The Conceptual Graphs (CG) mechanism was introduced by Sowa (1984) for knowledge representation as part of his Conceptual Structures theory. The original text listed two dozen or so thematic relations, such as destination and initiator. In all, 37 conceptual relations were defined. This inventory formed the basis for most work in Conceptual Graphs. Recently, Sowa (1999) updated the inventory to allow for better hierarchical structuring and to incorporate the important thematic roles identified by Somers (1987). Table 4 shows a sample of these roles, along with usage estimates based on corpus analysis (O’Hara 2005). 2.4.3 Factotum. The Factotum semantic network (Cassidy 2000) developed by Micra, Inc., makes explicit many of the relations in Roget’s Thesaurus.1 Outside of proprietary resources such as Cyc, Factotum is the most comprehensive KB with respect to functional relations, which are taken here to be non-hierarchical relations, excluding attributes. OpenCyc does include definitions of many non-hierarchical relations. However, there are not many instantiations (i.e., relationship assertions), b</context>
</contexts>
<marker>Somers, 1987</marker>
<rawString>Somers, Harold L. 1987. Valency and Case in Computational Linguistics. Edinburgh University Press, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Sowa</author>
</authors>
<date>1984</date>
<booktitle>Conceptual Structures in Mind and Machines.</booktitle>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="18951" citStr="Sowa 1984" startWordPosition="2800" endWordPosition="2801">c and semantic information on various usages of prepositions, which often are not represented well in semantic lexicons (e.g., they are not included at all in WordNet). The Preposition Project uses the sense distinctions from the Oxford Dictionary of English and integrates syntactic information about prepositions from comprehensive grammar references. 156 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation 2.4 Inventories for Knowledge Representation This section describes three case inventories: one developed for the Cyc KB (Lenat 1995), one used to define Conceptual Graphs (Sowa 1984), and one for the Factotum KB (Cassidy 2000). The first two are based on a traditional knowledge representation paradigm. With respect to natural language processing, these approaches are more representative of the earlier approaches in which deep understanding is the chief goal. Factotum is also based on a knowledge representation paradigm, but in a sense also reflects the empirical aspect of the corpus annotation approach, because the annotations were developed to address the relations implicit in Roget’s Thesaurus. In this article, relation disambiguation experiments are only presented for </context>
<context position="21005" citStr="Sowa (1984)" startWordPosition="3110" endWordPosition="3111"> ResearchCyc, which contains most of the KB except for proprietary information (e.g., internal bookkeeping assertions). Cyc uses a wide range of role types: very general roles (e.g., beneficiary); commonly occurring situational roles (e.g., victim); and highly specialized roles (e.g., catalyst). Of the 8,756 concepts in OpenCyc, 130 are for event-based roles (i.e., instances of actorslot) with 51 other semantic roles (i.e., other instances of role). Table 3 shows the most commonly used event-based roles in the KB. 2.4.2 Conceptual Graphs. The Conceptual Graphs (CG) mechanism was introduced by Sowa (1984) for knowledge representation as part of his Conceptual Structures theory. The original text listed two dozen or so thematic relations, such as destination and initiator. In all, 37 conceptual relations were defined. This inventory formed the basis for most work in Conceptual Graphs. Recently, Sowa (1999) updated the inventory to allow for better hierarchical structuring and to incorporate the important thematic roles identified by Somers (1987). Table 4 shows a sample of these roles, along with usage estimates based on corpus analysis (O’Hara 2005). 2.4.3 Factotum. The Factotum semantic netwo</context>
</contexts>
<marker>Sowa, 1984</marker>
<rawString>Sowa, John F. 1984. Conceptual Structures in Mind and Machines. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Sowa</author>
</authors>
<title>Knowledge Representation: Logical, Philosophical, and Computational Foundations.</title>
<date>1999</date>
<publisher>Brooks Cole Publishing,</publisher>
<location>Pacific Grove, CA.</location>
<contexts>
<context position="21311" citStr="Sowa (1999)" startWordPosition="3156" endWordPosition="3157">756 concepts in OpenCyc, 130 are for event-based roles (i.e., instances of actorslot) with 51 other semantic roles (i.e., other instances of role). Table 3 shows the most commonly used event-based roles in the KB. 2.4.2 Conceptual Graphs. The Conceptual Graphs (CG) mechanism was introduced by Sowa (1984) for knowledge representation as part of his Conceptual Structures theory. The original text listed two dozen or so thematic relations, such as destination and initiator. In all, 37 conceptual relations were defined. This inventory formed the basis for most work in Conceptual Graphs. Recently, Sowa (1999) updated the inventory to allow for better hierarchical structuring and to incorporate the important thematic roles identified by Somers (1987). Table 4 shows a sample of these roles, along with usage estimates based on corpus analysis (O’Hara 2005). 2.4.3 Factotum. The Factotum semantic network (Cassidy 2000) developed by Micra, Inc., makes explicit many of the relations in Roget’s Thesaurus.1 Outside of proprietary resources such as Cyc, Factotum is the most comprehensive KB with respect to functional relations, which are taken here to be non-hierarchical relations, excluding attributes. Ope</context>
<context position="25681" citStr="Sowa (1999" startWordPosition="3795" endWordPosition="3796">ver 93,000 assertions. Some of these are quite specialized (e.g., has-brandname). In addition, there are quite a few inverse relations, because most of the relations are not symmetrical. Certain features of the knowledge representation are ignored during the relation extraction used later. For example, relation specifications can have qualifier prefixes, such as an ampersand to indicate that the relationship only sometimes holds. 158 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation Table 4 Common semantic roles used in Conceptual Graphs. Inventory and descriptions based on Sowa (1999, pages 502–510). The term situation is used in place of Sowa’s nexus (i.e., “fact of togetherness”), which also covers spatial structures. Freq. gives estimated relative frequencies from O’Hara (2005). Role Freq. agent .267 attribute .155 characteristic .080 theme .064 patient .061 location .053 possession .035 part .035 origin .035 experiencer .035 result .032 instrument .027 recipient .019 destination .013 point-in-time .011 path .011 accompaniment .011 effector .008 beneficiary .008 matter .005 manner .005 source .003 resource .003 product .003 medium .003 goal .003 duration .003 because .</context>
<context position="37491" citStr="Sowa 1999" startWordPosition="5466" endWordPosition="5467">including a few roles based on the PTB, Cyc, and Conceptual Graphs invenTable 7 Abstract roles defined by Gildea and Jurafsky based on FrameNet. Taken from Gildea and Jurafsky (2002). agent cause degree experiencer force goal instrument location manner null path patient percept proposition result source state topic 162 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation Table 8 Inventory of semantic relations for definition analysis. This inventory is inspired by the roles in Table 7 and is primarily based on FrameNet (Fillmore, Wooters, and Baker 2001) and Conceptual Graphs (Sowa 1999); it also includes roles based on the PTB and Cyc inventories. Relation Description accompaniment entity that participates with another entity agent entity voluntarily performing an action amount quantity used as a measure of some characteristic area region in which the action takes place category general type or class of which the item is an instance cause non-agentive entity that produces an effect characteristic general properties of entities context background for situation or predication direction either spatial source or goal (same as in PTB) distance spatial extent of motion duration pe</context>
<context position="91532" citStr="Sowa 1999" startWordPosition="13760" endWordPosition="13761">n about two different types of semantic role resources is provided. The emphasis is on corpus-based resources providing annotations of naturally occurring text. The Penn Treebank (Marcus et al. 1994) covers general roles for verbal adjuncts and FrameNet (Fillmore, Wooters, and Baker 2001) includes a wide range of domain-specific roles for all verbal arguments. In addition, semantic role inventories from knowledge bases are investigated. Cyc (Lehmann 1996) provides fine-grained role distinctions, Factotum (Cassidy 2000) includes a variety of functional relations, and work in Conceptual Graphs (Sowa 1999) emphasizes roles for attributes. Relations from both types of resources are considered when developing the inventory of relations used for definition analysis, as shown in Table 8. The disambiguation concentrates on relations indicated by prepositional phrases, and is framed as word-sense disambiguation for the preposition in question. A new 180 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation type of feature for word-sense disambiguation is introduced, using WordNet hypernyms as collocations rather than just words, as is typically done. The full feature set is shown in Fi</context>
</contexts>
<marker>Sowa, 1999</marker>
<rawString>Sowa, John F. 1999. Knowledge Representation: Logical, Philosophical, and Computational Foundations. Brooks Cole Publishing, Pacific Grove, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohini Srihari</author>
<author>Cheng Niu</author>
<author>Wei Li</author>
</authors>
<title>A hybrid approach for named entity and sub-type tagging.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference,</booktitle>
<pages>247--254</pages>
<location>Seattle.</location>
<marker>Srihari, Niu, Li, 2001</marker>
<rawString>Srihari, Rohini, Cheng Niu, and Wei Li. 2001. A hybrid approach for named entity and sub-type tagging. In Proceedings of the 6th Applied Natural Language Processing Conference, pages 247–254, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Kenneth McKeever</author>
<author>Rebecca F Bruce</author>
</authors>
<title>Mapping collocational properties into machine learning features.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<booktitle>In Proceedings of the 6th Workshop on Very Large Corpora (WVLC-98),</booktitle>
<volume>35</volume>
<pages>225--233</pages>
<location>Montreal.</location>
<marker>Wiebe, McKeever, Bruce, 1998</marker>
<rawString>Wiebe, Janyce, Kenneth McKeever, and Rebecca F. Bruce. 1998. Mapping collocational properties into machine learning features. In Proceedings of the 6th Workshop on Very Large Corpora (WVLC-98), pages 225–233, Montreal. Computational Linguistics Volume 35, Number 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Brian M Slator</author>
<author>Louise Guthrie</author>
</authors>
<title>Electric Words.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Wilks, Slator, Guthrie, 1996</marker>
<rawString>Wilks, Yorick, Brian M. Slator, and Louise Guthrie. 1996. Electric Words. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="49758" citStr="Witten and Frank 1999" startWordPosition="7297" endWordPosition="7300">ords of the target Features: Prep: preposition being classified POS±i: part-of-speech of word at offset i Word±i: stem of word at offset i WordCollr: context has word collocation for role r HypernymCollr: context has hypernym collocation for role r Collocation context: Word: anywhere in the sentence Hypernym: within 5 words of target preposition Collocation selection: Frequency: f (word) &gt; 1 Conditional probability: P(C|coll) ≥ .50 Relative percent change: (P(C|coll) − P(C))/P(C) ≥ .20 Organization: per-class-binary Model selection: C4.5 Decision tree via Weka’s J4.8 classifier (Quinlan 1993; Witten and Frank 1999) Figure 4 Feature settings used in preposition classification experiments. Aspects that differ from a typical WSD system are italicized. 166 O’Hara and Wiebe Exploiting Resources for Preposition Disambiguation prepositions (i.e., a five-word context window).7 This reduced window size is used to make the hypernym collocations more related to the prepositional object and the modified term. The feature settings in Figure 4 are used in three different configurations: wordbased collocations alone, hypernym collocations alone, and both collocations together. Combining the two types generally produce</context>
<context position="54419" citStr="Witten and Frank 1999" startWordPosition="7999" endWordPosition="8002">oral interpretation becomes more likely. Therefore, these class-based lexical associations capture commonsense usages of the preposition at. 3.2.2 Results. The classification results for these prepositions in the Penn Treebank show that this approach is very effective. Table 11 shows the accuracy when disambiguating the 14 prepositions using a single classifier with 6 roles. Table 11 also shows the perclass statistics, showing that there are difficulties tagging the manner role (e.g., lowest F-score). For the single-classifier case, the overall accuracy is 89.3%, using Weka’s J4.8 classifier (Witten and Frank 1999), which is an implementation of Quinlan’s (1993) C4.5 decision tree learner. For comparison, Table 12 shows the results for individual classifiers created for the prepositions annotated in PTB. A few prepositions only have small data sets, such as of which is used more for specialization relations (e.g., category) than thematic ones. This table is ordered by entropy, which measures the inherent ambiguity in the classes as given by the annotations. Note that the Baseline column is the probability of the most frequent sense, which is a common estimate of the lower bound for classification Table </context>
<context position="72738" citStr="Witten and Frank 1999" startWordPosition="10844" endWordPosition="10847">ontext: Source and target terms from relationship ((source, relation, target)) Features: POSsource: part-of-speech of the source term POStarget: part-of-speech of the target term Prep: preposition serving as relation marker (“n/a” if not inferable) WordCollr: 1 iff context contains any word collocation for relation r HypernymCollr: 1 iff context contains any hypernym collocation for relation r Collocation selection: Frequency: f (word) &gt; 1 Relative percent change: (P(C|coll) − P(C))/P(C) &gt; .20 Organization: per-class-binary grouping Model selection: Decision tree using Weka’s J4.8 classifier (Witten and Frank 1999) Figure 5 Features used in Factotum role classification experiments. Simplified version of Figure 4: Context only consists of the source and target terms. 3.4.3 Results. To make the task more similar to the PTB and FrameNet cases covered previously, only the functional relations in Factotum are used. These are determined by removing the hierarchical relations (e.g., has-subtype and has-part) along with the attribute relations (e.g., is-property-of). In addition, in cases where there are inverse functions (e.g., causes and is-caused-by), the most frequently occurring relation of each inverse pa</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Witten, Ian H. and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semantic role labeling of prepositional phrases.</title>
<date>2006</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="89633" citStr="Ye and Baldwin (2006)" startWordPosition="13472" endWordPosition="13475">tem incorporates a large variety of features, building upon several different preceding approaches, such as including extensions to the path features from Gildea and Jurafsky (2002). Their lexical features include the predicate root word, headwords for the sentence constituents and PPs, as well as their first and last words. Koomen et al. (2005) likewise use a large feature set. They use an optimization phase to maximize satisfaction of the constraints imposed by the PropBank data set, such as the number of arguments for particular predicates (e.g., just two for stalk, arg0 and arg1). Lastly, Ye and Baldwin (2006) show how filtering can be used to constrain the hypernyms selected to serve as collocations, building upon our earlier work (O’Hara and Wiebe 2003). They report 87.7% accuracy in a setup similar to ours over PTB (i.e., a gain of 2 percentage points). They use a different type of collocation feature than ours: having a binary feature for each potential collocation rather than a single feature per class. That is, they use Over-Range Binary rather than Per-Class Binary (Wiebe, McKeever, and Bruce 1998). Moreover, they include several hundred of these features, rather than our seven (benefactive </context>
<context position="94307" citStr="Ye and Baldwin (2006)" startWordPosition="14160" endWordPosition="14163">sed on hypernyms can be beneficial for improved coverage. Other aspects of this approach are geared specifically to our goal of supporting lexical acquisition from dictionaries, which was the motivation for the emphasis on preposition disambiguation. Isolating the preposition annotations allows the classifiers to be more readily tailored to definition analysis, especially because predicate frames are not assumed as with other FrameNet relation disambiguation. Future work will investigate combining the general relation classifiers with preposition disambiguation classifiers, such as is done in Ye and Baldwin (2006). Future work will also investigate improvements to the application to definition analysis. Currently, FrameNet roles are always mapped to the same common inventory role (e.g., place to location). However, this should account for the frame of the annotation and perhaps other context information. Lastly, we will also look for more resources to exploit for preposition disambiguation (e.g., ResearchCyc). Acknowledgments The experimentation for this article was greatly facilitated though the use of computing resources at New Mexico State University. We are also grateful for the extremely helpful c</context>
</contexts>
<marker>Ye, Baldwin, 2006</marker>
<rawString>Ye, Patrick and Timothy Baldwin. 2006. Semantic role labeling of prepositional phrases. ACM Transactions on Asian Language Information Processing (TALIP), 5(3):228–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>MELB-YB: Preposition sense disambiguation using rich semantic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>241--244</pages>
<location>Prague.</location>
<contexts>
<context position="90489" citStr="Ye and Baldwin 2007" startWordPosition="13609" endWordPosition="13612">entage points). They use a different type of collocation feature than ours: having a binary feature for each potential collocation rather than a single feature per class. That is, they use Over-Range Binary rather than Per-Class Binary (Wiebe, McKeever, and Bruce 1998). Moreover, they include several hundred of these features, rather than our seven (benefactive previously included), which is likely the main source of improvement. Again, the per-class binary organization is a bag of words approach, so it works well only with a limited number of potential collocations. Follow-up work of theirs (Ye and Baldwin 2007) fared well in the recent preposition disambiguation competition, held as part of SemEval-2007 (Litkowski and Hargraves 2007). Thus, an immediate area for future work will be to incorporate such improved feature sets. We will also investigate addressing sentential role constraints as in general semantic role tagging. 5. Conclusion This article shows how to exploit semantic role resources for preposition disambiguation. Information about two different types of semantic role resources is provided. The emphasis is on corpus-based resources providing annotations of naturally occurring text. The Pe</context>
</contexts>
<marker>Ye, Baldwin, 2007</marker>
<rawString>Ye, Patrick and Timothy Baldwin. 2007. MELB-YB: Preposition sense disambiguation using rich semantic features. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 241–244, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Valia Kordoni</author>
<author>Aline Villavicencio</author>
</authors>
<title>Prepositions in Applications: A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and Introduction to the Special Issue.</title>
<date>2009</date>
<journal>Computational Linguistics</journal>
<volume>35</volume>
<pages>119--149</pages>
<note>[Citation] [PDF] [PDF Plus]</note>
<marker>Baldwin, Kordoni, Villavicencio, 2009</marker>
<rawString>1. Timothy Baldwin, Valia Kordoni, Aline Villavicencio. 2009. Prepositions in Applications: A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and Introduction to the Special Issue. Computational Linguistics 35:2, 119-149. [Citation] [PDF] [PDF Plus]</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>