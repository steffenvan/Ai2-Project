<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001451">
<title confidence="0.977978">
Shallow Dependency Labeling
</title>
<author confidence="0.997025">
Manfred Klenner
</author>
<affiliation confidence="0.9988695">
Institute of Computational Linguistics
University of Zurich
</affiliation>
<email confidence="0.98647">
klenner@cl.unizh.ch
</email>
<sectionHeader confidence="0.993656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999710625">
We present a formalization of dependency
labeling with Integer Linear Programming.
We focus on the integration of subcatego-
rization into the decision making process,
where the various subcategorization frames
of a verb compete with each other. A maxi-
mum entropy model provides the weights for
ILP optimization.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894527777778">
Machine learning classifiers are widely used, al-
though they lack one crucial model property: they
can’t adhere to prescriptive knowledge. Take gram-
matical role (GR) labeling, which is a kind of (shal-
low) dependency labeling, as an example: chunk-
verb-pairs are classified according to a GR (cf.
(Buchholz, 1999)). The trials are independent of
each other, thus, local decisions are taken such that
e.g. a unique GR of a verb might (erroneously) get
multiply instantiated etc. Moreover, if there are al-
ternative subcategorization frames of a verb, they
must not be confused by mixing up GR from dif-
ferent frames to a non-existent one. Often, a subse-
quent filter is used to repair such inconsistent solu-
tions. But usually there are alternative solutions, so
the demand for an optimal repair arises.
We apply the optimization method Integer Linear
Programming (ILP) to (shallow) dependency label-
ing in order to generate a globally optimized con-
sistent dependency labeling for a given sentence.
A maximum entropy classifier, trained on vectors
with morphological, syntactic and positional infor-
mation automatically derived from the TIGER tree-
bank (German), supplies probability vectors that are
used as weights in the optimization process. Thus,
the probabilities of the classifier do not any longer
provide (as usually) the solution (i.e. by picking out
the most probable candidate), but count as proba-
bilistic suggestions to a - globally consistent - solu-
tion. More formally, the dependency labeling prob-
lem is: given a sentence with (i) verbs, , (ii) NP
and PP chunksl, , label all pairs (
) with a dependency relation (including a
class for the null assignment) such that all chunks
get attached and for each verb exactly one subcate-
gorization frame is instantiated.
</bodyText>
<sectionHeader confidence="0.963596" genericHeader="introduction">
2 Integer Linear Programming
</sectionHeader>
<bodyText confidence="0.647886">
Integer Linear Programming is the name of a class
of constraint satisfaction algorithms which are re-
stricted to a numerical representation of the problem
to be solved. The objective is to optimize (e.g. max-
imize) a linear equation called the objective function
(a) in Fig. 1) given a set of constraints (b) in Fig. 1):
</bodyText>
<figureCaption confidence="0.999339">
Figure 1: ILP Specification
</figureCaption>
<bodyText confidence="0.988108583333333">
where, and are variables,
,and are constants.
For dependency labeling we have: are binary
class variables that indicate the (non-) assignment of
a chunk to a dependency relation of a subcat
frame of a verb. Thus, three indices are needed:
. If such an indicator variable is set to
1 in the course of the maximization task, then the
dependency label between these chunks is said to
hold, otherwise ( ) it doesn’t hold.
from Fig.1 are interpreted as weights that represent
the impact of an assignment.
</bodyText>
<sectionHeader confidence="0.988858" genericHeader="method">
3 Dependency Labeling with ILP
</sectionHeader>
<bodyText confidence="0.886614333333333">
Given the chunks (NP, PP and verbs) of a sen-
tence, each pair is formed. It can
&apos;Note that we use base chunks instead of heads.
</bodyText>
<page confidence="0.974307">
201
</page>
<note confidence="0.701463">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 201–204,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.99879">
Figure 2: Objective Function
</figureCaption>
<bodyText confidence="0.992598053571428">
stand in one of eight dependency relations, includ-
ing a pseudo relation representing the null class.
We consider the most important dependency labels:
subject ( ), direct object ( ), indirect object (❄),
clausal complement (), prepositional complement
( ), attributive (NP or PP) attachment ( ) and ad-
junct ( ). Although coarse-grained, this set allows
us to capture all functional dependencies and to con-
struct a dependency tree for every sentence in the
corpus2. Technically, indicator variables are used
to represent attachment decisions. Together with a
weight, they form the addend of the objective func-
tion. In the case of attributive modifiers or adjuncts
(the non-governable labels), the indicator variables
correspond to triples. There are two labels of this
type: represents that chunk modifies chunk
and represents that chunk is in an adjunct
relation to chunk. and are defined as the
weighted sum of such pairs (cf. Eq. 1 and Eq 2.
from Fig. 2), the weights (e.g. ) stem from the
statistical model.
For subcategorized labels, we have quadruples,
consisting of a label name , a frame index,
a verb and a chunk (also verb chunks are al-
lowed as a): . We define to be the
weighted sum of all label instantiations of all verbs
(and their subcat frames), see Eq. 3 in Fig. 2.
The subscript is a list of pairs, where each
2Note that we are not interested in dependencies beyond the
(base) chunk level
pair consists of a label and a subcat frame index.
This way, represents all subcat frames of a
verb. For example, of “to believe” could be:
. There
are three frames, the first one requires a and a .
Consider the sentence “He believes these stories”.
We have = believes and = He, believes,
stories . Assume to be the of “to believe” as
defined above. Then, e.g. represents the
assignment of “stories” as the filler of the subject
relation of the second subcat frame of “believes”.
To get a dependency tree, every chunk must find
a head (chunk), except the root verb. We define a
root verb as a verb that stands in the relation
to all other verbs. (cf. Eq.4 from Fig.2) is the
weighted sum of all null assignment decisions. It is
part of the maximization task and thus has an impact
(a weight). The objective function is defined as the
sum of equations 1 to 4 (Eq.5 from Fig.2).
So far, our formalization was devoted to the maxi-
mization task, i.e. which chunks are in a dependency
relation, what is the label and what is the impact.
Without any further (co-occurrence) restrictions, ev-
ery pair of chunks would get related with every la-
bel. In order to assure a valid linguistic model, con-
straints have to be formulated.
</bodyText>
<sectionHeader confidence="0.985057" genericHeader="method">
4 Basic Global Constraints
</sectionHeader>
<bodyText confidence="0.994953333333333">
Every chunk from ( ) must find a head,
that is, be bound either as an attribute, adjunct or a
verb complement. This requires all indicator vari-
ables with as the dependent (second index) to sum
up to exactly 1.
A verb is attached to any other verb either as a
clausal object (of some verb frame) or as (null
class) indicating that there is no dependency relation
between them.
</bodyText>
<equation confidence="0.7791415">
(6)
(7)
</equation>
<page confidence="0.985856">
202
</page>
<bodyText confidence="0.962324">
This does not exclude that a verb gets attached to
several verbs as a. We capture this by constraint 8:
</bodyText>
<equation confidence="0.519987">
(8)
</equation>
<bodyText confidence="0.997130666666667">
Another (complementary) constraint is that a depen-
dency label of a verb must have at most one filler.
We first introduce a indicator variable
</bodyText>
<equation confidence="0.631334">
(9)
</equation>
<bodyText confidence="0.998545375">
In order to serve as an indicator of whether a label
(of a frame of a verb) is active or inactive, we
restrict to be at most 1:
or or both are zero, but if
one of them is set to one, then = 1. Moreover,
as we show in the next section, the selection of the
label indicator variable of a frame enforces the frame
to be selected as well3.
</bodyText>
<sectionHeader confidence="0.477766" genericHeader="method">
5 Subcategorization as a Global Constraint
</sectionHeader>
<bodyText confidence="0.96542525">
The problem with the selection among multiple sub-
cat frames is to guarantee a valid distribution of
chunks to verb frames. We don’t want to have chunk
be labeled according to verb frameand chunk
according to verb frame. Any valid attachment
must be coherent (address one verb frame) and com-
plete (select all of its labels).
We introduce an indicator variable with frame
and verb indices. Since exactly one frame of a verb
has to be active at the end, we restrict:
(11)
( is the number of subcat frames of verb)
However, we would like to couple a verb’s ()
frame () to the frame’s label set and restrict it to
be active (i.e. set to one) only if all of its labels
are active. To achieve this, we require equivalence,
3There are more constraints, e.g. that no two chunks can be
attached to each other symmetrically (being chunk and modifier
of each other at the same time). We won’t introduce them here.
namely that selecting any label of a frame is equiv-
alent to selecting the frame. As defined in equation
10, a label is active, if the label indicator variable
( ) is set to one. Equivalence is represented by
identity, we thus get (cf. constraint 12):
</bodyText>
<equation confidence="0.759446">
(12)
</equation>
<bodyText confidence="0.8235874">
is set to one
(zero) and all other of the same subcat frame
are forced to be one (completeness). Constraint 11
ensures that exactly one subcat frame can be ac-
tive (coherence).
</bodyText>
<sectionHeader confidence="0.809993" genericHeader="method">
6 Maximum Entropy and ILP Weights
</sectionHeader>
<bodyText confidence="0.999266733333333">
A maximum entropy approach was used to induce
a probability model that serves as the basis for the
ILP weights. The model was trained on the TIGER
treebank (Brants et al., 2002) with feature vectors
stemming from the following set of features: the
part of speech tags of the two candidate chunks, the
distance between them in chunks, the number of in-
tervening verbs, the number of intervening punctu-
ation marks, person, case and number features, the
chunks, the direction of the dependency relation (left
or right) and a passive/active voice flag.
The output of the maxent model is for each pair of
chunks a probability vector, where each entry repre-
sents the probability that the two chunks are related
by a particular label ( including ).
</bodyText>
<sectionHeader confidence="0.973076" genericHeader="method">
7 Empirical Results
</sectionHeader>
<bodyText confidence="0.999965411764706">
A 80% training set (32,000 sentences) resulted in
about 700,000 vectors, each vector representing ei-
ther a proper dependency labeling of two chunks, or
a null class pairing. The accuracy of the maximum
entropy classifier was 87.46%. Since candidate pairs
are generated with only a few restrictions, most pair-
ings are null class labelings. They form the majority
class and thus get a strong bias. If we evaluate the
dependency labels, therefore, the results drop appre-
ciably. The maxent precision then is 62.73% (recall
is 85.76%, f-measure is 72.46 %).
Our first experiment was devoted to find out how
good our ILP approach was given that the correct
subcat frame was pre-selected by an oracle. Only
the decision which pairs are labeled with which de-
pendency label was left to ILP (also the selection
and assignment of the non subcategorized labels).
</bodyText>
<figure confidence="0.878797714285714">
:
(10)
To illustrate this by the example previously given:
the subject of the second verb frame of “to believe”
is defined as (with ).
Either
If any is set to one (zero), then
</figure>
<page confidence="0.9908">
203
</page>
<table confidence="0.995511444444444">
Prec Rec F-Mea Prec Rec F-Mea
91.4 86.1 88.7 90.3 80.9 85.4
90.4 83.3 86.7 81.4 73.3 77.2
88.5 76.9 82.3 75.8 55.5 64.1
79.3 73.7 76.4 77.8 40.9 55.6
98.6 94.1 96.3 91.4 86.7 89.1
76.7 75.6 76.1 74.5 72.3 73.4
75.7 76.9 76.3 74.1 74.2 74.2
F F
</table>
<bodyText confidence="0.781878">
There are 8000 sentence with 36,509 labels in the
test set; ILP retrieved 37,173; 31,680 were correct.
Overall precision is 85.23%, recall is 86.77%, the
f-measure is 85.99% (F in Fig. 3).
</bodyText>
<figureCaption confidence="0.997456">
Figure 3: Pre-selected versus Competing Frames
</figureCaption>
<bodyText confidence="0.99983588">
The results of the governable labels ( down to
) are good, except PP complements ( ) with a f-
measure of 76.4%. The errors made with : the
wrong chunks are deemed to stand in a dependency
relation or the wrong label (e.g. instead of )
was chosen for an otherwise valid pair. This is not a
problem of ILP, but one of the statistical model - the
weights do not discriminate well. Improvements of
the statistical model will push ILP’s precision.
Clearly, performance drops if we remove the sub-
cat frame oracle letting all subcat frames of a verb
compete with each other (F , Fig.3). How close
can F come to the oracle setting F . The
overall precision of the F setting is 81.8%, re-
call is 85.8% and the f-measure is 83.7% (f-measure
of F was 85.9%). This is not too far away.
We have also evaluated how good our model is at
finding the correct subcat frame (as a whole). First
some statistics: In the test set are 23 different sub-
cat frames (types) with 16,137 occurrences (token).
15,239 out of these are cases where the underlying
verb has more than one subcat frame (only here do
we have a selection problem). The precision was
71.5%, i.e. the correct subcat frame was selected in
10,896 out of 15,239 cases.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999957380952381">
ILP has been applied to various NLP problems in-
cluding semantic role labeling (Punyakanok et al.,
2004), which is similar to dependency labeling: both
can benefit from verb specific information. Actually,
(Punyakanok et al., 2004) take into account to some
extent verb specific information. They disallow ar-
gument types a verb does not “subcategorize for” by
setting an occurrence constraint. However, they do
not impose co-occurrence restrictions as we do (al-
lowing for competing subcat frames).
None of the approaches to grammatical role label-
ing tries to scale up to dependency labeling. More-
over, they suffer from the problem of inconsistent
classifier output (e.g. (Buchholz, 1999)). A com-
parison of the empirical results is difficult, since e.g.
the number and type of grammatical/dependency re-
lations differ (the same is true wrt. German depen-
dency parsers, e.g (Foth et al., 2005)). However, our
model seeks to integrate the (probabilistic) output of
such systems and - in the best case - boosts the re-
sults, or at least turn it into a consistent solution.
</bodyText>
<sectionHeader confidence="0.989862" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999205">
We have introduced a model for shallow depen-
dency labeling where data-driven and theory-driven
aspects are combined in a principled way. A clas-
sifier provides empirically justified weights, linguis-
tic theory contributes well-motivated global restric-
tions, both are combined under the regiment of opti-
mization. The empirical results of our approach are
promising. However, we have made idealized as-
sumptions (small inventory of dependency relations
and treebank derived chunks) that clearly must be
replaced by a realistic setting in our future work.
Acknowledgment. I would like to thank Markus
Dreyer for fruitful (“long distance”) discussions and
the (steadily improved) maximum entropy models.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999762285714286">
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius and George Smith. 2002. The TIGER Tree-
bank. Proc. of the Wshp. on Treebanks and Linguistic
Theories Sozopol.
Sabine Buchholz, Jorn Veenstra and Walter Daelemans.
1999. Cascaded Grammatical Relation Assignment.
EMNLP-VLC’99, the Joint SIGDAT Conference on
Empirical Methods in NLP and Very Large Corpora.
Kilian Foth, Wolfgang Menzel, and Ingo Schr¨oder. Ro-
bust parsing with weighted constraints. Natural Lan-
guage Engineering, 11(1):1-25 2005.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and
Dave Zimak. 2004. Semantic Role Labeling via Inte-
ger Linear Programming Inference. COLING ’04.
</reference>
<page confidence="0.998895">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741769">
<title confidence="0.9998">Shallow Dependency Labeling</title>
<author confidence="0.999664">Manfred Klenner</author>
<affiliation confidence="0.9999265">Institute of Computational Linguistics University of Zurich</affiliation>
<email confidence="0.969808">klenner@cl.unizh.ch</email>
<abstract confidence="0.972795111111111">We present a formalization of dependency labeling with Integer Linear Programming. We focus on the integration of subcategorization into the decision making process, where the various subcategorization frames of a verb compete with each other. A maximum entropy model provides the weights for ILP optimization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<date>2002</date>
<booktitle>The TIGER Treebank. Proc. of the Wshp. on Treebanks and Linguistic Theories Sozopol.</booktitle>
<contexts>
<context position="8646" citStr="Brants et al., 2002" startWordPosition="1493" endWordPosition="1496">ny label of a frame is equivalent to selecting the frame. As defined in equation 10, a label is active, if the label indicator variable ( ) is set to one. Equivalence is represented by identity, we thus get (cf. constraint 12): (12) is set to one (zero) and all other of the same subcat frame are forced to be one (completeness). Constraint 11 ensures that exactly one subcat frame can be active (coherence). 6 Maximum Entropy and ILP Weights A maximum entropy approach was used to induce a probability model that serves as the basis for the ILP weights. The model was trained on the TIGER treebank (Brants et al., 2002) with feature vectors stemming from the following set of features: the part of speech tags of the two candidate chunks, the distance between them in chunks, the number of intervening verbs, the number of intervening punctuation marks, person, case and number features, the chunks, the direction of the dependency relation (left or right) and a passive/active voice flag. The output of the maxent model is for each pair of chunks a probability vector, where each entry represents the probability that the two chunks are related by a particular label ( including ). 7 Empirical Results A 80% training s</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius and George Smith. 2002. The TIGER Treebank. Proc. of the Wshp. on Treebanks and Linguistic Theories Sozopol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
<author>Walter Daelemans</author>
</authors>
<date>1999</date>
<booktitle>Cascaded Grammatical Relation Assignment. EMNLP-VLC’99, the Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.</booktitle>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>Sabine Buchholz, Jorn Veenstra and Walter Daelemans. 1999. Cascaded Grammatical Relation Assignment. EMNLP-VLC’99, the Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Foth</author>
<author>Wolfgang Menzel</author>
<author>Ingo Schr¨oder</author>
</authors>
<title>Robust parsing with weighted constraints.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<pages>11--1</pages>
<marker>Foth, Menzel, Schr¨oder, 2005</marker>
<rawString>Kilian Foth, Wolfgang Menzel, and Ingo Schr¨oder. Robust parsing with weighted constraints. Natural Language Engineering, 11(1):1-25 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dave Zimak</author>
</authors>
<title>Semantic Role Labeling via Integer Linear Programming Inference.</title>
<date>2004</date>
<journal>COLING</journal>
<volume>04</volume>
<contexts>
<context position="12067" citStr="Punyakanok et al., 2004" startWordPosition="2100" endWordPosition="2103">easure is 83.7% (f-measure of F was 85.9%). This is not too far away. We have also evaluated how good our model is at finding the correct subcat frame (as a whole). First some statistics: In the test set are 23 different subcat frames (types) with 16,137 occurrences (token). 15,239 out of these are cases where the underlying verb has more than one subcat frame (only here do we have a selection problem). The precision was 71.5%, i.e. the correct subcat frame was selected in 10,896 out of 15,239 cases. 8 Related Work ILP has been applied to various NLP problems including semantic role labeling (Punyakanok et al., 2004), which is similar to dependency labeling: both can benefit from verb specific information. Actually, (Punyakanok et al., 2004) take into account to some extent verb specific information. They disallow argument types a verb does not “subcategorize for” by setting an occurrence constraint. However, they do not impose co-occurrence restrictions as we do (allowing for competing subcat frames). None of the approaches to grammatical role labeling tries to scale up to dependency labeling. Moreover, they suffer from the problem of inconsistent classifier output (e.g. (Buchholz, 1999)). A comparison o</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dave Zimak. 2004. Semantic Role Labeling via Integer Linear Programming Inference. COLING ’04.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>