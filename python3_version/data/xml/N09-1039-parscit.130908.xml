<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.979338">
Positive Results for Parsing with a Bounded Stack using a Model-Based
Right-Corner Transform
</title>
<author confidence="0.99655">
William Schuler
</author>
<affiliation confidence="0.999603">
Dept. of Computer Science and Engineering
</affiliation>
<address confidence="0.531203">
Minneapolis, MN
</address>
<email confidence="0.997817">
schuler@cs.umn.edu
</email>
<sectionHeader confidence="0.994794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99969796">
Statistical parsing models have recently been
proposed that employ a bounded stack in time-
series (left-to-right) recognition, using a right-
corner transform defined over training trees to
minimize stack use (Schuler et al., 2008). Cor-
pus results have shown that a vast majority
of naturally-occurring sentences can be parsed
in this way using a very small stack bound
of three to four elements. This suggests that
the standard cubic-time CKY chart-parsing
algorithm, which implicitly assumes an un-
bounded stack, may be wasting probability
mass on trees whose complexity is beyond hu-
man recognition or generation capacity. This
paper first describes a version of the right-
corner transform that is defined over entire
probabilistic grammars (cast as infinite sets
of generable trees), in order to ensure a fair
comparison between bounded-stack and un-
bounded PCFG parsing using a common un-
derlying model; then it presents experimental
results that show a bounded-stack right-corner
parser using a transformed version of a gram-
mar significantly outperforms an unbounded-
stack CKY parser using the original grammar.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936288888889">
Statistical parsing models have recently been pro-
posed that employ a bounded stack in time-series
(left-to-right) recognition, in order to directly and
tractably incorporate incremental phenomena such
as (co-)reference or disfluency into parsing deci-
sions (Schuler et al., 2008; Miller and Schuler,
2008). These models make use of a right-corner
tree transform, based on the left-corner transform
described by Johnson (1998), and are supported by
corpus results suggesting that most sentences (in En-
glish, at least) can be parsed using a very small
stack bound of three to four elements (Schuler et
al., 2008). This raises an interesting question: if
most sentences can be recognized with only three
or four elements of stack memory, is the standard
cubic-time CKY chart-parsing algorithm, which im-
plicitly assumes an unbounded stack, wasting prob-
ability mass on trees whose complexity is beyond
human recognition or generation capacity?
This paper presents parsing accuracy results us-
ing transformed and untransformed versions of a
corpus-trained probabilistic context-free grammar
suggesting that this is indeed the case. Experimental
results show a bounded-memory time-series parser
using a transformed version of a grammar signifi-
cantly outperforms an unbounded-stack CKY parser
using the original grammar.
Unlike the tree-based transforms described previ-
ously, the model-based transform described in this
paper does not introduce additional context from
corpus data beyond that contained in the origi-
nal probabilistic grammar, making it possible to
present a fair comparison between bounded- and
unbounded-stack versions of the same model. Since
this transform takes a probabilistic grammar as in-
put, it can also easily accommodate horizontal and
vertical Markovisation (annotating grammar sym-
bols with parent and sibling categories) as described
by Collins (1997) and subsequently.
The remainder of this paper is organized as fol-
lows: Section 2 describes related approaches to pars-
ing with stack bounds; Section 3 describes an exist-
ing bounded-stack parsing framework using a right-
corner transform defined over individual trees; Sec-
tion 4 describes a redefinition of this transform to ap-
</bodyText>
<page confidence="0.980862">
344
</page>
<note confidence="0.8906145">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 344–352,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999672857142857">
ply to entire probabilistic grammars, cast as infinite
sets of generable trees; and Section 5 describes an
evaluation of this transform on the Wall Street Jour-
nal corpus of the Penn Treebank showing improved
results for a transformed bounded-stack version of a
probabilistic grammar over the original unbounded
grammar.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9997955625">
The model examined here is formally similar to
Combinatorial Categorial Grammar (CCG) (Steed-
man, 2000). But the CCG account is a competence
model as well as a performance model, in that it
seeks to unify category representations used in pro-
cessing with learned generalizations about argument
structure; whereas the model described in this paper
is exclusively a performance model, allowing gen-
eralizations about lexical argument structures to be
learned in some other representation, then combined
with probabilistic information about parsing strate-
gies to yield a set of derived incomplete constituents.
As a result, the model described in this paper has a
freer hand to satisfy strict working memory bounds,
which may not permit some of the alternative com-
position operations proposed in the CCG account,
thought to be associated with available prosody and
quantifier scope analyses.1
Other models (Abney and Johnson, 1991; Gibson,
1991) seek to explain human processing difficulties
as a result of memory capacity limits in parsing or-
dinary phrase structure trees. The Abney-Johnson
and Gibson models adopt a left-corner parsing strat-
egy, of which the right-corner transform described in
this paper is a variant, in order to minimize memory
usage. But the transform-based model described in
this paper exploits a conception of chunking (Miller,
1956) — in this case, grouping recognized words
into stacked-up incomplete constituents — to oper-
ate within much stricter estimates of human short-
term memory bounds (Cowan, 2001) than assumed
by Abney and Johnson.
</bodyText>
<footnote confidence="0.995966428571428">
1The lack of support for some of these available scope anal-
yses may not necessarily be problematic for the present model.
The complexity of interpreting nested raised quantifiers may
place them beyond the capability of human interactive incre-
mental interpretation, but not beyond the capability of post-hoc
interpretation (understood after the listener has had time to think
about it).
</footnote>
<bodyText confidence="0.998560666666667">
Several existing incremental systems are orga-
nized around a left-corner parsing strategy (Roark,
2001; Henderson, 2004). But these systems gen-
erally keep large numbers of constituents open for
modifier attachment in each hypothesis. This al-
lows modifiers to be attached as right children of any
such open constituent. But if any number of open
constituents are allowed, then either the assumption
that stored elements have fixed syntactic (and se-
mantic) structure will be violated, or the assump-
tion that syntax operates within a bounded mem-
ory store will be violated, both of which are psy-
cholinguistically attractive as simplifying assump-
tions. The HHMM model examined in this pa-
per upholds both the fixed-element and bounded-
memory assumptions by hypothesizing fixed reduc-
tions of right child constituents into incomplete par-
ents in the same memory element, to make room for
new constituents that may be introduced at a later
time. These in-element reductions are defined natu-
rally on phrase structure trees as the result of align-
ing right-corner transformed constituent structures
to sequences of random variables in a factored time-
series model.
</bodyText>
<sectionHeader confidence="0.993695" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.999993214285714">
The recognition model examined in this paper is a
factored time-series model, based on a Hierarchic
Hidden Markov Model (Murphy and Paskin, 2001),
which probabilistically estimates the contents of a
memory store of three to four partially-completed
constituents over time. Probabilities for expansions,
transitions and reductions in this model can be de-
fined over trees in a training corpus, transformed
and mapped to the random variables in an HHMM
(Schuler et al., 2008). In Section 4 these probabil-
ities will be computed directly from a probabilistic
context-free grammar, in order to evaluate the con-
tribution of stack bounds without introducing addi-
tional corpus context into the model.
</bodyText>
<subsectionHeader confidence="0.997316">
3.1 A Bounded-Stack Model
</subsectionHeader>
<bodyText confidence="0.9952946">
HHMMs are factored HMMs which mimic a
bounded-memory pushdown automaton (PDA), sup-
porting simple push and pop operations on a
bounded stack-like memory store.
HMMs characterize speech or text as a sequence
</bodyText>
<page confidence="0.998465">
345
</page>
<bodyText confidence="0.9904194">
of hidden states qt (in this case, stacked-up syntac-
tic categories) and observed states ot (in this case,
words) at corresponding time steps t. A most likely
sequence of hidden states ˆq1..T can then be hypothe-
sized given any sequence of observed states o1..T:
</bodyText>
<equation confidence="0.9976478">
P(q1..T |o1..T) (1)
P(q1..T)·P(o1..T  |q1..T) (2)
T POA(qt  |qt-1)·POB(ot  |qt) (3)
H
t=1
</equation>
<bodyText confidence="0.9263746">
using Bayes’ Law (Equation 2) and Markov in-
dependence assumptions (Equation 3) to define
a full P(q1..T  |o1..T) probability as the product
of a Transition Model (ΘA) prior probability
P(q1..T) def = Ht POA(qt  |qt-1) and an Observation
Model (ΘB) likelihood probability P(o1..T |q1..T )def =
Ht POB(ot  |qt).
Transition probabilities POA(qt  |qt-1) over com-
plex hidden states qt can be modeled using synchro-
nized levels of stacked-up component HMMs in an
HHMM. HHMM transition probabilities are calcu-
lated in two phases: a reduce phase (resulting in an
intermediate, marginalized state ft), in which com-
ponent HMMs may terminate; and a shift phase (re-
sulting in a modeled state qt), in which unterminated
HMMs transition, and terminated HMMs are re-
initialized from their parent HMMs. Variables over
intermediate ft and modeled qt states are factored
into sequences of depth-specific variables – one for
each of D levels in the HHMM hierarchy:
</bodyText>
<equation confidence="0.9993715">
ft = hf1 t ... fDt i(4)
qt = hq1t ... qDt i (5)
</equation>
<bodyText confidence="0.999890333333333">
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific reduce ΘR,d and shift ΘS,d models:
</bodyText>
<equation confidence="0.974606222222222">
POA(qt|qt-1) = J: P(ft|qt-1)·P(qt|ft qt-1) (6)
ft
POR,d(fd t |fd+1
t qd t-1qd-1
t-1 )·
POS,d(qdt |fd+1
t fdt qd t-1qd-1
t ) (7)
with fD+1
</equation>
<bodyText confidence="0.958619">
t and q0t defined as constants. In Viterbi
decoding, the sums are replaced with argmax opera-
tors. This decoding process preserves ambiguity by
</bodyText>
<figureCaption confidence="0.99819575">
Figure 1: Graphical representation of a Hierarchic Hid-
den Markov Model. Circles denote random variables, and
edges denote conditional dependencies. Shaded circles
are observations.
</figureCaption>
<bodyText confidence="0.99743292">
maintaining competing analyses of the entire mem-
ory store. A graphical representation of an HHMM
with three levels is shown in Figure 1.
Shift and reduce probabilities can then be defined
in terms of finitely recursive Finite State Automata
(FSAs) with probability distributions over transition,
recursive expansion, and final-state status of states at
each hierarchy level. In the version of HHMMs used
in this paper, each intermediate variable is a reduc-
tion or non-reduction state fdt ∈ G ∪ {1, 0} (indi-
cating, respectively, a complete reduced constituent
of some grammatical category from domain G, or
a failure to reduce due to an ‘active’ transition be-
ing performed, or a failure to reduce due to an
‘awaited’ transition being performed, as defined in
Section 4.3); and each modeled variable is a syn-
tactic state qdt ∈ G × G (describing an incomplete
constituent consisting of an active grammatical cat-
egory from domain G and an awaited grammatical
category from domain G). An intermediate vari-
able fdt at depth d may indicate reduction or non-
reduction according to ΘF-Rd,d if there is a reduction
at the depth level immediately below d, but must in-
dicate non-reduction (0) with probability 1 if there
was no reduction below:2
</bodyText>
<equation confidence="0.9973078">
POR,d(fdt  |fd+1
t qd t-1qd-1
t-1 )def =
d+1 d = 1
1 8
</equation>
<bodyText confidence="0.773658">
if fif td+1 ∈G : [POF-RO (fdt  |qdt-1,qd-1) ( ) t-1
</bodyText>
<footnote confidence="0.9073425">
2Here [·] is an indicator function: [φ] = 1 if φ is true, 0
otherwise.
</footnote>
<figure confidence="0.995716580645161">
. . .
. . .
. . .
. . .
f1t−1
f2t−1
f3t−1
ot−1
q1t−1
q2t−1
q3t−1
f1 t
f2t
f3t
q1t
q2t
q3t
ot
ˆq1..T =argmax
q1..T
= argmax
q1..T
def
= argmax
q1..T
J: def =
f1..D
t
D
H
d=1
</figure>
<page confidence="0.994605">
346
</page>
<bodyText confidence="0.9469422">
where fD+1
t EG and q0 t = ROOT.
Shift probabilities over the modeled variable qdt
at each level are defined using level-specific transi-
tion OQ-Tr,d and expansion OQ-Ex,d models:
</bodyText>
<equation confidence="0.997529923076923">
POS,d(qd t  |fd+1
t fd t qd t-1qd-1
t )def =
{ if fd+1
t VG, fdt VG : [qdt =qdt-1]
t )
if fd+1
t EG, fdt VG : POQ-Tr,d(qdt  |fd+1
t fdt qdt-1qd-1
t EG, fdt EG : POQ-Ex,d(qdt  |qd-1
if fd+1
t )
(9)
</equation>
<bodyText confidence="0.998520761904762">
where fD+1
t EG and q0t = ROOT. This model is
conditioned on reduce variables at and immediately
below the current FSA level. If there is no reduc-
tion immediately below the current level (the first
case above), it deterministically copies the current
FSA state forward to the next time step. If there
is a reduction immediately below the current level
but no reduction at the current level (the second case
above), it transitions the FSA state at the current
level, according to the distribution OQ-Tr,d. And if
there is a reduction at the current level (the third case
above), it re-initializes this state given the state at the
level above, according to the distribution OQ-Ex,d.
The overall effect is that higher-level FSAs are al-
lowed to transition only when lower-level FSAs ter-
minate. An HHMM therefore behaves like a prob-
abilistic implementation of a pushdown automaton
(or shift–reduce parser) with a finite stack, where the
maximum stack depth is equal to the number of lev-
els in the HHMM hierarchy.
</bodyText>
<subsectionHeader confidence="0.992458">
3.2 Tree-Based Transforms
</subsectionHeader>
<bodyText confidence="0.999584181818182">
The right-corner transform used in this paper is sim-
ply the left-right dual of a left-corner transform
(Johnson, 1998). It transforms all right branching
sequences in a phrase structure tree into left branch-
ing sequences of symbols of the form Aη/Aη·µ, de-
noting an incomplete instance of an ‘active’ category
Aη lacking an instance of an ‘awaited’ category Aη·µ
yet to come.3 These incomplete constituent cate-
gories have the same form and much of the same
meaning as non-constituent categories in a Combi-
natorial Categorial Grammar (Steedman, 2000).
</bodyText>
<footnote confidence="0.853081">
3Here rl and µ are node addresses in a binary-branching tree,
defined as paths of left (0) or right (1) branches from the root.
</footnote>
<bodyText confidence="0.913251">
Rewrite rules for the right-corner transform are
shown below:4
</bodyText>
<listItem confidence="0.937728421052632">
• Beginning case: the top of a right-expanding
sequence in an ordinary phrase structure tree is
mapped to the bottom of a left-expanding se-
quence in a right-corner transformed tree:
This case of the right-corner transform may be
considered a constrained version of CCG type
raising.
• Middle case: each subsequent branch in a
right-expanding sequence of an ordinary phrase
structure tree is mapped to a branch in a left-
expanding sequence of the transformed tree:
(11)
This case of the right-corner transform may be
considered a constrained version of CCG for-
ward function composition.
• Ending case: the bottom of a right-expanding
sequence in an ordinary phrase structure tree is
mapped to the top of a left-expanding sequence
in a right-corner transformed tree:
</listItem>
<equation confidence="0.532536">
Aη
α Aη·µ
aη·µ
</equation>
<bodyText confidence="0.864909666666667">
This case of the right-corner transform may be
considered a constrained version of CCG for-
ward function application.
</bodyText>
<footnote confidence="0.5438774">
4These rules can be applied recursively from bottom up
on a source tree, synchronously associating subtree structures
matched to variables α, Q, and γ on the left side of each rule
with transformed representations of these subtree structures on
the right.
</footnote>
<figure confidence="0.995412828571429">
Aη·0
α
Aη·1
β
Aη
Aη
(10)
Aη/Aη·1
Aη·0
β
�
α
Aη
Aη
α Aη·µ
γ
Aη/Aη·µ·1
�
Aη·µ·0
β
Aη·µ·1
γ
Aη·µ·0
β
Aη/Aη·µ
α
Aη
� Aη/Aη·µ
α
Aη·µ
aη·µ
(12)
347
a) binary-branching phrase structure tree:
new
</figure>
<figureCaption confidence="0.987696">
Figure 2: Trees resulting from a) a sample phrase structure tree for the sentence Strong demand for New York City’s
general obligations bonds propped up the municipal market, and b) a right-corner transform of this tree. Sequences of
left children are recognized from the bottom up through in-element transitions in a Hierarchic Hidden Markov Model.
Right children are recognized by expanding to additional stack elements.
</figureCaption>
<figure confidence="0.99801770212766">
NNS
bonds
S
JJ
strong
NN
demand
NP
PP
IN
for
NNP
new
S
NNS
NN
obligation
VP
NP
PRT
DT
the
NPpos
up
POS
’s
NNS
NNP
NNP
JJ
general
NP
VBN
propped
VBN
JJ
municipal
NP
NN
NNP
york
NNP
city
S/NN
NN
market
JJ
NN
market
S/NN
b) result of right-corner transform:
S/NP
DT
the
S/VP
NP
PRT
NP/NNS
NNS
bonds
up
NP/NNS
NN
VBN
VBN/PRT
VBN
propped
municipal
IN
for
POS
’s
NP/PP
NP
NPpos/POS
NNP
NP/NNS
NPpos
obligation
JJ
general
NP/NP
NP/NN
JJ
strong
NN
demand
NNP/NNP
NNP/NNP
NNP
NNP
city
NNP
york
</figure>
<bodyText confidence="0.999873421052631">
The completeness of the above transform rules can
be demonstrated by the fact that they cover all pos-
sible subtree configurations (with the exception of
bare terminals, which are simply copied). The
soundness of the above transform rules can be
demonstrated by the fact that each rule transforms
a right-branching subtree into a left-branching sub-
tree labeled with an incomplete constituent.
An example of a right-corner transformed tree is
shown in Figure 2(b). An important property of this
transform is that it is reversible. Rewrite rules for re-
versing a right-corner transform are simply the con-
verse of those shown above.
Sequences of left children in the resulting mostly-
left-branching trees are recognized from the bot-
tom up, through transitions at the same stack ele-
ment. Right children, which are much less frequent
in the resulting trees, are recognized through cross-
element expansions in a bounded-stack recognizer.
</bodyText>
<sectionHeader confidence="0.997789" genericHeader="method">
4 Model-Based Transforms
</sectionHeader>
<bodyText confidence="0.9999548">
In order to compare bounded- and unbounded-stack
versions of the same model, the formulation of
the right-corner and bounded-stack transforms in-
troduced in this paper does not map trees to trees,
but rather maps probability models to probability
</bodyText>
<page confidence="0.997128">
348
</page>
<bodyText confidence="0.999747947368421">
models. This eliminates complications in comparing
models with different numbers of dependent vari-
ables — and thus different numbers of free parame-
ters — because the model which ordinarily has more
free parameters (the HHMM, in this case) is derived
from the model that has fewer (the PCFG). Since
they are derived from a simpler underlying model,
the additional parameters of the HHMM are not free.
Mapping probability models from one format to
another can be thought of as mapping the infinite
sets of trees that are defined by these models from
one format to another. Probabilities in the trans-
formed model are therefore defined by calculating
probabilities for the relevant substructures in the
source model, then marginalizing out the values of
nodes in these structures that do not appear in the
desired expression in the target model.
A bounded-stack HHMM OQ,F can therefore be
derived from an unbounded PCFG OG by:
</bodyText>
<listItem confidence="0.797672">
1. organizing the rules in the source PCFG
model OG into direction-specific versions (dis-
tinguishing rules for expanding left and right
children, which occur respectively as active and
awaited constituent categories in incomplete
constituent labels);
2. enforcing depth limits on these direction-
specific rules; and
3. mapping these probabilities to HHMM random
variable positions at the appropriate depth.
</listItem>
<subsectionHeader confidence="0.988894">
4.1 Direction-specific rules
</subsectionHeader>
<bodyText confidence="0.99995656">
An inspection of the tree-based right-corner trans-
form rewrites defined in Section 3.2 will show two
things: first, that constituents occurring as left chil-
dren in an original tree (with addresses ending in
‘0’) always become active constituents (occurring
before the slash, or without a slash) in incomplete
constituent categories, and constituents occurring as
right children in an original tree (with addresses end-
ing in ‘1’) always become awaited constituents (oc-
curring after the slash); and second, that left chil-
dren expand locally downward in the transformed
tree (so each Aη·0/... locally dominates Aη·0·0/...),
whereas right children expand locally upward (so
each .../Aη·1 is locally dominated by .../Aη·1·1).
This means that rules from the original grammar —
if distinguished into rules applying only to left and
right children (active and awaited constituents) —
can still be locally modeled following a right-corner
transform. A transformed tree can be generated
in this way by expanding downward along the ac-
tive constituents in a transformed tree, then turning
around and expanding upward to fill in the awaited
constituents, then turning around again to generate
the active constituents at the next depth level, and so
on.
</bodyText>
<subsectionHeader confidence="0.992477">
4.2 Depth bounds
</subsectionHeader>
<bodyText confidence="0.996797027777778">
The locality of the original grammar rules in a right-
corner transformed tree allows memory limits on in-
complete constituents to be applied directly as depth
bounds in the zig-zag generation traversal defined
above. These depth limits correspond directly to the
depth levels in an HHMM.
In the experiments described in Section 5,
direction-specific and depth-specific versions of the
original grammar rules are implemented in an ordi-
nary CKY-style dynamic-programming parser, and
can therefore simply be cut off at a particular depth
level with no renormalization.
But in an HHMM, this will result in label-bias ef-
fects, in which expanded constituents may have no
valid reduction, forcing the system to define distri-
butions for composing constituents that are not com-
patible. For example, if a constituent is expanded at
depth D, and that constituent has no expansions that
can be completely processed within depth D, it will
not be able to reduce, and will remain incompatible
with the incomplete constituent above it. Probabili-
ties for depth-bounded rules must therefore be renor-
malized to the domain of allowable trees that can be
generated within D depth levels, in order to guaran-
tee consistent probabilities for HHMM recognition.
This is done by determining the (depth- and
direction-specific) probability POB-L,d(1 |Aη·0)
or POB-R,d(1 |Aη·1) that a tree generated at each
depth d and rooted by a left or right child will fit
within depth D. These probabilities are then esti-
mated using an approximate inference algorithm,
similar to that used in value iteration (Bellman,
1957), which estimates probabilities of infinite trees
by exploiting the fact that increasingly longer trees
contribute exponentially decreasing probability
mass (since each non-terminal expansion must
</bodyText>
<page confidence="0.998071">
349
</page>
<bodyText confidence="0.999285833333333">
avoid generating a terminal with some probability
at each step from the top down), so a sum over
probabilities of trees with increasing length k is
guaranteed to converge. The algorithm calculates
probabilities of trees with increasing length k until
convergence, or to some arbitrary limit K:
</bodyText>
<equation confidence="0.999771571428571">
POG(A,7·0 -+ A,7·0·0 A,7·0·1)
· POB-L,d,k−1(1  |A,7·0·0)
· POB-R,d,k−1(1|A,7·0·1) (13)
POB-R,d,k(1  |A,7·1) def =
X POG(A,7·1 -+ A,7·1·0 A,7·1·1)
Aη·1·0, · POB-L,d+1,k−1(1 |A,7·1·0)
Aη·1·1 · POB-R,d,k−1(1|A,7·1·1) (14)
</equation>
<bodyText confidence="0.989585">
Normalized probability distributions for depth-
bounded expansions OG-L,d and OG-R,d can now be
calculated using converged OB-L,d and OB-R,d esti-
mates:
</bodyText>
<equation confidence="0.9381015">
def
POG-L,d (A,7·0 A,7·0·0 A ·0·1)=
POG(A,7·0 A,7·0·0 A,7·0·1)
· POB-L,d(1  |A,7·0·0) · POB-R,d(1  |A,7·0·1) (15)
POG-R,d(A,7·1 A,7·1·0 A,7·1·1) def =
POG(A,7·1 A,7·1·0 A,7·1·1)
· POB-L,d+1(1  |A,7·1·0) · POB-R,d(1  |A,7·1·1) (16)
4.3 HHMM probabilities
</equation>
<bodyText confidence="0.967963416666667">
Converting PCFGs to HHMMs requires the calcu-
∗
lation of expected frequencies FOG-L*,d(A,7 A,7·µ)
of generating symbols A,7·µ in the left-progeny of a
nonterminal symbol A,7 (in other words, of A,7·µ be-
ing a left child of A,7, or a left child of a left child
of A,7, etc.). This is done by summing over sub-
trees of increasing length k using the same approx-
imate inference technique described in Section 4.2,
which guarantees convergence since each subtree of
increasing length contributes exponentially decreas-
ing probability mass to the sum:
</bodyText>
<equation confidence="0.967511066666667">
FOG-L*,d(A,7 � A,7·µ) = ∞ k
∗ X FOG-L*,d(A,7 + A,7·µ)
k=0 (17)
POB-L,d,k(1  |A,7·0) def=
X
Aη·1·0,
Aη·1·1
where:
k
FOG-L* d (A,7 + A,7·0k )
k−1
POG-L*,d(A,7 + A,7·0k−1)
· POG-L,d(A,7·0k−1 -+ A,7·0k A,7·0k−1·1) (18)
and POG-L*,d(A,7 � A′
0 ,7) = [A,7 = A′,7].
</equation>
<bodyText confidence="0.962678875">
A complete HHMM can now be defined us-
ing depth-bounded right-corner PCFG probabilities.
HHMM probabilities will be defined over syntac-
tic states consisting of incomplete constituent cat-
egories A,7/A,7·µ.
Expansions depend on only the incomplete con-
stituent category ../A,7 (for any active category ‘..’)
1.
</bodyText>
<equation confidence="0.961825666666667">
at %−.
POQ-Ex,d(a,7·0·µ  |../A,7) =
PAη·0, POG-R,d−1(A,7 - A,7·0 A,7·1)·
Aη·1 ∗
FOG-L*,d(A,7·0 � a,7·0·µ)
PAη·0, POG-R,d−1(A,7 - A,7·0 A,7·1)·
aη·0·µ
Aη·1, FOG-L*,d(A,7·0 � a,7·0·µ)
∗
</equation>
<bodyText confidence="0.999893285714286">
Transitions depend on whether an ‘active’ or
‘awaited’ transition was performed at the current
level. If an active transition was performed (where
fdt = 1), the transition depends on only the in-
complete constituent category A,7·0·µ·0/.. (for any
awaited category ‘..’) at qdt−1, and the incomplete
constituent category ../A,7 (for any active category
</bodyText>
<equation confidence="0.943387190476191">
d−1.
’) at qt−1 •
POQ-Tr,d(A,7·0·µ/A,7·0·µ·1 |11 A,7·0·µ·0/..1../A,7) =
Aη·0,
Aη·1
X POG-R,d−1(A,7 - A,7·0 A,7·1)·
FΘG-L*,d(Aη0�Aη0µ0)−FΘG-L*,d(Aη0 �Aη0µ0)
FΘG-L*,d(Aη·0 �Aη·0·µ) ·
∗ 0
∗
POG-L,d(A,7·0·µ - A,7·0·µ·0 A,7·0·µ·1)
Aη·0·µ,
Aη·0,
Aη·1,
X POG-R,d−1(A,7 - A,7·0 A,7·1)·
FΘG-L*,d(Aη0,Aη0µ0)−FΘG-L*,d(Aη0-&apos;Aη0µ0)
FΘG-L*,d(Aη·0-&apos;Aη·0·µ) ·
∗ 0
∗
POG-L,d(A,7·0·µ - A,7·0·µ·0 A,7·0·µ·1)
Aη·0·µ·1
</equation>
<bodyText confidence="0.9462988">
(20)
If an awaited transition was performed (where fdt =
0), the transition depends on only the complete con-
stituent category A,7·µ·0 at fd+1
t , and the incomplete
</bodyText>
<figure confidence="0.848754461538462">
X
Aη·0k−1,
Aη·0k−1·1
(19)
350
model (sect 22–24, len&gt;40) F
unbounded PCFG 66.03
bounded PCFG (D=4) 66.08
constituent category Aη/Aη·µ at qdt−1:
PΘQ-Tr,d(Aη/Aη·µ·1 |0, Aη·µ·0, Aη/Aη·µ) =
PΘG-R,d(Aη·µ - Aη·µ·0 Aη·µ·1) (21)
E
Aη·µ·1 PΘG-R,d(Aη·µ -+ Aη·µ·0 Aη·µ·1)
</figure>
<bodyText confidence="0.969302166666667">
Reduce probabilities depend on the complete con-
stituent category at fd+1
t , and the incomplete con-
stituent category Aη·0·µ·0/.. (for any awaited cate-
gory ‘..’) at qdt−1, and the incomplete constituent cat-
egory ../Aη (for any active category ‘..’) at qd−1
t−1 . If
the complete constituent category at fd+1
t does not
match the awaited category of qdt−1, the probability
is [fdt = f0]. If the complete constituent category
at fd+1
</bodyText>
<equation confidence="0.9911643">
t does match the awaited category of qdt−1:
PΘF-Rd,d(1  |Aη·0·µ/.., ../Aη) =
EAη·0,Aη·1 PΘG-R,d−1((A,1η - Aη·0 Aη·1)·
(FΘG-L*,d(Aη·0 _*+Aη·0·µ)
−FΘG-L*,d (Aη·0 �0 Aη·0·µ)
E
Aη·0,Aη·1 PΘG-R,d−1(Aη Aη·0 Aη·1)·
F-L
ΘG*,d Aη·0·µ)
∗
and:
PΘF-Rd,d(Aη·0·µ  |Aη·0·µ/.., ../Aη) =
E
Aη·0,Aη·1 PΘG-R,d−1(Aη - Aη·0 Aη·1)·
0
FΘG-L*,d(Aη·0 � Aη·0·µ)
E
Aη·0,Aη·1 PΘG-R,d−1(Aη Aη·0 Aη·1)·
∗
FΘG-L*,d(Aη·0 Aη·0·µ)
</equation>
<bodyText confidence="0.999987666666667">
The correctness of the above distributions can be
demonstrated by the fact that all terms other than
ΘG-L,d and ΘG-R,d probabilities will cancel out in
any sequence of transitions between an expansion
and a reduction, leaving only those terms that would
appear as factors in an ordinary PCFG parse.5
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999472">
A PCFG model was extracted from sections 2–21
of the Wall Street Journal Treebank. In order to
keep the transform process manageable, punctua-
tion was removed from the corpus, and rules oc-
curring less frequently than 10 times in the corpus
</bodyText>
<footnote confidence="0.816558333333333">
5It is important to note, however, that these probabilities are
not necessarily incrementally balanced, so this correctness only
applies to parsing with an infinite beam.
</footnote>
<tableCaption confidence="0.9954085">
Table 1: Results of CKY parsing using bounded and un-
bounded PCFG.
</tableCaption>
<bodyText confidence="0.999975384615385">
were deleted from the PCFG. The right-corner and
bounded-stack transforms described in the previous
section were then applied to the PCFG. The origi-
nal and bounded PCFG models were evaluated in a
CKY recognizer on sections 22–24 of the Treebank,
with results shown in Table 1.6 Results were signif-
icant only for sentences longer than 40 words. On
these sentences, the bounded PCFG model achieves
about a .15% reduction of error over the original
PCFG (p &lt; .1 using one-tailed pairwise t-test). This
suggests that on long sentences the probability mass
wasted due to parsing with an unbounded stack is
substantial enough to impact parsing accuracy.
</bodyText>
<sectionHeader confidence="0.999403" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999956933333333">
Previous work has explored bounded-stack parsing
using a right-corner transform defined on trees to
minimize stack usage. HHMM parsers trained on
applications of this tree-based transform of train-
ing corpora have shown improvements over ordinary
PCFG models, but this may have been attributable to
the richer dependencies of the HHMM.
This paper has presented an approximate in-
ference algorithm for transforming entire PCFGs,
rather than individual trees, into equivalent right-
corner bounded-stack HHMMs. Moreover, a com-
parison with an untransformed PCFG model sug-
gests that the probability mass wasted due to pars-
ing with an unbounded stack is substantial enough
to impact parsing accuracy.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.789198714285714">
This research was supported by NSF CAREER
award 0447685 and by NASA under award
NNX08AC36A. The views expressed are not nec-
essarily endorsed by the sponsors.
6A CKY recognizer was used in both cases in order to avoid
introducing errors due to model approximation or beam limits
necessary for incremental processing with large grammars.
</bodyText>
<page confidence="0.998175">
351
</page>
<sectionHeader confidence="0.99382" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999815952380952">
Steven P. Abney and Mark Johnson. 1991. Memory re-
quirements and local ambiguities of parsing strategies.
J. Psycholinguistic Research, 20(3):233–250.
Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL ’97).
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage ca-
pacity. Behavioral and Brain Sciences, 24:87–185.
Edward Gibson. 1991. A computational theory of hu-
man linguistic processing: Memory limitations and
processing breakdown. Ph.D. thesis, Carnegie Mellon.
James Henderson. 2004. Lookahead in deterministic
left-corner parsing. In Proc. Workshop on Incremen-
tal Parsing: Bringing Engineering and Cognition To-
gether, Barcelona, Spain.
Mark Johnson. 1998. Finite state approximation of
constraint-based grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL, pages
619–623.
Tim Miller and William Schuler. 2008. A syntactic time-
series model for parsing fluent and disfluent speech. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING’08).
George A. Miller. 1956. The magical number seven, plus
or minus two: Some limits on our capacity for process-
ing information. Psychological Review, 63:81–97.
Kevin P. Murphy and Mark A. Paskin. 2001. Linear time
inference in hierarchical HMMs. In Proc. NIPS, pages
833–840.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2008. Toward a psycholinguistically-
motivated model of language. In Proceedings of COL-
ING, Manchester, UK, August.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
</reference>
<page confidence="0.99826">
352
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511534">
<title confidence="0.9787105">Positive Results for Parsing with a Bounded Stack using a Right-Corner Transform</title>
<author confidence="0.959728">William</author>
<affiliation confidence="0.867335">Dept. of Computer Science and Minneapolis,</affiliation>
<email confidence="0.999831">schuler@cs.umn.edu</email>
<abstract confidence="0.990302192307692">Statistical parsing models have recently been proposed that employ a bounded stack in timeseries (left-to-right) recognition, using a rightcorner transform defined over training trees to minimize stack use (Schuler et al., 2008). Corpus results have shown that a vast majority of naturally-occurring sentences can be parsed in this way using a very small stack bound of three to four elements. This suggests that the standard cubic-time CKY chart-parsing which implicitly assumes an unmay be wasting probability mass on trees whose complexity is beyond human recognition or generation capacity. This paper first describes a version of the rightcorner transform that is defined over entire probabilistic grammars (cast as infinite sets of generable trees), in order to ensure a fair comparison between bounded-stack and unbounded PCFG parsing using a common underlying model; then it presents experimental results that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack CKY parser using the original grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
<author>Mark Johnson</author>
</authors>
<title>Memory requirements and local ambiguities of parsing strategies.</title>
<date>1991</date>
<journal>J. Psycholinguistic Research,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="4967" citStr="Abney and Johnson, 1991" startWordPosition="741" endWordPosition="744">; whereas the model described in this paper is exclusively a performance model, allowing generalizations about lexical argument structures to be learned in some other representation, then combined with probabilistic information about parsing strategies to yield a set of derived incomplete constituents. As a result, the model described in this paper has a freer hand to satisfy strict working memory bounds, which may not permit some of the alternative composition operations proposed in the CCG account, thought to be associated with available prosody and quantifier scope analyses.1 Other models (Abney and Johnson, 1991; Gibson, 1991) seek to explain human processing difficulties as a result of memory capacity limits in parsing ordinary phrase structure trees. The Abney-Johnson and Gibson models adopt a left-corner parsing strategy, of which the right-corner transform described in this paper is a variant, in order to minimize memory usage. But the transform-based model described in this paper exploits a conception of chunking (Miller, 1956) — in this case, grouping recognized words into stacked-up incomplete constituents — to operate within much stricter estimates of human shortterm memory bounds (Cowan, 200</context>
</contexts>
<marker>Abney, Johnson, 1991</marker>
<rawString>Steven P. Abney and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. J. Psycholinguistic Research, 20(3):233–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Bellman</author>
</authors>
<title>Dynamic Programming.</title>
<date>1957</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ.</location>
<contexts>
<context position="21421" citStr="Bellman, 1957" startWordPosition="3449" endWordPosition="3450">patible with the incomplete constituent above it. Probabilities for depth-bounded rules must therefore be renormalized to the domain of allowable trees that can be generated within D depth levels, in order to guarantee consistent probabilities for HHMM recognition. This is done by determining the (depth- and direction-specific) probability POB-L,d(1 |Aη·0) or POB-R,d(1 |Aη·1) that a tree generated at each depth d and rooted by a left or right child will fit within depth D. These probabilities are then estimated using an approximate inference algorithm, similar to that used in value iteration (Bellman, 1957), which estimates probabilities of infinite trees by exploiting the fact that increasingly longer trees contribute exponentially decreasing probability mass (since each non-terminal expansion must 349 avoid generating a terminal with some probability at each step from the top down), so a sum over probabilities of trees with increasing length k is guaranteed to converge. The algorithm calculates probabilities of trees with increasing length k until convergence, or to some arbitrary limit K: POG(A,7·0 -+ A,7·0·0 A,7·0·1) · POB-L,d,k−1(1 |A,7·0·0) · POB-R,d,k−1(1|A,7·0·1) (13) POB-R,d,k(1 |A,7·1)</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>Richard Bellman. 1957. Dynamic Programming. Princeton University Press, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL ’97).</booktitle>
<contexts>
<context position="3188" citStr="Collins (1997)" startWordPosition="470" endWordPosition="471"> unbounded-stack CKY parser using the original grammar. Unlike the tree-based transforms described previously, the model-based transform described in this paper does not introduce additional context from corpus data beyond that contained in the original probabilistic grammar, making it possible to present a fair comparison between bounded- and unbounded-stack versions of the same model. Since this transform takes a probabilistic grammar as input, it can also easily accommodate horizontal and vertical Markovisation (annotating grammar symbols with parent and sibling categories) as described by Collins (1997) and subsequently. The remainder of this paper is organized as follows: Section 2 describes related approaches to parsing with stack bounds; Section 3 describes an existing bounded-stack parsing framework using a rightcorner transform defined over individual trees; Section 4 describes a redefinition of this transform to ap344 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 344–352, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ply to entire probabilistic grammars, cast as infinite sets of generable trees; </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL ’97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Cowan</author>
</authors>
<title>The magical number 4 in shortterm memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences,</title>
<date>2001</date>
<pages>24--87</pages>
<contexts>
<context position="5569" citStr="Cowan, 2001" startWordPosition="836" endWordPosition="837">nson, 1991; Gibson, 1991) seek to explain human processing difficulties as a result of memory capacity limits in parsing ordinary phrase structure trees. The Abney-Johnson and Gibson models adopt a left-corner parsing strategy, of which the right-corner transform described in this paper is a variant, in order to minimize memory usage. But the transform-based model described in this paper exploits a conception of chunking (Miller, 1956) — in this case, grouping recognized words into stacked-up incomplete constituents — to operate within much stricter estimates of human shortterm memory bounds (Cowan, 2001) than assumed by Abney and Johnson. 1The lack of support for some of these available scope analyses may not necessarily be problematic for the present model. The complexity of interpreting nested raised quantifiers may place them beyond the capability of human interactive incremental interpretation, but not beyond the capability of post-hoc interpretation (understood after the listener has had time to think about it). Several existing incremental systems are organized around a left-corner parsing strategy (Roark, 2001; Henderson, 2004). But these systems generally keep large numbers of constit</context>
</contexts>
<marker>Cowan, 2001</marker>
<rawString>Nelson Cowan. 2001. The magical number 4 in shortterm memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences, 24:87–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>A computational theory of human linguistic processing: Memory limitations and processing breakdown.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon.</institution>
<contexts>
<context position="4982" citStr="Gibson, 1991" startWordPosition="745" endWordPosition="746">ibed in this paper is exclusively a performance model, allowing generalizations about lexical argument structures to be learned in some other representation, then combined with probabilistic information about parsing strategies to yield a set of derived incomplete constituents. As a result, the model described in this paper has a freer hand to satisfy strict working memory bounds, which may not permit some of the alternative composition operations proposed in the CCG account, thought to be associated with available prosody and quantifier scope analyses.1 Other models (Abney and Johnson, 1991; Gibson, 1991) seek to explain human processing difficulties as a result of memory capacity limits in parsing ordinary phrase structure trees. The Abney-Johnson and Gibson models adopt a left-corner parsing strategy, of which the right-corner transform described in this paper is a variant, in order to minimize memory usage. But the transform-based model described in this paper exploits a conception of chunking (Miller, 1956) — in this case, grouping recognized words into stacked-up incomplete constituents — to operate within much stricter estimates of human shortterm memory bounds (Cowan, 2001) than assumed</context>
</contexts>
<marker>Gibson, 1991</marker>
<rawString>Edward Gibson. 1991. A computational theory of human linguistic processing: Memory limitations and processing breakdown. Ph.D. thesis, Carnegie Mellon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Lookahead in deterministic left-corner parsing.</title>
<date>2004</date>
<booktitle>In Proc. Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="6110" citStr="Henderson, 2004" startWordPosition="917" endWordPosition="918">thin much stricter estimates of human shortterm memory bounds (Cowan, 2001) than assumed by Abney and Johnson. 1The lack of support for some of these available scope analyses may not necessarily be problematic for the present model. The complexity of interpreting nested raised quantifiers may place them beyond the capability of human interactive incremental interpretation, but not beyond the capability of post-hoc interpretation (understood after the listener has had time to think about it). Several existing incremental systems are organized around a left-corner parsing strategy (Roark, 2001; Henderson, 2004). But these systems generally keep large numbers of constituents open for modifier attachment in each hypothesis. This allows modifiers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have fixed syntactic (and semantic) structure will be violated, or the assumption that syntax operates within a bounded memory store will be violated, both of which are psycholinguistically attractive as simplifying assumptions. The HHMM model examined in this paper upholds both the fixed-element and</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Lookahead in deterministic left-corner parsing. In Proc. Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Finite state approximation of constraint-based grammars using left-corner grammar transforms.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>619--623</pages>
<contexts>
<context position="1743" citStr="Johnson (1998)" startWordPosition="257" endWordPosition="258">ults that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack CKY parser using the original grammar. 1 Introduction Statistical parsing models have recently been proposed that employ a bounded stack in time-series (left-to-right) recognition, in order to directly and tractably incorporate incremental phenomena such as (co-)reference or disfluency into parsing decisions (Schuler et al., 2008; Miller and Schuler, 2008). These models make use of a right-corner tree transform, based on the left-corner transform described by Johnson (1998), and are supported by corpus results suggesting that most sentences (in English, at least) can be parsed using a very small stack bound of three to four elements (Schuler et al., 2008). This raises an interesting question: if most sentences can be recognized with only three or four elements of stack memory, is the standard cubic-time CKY chart-parsing algorithm, which implicitly assumes an unbounded stack, wasting probability mass on trees whose complexity is beyond human recognition or generation capacity? This paper presents parsing accuracy results using transformed and untransformed versi</context>
<context position="13148" citStr="Johnson, 1998" startWordPosition="2107" endWordPosition="2108"> current level (the third case above), it re-initializes this state given the state at the level above, according to the distribution OQ-Ex,d. The overall effect is that higher-level FSAs are allowed to transition only when lower-level FSAs terminate. An HHMM therefore behaves like a probabilistic implementation of a pushdown automaton (or shift–reduce parser) with a finite stack, where the maximum stack depth is equal to the number of levels in the HHMM hierarchy. 3.2 Tree-Based Transforms The right-corner transform used in this paper is simply the left-right dual of a left-corner transform (Johnson, 1998). It transforms all right branching sequences in a phrase structure tree into left branching sequences of symbols of the form Aη/Aη·µ, denoting an incomplete instance of an ‘active’ category Aη lacking an instance of an ‘awaited’ category Aη·µ yet to come.3 These incomplete constituent categories have the same form and much of the same meaning as non-constituent categories in a Combinatorial Categorial Grammar (Steedman, 2000). 3Here rl and µ are node addresses in a binary-branching tree, defined as paths of left (0) or right (1) branches from the root. Rewrite rules for the right-corner trans</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. Finite state approximation of constraint-based grammars using left-corner grammar transforms. In Proceedings of COLING/ACL, pages 619–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Miller</author>
<author>William Schuler</author>
</authors>
<title>A syntactic timeseries model for parsing fluent and disfluent speech.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING’08).</booktitle>
<contexts>
<context position="1623" citStr="Miller and Schuler, 2008" startWordPosition="237" endWordPosition="240">fair comparison between bounded-stack and unbounded PCFG parsing using a common underlying model; then it presents experimental results that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack CKY parser using the original grammar. 1 Introduction Statistical parsing models have recently been proposed that employ a bounded stack in time-series (left-to-right) recognition, in order to directly and tractably incorporate incremental phenomena such as (co-)reference or disfluency into parsing decisions (Schuler et al., 2008; Miller and Schuler, 2008). These models make use of a right-corner tree transform, based on the left-corner transform described by Johnson (1998), and are supported by corpus results suggesting that most sentences (in English, at least) can be parsed using a very small stack bound of three to four elements (Schuler et al., 2008). This raises an interesting question: if most sentences can be recognized with only three or four elements of stack memory, is the standard cubic-time CKY chart-parsing algorithm, which implicitly assumes an unbounded stack, wasting probability mass on trees whose complexity is beyond human re</context>
</contexts>
<marker>Miller, Schuler, 2008</marker>
<rawString>Tim Miller and William Schuler. 2008. A syntactic timeseries model for parsing fluent and disfluent speech. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>The magical number seven, plus or minus two: Some limits on our capacity for processing information.</title>
<date>1956</date>
<journal>Psychological Review,</journal>
<pages>63--81</pages>
<contexts>
<context position="5396" citStr="Miller, 1956" startWordPosition="809" endWordPosition="810">he alternative composition operations proposed in the CCG account, thought to be associated with available prosody and quantifier scope analyses.1 Other models (Abney and Johnson, 1991; Gibson, 1991) seek to explain human processing difficulties as a result of memory capacity limits in parsing ordinary phrase structure trees. The Abney-Johnson and Gibson models adopt a left-corner parsing strategy, of which the right-corner transform described in this paper is a variant, in order to minimize memory usage. But the transform-based model described in this paper exploits a conception of chunking (Miller, 1956) — in this case, grouping recognized words into stacked-up incomplete constituents — to operate within much stricter estimates of human shortterm memory bounds (Cowan, 2001) than assumed by Abney and Johnson. 1The lack of support for some of these available scope analyses may not necessarily be problematic for the present model. The complexity of interpreting nested raised quantifiers may place them beyond the capability of human interactive incremental interpretation, but not beyond the capability of post-hoc interpretation (understood after the listener has had time to think about it). Sever</context>
</contexts>
<marker>Miller, 1956</marker>
<rawString>George A. Miller. 1956. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63:81–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
<author>Mark A Paskin</author>
</authors>
<title>Linear time inference in hierarchical HMMs.</title>
<date>2001</date>
<booktitle>In Proc. NIPS,</booktitle>
<pages>833--840</pages>
<contexts>
<context position="7298" citStr="Murphy and Paskin, 2001" startWordPosition="1105" endWordPosition="1108">r upholds both the fixed-element and boundedmemory assumptions by hypothesizing fixed reductions of right child constituents into incomplete parents in the same memory element, to make room for new constituents that may be introduced at a later time. These in-element reductions are defined naturally on phrase structure trees as the result of aligning right-corner transformed constituent structures to sequences of random variables in a factored timeseries model. 3 Background The recognition model examined in this paper is a factored time-series model, based on a Hierarchic Hidden Markov Model (Murphy and Paskin, 2001), which probabilistically estimates the contents of a memory store of three to four partially-completed constituents over time. Probabilities for expansions, transitions and reductions in this model can be defined over trees in a training corpus, transformed and mapped to the random variables in an HHMM (Schuler et al., 2008). In Section 4 these probabilities will be computed directly from a probabilistic context-free grammar, in order to evaluate the contribution of stack bounds without introducing additional corpus context into the model. 3.1 A Bounded-Stack Model HHMMs are factored HMMs whi</context>
</contexts>
<marker>Murphy, Paskin, 2001</marker>
<rawString>Kevin P. Murphy and Mark A. Paskin. 2001. Linear time inference in hierarchical HMMs. In Proc. NIPS, pages 833–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="6092" citStr="Roark, 2001" startWordPosition="915" endWordPosition="916">to operate within much stricter estimates of human shortterm memory bounds (Cowan, 2001) than assumed by Abney and Johnson. 1The lack of support for some of these available scope analyses may not necessarily be problematic for the present model. The complexity of interpreting nested raised quantifiers may place them beyond the capability of human interactive incremental interpretation, but not beyond the capability of post-hoc interpretation (understood after the listener has had time to think about it). Several existing incremental systems are organized around a left-corner parsing strategy (Roark, 2001; Henderson, 2004). But these systems generally keep large numbers of constituents open for modifier attachment in each hypothesis. This allows modifiers to be attached as right children of any such open constituent. But if any number of open constituents are allowed, then either the assumption that stored elements have fixed syntactic (and semantic) structure will be violated, or the assumption that syntax operates within a bounded memory store will be violated, both of which are psycholinguistically attractive as simplifying assumptions. The HHMM model examined in this paper upholds both the</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Toward a psycholinguisticallymotivated model of language.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Manchester, UK,</location>
<contexts>
<context position="1596" citStr="Schuler et al., 2008" startWordPosition="233" endWordPosition="236"> in order to ensure a fair comparison between bounded-stack and unbounded PCFG parsing using a common underlying model; then it presents experimental results that show a bounded-stack right-corner parser using a transformed version of a grammar significantly outperforms an unboundedstack CKY parser using the original grammar. 1 Introduction Statistical parsing models have recently been proposed that employ a bounded stack in time-series (left-to-right) recognition, in order to directly and tractably incorporate incremental phenomena such as (co-)reference or disfluency into parsing decisions (Schuler et al., 2008; Miller and Schuler, 2008). These models make use of a right-corner tree transform, based on the left-corner transform described by Johnson (1998), and are supported by corpus results suggesting that most sentences (in English, at least) can be parsed using a very small stack bound of three to four elements (Schuler et al., 2008). This raises an interesting question: if most sentences can be recognized with only three or four elements of stack memory, is the standard cubic-time CKY chart-parsing algorithm, which implicitly assumes an unbounded stack, wasting probability mass on trees whose co</context>
<context position="7625" citStr="Schuler et al., 2008" startWordPosition="1155" endWordPosition="1158">the result of aligning right-corner transformed constituent structures to sequences of random variables in a factored timeseries model. 3 Background The recognition model examined in this paper is a factored time-series model, based on a Hierarchic Hidden Markov Model (Murphy and Paskin, 2001), which probabilistically estimates the contents of a memory store of three to four partially-completed constituents over time. Probabilities for expansions, transitions and reductions in this model can be defined over trees in a training corpus, transformed and mapped to the random variables in an HHMM (Schuler et al., 2008). In Section 4 these probabilities will be computed directly from a probabilistic context-free grammar, in order to evaluate the contribution of stack bounds without introducing additional corpus context into the model. 3.1 A Bounded-Stack Model HHMMs are factored HMMs which mimic a bounded-memory pushdown automaton (PDA), supporting simple push and pop operations on a bounded stack-like memory store. HMMs characterize speech or text as a sequence 345 of hidden states qt (in this case, stacked-up syntactic categories) and observed states ot (in this case, words) at corresponding time steps t. </context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2008</marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2008. Toward a psycholinguisticallymotivated model of language. In Proceedings of COLING, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press/Bradford Books,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4145" citStr="Steedman, 2000" startWordPosition="615" endWordPosition="617"> Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 344–352, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ply to entire probabilistic grammars, cast as infinite sets of generable trees; and Section 5 describes an evaluation of this transform on the Wall Street Journal corpus of the Penn Treebank showing improved results for a transformed bounded-stack version of a probabilistic grammar over the original unbounded grammar. 2 Related Work The model examined here is formally similar to Combinatorial Categorial Grammar (CCG) (Steedman, 2000). But the CCG account is a competence model as well as a performance model, in that it seeks to unify category representations used in processing with learned generalizations about argument structure; whereas the model described in this paper is exclusively a performance model, allowing generalizations about lexical argument structures to be learned in some other representation, then combined with probabilistic information about parsing strategies to yield a set of derived incomplete constituents. As a result, the model described in this paper has a freer hand to satisfy strict working memory </context>
<context position="13578" citStr="Steedman, 2000" startWordPosition="2176" endWordPosition="2177"> number of levels in the HHMM hierarchy. 3.2 Tree-Based Transforms The right-corner transform used in this paper is simply the left-right dual of a left-corner transform (Johnson, 1998). It transforms all right branching sequences in a phrase structure tree into left branching sequences of symbols of the form Aη/Aη·µ, denoting an incomplete instance of an ‘active’ category Aη lacking an instance of an ‘awaited’ category Aη·µ yet to come.3 These incomplete constituent categories have the same form and much of the same meaning as non-constituent categories in a Combinatorial Categorial Grammar (Steedman, 2000). 3Here rl and µ are node addresses in a binary-branching tree, defined as paths of left (0) or right (1) branches from the root. Rewrite rules for the right-corner transform are shown below:4 • Beginning case: the top of a right-expanding sequence in an ordinary phrase structure tree is mapped to the bottom of a left-expanding sequence in a right-corner transformed tree: This case of the right-corner transform may be considered a constrained version of CCG type raising. • Middle case: each subsequent branch in a right-expanding sequence of an ordinary phrase structure tree is mapped to a bran</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press/Bradford Books, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>