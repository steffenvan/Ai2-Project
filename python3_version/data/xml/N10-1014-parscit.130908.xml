<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000061">
<title confidence="0.994418">
Unsupervised Syntactic Alignment with Inversion Transduction Grammars
</title>
<author confidence="0.998722">
Adam Pauls Dan Klein
</author>
<affiliation confidence="0.997784">
Computer Science Division
University of California at Berkeley
</affiliation>
<email confidence="0.998829">
{adpauls,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994005" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995898071428571">
Syntactic machine translation systems cur-
rently use word alignments to infer syntactic
correspondences between the source and tar-
get languages. Instead, we propose an un-
supervised ITG alignment model that directly
aligns syntactic structures. Our model aligns
spans in a source sentence to nodes in a target
parse tree. We show that our model produces
syntactically consistent analyses where possi-
ble, while being robust in the face of syntactic
divergence. Alignment quality and end-to-end
translation experiments demonstrate that this
consistency yields higher quality alignments
than our baseline.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859380952381">
Syntactic machine translation has advanced signif-
icantly in recent years, and multiple variants cur-
rently achieve state-of-the-art translation quality.
Many of these systems exploit linguistically-derived
syntactic information either on the target side (Gal-
ley et al., 2006), the source side (Huang et al., 2006),
or both (Liu et al., 2009). Still others induce their
syntax from the data (Chiang, 2005). Despite differ-
ences in detail, the vast majority of syntactic meth-
ods share a critical dependence on word alignments.
In particular, they infer syntactic correspondences
between the source and target languages through
word alignment patterns, sometimes in combination
with constraints from parser outputs.
However, word alignments are not perfect indi-
cators of syntactic alignment, and syntactic systems
are very sensitive to word alignment behavior. Even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, while
unaligned words can result in an exponentially large
set of extractable rules to choose from. Researchers
</bodyText>
<author confidence="0.976853">
David Chiang Kevin Knight
</author>
<affiliation confidence="0.9937075">
Information Sciences Institute
University of Southern California
</affiliation>
<email confidence="0.994231">
{chiang,knight}@isi.edu
</email>
<bodyText confidence="0.9998795">
have worked to incorporate syntactic information
into word alignments, resulting in improvements to
both alignment quality (Cherry and Lin, 2006; DeN-
ero and Klein, 2007), and translation quality (May
and Knight, 2007; Fossum et al., 2008).
In this paper, we remove the dependence on word
alignments and instead directly model the syntactic
correspondences in the data, in a manner broadly
similar to Yamada and Knight (2001). In particu-
lar, we propose an unsupervised model that aligns
nodes of a parse tree (or forest) in one language to
spans of a sentence in another. Our model is an in-
stance of the inversion transduction grammar (ITG)
formalism (Wu, 1997), constrained in such a way
that one side of the synchronous derivation respects
a syntactic parse. Our model is best suited to sys-
tems which use source- or target-side trees only.
The design of our model is such that, for divergent
structures, a structurally integrated backoff to flatter
word-level (or null) analyses is available. There-
fore, our model is empirically robust to the case
where syntactic divergence between languages pre-
vents syntactically accurate ITG derivations.
We show that, with appropriate pruning, our
model can be efficiently trained on large parallel cor-
pora. When compared to standard word-alignment-
backed baselines, our model produces more con-
sistent analyses of parallel sentences, leading to
high-count, high-quality transfer rules. End-to-
end translation experiments demonstrate that these
higher quality rules improve translation quality by
1.0 BLEU over a word-alignment-backed baseline.
</bodyText>
<sectionHeader confidence="0.989814" genericHeader="method">
2 Syntactic Rule Extraction
</sectionHeader>
<bodyText confidence="0.984601">
Our model is intended for use in syntactic transla-
tion models which make use of syntactic parses on
either the target (Galley et al., 2006) or source side
(Huang et al., 2006; Liu et al., 2006). Our model’s
</bodyText>
<page confidence="0.953633">
118
</page>
<note confidence="0.845976">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118–126,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
S
</note>
<figureCaption confidence="0.573683">
Figure 1: A single incorrect alignment removes an ex-
tractable node, and hence several desirable rules. We
represent correct extractable nodes in bold, spurious ex-
tractable nodes with a *, and incorrectly blocked ex-
tractable nodes in bold strikethrough.
</figureCaption>
<bodyText confidence="0.999980647058824">
chief purpose is to align nodes in the syntactic parse
in one language to spans in the other – an alignment
we will refer to as a “syntactic” alignment. These
alignments are employed by standard syntactic rule
extraction algorithms, for example, the GHKM al-
gorithm of Galley et al. (2004). Following that work,
we will assume parses are present in the target lan-
guage, though our model applies in either direction.
Currently, although syntactic systems make use of
syntactic alignments, these alignments must be in-
duced indirectly from word-level alignments. Pre-
vious work has discussed at length the poor interac-
tion of word-alignments with syntactic rule extrac-
tion (DeNero and Klein, 2007; Fossum et al., 2008).
For completeness, we provide a brief example of this
interaction, but for a more detailed discussion we re-
fer the reader to these presentations.
</bodyText>
<subsectionHeader confidence="0.996988">
2.1 Interaction with Word Alignments
</subsectionHeader>
<bodyText confidence="0.99985408">
Syntactic systems begin rule extraction by first iden-
tifying, for each node in the target parse tree, a
span of the foreign sentence which (1) contains ev-
ery source word that aligns to a target word in the
yield of the node and (2) contains no source words
that align outside that yield. Only nodes for which
a non-empty span satisfying (1) and (2) exists may
form the root or leaf of a translation rule; for that
reason, we will refer to these nodes as extractable
nodes.
Since extractable nodes are inferred based on
word alignments, spurious word alignments can rule
out otherwise desirable extraction points. For exam-
ple, consider the alignment in Figure 1. This align-
ment, produced by GIZA++ (Och and Ney, 2003),
contains 4 correct alignments (the filled circles),
but incorrectly aligns the to the Chinese past tense
marker f (the hollow circle). This mistaken align-
ment produces the incorrect rule (DT → the ; ),
and also blocks the extraction of (VBN → fallen ;
Ai T).
More high-level syntactic transfer rules are also
ruled out, for example, the “the insertion rule” (NP
→ the NN1 NN2 ; NN1 NN2) and the high-level (S
→ NP1 VP2 ; NP1 VP2).
</bodyText>
<sectionHeader confidence="0.996305" genericHeader="method">
3 A Syntactic Alignment Model
</sectionHeader>
<bodyText confidence="0.971483214285714">
The most common approach to avoiding these prob-
lems is to inject knowledge about syntactic con-
straints into a word alignment model (Cherry and
Lin, 2006; DeNero and Klein, 2007; Fossum et al.,
2008).1 While syntactically aware, these models re-
main limited by the word alignment models that un-
derly them.
Here, we describe a model which directly infers
alignments of nodes in the target-language parse tree
to spans of the source sentence. Formally, our model
is an instance of a Synchronous Context-Free Gram-
mar (see Chiang (2004) for a review), or SCFG,
which generates an English (target) parse tree T and
foreign (source) sentence f given a target sentence e.
The generative process underlying this model pro-
duces a derivation d of SCFG rules, from which T
and f can be read off; because we condition on e,
the derivations produce e with probability 1. This
model places a distribution over T and f given by
p(T, f  |e) = 1: p(d  |e) = 1: ri p(r  |e)
d d rEd
where the sum is over derivations d which yield T
and f. The SCFG rules r come from one of 4 types,
pictured in Table 1. In general, because our model
can generate English trees, it permits inference over
forests. Although we will restrict ourselves to a sin-
gle parse tree for our experiments, in this section, we
discuss the more general case.
</bodyText>
<footnote confidence="0.997594333333333">
1One notable exception is May and Knight (2007), who pro-
duces syntactic alignments using syntactic rules derived from
word-aligned data.
</footnote>
<figure confidence="0.970038">
the trade surplus has drastically fallen
NP
DT* NN NN
VP
VBZ ADVP
RB VBN
R19
trade
iMA
surplus
MEX
drastically
A!P
fall
T
(past)
</figure>
<page confidence="0.994324">
119
</page>
<table confidence="0.9952956">
Rule Type Root English Foreign Example Instantiation
TERMINAL E e ft FOUR → four ; VQ
UNARY A B fl B fr CD → FOUR ;f FOUR
BINARYMONO A B C fl B f,,,, C fr NP → NN NN ;c NN RAJ NN e
BINARYINV A B C fl C f,,,, B fr PP → IN NP ; -t NP e IN e
</table>
<tableCaption confidence="0.986568">
Table 1: Types of rules present in the SCFG describing our model, along with some sample instantiations of each type.
Empty word sequences f have been explicitly marked with an e.
</tableCaption>
<bodyText confidence="0.998088675">
The first rule type is the TERMINAL production,
which rewrites a terminal symbol2 E as its En-
glish word e and a (possibly empty) sequence of
foreign words ft. Generally speaking, the majority
of foreign words are generated using this rule. It
is only when a straightforward word-to-word corre-
spondence cannot be found that our model resorts to
generating foreign words elsewhere.
We can also rewrite a non-terminal symbol A us-
ing a UNARY production, which on the English side
produces a single symbol B, and on the foreign side
produces the symbol B, with sequences of words fl
to its left and fr to its right.
Finally, there are two binary productions: BINA-
RYMONO rewrites A with two non-terminals B and
C on the English side, and the same non-terminals
B and C in monotonic order on the foreign side,
with sequences of words fl, fr, and fm to the left,
right, and the middle. BINARYINV inverts the or-
der in which the non-terminals B and C are written
on the source side, allowing our model to capture a
large subset of possible reorderings (Wu, 1997).
Derivations from this model have two key prop-
erties: first, the English side of a derivation is con-
strained to form a valid constituency parse, as is re-
quired in a syntax system with target-side syntax;
and second, for each parse node in the English pro-
jection, there is exactly one (possibly empty) con-
tiguous span of the foreign side which was gener-
ated from that non-terminal or one of its descen-
dants. Identifying extractable nodes from a deriva-
tion is thus trivial: any node aligned to a non-empty
foreign span is extractable.
In Figure 2, we show a sample sentence pair frag-
2For notational convenience, we imagine that for each par-
ticular English word e, there is a special preterminal symbol E
which produces it. These symbols E act like any other non-
terminal in the grammar with respect to the parameterization in
Section 3.1. To denote standard non-terminals, we will use A,
B, and C.
</bodyText>
<figure confidence="0.904760888888889">
PP
PP → IN NP ; r± NP
NP → DT NNS ; DT NNS
IN → BEFORE ;BEFORE
BEFORE → before ; I-L, SIJ
DT → THE ; THE
THE → the ;
NNS → ELECTIONS ; ELECTIONS
ELECTIONS → elections ;
</figure>
<figureCaption confidence="0.972596">
Figure 2: Top: A synchronous derivation of a small sen-
</figureCaption>
<bodyText confidence="0.982179">
tence pair fragment under our model. The English pro-
jection of the derivation represents a valid constituency
parse, while the foreign projection is less constrained.
We connect each foreign terminal with a dashed line to
the node in the English side of the synchronous deriva-
tion at which it is generated. The foreign span assigned
to each English node is indicated with indices. All nodes
with non-empty spans, shown in boldface, are extractable
nodes. Bottom: The SCFG rules used in the derivation.
ment as generated by our model. Our model cor-
rectly identifies that the English the aligns to nothing
on the foreign side. Our model also effectively cap-
tures the one-to-many alignment3 of elections to a
</bodyText>
<footnote confidence="0.628614666666667">
3While our model does not explicitly produce many-to-one
alignments, many-to-one rules can be discovered via rule com-
position (Galley et al., 2006).
</footnote>
<figure confidence="0.998664583333333">
PP[0,4]
NP[1,3]
IN[3,4]
before[3,4]
0 at 1 parliament 2 election 3 before 4
在 之前
DT
NNS
NP IN
DT[1,1] NNS[1,3]
the[1,1] elections[1,3]
IN
</figure>
<page confidence="0.962569">
120
</page>
<bodyText confidence="0.998540111111111">
k w&apos;1#. Finally, our model correctly analyzes the
Chinese circumposition ;(E ... �_&apos; Vit! (before ... ). In
this construction, orf carries the meaning of “be-
fore”, and thus correctly aligns to before, while ;(E
functions as a generic preposition, which our model
handles by attaching it to the PP. This analysis per-
mits the extraction of the general rule (PP → IN1
NP2 ; ;(E NP2 IN1), and the more lexicalized (PP →
before NP ; r± NP U).
</bodyText>
<subsectionHeader confidence="0.998982">
3.1 Parameterization
</subsectionHeader>
<bodyText confidence="0.997450466666667">
In principle, our model could have one parameter for
each instantiation r of a rule type. This model would
have an unmanageable number of parameters, pro-
ducing both computational and modeling issues – it
is well known that unsupervised models with large
numbers of parameters are prone to degenerate anal-
yses of the data (DeNero et al., 2006). One solution
might be to apply an informed prior with a compu-
tationally tractable inference procedure (e.g. Cohn
and Blunsom (2009) or Liu and Gildea (2009)). We
opt here for the simpler, statistically more robust so-
lution of making independence assumptions to keep
the number of parameters at a reasonable level.
Concretely, we define the probability of the BI-
NARYMONO rule,4
</bodyText>
<equation confidence="0.613327">
p(r = A → B C;fl B f,,,, C fT|A,eA)
</equation>
<bodyText confidence="0.984816727272727">
which conditions on the root of the rule A and the
English yield eA, as
pg(A → B C  |A, eA) · pinv(I  |B, C)·
pleft(fl  |A, eA)·pmid(fm  |A, eA)·pright(fr  |A, eA)
In words, we assume that the rule probability de-
composes into a monolingual PCFG grammar prob-
ability pg, an inversion probability pinv, and a proba-
bility of left, middle, and right word sequences pleft,
pmid, and pright.5 Because we condition on e, the
monolingual grammar probability pg must form a
distribution which produces e with probability 1.6
</bodyText>
<footnote confidence="0.96761475">
4In the text, we only describe the factorization for the BI-
NARYMONO rule. For a parameterization of all rules, we refer
the reader to Table 2.
5All parameters in our model are multinomial distributions.
6A simple case of such a distribution is one which places all
of its mass on a single tree. More complex distributions can be
obtained by conditioning an arbitrary PCFG on a (Goodman,
1998).
</footnote>
<bodyText confidence="0.990093">
We further assume that the probability of produc-
ing a foreign word sequence fl decomposes as:
</bodyText>
<equation confidence="0.9655295">
pleft(fl  |A, eA) = pl(|fl |= m  |A) H p(fj  |A, eA)
j=1
</equation>
<bodyText confidence="0.99821">
where m is the length of the sequence fl. The pa-
rameter pl is a left length distribution. The prob-
abilities pmid, pright, decompose in the same way,
except substituting a separate length distribution pm
and pr for pl. For the TERMINAL rule, we emit ft
with a similarly decomposed distribution pterm us-
ing length distribution pw.
We define the probability of generating a foreign
word fj as
</bodyText>
<equation confidence="0.9960235">
1
|eA |pt(fj  |ei)
</equation>
<bodyText confidence="0.998955866666667">
with i ∈ eA denoting an index ranging over the in-
dices of the English words contained in eA. The
reader may recognize the above expressions as the
probability assigned by IBM Model 1 (Brown et al.,
1993) of generating the words fl given the words eA,
with one important difference – the length m of the
foreign sentence is often not modeled, so the term
pl(|fl |= m  |A) is set to a constant and ignored.
Parameterizing this length allows our model to ef-
fectively control the number of words produced at
different levels of the derivation.
It is worth noting how each parameter affects the
model’s behavior. The pt distribution is a standard
“translation” table, familiar from the IBM Models.
The pinv distribution is a “distortion” parameter, and
models the likelihood of inverting non-terminals B
and C. This parameter can capture, for example,
the high likelihood that prepositions IN and noun
phrases NP often invert in Chinese due to its use
of postpositions. The non-terminal length distribu-
tions pl, pm, and pr model the probability of “back-
ing off” and emitting foreign words at non-terminals
when a more refined analysis cannot be found. If
these parameters place high mass on 0 length word
sequences, this heavily penalizes this backoff be-
haviour. For the TERMINAL rule, the length distri-
bution pw parameterizes the number of words pro-
duced for a particular English word e, functioning
similarly to the “fertilities” employed by IBM Mod-
els 3 and 4 (Brown et al., 1993). This allows us
</bodyText>
<equation confidence="0.734329666666667">
�
p(fj  |A, eA) =
iEe,4
</equation>
<page confidence="0.972998">
121
</page>
<bodyText confidence="0.99996">
to model, for example, the tendency of English de-
terminers the and a translate to nothing in the Chi-
nese, and of English names to align to multiple Chi-
nese words. In general, we expect an English word
to usually align to one Chinese word, and so we
place a weak Dirichlet prior on on the pe distribution
which puts extra mass on 1-length word sequences.
This is helpful for avoiding the “garbage collection”
(Moore, 2004) problem for rare words.
</bodyText>
<subsectionHeader confidence="0.99976">
3.2 Exploiting Non-Terminal Labels
</subsectionHeader>
<bodyText confidence="0.9999823">
There are often foreign words that do not correspond
well to any English word, which our model must
also handle. We elected for a simple augmentation
to our model to account for these words. When gen-
erating foreign word sequences f at a non-terminal
(i.e. via the UNARY or BINARY productions), we
also allow for the production of foreign words from
the non-terminal symbol A. We modify p(fj  |eA)
from the previous section to allow production of fj
directly from the non-terminal7 A:
</bodyText>
<equation confidence="0.999507">
p(fj  |eA) = p.t - p(fj  |A)
1
|eA|pt(fj  |ei)
</equation>
<bodyText confidence="0.999212761904762">
where pnt is a global binomial parameter which con-
trols how often such alignments are made.
This necessitates the inclusion of parameters like
pt( R`,  |NP) into our translation table. Generally,
these parameters do not contain much information,
but rather function like a traditional NULL rooted
at some position in the tree. However, in some
cases, the particular annotation used by the Penn
Treebank (Marcus et al., 1993) (and hence most
parsers) allows for some interesting parameters to
be learned. For example, we found that our aligner
often matched the Chinese word f, which marks
the past tense (among other things), to the preter-
minals VBD and VBN, which denote the English
simple past and perfect tense. Additionally, Chinese
measure words like ^ and -, often align to the CD
(numeral) preterminal. These generalizations can be
quite useful – where a particular number might pre-
dict a measure word quite poorly, the generalization
that measure words co-occur with the CD tag is very
robust.
</bodyText>
<footnote confidence="0.869362">
7For terminal symbols E, this production is not possible.
</footnote>
<subsectionHeader confidence="0.945594">
3.3 Membership in ITG
</subsectionHeader>
<bodyText confidence="0.999962285714286">
The generative process which describes our model
contains a class of grammars larger than the com-
putationally efficient class of ITG grammars. For-
tunately, the parameterization described above not
only reduces the number of parameters to a man-
ageable level, but also introduces independence as-
sumptions which permit synchronous binarization
(Zhang et al., 2006) of our grammar. Any SCFG that
can be synchronously binarized is an ITG, meaning
that our parameterization permits efficient inference
algorithms which we will make use of in the next
section. Although several binarizations are possi-
ble, we give one such binarization and its associated
probabilities in Table 2.
</bodyText>
<subsectionHeader confidence="0.984652">
3.4 Robustness to Syntactic Divergence
</subsectionHeader>
<bodyText confidence="0.999759866666666">
Generally speaking, ITG grammars have proven
more useful without the monolingual syntactic con-
straints imposed by a target parse tree. When deriva-
tions are restricted to respect a target-side parse tree,
many desirable alignments are ruled out when the
syntax of the two languages diverges, and align-
ment quality drops precipitously (Zhang and Gildea,
2004), though attempts have been made to address
this issue (Gildea, 2003).
Our model is designed to degrade gracefully in
the case of syntactic divergence. Because it can pro-
duce foreign words at any level of the derivation,
our model can effectively back off to a variant of
Model 1 in the case where an ITG derivation that
both respects the target parse tree and the desired
word-level alignments cannot be found.
For example, consider the sentence pair fragment
in Figure 3. It is not possible to produce an ITG
derivation of this fragment that both respects the
English tree and also aligns all foreign words to
their obvious English counterparts. Our model han-
dles this case by attaching the troublesome WT,� at
the uppermost VP. This analysis captures 3 of the
4 word-level correspondences, and also permits ex-
traction of abstract rules like (S —* NP VP ; NP VP)
and (NP —* the NN ; NN).
Unfortunately, this analysis leaves the English
word tomorrow with an empty foreign span, permit-
ting extraction of the incorrect translation (VP —*
announced tomorrow ; �Tj ), among others. Our
</bodyText>
<equation confidence="0.9850975">
� (� � p.t) - �
iEe,4
</equation>
<page confidence="0.991679">
122
</page>
<table confidence="0.9970289">
Rule Type Root English side Foreign side Probability
TERMINAL E e wt pterm(wt  |E)
UNARY A Bu wl Bu pg(A → B  |A)pleft(wl  |A, eA)
Bu B B wr pright(wr |A, eA)
BINARY A A1 wl A1 pleft(wl  |A, eA)
A1 B C1 B C1 pg(A → B C  |A)pinv(I=false  |B, C)
A1 B C1 C1 B pg(A → B C  |A)pinv(I=true  |B, C)
1 2 2 pmid(fm  |A, eA)
C C fm C pright(fr  |A, eA)
C2 C C fr
</table>
<tableCaption confidence="0.996514">
Table 2: A synchronous binarization of the SCFG describing our model.
</tableCaption>
<figure confidence="0.433277">
明天 将 公布 名�
</figure>
<figureCaption confidence="0.989314">
Figure 3: The graceful degradation of our model in the
</figureCaption>
<bodyText confidence="0.947923909090909">
face of syntactic divergence. It is not possible to align
all foreign words with their obvious English counterparts
with an ITG derivation. Instead, our model analyzes as
much as possible, but must resort to emitting RAS high
in the tree.
point here is not that our model’s analysis is “cor-
rect”, but “good enough” without resorting to more
computationally complicated models. In general,
our model follows an “extract as much as possi-
ble” approach. We hypothesize that this approach
will capture important syntactic generalizations, but
it also risks including low-quality rules. It is an em-
pirical question whether this approach is effective,
and we investigate this issue further in Section 5.3.
There are possibilities for improving our model’s
treatment of syntactic divergence. One option is
to allow the model to select trees which are more
consistent with the alignment (Burkett et al., 2010),
which our model can do since it permits efficient in-
ference over forests. The second is to modify the
generative process slightly, perhaps by including the
“clone” operator of Gildea (2003).
</bodyText>
<sectionHeader confidence="0.90725" genericHeader="method">
4 Learning and Inference
</sectionHeader>
<subsectionHeader confidence="0.981376">
4.1 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.99996525">
The parameters of our model can be efficiently
estimated in an unsupervised fashion using the
Expectation-Maximization (EM) algorithm. The E-
step requires the computation of expected counts un-
der our model for each multinomial parameter. We
omit the details of obtaining expected counts for
each distribution, since they can be obtained using
simple arithmetic from a single quantity, namely, the
expected count of a particular instantiation of a syn-
chronous rule r. This expectation is a standard quan-
tity that can be computed in O(n6) time using the
bitext Inside-Outside dynamic program (Wu, 1997).
</bodyText>
<subsectionHeader confidence="0.936449">
4.2 Dynamic Program Pruning
</subsectionHeader>
<bodyText confidence="0.999966047619048">
While our model permits O(n6) inference over a
forest of English trees, inference over a full forest
would be very slow, and so we fix a single n-ary En-
glish tree obtained from a monolingual parser. How-
ever, it is worth noting that the English side of the
ITG derivation is not completely fixed. Where our
English trees are more than binary branching, we
permit any binarization in our dynamic program.
For efficiency, we also ruled out span alignments
that are extremely lopsided, for example, a 1-word
English span aligned to a 20-word foreign span.
Specifically, we pruned any span alignment in which
one side is more than 5 times larger than the other.
Finally, we employ pruning based on high-
precision alignments from simpler models (Cherry
and Lin, 2007; Haghighi et al., 2009). We com-
pute word-to-word alignments by finding all word
pairs which have a posterior of at least 0.7 according
to both forward and reverse IBM Model 1 parame-
ters, and prune any span pairs which invalidate more
than 3 of these alignments. In total, this pruning re-
</bodyText>
<figure confidence="0.989245">
VP[2,3]
VP[2,3]
VBN[2,3] NN[3,3]
MD[1,2]
DT[3,3] NN[3,4]
VB[2,2]
NP[3,4]
VP[0,3]
S[0,4]
the[3,3] list[3,4] will[1,2] be[2,2] announced[2,3] tomorrow[3,3]
0 tomorrow 1 will 2 announce 3 list 4
</figure>
<page confidence="0.9944">
123
</page>
<table confidence="0.999909833333333">
Span P R F1
Syntactic Alignment 50.9 83.0 63.1
GIZA++ 56.1 67.3 61.2
Rule P R F1
Syntactic Alignment 39.6 40.3 39.9
GIZA++ 46.2 34.7 39.6
</table>
<tableCaption confidence="0.9957595">
Table 3: Alignment quality results for our syntactic
aligner and our GIZA++ baseline.
</tableCaption>
<bodyText confidence="0.995082666666667">
duced computation from approximately 1.5 seconds
per sentence to about 0.3 seconds per sentence, a
speed-up of a factor of 5.
</bodyText>
<subsectionHeader confidence="0.999415">
4.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999742636363636">
Given a trained model, we extract a tree-to-string
alignment as follows: we compute, for each node
in the English tree, the posterior probability of a
particular foreign span assignment using the same
dynamic program needed for EM. We then com-
pute the set of span assignments which maximizes
the sum of these posteriors, constrained such that
the foreign span assignments nest in the obvious
way. This algorithm is a natural synchronous gener-
alization of the monolingual Maximum Constituents
Parse algorithm of Goodman (1996).
</bodyText>
<sectionHeader confidence="0.999702" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999896">
5.1 Alignment Quality
</subsectionHeader>
<bodyText confidence="0.999989294117647">
We first evaluated our alignments against gold stan-
dard annotations. Our training data consisted of the
2261 manually aligned and translated sentences of
the Chinese Treebank (Bies et al., 2007) and approx-
imately half a million unlabeled sentences of parallel
Chinese-English newswire. The unlabeled data was
subsampled (Li et al., 2009) from a larger corpus by
selecting sentences which have good tune and test
set coverage, and limited to sentences of length at
most 40. We parsed the English side of the train-
ing data with the Berkeley parser.8 For our baseline
alignments, we used GIZA++, trained in the stan-
dard way.9 We used the grow-diag-final alignment
heuristic, as we found it outperformed union in early
experiments.
We trained our unsupervised syntactic aligner on
the concatenation of the labelled and unlabelled
</bodyText>
<footnote confidence="0.754082">
8http://code.google.com/p/berkeleyparser/
95 iterations of model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4.
</footnote>
<bodyText confidence="0.999370684210527">
data. As is standard in unsupervised alignment mod-
els, we initialized the translation parameters pt by
first training 5 iterations of IBM Model 1 using the
joint training algorithm of Liang et al. (2006), and
then trained our model for 5 EM iterations. We
extracted syntactic rules using a re-implementation
of the Galley et al. (2006) algorithm from both our
syntactic alignments and the GIZA++ alignments.
We handle null-aligned words by extracting every
consistent derivation, and extracted composed rules
consisting of at most 3 minimal rules.
We evaluate our alignments against the gold stan-
dard in two ways. We calculated Span F-score,
which compares the set of extractable nodes paired
with a foreign span, and Rule F-score (Fossum et al.,
2008) over minimal rules. The results are shown in
Table 3. By both measures, our syntactic aligner ef-
fectively trades recall for precision when compared
to our baseline, slightly increasing overall F-score.
</bodyText>
<subsectionHeader confidence="0.999601">
5.2 Translation Quality
</subsectionHeader>
<bodyText confidence="0.99999145">
For our translation system, we used a re-
implementation of the syntactic system of Galley et
al. (2006). For the translation rules extracted from
our data, we computed standard features based on
relative frequency counts, and tuned their weights
using MERT (Och, 2003). We also included a
language model feature, using a 5-gram language
model trained on 220 million words of English text
using the SRILM Toolkit (Stolcke, 2002).
For tuning and test data, we used a subset of the
NIST MT04 and MT05 with sentences of length at
most 40. We used the first 1000 sentences of this set
for tuning and the remaining 642 sentences as test
data. We used the decoder described in DeNero et
al. (2009) during both tuning and testing.
We provide final tune and test set results in Ta-
ble 4. Our alignments produce a 1.0 BLEU improve-
ment over the baseline. Our reported syntactic re-
sults were obtained when rules were thresholded by
count; we discuss this in the next section.
</bodyText>
<subsectionHeader confidence="0.998662">
5.3 Analysis
</subsectionHeader>
<bodyText confidence="0.9998566">
As discussed in Section 3.4, our aligner is designed
to extract many rules, which risks inadvertently ex-
tracting low-quality rules. To quantify this, we
first examined the number of rules extracted by our
aligner as compared with GIZA++. After relativiz-
</bodyText>
<page confidence="0.992709">
124
</page>
<table confidence="0.9990475">
Tune Test
Syntactic Alignment 29.78 29.83
GIZA++ 28.76 28.84
GIZA++ high count 25.51 25.38
</table>
<tableCaption confidence="0.9543066">
Table 4: Final tune and test set results for our grammars
extracted using the baseline GIZA++ alignments and our
syntactic aligner. When we filter the GIZA++ grammars
with the same count thresholds used for our aligner (“high
count”), BLEU score drops substantially.
</tableCaption>
<bodyText confidence="0.999921435897436">
ing to the tune and test set, we extracted approx-
imately 32 million unique rules using our aligner,
but only 3 million with GIZA++. To check that
we were not just extracting extra low-count, low-
quality rules, we plotted the number of rules with
a particular count in Figure 4. We found that while
our aligner certainly extracts many more low-count
rules, it also extracts many more high-count rules.
Of course, high-count rules are not guaranteed
to be high quality. To verify that frequent rules
were better for translation, we experimented with
various methods of thresholding to remove rules
with low count extracted from using aligner. We
found in early development found that removing
low-count rules improved translation performance
substantially. In particular, we settled on the follow-
ing scheme: we kept all rules with a single foreign
terminal on the right-hand side. For entirely lexical
(gapless) rules, we kept all rules occurring at least
3 times. For unlexicalized rules, we kept all rules
occurring at least 20 times per gap. For rules which
mixed gaps and lexical items, we kept all rules oc-
curring at least 10 times per gap. This left us with
a grammar about 600 000 rules, the same grammar
which gave us our final results reported in Table 4.
In contrast to our syntactic aligner, rules extracted
using GIZA++ could not be so aggressively pruned.
When pruned using the same count thresholds, ac-
curacy dropped by more than 3.0 BLEU on the tune
set, and similarly on the test set (see Table 4). To
obtain the accuracy shown in our final results (our
best results with GIZA++), we had to adjust the
count threshold to include all lexicalized rules, all
unlexicalized rules, and mixed rules occurring at
least twice per gap. With these count thresholds, the
GIZA++ grammar contained about 580 000 rules,
roughly the same number as our syntactic grammar.
We also manually searched the grammars for
rules that had high count in the syntactically-
</bodyText>
<figure confidence="0.994655">
0 200 400 600 800 1000
Count
</figure>
<figureCaption confidence="0.99651225">
Figure 4: Number of extracted translation rules with a
particular count. Grammars extracted from our syntactic
aligner produce not only more low-count rules, but also
more high-count rules than GIZA++.
</figureCaption>
<bodyText confidence="0.999896076923077">
extracted grammar and low (or 0) count in the
GIZA++ grammar. Of course, we can always
cherry-pick such examples, but a few rules were il-
luminating. For example, for the ;(E ... ;L1 Vit! con-
struction discussed earlier, our aligner permits ex-
traction of the general rule (PP —* IN1 NP2 ; r± NP2
IN1) 3087 times, and the lexicalized rule (PP —* be-
fore NP ; ;(E NP orf) 118 times. In constrast, the
GIZA++ grammar extracts the latter only 23 times
and the former not at all. The more complex rule
(NP —* NP2 , who S1 , ; S1 R`, NP2), which captures
a common appositive construction, was absent from
the GIZA++ grammar but occurred 63 in ours.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999875846153846">
We have described a syntactic alignment model
which explicitly aligns nodes of a syntactic parse in
one language to spans in another, making it suitable
for use in many syntactic translation systems. Our
model is unsupervised and can be efficiently trained
with a straightforward application of EM. We have
demonstrated that our model can accurately capture
many syntactic correspondences, and is robust in the
face of syntactic divergence between language pairs.
Our aligner permits the extraction of more reliable,
high-count rules when compared to a standard word-
alignment baseline. These high-count rules also pro-
duce improvements in BLEU score.
</bodyText>
<sectionHeader confidence="0.997114" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<footnote confidence="0.380589">
This project is funded in part by the NSF under grant 0643742;
by BBN under DARPA contract HR0011-06-C-0022; and an
NSERC Postgraduate Fellowship. The authors would like to
thank Michael Auli for his input.
</footnote>
<figure confidence="0.98413425">
1e+00 1e+02 1e+04 1e+06
Syntactic
GUA_
Number of rules with count
</figure>
<page confidence="0.991832">
125
</page>
<sectionHeader confidence="0.989078" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999806557522124">
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007.
English chinese translation treebank v 1.0. web download.
In LDC2007T02.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263–311.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint pars-
ing and alignment with weakly synchronized grammar. In
Proceedings of the North American Association for Compu-
tational Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints
for word alignment through discriminative training. In Pro-
ceedings of the Association of Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion transduction
grammar for joint phrasal translation modeling. In Workshop
on Syntax and Structure in Statistical Translation.
David Chiang. 2004. Evaluating grammar formalisms for ap-
plications to natural language processing and biological se-
quence analysis. Ph.D. thesis, University of Pennsylvania.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In The Annual Conference of
the Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of
syntax-directed tree to string grammar induction. In Pro-
ceedings of the Conference on Emprical Methods for Natural
Language Processing.
John DeNero and Dan Klein. 2007. Tailoring word alignments
to syntactic machine translation. In The Annual Conference
of the Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006.
Why generative phrase models underperform surface heuris-
tics. In Workshop on Statistical Machine Translation at
NAACL.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of NAACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Us-
ing syntax to improve word alignment precision for syntax-
based machine translation. In Proceedings of the Third
Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proceed-
ings of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In Proceedings of the Association for Compu-
tational Linguistics.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Association for Computational Linguis-
tics.
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Aria Haghighi, John Blitzer, John Denero, and Dan Klein.
2009. Better word alignments with supervised itg models.
In Proceedings of the Association for Computational Lin-
guistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of locality.
In Proceedings of CHSLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch,
Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton,
Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an
open source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical Ma-
chine Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by
agreement. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2009. Bayesian learning of
phrasal tree-to-string templates. In Proceedings of EMNLP.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation. In
Proceedings of the Association for Computational Linguis-
tics.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-
tree translation with packed forests. In Proceedings of ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: The Penn Treebank.
In Computational Linguistics.
Jonathan May and Kevin Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of the Con-
ference on Emprical Methods for Natural Language Pro-
cessing.
Robert C. Moore. 2004. Improving ibm word alignment model
1. In The Annual Conference of the Association for Compu-
tational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29:19–51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the Association
for Computational Linguistics.
Andreas Stolcke. 2002. SRILM: An extensible language mod-
eling toolkit. In ICSLP 2002.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23:377–404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the Association of
Computational Linguistics.
Hao Zhang and Daniel Gildea. 2004. Syntax-based alignment:
supervised or unsupervised? In Proceedings of the Confer-
ence on Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proceedings of the North American Chapter of the Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.998529">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973577">
<title confidence="0.99972">Unsupervised Syntactic Alignment with Inversion Transduction Grammars</title>
<author confidence="0.999704">Adam Pauls Dan</author>
<affiliation confidence="0.992152">Computer Science University of California at</affiliation>
<abstract confidence="0.9993042">Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages. Instead, we propose an unsupervised ITG alignment model that directly aligns syntactic structures. Our model aligns spans in a source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Martha Palmer</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
</authors>
<title>English chinese translation treebank v 1.0. web download.</title>
<date>2007</date>
<booktitle>In LDC2007T02.</booktitle>
<contexts>
<context position="24584" citStr="Bies et al., 2007" startWordPosition="4134" endWordPosition="4137">bility of a particular foreign span assignment using the same dynamic program needed for EM. We then compute the set of span assignments which maximizes the sum of these posteriors, constrained such that the foreign span assignments nest in the obvious way. This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). 5 Experiments 5.1 Alignment Quality We first evaluated our alignments against gold standard annotations. Our training data consisted of the 2261 manually aligned and translated sentences of the Chinese Treebank (Bies et al., 2007) and approximately half a million unlabeled sentences of parallel Chinese-English newswire. The unlabeled data was subsampled (Li et al., 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseline alignments, we used GIZA++, trained in the standard way.9 We used the grow-diag-final alignment heuristic, as we found it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation</context>
</contexts>
<marker>Bies, Palmer, Mott, Warner, 2007</marker>
<rawString>Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007. English chinese translation treebank v 1.0. web download. In LDC2007T02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="14383" citStr="Brown et al., 1993" startWordPosition="2425" endWordPosition="2428">eA) j=1 where m is the length of the sequence fl. The parameter pl is a left length distribution. The probabilities pmid, pright, decompose in the same way, except substituting a separate length distribution pm and pr for pl. For the TERMINAL rule, we emit ft with a similarly decomposed distribution pterm using length distribution pw. We define the probability of generating a foreign word fj as 1 |eA |pt(fj |ei) with i ∈ eA denoting an index ranging over the indices of the English words contained in eA. The reader may recognize the above expressions as the probability assigned by IBM Model 1 (Brown et al., 1993) of generating the words fl given the words eA, with one important difference – the length m of the foreign sentence is often not modeled, so the term pl(|fl |= m |A) is set to a constant and ignored. Parameterizing this length allows our model to effectively control the number of words produced at different levels of the derivation. It is worth noting how each parameter affects the model’s behavior. The pt distribution is a standard “translation” table, familiar from the IBM Models. The pinv distribution is a “distortion” parameter, and models the likelihood of inverting non-terminals B and C</context>
<context position="15657" citStr="Brown et al., 1993" startWordPosition="2639" endWordPosition="2642">elihood that prepositions IN and noun phrases NP often invert in Chinese due to its use of postpositions. The non-terminal length distributions pl, pm, and pr model the probability of “backing off” and emitting foreign words at non-terminals when a more refined analysis cannot be found. If these parameters place high mass on 0 length word sequences, this heavily penalizes this backoff behaviour. For the TERMINAL rule, the length distribution pw parameterizes the number of words produced for a particular English word e, functioning similarly to the “fertilities” employed by IBM Models 3 and 4 (Brown et al., 1993). This allows us � p(fj |A, eA) = iEe,4 121 to model, for example, the tendency of English determiners the and a translate to nothing in the Chinese, and of English names to align to multiple Chinese words. In general, we expect an English word to usually align to one Chinese word, and so we place a weak Dirichlet prior on on the pe distribution which puts extra mass on 1-length word sequences. This is helpful for avoiding the “garbage collection” (Moore, 2004) problem for rare words. 3.2 Exploiting Non-Terminal Labels There are often foreign words that do not correspond well to any English wo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics.</booktitle>
<contexts>
<context position="21349" citStr="Burkett et al., 2010" startWordPosition="3610" endWordPosition="3613">sis is “correct”, but “good enough” without resorting to more computationally complicated models. In general, our model follows an “extract as much as possible” approach. We hypothesize that this approach will capture important syntactic generalizations, but it also risks including low-quality rules. It is an empirical question whether this approach is effective, and we investigate this issue further in Section 5.3. There are possibilities for improving our model’s treatment of syntactic divergence. One option is to allow the model to select trees which are more consistent with the alignment (Burkett et al., 2010), which our model can do since it permits efficient inference over forests. The second is to modify the generative process slightly, perhaps by including the “clone” operator of Gildea (2003). 4 Learning and Inference 4.1 Parameter Estimation The parameters of our model can be efficiently estimated in an unsupervised fashion using the Expectation-Maximization (EM) algorithm. The Estep requires the computation of expected counts under our model for each multinomial parameter. We omit the details of obtaining expected counts for each distribution, since they can be obtained using simple arithmet</context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammar. In Proceedings of the North American Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="2138" citStr="Cherry and Lin, 2006" startWordPosition="294" endWordPosition="297">ts. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers David Chiang Kevin Knight Information Sciences Institute University of Southern California {chiang,knight}@isi.edu have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a s</context>
<context position="6495" citStr="Cherry and Lin, 2006" startWordPosition="1007" endWordPosition="1010">2003), contains 4 correct alignments (the filled circles), but incorrectly aligns the to the Chinese past tense marker f (the hollow circle). This mistaken alignment produces the incorrect rule (DT → the ; ), and also blocks the extraction of (VBN → fallen ; Ai T). More high-level syntactic transfer rules are also ruled out, for example, the “the insertion rule” (NP → the NN1 NN2 ; NN1 NN2) and the high-level (S → NP1 VP2 ; NP1 VP2). 3 A Syntactic Alignment Model The most common approach to avoiding these problems is to inject knowledge about syntactic constraints into a word alignment model (Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008).1 While syntactically aware, these models remain limited by the word alignment models that underly them. Here, we describe a model which directly infers alignments of nodes in the target-language parse tree to spans of the source sentence. Formally, our model is an instance of a Synchronous Context-Free Grammar (see Chiang (2004) for a review), or SCFG, which generates an English (target) parse tree T and foreign (source) sentence f given a target sentence e. The generative process underlying this model produces a derivation d of SCFG rules, from </context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discriminative training. In Proceedings of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="22980" citStr="Cherry and Lin, 2007" startWordPosition="3876" endWordPosition="3879">ngle n-ary English tree obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we pruned any span alignment in which one side is more than 5 times larger than the other. Finally, we employ pruning based on highprecision alignments from simpler models (Cherry and Lin, 2007; Haghighi et al., 2009). We compute word-to-word alignments by finding all word pairs which have a posterior of at least 0.7 according to both forward and reverse IBM Model 1 parameters, and prune any span pairs which invalidate more than 3 of these alignments. In total, this pruning reVP[2,3] VP[2,3] VBN[2,3] NN[3,3] MD[1,2] DT[3,3] NN[3,4] VB[2,2] NP[3,4] VP[0,3] S[0,4] the[3,3] list[3,4] will[1,2] be[2,2] announced[2,3] tomorrow[3,3] 0 tomorrow 1 will 2 announce 3 list 4 123 Span P R F1 Syntactic Alignment 50.9 83.0 63.1 GIZA++ 56.1 67.3 61.2 Rule P R F1 Syntactic Alignment 39.6 40.3 39.9 </context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Evaluating grammar formalisms for applications to natural language processing and biological sequence analysis.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6873" citStr="Chiang (2004)" startWordPosition="1071" endWordPosition="1072">NN1 NN2) and the high-level (S → NP1 VP2 ; NP1 VP2). 3 A Syntactic Alignment Model The most common approach to avoiding these problems is to inject knowledge about syntactic constraints into a word alignment model (Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008).1 While syntactically aware, these models remain limited by the word alignment models that underly them. Here, we describe a model which directly infers alignments of nodes in the target-language parse tree to spans of the source sentence. Formally, our model is an instance of a Synchronous Context-Free Grammar (see Chiang (2004) for a review), or SCFG, which generates an English (target) parse tree T and foreign (source) sentence f given a target sentence e. The generative process underlying this model produces a derivation d of SCFG rules, from which T and f can be read off; because we condition on e, the derivations produce e with probability 1. This model places a distribution over T and f given by p(T, f |e) = 1: p(d |e) = 1: ri p(r |e) d d rEd where the sum is over derivations d which yield T and f. The SCFG rules r come from one of 4 types, pictured in Table 1. In general, because our model can generate English</context>
</contexts>
<marker>Chiang, 2004</marker>
<rawString>David Chiang. 2004. Evaluating grammar formalisms for applications to natural language processing and biological sequence analysis. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1214" citStr="Chiang, 2005" startWordPosition="168" endWordPosition="169">e being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponent</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Emprical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="12438" citStr="Cohn and Blunsom (2009)" startWordPosition="2079" endWordPosition="2082"> permits the extraction of the general rule (PP → IN1 NP2 ; ;(E NP2 IN1), and the more lexicalized (PP → before NP ; r± NP U). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the BINARYMONO rule,4 p(r = A → B C;fl B f,,,, C fT|A,eA) which conditions on the root of the rule A and the English yield eA, as pg(A → B C |A, eA) · pinv(I |B, C)· pleft(fl |A, eA)·pmid(fm |A, eA)·pright(fr |A, eA) In words, we assume that the rule probability decomposes into a monolingual PCFG grammar probability pg, an inversion probability pinv, and a probability of le</context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proceedings of the Conference on Emprical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2163" citStr="DeNero and Klein, 2007" startWordPosition="298" endWordPosition="302">nments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers David Chiang Kevin Knight Information Sciences Institute University of Southern California {chiang,knight}@isi.edu have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model</context>
<context position="4958" citStr="DeNero and Klein, 2007" startWordPosition="739" endWordPosition="742">e to spans in the other – an alignment we will refer to as a “syntactic” alignment. These alignments are employed by standard syntactic rule extraction algorithms, for example, the GHKM algorithm of Galley et al. (2004). Following that work, we will assume parses are present in the target language, though our model applies in either direction. Currently, although syntactic systems make use of syntactic alignments, these alignments must be induced indirectly from word-level alignments. Previous work has discussed at length the poor interaction of word-alignments with syntactic rule extraction (DeNero and Klein, 2007; Fossum et al., 2008). For completeness, we provide a brief example of this interaction, but for a more detailed discussion we refer the reader to these presentations. 2.1 Interaction with Word Alignments Syntactic systems begin rule extraction by first identifying, for each node in the target parse tree, a span of the foreign sentence which (1) contains every source word that aligns to a target word in the yield of the node and (2) contains no source words that align outside that yield. Only nodes for which a non-empty span satisfying (1) and (2) exists may form the root or leaf of a transla</context>
<context position="6519" citStr="DeNero and Klein, 2007" startWordPosition="1011" endWordPosition="1014">ect alignments (the filled circles), but incorrectly aligns the to the Chinese past tense marker f (the hollow circle). This mistaken alignment produces the incorrect rule (DT → the ; ), and also blocks the extraction of (VBN → fallen ; Ai T). More high-level syntactic transfer rules are also ruled out, for example, the “the insertion rule” (NP → the NN1 NN2 ; NN1 NN2) and the high-level (S → NP1 VP2 ; NP1 VP2). 3 A Syntactic Alignment Model The most common approach to avoiding these problems is to inject knowledge about syntactic constraints into a word alignment model (Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008).1 While syntactically aware, these models remain limited by the word alignment models that underly them. Here, we describe a model which directly infers alignments of nodes in the target-language parse tree to spans of the source sentence. Formally, our model is an instance of a Synchronous Context-Free Grammar (see Chiang (2004) for a review), or SCFG, which generates an English (target) parse tree T and foreign (source) sentence f given a target sentence e. The generative process underlying this model produces a derivation d of SCFG rules, from which T and f can be rea</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Workshop on Statistical Machine Translation at NAACL.</booktitle>
<contexts>
<context position="12305" citStr="DeNero et al., 2006" startWordPosition="2058" endWordPosition="2061">y aligns to before, while ;(E functions as a generic preposition, which our model handles by attaching it to the PP. This analysis permits the extraction of the general rule (PP → IN1 NP2 ; ;(E NP2 IN1), and the more lexicalized (PP → before NP ; r± NP U). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the BINARYMONO rule,4 p(r = A → B C;fl B f,,,, C fT|A,eA) which conditions on the root of the rule A and the English yield eA, as pg(A → B C |A, eA) · pinv(I |B, C)· pleft(fl |A, eA)·pmid(fm |A, eA)·pright(fr |A, eA) In words, we assume that t</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In Workshop on Statistical Machine Translation at NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Mohit Bansal</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Efficient parsing for transducer grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="27027" citStr="DeNero et al. (2009)" startWordPosition="4531" endWordPosition="4534">ic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic results were obtained when rules were thresholded by count; we discuss this in the next section. 5.3 Analysis As discussed in Section 3.4, our aligner is designed to extract many rules, which risks inadvertently extracting low-quality rules. To quantify this, we first examined the number of rules extracted by our aligner as compared with GIZA++. After relativiz124 Tune Test Syntactic Alignment 29.78 29.83 GIZA++ 28.76 28</context>
</contexts>
<marker>DeNero, Bansal, Pauls, Klein, 2009</marker>
<rawString>John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. 2009. Efficient parsing for transducer grammars. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improve word alignment precision for syntaxbased machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="2232" citStr="Fossum et al., 2008" startWordPosition="310" endWordPosition="313">systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers David Chiang Kevin Knight Information Sciences Institute University of Southern California {chiang,knight}@isi.edu have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees onl</context>
<context position="4980" citStr="Fossum et al., 2008" startWordPosition="743" endWordPosition="746">– an alignment we will refer to as a “syntactic” alignment. These alignments are employed by standard syntactic rule extraction algorithms, for example, the GHKM algorithm of Galley et al. (2004). Following that work, we will assume parses are present in the target language, though our model applies in either direction. Currently, although syntactic systems make use of syntactic alignments, these alignments must be induced indirectly from word-level alignments. Previous work has discussed at length the poor interaction of word-alignments with syntactic rule extraction (DeNero and Klein, 2007; Fossum et al., 2008). For completeness, we provide a brief example of this interaction, but for a more detailed discussion we refer the reader to these presentations. 2.1 Interaction with Word Alignments Syntactic systems begin rule extraction by first identifying, for each node in the target parse tree, a span of the foreign sentence which (1) contains every source word that aligns to a target word in the yield of the node and (2) contains no source words that align outside that yield. Only nodes for which a non-empty span satisfying (1) and (2) exists may form the root or leaf of a translation rule; for that re</context>
<context position="6541" citStr="Fossum et al., 2008" startWordPosition="1015" endWordPosition="1018">ed circles), but incorrectly aligns the to the Chinese past tense marker f (the hollow circle). This mistaken alignment produces the incorrect rule (DT → the ; ), and also blocks the extraction of (VBN → fallen ; Ai T). More high-level syntactic transfer rules are also ruled out, for example, the “the insertion rule” (NP → the NN1 NN2 ; NN1 NN2) and the high-level (S → NP1 VP2 ; NP1 VP2). 3 A Syntactic Alignment Model The most common approach to avoiding these problems is to inject knowledge about syntactic constraints into a word alignment model (Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008).1 While syntactically aware, these models remain limited by the word alignment models that underly them. Here, we describe a model which directly infers alignments of nodes in the target-language parse tree to spans of the source sentence. Formally, our model is an instance of a Synchronous Context-Free Grammar (see Chiang (2004) for a review), or SCFG, which generates an English (target) parse tree T and foreign (source) sentence f given a target sentence e. The generative process underlying this model produces a derivation d of SCFG rules, from which T and f can be read off; because we cond</context>
<context position="26111" citStr="Fossum et al., 2008" startWordPosition="4377" endWordPosition="4380">f IBM Model 1 using the joint training algorithm of Liang et al. (2006), and then trained our model for 5 EM iterations. We extracted syntactic rules using a re-implementation of the Galley et al. (2006) algorithm from both our syntactic alignments and the GIZA++ alignments. We handle null-aligned words by extracting every consistent derivation, and extracted composed rules consisting of at most 3 minimal rules. We evaluate our alignments against the gold standard in two ways. We calculated Span F-score, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table 3. By both measures, our syntactic aligner effectively trades recall for precision when compared to our baseline, slightly increasing overall F-score. 5.2 Translation Quality For our translation system, we used a reimplementation of the syntactic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improve word alignment precision for syntaxbased machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4555" citStr="Galley et al. (2004)" startWordPosition="677" endWordPosition="680">les, California, June 2010. c�2010 Association for Computational Linguistics S Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chief purpose is to align nodes in the syntactic parse in one language to spans in the other – an alignment we will refer to as a “syntactic” alignment. These alignments are employed by standard syntactic rule extraction algorithms, for example, the GHKM algorithm of Galley et al. (2004). Following that work, we will assume parses are present in the target language, though our model applies in either direction. Currently, although syntactic systems make use of syntactic alignments, these alignments must be induced indirectly from word-level alignments. Previous work has discussed at length the poor interaction of word-alignments with syntactic rule extraction (DeNero and Klein, 2007; Fossum et al., 2008). For completeness, we provide a brief example of this interaction, but for a more detailed discussion we refer the reader to these presentations. 2.1 Interaction with Word Al</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1085" citStr="Galley et al., 2006" startWordPosition="142" endWordPosition="146"> source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spur</context>
<context position="3742" citStr="Galley et al., 2006" startWordPosition="546" endWordPosition="549">G derivations. We show that, with appropriate pruning, our model can be efficiently trained on large parallel corpora. When compared to standard word-alignmentbacked baselines, our model produces more consistent analyses of parallel sentences, leading to high-count, high-quality transfer rules. End-toend translation experiments demonstrate that these higher quality rules improve translation quality by 1.0 BLEU over a word-alignment-backed baseline. 2 Syntactic Rule Extraction Our model is intended for use in syntactic translation models which make use of syntactic parses on either the target (Galley et al., 2006) or source side (Huang et al., 2006; Liu et al., 2006). Our model’s 118 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118–126, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics S Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chief purpose is to align nodes in the syntactic parse in one language to s</context>
<context position="11360" citStr="Galley et al., 2006" startWordPosition="1893" endWordPosition="1896"> the English side of the synchronous derivation at which it is generated. The foreign span assigned to each English node is indicated with indices. All nodes with non-empty spans, shown in boldface, are extractable nodes. Bottom: The SCFG rules used in the derivation. ment as generated by our model. Our model correctly identifies that the English the aligns to nothing on the foreign side. Our model also effectively captures the one-to-many alignment3 of elections to a 3While our model does not explicitly produce many-to-one alignments, many-to-one rules can be discovered via rule composition (Galley et al., 2006). PP[0,4] NP[1,3] IN[3,4] before[3,4] 0 at 1 parliament 2 election 3 before 4 在 之前 DT NNS NP IN DT[1,1] NNS[1,3] the[1,1] elections[1,3] IN 120 k w&apos;1#. Finally, our model correctly analyzes the Chinese circumposition ;(E ... �_&apos; Vit! (before ... ). In this construction, orf carries the meaning of “before”, and thus correctly aligns to before, while ;(E functions as a generic preposition, which our model handles by attaching it to the PP. This analysis permits the extraction of the general rule (PP → IN1 NP2 ; ;(E NP2 IN1), and the more lexicalized (PP → before NP ; r± NP U). 3.1 Parameterizati</context>
<context position="25694" citStr="Galley et al. (2006)" startWordPosition="4312" endWordPosition="4315">it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation of the labelled and unlabelled 8http://code.google.com/p/berkeleyparser/ 95 iterations of model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. data. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al. (2006), and then trained our model for 5 EM iterations. We extracted syntactic rules using a re-implementation of the Galley et al. (2006) algorithm from both our syntactic alignments and the GIZA++ alignments. We handle null-aligned words by extracting every consistent derivation, and extracted composed rules consisting of at most 3 minimal rules. We evaluate our alignments against the gold standard in two ways. We calculated Span F-score, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table 3. By both measures, our syntactic aligner effectively trades recall for precision when compared to our baseline, slightly increasi</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18929" citStr="Gildea, 2003" startWordPosition="3179" endWordPosition="3180">use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when the syntax of the two languages diverges, and alignment quality drops precipitously (Zhang and Gildea, 2004), though attempts have been made to address this issue (Gildea, 2003). Our model is designed to degrade gracefully in the case of syntactic divergence. Because it can produce foreign words at any level of the derivation, our model can effectively back off to a variant of Model 1 in the case where an ITG derivation that both respects the target parse tree and the desired word-level alignments cannot be found. For example, consider the sentence pair fragment in Figure 3. It is not possible to produce an ITG derivation of this fragment that both respects the English tree and also aligns all foreign words to their obvious English counterparts. Our model handles thi</context>
<context position="21540" citStr="Gildea (2003)" startWordPosition="3643" endWordPosition="3644">roach will capture important syntactic generalizations, but it also risks including low-quality rules. It is an empirical question whether this approach is effective, and we investigate this issue further in Section 5.3. There are possibilities for improving our model’s treatment of syntactic divergence. One option is to allow the model to select trees which are more consistent with the alignment (Burkett et al., 2010), which our model can do since it permits efficient inference over forests. The second is to modify the generative process slightly, perhaps by including the “clone” operator of Gildea (2003). 4 Learning and Inference 4.1 Parameter Estimation The parameters of our model can be efficiently estimated in an unsupervised fashion using the Expectation-Maximization (EM) algorithm. The Estep requires the computation of expected counts under our model for each multinomial parameter. We omit the details of obtaining expected counts for each distribution, since they can be obtained using simple arithmetic from a single quantity, namely, the expected count of a particular instantiation of a synchronous rule r. This expectation is a standard quantity that can be computed in O(n6) time using t</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>Daniel Gildea. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24352" citStr="Goodman (1996)" startWordPosition="4101" endWordPosition="4102">er sentence to about 0.3 seconds per sentence, a speed-up of a factor of 5. 4.3 Decoding Given a trained model, we extract a tree-to-string alignment as follows: we compute, for each node in the English tree, the posterior probability of a particular foreign span assignment using the same dynamic program needed for EM. We then compute the set of span assignments which maximizes the sum of these posteriors, constrained such that the foreign span assignments nest in the obvious way. This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). 5 Experiments 5.1 Alignment Quality We first evaluated our alignments against gold standard annotations. Our training data consisted of the 2261 manually aligned and translated sentences of the Chinese Treebank (Bies et al., 2007) and approximately half a million unlabeled sentences of parallel Chinese-English newswire. The unlabeled data was subsampled (Li et al., 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseli</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing Inside-Out.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="13622" citStr="Goodman, 1998" startWordPosition="2288" endWordPosition="2289"> pinv, and a probability of left, middle, and right word sequences pleft, pmid, and pright.5 Because we condition on e, the monolingual grammar probability pg must form a distribution which produces e with probability 1.6 4In the text, we only describe the factorization for the BINARYMONO rule. For a parameterization of all rules, we refer the reader to Table 2. 5All parameters in our model are multinomial distributions. 6A simple case of such a distribution is one which places all of its mass on a single tree. More complex distributions can be obtained by conditioning an arbitrary PCFG on a (Goodman, 1998). We further assume that the probability of producing a foreign word sequence fl decomposes as: pleft(fl |A, eA) = pl(|fl |= m |A) H p(fj |A, eA) j=1 where m is the length of the sequence fl. The parameter pl is a left length distribution. The probabilities pmid, pright, decompose in the same way, except substituting a separate length distribution pm and pr for pl. For the TERMINAL rule, we emit ft with a similarly decomposed distribution pterm using length distribution pw. We define the probability of generating a foreign word fj as 1 |eA |pt(fj |ei) with i ∈ eA denoting an index ranging over</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John Denero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised itg models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23004" citStr="Haghighi et al., 2009" startWordPosition="3880" endWordPosition="3883">e obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we pruned any span alignment in which one side is more than 5 times larger than the other. Finally, we employ pruning based on highprecision alignments from simpler models (Cherry and Lin, 2007; Haghighi et al., 2009). We compute word-to-word alignments by finding all word pairs which have a posterior of at least 0.7 according to both forward and reverse IBM Model 1 parameters, and prune any span pairs which invalidate more than 3 of these alignments. In total, this pruning reVP[2,3] VP[2,3] VBN[2,3] NN[3,3] MD[1,2] DT[3,3] NN[3,4] VB[2,2] NP[3,4] VP[0,3] S[0,4] the[3,3] list[3,4] will[1,2] be[2,2] announced[2,3] tomorrow[3,3] 0 tomorrow 1 will 2 announce 3 list 4 123 Span P R F1 Syntactic Alignment 50.9 83.0 63.1 GIZA++ 56.1 67.3 61.2 Rule P R F1 Syntactic Alignment 39.6 40.3 39.9 GIZA++ 46.2 34.7 39.6 Ta</context>
</contexts>
<marker>Haghighi, Blitzer, Denero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John Denero, and Dan Klein. 2009. Better word alignments with supervised itg models. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>A syntax-directed translator with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of CHSLP.</booktitle>
<contexts>
<context position="1123" citStr="Huang et al., 2006" startWordPosition="150" endWordPosition="153">arse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a l</context>
<context position="3777" citStr="Huang et al., 2006" startWordPosition="553" endWordPosition="556">ropriate pruning, our model can be efficiently trained on large parallel corpora. When compared to standard word-alignmentbacked baselines, our model produces more consistent analyses of parallel sentences, leading to high-count, high-quality transfer rules. End-toend translation experiments demonstrate that these higher quality rules improve translation quality by 1.0 BLEU over a word-alignment-backed baseline. 2 Syntactic Rule Extraction Our model is intended for use in syntactic translation models which make use of syntactic parses on either the target (Galley et al., 2006) or source side (Huang et al., 2006; Liu et al., 2006). Our model’s 118 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118–126, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics S Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chief purpose is to align nodes in the syntactic parse in one language to spans in the other – an alignment we</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A syntax-directed translator with extended domain of locality. In Proceedings of CHSLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
</authors>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, </marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: an open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation.</booktitle>
<marker>Weese, Zaidan, 2009</marker>
<rawString>Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25562" citStr="Liang et al. (2006)" startWordPosition="4290" endWordPosition="4293">ur baseline alignments, we used GIZA++, trained in the standard way.9 We used the grow-diag-final alignment heuristic, as we found it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation of the labelled and unlabelled 8http://code.google.com/p/berkeleyparser/ 95 iterations of model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. data. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al. (2006), and then trained our model for 5 EM iterations. We extracted syntactic rules using a re-implementation of the Galley et al. (2006) algorithm from both our syntactic alignments and the GIZA++ alignments. We handle null-aligned words by extracting every consistent derivation, and extracted composed rules consisting of at most 3 minimal rules. We evaluate our alignments against the gold standard in two ways. We calculated Span F-score, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of phrasal tree-to-string templates.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="12463" citStr="Liu and Gildea (2009)" startWordPosition="2084" endWordPosition="2087">the general rule (PP → IN1 NP2 ; ;(E NP2 IN1), and the more lexicalized (PP → before NP ; r± NP U). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the BINARYMONO rule,4 p(r = A → B C;fl B f,,,, C fT|A,eA) which conditions on the root of the rule A and the English yield eA, as pg(A → B C |A, eA) · pinv(I |B, C)· pleft(fl |A, eA)·pmid(fm |A, eA)·pright(fr |A, eA) In words, we assume that the rule probability decomposes into a monolingual PCFG grammar probability pg, an inversion probability pinv, and a probability of left, middle, and right wor</context>
</contexts>
<marker>Liu, Gildea, 2009</marker>
<rawString>Ding Liu and Daniel Gildea. 2009. Bayesian learning of phrasal tree-to-string templates. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-to-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3796" citStr="Liu et al., 2006" startWordPosition="557" endWordPosition="560">r model can be efficiently trained on large parallel corpora. When compared to standard word-alignmentbacked baselines, our model produces more consistent analyses of parallel sentences, leading to high-count, high-quality transfer rules. End-toend translation experiments demonstrate that these higher quality rules improve translation quality by 1.0 BLEU over a word-alignment-backed baseline. 2 Syntactic Rule Extraction Our model is intended for use in syntactic translation models which make use of syntactic parses on either the target (Galley et al., 2006) or source side (Huang et al., 2006; Liu et al., 2006). Our model’s 118 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118–126, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics S Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chief purpose is to align nodes in the syntactic parse in one language to spans in the other – an alignment we will refer to as a</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-totree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-totree translation with packed forests. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="17132" citStr="Marcus et al., 1993" startWordPosition="2894" endWordPosition="2897">foreign words from the non-terminal symbol A. We modify p(fj |eA) from the previous section to allow production of fj directly from the non-terminal7 A: p(fj |eA) = p.t - p(fj |A) 1 |eA|pt(fj |ei) where pnt is a global binomial parameter which controls how often such alignments are made. This necessitates the inclusion of parameters like pt( R`, |NP) into our translation table. Generally, these parameters do not contain much information, but rather function like a traditional NULL rooted at some position in the tree. However, in some cases, the particular annotation used by the Penn Treebank (Marcus et al., 1993) (and hence most parsers) allows for some interesting parameters to be learned. For example, we found that our aligner often matched the Chinese word f, which marks the past tense (among other things), to the preterminals VBD and VBN, which denote the English simple past and perfect tense. Additionally, Chinese measure words like ^ and -, often align to the CD (numeral) preterminal. These generalizations can be quite useful – where a particular number might predict a measure word quite poorly, the generalization that measure words co-occur with the CD tag is very robust. 7For terminal symbols </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Syntactic re-alignment models for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Emprical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="2210" citStr="May and Knight, 2007" startWordPosition="306" endWordPosition="309">gnment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers David Chiang Kevin Knight Information Sciences Institute University of Southern California {chiang,knight}@isi.edu have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or</context>
<context position="7694" citStr="May and Knight (2007)" startWordPosition="1224" endWordPosition="1227">d of SCFG rules, from which T and f can be read off; because we condition on e, the derivations produce e with probability 1. This model places a distribution over T and f given by p(T, f |e) = 1: p(d |e) = 1: ri p(r |e) d d rEd where the sum is over derivations d which yield T and f. The SCFG rules r come from one of 4 types, pictured in Table 1. In general, because our model can generate English trees, it permits inference over forests. Although we will restrict ourselves to a single parse tree for our experiments, in this section, we discuss the more general case. 1One notable exception is May and Knight (2007), who produces syntactic alignments using syntactic rules derived from word-aligned data. the trade surplus has drastically fallen NP DT* NN NN VP VBZ ADVP RB VBN R19 trade iMA surplus MEX drastically A!P fall T (past) 119 Rule Type Root English Foreign Example Instantiation TERMINAL E e ft FOUR → four ; VQ UNARY A B fl B fr CD → FOUR ;f FOUR BINARYMONO A B C fl B f,,,, C fr NP → NN NN ;c NN RAJ NN e BINARYINV A B C fl C f,,,, B fr PP → IN NP ; -t NP e IN e Table 1: Types of rules present in the SCFG describing our model, along with some sample instantiations of each type. Empty word sequences</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>Jonathan May and Kevin Knight. 2007. Syntactic re-alignment models for machine translation. In Proceedings of the Conference on Emprical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving ibm word alignment model 1.</title>
<date>2004</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16122" citStr="Moore, 2004" startWordPosition="2727" endWordPosition="2728">r of words produced for a particular English word e, functioning similarly to the “fertilities” employed by IBM Models 3 and 4 (Brown et al., 1993). This allows us � p(fj |A, eA) = iEe,4 121 to model, for example, the tendency of English determiners the and a translate to nothing in the Chinese, and of English names to align to multiple Chinese words. In general, we expect an English word to usually align to one Chinese word, and so we place a weak Dirichlet prior on on the pe distribution which puts extra mass on 1-length word sequences. This is helpful for avoiding the “garbage collection” (Moore, 2004) problem for rare words. 3.2 Exploiting Non-Terminal Labels There are often foreign words that do not correspond well to any English word, which our model must also handle. We elected for a simple augmentation to our model to account for these words. When generating foreign word sequences f at a non-terminal (i.e. via the UNARY or BINARY productions), we also allow for the production of foreign words from the non-terminal symbol A. We modify p(fj |eA) from the previous section to allow production of fj directly from the non-terminal7 A: p(fj |eA) = p.t - p(fj |A) 1 |eA|pt(fj |ei) where pnt is </context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving ibm word alignment model 1. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="5880" citStr="Och and Ney, 2003" startWordPosition="897" endWordPosition="900">ee, a span of the foreign sentence which (1) contains every source word that aligns to a target word in the yield of the node and (2) contains no source words that align outside that yield. Only nodes for which a non-empty span satisfying (1) and (2) exists may form the root or leaf of a translation rule; for that reason, we will refer to these nodes as extractable nodes. Since extractable nodes are inferred based on word alignments, spurious word alignments can rule out otherwise desirable extraction points. For example, consider the alignment in Figure 1. This alignment, produced by GIZA++ (Och and Ney, 2003), contains 4 correct alignments (the filled circles), but incorrectly aligns the to the Chinese past tense marker f (the hollow circle). This mistaken alignment produces the incorrect rule (DT → the ; ), and also blocks the extraction of (VBN → fallen ; Ai T). More high-level syntactic transfer rules are also ruled out, for example, the “the insertion rule” (NP → the NN1 NN2 ; NN1 NN2) and the high-level (S → NP1 VP2 ; NP1 VP2). 3 A Syntactic Alignment Model The most common approach to avoiding these problems is to inject knowledge about syntactic constraints into a word alignment model (Cherr</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26605" citStr="Och, 2003" startWordPosition="4456" endWordPosition="4457">core, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table 3. By both measures, our syntactic aligner effectively trades recall for precision when compared to our baseline, slightly increasing overall F-score. 5.2 Translation Quality For our translation system, we used a reimplementation of the syntactic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic re</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM: An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP</booktitle>
<contexts>
<context position="26764" citStr="Stolcke, 2002" startWordPosition="4482" endWordPosition="4483">own in Table 3. By both measures, our syntactic aligner effectively trades recall for precision when compared to our baseline, slightly increasing overall F-score. 5.2 Translation Quality For our translation system, we used a reimplementation of the syntactic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic results were obtained when rules were thresholded by count; we discuss this in the next section. 5.3 Analysis As discussed in Section 3.4, our aligner is designe</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM: An extensible language modeling toolkit. In ICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="2654" citStr="Wu, 1997" startWordPosition="384" endWordPosition="385">to word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees only. The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available. Therefore, our model is empirically robust to the case where syntactic divergence between languages prevents syntactically accurate ITG derivations. We show that, with appropriate pruning, our model can be efficiently trained on large parallel corpora. When compared</context>
<context position="9388" citStr="Wu, 1997" startWordPosition="1546" endWordPosition="1547">ction, which on the English side produces a single symbol B, and on the foreign side produces the symbol B, with sequences of words fl to its left and fr to its right. Finally, there are two binary productions: BINARYMONO rewrites A with two non-terminals B and C on the English side, and the same non-terminals B and C in monotonic order on the foreign side, with sequences of words fl, fr, and fm to the left, right, and the middle. BINARYINV inverts the order in which the non-terminals B and C are written on the source side, allowing our model to capture a large subset of possible reorderings (Wu, 1997). Derivations from this model have two key properties: first, the English side of a derivation is constrained to form a valid constituency parse, as is required in a syntax system with target-side syntax; and second, for each parse node in the English projection, there is exactly one (possibly empty) contiguous span of the foreign side which was generated from that non-terminal or one of its descendants. Identifying extractable nodes from a derivation is thus trivial: any node aligned to a non-empty foreign span is extractable. In Figure 2, we show a sample sentence pair frag2For notational co</context>
<context position="22191" citStr="Wu, 1997" startWordPosition="3744" endWordPosition="3745">stimation The parameters of our model can be efficiently estimated in an unsupervised fashion using the Expectation-Maximization (EM) algorithm. The Estep requires the computation of expected counts under our model for each multinomial parameter. We omit the details of obtaining expected counts for each distribution, since they can be obtained using simple arithmetic from a single quantity, namely, the expected count of a particular instantiation of a synchronous rule r. This expectation is a standard quantity that can be computed in O(n6) time using the bitext Inside-Outside dynamic program (Wu, 1997). 4.2 Dynamic Program Pruning While our model permits O(n6) inference over a forest of English trees, inference over a full forest would be very slow, and so we fix a single n-ary English tree obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we p</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="2418" citStr="Yamada and Knight (2001)" startWordPosition="340" endWordPosition="343">esult in an exponentially large set of extractable rules to choose from. Researchers David Chiang Kevin Knight Information Sciences Institute University of Southern California {chiang,knight}@isi.edu have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees only. The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available. Therefore, our model is empiri</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntax-based alignment: supervised or unsupervised?</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="18860" citStr="Zhang and Gildea, 2004" startWordPosition="3166" endWordPosition="3169">our parameterization permits efficient inference algorithms which we will make use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when the syntax of the two languages diverges, and alignment quality drops precipitously (Zhang and Gildea, 2004), though attempts have been made to address this issue (Gildea, 2003). Our model is designed to degrade gracefully in the case of syntactic divergence. Because it can produce foreign words at any level of the derivation, our model can effectively back off to a variant of Model 1 in the case where an ITG derivation that both respects the target parse tree and the desired word-level alignments cannot be found. For example, consider the sentence pair fragment in Figure 3. It is not possible to produce an ITG derivation of this fragment that both respects the English tree and also aligns all forei</context>
</contexts>
<marker>Zhang, Gildea, 2004</marker>
<rawString>Hao Zhang and Daniel Gildea. 2004. Syntax-based alignment: supervised or unsupervised? In Proceedings of the Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18151" citStr="Zhang et al., 2006" startWordPosition="3057" endWordPosition="3060">zations can be quite useful – where a particular number might predict a measure word quite poorly, the generalization that measure words co-occur with the CD tag is very robust. 7For terminal symbols E, this production is not possible. 3.3 Membership in ITG The generative process which describes our model contains a class of grammars larger than the computationally efficient class of ITG grammars. Fortunately, the parameterization described above not only reduces the number of parameters to a manageable level, but also introduces independence assumptions which permit synchronous binarization (Zhang et al., 2006) of our grammar. Any SCFG that can be synchronously binarized is an ITG, meaning that our parameterization permits efficient inference algorithms which we will make use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>