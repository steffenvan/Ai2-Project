<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008503">
<title confidence="0.9980815">
Effective Document-Level Features for Chinese Patent Word
Segmentation
</title>
<author confidence="0.997882">
Si Li
</author>
<affiliation confidence="0.848257666666667">
Chinese Language Processing Group
Brandeis University
Waltham, MA 02453, USA
</affiliation>
<email confidence="0.998907">
lisi@brandeis.edu
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998175">
A patent is a property right for an inven-
tion granted by the government to the in-
ventor. Patents often have a high con-
centration of scientific and technical terms
that are rare in everyday language. How-
ever, some scientific and technical terms
usually appear with high frequency only
in one specific patent. In this paper, we
propose a pragmatic approach to Chinese
word segmentation on patents where we
train a sequence labeling model based on
a group of novel document-level features.
Experiments show that the accuracy of our
model reached 96.3% (F1 score) on the de-
velopment set and 95.0% on a held-out test
set.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999023047619048">
It is well known that Chinese text does not come
with natural word delimiters, and the first step
for many Chinese language processing tasks is
word segmentation, the automatic determination
of word boundaries in Chinese text. Tremendous
progress was made in this area in the last decade
or so due to the availability of large-scale human
segmented corpora coupled with better statistical
modeling techniques. On the data side, there exist
a few large-scale human annotated corpora based
on established word segmentation standards, and
these include the Chinese TreeBank (Xue et al.,
2005), the Sinica Balanced Corpus (Chen et al.,
1996), the PKU Peoples’ Daily Corpus (Duan et
al., 2003), and the LIVAC balanced corpus (T’sou
et al., 1997). Another driver for the improvemen-
t in Chinese word segmentation accuracy comes
from the evolution of statistical modeling tech-
niques. Dictionaries used to play a central role
in early heuristics-based word segmentation tech-
niques (Chen and Liu, 1996; Sproat et al., 1996).
</bodyText>
<author confidence="0.540627">
Nianwen Xue
</author>
<affiliation confidence="0.550532666666667">
Chinese Language Processing Group
Brandeis University
Waltham, MA 02453, USA
</affiliation>
<email confidence="0.941782">
xuen@brandeis.edu
</email>
<bodyText confidence="0.999862925">
Modern word segmentation systems have moved
away from dictionary-based approaches in favor
of character tagging approaches. This allows the
word segmentation problem to be modeled as a
sequence labeling problem, and lends itself to dis-
criminative sequence modeling techniques (Xue,
2003; Peng et al., 2004). With these better model-
ing techniques, state-of-the-art systems routinely
report accuracy in the high 90%, and a few recen-
t systems report accuracies of over 98% in F1 s-
core (Sun, 2011; Zeng et al., 2013b).
Chinese word segmentation is not a solved
problem however and significant challenges re-
main. Advanced word segmentation systems per-
form very well in domains such as newswire
where everyday language is used and there is a
large amount of human annotated training data.
There is often a rapid degradation in performance
when systems trained on one domain (let us call it
the source domain) are used to segment data in a
different domain (let us call it the target domain).
This problem is especially severe when the target
domain is distant from the source domain. This is
the problem we are facing when we perform word
segmentation on Chinese patent data. The word
segmentation accuracy on Chinese patents is very
poor if the word segmentation model is trained on
the Chinese TreeBank data, which consists of data
sources from a variety of genres but no patents.
To address this issue, we annotated a corpus of
142 patents which contain about 440K words ac-
cording to the Chinese TreeBank standards. We
trained a character-tagging based CRF model for
word segmentation, and based on the writing style
of patents, we propose a group of document-level
features as well as a novel character part-of-speech
feature (C_POS). Our results show these new fea-
tures are effective and we are able to achieve an
accuracy of 96.3% (F1 score) on the development
set and 95% (F1 score) on the test set.
</bodyText>
<page confidence="0.984924">
199
</page>
<bodyText confidence="0.557536">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 199–205,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.93674" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.9999756">
We adopt the character-based sequence labeling
approach, first proposed in (Xue, 2003), as our
modeling technique for its simplicity and effec-
tiveness. This approach treats each sentence as a
sequence of characters and assigns to each charac-
ter a label that indicates its position in the word. In
this paper, we use the BMES tag set to indicate the
character positions. The tag set has four labels that
represent for possible positions a character can oc-
cupy within a word: B for beginning, M for mid-
dle, E for ending, and S for a single character as a
word. After each character in a sentence is tagged
with a BMES label, a sequence of words can be
derived from this labeled character sequence.
We train a Conditional Random Field (CRF)
(Lafferty et al., 2001) model for this sequence
labeling. When extracting features to train a
CRF model from a sequence of n characters
C1C2...Ci−1CiCi+1...Cn, we extract features for
each character Ci from a fixed window. We start
with a set of core features extracted from the anno-
tated corpus that have been shown to be effective
in previous works and propose some new features
for patent word segmentation. We describe each
group of features in detail below.
</bodyText>
<subsectionHeader confidence="0.972435">
2.1 Character features (CF)
</subsectionHeader>
<bodyText confidence="0.999208166666667">
When predicting the position of a character with-
in a word, features based on its surrounding char-
acters and their types have shown to be the most
effective features for this task (Xue, 2003). There
are some variations of these features depending on
the window size in terms of the number of char-
acters to examine, and here we adopt the feature
templates used in (Ng and Low, 2004).
Character N-gram features The N-gram fea-
tures are various combinations of the surrounding
characters of the candidate character Ci. The 10
features we used are listed below:
</bodyText>
<listItem confidence="0.987874666666667">
• Character unigrams: Ck (i − 3 &lt; k &lt; i + 3)
• Character bigrams: CkCk+1 (i − 3 &lt; k &lt;
i + 2) and Ck−1Ck+1 (k = i)
</listItem>
<bodyText confidence="0.996210857142857">
Character type N-gram features We classify
the characters in Chinese text into 4 types: Chi-
nese characters or hanzi, English letters, numbers
and others. Ti is the character type of Ci. The
character type has been used in the previous work-
s in various forms (Ng and Low, 2004; Jiang et al.,
2009), and the 4 features we use are as follows:
</bodyText>
<listItem confidence="0.974535">
• Character type unigrams: Tk (k = i)
• Character type bigrams: TkTk+1 (i−2 &lt; k &lt;
i + 1) and Tk−1Tk+1 (k = i)
</listItem>
<bodyText confidence="0.999827666666667">
Starting with this baseline, we extract some new
features to improve Chinese patent word segmen-
tation accuracy.
</bodyText>
<subsectionHeader confidence="0.999314">
2.2 POS of single-character words (C_POS)
</subsectionHeader>
<bodyText confidence="0.999984382352941">
Chinese words are composed of Chinese hanzi,
and an overwhelming majority of these Chinese
characters can be single-character words them-
selves in some context. In fact, most of the multi-
character words are compounds that are 2-4 char-
acters in length. The formation of these compound
words is not random and abide by word formation
rules that are similar to the formation of phras-
es (Xue, 2000; Packard, 2000). In fact, the Chi-
nese TreeBank word segmentation guidelines (X-
ia, 2000) specify how words are segmented based
on the part-of-speech (POS) of their componen-
t characters. We hypothesize that the POS tags
of the single-character words would be useful in-
formation to help predict how they form the com-
pound words, and these POS tags are more fine-
grained information than the character type infor-
mation described in the previous section, but are
more robust and more generalizable than the char-
acters themselves.
Since we do not have POS-tagged patent da-
ta, we extract this information from the Chinese
TreeBank (CTB) 7.0, a 1.2-million-word out-of-
domain dataset. We extract the POS tags for al-
l the single-character words in the CTB. Some of
the single-character words will have more than one
POS tag. In this case, we select the POS tag with
the highest frequency as the C_POS tag for this
character. The result of this extraction process is
a list of single-character Chinese words, each of
which is assigned a single POS tag.
When extracting features for the target character
Ci, if Ci is in this list, the POS tag of Ci is used as
a feature for this target character.
</bodyText>
<subsectionHeader confidence="0.98785">
2.3 Document-level features
</subsectionHeader>
<bodyText confidence="0.999768">
A patent is a property right for an invention grant-
ed by the government to the inventor, and many of
the patents have a high concentration of scientif-
ic and technical terms. From a machine learning
perspective, these terms are hard to detect and seg-
ment because they are often &amp;quot;new words&amp;quot; that are
not seen in everyday language. These technical
</bodyText>
<page confidence="0.956367">
200
</page>
<figure confidence="0.693987">
Algorithm 1 Longest n-gram sequence extraction.
Input:
Sentences {si} in patent Pi;
Output:
Longest n-gram sequence list for Pi;
</figure>
<listItem confidence="0.99623">
1: For each sentence si in Pi do:
n-gram sequence extraction
(2≤n≤length(si));
2: Count the frequency of each n-gram sequence;
3: Delete the sequence if its frequency&lt;2;
4: Delete sequence i if it is contained in a longer
sequence j;
5: All the remaining sequences form a longest n-
gram sequence list for Pi;
6: return Longest n-gram sequences list.
</listItem>
<bodyText confidence="0.999389484848485">
terminologies also tend to be very sparse, either
because they are related to the latest invention that
has not made into everyday language, or because
our limited patent dataset cannot possibly cover all
possible technical topics. However, these techni-
cal terms are also topical and they tend to have
high relative frequency within a patent document
even though they are sparse in the entire patent da-
ta set. We attempt to exploit this distribution prop-
erty with some document-level features which are
extracted based on each patent document.
Longest n-gram features (LNG) We propose a
longest n-gram (LNG) feature as a document-level
feature. Each patent document is treated as an in-
dependent unit and the candidate longest n-gram
sequence lists for each patent are obtained as de-
scribed in Algorithm 1.
For a given patent, the LNG feature value for the
target character Ci’s LNG is set to &apos;S&apos; if the bigram
(Ci, Ci+1) are the first two characters of an n-gram
sequence in this patent’s longest n-gram sequence
list. If (Ci−1, Ci) are the last two characters of an
n-gram sequence in this patent’s longest n-gram
sequence list, the target character Ci’s LNG is set
to &apos;F&apos;. It is set to &apos;O&apos; otherwise. If Ci can be labeled
as both&apos;S&apos; and &apos;F&apos; at the same time, label&apos;T&apos; will be
given as the final label. For example, if &apos;α&apos; is the
target character Ci in patent A and the sequence
&apos;α—t&amp; &apos; is in patent A’s longest n-gram se-
quence list. If the character next to &apos;α&apos; is &apos;—&apos;, the
value of the LNG feature is set to &apos;S&apos;. If the next
character is not &apos;—&apos;, the value of the LNG feature
is set to &apos;O&apos;.
</bodyText>
<table confidence="0.763157">
Algorithm 2 Pseudo KL divergence.
Input:
Sentences {si} in patent Pi;
Output:
Pseudo KL divergence values between differ-
ent characters in Pi;
</table>
<listItem confidence="0.9786526">
1: For each sentence si in Pi do:
trigram sequences extraction;
2: Count the frequency of each trigram;
3: Delete the trigram if its frequency&lt;2;
4: For Ci in trigram CiCi+1Ci+2 do :
</listItem>
<equation confidence="0.99441975">
PKL(Ci, Ci+1) = p(Ci1)log p(Ci1)
p(Ci+12) (1)
PKL(Ci, Ci+2) = p(Ci1)log p(Ci13 (2)
p(Ci+2 )
</equation>
<bodyText confidence="0.992935548387097">
The superscripts {1,2,3} indicate the character
position in trigram sequences;
5: return PKL(Ci, Ci+1) and PKL(Ci, Ci+2)
for the first character Ci in each trigram.
Pseudo Kullback-Leibler divergence (PKL)
The second document-level feature we propose
is the Pseudo Kullback-Leibler divergence fea-
ture which is calculated following the form of
the Kullback-Leibler divergence. The relative
position information is very important for Chi-
nese word segmentation as a sequence labeling
task. Characters XY may constitute a meaningful
word, but characters Y X may not be. Therefore,
if we want to determine whether character X and
character Y can form a word, the relative position
of these two characters should be considered. We
adopt a pseudo KL divergence with the relative po-
sition information as a measure of the association
strength between two adjacent characters X and
Y . The pseudo KL divergence is an asymmetric
measure. The PKL value between character X
and character Y is described in Algorithm 2.
The PKL values are real numbers and are s-
parse. A common solution to sparsity reduction
is binning. We rank the PKL values between t-
wo adjacent characters in each patent from low to
high, and then divide all values into five bins. Each
bin is assigned a unique ID and all PKL values in
the same bin are replaced by this ID. This ID is
then used as the PKL feature value for the target
character Ci.
</bodyText>
<page confidence="0.991609">
201
</page>
<bodyText confidence="0.998452071428572">
Pointwise Mutual information (PMI) Point-
wise Mutual information has been widely used
in previous work on Chinese word segmentation
(Sun and Xu, 2011; Zhang et al., 2013b) and it is a
measure of the mutual dependence of two strings
and reflects the tendency of two strings appearing
in one word. In previous work, PMI statistics are
gathered on the entire data set, and here we gather
PMI statistics for each patent in an attempt to cap-
ture character strings with high PMI in a particu-
lar patent. The procedure for calculating PMI is
the same as that for computing pseudo KL diver-
gence, but the functions (1) and (2) are replaced
with the following functions:
</bodyText>
<equation confidence="0.99980525">
PMI(Ci, Ci+1) = log p(Ci1, Ci+123 (3)
p(Ci )p(Ci+1 )
PMI(Ci, Ci+2) =log p(Ci1, Ci+233 (4)
p(Ci )p(Ci+2 )
</equation>
<bodyText confidence="0.9995586">
For the target character Ci, we obtain the values
for PMI(Ci, Ci+1) and PMI(Ci, Ci+2). In each
patent document, we rank these values from high
to low and divided them into five bins. Then the
PMI feature values are represented by the bin IDs.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999753">
3.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.999477190476191">
We annotated 142 Chinese patents following the
CTB word segmentation guidelines (Xia, 2000).
Since the original guidelines are mainly designed
to cover non-technical everyday language, many
scientific and technical terms found in patents are
not covered in the guidelines. We had to extend
the CTB word segmentation guidelines to han-
dle these new words. Deciding on how to seg-
ment these scientific and technical terms is a big
challenge since these patents cover many differ-
ent technical fields and without proper technical
background, even a native speaker has difficulty
in segmenting them properly. For difficult scien-
tific and technical terms, we consult BaiduBaike
(&amp;quot;Baidu Encyclopedia&amp;quot;)1, which we use as a scien-
tific and technical terminology dictionary during
our annotation. There are still many words that
do not appear in BaiduBaiKe, and these include
chemical names and formulas. These chemical
names and formulas (e.g., &amp;quot;1—A—3—S&amp;i95
X/1-bromo-3-chloropropane&amp;quot;) are usually very
</bodyText>
<footnote confidence="0.928188">
1http://baike.baidu.com/
</footnote>
<tableCaption confidence="0.9051495">
Table 1: Training, development and test data on
Patent data
</tableCaption>
<table confidence="0.992265">
Data set # of words # of patent
Training 345336 113
Devel. 46196 14
Test 48351 15
</table>
<bodyText confidence="0.999150176470588">
long, and unlike everyday words, they often have
numbers and punctuation marks in them. We de-
cided not to try segmenting the internal structures
of such chemical terms and treat them as single
words, because without a technical background in
chemistry, it is very hard to segment their internal
structures consistently.
The annotated patent dataset covers many topics
and they include chemistry, mechanics, medicine,
etc. If we consider the words in our annotated
dataset but not in CTB 7.0 data as new words (or
out-of-vocabulary, OOV), the new words account
for 18.3% of the patent corpus by token and 68.1%
by type. This shows that there is a large number of
words in the patent corpus that are not in the ev-
eryday language vocabulary. Table 1 presents the
data split used in our experiments.
</bodyText>
<subsectionHeader confidence="0.999566">
3.2 Main results
</subsectionHeader>
<bodyText confidence="0.999959833333333">
We use CRF++ (Kudo, 2013) to train our sequence
labeling model. Precision, recall, F1 score and
ROOV are used to evaluate our word segmentation
methods, where ROOV for our purposes means the
recall of new words which do not appear in CTB
7.0 but in patent data.
Table 2 shows the segmentation results on the
development and test sets with different feature
templates and different training sets. The CTB
training set includes the entire CTB 7.0, which has
1.2 million words. The model with the CF fea-
ture template is considered to be the baseline sys-
tem. We conducted 4 groups of experiments based
on the different datasets: (1) patent training set +
patent development set; (2) patent training set +
patent test set; (3) CTB training set + patent de-
velopment set; (4) CTB training set + patent test
set.
The results in Table 2 show that the model-
s trained on the patent data outperform the mod-
els trained on the CTB data by a big margin on
both the development and test set, even if the CTB
training set is much bigger. That proves the im-
portance of having a training set in the same do-
</bodyText>
<page confidence="0.998517">
202
</page>
<tableCaption confidence="0.999473">
Table 2: Segmentation performance with different feature sets on different datasets.
</tableCaption>
<table confidence="0.999034846153846">
Train set Test set Features P R F1 Roov
Patent train Patent dev. CF 95.34 95.28 95.32 90.02
CF+C_POS 95.58 95.40 95.49 90.40
CF+C_POS+LNG 96.32 96.00 96.15 91.22
CF+C_POS+PKL 95.62 95.41 95.51 90.40
CF+C_POS+PMI 95.65 95.40 95.53 89.94
CF+C_POS+PMI+PKL 95.72 95.53 95.62 90.37
CF+C_POS+LNG+PMI 96.42 96.09 96.26 91.66
CF+C_POS+LNG+PMI+PKL 96.48 96.12 96.30 91.69
Patent train Patent test CF 93.98 94.49 94.23 85.19
CF+C_POS+LNG+PKL+PMI 94.89 95.10 95.00 87.89
CTB train Patent dev. CF+C_POS+LNG+PKL+PMI 89.04 90.75 89.89 72.80
CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89
</table>
<bodyText confidence="0.9945242">
main. The results also show that adding the new
features we proposed leads to consistent improve-
ment across all experimental conditions, and that
the LNG features are the most effective and bring
about the largest improvement in accuracy.
</bodyText>
<sectionHeader confidence="0.999864" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999975135135135">
Most of the previous work on Chinese word seg-
mentation focused on newswire, and one wide-
ly adopted technique is character-based represen-
tation combined with sequential learning models
(Xue, 2003; Low et al., 2005; Zhao et al., 2006;
Sun and Xu, 2011; Zeng et al., 2013b; Zhang
et al., 2013b; Wang and Kan, 2013). More re-
cently, word-based models using perceptron learn-
ing techniques (Zhang and Clark, 2007) also pro-
duce very competitive results. There are also some
recent successful attempts to combine character-
based and word-based techniques (Sun, 2010;
Zeng et al., 2013a).
As Chinese word segmentation has reached a
very high accuracy in the newswire domain, the
attention of the field has started to shift to other
domains where there are few annotated resources
and the problem is more challenging, such as work
on the word segmentation of literature data (Li-
u and Zhang, 2012) and informal language gen-
res (Wang and Kan, 2013; Zhang et al., 2013a).
Patents are distinctly different from the above gen-
res as they contain scientific and technical terms
that require some special training to understand.
There has been very little work in this area, and
the only work that is devoted to Chinese word
segmentation is (Guo et al., 2012), which reports
work on Chinese patent word segmentation with
a fairly small test set without any annotated train-
ing data in the target domain. They reported an
accuracy of 86.42% (F1 score), but the results are
incomparable with ours as their evaluation data is
not available to us. We differ from their work in
that we manually segmented a significant amount
of data, and trained a model with document-level
features designed to capture the characteristics of
patent data.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999952916666667">
In this paper, we presented an accurate character-
based word segmentation model for Chinese
patents. Our contributions are two-fold. Our first
contribution is that we have annotated a signifi-
cant amount of Chinese patent data and we plan
to release this data once the copyright issues have
been cleared. Our second contribution is that we
designed document-level features to capture the
distributional characteristics of the scientific and
technical terms in patents. Experimental results
showed that the document-level features we pro-
posed are effective for patent word segmentation.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999651333333333">
This paper is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) vi-
a contract NO. D11PC20154. All views ex-
pressed in this paper are those of the authors and
do not necessarily represent the view of IARPA,
DoI/NBC, or the U.S. Government.
</bodyText>
<page confidence="0.99851">
203
</page>
<sectionHeader confidence="0.990007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999846432692308">
Keh-Jiann Chen and Shing-Huan Liu. 1996. Word
Identification for Mandarin Chinese Sentences. In
Proceedings of COLING’92, pages 101–107.
Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and
Hui-Li Hsu. 1996. Sinica Corpus: Design Method-
ology for Balanced Corpora. In Proceedings of the
11 th Pacific Asia Conference on Language, Infor-
mation and Computation, pages 167–176.
Huiming Duan, Xiaojing Bai, Baobao Chang, and Shi-
wen Yu. 2003. Chinese word segmentation at
Peking University. In Proceedings of the second
SIGHAN workshop on Chinese language process-
ing, pages 152–155.
Zhen Guo, Yujie Zhang, Chen Su, and Jinan Xu. 2012.
Exploration of N-gram Features for the Domain
Adaptation of Chinese Word Segmentation. In Pro-
ceedings of Natural Language Processing and Chi-
nese Computing Natural Language Processing and
Chinese Computing, pages 121–131.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging - A Case
Study. In Proceedings of ACL’09, pages 522–530.
Taku Kudo. 2013. CRF++: Yet Another CRF toolkit.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML’01, pages
282–289.
Yang Liu and Yue Zhang. 2012. Unsupervised Do-
main Adaptation for Joint Segmentation and POS-
Tagging. In Proceedings of COLING’12, pages
745–754.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. In Proceedings of the 4th SIGHAN
Workshop on Chinese Language Processing, pages
970–979.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based? In Proceedings of
EMNLP’04, pages 277–284.
Jerome Packard. 2000. The Morphology of Chinese: a
cognitive and linguistic approach. Cambridge Uni-
versity Press.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese Segmentation and New Word Detec-
tion using Conditional Random Fields. In Proceed-
ings of COLING’04.
Richard Sproat, Chilin Shih, William Gale, and Nan-
cy Chang. 1996. A Stochastic Finite-State Word-
Segmentation Algorithm for Chinese. Computation-
al Linguistics, 22(3):377–404.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
Word Segmentation Using Unlabeled Data. In Pro-
ceedings of EMNLP’11, pages 970–979.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and com-
bination. In Proceedings of ACL’10, pages 1211–
1219.
Weiwei Sun. 2011. A Stacked Sub-Word Model
for Joint Chinese Word Segmentation and Part-of-
Speech Tagging. In Proceedings of ACL’11, pages
1385–1394.
Benjamin K. T’sou, Hing-Lung Lin, Godfrey Liu,
Terence Chan, Jerome Hu, Ching hai Chew, and
John K.P. Tse. 1997. A Synchronous Chinese Lan-
guage Corpus from Different Speech Communities:
Construction and Application. International Jour-
nal of Computational Linguistics and Chinese Lan-
guage Processing, 2(1):91–104.
Aobo Wang and Min-Yen Kan. 2013. Mining Infor-
mal Language from Chinese Microtext: Joint Word
Recognition and Segmentation. In Proceedings of
ACL’13, pages 731–741.
Fei Xia. 2000. The segmentation guidelines for the
Penn Chinese Treebank (3.0).
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2):207–238.
Nianwen Xue. 2000. Defining and identifying words
in Chinese. Ph.D. thesis, University of Delaware.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. International Journal of Com-
putational Linguistics and Chinese Language Pro-
cessing, 8(1):29–48.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and
Isabel Trancoso. 2013a. Co-regularizing character-
based and word-based models for semi-supervised
Chinese word segmentation. In Proceedings of A-
CL’13, pages 171–176.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and
Isabel Trancoso. 2013b. Graph-based Semi-
Supervised Model for Joint Chinese Word Segmen-
tation and Part-of-Speech Tagging. In Proceedings
ofACL’13, pages 770–779.
Yue Zhang and Stephen Clark. 2007. Chinese Seg-
mentation Using a Word-based Perceptron Algorith-
m. In Proceedings of ACL’07, pages 840–847.
Longkai Zhang, Li Li, Zhengyan He, Houfeng Wang,
and Ni Sun. 2013a. Improving Chinese Word Seg-
mentation on Micro-blog Using Rich Punctuations.
In Proceedings ofACL’13, pages 177–182.
</reference>
<page confidence="0.984454">
204
</page>
<reference confidence="0.9988565">
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013b. Exploring Representations from
Unlabeled Data with Co-training for Chinese Word
Segmentation. In Proceedings of EMNLP’13, pages
311–321.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the 5th
SIGHAN Workshop on Chinese Language Process-
ing, pages 162–165.
</reference>
<page confidence="0.99887">
205
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418679">
<title confidence="0.9996345">Effective Document-Level Features for Chinese Patent Segmentation</title>
<author confidence="0.902568">Si</author>
<affiliation confidence="0.773426">Chinese Language Processing</affiliation>
<address confidence="0.731227">Brandeis Waltham, MA 02453,</address>
<email confidence="0.999913">lisi@brandeis.edu</email>
<abstract confidence="0.997099294117647">A patent is a property right for an invention granted by the government to the inventor. Patents often have a high concentration of scientific and technical terms that are rare in everyday language. However, some scientific and technical terms usually appear with high frequency only in one specific patent. In this paper, we propose a pragmatic approach to Chinese word segmentation on patents where we train a sequence labeling model based on a group of novel document-level features. Experiments show that the accuracy of our reached 96.3% on the development set and 95.0% on a held-out test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Shing-Huan Liu</author>
</authors>
<title>Word Identification for Mandarin Chinese Sentences.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING’92,</booktitle>
<pages>101--107</pages>
<contexts>
<context position="1804" citStr="Chen and Liu, 1996" startWordPosition="282" endWordPosition="285">ter statistical modeling techniques. On the data side, there exist a few large-scale human annotated corpora based on established word segmentation standards, and these include the Chinese TreeBank (Xue et al., 2005), the Sinica Balanced Corpus (Chen et al., 1996), the PKU Peoples’ Daily Corpus (Duan et al., 2003), and the LIVAC balanced corpus (T’sou et al., 1997). Another driver for the improvement in Chinese word segmentation accuracy comes from the evolution of statistical modeling techniques. Dictionaries used to play a central role in early heuristics-based word segmentation techniques (Chen and Liu, 1996; Sproat et al., 1996). Nianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to discriminative sequence modeling techniques (Xue, 2003; Peng et al., 2004). With these better modeling techniques, state-of-the-art systems routinely report accuracy in the high 90%, and a few recent systems report accuracies of over 98%</context>
</contexts>
<marker>Chen, Liu, 1996</marker>
<rawString>Keh-Jiann Chen and Shing-Huan Liu. 1996. Word Identification for Mandarin Chinese Sentences. In Proceedings of COLING’92, pages 101–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Chu-Ren Huang</author>
<author>Li-Ping Chang</author>
<author>Hui-Li Hsu</author>
</authors>
<title>Sinica Corpus: Design Methodology for Balanced Corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 11 th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>167--176</pages>
<contexts>
<context position="1450" citStr="Chen et al., 1996" startWordPosition="226" endWordPosition="229">at Chinese text does not come with natural word delimiters, and the first step for many Chinese language processing tasks is word segmentation, the automatic determination of word boundaries in Chinese text. Tremendous progress was made in this area in the last decade or so due to the availability of large-scale human segmented corpora coupled with better statistical modeling techniques. On the data side, there exist a few large-scale human annotated corpora based on established word segmentation standards, and these include the Chinese TreeBank (Xue et al., 2005), the Sinica Balanced Corpus (Chen et al., 1996), the PKU Peoples’ Daily Corpus (Duan et al., 2003), and the LIVAC balanced corpus (T’sou et al., 1997). Another driver for the improvement in Chinese word segmentation accuracy comes from the evolution of statistical modeling techniques. Dictionaries used to play a central role in early heuristics-based word segmentation techniques (Chen and Liu, 1996; Sproat et al., 1996). Nianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging app</context>
</contexts>
<marker>Chen, Huang, Chang, Hsu, 1996</marker>
<rawString>Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and Hui-Li Hsu. 1996. Sinica Corpus: Design Methodology for Balanced Corpora. In Proceedings of the 11 th Pacific Asia Conference on Language, Information and Computation, pages 167–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huiming Duan</author>
<author>Xiaojing Bai</author>
<author>Baobao Chang</author>
<author>Shiwen Yu</author>
</authors>
<title>Chinese word segmentation at Peking University.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language processing,</booktitle>
<pages>152--155</pages>
<contexts>
<context position="1501" citStr="Duan et al., 2003" startWordPosition="235" endWordPosition="238">imiters, and the first step for many Chinese language processing tasks is word segmentation, the automatic determination of word boundaries in Chinese text. Tremendous progress was made in this area in the last decade or so due to the availability of large-scale human segmented corpora coupled with better statistical modeling techniques. On the data side, there exist a few large-scale human annotated corpora based on established word segmentation standards, and these include the Chinese TreeBank (Xue et al., 2005), the Sinica Balanced Corpus (Chen et al., 1996), the PKU Peoples’ Daily Corpus (Duan et al., 2003), and the LIVAC balanced corpus (T’sou et al., 1997). Another driver for the improvement in Chinese word segmentation accuracy comes from the evolution of statistical modeling techniques. Dictionaries used to play a central role in early heuristics-based word segmentation techniques (Chen and Liu, 1996; Sproat et al., 1996). Nianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem </context>
</contexts>
<marker>Duan, Bai, Chang, Yu, 2003</marker>
<rawString>Huiming Duan, Xiaojing Bai, Baobao Chang, and Shiwen Yu. 2003. Chinese word segmentation at Peking University. In Proceedings of the second SIGHAN workshop on Chinese language processing, pages 152–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Guo</author>
<author>Yujie Zhang</author>
<author>Chen Su</author>
<author>Jinan Xu</author>
</authors>
<title>Exploration of N-gram Features for the Domain Adaptation of Chinese Word Segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of Natural Language Processing and Chinese Computing Natural Language Processing and Chinese Computing,</booktitle>
<pages>121--131</pages>
<contexts>
<context position="18691" citStr="Guo et al., 2012" startWordPosition="3145" endWordPosition="3148">ery high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genres (Wang and Kan, 2013; Zhang et al., 2013a). Patents are distinctly different from the above genres as they contain scientific and technical terms that require some special training to understand. There has been very little work in this area, and the only work that is devoted to Chinese word segmentation is (Guo et al., 2012), which reports work on Chinese patent word segmentation with a fairly small test set without any annotated training data in the target domain. They reported an accuracy of 86.42% (F1 score), but the results are incomparable with ours as their evaluation data is not available to us. We differ from their work in that we manually segmented a significant amount of data, and trained a model with document-level features designed to capture the characteristics of patent data. 5 Conclusion In this paper, we presented an accurate characterbased word segmentation model for Chinese patents. Our contribu</context>
</contexts>
<marker>Guo, Zhang, Su, Xu, 2012</marker>
<rawString>Zhen Guo, Yujie Zhang, Chen Su, and Jinan Xu. 2012. Exploration of N-gram Features for the Domain Adaptation of Chinese Word Segmentation. In Proceedings of Natural Language Processing and Chinese Computing Natural Language Processing and Chinese Computing, pages 121–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging - A Case Study.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL’09,</booktitle>
<pages>522--530</pages>
<contexts>
<context position="6255" citStr="Jiang et al., 2009" startWordPosition="1037" endWordPosition="1040">plates used in (Ng and Low, 2004). Character N-gram features The N-gram features are various combinations of the surrounding characters of the candidate character Ci. The 10 features we used are listed below: • Character unigrams: Ck (i − 3 &lt; k &lt; i + 3) • Character bigrams: CkCk+1 (i − 3 &lt; k &lt; i + 2) and Ck−1Ck+1 (k = i) Character type N-gram features We classify the characters in Chinese text into 4 types: Chinese characters or hanzi, English letters, numbers and others. Ti is the character type of Ci. The character type has been used in the previous works in various forms (Ng and Low, 2004; Jiang et al., 2009), and the 4 features we use are as follows: • Character type unigrams: Tk (k = i) • Character type bigrams: TkTk+1 (i−2 &lt; k &lt; i + 1) and Tk−1Tk+1 (k = i) Starting with this baseline, we extract some new features to improve Chinese patent word segmentation accuracy. 2.2 POS of single-character words (C_POS) Chinese words are composed of Chinese hanzi, and an overwhelming majority of these Chinese characters can be single-character words themselves in some context. In fact, most of the multicharacter words are compounds that are 2-4 characters in length. The formation of these compound words is </context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging - A Case Study. In Proceedings of ACL’09, pages 522–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<title>CRF++: Yet Another CRF toolkit.</title>
<date>2013</date>
<contexts>
<context position="15453" citStr="Kudo, 2013" startWordPosition="2607" endWordPosition="2608"> background in chemistry, it is very hard to segment their internal structures consistently. The annotated patent dataset covers many topics and they include chemistry, mechanics, medicine, etc. If we consider the words in our annotated dataset but not in CTB 7.0 data as new words (or out-of-vocabulary, OOV), the new words account for 18.3% of the patent corpus by token and 68.1% by type. This shows that there is a large number of words in the patent corpus that are not in the everyday language vocabulary. Table 1 presents the data split used in our experiments. 3.2 Main results We use CRF++ (Kudo, 2013) to train our sequence labeling model. Precision, recall, F1 score and ROOV are used to evaluate our word segmentation methods, where ROOV for our purposes means the recall of new words which do not appear in CTB 7.0 but in patent data. Table 2 shows the segmentation results on the development and test sets with different feature templates and different training sets. The CTB training set includes the entire CTB 7.0, which has 1.2 million words. The model with the CF feature template is considered to be the baseline system. We conducted 4 groups of experiments based on the different datasets: </context>
</contexts>
<marker>Kudo, 2013</marker>
<rawString>Taku Kudo. 2013. CRF++: Yet Another CRF toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML’01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4821" citStr="Lafferty et al., 2001" startWordPosition="780" endWordPosition="783">fectiveness. This approach treats each sentence as a sequence of characters and assigns to each character a label that indicates its position in the word. In this paper, we use the BMES tag set to indicate the character positions. The tag set has four labels that represent for possible positions a character can occupy within a word: B for beginning, M for middle, E for ending, and S for a single character as a word. After each character in a sentence is tagged with a BMES label, a sequence of words can be derived from this labeled character sequence. We train a Conditional Random Field (CRF) (Lafferty et al., 2001) model for this sequence labeling. When extracting features to train a CRF model from a sequence of n characters C1C2...Ci−1CiCi+1...Cn, we extract features for each character Ci from a fixed window. We start with a set of core features extracted from the annotated corpus that have been shown to be effective in previous works and propose some new features for patent word segmentation. We describe each group of features in detail below. 2.1 Character features (CF) When predicting the position of a character within a word, features based on its surrounding characters and their types have shown t</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML’01, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yue Zhang</author>
</authors>
<title>Unsupervised Domain Adaptation for Joint Segmentation and POSTagging.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING’12,</booktitle>
<pages>745--754</pages>
<contexts>
<context position="18336" citStr="Liu and Zhang, 2012" startWordPosition="3083" endWordPosition="3087">2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genres (Wang and Kan, 2013; Zhang et al., 2013a). Patents are distinctly different from the above genres as they contain scientific and technical terms that require some special training to understand. There has been very little work in this area, and the only work that is devoted to Chinese word segmentation is (Guo et al., 2012), which reports work on Chinese patent word segmentation with a fairly small test set without any annotated training data in the target domain. They reported an accuracy of 86.42% (F1 score), but the results are incomparable with ours as their e</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Yang Liu and Yue Zhang. 2012. Unsupervised Domain Adaptation for Joint Segmentation and POSTagging. In Proceedings of COLING’12, pages 745–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A Maximum Entropy Approach to Chinese Word Segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>970--979</pages>
<contexts>
<context position="17664" citStr="Low et al., 2005" startWordPosition="2971" endWordPosition="2974">.10 95.00 87.89 CTB train Patent dev. CF+C_POS+LNG+PKL+PMI 89.04 90.75 89.89 72.80 CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such a</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A Maximum Entropy Approach to Chinese Word Segmentation. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing, pages 970–979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese Partof-Speech Tagging: One-at-a-Time or All-at-Once? Word-Based or Character-Based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04,</booktitle>
<pages>277--284</pages>
<contexts>
<context position="5669" citStr="Ng and Low, 2004" startWordPosition="926" endWordPosition="929">tures extracted from the annotated corpus that have been shown to be effective in previous works and propose some new features for patent word segmentation. We describe each group of features in detail below. 2.1 Character features (CF) When predicting the position of a character within a word, features based on its surrounding characters and their types have shown to be the most effective features for this task (Xue, 2003). There are some variations of these features depending on the window size in terms of the number of characters to examine, and here we adopt the feature templates used in (Ng and Low, 2004). Character N-gram features The N-gram features are various combinations of the surrounding characters of the candidate character Ci. The 10 features we used are listed below: • Character unigrams: Ck (i − 3 &lt; k &lt; i + 3) • Character bigrams: CkCk+1 (i − 3 &lt; k &lt; i + 2) and Ck−1Ck+1 (k = i) Character type N-gram features We classify the characters in Chinese text into 4 types: Chinese characters or hanzi, English letters, numbers and others. Ti is the character type of Ci. The character type has been used in the previous works in various forms (Ng and Low, 2004; Jiang et al., 2009), and the 4 fe</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Partof-Speech Tagging: One-at-a-Time or All-at-Once? Word-Based or Character-Based? In Proceedings of EMNLP’04, pages 277–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Packard</author>
</authors>
<title>The Morphology of Chinese: a cognitive and linguistic approach.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6971" citStr="Packard, 2000" startWordPosition="1165" endWordPosition="1166">rams: TkTk+1 (i−2 &lt; k &lt; i + 1) and Tk−1Tk+1 (k = i) Starting with this baseline, we extract some new features to improve Chinese patent word segmentation accuracy. 2.2 POS of single-character words (C_POS) Chinese words are composed of Chinese hanzi, and an overwhelming majority of these Chinese characters can be single-character words themselves in some context. In fact, most of the multicharacter words are compounds that are 2-4 characters in length. The formation of these compound words is not random and abide by word formation rules that are similar to the formation of phrases (Xue, 2000; Packard, 2000). In fact, the Chinese TreeBank word segmentation guidelines (Xia, 2000) specify how words are segmented based on the part-of-speech (POS) of their component characters. We hypothesize that the POS tags of the single-character words would be useful information to help predict how they form the compound words, and these POS tags are more finegrained information than the character type information described in the previous section, but are more robust and more generalizable than the characters themselves. Since we do not have POS-tagged patent data, we extract this information from the Chinese T</context>
</contexts>
<marker>Packard, 2000</marker>
<rawString>Jerome Packard. 2000. The Morphology of Chinese: a cognitive and linguistic approach. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese Segmentation and New Word Detection using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING’04.</booktitle>
<contexts>
<context position="2241" citStr="Peng et al., 2004" startWordPosition="344" endWordPosition="347">racy comes from the evolution of statistical modeling techniques. Dictionaries used to play a central role in early heuristics-based word segmentation techniques (Chen and Liu, 1996; Sproat et al., 1996). Nianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to discriminative sequence modeling techniques (Xue, 2003; Peng et al., 2004). With these better modeling techniques, state-of-the-art systems routinely report accuracy in the high 90%, and a few recent systems report accuracies of over 98% in F1 score (Sun, 2011; Zeng et al., 2013b). Chinese word segmentation is not a solved problem however and significant challenges remain. Advanced word segmentation systems perform very well in domains such as newswire where everyday language is used and there is a large amount of human annotated training data. There is often a rapid degradation in performance when systems trained on one domain (let us call it the source domain) are</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese Segmentation and New Word Detection using Conditional Random Fields. In Proceedings of COLING’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A Stochastic Finite-State WordSegmentation Algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="1826" citStr="Sproat et al., 1996" startWordPosition="286" endWordPosition="289">ling techniques. On the data side, there exist a few large-scale human annotated corpora based on established word segmentation standards, and these include the Chinese TreeBank (Xue et al., 2005), the Sinica Balanced Corpus (Chen et al., 1996), the PKU Peoples’ Daily Corpus (Duan et al., 2003), and the LIVAC balanced corpus (T’sou et al., 1997). Another driver for the improvement in Chinese word segmentation accuracy comes from the evolution of statistical modeling techniques. Dictionaries used to play a central role in early heuristics-based word segmentation techniques (Chen and Liu, 1996; Sproat et al., 1996). Nianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to discriminative sequence modeling techniques (Xue, 2003; Peng et al., 2004). With these better modeling techniques, state-of-the-art systems routinely report accuracy in the high 90%, and a few recent systems report accuracies of over 98% in F1 score (Sun, 201</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A Stochastic Finite-State WordSegmentation Algorithm for Chinese. Computational Linguistics, 22(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese Word Segmentation Using Unlabeled Data.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP’11,</booktitle>
<pages>970--979</pages>
<contexts>
<context position="12563" citStr="Sun and Xu, 2011" startWordPosition="2126" endWordPosition="2129">between character X and character Y is described in Algorithm 2. The PKL values are real numbers and are sparse. A common solution to sparsity reduction is binning. We rank the PKL values between two adjacent characters in each patent from low to high, and then divide all values into five bins. Each bin is assigned a unique ID and all PKL values in the same bin are replaced by this ID. This ID is then used as the PKL feature value for the target character Ci. 201 Pointwise Mutual information (PMI) Pointwise Mutual information has been widely used in previous work on Chinese word segmentation (Sun and Xu, 2011; Zhang et al., 2013b) and it is a measure of the mutual dependence of two strings and reflects the tendency of two strings appearing in one word. In previous work, PMI statistics are gathered on the entire data set, and here we gather PMI statistics for each patent in an attempt to capture character strings with high PMI in a particular patent. The procedure for calculating PMI is the same as that for computing pseudo KL divergence, but the functions (1) and (2) are replaced with the following functions: PMI(Ci, Ci+1) = log p(Ci1, Ci+123 (3) p(Ci )p(Ci+1 ) PMI(Ci, Ci+2) =log p(Ci1, Ci+233 (4)</context>
<context position="17701" citStr="Sun and Xu, 2011" startWordPosition="2979" endWordPosition="2982"> CF+C_POS+LNG+PKL+PMI 89.04 90.75 89.89 72.80 CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of li</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese Word Segmentation Using Unlabeled Data. In Proceedings of EMNLP’11, pages 970–979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and character-based word segmentation models: Comparison and combination.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL’10,</booktitle>
<pages>1211--1219</pages>
<contexts>
<context position="18007" citStr="Sun, 2010" startWordPosition="3029" endWordPosition="3030">t the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genres (Wang and Kan, 2013; Zhang et al., 2013a). Patents are distinctly different from the above genres as they contain scientific and technical terms that require some special training to understand. There has been very little work in this area, </context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Word-based and character-based word segmentation models: Comparison and combination. In Proceedings of ACL’10, pages 1211– 1219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-ofSpeech Tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL’11,</booktitle>
<pages>1385--1394</pages>
<contexts>
<context position="2427" citStr="Sun, 2011" startWordPosition="378" endWordPosition="379">., 1996). Nianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to discriminative sequence modeling techniques (Xue, 2003; Peng et al., 2004). With these better modeling techniques, state-of-the-art systems routinely report accuracy in the high 90%, and a few recent systems report accuracies of over 98% in F1 score (Sun, 2011; Zeng et al., 2013b). Chinese word segmentation is not a solved problem however and significant challenges remain. Advanced word segmentation systems perform very well in domains such as newswire where everyday language is used and there is a large amount of human annotated training data. There is often a rapid degradation in performance when systems trained on one domain (let us call it the source domain) are used to segment data in a different domain (let us call it the target domain). This problem is especially severe when the target domain is distant from the source domain. This is the pr</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-ofSpeech Tagging. In Proceedings of ACL’11, pages 1385–1394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin K T’sou</author>
<author>Hing-Lung Lin</author>
<author>Godfrey Liu</author>
<author>Terence Chan</author>
<author>Jerome Hu</author>
<author>Ching hai Chew</author>
<author>John K P Tse</author>
</authors>
<title>A Synchronous Chinese Language Corpus from Different Speech Communities: Construction and Application.</title>
<date>1997</date>
<journal>International Journal of Computational Linguistics and Chinese Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<marker>T’sou, Lin, Liu, Chan, Hu, Chew, Tse, 1997</marker>
<rawString>Benjamin K. T’sou, Hing-Lung Lin, Godfrey Liu, Terence Chan, Jerome Hu, Ching hai Chew, and John K.P. Tse. 1997. A Synchronous Chinese Language Corpus from Different Speech Communities: Construction and Application. International Journal of Computational Linguistics and Chinese Language Processing, 2(1):91–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL’13,</booktitle>
<pages>731--741</pages>
<contexts>
<context position="17763" citStr="Wang and Kan, 2013" startWordPosition="2991" endWordPosition="2994">ent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genr</context>
</contexts>
<marker>Wang, Kan, 2013</marker>
<rawString>Aobo Wang and Min-Yen Kan. 2013. Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation. In Proceedings of ACL’13, pages 731–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>The segmentation guidelines for the Penn Chinese Treebank (3.0).</title>
<date>2000</date>
<contexts>
<context position="7043" citStr="Xia, 2000" startWordPosition="1176" endWordPosition="1178">ine, we extract some new features to improve Chinese patent word segmentation accuracy. 2.2 POS of single-character words (C_POS) Chinese words are composed of Chinese hanzi, and an overwhelming majority of these Chinese characters can be single-character words themselves in some context. In fact, most of the multicharacter words are compounds that are 2-4 characters in length. The formation of these compound words is not random and abide by word formation rules that are similar to the formation of phrases (Xue, 2000; Packard, 2000). In fact, the Chinese TreeBank word segmentation guidelines (Xia, 2000) specify how words are segmented based on the part-of-speech (POS) of their component characters. We hypothesize that the POS tags of the single-character words would be useful information to help predict how they form the compound words, and these POS tags are more finegrained information than the character type information described in the previous section, but are more robust and more generalizable than the characters themselves. Since we do not have POS-tagged patent data, we extract this information from the Chinese TreeBank (CTB) 7.0, a 1.2-million-word out-ofdomain dataset. We extract t</context>
<context position="13548" citStr="Xia, 2000" startWordPosition="2301" endWordPosition="2302">ng PMI is the same as that for computing pseudo KL divergence, but the functions (1) and (2) are replaced with the following functions: PMI(Ci, Ci+1) = log p(Ci1, Ci+123 (3) p(Ci )p(Ci+1 ) PMI(Ci, Ci+2) =log p(Ci1, Ci+233 (4) p(Ci )p(Ci+2 ) For the target character Ci, we obtain the values for PMI(Ci, Ci+1) and PMI(Ci, Ci+2). In each patent document, we rank these values from high to low and divided them into five bins. Then the PMI feature values are represented by the bin IDs. 3 Experiments 3.1 Data preparation We annotated 142 Chinese patents following the CTB word segmentation guidelines (Xia, 2000). Since the original guidelines are mainly designed to cover non-technical everyday language, many scientific and technical terms found in patents are not covered in the guidelines. We had to extend the CTB word segmentation guidelines to handle these new words. Deciding on how to segment these scientific and technical terms is a big challenge since these patents cover many different technical fields and without proper technical background, even a native speaker has difficulty in segmenting them properly. For difficult scientific and technical terms, we consult BaiduBaike (&amp;quot;Baidu Encyclopedia&amp;quot;</context>
</contexts>
<marker>Xia, 2000</marker>
<rawString>Fei Xia. 2000. The segmentation guidelines for the Penn Chinese Treebank (3.0).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="1402" citStr="Xue et al., 2005" startWordPosition="218" endWordPosition="221">ut test set. 1 Introduction It is well known that Chinese text does not come with natural word delimiters, and the first step for many Chinese language processing tasks is word segmentation, the automatic determination of word boundaries in Chinese text. Tremendous progress was made in this area in the last decade or so due to the availability of large-scale human segmented corpora coupled with better statistical modeling techniques. On the data side, there exist a few large-scale human annotated corpora based on established word segmentation standards, and these include the Chinese TreeBank (Xue et al., 2005), the Sinica Balanced Corpus (Chen et al., 1996), the PKU Peoples’ Daily Corpus (Duan et al., 2003), and the LIVAC balanced corpus (T’sou et al., 1997). Another driver for the improvement in Chinese word segmentation accuracy comes from the evolution of statistical modeling techniques. Dictionaries used to play a central role in early heuristics-based word segmentation techniques (Chen and Liu, 1996; Sproat et al., 1996). Nianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-ba</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Defining and identifying words in Chinese.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Delaware.</institution>
<contexts>
<context position="6955" citStr="Xue, 2000" startWordPosition="1163" endWordPosition="1164">er type bigrams: TkTk+1 (i−2 &lt; k &lt; i + 1) and Tk−1Tk+1 (k = i) Starting with this baseline, we extract some new features to improve Chinese patent word segmentation accuracy. 2.2 POS of single-character words (C_POS) Chinese words are composed of Chinese hanzi, and an overwhelming majority of these Chinese characters can be single-character words themselves in some context. In fact, most of the multicharacter words are compounds that are 2-4 characters in length. The formation of these compound words is not random and abide by word formation rules that are similar to the formation of phrases (Xue, 2000; Packard, 2000). In fact, the Chinese TreeBank word segmentation guidelines (Xia, 2000) specify how words are segmented based on the part-of-speech (POS) of their component characters. We hypothesize that the POS tags of the single-character words would be useful information to help predict how they form the compound words, and these POS tags are more finegrained information than the character type information described in the previous section, but are more robust and more generalizable than the characters themselves. Since we do not have POS-tagged patent data, we extract this information fr</context>
</contexts>
<marker>Xue, 2000</marker>
<rawString>Nianwen Xue. 2000. Defining and identifying words in Chinese. Ph.D. thesis, University of Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<journal>International Journal of Computational Linguistics and Chinese Language Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2221" citStr="Xue, 2003" startWordPosition="342" endWordPosition="343">tation accuracy comes from the evolution of statistical modeling techniques. Dictionaries used to play a central role in early heuristics-based word segmentation techniques (Chen and Liu, 1996; Sproat et al., 1996). Nianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to discriminative sequence modeling techniques (Xue, 2003; Peng et al., 2004). With these better modeling techniques, state-of-the-art systems routinely report accuracy in the high 90%, and a few recent systems report accuracies of over 98% in F1 score (Sun, 2011; Zeng et al., 2013b). Chinese word segmentation is not a solved problem however and significant challenges remain. Advanced word segmentation systems perform very well in domains such as newswire where everyday language is used and there is a large amount of human annotated training data. There is often a rapid degradation in performance when systems trained on one domain (let us call it th</context>
<context position="4146" citStr="Xue, 2003" startWordPosition="660" endWordPosition="661">riting style of patents, we propose a group of document-level features as well as a novel character part-of-speech feature (C_POS). Our results show these new features are effective and we are able to achieve an accuracy of 96.3% (F1 score) on the development set and 95% (F1 score) on the test set. 199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 199–205, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2 Method We adopt the character-based sequence labeling approach, first proposed in (Xue, 2003), as our modeling technique for its simplicity and effectiveness. This approach treats each sentence as a sequence of characters and assigns to each character a label that indicates its position in the word. In this paper, we use the BMES tag set to indicate the character positions. The tag set has four labels that represent for possible positions a character can occupy within a word: B for beginning, M for middle, E for ending, and S for a single character as a word. After each character in a sentence is tagged with a BMES label, a sequence of words can be derived from this labeled character </context>
<context position="5479" citStr="Xue, 2003" startWordPosition="893" endWordPosition="894">ng features to train a CRF model from a sequence of n characters C1C2...Ci−1CiCi+1...Cn, we extract features for each character Ci from a fixed window. We start with a set of core features extracted from the annotated corpus that have been shown to be effective in previous works and propose some new features for patent word segmentation. We describe each group of features in detail below. 2.1 Character features (CF) When predicting the position of a character within a word, features based on its surrounding characters and their types have shown to be the most effective features for this task (Xue, 2003). There are some variations of these features depending on the window size in terms of the number of characters to examine, and here we adopt the feature templates used in (Ng and Low, 2004). Character N-gram features The N-gram features are various combinations of the surrounding characters of the candidate character Ci. The 10 features we used are listed below: • Character unigrams: Ck (i − 3 &lt; k &lt; i + 3) • Character bigrams: CkCk+1 (i − 3 &lt; k &lt; i + 2) and Ck−1Ck+1 (k = i) Character type N-gram features We classify the characters in Chinese text into 4 types: Chinese characters or hanzi, Eng</context>
<context position="17646" citStr="Xue, 2003" startWordPosition="2969" endWordPosition="2970">MI 94.89 95.10 95.00 87.89 CTB train Patent dev. CF+C_POS+LNG+PKL+PMI 89.04 90.75 89.89 72.80 CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more c</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese Word Segmentation as Character Tagging. International Journal of Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
</authors>
<title>Co-regularizing characterbased and word-based models for semi-supervised Chinese word segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL’13,</booktitle>
<pages>171--176</pages>
<contexts>
<context position="2446" citStr="Zeng et al., 2013" startWordPosition="380" endWordPosition="383">ianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to discriminative sequence modeling techniques (Xue, 2003; Peng et al., 2004). With these better modeling techniques, state-of-the-art systems routinely report accuracy in the high 90%, and a few recent systems report accuracies of over 98% in F1 score (Sun, 2011; Zeng et al., 2013b). Chinese word segmentation is not a solved problem however and significant challenges remain. Advanced word segmentation systems perform very well in domains such as newswire where everyday language is used and there is a large amount of human annotated training data. There is often a rapid degradation in performance when systems trained on one domain (let us call it the source domain) are used to segment data in a different domain (let us call it the target domain). This problem is especially severe when the target domain is distant from the source domain. This is the problem we are facing</context>
<context position="17720" citStr="Zeng et al., 2013" startWordPosition="2983" endWordPosition="2986">PMI 89.04 90.75 89.89 72.80 CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu </context>
</contexts>
<marker>Zeng, Wong, Chao, Trancoso, 2013</marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Isabel Trancoso. 2013a. Co-regularizing characterbased and word-based models for semi-supervised Chinese word segmentation. In Proceedings of ACL’13, pages 171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
</authors>
<title>Graph-based SemiSupervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL’13,</booktitle>
<pages>770--779</pages>
<contexts>
<context position="2446" citStr="Zeng et al., 2013" startWordPosition="380" endWordPosition="383">ianwen Xue Chinese Language Processing Group Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to discriminative sequence modeling techniques (Xue, 2003; Peng et al., 2004). With these better modeling techniques, state-of-the-art systems routinely report accuracy in the high 90%, and a few recent systems report accuracies of over 98% in F1 score (Sun, 2011; Zeng et al., 2013b). Chinese word segmentation is not a solved problem however and significant challenges remain. Advanced word segmentation systems perform very well in domains such as newswire where everyday language is used and there is a large amount of human annotated training data. There is often a rapid degradation in performance when systems trained on one domain (let us call it the source domain) are used to segment data in a different domain (let us call it the target domain). This problem is especially severe when the target domain is distant from the source domain. This is the problem we are facing</context>
<context position="17720" citStr="Zeng et al., 2013" startWordPosition="2983" endWordPosition="2986">PMI 89.04 90.75 89.89 72.80 CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu </context>
</contexts>
<marker>Zeng, Wong, Chao, Trancoso, 2013</marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Isabel Trancoso. 2013b. Graph-based SemiSupervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. In Proceedings ofACL’13, pages 770–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese Segmentation Using a Word-based Perceptron Algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07,</booktitle>
<pages>840--847</pages>
<contexts>
<context position="17858" citStr="Zhang and Clark, 2007" startWordPosition="3005" endWordPosition="3008">g the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and informal language genres (Wang and Kan, 2013; Zhang et al., 2013a). Patents are distinctly different from the above g</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese Segmentation Using a Word-based Perceptron Algorithm. In Proceedings of ACL’07, pages 840–847.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longkai Zhang</author>
<author>Li Li</author>
<author>Zhengyan He</author>
<author>Houfeng Wang</author>
<author>Ni Sun</author>
</authors>
<title>Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL’13,</booktitle>
<pages>177--182</pages>
<contexts>
<context position="12583" citStr="Zhang et al., 2013" startWordPosition="2130" endWordPosition="2133">X and character Y is described in Algorithm 2. The PKL values are real numbers and are sparse. A common solution to sparsity reduction is binning. We rank the PKL values between two adjacent characters in each patent from low to high, and then divide all values into five bins. Each bin is assigned a unique ID and all PKL values in the same bin are replaced by this ID. This ID is then used as the PKL feature value for the target character Ci. 201 Pointwise Mutual information (PMI) Pointwise Mutual information has been widely used in previous work on Chinese word segmentation (Sun and Xu, 2011; Zhang et al., 2013b) and it is a measure of the mutual dependence of two strings and reflects the tendency of two strings appearing in one word. In previous work, PMI statistics are gathered on the entire data set, and here we gather PMI statistics for each patent in an attempt to capture character strings with high PMI in a particular patent. The procedure for calculating PMI is the same as that for computing pseudo KL divergence, but the functions (1) and (2) are replaced with the following functions: PMI(Ci, Ci+1) = log p(Ci1, Ci+123 (3) p(Ci )p(Ci+1 ) PMI(Ci, Ci+2) =log p(Ci1, Ci+233 (4) p(Ci )p(Ci+2 ) For </context>
<context position="17741" citStr="Zhang et al., 2013" startWordPosition="2987" endWordPosition="2990">9 72.80 CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and </context>
</contexts>
<marker>Zhang, Li, He, Wang, Sun, 2013</marker>
<rawString>Longkai Zhang, Li Li, Zhengyan He, Houfeng Wang, and Ni Sun. 2013a. Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations. In Proceedings ofACL’13, pages 177–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longkai Zhang</author>
<author>Houfeng Wang</author>
<author>Xu Sun</author>
<author>Mairgup Mansur</author>
</authors>
<title>Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP’13,</booktitle>
<pages>311--321</pages>
<contexts>
<context position="12583" citStr="Zhang et al., 2013" startWordPosition="2130" endWordPosition="2133">X and character Y is described in Algorithm 2. The PKL values are real numbers and are sparse. A common solution to sparsity reduction is binning. We rank the PKL values between two adjacent characters in each patent from low to high, and then divide all values into five bins. Each bin is assigned a unique ID and all PKL values in the same bin are replaced by this ID. This ID is then used as the PKL feature value for the target character Ci. 201 Pointwise Mutual information (PMI) Pointwise Mutual information has been widely used in previous work on Chinese word segmentation (Sun and Xu, 2011; Zhang et al., 2013b) and it is a measure of the mutual dependence of two strings and reflects the tendency of two strings appearing in one word. In previous work, PMI statistics are gathered on the entire data set, and here we gather PMI statistics for each patent in an attempt to capture character strings with high PMI in a particular patent. The procedure for calculating PMI is the same as that for computing pseudo KL divergence, but the functions (1) and (2) are replaced with the following functions: PMI(Ci, Ci+1) = log p(Ci1, Ci+123 (3) p(Ci )p(Ci+1 ) PMI(Ci, Ci+2) =log p(Ci1, Ci+233 (4) p(Ci )p(Ci+2 ) For </context>
<context position="17741" citStr="Zhang et al., 2013" startWordPosition="2987" endWordPosition="2990">9 72.80 CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Liu and Zhang, 2012) and </context>
</contexts>
<marker>Zhang, Wang, Sun, Mansur, 2013</marker>
<rawString>Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup Mansur. 2013b. Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation. In Proceedings of EMNLP’13, pages 311–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved Chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>162--165</pages>
<contexts>
<context position="17683" citStr="Zhao et al., 2006" startWordPosition="2975" endWordPosition="2978">B train Patent dev. CF+C_POS+LNG+PKL+PMI 89.04 90.75 89.89 72.80 CTB train Patent test CF+C_POS+LNG+PKL+PMI 87.88 89.03 88.45 70.89 main. The results also show that adding the new features we proposed leads to consistent improvement across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. 4 Related work Most of the previous work on Chinese word segmentation focused on newswire, and one widely adopted technique is character-based representation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More recently, word-based models using perceptron learning techniques (Zhang and Clark, 2007) also produce very competitive results. There are also some recent successful attempts to combine characterbased and word-based techniques (Sun, 2010; Zeng et al., 2013a). As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word </context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An improved Chinese word segmentation system with conditional random field. In Proceedings of the 5th SIGHAN Workshop on Chinese Language Processing, pages 162–165.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>