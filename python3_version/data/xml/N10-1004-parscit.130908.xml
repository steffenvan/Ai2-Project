<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000662">
<title confidence="0.997398">
Automatic Domain Adaptation for Parsing
</title>
<author confidence="0.999659">
David McCloskya,b Eugene Charniakb Mark Johnsonc,b
</author>
<affiliation confidence="0.999395">
aStanford University bBrown University cMacquarie University
</affiliation>
<address confidence="0.873324">
Stanford, CA, USA Providence, RI, USA Sydney, NSW, Australia
</address>
<email confidence="0.999833">
mcclosky@stanford.edu ec@cs.brown.edu mjohnson@science.mq.edu.au
</email>
<sectionHeader confidence="0.993924" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999627208333333">
Current statistical parsers tend to perform well
only on their training domain and nearby gen-
res. While strong performance on a few re-
lated domains is sufficient for many situations,
it is advantageous for parsers to be able to gen-
eralize to a wide variety of domains. When
parsing document collections involving het-
erogeneous domains (e.g. the web), the op-
timal parsing model for each document is typ-
ically not obvious. We study this problem as
a new task — multiple source parser adapta-
tion. Our system trains on corpora from many
different domains. It learns not only statistics
of those domains but quantitative measures of
domain differences and how those differences
affect parsing accuracy. Given a specific tar-
get text, the resulting system proposes linear
combinations ofparsing models trained on the
source corpora. Tested across six domains,
our system outperforms all non-oracle base-
lines including the best domain-independent
parsing model. Thus, we are able to demon-
strate the value of customizing parsing models
to specific domains.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9464452">
In statistical parsing literature, it is common to see
parsers trained and tested on the same textual do-
main (Charniak and Johnson, 2005; McClosky et
al., 2006a; Petrov and Klein, 2007; Carreras et al.,
2008; Suzuki et al., 2009, among others). Unfor-
tunately, the performance of these systems degrades
on sentences drawn from a different domain. This
issue can be seen across different parsing models
(Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006;
McClosky et al., 2006b). Given that some aspects of
</bodyText>
<page confidence="0.980406">
28
</page>
<bodyText confidence="0.999934444444445">
syntax are domain dependent (typically at the lexi-
cal level), single parsing models tend to not perform
well across all domains (see Table 1). Thus, statis-
tical parsers inevitably learn some domain-specific
properties in addition to the more general properties
of a language’s syntax. Recently, Daum´e III (2007)
and Finkel and Manning (2009) showed techniques
for training models that attempt to separate domain-
specific and general properties. However, even when
given models for multiple training domains, it is not
straightforward to determine which model performs
best on an arbitrary piece of novel text.
This problem comes to the fore when one wants
to parse document collections where each document
is potentially its own domain. This shows up par-
ticularly when parsing the web. Recently, there
has been much interest in applying parsers to the
web for the purposes of information extraction and
other forms of analysis (c.f. the CLSP 2009 summer
workshop “Parsing the Web: Large-Scale Syntactic
Processing”). The scale of the web demands an au-
tomatic solution to the domain detection and adap-
tation problems. Furthermore, it is not obvious that
human annotators can determine the optimal parsing
models for each web page.
Our goal is to study this exact problem. We create
a new parsing task, multiple source parser adapta-
tion, designed to capture cross-domain performance
along with evaluation metrics and baselines. Our
new task involves training parsing models on labeled
and unlabeled corpora from a variety of domains
(source domains). This is in contrast to standard do-
main adaptation tasks where there is a single source
domain. For evaluation, one is given a text (target
text) but not the identity of its domain. The chal-
lenge is determining how to best use the available
</bodyText>
<note confidence="0.721947">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 28–36,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.924086142857143">
Test
Train BNC GENIA BROWN SWBD ETT WSJ Average
GENIA 66.3 83.6 64.6 51.6 69.0 66.6 67.0
BROWN 81.0 71.5 86.3 79.0 80.9 80.6 79.9
SWBD 70.8 62.9 75.5 89.0 75.9 69.1 73.9
ETT 72.7 65.3 75.4 75.2 81.9 73.2 73.9
WSJ 82.5 74.9 83.8 78.5 83.4 89.0 82.0
</table>
<tableCaption confidence="0.9719965">
Table 1: Cross-domain f-score performance of the Charniak (2000) parser. Averages are macro-averages.
Performance drops as training and test domains diverge. On average, the WSJ model is the most accurate.
</tableCaption>
<bodyText confidence="0.97736236">
resources from training to maximize accuracy across
multiple target texts.
Broadly put, we model how domain differences
influence parsing accuracy. This is done by taking
several computational measures of domain differ-
ences between the target text and each source do-
main. We use these features in a simple linear re-
gression model which is trained to predict the accu-
racy of a parsing model (or, more generally, a mix-
ture of parsing models) on a target text. To parse
the target text, one simply uses the mixture of pars-
ing models with the highest predicted accuracy. We
show that our method is able to predict these accu-
racies quite well and thus effectively rank parsing
models formed from mixtures of labeled and auto-
matically labeled corpora.
In Section 2, we detail recent work on similar
tasks. Our regression-based approach is covered in
Section 3. We describe an evaluation strategy in Sec-
tion 4. Section 5 presents new baselines which are
intended to give a sense of current approaches and
their limitations. The results of our experiments are
detailed in Section 6 where we show that our system
outperforms all non-oracle baselines. We conclude
with a discussion and future work (Section 7).
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9998874">
The closest work to ours is Plank and Sima’an
(2008), where unlabeled text is used to group sen-
tences from WSJ into subdomains. The authors cre-
ate a model for each subdomain which weights trees
from its subdomain more highly than others. Given
the domain specific models, they consider different
parse combination strategies. Unfortunately, these
methods do not yield a statistically significant im-
provement.
Multiple source domain adaptation has been done
for other tasks (e.g. classification in (Blitzer et
al., 2007; Daum´e III, 2007; Dredze and Cram-
mer, 2008)) and is related to multitask learning.
Daum´e III (2007) shows that an extremely sim-
ple method delivers solid performance on a num-
ber of domain adaptation classification tasks. This is
achieved by making a copy of each feature for each
source domain plus the “general” pseudodomain
(for capturing domain independent features). This
allows the classifier to directly model which features
are domain-specific. Finkel and Manning (2009)
demonstrate the hierarchical Bayesian extension of
this where domain-specific models draw from a gen-
eral base distribution. This is applied to classifica-
tion (named entity recognition) as well as depen-
dency parsing. These works describe how to train
models in many different domains but sidestep the
problem of domain detection. Thus, our work is or-
thogonal to theirs.
Our domain detection strategy draws on work in
parser accuracy prediction (Ravi et al., 2008; Kawa-
hara and Uchimoto, 2008). These works aim to pre-
dict the parser performance on a given target sen-
tence. Ravi et al. (2008) frame this as a regression
problem. Kawahara and Uchimoto (2008) treat it
as a binary classification task and predict whether
a specific parse is at a certain level of accuracy or
higher. Ravi et al. (2008) show that their system
can be used to return a ranking over different parsing
models which we extend to the multiple domain set-
ting. They also demonstrate that training their model
on WSJ allows them to accurately predict parsing
accuracy on the BROWN corpus. In contrast, our
models are trained over multiple domains to model
which factors influence cross-domain performance.
</bodyText>
<page confidence="0.999205">
29
</page>
<sectionHeader confidence="0.99741" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999878">
We start with the assumption that all target domains
are mixtures of our source domains.1 Intuitively,
these mixtures should give higher probability mass
to more similar source domains. This raises the
question of how to measure the similarity between
domains. Our method uses multiple complemen-
tary similarity measures between the target and each
source. We feed these similarity measures into a re-
gression model which learns how domain dissimi-
larities hurt parse accuracy. Thus, to parse a target
domain, we need only find the input that maximizes
the regression function — that is, the highest scoring
mixture of source domains. Our system is similar to
Ravi et al. (2008) in that both use regression to pre-
dict f-scores and some of the features are related.
</bodyText>
<subsectionHeader confidence="0.944639">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.987576385964913">
Our features are designed to help the regression
model determine if a particular source domain mix-
ture is well suited for a target domain as well as the
quality of a source domain mixture. While we ex-
plored a large number of features, we present here
only the three that were chosen by our feature selec-
tion method (Section 6.2).
Two of our features, COSINETOP50 and UN-
KWORDS, are designed to approximate how simi-
lar the target domain is to a specific source domain.
Only the surface form of the target text and auto-
matic analyses are available (e.g. we can tag or parse
the target text, but cannot use gold tags or trees).
Relative word frequencies are an important in-
dicator of domain. Cosine similarity uses a spa-
tial representation to summarize the word frequen-
cies in a corpus as a single vector. A common
method is to represent each corpus as a vector of
frequencies of the k most frequent words (Sch¨utze,
1995). This method assigns high similarity to do-
mains with a large amount of overlap in the high-
frequency vocabulary items. We experimented with
several orders of magnitude for k (our feature selec-
tion method later chose k = 50 — see Section 6.2).
Our second feature for comparing domains, UN-
1This may seem like a major limitation, but as we will show
later, our method works quite well at incorporating self-trained
(automatically parsed) corpora which can typically be obtained
for any domain.
KWORDS, returns the percentage of words in one
domain which never appear in the other domain.
This can be done on the word type or token level.
We opt for tokens since unknown words pose prob-
lems for parsing each time they occur. UNKWORDS
provides the percentage of words in the source
domain that are never seen in the target domain.
Whereas COSINETOP50 examines how similar the
high frequency words are from one domain, UN-
KWORDS tends to focus on the overlap of low fre-
quency words.
As described, COSINETOP50 and UNKWORDS
are functions only of two source domains and do not
take the mixing weights of source domains into ac-
count. We experimented with several methods of in-
corporating mixing weights into the feature value.
In practice, the one which worked best for us is to
divide the mixture weight of the source domain by
the raw feature value. This has the nice property that
when a source is not used, the adjusted feature value
is zero regardless of the raw feature value.
From pilot studies, we learned that a uniform mix-
ture of available source domains gave strong results
(further details on this in Section 5). Our last feature,
ENTROPY, is intended to let the regression system
leverage this and measures the entropy of the distri-
bution over source domains. This provides a sense
of uniformity.
</bodyText>
<subsectionHeader confidence="0.999849">
3.2 Predicting cross-domain accuracy
</subsectionHeader>
<bodyText confidence="0.999926833333333">
For a given source domain mixture, we can create
a parsing model by linearly interpolating the pars-
ing model statistics from each source domain. The
key component of our approach is a domain-aware
linear regression model which predicts how well a
specific parsing model will do on a given target text.
The linear regressor is given values from the three
features from the previous section (COSINETOP50,
UNKWORDS, and ENTROPY) and returns an esti-
mate of the f-score the parsing model would achieve
the target text.
Training data for the regressor consists of ex-
amples of source domain mixtures and their ac-
tual f-scores on target texts. To produce this, we
randomly sampled source domain mixtures, created
parsing models for those mixtures, and then evalu-
ated the parsing models on all of our target texts.
We used a simple technique for randomly sam-
</bodyText>
<page confidence="0.972898">
30
</page>
<figure confidence="0.9918475">
0 200 400 600 800 1000
Number of mixed parsing model samples
</figure>
<figureCaption confidence="0.999598">
Figure 1: Cumulative oracle f-score (averaged over
</figureCaption>
<bodyText confidence="0.927469966666666">
all target domains) as more models are randomly
sampled. Most of the improvement comes the first
200 samples indicating that our samples seem to be
sufficient to cover the space of good source domain
mixtures.
pling source domain mixtures. First, we sample the
number of source domains to use. We draw values
from an exponential distribution and take their inte-
ger value until we obtain a number between two and
the number of source domains. This is parametrized
so that we typically only use a few corpora but still
have some chance of using all of them. Once we
know the number of source domains, we sample
their identities uniformly at random without replace-
ment from the list of all source domains. Finally,
we sample the weights for the source domains uni-
formly from a simplex. The dimension of the sim-
plex is the same as the number of source domains
so we end up with a probability distribution over the
sampled source domains.
In total, we sampled 1,040 source domain mix-
tures. We evaluated each of these source domain
mixtures on the six target domains giving us 6,240
data points in total. One may be concerned that
this is insufficient to cover the large space of source
domain mixtures. However, we show in Figure 1
that only about 200 samples are sufficient to achieve
good oracle performance2 in practice.
2We calculate this by picking the best available model for
each target domain and taking the average of their f-scores.
</bodyText>
<figure confidence="0.954998375">
Train Test
Source Target Source Target
C \ {t} C \ {t} C \ {t} {t}
(a) Out-of-domain evaluation
Train Test
Source Target Source Target
C C \ {t} C {t}
(b) In-domain evaluation
</figure>
<tableCaption confidence="0.799925666666667">
Table 2: List of domains allowed in single round of
evaluation. In each round, the evaluation corpus is t.
C is the set of all target domains.
</tableCaption>
<sectionHeader confidence="0.997938" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.980964548387097">
Multiple-source domain adaptation is a new task for
parsing and thus some thought must be given to eval-
uation methodology. We describe two evaluation
scenarios which differ in how foreign the target text
is from our source domains. Schemas for these eval-
uation scenarios are shown in Table 2. Note that
training and testing here refer to training and testing
of our regression model, not the parsing models.
In the first scenario, out-of-domain evaluation,
one target domain is completely removed from con-
sideration and only used to evaluate proposed mod-
els at test time. The regressor is trained on training
points that use any of the remaining corpora, C\{t},
as sources or targets. For example, if t = WSJ, we
can train the regressor on all data points which don’t
use WSJ (or any self-trained corpora derived from
WSJ) as a source or target domain. At test time, we
are given the text of WSJ’s test set. From this, our
system creates a parsing model using the remaining
available corpora for parsing the raw WSJ text.
This evaluation scenario is intended to evaluate
how well our system can adapt to an entirely new
domain with only raw text from the new domain
(for example, parsing biomedical text when none
is available in our list of source domains). Ide-
ally, we would have a large number of web pages
or other documents from other domains which we
could use solely for evaluation. Unfortunately, at
this time, only a handful of domains have been an-
notated with constituency structures under the same
This can pick different models for each target domain.
</bodyText>
<figure confidence="0.996023666666667">
87.5
87.0
86.5
orac e f score
86.0
85.5
85.0
84.5
84.0
</figure>
<page confidence="0.999806">
31
</page>
<bodyText confidence="0.999946045454546">
annotation guidelines. Instead, we hold out each
hand-annotated domain, t, (including any automat-
ically parsed corpora derived from that source do-
main) as a test set in a round-robin fashion.3 For
each round of the round robin we obtain an f-score
and we report the mean and variance of the f-scores
for each model.
The second scenario, in-domain evaluation, al-
lows the target domain, t, to be used as a source
domain in training but not as a target domain. This
is intended to evaluate the situation where the target
domain is not actually that different from our source
domains. The in-domain evaluation can approxi-
mate how our system would perform when, for ex-
ample, we have WSJ as a source domain and the tar-
get text is news from a source other than WSJ. Thus,
our model still has to learn that WSJ and the North
American News Text corpus (NANC) are good for
parsing news text like WSJ without seeing any direct
evaluations of the sort (WSJ and NANC can be used
in models which are evaluated on all other corpora,
though).
</bodyText>
<sectionHeader confidence="0.994325" genericHeader="method">
5 Baselines
</sectionHeader>
<bodyText confidence="0.99778495">
Given that this is a new task for parsing, we needed
to create baselines which demonstrate the current
approaches to multiple-source domain adaptation.
One approach is to take all available corpora and
mix them together uniformly.4 The UNIFORM base-
line does exactly this using the available hand-built
training corpora. SELF-TRAINED UNIFORM uses
self-trained corpora as well. In the out-of-domain
scenario, these exclude the held out domain, but in
the in-domain setting, the held out domain is in-
cluded. These baselines are similar to the ALL and
WEIGHTED baselines in Daum´e III (2007).
Another simple baseline is to use the same pars-
ing model regardless of target domain. This is how
large heterogeneous document collections are typi-
cally parsed currently. We use the WSJ corpus since
it is the best single corpus for parsing all six target
domains (see Table 1). We refer to this baseline as
FIXED SET: WSJ. In the out-of-domain scenario,
we fall back to SELF-TRAINED UNIFORM when the
</bodyText>
<footnote confidence="0.880926">
3Thus, the schemas in Table 2 are schemas for each round.
4Accounting for size so that the larger corpora don’t over-
whelm the smaller ones.
</footnote>
<bodyText confidence="0.999752045454545">
target domain is WSJ while the in-domain scenario
uses the WSJ model throughout.
There are several interesting oracle baselines as
well which serve to measure the limits of our ap-
proach. These baselines examine the resulting
f-scores of models and pick the best model accord-
ing to some criteria. The first oracle baseline is
BEST SINGLE CORPUS which parses each corpus
with the source domain that maximizes performance
on the target domain. In almost all cases, this base-
line selects each corpus to parse itself.
Our second oracle baseline, BEST SEEN, chooses
the best parsing model from all those explored for
each test set. Recall that while training the regres-
sion model in Section 3.2, we needed to explore
many possible source domain mixtures to approxi-
mate the complete space of mixed parsing models.
To the extent that we can fully explore the space of
mixed parsing models, this baseline represents an
upper bound for model mixing approaches. Since
fully exploring the space of possible weightings is
intractable, it is not a true upper bound. While it
is theoretically possible to beat this pseudo-upper
bound, (indeed, this is the mark of a good domain
detection system) it is far from easy. We provide
BEST SINGLE CORPUS and BEST SEEN for both
in-domain and out-of-domain scenarios. The out-of-
domain scenario restricts the set of possible models
to those not including the target domain.
Finally, we searched for the BEST OVERALL
MODEL. This is the model with the highest aver-
age f-score across all six target domains. This base-
line can be thought of as an oracle version of FIXED
SET: WSJ and demonstrates the limit of using a sin-
gle parsing model regardless of target domain. Natu-
rally, the very nature of this baseline places it only in
the in-domain evaluation scenario. Since it was able
to select the model according to f-scores on our six
target domains, its performance on domains outside
that set is not guaranteed.
To provide a better sense of the space of mixed
parsing models, we also provide the WORST SEEN
baseline which picks the worst model available for a
specific target corpus.5
</bodyText>
<footnote confidence="0.9975775">
5This turns out to be GENIA for all corpora other than GENIA
and SWBD when the target domain is GENIA.
</footnote>
<page confidence="0.999138">
32
</page>
<sectionHeader confidence="0.998903" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999994166666667">
Our experiments use the Charniak (2000) generative
parser. We describe the corpora used in our nine
source and six target domains in Section 6.1. In Sec-
tion 6.2, we provide a greedy strategy for picking
features to include in our regression model. The re-
sults of our experiments are in Section 6.3.
</bodyText>
<subsectionHeader confidence="0.988697">
6.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999931323529411">
We aimed to include as many different domains as
possible annotated under compatible schemes. We
also tried to include human-annotated corpora and
automatically labeled corpora (self-trained corpora
as in McClosky et al. (2006a) which have been
shown to work well across domains). Our final
set includes text from news (WSJ, NANC), broad-
cast news (ETT), literature (BROWN, GUTENBERG),
biomedical (GENIA, MEDLINE), spontaneous speech
(SWBD), and the British National Corpus (BNC). In
our experiments, self-trained corpora cannot be used
as target domains since we lack gold annotations and
BNC is not used as a source domain due to its size.
An overview of our corpora is shown in Table 3.
We use news articles portion of the Wall Street
Journal corpus (WSJ) from the Penn Treebank (Mar-
cus et al., 1993) in conjunction with the self-trained
North American News Text Corpus (NANC, Graff
(1995)). The English Translation Treebank, ETT
(Bies, 2007), is the translation6 of broadcast news
in Arabic. For literature, we use the BROWN cor-
pus (Francis and Kuˇcera, 1979) and the same di-
vision as (Gildea, 2001; Bacchiani et al., 2006;
McClosky et al., 2006b). We also use raw sen-
tences which we downloaded from Project Guten-
berg7 as a self-trained corpus. The Switchboard cor-
pus (SWBD) consists of transcribed telephone con-
versations. While the original trees include disflu-
ency information, we assume our speech corpora
have had speech repairs excised (e.g. using a sys-
tem such as Johnson et al. (2004)). Our biomedi-
cal data comes from the GENIA treebank8 (Tateisi
et al., 2005), a corpus of abstracts from the Med-
line database.9 We downloaded additional sentences
</bodyText>
<footnote confidence="0.998934">
6The transcription and translation were done by humans.
7http://gutenberg.org/
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
9http://www.ncbi.nlm.nih.gov/PubMed/
</footnote>
<bodyText confidence="0.999849444444444">
from Medline for our self-trained MEDLINE corpus.
Unlike the other two self-trained corpora, we include
two versions of MEDLINE. These differ on whether
they were parsed using GENIA or WSJ as a base
model to study the effect on cross-domain perfor-
mance. Finally, we use a small number of sentences
from the British National Corpus (BNC) (Foster and
van Genabith, 2008).10 The sentences were chosen
randomly, so each one is potentially from a different
domain. On the other hand, BNC can be thought of
as its own domain in that it contains significant lex-
ical differences from the American English used in
our other corpora.
We preprocessed the corpora to standardize many
of the annotation differences. Thus, our results on
them may be slightly different than other works on
these corpora. Nevertheless, these changes should
not significantly impact overall the performance.
</bodyText>
<subsectionHeader confidence="0.999585">
6.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.99996084">
While our final model uses only three features, we
considered many other possible features (not de-
scribed due to space constraints). In order to explore
these without hill climbing on our test data, we cre-
ated a round-robin tuning scenario. Since the out-
of-domain evaluation scenario holds out one target
domain, this gives us six test evaluation rounds. For
each of these six rounds, we hold out one of the re-
maining five target domains for tuning. This gives
us 30 tuning evaluation rounds and we pick our fea-
tures to optimize our aggregate performance over all
of them. A model that performs well in this situation
has proven that it has useful features which transfer
to unknown target domains.
The next step is to determine the loss function
to minimize. Our primary guide is oracle f-score
loss which we determine as follows. We take all
test data points (i.e. mixed parsing models evalu-
ated on the target domain) and predict their f-scores
with our model. In particular for this measure, we
are interested in the point with the highest predicted
f-score. We take its actual f-score which we call
the candidate f-score. When tuning, we know the
true f-scores of all test points. The difference be-
tween the highest f-score (the oracle f-score for
</bodyText>
<footnote confidence="0.9172945">
10http://nclt.computing.dcu.ie/˜jfoster/
resources/, downloaded January 8th, 2009.
</footnote>
<page confidence="0.993044">
33
</page>
<table confidence="0.998093727272727">
Corpus Source? Target? Average length Train Tune Test
BNC • 28.3 — — 1,000
BROWN • • 20.0 19,786 2,082 2,439
ETT • • 25.6 2,639 1,029 1,166
GENIA • • 27.5 14,326 1,361 1,360
MEDLINE • 27.2 278,192 — —
SWBD • • 9.2 92,536 5,895 6,051
WSJ • • 25.5 39,832 1,346 2,416
NANC • 23.2 915,794 — —
GUTENBERG • 26.2 689,782 — —
MEDLINE • 27.2 278,192 — —
</table>
<tableCaption confidence="0.995895">
Table 3: List of source and target domains, sizes of each division in trees, and average sentence length.
Indented rows indicate self-trained corpora parsed using the non-indented row as a base parser.
</tableCaption>
<bodyText confidence="0.999880307692308">
this dataset) and the candidate f-score is the oracle
f-score loss. Ties need to be handled correctly to
avoid degenerate models.11 If there is a tie for high-
est predicted f-score, the candidate f-score is the
one with the lowest actual f-score. This approach
is conservative but ensures that regression models
which give everything the same predicted f-score do
not receive zero oracle f-score loss.
Armed with a tuning regime and a loss function,
we created a procedure to pick the combination of
features to use. We used a parallelized best-first
search procedure. At each round, it expanded the
current best set of features by adding or removing
each feature where ‘best’ was determined by the loss
function. We explored over 6,000 settings, though
the best setting of (UNKWORDS, COSINETOP50,
ENTROPY) was found within the first 200 settings
explored. The best setting obtains an oracle f-score
loss of 0.37 and a root mean squared error of 0.48
— these numbers are quite low and show the high
accuracy of our regression model (similar to those
in Ravi et al. (2008)). Additionally, the features are
complementary in that UNKWORDS focuses on low
frequency words whereas COSINETOP50 looks only
at high frequency words and ENTROPY functions as
a regularizer.
</bodyText>
<subsectionHeader confidence="0.867334">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999744">
We present an overview of our final results for out-
of-domain and in-domain evaluation in Table 4. The
</bodyText>
<footnote confidence="0.9434675">
11For example, regression models which assign every parsing
model the same f-score.
</footnote>
<bodyText confidence="0.99994975">
results include the f-score macro-averaged over the
six target domains and their standard deviation.
In both situations, the FIXED SET: WSJ baseline
performs fairly poorly. Not surprisingly, assuming
all of our target domains are close enough to WSJ
works badly for our set of target domains and it
does particularly poorly on SWBD and GENIA. On
average, the UNIFORM baseline does slightly bet-
ter for out-of-domain and over 3% better for in-
domain. UNIFORM actually does fairly well on out-
of-domain except on GENIA. In general, using more
source domains is better which partially explains the
success of UNIFORM. This seems to be the case
since even if a source domain is terribly mismatched
with the target domain, it may still be able to fill
in some holes left by the other source domains. Of
course, if it overpowers more relevant domains, per-
formance may suffer. The SELF-TRAINED UNI-
FORM baseline uses even more source domains as
well as the largest ones. In both scenarios, this dra-
matically improves performance and is the second
best non-oracle system. This baseline provides more
evidence as to the power of self-training for improv-
ing parser adaptation. If we excluded all self-trained
corpora, our performance on this task would be sub-
stantially worse. We believe the self-trained cor-
pora are beneficial in this task since they help reduce
data sparsity of smaller corpora. The BEST SINGLE
CORPUS baseline is poor in the out-of-domain sce-
nario primarily because the actual best single corpus
is excluded by the task specification in most cases.
When we move to in-domain, this baseline improves
</bodyText>
<page confidence="0.995727">
34
</page>
<bodyText confidence="0.78778">
Oracle Baseline or model Average f-score
</bodyText>
<listItem confidence="0.82072">
• Worst seen 62.0 f 6.1
• Best single corpus 81.0 f 2.9
</listItem>
<figure confidence="0.651612666666667">
Fixed set: WSJ 81.0 f 3.5
Uniform 81.4 f 3.6
Self-trained uniform 83.4 f 2.5
Our model 84.0 f 2.5
• Best seen 84.3 f 2.6
(a) Out-of-domain evaluation
Oracle Baseline or model Average f-score
Fixed set: WSJ 82.0 f 4.8
Uniform 85.4 f 2.4
• Best single corpus 85.6 f 2.9
Self-trained uniform 86.1 f 2.0
• Best overall model 86.2 f 1.9
Our model 86.9 f 2.4
• Best seen 87.5 f 2.1
(b) In-domain evaluation
</figure>
<tableCaption confidence="0.938019">
Table 4: Baselines and final results for the two multiple-source domain adaptation evaluation scenarios.
Results include f-scores, macro-averaged over all six target domains and their standard deviations.
</tableCaption>
<bodyText confidence="0.99985448">
but is still worse than SELF-TRAINED UNIFORM on
average. It beats SELF-TRAINED UNIFORM primar-
ily on WSJ, SWBD, and GENIA indicating that these
three domains are best when not diluted by others.
By definition, the WORST SEEN baseline does terri-
bly, almost 20% worse then BEST SINGLE CORPUS.
Our model is the best non-oracle system for both
evaluation scenarios. For out-of-domain evaluation,
our system is only 0.3% worse than the BEST SEEN
models for each target domain. For the in-domain
scenario, we are within 0.6% of the BEST SEEN
models. For a sense of scale, our out-of-domain and
in-domain f-scores on WSJ are 83.1% and 89.8%
respectively. Both numbers are quite close to the
BEST SEEN baseline. Additionally, our model is
0.7% better than the BEST OVERALL MODEL. Re-
call that the BEST OVERALL MODEL is the single
model with the best performance across all six tar-
get domains.12 By beating this baseline, we show
that there is value in customizing parsing models
to the target domain. It is also interesting that the
BEST OVERALL MODEL is only marginally better
than SELF-TRAINED UNIFORM. Without any fur-
ther information about the target corpus, an unin-
formed prior appears best.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999934666666667">
We have shown that for both out-of-domain and in-
domain evaluations, our system is well adapted to
predicting the effects of domain divergence on pars-
</bodyText>
<footnote confidence="0.908777">
12Somewhat surprisingly, the best overall model uses almost
entirely self-trained corpora consisting of 9.5% GUTENBERG,
60.3% NANC, 26.0% MEDLINE (by GENIA), and 4.2% SWBD.
</footnote>
<bodyText confidence="0.999947703703704">
ing accuracy. Using the parsing model with the
highest predicted f-score leads to great performance
in practice. There is a substantial benefit to doing
this over existing approaches (using the same model
for all domains or mixing all training data together
uniformly). Creating a number of domain-specific
models and mixing them together as needed is a vi-
able approach.
One can think of our system as trying to esti-
mate document-level context. Our representation of
this context is simply a distribution over our source
domains, but one can imagine more complex op-
tions such as a high-dimensional vector space. Ad-
ditionally, our model separates domain and syntax
estimation, but a future direction is to learn these
jointly. This would combine our work with (Daum´e
III, 2007; Finkel and Manning, 2009).
We have focused on the Charniak (2000) parser,
the first stage in the two stage Charniak and John-
son (2005) reranking parser. Applying our methods
to other generative parsers (such as (Collins, 1999;
Petrov and Klein, 2007)) is trivial, but it is less clear
how our methods can be applied to the discrimina-
tive reranker component of the two stage parser. One
avenue of approach is to incorporate the domain rep-
resentation into the feature space, as in Daum´e III
(2007) but with more complex domain information.
</bodyText>
<sectionHeader confidence="0.998341" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.95364025">
This work was performed while the first author was
at Brown and supported by DARPA GALE contract
HR0011-06-2-0001. We would like to thank the BLLIP
team and our anonymous reviewers for their comments.
</bodyText>
<page confidence="0.998872">
35
</page>
<sectionHeader confidence="0.989612" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709095238095">
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41–68.
Ann Bies. 2007. GALE Phase 3 Release 1 - English
Translation Treebank. Linguistic Data Consortium.
LDC2007E105.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Association for Computational Linguistics, Prague,
Czech Republic.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL 2008, pages 9–16, Manchester, England, Au-
gust.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the ACL 2005, pages 173–180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American Chapter
of the ACL (NAACL), pages 132–139.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, The Uni-
versity of Pennsylvania.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL 2007, Prague, Czech
Republic.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of the EMNLP 2008, pages 689–697, Honolulu,
Hawaii, October.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of HLT-NAACL 2009, pages 602–610, Boulder,
Colorado, June.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the bnc: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings LREC 2008,
Marrakech, Morocco, May.
W. Nelson Francis and Henry Kuˇcera. 1979. Manual
of Information to accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers. Brown University, Providence,
Rhode Island.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Empirical Methods in Natural Language
Processing (EMNLP), pages 167–202.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An improved model for recognizing disfluen-
cies in conversational speech. In Proc. of the Rich Text
2004 Fall Workshop (RT-04F).
Daisuke Kawahara and Kiyotaka Uchimoto. 2008.
Learning reliability of parses for domain adaptation
of dependency parsing. In Third International Joint
Conference on Natural Language Processing (IJCNLP
’08).
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Comp. Linguis-
tics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006a. Effective self-training for parsing. In Proceed-
ings ofHLT-NAACL 2006, pages 152–159.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of COLING-ACL 2006,
pages 337–344, Sydney, Australia, July. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404–411, Rochester, New York, April. Association for
Computational Linguistics.
Barbara Plank and Khalil Sima’an. 2008. Subdomain
sensitive statistical parsing using raw corpora. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC’08), Marrakech, Mo-
rocco, May.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic prediction of parser accuracy. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 887–896, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Hinrich Sch¨utze. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th conference of the
EACL, pages 141–148.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proc. Applied Natural Language Processing
(ANLP), pages 96–102.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings EMNLP 2009, pages 551–560, Singa-
pore, August.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun’ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. Proceedings of IJCNLP 2005, Compan-
ion volume, pages 222–227.
</reference>
<page confidence="0.998924">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956971">
<title confidence="0.977885">Automatic Domain Adaptation for Parsing</title>
<affiliation confidence="0.999977">University University University</affiliation>
<address confidence="0.994583">Stanford, CA, USA Providence, RI, USA Sydney, NSW, Australia</address>
<email confidence="0.998829">mcclosky@stanford.eduec@cs.brown.edumjohnson@science.mq.edu.au</email>
<abstract confidence="0.99939188">Current statistical parsers tend to perform well only on their training domain and nearby genres. While strong performance on a few related domains is sufficient for many situations, it is advantageous for parsers to be able to generalize to a wide variety of domains. When parsing document collections involving heterogeneous domains (e.g. the web), the optimal parsing model for each document is typically not obvious. We study this problem as new task — source parser adapta- Our system trains on corpora from many different domains. It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations ofparsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1802" citStr="Bacchiani et al., 2006" startWordPosition="268" endWordPosition="271">selines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finkel and Manning (2009) showed techniques for training models that attempt to separate domainspecific and general properties. However, even when given models for multiple training domains, it is not straightforward to determ</context>
<context position="21419" citStr="Bacchiani et al., 2006" startWordPosition="3577" endWordPosition="3580">f-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the self-trained North American News Text Corpus (NANC, Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2004)). Our biomedical data comes from the GENIA treebank8 (Tateisi et al., 2005), a corpus of abstracts from the Medline database.9 We downloaded additional sentences 6The transcription and translation were done by humans. 7http://gut</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech and Language, 20(1):41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
</authors>
<title>English Translation Treebank. Linguistic Data Consortium.</title>
<date>2007</date>
<journal>GALE Phase</journal>
<volume>3</volume>
<contexts>
<context position="21238" citStr="Bies, 2007" startWordPosition="3547" endWordPosition="3548">roadcast news (ETT), literature (BROWN, GUTENBERG), biomedical (GENIA, MEDLINE), spontaneous speech (SWBD), and the British National Corpus (BNC). In our experiments, self-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the self-trained North American News Text Corpus (NANC, Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2004)). Our biomedical data comes from the GENIA treeb</context>
</contexts>
<marker>Bies, 2007</marker>
<rawString>Ann Bies. 2007. GALE Phase 3 Release 1 - English Translation Treebank. Linguistic Data Consortium. LDC2007E105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6032" citStr="Blitzer et al., 2007" startWordPosition="956" endWordPosition="959">s all non-oracle baselines. We conclude with a discussion and future work (Section 7). 2 Related work The closest work to ours is Plank and Sima’an (2008), where unlabeled text is used to group sentences from WSJ into subdomains. The authors create a model for each subdomain which weights trees from its subdomain more highly than others. Given the domain specific models, they consider different parse combination strategies. Unfortunately, these methods do not yield a statistically significant improvement. Multiple source domain adaptation has been done for other tasks (e.g. classification in (Blitzer et al., 2007; Daum´e III, 2007; Dredze and Crammer, 2008)) and is related to multitask learning. Daum´e III (2007) shows that an extremely simple method delivers solid performance on a number of domain adaptation classification tasks. This is achieved by making a copy of each feature for each source domain plus the “general” pseudodomain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distributi</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>9--16</pages>
<location>Manchester, England,</location>
<contexts>
<context position="1557" citStr="Carreras et al., 2008" startWordPosition="230" endWordPosition="233">d how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations ofparsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Dau</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of CoNLL 2008, pages 9–16, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1486" citStr="Charniak and Johnson, 2005" startWordPosition="218" endWordPosition="221">atistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations ofparsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in additi</context>
<context position="31099" citStr="Charniak and Johnson (2005)" startWordPosition="5174" endWordPosition="5178">er of domain-specific models and mixing them together as needed is a viable approach. One can think of our system as trying to estimate document-level context. Our representation of this context is simply a distribution over our source domains, but one can imagine more complex options such as a high-dimensional vector space. Additionally, our model separates domain and syntax estimation, but a future direction is to learn these jointly. This would combine our work with (Daum´e III, 2007; Finkel and Manning, 2009). We have focused on the Charniak (2000) parser, the first stage in the two stage Charniak and Johnson (2005) reranking parser. Applying our methods to other generative parsers (such as (Collins, 1999; Petrov and Klein, 2007)) is trivial, but it is less clear how our methods can be applied to the discriminative reranker component of the two stage parser. One avenue of approach is to incorporate the domain representation into the feature space, as in Daum´e III (2007) but with more complex domain information. Acknowledgments This work was performed while the first author was at Brown and supported by DARPA GALE contract HR0011-06-2-0001. We would like to thank the BLLIP team and our anonymous reviewer</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the ACL 2005, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the North American Chapter of the ACL (NAACL),</booktitle>
<pages>132--139</pages>
<contexts>
<context position="4156" citStr="Charniak (2000)" startWordPosition="652" endWordPosition="653">arget text) but not the identity of its domain. The challenge is determining how to best use the available Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 28–36, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Test Train BNC GENIA BROWN SWBD ETT WSJ Average GENIA 66.3 83.6 64.6 51.6 69.0 66.6 67.0 BROWN 81.0 71.5 86.3 79.0 80.9 80.6 79.9 SWBD 70.8 62.9 75.5 89.0 75.9 69.1 73.9 ETT 72.7 65.3 75.4 75.2 81.9 73.2 73.9 WSJ 82.5 74.9 83.8 78.5 83.4 89.0 82.0 Table 1: Cross-domain f-score performance of the Charniak (2000) parser. Averages are macro-averages. Performance drops as training and test domains diverge. On average, the WSJ model is the most accurate. resources from training to maximize accuracy across multiple target texts. Broadly put, we model how domain differences influence parsing accuracy. This is done by taking several computational measures of domain differences between the target text and each source domain. We use these features in a simple linear regression model which is trained to predict the accuracy of a parsing model (or, more generally, a mixture of parsing models) on a target text. </context>
<context position="20022" citStr="Charniak (2000)" startWordPosition="3349" endWordPosition="3350"> model regardless of target domain. Naturally, the very nature of this baseline places it only in the in-domain evaluation scenario. Since it was able to select the model according to f-scores on our six target domains, its performance on domains outside that set is not guaranteed. To provide a better sense of the space of mixed parsing models, we also provide the WORST SEEN baseline which picks the worst model available for a specific target corpus.5 5This turns out to be GENIA for all corpora other than GENIA and SWBD when the target domain is GENIA. 32 6 Experiments Our experiments use the Charniak (2000) generative parser. We describe the corpora used in our nine source and six target domains in Section 6.1. In Section 6.2, we provide a greedy strategy for picking features to include in our regression model. The results of our experiments are in Section 6.3. 6.1 Corpora We aimed to include as many different domains as possible annotated under compatible schemes. We also tried to include human-annotated corpora and automatically labeled corpora (self-trained corpora as in McClosky et al. (2006a) which have been shown to work well across domains). Our final set includes text from news (WSJ, NAN</context>
<context position="31030" citStr="Charniak (2000)" startWordPosition="5164" endWordPosition="5165">ng all training data together uniformly). Creating a number of domain-specific models and mixing them together as needed is a viable approach. One can think of our system as trying to estimate document-level context. Our representation of this context is simply a distribution over our source domains, but one can imagine more complex options such as a high-dimensional vector space. Additionally, our model separates domain and syntax estimation, but a future direction is to learn these jointly. This would combine our work with (Daum´e III, 2007; Finkel and Manning, 2009). We have focused on the Charniak (2000) parser, the first stage in the two stage Charniak and Johnson (2005) reranking parser. Applying our methods to other generative parsers (such as (Collins, 1999; Petrov and Klein, 2007)) is trivial, but it is less clear how our methods can be applied to the discriminative reranker component of the two stage parser. One avenue of approach is to incorporate the domain representation into the feature space, as in Daum´e III (2007) but with more complex domain information. Acknowledgments This work was performed while the first author was at Brown and supported by DARPA GALE contract HR0011-06-2-0</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the North American Chapter of the ACL (NAACL), pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, The University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
</authors>
<title>Online methods for multi-domain learning and adaptation.</title>
<date>2008</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<pages>689--697</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="6077" citStr="Dredze and Crammer, 2008" startWordPosition="963" endWordPosition="967">with a discussion and future work (Section 7). 2 Related work The closest work to ours is Plank and Sima’an (2008), where unlabeled text is used to group sentences from WSJ into subdomains. The authors create a model for each subdomain which weights trees from its subdomain more highly than others. Given the domain specific models, they consider different parse combination strategies. Unfortunately, these methods do not yield a statistically significant improvement. Multiple source domain adaptation has been done for other tasks (e.g. classification in (Blitzer et al., 2007; Daum´e III, 2007; Dredze and Crammer, 2008)) and is related to multitask learning. Daum´e III (2007) shows that an extremely simple method delivers solid performance on a number of domain adaptation classification tasks. This is achieved by making a copy of each feature for each source domain plus the “general” pseudodomain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named </context>
</contexts>
<marker>Dredze, Crammer, 2008</marker>
<rawString>Mark Dredze and Koby Crammer. 2008. Online methods for multi-domain learning and adaptation. In Proceedings of the EMNLP 2008, pages 689–697, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL 2009,</booktitle>
<pages>602--610</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="2201" citStr="Finkel and Manning (2009)" startWordPosition="331" endWordPosition="334">09, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finkel and Manning (2009) showed techniques for training models that attempt to separate domainspecific and general properties. However, even when given models for multiple training domains, it is not straightforward to determine which model performs best on an arbitrary piece of novel text. This problem comes to the fore when one wants to parse document collections where each document is potentially its own domain. This shows up particularly when parsing the web. Recently, there has been much interest in applying parsers to the web for the purposes of information extraction and other forms of analysis (c.f. the CLSP </context>
<context position="6511" citStr="Finkel and Manning (2009)" startWordPosition="1030" endWordPosition="1033">atistically significant improvement. Multiple source domain adaptation has been done for other tasks (e.g. classification in (Blitzer et al., 2007; Daum´e III, 2007; Dredze and Crammer, 2008)) and is related to multitask learning. Daum´e III (2007) shows that an extremely simple method delivers solid performance on a number of domain adaptation classification tasks. This is achieved by making a copy of each feature for each source domain plus the “general” pseudodomain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named entity recognition) as well as dependency parsing. These works describe how to train models in many different domains but sidestep the problem of domain detection. Thus, our work is orthogonal to theirs. Our domain detection strategy draws on work in parser accuracy prediction (Ravi et al., 2008; Kawahara and Uchimoto, 2008). These works aim to predict the parser performance on a given target sentence. Ravi et al. (2008) frame thi</context>
<context position="30990" citStr="Finkel and Manning, 2009" startWordPosition="5155" endWordPosition="5158">ches (using the same model for all domains or mixing all training data together uniformly). Creating a number of domain-specific models and mixing them together as needed is a viable approach. One can think of our system as trying to estimate document-level context. Our representation of this context is simply a distribution over our source domains, but one can imagine more complex options such as a high-dimensional vector space. Additionally, our model separates domain and syntax estimation, but a future direction is to learn these jointly. This would combine our work with (Daum´e III, 2007; Finkel and Manning, 2009). We have focused on the Charniak (2000) parser, the first stage in the two stage Charniak and Johnson (2005) reranking parser. Applying our methods to other generative parsers (such as (Collins, 1999; Petrov and Klein, 2007)) is trivial, but it is less clear how our methods can be applied to the discriminative reranker component of the two stage parser. One avenue of approach is to incorporate the domain representation into the feature space, as in Daum´e III (2007) but with more complex domain information. Acknowledgments This work was performed while the first author was at Brown and suppor</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Hierarchical bayesian domain adaptation. In Proceedings of HLT-NAACL 2009, pages 602–610, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Parser evaluation and the bnc: Evaluating 4 constituency parsers with 3 metrics.</title>
<date>2008</date>
<booktitle>In Proceedings LREC 2008,</booktitle>
<location>Marrakech, Morocco,</location>
<marker>Foster, van Genabith, 2008</marker>
<rawString>Jennifer Foster and Josef van Genabith. 2008. Parser evaluation and the bnc: Evaluating 4 constituency parsers with 3 metrics. In Proceedings LREC 2008, Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kuˇcera</author>
</authors>
<title>Manual of Information to accompany a Standard Corpus of Present-day Edited American English, for use with Digital Computers.</title>
<date>1979</date>
<institution>Brown University,</institution>
<location>Providence, Rhode Island.</location>
<marker>Francis, Kuˇcera, 1979</marker>
<rawString>W. Nelson Francis and Henry Kuˇcera. 1979. Manual of Information to accompany a Standard Corpus of Present-day Edited American English, for use with Digital Computers. Brown University, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>167--202</pages>
<contexts>
<context position="1778" citStr="Gildea, 2001" startWordPosition="266" endWordPosition="267"> non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finkel and Manning (2009) showed techniques for training models that attempt to separate domainspecific and general properties. However, even when given models for multiple training domains, it is not s</context>
<context position="21395" citStr="Gildea, 2001" startWordPosition="3575" endWordPosition="3576">periments, self-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the self-trained North American News Text Corpus (NANC, Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2004)). Our biomedical data comes from the GENIA treebank8 (Tateisi et al., 2005), a corpus of abstracts from the Medline database.9 We downloaded additional sentences 6The transcription and translation were don</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Empirical Methods in Natural Language Processing (EMNLP), pages 167–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>North American News Text Corpus. Linguistic Data Consortium.</title>
<date>1995</date>
<tech>LDC95T21.</tech>
<contexts>
<context position="21185" citStr="Graff (1995)" startWordPosition="3540" endWordPosition="3541">. Our final set includes text from news (WSJ, NANC), broadcast news (ETT), literature (BROWN, GUTENBERG), biomedical (GENIA, MEDLINE), spontaneous speech (SWBD), and the British National Corpus (BNC). In our experiments, self-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the self-trained North American News Text Corpus (NANC, Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2</context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>David Graff. 1995. North American News Text Corpus. Linguistic Data Consortium. LDC95T21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
<author>Matthew Lease</author>
</authors>
<title>An improved model for recognizing disfluencies in conversational speech.</title>
<date>2004</date>
<booktitle>In Proc. of the Rich Text 2004 Fall Workshop (RT-04F).</booktitle>
<contexts>
<context position="21789" citStr="Johnson et al. (2004)" startWordPosition="3638" endWordPosition="3641">ANC, Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2004)). Our biomedical data comes from the GENIA treebank8 (Tateisi et al., 2005), a corpus of abstracts from the Medline database.9 We downloaded additional sentences 6The transcription and translation were done by humans. 7http://gutenberg.org/ 8http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/ 9http://www.ncbi.nlm.nih.gov/PubMed/ from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE. These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain performance. Finally, we use a sma</context>
</contexts>
<marker>Johnson, Charniak, Lease, 2004</marker>
<rawString>Mark Johnson, Eugene Charniak, and Matthew Lease. 2004. An improved model for recognizing disfluencies in conversational speech. In Proc. of the Rich Text 2004 Fall Workshop (RT-04F).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>Learning reliability of parses for domain adaptation of dependency parsing.</title>
<date>2008</date>
<booktitle>In Third International Joint Conference on Natural Language Processing (IJCNLP ’08).</booktitle>
<contexts>
<context position="7003" citStr="Kawahara and Uchimoto, 2008" startWordPosition="1107" endWordPosition="1111">domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named entity recognition) as well as dependency parsing. These works describe how to train models in many different domains but sidestep the problem of domain detection. Thus, our work is orthogonal to theirs. Our domain detection strategy draws on work in parser accuracy prediction (Ravi et al., 2008; Kawahara and Uchimoto, 2008). These works aim to predict the parser performance on a given target sentence. Ravi et al. (2008) frame this as a regression problem. Kawahara and Uchimoto (2008) treat it as a binary classification task and predict whether a specific parse is at a certain level of accuracy or higher. Ravi et al. (2008) show that their system can be used to return a ranking over different parsing models which we extend to the multiple domain setting. They also demonstrate that training their model on WSJ allows them to accurately predict parsing accuracy on the BROWN corpus. In contrast, our models are traine</context>
</contexts>
<marker>Kawahara, Uchimoto, 2008</marker>
<rawString>Daisuke Kawahara and Kiyotaka Uchimoto. 2008. Learning reliability of parses for domain adaptation of dependency parsing. In Third International Joint Conference on Natural Language Processing (IJCNLP ’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Comp. Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="21096" citStr="Marcus et al., 1993" startWordPosition="3524" endWordPosition="3528">-trained corpora as in McClosky et al. (2006a) which have been shown to work well across domains). Our final set includes text from news (WSJ, NANC), broadcast news (ETT), literature (BROWN, GUTENBERG), biomedical (GENIA, MEDLINE), spontaneous speech (SWBD), and the British National Corpus (BNC). In our experiments, self-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the self-trained North American News Text Corpus (NANC, Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our spee</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Comp. Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<pages>152--159</pages>
<contexts>
<context position="1509" citStr="McClosky et al., 2006" startWordPosition="222" endWordPosition="225">t quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations ofparsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general </context>
<context position="20520" citStr="McClosky et al. (2006" startWordPosition="3428" endWordPosition="3431">corpora other than GENIA and SWBD when the target domain is GENIA. 32 6 Experiments Our experiments use the Charniak (2000) generative parser. We describe the corpora used in our nine source and six target domains in Section 6.1. In Section 6.2, we provide a greedy strategy for picking features to include in our regression model. The results of our experiments are in Section 6.3. 6.1 Corpora We aimed to include as many different domains as possible annotated under compatible schemes. We also tried to include human-annotated corpora and automatically labeled corpora (self-trained corpora as in McClosky et al. (2006a) which have been shown to work well across domains). Our final set includes text from news (WSJ, NANC), broadcast news (ETT), literature (BROWN, GUTENBERG), biomedical (GENIA, MEDLINE), spontaneous speech (SWBD), and the British National Corpus (BNC). In our experiments, self-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006a. Effective self-training for parsing. In Proceedings ofHLT-NAACL 2006, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>337--344</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1509" citStr="McClosky et al., 2006" startWordPosition="222" endWordPosition="225">t quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations ofparsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general </context>
<context position="20520" citStr="McClosky et al. (2006" startWordPosition="3428" endWordPosition="3431">corpora other than GENIA and SWBD when the target domain is GENIA. 32 6 Experiments Our experiments use the Charniak (2000) generative parser. We describe the corpora used in our nine source and six target domains in Section 6.1. In Section 6.2, we provide a greedy strategy for picking features to include in our regression model. The results of our experiments are in Section 6.3. 6.1 Corpora We aimed to include as many different domains as possible annotated under compatible schemes. We also tried to include human-annotated corpora and automatically labeled corpora (self-trained corpora as in McClosky et al. (2006a) which have been shown to work well across domains). Our final set includes text from news (WSJ, NANC), broadcast news (ETT), literature (BROWN, GUTENBERG), biomedical (GENIA, MEDLINE), spontaneous speech (SWBD), and the British National Corpus (BNC). In our experiments, self-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of COLING-ACL 2006, pages 337–344, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="1534" citStr="Petrov and Klein, 2007" startWordPosition="226" endWordPosition="229">of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations ofparsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Khalil Sima’an</author>
</authors>
<title>Subdomain sensitive statistical parsing using raw corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<marker>Plank, Sima’an, 2008</marker>
<rawString>Barbara Plank and Khalil Sima’an. 2008. Subdomain sensitive statistical parsing using raw corpora. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
<author>Radu Soricut</author>
</authors>
<title>Automatic prediction of parser accuracy.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>887--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="6973" citStr="Ravi et al., 2008" startWordPosition="1103" endWordPosition="1106">ain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named entity recognition) as well as dependency parsing. These works describe how to train models in many different domains but sidestep the problem of domain detection. Thus, our work is orthogonal to theirs. Our domain detection strategy draws on work in parser accuracy prediction (Ravi et al., 2008; Kawahara and Uchimoto, 2008). These works aim to predict the parser performance on a given target sentence. Ravi et al. (2008) frame this as a regression problem. Kawahara and Uchimoto (2008) treat it as a binary classification task and predict whether a specific parse is at a certain level of accuracy or higher. Ravi et al. (2008) show that their system can be used to return a ranking over different parsing models which we extend to the multiple domain setting. They also demonstrate that training their model on WSJ allows them to accurately predict parsing accuracy on the BROWN corpus. In c</context>
<context position="8375" citStr="Ravi et al. (2008)" startWordPosition="1335" endWordPosition="1338">mixtures of our source domains.1 Intuitively, these mixtures should give higher probability mass to more similar source domains. This raises the question of how to measure the similarity between domains. Our method uses multiple complementary similarity measures between the target and each source. We feed these similarity measures into a regression model which learns how domain dissimilarities hurt parse accuracy. Thus, to parse a target domain, we need only find the input that maximizes the regression function — that is, the highest scoring mixture of source domains. Our system is similar to Ravi et al. (2008) in that both use regression to predict f-scores and some of the features are related. 3.1 Features Our features are designed to help the regression model determine if a particular source domain mixture is well suited for a target domain as well as the quality of a source domain mixture. While we explored a large number of features, we present here only the three that were chosen by our feature selection method (Section 6.2). Two of our features, COSINETOP50 and UNKWORDS, are designed to approximate how similar the target domain is to a specific source domain. Only the surface form of the targ</context>
<context position="25966" citStr="Ravi et al. (2008)" startWordPosition="4327" endWordPosition="4330">created a procedure to pick the combination of features to use. We used a parallelized best-first search procedure. At each round, it expanded the current best set of features by adding or removing each feature where ‘best’ was determined by the loss function. We explored over 6,000 settings, though the best setting of (UNKWORDS, COSINETOP50, ENTROPY) was found within the first 200 settings explored. The best setting obtains an oracle f-score loss of 0.37 and a root mean squared error of 0.48 — these numbers are quite low and show the high accuracy of our regression model (similar to those in Ravi et al. (2008)). Additionally, the features are complementary in that UNKWORDS focuses on low frequency words whereas COSINETOP50 looks only at high frequency words and ENTROPY functions as a regularizer. 6.3 Results We present an overview of our final results for outof-domain and in-domain evaluation in Table 4. The 11For example, regression models which assign every parsing model the same f-score. results include the f-score macro-averaged over the six target domains and their standard deviation. In both situations, the FIXED SET: WSJ baseline performs fairly poorly. Not surprisingly, assuming all of our </context>
</contexts>
<marker>Ravi, Knight, Soricut, 2008</marker>
<rawString>Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Automatic prediction of parser accuracy. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 887–896, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th conference of the EACL,</booktitle>
<pages>141--148</pages>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In Proceedings of the 7th conference of the EACL, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>The domain dependence of parsing.</title>
<date>1997</date>
<booktitle>In Proc. Applied Natural Language Processing (ANLP),</booktitle>
<pages>96--102</pages>
<contexts>
<context position="1764" citStr="Sekine, 1997" startWordPosition="264" endWordPosition="265">utperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finkel and Manning (2009) showed techniques for training models that attempt to separate domainspecific and general properties. However, even when given models for multiple training domain</context>
</contexts>
<marker>Sekine, 1997</marker>
<rawString>Satoshi Sekine. 1997. The domain dependence of parsing. In Proc. Applied Natural Language Processing (ANLP), pages 96–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings EMNLP 2009,</booktitle>
<pages>551--560</pages>
<location>Singapore,</location>
<contexts>
<context position="1578" citStr="Suzuki et al., 2009" startWordPosition="234" endWordPosition="237"> affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations ofparsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of 28 syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Fi</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In Proceedings EMNLP 2009, pages 551–560, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuka Tateisi</author>
<author>Akane Yakushiji</author>
<author>Tomoko Ohta</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Syntax Annotation for the GENIA corpus.</title>
<date>2005</date>
<booktitle>Proceedings of IJCNLP 2005, Companion volume,</booktitle>
<pages>222--227</pages>
<contexts>
<context position="21865" citStr="Tateisi et al., 2005" startWordPosition="3651" endWordPosition="3654">he translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2004)). Our biomedical data comes from the GENIA treebank8 (Tateisi et al., 2005), a corpus of abstracts from the Medline database.9 We downloaded additional sentences 6The transcription and translation were done by humans. 7http://gutenberg.org/ 8http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/ 9http://www.ncbi.nlm.nih.gov/PubMed/ from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE. These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain performance. Finally, we use a small number of sentences from the British National Corpus (BNC) (Foster and va</context>
</contexts>
<marker>Tateisi, Yakushiji, Ohta, Tsujii, 2005</marker>
<rawString>Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Jun’ichi Tsujii. 2005. Syntax Annotation for the GENIA corpus. Proceedings of IJCNLP 2005, Companion volume, pages 222–227.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>