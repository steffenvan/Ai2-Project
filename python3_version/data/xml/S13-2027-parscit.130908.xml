<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001619">
<title confidence="0.798487">
SFS-TUE: Compound Paraphrasing with a Language Model and
Discriminative Reranking
Yannick Versley
</title>
<author confidence="0.655419">
SfS / SFB 833
</author>
<affiliation confidence="0.941289">
University of T¨ubingen
</affiliation>
<email confidence="0.994299">
versley@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.995565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903666666667">
This paper presents an approach for gener-
ating free paraphrases of compounds (task 4
at SemEval 2013) by decomposing the train-
ing data into a collection of templates and
fillers and recombining/scoring these based on
a generative language model and discrimina-
tive MaxEnt reranking.
The system described in this paper achieved
the highest score (with a very small margin) in
the (default) isomorphic setting of the scorer,
for which it was optimized, at a disadvantage
to the non-isomorphic score.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999556555555555">
Compounds are an interesting phenomenon in nat-
ural language semantics as they normally realize a
semantic relation (between head and modifier noun)
that is both highly ambiguous as to the type of rela-
tion and usually nonambiguous as to the concepts it
relates (namely, those of the two nouns).
Besides inventory-based approaches, where the
relation is classified into a fixed number of relations,
many researchers have argued that the full variabil-
ity of the semantic relations inherent in compounds
is best captured with paraphrases: Lauer (1995) pro-
poses to use a preposition as a proxy for the meaning
of a compound. Finin (1980) and later Nakov (2008)
and others propose less restrictive schemes based on
paraphrasing verbs.
A previous SemEval task (task 9 in 2010; But-
nariu et al., 2009). The most successsful approaches
for this task such as Nulty and Costello (2010), Li
et al. (2010), and Wubben (2010), or the subse-
quent approach of Wijaya and Gianfortoni (2011),
all make efficient use of both the training data and
general evidence from WordNet or statistics derived
from large corpora. The paper of Li et al. men-
tions that solely inducing a global ranking of para-
phrasing verbs from the training data (looking which
verb is ranked higher in those cases where both were
considered for the same compound) yielded higher
scores than an unsupervised approach based on the
semantic resources, underlining the need to combine
training data and resources efficiently.
SemEval 2013 task 4 The present task on pro-
viding free paraphrases for noun compounds (Hen-
drickx et al., 2013) uses a dataset collected from Me-
chanical Turk workers asked to paraphrase a given
compound (without context). Prepositional, verbal,
and other paraphrases all occur in the data:
</bodyText>
<listItem confidence="0.98317525">
(1) a. bar for wine
b. bar that serves wine
c. bar where wine is sold
d. sweet vinegar made from wine
</listItem>
<bodyText confidence="0.9998972">
In the examples, the words of the compound (wine
bar and wine vinegar, respectively) are put in ital-
ics, and other content words in the paraphrase are
underlined.
It is clear that certain paraphrases (X for Y) will be
common across many compounds, whereas the ones
containing more lexical material will differ even be-
tween relatively similar compounds (consider wine
bar from the example, and liquor store, which al-
lows paraphrase c, but not paraphrase b).
</bodyText>
<page confidence="0.943194">
148
</page>
<bodyText confidence="0.783929">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 148–152, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.948381" genericHeader="general terms">
2 General Approach
</sectionHeader>
<bodyText confidence="0.999868647058824">
The approach chosen in the SFS-TUE system is
based on first retrieving a number of similar com-
pounds, then extracting a set of building blocks (pat-
terns and fillers) from these compounds, recombin-
ing these building blocks, and finally ranking the
list of potential paraphrases. The final list is post-
processed by keeping only one variant of each set
of paraphrases that only differ in a determiner (e.g.,
‘strike from air’ and ‘strike from the air’) in order
to make a 1:1 mapping between system response and
gold standard possible.
As a first step, the system retrieves the most simi-
lar compounds from the training data.
This is achieved Lin’s wordnet similarity measure
(Lin, 1998) using the implementation in NLTK (Bird
et al., 2009). The similarity of two compounds X1Y1
and X2Y2 is calculated as
</bodyText>
<equation confidence="0.9998105">
sC = min(sim(X1, X2), sim(Y1, Y2)) +
0.1 · (sim(X1, X2) + sim(Y1, Y2))
</equation>
<bodyText confidence="0.997561923076923">
which represents a compromise between requiring
that both modifier and head are approximately sim-
ilar, and still giving a small boost to pairs that have
very high modifier similarity but low head similar-
ity, or vice versa. For training, the target compound
is excluded from the most-similar compounds list so
that candidate construction is only based on actual
neighbours.
The paraphrases for the most similar compound
entries (such as 2a) are broken down into templates
(2b) and fillers (2c), by replacing modifier and head
by X and Y , respectively, and other content words
by their part-of-speech tag.
</bodyText>
<listItem confidence="0.796286666666667">
(2) a. bar that serves wine
b. X that VBZ Y
c. VBZ:serve
</listItem>
<bodyText confidence="0.999912625">
Conversely, template fillers consist of all the ex-
tracted content words, categorized by their part-of-
speech. (Part-of-speech tags were assigned using the
Stanford POS tagger: Toutanova et al., 2003).
Both paraphrase templates and template fillers are
weighted by the product of the similarity value sC
between the target compound and the neighbour, and
the total frequency of occurrence in that neighbour’s
</bodyText>
<equation confidence="0.979209125">
type examples
Y of Y of X (159) / Y of the X (59) / Y of a X (47)
Y for Y for X (114) / Y for the X (33)
Y VBZ Y that VBZ X (91)/ Y which VBZ X (45)
Y VBG Y VBG X (90) / Y VBG the X/ Y VBG with X
Y VBN Y VBN for X (82) / Y VBN by X (52)
Y in Y in X (31)
Y on Y on X (38)
</equation>
<tableCaption confidence="0.994155">
Table 1: Most frequent paraphrase pattern types and pat-
tern instances
</tableCaption>
<bodyText confidence="0.998811739130435">
paraphrases. (For example, if Mechanical Turk par-
ticipants named “bar that sells wine” twice and “bar
that serves wine” once, the total frequency of “X
that VBZ Y ” would be three).
Paraphrase candidates are then constructed by
combining any paraphrase templates from a simi-
larity neighbour with any fillers matching the given
part-of-speech tag. The list of all candidates is cut
down to a shortlist of 512 paraphrase candidates.
These are subsequently ranked by assigning features
to each of the candidate paraphrases and scoring
them using weights learned in a maximum ranker
by optimizing a loss derived from the probability of
all candidates that have been mentioned at least two
times in the training set in proportion to the probabil-
ity of all candidates that are not part of the training
annotation for that compound at all. (Paraphrases
that were named only once are not used for the pa-
rameter estimation).
After scoring, determiners are removed from the
paraphrase string and duplicates are removed from
the list. The generated list is cut off to yield at most
60 items.
</bodyText>
<subsectionHeader confidence="0.994053">
2.1 Data Sources
</subsectionHeader>
<bodyText confidence="0.999904142857143">
As sources of evidence in the fit (or lack thereof)
of a given verb (as a suspected template filler) with
the two target words of a compounds, we use data
derived from the fifth revision of the English Giga-
word1, tokenized, tagged and parsed with the RASP
parsing toolchain (Briscoe et al., 2006), and from
Google’s web n-gram dataset2.
</bodyText>
<footnote confidence="0.9914642">
1Robert Parker, David Graff, Junbo Kong, Ke Chen and
Kazuaki Maeda (2011): English Gigaword Fifth Edition.
LDC2011T07, Linguistic Data Consortium, Philadelphia.
2Thorsten Brants, Alex Franz (2006): Web 1T 5-gram Ver-
sion 1. LDC2006T13, Linguistic Data Consortium, Philadel-
</footnote>
<page confidence="0.997283">
149
</page>
<bodyText confidence="0.999959222222222">
To reproduce very general estimates of linguis-
tic plausibility, we built a four-gram language model
based on the combined text of the English Gigaword
and the British National Corpus (Burnard, 1995),
using the KenLM toolkit (Heafield, 2011). On the
one hand, free paraphrases are quite unrestricted,
which means that the language model helps also in
the case of more exotic paraphrases such as (1d)
in the first section. On the other hand, many of
the more specialized aspects of plausibility such as
preposition attachment or selectional preferences for
subjects and direct objects can be cast as modeling
(smoothed) probabilities for a certain class of short
surface strings, for which an n-gram model is a use-
ful first approximation.
Using the grammatical relations extracted by the
RASP toolkit, we created a database of plausible
verb-subject and verb-object combinations, defined
as having a positive pointwise mutual information
score.
In a similar fashion, we used a list of verbs and
the morphg morphological realizer (Minnen et al.,
2001) to extract all occurrences of the patterns “N
PREP N”, “N PREP (DET) N” for noun-preposition-
noun combinations, and “N that VBZ” as well as “N
VBN by” for finding typical cases of an active or pas-
sive verb that modifies a given noun.
</bodyText>
<subsectionHeader confidence="0.998485">
2.2 Ranking features
</subsectionHeader>
<bodyText confidence="0.998033666666667">
The following properties used to score each para-
phrase candidate (using weights learned by the Max-
Ent ranker):
</bodyText>
<listItem confidence="0.967983">
• language model score lm
</listItem>
<bodyText confidence="0.906467">
The score assigned by the 4-gram model
learned on the English Gigaword and the BNC.
</bodyText>
<listItem confidence="0.905384">
• pattern type tp=type
</listItem>
<bodyText confidence="0.9998976">
The pattern type (usually the first two ‘interest-
ing’ tokens from the paraphrase template, i.e.,
filtering out determiners and auxiliaries). A list
of the most frequent pattern types can be found
in Table 1.
</bodyText>
<listItem confidence="0.853077">
• pattern weight pat
</listItem>
<bodyText confidence="0.91762125">
The pattern weight as the sum of the (neighbour
similarity times number of occurrences) contri-
bution from each pattern template.
phia.
</bodyText>
<listItem confidence="0.922126">
• linking preposition prep prep=type
</listItem>
<bodyText confidence="0.971874142857143">
This feature correlates occurring prepositions
(prep) to types of patterns, with the goal
of learning high feature weights for preposi-
tion/type combinations that fit well together.
The obvious example for this would be, e.g.,
that the of preposition pattern fits well with
Y of X paraphrases.
</bodyText>
<listItem confidence="0.97895">
• absent preposition noprep=type
</listItem>
<bodyText confidence="0.5755605">
This feature is set when no X prep Y or similar
pattern could be found.
</bodyText>
<listItem confidence="0.785624333333333">
• subject preference (VBG, VBZ)
subj subj0, subj n that vbz
object preference (VBN)
</listItem>
<bodyText confidence="0.97022">
obj dobj0, obj n vbn by
In cases of verbal paraphrases where the com-
pound head is the subject, we can directly
check for corpus evidence for the correspond-
ing subject-verb pattern. A similar check is
done for verb-object (or verb-patient) patterns
in the paraphrases that involve the head in a
passive construction.
</bodyText>
<listItem confidence="0.9778215">
• frequent/infrequent subject verb (VBG, VBZ)
subj verb, subj infrequent
</listItem>
<bodyText confidence="0.9997282">
Some verbs (belong, come, concern, consist,
contain, deal, give, have, involve, make, pro-
vide, regard, run, sell, show, use, work) oc-
cur frequent enough that we want to introduce
a (data-induced) bias towards or away from
them. Other verbs, which are more rare, are
treated as a single class in this regard (which
means that their goodness of fit is mostly rep-
resented through the language model and the
selectional preference models).
</bodyText>
<listItem confidence="0.670388">
• frequent/infrequent object verb (VBN)
</listItem>
<bodyText confidence="0.998894833333333">
a similar distinction is made for a list of
verbs that often occur in passive form (ap-
pointed, associated, based, carried, caused,
conducted, designed, found, given, held, kept,
meant, needed, performed, placed, prepared,
produced, provided, related, taken)
</bodyText>
<listItem confidence="0.935691666666667">
• co-occurrence of filler with X (other patterns)
other POS cooc, other POS none
For pattern types where we cannot use one of
</listItem>
<page confidence="0.993382">
150
</page>
<table confidence="0.999700166666667">
System isomorphic non-isom.
SFS 0.2313 0.1795
IIITH 0.2309 0.2584
MELODII 0.1300 0.5485
MELODIII 0.1358 0.5360
of+for baseline 0.0472 0.8294
</table>
<tableCaption confidence="0.999151">
Table 2: Official evaluation results + simple baseline
</tableCaption>
<bodyText confidence="0.99966725">
the selectional preference models, we use a
model akin to Pado&amp;Lapata’s (2007) syntax-
based model that provides association scores
based on syntactic dependency arc distance.
</bodyText>
<sectionHeader confidence="0.988359" genericHeader="keywords">
3 Evaluation Results
</sectionHeader>
<bodyText confidence="0.981861666666667">
The official evaluation results for the task are sum-
marized in Table 2. Two evaluation scores were
used:
</bodyText>
<listItem confidence="0.999500166666667">
• Isomorphic scoring maps system paraphrases
to (unmapped) paraphrases from the reference
dataset, and requires systems to produce the
full set of paraphrases gathered from Mechani-
cal Turk workers in order to get a perfect score.
• Nonisomorphic scoring scores each system
</listItem>
<bodyText confidence="0.942572391304348">
paraphrase with respect to the best match from
the reference dataset, and averages these scores
over all system paraphrases. A system that
performs well in nonisomorphic scoring does
not need to produce all paraphrases, but will
get punished for producing non-reliable para-
phrases.
As apparent from the table, systems either score well
on the isomorphic score (producing a large number
of paraphrases in order to get good coverage of the
range of expressions in the reference) or on the non-
isomorphic score (producing a smaller number of
paraphrases that are highly ranked in the reference).
The difference is also apparent in the case of a hy-
pothetical system that produces “Y for X” and and
“Y of X” as the paraphrase for any compound (e.g.
bar for wine and bar of wine for wine bar). Because
these paraphrases occur quite often as most frequent
responses, this would yield a high non-isomorphic
score, but an isomorphic score that is very low.
During system development, the relative quality
of system paraphrases for each compound was es-
timated using Maximum Average Precision (MAP)
</bodyText>
<table confidence="0.972393727272727">
Compound closest neighbour MAP R„La.
share holding withdrawal line 1.000 0.800
union power community life 1.000 0.750
truth value accounting treatment 1.000 0.750
amateur championship computer study 1.000 0.750
government authority unit manager 1.000 0.680
wine bar computer industry 0.000 0.040
mammoth task consumer benefit 0.000 0.040
obstacle course work area 0.000 0.040
operating system telephone system 0.000 0.000
deadweight burden divorce rate 0.000 0.000
</table>
<tableCaption confidence="0.9862655">
Table 3: Best and worst compounds in cross-validation
on the training data
</tableCaption>
<bodyText confidence="0.999958565217391">
and the total achievable recall (R���) of the gen-
erated paraphrase list. Table 3 shows the MAP
score (for paraphrases that were listed at least two
times) and achievable recall (for all paraphrases).
These measures, unlike the official scores, do not
attempt to deal with paraphrase variants (e.g. dif-
ferent prepositions for a verbal paraphrase), but are
robust and simple enough to give an impression of
the quality of the system response.
As can be seen by looking at the achievable re-
call figures, it is not always the case that all refer-
ence paraphrases are in the list that is ranked by the
MaxEnt model. In the lower half of table 3, we see
that for these cases, the most-similar item selected
by the WordNet-based similarity measure is not very
close semantically; whether this is the only influ-
encing factor remains to be seen since some of the
best-ranked items in the upper half are also abstract
concepts with only-somewhat-close neighbours. Fu-
ture work would therefore have to cover both im-
provements to the similarity measure itself and to the
ranking mechanism used for the reranking of gener-
ated paraphrases.
</bodyText>
<sectionHeader confidence="0.997491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996755666666667">
The author’s work was funded as part of SFB
833 (“Constitution of Meaning”) by the Deutsche
Forschungsgemeinschaft (DFG).
</bodyText>
<sectionHeader confidence="0.998168" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.947211">
Bird, S., Loper, E., and Klein, E. (2009). Natural
Language Processing with Python. O’Reilly Me-
dia Inc.
</reference>
<page confidence="0.991304">
151
</page>
<reference confidence="0.980114855072464">
Briscoe, E., Carroll, J., and Watson, R. (2006). The
second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions.
Burnard, L., editor (1995). Users Reference Guide
British National Corpus Version 1.0. Oxford Uni-
versity Computing Service.
Butnariu, C., Kim, S. N., Nakov, P., Seaghdha,
D. O., Spakowicz, S., and Veale, T. (2009).
SemEval-2010 task 9: The interpretation of noun
compounds using paraphrasing verbs and prepo-
sition. In Proceedings of the NAACL HLT Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions.
Finin, T. W. (1980). The semantic interpretation of
compound nominals. Report T-96, University of
Illinois, Coordinated Science Laboratory.
Heafield, K. (2011). KenLM: Faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation.
Hendrickx, I., Kozareva, Z., Nakov, P., S´eaghdha,
D. O., Szpakowicz, S., and Veale, T. (2013).
SemEval-2013 task 4: Free paraphrases of noun
compounds. In Proceedings of the International
Workshop on Semantic Evaluation, SemEval ’13.
Lauer, M. (1995). Corpus statistics meet the noun
compound: some empirical results. In Proceed-
ings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 1995).
Li, G., Lopez-Fernandez, A., and Veale, T. (2010).
Ucd-goggle: A hybrid system for noun compound
paraphrasing. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation.
Lin, D. (1998). An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning.
Minnen, G., Caroll, J., and Pearce, D. (2001). Ap-
plied morphological processing of English. Natu-
ral Language Engineering, 7(3):207–223.
Nakov, P. (2008). Noun compound interpretation
using paraphrasing verbs: Feasibility study. In
Dochev, D., Pistore, M., and Traverso, P., ed-
itors, Artificial Intelligence: Methodology, Sys-
tems, and Applications, volume 5253 of Lec-
ture Notes in Computer Science, pages 103–117.
Springer Berlin Heidelberg.
Nulty, P. and Costello, F. (2010). Ucd-pn: Selecting
general paraphrases using conditional probability.
In Proceedings of the 5th International Workshop
on Semantic Evaluation.
Pad´o, S. and Lapata, M. (2007). Dependency-based
construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.
Toutanova, K., Klein, D., Manning, C. D., and
Singer, Y. (2003). Feature-rich part-of-speech
tagging with a cyclic dependency network. In
Proc. NAACL 2003, pages 252–259.
Wijaya, D. T. and Gianfortoni, P. (2011). ”nut
case: what does it mean?”: understanding se-
mantic relationship between nouns in noun com-
pounds through paraphrasing and ranking the
paraphrases. In Proceedings of the 1st inter-
national workshop on Search and mining entity-
relationship data, SMER ’11, pages 9–14.
Wubben, S. (2010). Uvt: Memory-based pairwise
ranking of paraphrasing verbs. In Proceedings of
the 5th International Workshop on Semantic Eval-
uation.
</reference>
<page confidence="0.997929">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239585">
<title confidence="0.998716">SFS-TUE: Compound Paraphrasing with a Language Model and Discriminative Reranking</title>
<author confidence="0.685514">Yannick</author>
<affiliation confidence="0.773874">SfS / SFB University of</affiliation>
<email confidence="0.937677">versley@sfs.uni-tuebingen.de</email>
<abstract confidence="0.996503916666667">This paper presents an approach for generating free paraphrases of compounds (task 4 at SemEval 2013) by decomposing the training data into a collection of templates and fillers and recombining/scoring these based on a generative language model and discriminative MaxEnt reranking. The system described in this paper achieved the highest score (with a very small margin) in (default) of the scorer, for which it was optimized, at a disadvantage</abstract>
<intro confidence="0.507121">the</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>E Loper</author>
<author>E Klein</author>
</authors>
<title>Natural Language Processing with Python.</title>
<date>2009</date>
<publisher>O’Reilly Media Inc.</publisher>
<contexts>
<context position="4017" citStr="Bird et al., 2009" startWordPosition="643" endWordPosition="646">a set of building blocks (patterns and fillers) from these compounds, recombining these building blocks, and finally ranking the list of potential paraphrases. The final list is postprocessed by keeping only one variant of each set of paraphrases that only differ in a determiner (e.g., ‘strike from air’ and ‘strike from the air’) in order to make a 1:1 mapping between system response and gold standard possible. As a first step, the system retrieves the most similar compounds from the training data. This is achieved Lin’s wordnet similarity measure (Lin, 1998) using the implementation in NLTK (Bird et al., 2009). The similarity of two compounds X1Y1 and X2Y2 is calculated as sC = min(sim(X1, X2), sim(Y1, Y2)) + 0.1 · (sim(X1, X2) + sim(Y1, Y2)) which represents a compromise between requiring that both modifier and head are approximately similar, and still giving a small boost to pairs that have very high modifier similarity but low head similarity, or vice versa. For training, the target compound is excluded from the most-similar compounds list so that candidate construction is only based on actual neighbours. The paraphrases for the most similar compound entries (such as 2a) are broken down into tem</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Bird, S., Loper, E., and Klein, E. (2009). Natural Language Processing with Python. O’Reilly Media Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL</booktitle>
<contexts>
<context position="6957" citStr="Briscoe et al., 2006" startWordPosition="1171" endWordPosition="1174">t part of the training annotation for that compound at all. (Paraphrases that were named only once are not used for the parameter estimation). After scoring, determiners are removed from the paraphrase string and duplicates are removed from the list. The generated list is cut off to yield at most 60 items. 2.1 Data Sources As sources of evidence in the fit (or lack thereof) of a given verb (as a suspected template filler) with the two target words of a compounds, we use data derived from the fifth revision of the English Gigaword1, tokenized, tagged and parsed with the RASP parsing toolchain (Briscoe et al., 2006), and from Google’s web n-gram dataset2. 1Robert Parker, David Graff, Junbo Kong, Ke Chen and Kazuaki Maeda (2011): English Gigaword Fifth Edition. LDC2011T07, Linguistic Data Consortium, Philadelphia. 2Thorsten Brants, Alex Franz (2006): Web 1T 5-gram Version 1. LDC2006T13, Linguistic Data Consortium, Philadel149 To reproduce very general estimates of linguistic plausibility, we built a four-gram language model based on the combined text of the English Gigaword and the British National Corpus (Burnard, 1995), using the KenLM toolkit (Heafield, 2011). On the one hand, free paraphrases are quit</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Briscoe, E., Carroll, J., and Watson, R. (2006). The second release of the RASP system. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burnard</author>
</authors>
<date>1995</date>
<booktitle>Users Reference Guide British National Corpus Version 1.0.</booktitle>
<institution>Oxford University Computing Service.</institution>
<contexts>
<context position="7471" citStr="Burnard, 1995" startWordPosition="1248" endWordPosition="1249">glish Gigaword1, tokenized, tagged and parsed with the RASP parsing toolchain (Briscoe et al., 2006), and from Google’s web n-gram dataset2. 1Robert Parker, David Graff, Junbo Kong, Ke Chen and Kazuaki Maeda (2011): English Gigaword Fifth Edition. LDC2011T07, Linguistic Data Consortium, Philadelphia. 2Thorsten Brants, Alex Franz (2006): Web 1T 5-gram Version 1. LDC2006T13, Linguistic Data Consortium, Philadel149 To reproduce very general estimates of linguistic plausibility, we built a four-gram language model based on the combined text of the English Gigaword and the British National Corpus (Burnard, 1995), using the KenLM toolkit (Heafield, 2011). On the one hand, free paraphrases are quite unrestricted, which means that the language model helps also in the case of more exotic paraphrases such as (1d) in the first section. On the other hand, many of the more specialized aspects of plausibility such as preposition attachment or selectional preferences for subjects and direct objects can be cast as modeling (smoothed) probabilities for a certain class of short surface strings, for which an n-gram model is a useful first approximation. Using the grammatical relations extracted by the RASP toolkit</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Burnard, L., editor (1995). Users Reference Guide British National Corpus Version 1.0. Oxford University Computing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Butnariu</author>
<author>S N Kim</author>
<author>P Nakov</author>
<author>D O Seaghdha</author>
<author>S Spakowicz</author>
<author>T Veale</author>
</authors>
<title>SemEval-2010 task 9: The interpretation of noun compounds using paraphrasing verbs and preposition.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions.</booktitle>
<contexts>
<context position="1478" citStr="Butnariu et al., 2009" startWordPosition="227" endWordPosition="231">biguous as to the type of relation and usually nonambiguous as to the concepts it relates (namely, those of the two nouns). Besides inventory-based approaches, where the relation is classified into a fixed number of relations, many researchers have argued that the full variability of the semantic relations inherent in compounds is best captured with paraphrases: Lauer (1995) proposes to use a preposition as a proxy for the meaning of a compound. Finin (1980) and later Nakov (2008) and others propose less restrictive schemes based on paraphrasing verbs. A previous SemEval task (task 9 in 2010; Butnariu et al., 2009). The most successsful approaches for this task such as Nulty and Costello (2010), Li et al. (2010), and Wubben (2010), or the subsequent approach of Wijaya and Gianfortoni (2011), all make efficient use of both the training data and general evidence from WordNet or statistics derived from large corpora. The paper of Li et al. mentions that solely inducing a global ranking of paraphrasing verbs from the training data (looking which verb is ranked higher in those cases where both were considered for the same compound) yielded higher scores than an unsupervised approach based on the semantic res</context>
</contexts>
<marker>Butnariu, Kim, Nakov, Seaghdha, Spakowicz, Veale, 2009</marker>
<rawString>Butnariu, C., Kim, S. N., Nakov, P., Seaghdha, D. O., Spakowicz, S., and Veale, T. (2009). SemEval-2010 task 9: The interpretation of noun compounds using paraphrasing verbs and preposition. In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W Finin</author>
</authors>
<title>The semantic interpretation of compound nominals.</title>
<date>1980</date>
<tech>Report T-96,</tech>
<institution>University of Illinois, Coordinated Science Laboratory.</institution>
<contexts>
<context position="1318" citStr="Finin (1980)" startWordPosition="203" endWordPosition="204">eresting phenomenon in natural language semantics as they normally realize a semantic relation (between head and modifier noun) that is both highly ambiguous as to the type of relation and usually nonambiguous as to the concepts it relates (namely, those of the two nouns). Besides inventory-based approaches, where the relation is classified into a fixed number of relations, many researchers have argued that the full variability of the semantic relations inherent in compounds is best captured with paraphrases: Lauer (1995) proposes to use a preposition as a proxy for the meaning of a compound. Finin (1980) and later Nakov (2008) and others propose less restrictive schemes based on paraphrasing verbs. A previous SemEval task (task 9 in 2010; Butnariu et al., 2009). The most successsful approaches for this task such as Nulty and Costello (2010), Li et al. (2010), and Wubben (2010), or the subsequent approach of Wijaya and Gianfortoni (2011), all make efficient use of both the training data and general evidence from WordNet or statistics derived from large corpora. The paper of Li et al. mentions that solely inducing a global ranking of paraphrasing verbs from the training data (looking which verb</context>
</contexts>
<marker>Finin, 1980</marker>
<rawString>Finin, T. W. (1980). The semantic interpretation of compound nominals. Report T-96, University of Illinois, Coordinated Science Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="7513" citStr="Heafield, 2011" startWordPosition="1254" endWordPosition="1255">rsed with the RASP parsing toolchain (Briscoe et al., 2006), and from Google’s web n-gram dataset2. 1Robert Parker, David Graff, Junbo Kong, Ke Chen and Kazuaki Maeda (2011): English Gigaword Fifth Edition. LDC2011T07, Linguistic Data Consortium, Philadelphia. 2Thorsten Brants, Alex Franz (2006): Web 1T 5-gram Version 1. LDC2006T13, Linguistic Data Consortium, Philadel149 To reproduce very general estimates of linguistic plausibility, we built a four-gram language model based on the combined text of the English Gigaword and the British National Corpus (Burnard, 1995), using the KenLM toolkit (Heafield, 2011). On the one hand, free paraphrases are quite unrestricted, which means that the language model helps also in the case of more exotic paraphrases such as (1d) in the first section. On the other hand, many of the more specialized aspects of plausibility such as preposition attachment or selectional preferences for subjects and direct objects can be cast as modeling (smoothed) probabilities for a certain class of short surface strings, for which an n-gram model is a useful first approximation. Using the grammatical relations extracted by the RASP toolkit, we created a database of plausible verb-</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Heafield, K. (2011). KenLM: Faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Hendrickx</author>
<author>Z Kozareva</author>
<author>P Nakov</author>
<author>D O S´eaghdha</author>
<author>S Szpakowicz</author>
<author>T Veale</author>
</authors>
<title>SemEval-2013 task 4: Free paraphrases of noun compounds.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13.</booktitle>
<marker>Hendrickx, Kozareva, Nakov, S´eaghdha, Szpakowicz, Veale, 2013</marker>
<rawString>Hendrickx, I., Kozareva, Z., Nakov, P., S´eaghdha, D. O., Szpakowicz, S., and Veale, T. (2013). SemEval-2013 task 4: Free paraphrases of noun compounds. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: some empirical results.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1233" citStr="Lauer (1995)" startWordPosition="186" endWordPosition="187">d, at a disadvantage to the non-isomorphic score. 1 Introduction Compounds are an interesting phenomenon in natural language semantics as they normally realize a semantic relation (between head and modifier noun) that is both highly ambiguous as to the type of relation and usually nonambiguous as to the concepts it relates (namely, those of the two nouns). Besides inventory-based approaches, where the relation is classified into a fixed number of relations, many researchers have argued that the full variability of the semantic relations inherent in compounds is best captured with paraphrases: Lauer (1995) proposes to use a preposition as a proxy for the meaning of a compound. Finin (1980) and later Nakov (2008) and others propose less restrictive schemes based on paraphrasing verbs. A previous SemEval task (task 9 in 2010; Butnariu et al., 2009). The most successsful approaches for this task such as Nulty and Costello (2010), Li et al. (2010), and Wubben (2010), or the subsequent approach of Wijaya and Gianfortoni (2011), all make efficient use of both the training data and general evidence from WordNet or statistics derived from large corpora. The paper of Li et al. mentions that solely induc</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Lauer, M. (1995). Corpus statistics meet the noun compound: some empirical results. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL 1995).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Li</author>
<author>A Lopez-Fernandez</author>
<author>T Veale</author>
</authors>
<title>Ucd-goggle: A hybrid system for noun compound paraphrasing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="1577" citStr="Li et al. (2010)" startWordPosition="245" endWordPosition="248">of the two nouns). Besides inventory-based approaches, where the relation is classified into a fixed number of relations, many researchers have argued that the full variability of the semantic relations inherent in compounds is best captured with paraphrases: Lauer (1995) proposes to use a preposition as a proxy for the meaning of a compound. Finin (1980) and later Nakov (2008) and others propose less restrictive schemes based on paraphrasing verbs. A previous SemEval task (task 9 in 2010; Butnariu et al., 2009). The most successsful approaches for this task such as Nulty and Costello (2010), Li et al. (2010), and Wubben (2010), or the subsequent approach of Wijaya and Gianfortoni (2011), all make efficient use of both the training data and general evidence from WordNet or statistics derived from large corpora. The paper of Li et al. mentions that solely inducing a global ranking of paraphrasing verbs from the training data (looking which verb is ranked higher in those cases where both were considered for the same compound) yielded higher scores than an unsupervised approach based on the semantic resources, underlining the need to combine training data and resources efficiently. SemEval 2013 task </context>
</contexts>
<marker>Li, Lopez-Fernandez, Veale, 2010</marker>
<rawString>Li, G., Lopez-Fernandez, A., and Veale, T. (2010). Ucd-goggle: A hybrid system for noun compound paraphrasing. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3964" citStr="Lin, 1998" startWordPosition="636" endWordPosition="637">number of similar compounds, then extracting a set of building blocks (patterns and fillers) from these compounds, recombining these building blocks, and finally ranking the list of potential paraphrases. The final list is postprocessed by keeping only one variant of each set of paraphrases that only differ in a determiner (e.g., ‘strike from air’ and ‘strike from the air’) in order to make a 1:1 mapping between system response and gold standard possible. As a first step, the system retrieves the most similar compounds from the training data. This is achieved Lin’s wordnet similarity measure (Lin, 1998) using the implementation in NLTK (Bird et al., 2009). The similarity of two compounds X1Y1 and X2Y2 is calculated as sC = min(sim(X1, X2), sim(Y1, Y2)) + 0.1 · (sim(X1, X2) + sim(Y1, Y2)) which represents a compromise between requiring that both modifier and head are approximately similar, and still giving a small boost to pairs that have very high modifier similarity but low head similarity, or vice versa. For training, the target compound is excluded from the most-similar compounds list so that candidate construction is only based on actual neighbours. The paraphrases for the most similar c</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Caroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="8321" citStr="Minnen et al., 2001" startWordPosition="1380" endWordPosition="1383">other hand, many of the more specialized aspects of plausibility such as preposition attachment or selectional preferences for subjects and direct objects can be cast as modeling (smoothed) probabilities for a certain class of short surface strings, for which an n-gram model is a useful first approximation. Using the grammatical relations extracted by the RASP toolkit, we created a database of plausible verb-subject and verb-object combinations, defined as having a positive pointwise mutual information score. In a similar fashion, we used a list of verbs and the morphg morphological realizer (Minnen et al., 2001) to extract all occurrences of the patterns “N PREP N”, “N PREP (DET) N” for noun-prepositionnoun combinations, and “N that VBZ” as well as “N VBN by” for finding typical cases of an active or passive verb that modifies a given noun. 2.2 Ranking features The following properties used to score each paraphrase candidate (using weights learned by the MaxEnt ranker): • language model score lm The score assigned by the 4-gram model learned on the English Gigaword and the BNC. • pattern type tp=type The pattern type (usually the first two ‘interesting’ tokens from the paraphrase template, i.e., filt</context>
</contexts>
<marker>Minnen, Caroll, Pearce, 2001</marker>
<rawString>Minnen, G., Caroll, J., and Pearce, D. (2001). Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
</authors>
<title>Noun compound interpretation using paraphrasing verbs: Feasibility study.</title>
<date>2008</date>
<journal>Artificial Intelligence: Methodology, Systems, and Applications,</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>5253</volume>
<pages>103--117</pages>
<editor>In Dochev, D., Pistore, M., and Traverso, P., editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="1341" citStr="Nakov (2008)" startWordPosition="207" endWordPosition="208">natural language semantics as they normally realize a semantic relation (between head and modifier noun) that is both highly ambiguous as to the type of relation and usually nonambiguous as to the concepts it relates (namely, those of the two nouns). Besides inventory-based approaches, where the relation is classified into a fixed number of relations, many researchers have argued that the full variability of the semantic relations inherent in compounds is best captured with paraphrases: Lauer (1995) proposes to use a preposition as a proxy for the meaning of a compound. Finin (1980) and later Nakov (2008) and others propose less restrictive schemes based on paraphrasing verbs. A previous SemEval task (task 9 in 2010; Butnariu et al., 2009). The most successsful approaches for this task such as Nulty and Costello (2010), Li et al. (2010), and Wubben (2010), or the subsequent approach of Wijaya and Gianfortoni (2011), all make efficient use of both the training data and general evidence from WordNet or statistics derived from large corpora. The paper of Li et al. mentions that solely inducing a global ranking of paraphrasing verbs from the training data (looking which verb is ranked higher in th</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Nakov, P. (2008). Noun compound interpretation using paraphrasing verbs: Feasibility study. In Dochev, D., Pistore, M., and Traverso, P., editors, Artificial Intelligence: Methodology, Systems, and Applications, volume 5253 of Lecture Notes in Computer Science, pages 103–117. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nulty</author>
<author>F Costello</author>
</authors>
<title>Ucd-pn: Selecting general paraphrases using conditional probability.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="1559" citStr="Nulty and Costello (2010)" startWordPosition="241" endWordPosition="244"> it relates (namely, those of the two nouns). Besides inventory-based approaches, where the relation is classified into a fixed number of relations, many researchers have argued that the full variability of the semantic relations inherent in compounds is best captured with paraphrases: Lauer (1995) proposes to use a preposition as a proxy for the meaning of a compound. Finin (1980) and later Nakov (2008) and others propose less restrictive schemes based on paraphrasing verbs. A previous SemEval task (task 9 in 2010; Butnariu et al., 2009). The most successsful approaches for this task such as Nulty and Costello (2010), Li et al. (2010), and Wubben (2010), or the subsequent approach of Wijaya and Gianfortoni (2011), all make efficient use of both the training data and general evidence from WordNet or statistics derived from large corpora. The paper of Li et al. mentions that solely inducing a global ranking of paraphrasing verbs from the training data (looking which verb is ranked higher in those cases where both were considered for the same compound) yielded higher scores than an unsupervised approach based on the semantic resources, underlining the need to combine training data and resources efficiently. </context>
</contexts>
<marker>Nulty, Costello, 2010</marker>
<rawString>Nulty, P. and Costello, F. (2010). Ucd-pn: Selecting general paraphrases using conditional probability. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Pad´o, S. and Lapata, M. (2007). Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="5013" citStr="Toutanova et al., 2003" startWordPosition="808" endWordPosition="811">the target compound is excluded from the most-similar compounds list so that candidate construction is only based on actual neighbours. The paraphrases for the most similar compound entries (such as 2a) are broken down into templates (2b) and fillers (2c), by replacing modifier and head by X and Y , respectively, and other content words by their part-of-speech tag. (2) a. bar that serves wine b. X that VBZ Y c. VBZ:serve Conversely, template fillers consist of all the extracted content words, categorized by their part-ofspeech. (Part-of-speech tags were assigned using the Stanford POS tagger: Toutanova et al., 2003). Both paraphrase templates and template fillers are weighted by the product of the similarity value sC between the target compound and the neighbour, and the total frequency of occurrence in that neighbour’s type examples Y of Y of X (159) / Y of the X (59) / Y of a X (47) Y for Y for X (114) / Y for the X (33) Y VBZ Y that VBZ X (91)/ Y which VBZ X (45) Y VBG Y VBG X (90) / Y VBG the X/ Y VBG with X Y VBN Y VBN for X (82) / Y VBN by X (52) Y in Y in X (31) Y on Y on X (38) Table 1: Most frequent paraphrase pattern types and pattern instances paraphrases. (For example, if Mechanical Turk part</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. NAACL 2003, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D T Wijaya</author>
<author>P Gianfortoni</author>
</authors>
<title>nut case: what does it mean?”: understanding semantic relationship between nouns in noun compounds through paraphrasing and ranking the paraphrases.</title>
<date>2011</date>
<booktitle>In Proceedings of the 1st international workshop on Search and mining entityrelationship data, SMER ’11,</booktitle>
<pages>9--14</pages>
<contexts>
<context position="1657" citStr="Wijaya and Gianfortoni (2011)" startWordPosition="258" endWordPosition="261">lation is classified into a fixed number of relations, many researchers have argued that the full variability of the semantic relations inherent in compounds is best captured with paraphrases: Lauer (1995) proposes to use a preposition as a proxy for the meaning of a compound. Finin (1980) and later Nakov (2008) and others propose less restrictive schemes based on paraphrasing verbs. A previous SemEval task (task 9 in 2010; Butnariu et al., 2009). The most successsful approaches for this task such as Nulty and Costello (2010), Li et al. (2010), and Wubben (2010), or the subsequent approach of Wijaya and Gianfortoni (2011), all make efficient use of both the training data and general evidence from WordNet or statistics derived from large corpora. The paper of Li et al. mentions that solely inducing a global ranking of paraphrasing verbs from the training data (looking which verb is ranked higher in those cases where both were considered for the same compound) yielded higher scores than an unsupervised approach based on the semantic resources, underlining the need to combine training data and resources efficiently. SemEval 2013 task 4 The present task on providing free paraphrases for noun compounds (Hendrickx e</context>
</contexts>
<marker>Wijaya, Gianfortoni, 2011</marker>
<rawString>Wijaya, D. T. and Gianfortoni, P. (2011). ”nut case: what does it mean?”: understanding semantic relationship between nouns in noun compounds through paraphrasing and ranking the paraphrases. In Proceedings of the 1st international workshop on Search and mining entityrelationship data, SMER ’11, pages 9–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wubben</author>
</authors>
<title>Uvt: Memory-based pairwise ranking of paraphrasing verbs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="1596" citStr="Wubben (2010)" startWordPosition="250" endWordPosition="251">ides inventory-based approaches, where the relation is classified into a fixed number of relations, many researchers have argued that the full variability of the semantic relations inherent in compounds is best captured with paraphrases: Lauer (1995) proposes to use a preposition as a proxy for the meaning of a compound. Finin (1980) and later Nakov (2008) and others propose less restrictive schemes based on paraphrasing verbs. A previous SemEval task (task 9 in 2010; Butnariu et al., 2009). The most successsful approaches for this task such as Nulty and Costello (2010), Li et al. (2010), and Wubben (2010), or the subsequent approach of Wijaya and Gianfortoni (2011), all make efficient use of both the training data and general evidence from WordNet or statistics derived from large corpora. The paper of Li et al. mentions that solely inducing a global ranking of paraphrasing verbs from the training data (looking which verb is ranked higher in those cases where both were considered for the same compound) yielded higher scores than an unsupervised approach based on the semantic resources, underlining the need to combine training data and resources efficiently. SemEval 2013 task 4 The present task </context>
</contexts>
<marker>Wubben, 2010</marker>
<rawString>Wubben, S. (2010). Uvt: Memory-based pairwise ranking of paraphrasing verbs. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>