<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.118523">
<figure confidence="0.991038909090909">
question
Answer
Type Prediction
Query
expansion
Ranked
Information
Retrieval
Answer
Selection
Answers
TREC
DOC
Database
Raw
Text
Named
Entity
Marked
Text
45
40
base
ngr−only
pos
expcat
expcat−markqw
35
30
error rate (%)
25
20
15
</figure>
<page confidence="0.9613705">
10
5
</page>
<table confidence="0.981211714285714">
0 100 200 300 400 500 600 700 800 900
number of features
Unigrams What year did World War II start
Morphed{P 0 S } what{WP} year{NN} do{VBD} World{NP} War{NP} II{NP} start{NN}
Bigrams what{wp} what{WP}_year{NN} what{WP}_do{VBD} ...
Expanded Hierarchy what{WP} year time_period measure abstraction year{NN} do{VBD} ...
Mark Question Word what_bqw year time_period measure abstraction year{NN} do{VBD} ...
</table>
<tableCaption confidence="0.999925">
Table 1: Data used for generating features for answer classification experiments
</tableCaption>
<bodyText confidence="0.999536125">
A peculiar feature of the architecture is that
improvements in answer type prediction do not
correlate directly with improvements in the
overall score of the system. The reason is that
parallel improvements must be made in the
named entity marking as well as in answer se-
lection in order to realize them in the overall
system.
</bodyText>
<sectionHeader confidence="0.997985" genericHeader="method">
3 Information Retrieval
</sectionHeader>
<bodyText confidence="0.999696023809524">
The purpose of the information retrieval module
is to search the database of documents to select
passages of text, containing information rele-
vant to the query. The database used in TREC-
9 has 978952 documents from several sources in-
cluding AP Newswire, Wall Street Journal, San
Jose Mercury News, Financial Times, Los An-
geles Times, and the Federal Broadcast Infor-
mation Service (FBIS). The database consists
of approximately 2.8 GB of text, representing
524 million words.
Our information retrieval subsystem uses a
two-pass approach. In the first pass, we
searched an encyclopedia database. The high-
est scoring passages were then used to create
expanded queries, applied in the second pass
scoring of the TREC passages. The data pre-
processing and relevance scoring techniques are
similar to the ones applied in the TREC Ad-
Hoc, SDR and CUR participations (Franz and
Roukos, 1998), (Franz et al., 1999).
Relevance scoring was based on morph uni-
gram and bigram features, extracted from the
text data in the following way: after the ini-
tial filtering, we tokenized the documents using
a statistical tokenizer. The tokenized text was
processed by a statistical part of speech(POS)
tagger (Merialdo, 1990). Based on the spellings
and the POS tags, the morphs were found by
looking up the morph corresponding to a given
word and POS tag in a table, e.g., the word
&amp;quot;running&amp;quot; tagged as verb was converted into
&amp;quot;run&amp;quot;, whereas the same word marked as adjec-
tive was left unchanged. The words not found
in the morph table were kept in their original
form. All the words were case-folded after the
morphological analysis was done. Hyphenated
words were then split into their components.
We used a modified Okapi (Robertson et al.,
1995) formula in the first-pass scoring. Uni-
grams and bigrams in the intersection of the
query and document contributed a score of:
</bodyText>
<equation confidence="0.998118666666667">
t f
s = x idf, (1)
Cl + C2 X avdidl + tf
</equation>
<bodyText confidence="0.9999475">
where t f is the term count for a document, dl is
the document length, avdl is the average length
of the documents in the collection and idf is the
inverse document frequency, computed as:
</bodyText>
<equation confidence="0.971859">
N — n + 0.5)
idf = log( ,
n+0.5
</equation>
<bodyText confidence="0.998564142857143">
where N is the total number of documents in
the corpus and n is the number of documents
containing a given n-gram. In Eq.(1) we used
Cl = 0.5,C2 = 1.5 for unigram scoring and
Cl = 0.05, c2 = 0.05 for the bigrams. The first
pass score was a linear combination of unigram
and bigram scores given by Eq.(1), with the un-
igram scores weight set to 0.8 and bigram scores
weight equal to 0.2.
We computed first-pass relevance scores for
82,277 overlapping passages, each containing
approximately 100 non-stop words, extracted
from 18,910 encyclopedia articles.
Based on the first pass passage ranking,
we constructed expanded queries using the lo-
cal context analysis (LCA) technique (Xu and
Croft, 1996). In the second pass scoring, the ex-
panded queries are used to score 2,632,807 pas-
sages based on the TREC-9 Q&amp;A corpus. The
passages were selected to contain approximately
200 non-stop words.
</bodyText>
<table confidence="0.941924">
MRR
passl, TREC 0.4605
pass2, TREC 0.4824
pass2, encyclopedia 0.5031
</table>
<tableCaption confidence="0.994359">
Table 2: Retrieval results.
</tableCaption>
<bodyText confidence="0.9986408">
Table 2 summarizes the information retrieval
results on the 146 development test set ques-
tions described below. The performance is mea-
sured by the Mean Reciprocal Rank (MRR)
(Voorhees and Tice, 1999) of the highest rank-
ing passage containing the answer string among
the top five passages. The first line of the ta-
ble shows the result of first pass scoring using
the TREC-9 Q&amp;A database. The second line
contains the result obtained with queries ex-
panded using the TREC database. The last line
of the table shows the result corresponding to
the system applied in our official submission,
with queries expanded using the encyclopedia
database.
</bodyText>
<sectionHeader confidence="0.951946" genericHeader="method">
4 Named Entity Annotation
</sectionHeader>
<bodyText confidence="0.999924115384615">
Named entity (NE) annotation is a markup of
the text with the class information. As men-
tioned above, our classes correspond to the
MUG classes due to the availability of training
data for these classes. We used the text corpora
available from the LDC to train the maximum
entropy model.
Windows of +/- 2 words, morphs, part-of-
speech (POS) tags and flags raised by pattern
grammars for DATE, MONEY, CARDINAL,
MEASURE, PERCENT, TIME, DURATION
classes and dictionary hits, along with the two
previous tags are created for each word. The
window for predicting tag(0) is shown in Ta-
ble 3. The window is the useful information
given to the maximum entropy feature genera-
tion system to make features about the tag of
the current word. The (-,+) signs give a clue
to the feature functions about the relative posi-
tion of this data to the word being tagged. Ad-
ditionally, the (-2,-1,+1,+2) give position infor-
mation to the feature functions. Each stream
has a fixed vocabulary and n-grams from this
vocabulary are created to be the features of the
maximum entropy model. The training data is
arranged to indicate a special category for be-
</bodyText>
<table confidence="0.9803108">
words w(-) w(-) I w(0) I w(+) w(+)
Morphs m(-) m(-) I m(0) I m(+) m(+)
POS A(-2) 1)(-1) I I)(0) I I)(1) p(2)
Flags 4-2) 4-1) I f(0) I f(1) f(2)
Tags t(-2).t(-1) t(-1) I
</table>
<tableCaption confidence="0.971214">
Table 3: Features used in the named entity
model for predicting tag(0).
</tableCaption>
<bodyText confidence="0.9938614375">
ginning each named entity, for example Begin-
PERSON, to find the boundaries of the named
entity.
The system explores multiple NE hypotheses
in parallel and keeps only those with high prob-
ability and proceeds with a beam-search algo-
rithm to find the most likely path for the whole
sentence. The performance of the named entity
detector is comparable to the performance cited
in (Borthwick et al., 1998) when training the
maximum entropy algorithm on only annotated
data. We omit the results here in the consid-
eration of space, but note that in the analysis
of the question answering system below, only 4
out of 64 errors are attributed directly to the
named entity marking for the 250 byte system.
</bodyText>
<sectionHeader confidence="0.983281" genericHeader="method">
5 Answer Selection
</sectionHeader>
<bodyText confidence="0.998239444444445">
We receive in this module the question, the class
of the answer that the question seeks and a
ranked set of passages (70) annotated with the
MUG classes. The optimal sentence that an-
swers the question is now sought. The TREC
length constraints of 250 byte and 50 byte are
then applied on the sentence.
The algorithm used in this module is listed
here:
</bodyText>
<listItem confidence="0.994409416666667">
1. Each retrieved passage is split into sen-
tences.
2. A window is formed around each sentence
(window size is 3 sentences)
3. The following distances are computed:
Matching Words, Thesaurus Match, Mis-
Match Words, Dispersion, and Cluster
Words. These are defined below.
4. The location or absence of the desired en-
tities is noted in the score.
5. Each of these distances are weighted, the
sentences ranked and the top 5 sentences
</listItem>
<bodyText confidence="0.99736015">
are then output.
The definition of the various distances are
Matching Words The TFIDF sum of the
number of question words that matched
answer words identically in the morphed
space. (+)
Thesaurus Match The TFIDF sum of the
number of question words that matched an-
swer words using a thesaurus match using
WordNet synonyms (Miller, 1990). (+)
Mis-Match Words The TFIDF sum of the
number of question content words that did
not match in this answer. (-)
Dispersion The number of answer words in
the candidate sentence that occur between
matching question words. (-)
Cluster Words The number of answer words
in the candidate sentence that occurred ad-
jacently in both the question and answer
candidate. (+)
Each distance has a weight applied and the
corresponding sign shown above attached to it.
The score for an answer is the sum of the dis-
tances and the top 5 sentences are then output.
To select the 250 or 50 byte answer chunk
from these sentences, the system identified the
longest mismatched pieces between the answer
string and the question. It then analyzed the
answer and the question to find where the cen-
ter of the match was using a subject-verb-
object assumption of the sentence. The system
then output either the subject or object por-
tion whichever had the least matches with the
question.
Answer selection as done above used ad-hoc
and heuristic distance metrics to seek an an-
swer. Future work by the authors will show how
to treat these distance metrics as features and
to develop a statistical model for answer selec-
tion for an open domain.
</bodyText>
<sectionHeader confidence="0.837691" genericHeader="method">
6 Development Set Analysis
</sectionHeader>
<bodyText confidence="0.9999434">
We wanted to maintain the TREC-9 question
database as a test set, but in order to do some
post-evaluation analysis, we chose a subset of
the questions as a development set for next
year. There were two classes of questions in
</bodyText>
<table confidence="0.999813266666667">
201 203 209 210 217 220 224 231 238 242
245 252 253 259 264 266 273 275 280 286
287 294 297 301 308 315 319 322 329 330
336 341 343 350 352 357 363 364 371 374
378 385 392 393 399 411 412 413 420 434
453 454 456 458 462 469 473 476 483 484
490 495 497 504 506 511 517 518 525 528
532 539 546 550 553 560 561 567 572 574
581 583 588 594 595 602 605 609 616 623
627 630 637 638 644 649 651 658 660 665
671 672 679 682 686 693 700 711 712 713
714 715 716 717 718 719 720 721 722 723
724 725 726 727 728 729 730 731 732 733
734 805 806 807 828 829 830 831 832 833
834 839 840 841 842 843
</table>
<tableCaption confidence="0.9926715">
Table 4: Question numbers chosen for the
TREC-9 development set.
</tableCaption>
<bodyText confidence="0.997749">
the TREC9 test: questions that had only one
phrasing and questions that had more than one
phrasing (rephrased). For example, the follow-
ing questions form a set:
</bodyText>
<listItem confidence="0.999958666666667">
• Original Form:
• Rephrased:
• Rephrased:
</listItem>
<bodyText confidence="0.999397909090909">
We wanted 20% of questions of each class in
the development test. The exact list of ques-
tions we used for our TREC-9 development test
set are shown in Table 4. The variant questions
we chose are shown in italics, and we added ev-
ery seventh question skipping the ones in the
above class to yield the 146 questions. A set of
regular expressions (answer patterns) which de-
tect the presense of the answer in a string was
developed for the set using the judgements file
provided by NIST.
The MRR for the entire system for the 250
byte system and the 50 byte system is shown in
Table 5. The results of our system in the official
evaluation and the development set evaluated
using pattern rules are close in both the 250
and 50 byte numbers. Furthermore, the results
indicate a 2% absolute MRR improvement using
the encyclopedia source to expand the original
questions.
Analysis of the components are shown in Ta-
ble 6. An error is attributed to a component if
</bodyText>
<table confidence="0.9946642">
System TREC9 DEV DEV
results ENCL TREC
expansion expansion
250 byte 0.457 0.437 0.417
50 byte 0.290 0.287 0.266
</table>
<tableCaption confidence="0.936429">
Table 5: MRR for TREC-9 and the chosen de-
velopment set.
</tableCaption>
<table confidence="0.999289">
Component Number of Errors
250 byte 50 byte
Answer Type 5 (3.4%) 7 (4.8%)
IR 19 (13%) 19 (13%)
NE 4 (2.7%) 5 (3.4%)
Answer Selection 36 (24.7%) 52 (35.6%)
System 64(43.8%) 83(56.8%)
</table>
<tableCaption confidence="0.78239">
Table 6: Component error rate for the TREC9
dev set for 250 byte system.
</tableCaption>
<bodyText confidence="0.999978896551724">
it is the first component that caused the failure
working left to right in our system architecture.
This analysis was carried out on the top-5 an-
swer strings. Thus, a failure occurs if there is no
answer produced by the system at all. Fixing
this error though need not correct the final an-
swer as it may invoke an error in a subsequent
component. Answer selection is still seen to be
the major cause of problems in our question an-
swering system.
Another viewpoint is to see the effect of the
system on the IR ranking results. This is shown
below in Figure 3. Finding the 250 bytes from
a passage that is of typical length 2.4K bytes
shows some degradation, but further finding the
50 byte answer has considerable degradation. In
Tables 7 and 8 we show the transition matrix
for the rank from IR passages to the Q&amp;A sys-
tem results . Note that there are significant
transitions between the IR rank and the Q&amp;A
rank, but that inspection of the final result in
Figure 3 shows that overall system performance
is similar to the performance of IR for the 250
byte system and degraded at 50 bytes. In Fig-
ure 3, we plot the number of queries which had
an answer at rank 1..5 and indicate no answer
produced by &gt;5. These results we believe points
to the possibility of making more improvements
in answer selection by reranking the results.
</bodyText>
<table confidence="0.999776333333333">
Q&amp;A rank IR rank Total
1 2 3 4 5 5+
1 29 9 5 3 2 5 53
2 10 2 1 0 0 0 13
3 2 2 1 0 1 0 6
4 1 1 0 1 1 2 6
5 2 1 0 0 1 0 4
5+ 13 7 2 1 1 40 64
Total 57 22 9 5 6 47 146
</table>
<tableCaption confidence="0.9858575">
Table 7: Rank transition matrix, IR ws Q&amp;A,
250 bytes.
</tableCaption>
<table confidence="0.998376888888889">
Q&amp;A rank IR rank Total
1 2 3 4 5 5+
1 20 5 2 1 0 3 31
2 5 2 1 0 0 1 9
3 6 2 1 1 1 0 11
4 3 1 0 0 1 1 6
5 2 1 1 0 1 1 6
5+ 21 11 4 3 3 41 83
Total 57 22 9 5 6 47 146
</table>
<tableCaption confidence="0.9619985">
Table 8: Rank transition matrix, IR ws Q&amp;A,
50 bytes.
</tableCaption>
<sectionHeader confidence="0.999351" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999989636363636">
In the last two years, several efforts at question
answering for open domain (Moldovan and et.
al., 1999; Voorhees and Tice, 1999) and FAQ do-
mains (Burke and et. al., 1997) have appeared.
Our approach at question answering has been to
follow the lead of the other participants in the
TREC evaluation but base our components on
maximum entropy modelling. We believe that
corpus based systems allow technologies to be
compared in a systematic approach, thus fur-
thering the field of question answering.
</bodyText>
<sectionHeader confidence="0.994611" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999961666666667">
We presented above our architecture and a
component-wise evaluation of the system in
the question answering problem. We devel-
oped maximum entropy formulations for both
question/answer classification and named entity
marking. The results presented above indicate
a 2% absolute MRR improvement using the en-
cyclopedia source to expand the original ques-
tions. The transition matrix of the IR to Q&amp;A
</bodyText>
<figure confidence="0.9925925">
90
80
retrieved passage
250 bytes
50 bytes
70
number of queries
60
50
40
30
20
10
0
1 2 3 4 5 &gt;5
position
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.883883">question Type Prediction Query expansion Ranked Information Retrieval Answer Selection Answers TREC DOC Database Raw Text Named Entity Marked</title>
<abstract confidence="0.973583331967214">Text 45 40 base ngr−only pos expcat expcat−markqw 35 30 error rate (%) 25 20 15 10 5 0 100 200 300 400 500 600 700 800 900 number of features Unigrams What year did World War II start Morphed{P 0 S } what{WP} year{NN} do{VBD} World{NP} War{NP} II{NP} start{NN} Bigrams what{wp} what{WP}_year{NN} what{WP}_do{VBD} ... Expanded Hierarchy what{WP} year time_period measure abstraction year{NN} do{VBD} ... Mark Question Word what_bqw year time_period measure abstraction year{NN} do{VBD} ... Table 1: Data used for generating features for answer classification experiments A peculiar feature of the architecture is that improvements in answer type prediction do not correlate directly with improvements in the overall score of the system. The reason is that parallel improvements must be made in the named entity marking as well as in answer selection in order to realize them in the overall system. 3 Information Retrieval The purpose of the information retrieval module is to search the database of documents to select passages of text, containing information relevant to the query. The database used in TREC- 9 has 978952 documents from several sources including AP Newswire, Wall Street Journal, San Jose Mercury News, Financial Times, Los Angeles Times, and the Federal Broadcast Information Service (FBIS). The database consists of approximately 2.8 GB of text, representing 524 million words. Our information retrieval subsystem uses a two-pass approach. In the first pass, searched an encyclopedia database. The highest scoring passages were then used to create expanded queries, applied in the second pass scoring of the TREC passages. The data preprocessing and relevance scoring techniques are similar to the ones applied in the TREC Ad- Hoc, SDR and CUR participations (Franz and Roukos, 1998), (Franz et al., 1999). Relevance scoring was based on morph unigram and bigram features, extracted from the text data in the following way: after the initial filtering, we tokenized the documents using a statistical tokenizer. The tokenized text was processed by a statistical part of speech(POS) tagger (Merialdo, 1990). Based on the spellings and the POS tags, the morphs were found by looking up the morph corresponding to a given word and POS tag in a table, e.g., the word &amp;quot;running&amp;quot; tagged as verb was converted into &amp;quot;run&amp;quot;, whereas the same word marked as adjective was left unchanged. The words not found in the morph table were kept in their original form. All the words were case-folded after the morphological analysis was done. Hyphenated words were then split into their components. We used a modified Okapi (Robertson et al., 1995) formula in the first-pass scoring. Unigrams and bigrams in the intersection of the query and document contributed a score of: t f = idf, (1) + C2 X f the term count for a document, document length, the average length of the documents in the collection and idf is the inverse document frequency, computed as: — n + idf = log( , n+0.5 the total number of documents in the corpus and n is the number of documents containing a given n-gram. In Eq.(1) we used Cl = 0.5,C2 = 1.5 for unigram scoring and Cl = 0.05, c2 = 0.05 for the bigrams. The first pass score was a linear combination of unigram and bigram scores given by Eq.(1), with the unigram scores weight set to 0.8 and bigram scores weight equal to 0.2. We computed first-pass relevance scores for 82,277 overlapping passages, each containing approximately 100 non-stop words, extracted from 18,910 encyclopedia articles. Based on the first pass passage ranking, we constructed expanded queries using the local context analysis (LCA) technique (Xu and Croft, 1996). In the second pass scoring, the expanded queries are used to score 2,632,807 passages based on the TREC-9 Q&amp;A corpus. The passages were selected to contain approximately 200 non-stop words. MRR passl, TREC 0.4605 pass2, TREC 0.4824 pass2, encyclopedia 0.5031 Table 2: Retrieval results. Table 2 summarizes the information retrieval results on the 146 development test set questions described below. The performance is measured by the Mean Reciprocal Rank (MRR) (Voorhees and Tice, 1999) of the highest ranking passage containing the answer string among the top five passages. The first line of the table shows the result of first pass scoring using the TREC-9 Q&amp;A database. The second line contains the result obtained with queries expanded using the TREC database. The last line of the table shows the result corresponding to the system applied in our official submission, with queries expanded using the encyclopedia database. 4 Named Entity Annotation Named entity (NE) annotation is a markup of the text with the class information. As mentioned above, our classes correspond to the MUG classes due to the availability of training data for these classes. We used the text corpora available from the LDC to train the maximum entropy model. Windows of +/- 2 words, morphs, part-ofspeech (POS) tags and flags raised by pattern grammars for DATE, MONEY, CARDINAL, MEASURE, PERCENT, TIME, DURATION classes and dictionary hits, along with the two previous tags are created for each word. The window for predicting tag(0) is shown in Table 3. The window is the useful information given to the maximum entropy feature generation system to make features about the tag of the current word. The (-,+) signs give a clue to the feature functions about the relative position of this data to the word being tagged. Additionally, the (-2,-1,+1,+2) give position information to the feature functions. Each stream has a fixed vocabulary and n-grams from this vocabulary are created to be the features of the maximum entropy model. The training data is to indicate a special category for bewords w(-) w(+) Morphs I I m(+) m(+) POS I I I)(1) Flags 4-2) 4-1) I f(0) I f(1) f(2) Tags t(-2).t(-1) t(-1) I Table 3: Features used in the named entity model for predicting tag(0). ginning each named entity, for example Begin- PERSON, to find the boundaries of the named entity. The system explores multiple NE hypotheses in parallel and keeps only those with high probability and proceeds with a beam-search algorithm to find the most likely path for the whole sentence. The performance of the named entity detector is comparable to the performance cited in (Borthwick et al., 1998) when training the maximum entropy algorithm on only annotated data. We omit the results here in the consideration of space, but note that in the analysis of the question answering system below, only 4 out of 64 errors are attributed directly to the named entity marking for the 250 byte system. 5 Answer Selection We receive in this module the question, the class of the answer that the question seeks and a ranked set of passages (70) annotated with the MUG classes. The optimal sentence that answers the question is now sought. The TREC length constraints of 250 byte and 50 byte are then applied on the sentence. The algorithm used in this module is listed here: 1. Each retrieved passage is split into sentences. 2. A window is formed around each sentence (window size is 3 sentences) 3. The following distances are computed: Matching Words, Thesaurus Match, Mis- Match Words, Dispersion, and Cluster Words. These are defined below. 4. The location or absence of the desired entities is noted in the score. 5. Each of these distances are weighted, the sentences ranked and the top 5 sentences are then output. The definition of the various distances are Matching Words The TFIDF sum of the number of question words that matched answer words identically in the morphed space. (+) Thesaurus Match The TFIDF sum of the number of question words that matched answer words using a thesaurus match using WordNet synonyms (Miller, 1990). (+) Mis-Match Words The TFIDF sum of the number of question content words that did not match in this answer. (-) Dispersion The number of answer words in the candidate sentence that occur between matching question words. (-) Cluster Words The number of answer words in the candidate sentence that occurred adjacently in both the question and answer candidate. (+) Each distance has a weight applied and the corresponding sign shown above attached to it. The score for an answer is the sum of the distances and the top 5 sentences are then output. To select the 250 or 50 byte answer chunk from these sentences, the system identified the longest mismatched pieces between the answer string and the question. It then analyzed the answer and the question to find where the center of the match was using a subject-verbobject assumption of the sentence. The system then output either the subject or object portion whichever had the least matches with the question. Answer selection as done above used ad-hoc and heuristic distance metrics to seek an answer. Future work by the authors will show how to treat these distance metrics as features and to develop a statistical model for answer selection for an open domain. 6 Development Set Analysis We wanted to maintain the TREC-9 question database as a test set, but in order to do some post-evaluation analysis, we chose a subset of the questions as a development set for next year. There were two classes of questions in</abstract>
<phone confidence="0.782145733333333">201 203 209 210 217 220 224 231 238 242 245 252 253 259 264 266 273 275 280 286 287 294 297 301 308 315 319 322 329 330 336 341 343 350 352 357 363 364 371 374 378 385 392 393 399 411 412 413 420 434 453 454 456 458 462 469 473 476 483 484 490 495 497 504 506 511 517 518 525 528 532 539 546 550 553 560 561 567 572 574 581 583 588 594 595 602 605 609 616 623 627 630 637 638 644 649 651 658 660 665 671 672 679 682 686 693 700 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 805 806 807 828 829 830 831 832 833 834 839 840 841 842</phone>
<abstract confidence="0.938013333333333">Table 4: Question numbers chosen for the TREC-9 development set. the TREC9 test: questions that had only one phrasing and questions that had more than one phrasing (rephrased). For example, the following questions form a set:</abstract>
<title confidence="0.662559">Original Form: • Rephrased: • Rephrased:</title>
<abstract confidence="0.937118701492538">We wanted 20% of questions of each class in the development test. The exact list of questions we used for our TREC-9 development test set are shown in Table 4. The variant questions we chose are shown in italics, and we added every seventh question skipping the ones in the above class to yield the 146 questions. A set of regular expressions (answer patterns) which detect the presense of the answer in a string was developed for the set using the judgements file provided by NIST. The MRR for the entire system for the 250 byte system and the 50 byte system is shown in Table 5. The results of our system in the official evaluation and the development set evaluated using pattern rules are close in both the 250 and 50 byte numbers. Furthermore, the results indicate a 2% absolute MRR improvement using the encyclopedia source to expand the original questions. Analysis of the components are shown in Table 6. An error is attributed to a component if System TREC9 results DEV ENCL expansion DEV TREC expansion 250 byte 0.457 0.437 0.417 50 byte 0.290 0.287 0.266 Table 5: MRR for TREC-9 and the chosen development set. Component Number of Errors 250 byte 50 byte Answer Type 5 (3.4%) 7 (4.8%) IR 19 (13%) 19 (13%) NE 4 (2.7%) 5 (3.4%) Answer Selection 36 (24.7%) 52 (35.6%) System 64(43.8%) 83(56.8%) Table 6: Component error rate for the TREC9 dev set for 250 byte system. it is the first component that caused the failure working left to right in our system architecture. This analysis was carried out on the top-5 answer strings. Thus, a failure occurs if there is no answer produced by the system at all. Fixing this error though need not correct the final answer as it may invoke an error in a subsequent component. Answer selection is still seen to be the major cause of problems in our question answering system. Another viewpoint is to see the effect of the system on the IR ranking results. This is shown below in Figure 3. Finding the 250 bytes from a passage that is of typical length 2.4K bytes shows some degradation, but further finding the 50 byte answer has considerable degradation. In Tables 7 and 8 we show the transition matrix for the rank from IR passages to the Q&amp;A system results . Note that there are significant transitions between the IR rank and the Q&amp;A rank, but that inspection of the final result in Figure 3 shows that overall system performance is similar to the performance of IR for the 250 byte system and degraded at 50 bytes. In Figure 3, we plot the number of queries which had an answer at rank 1..5 and indicate no answer produced by &gt;5. These results we believe points to the possibility of making more improvements in answer selection by reranking the results. Q&amp;A rank IR rank Total 1 2 3 4 5 5+</abstract>
<phone confidence="0.802565833333333">1 29 9 5 3 2 5 53 2 10 2 1 0 0 0 13 3 2 2 1 0 1 0 6 4 1 1 0 1 1 2 6 5 2 1 0 0 1 0 4 5+ 13 7 2 1 1 40 64</phone>
<note confidence="0.5288374">Total 57 22 9 5 6 47 146 Table 7: Rank transition matrix, IR ws Q&amp;A, 250 bytes. Q&amp;A rank IR rank Total 1 2 3 4 5 5+</note>
<phone confidence="0.8556545">1 20 5 2 1 0 3 31 2 5 2 1 0 0 1 9 3 6 2 1 1 1 0 11 4 3 1 0 0 1 1 6 5 2 1 1 0 1 1 6 5+ 21 11 4 3 3 41 83</phone>
<abstract confidence="0.9090514375">Total 57 22 9 5 6 47 146 Table 8: Rank transition matrix, IR ws Q&amp;A, 50 bytes. 7 Related Work In the last two years, several efforts at question answering for open domain (Moldovan and et. al., 1999; Voorhees and Tice, 1999) and FAQ domains (Burke and et. al., 1997) have appeared. Our approach at question answering has been to follow the lead of the other participants in the TREC evaluation but base our components on maximum entropy modelling. We believe that corpus based systems allow technologies to be compared in a systematic approach, thus furthering the field of question answering. 8 Conclusion We presented above our architecture and a component-wise evaluation of the system in the question answering problem. We developed maximum entropy formulations for both question/answer classification and named entity marking. The results presented above indicate a 2% absolute MRR improvement using the encyclopedia source to expand the original questions. The transition matrix of the IR to Q&amp;A 90 80 retrieved passage 250 bytes 50 bytes 70 number of queries</abstract>
<note confidence="0.873002625">60 50 40 30 20 10 0 1 2 3 4 5 &gt;5</note>
<intro confidence="0.961778">position</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>