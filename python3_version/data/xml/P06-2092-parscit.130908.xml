<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030797">
<title confidence="0.863324">
ATLAS – a new text alignment architecture
</title>
<author confidence="0.992919">
Bettina Schrader
</author>
<affiliation confidence="0.997432">
Institute of cognitive Science
University of Osnabr¨uck
</affiliation>
<address confidence="0.876058">
49069 Osnabr¨uck
</address>
<email confidence="0.995568">
bschrade@uos.de
</email>
<sectionHeader confidence="0.993799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967608695652">
We are presenting a new, hybrid align-
ment architecture for aligning bilingual,
linguistically annotated parallel corpora.
It is able to align simultaneously at para-
graph, sentence, phrase and word level,
using statistical and heuristic cues, along
with linguistics-based rules. The system
currently aligns English and German texts,
and the linguistic annotation used covers
POS-tags, lemmas and syntactic constitu-
tents. However, as the system is highly
modular, we can easily adapt it to new lan-
guage pairs and other types of annotation.
The hybrid nature of the system allows
experiments with a variety of alignment
cues to find solutions to word alignment
problems like the correct alignment of rare
words and multiwords, or how to align
despite syntactic differences between two
languages.
First performance tests are promising, and
we are setting up a gold standard for a
thorough evaluation of the system.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999858142857143">
Aligning parallel text, i.e. automatically setting
the sentences or words in one text into correspon-
dence with their equivalents in a translation, is a
very useful preprocessing step for a range of ap-
plications, including but not limited to machine
translation (Brown et al., 1993), cross-language
information retrieval (Hiemstra, 1996), dictionary
creation (Smadja et al., 1996) and induction of
NLP-tools (Kuhn, 2004). Aligned corpora can be
also be used in translation studies (Neumann and
Hansen-Schirra, 2005).
The alignment of sentences can be done suffi-
ciently well using cues such as sentence length
(Gale and Church, 1993) or cognates (Simard et
al., 1992). Word alignment, however, is almost ex-
clusively done using statistics (Brown et al., 1993;
Hiemstra, 1996; Vogel et al., 1999; Toutanova et
al., 2002).
Hence it is difficult to align so-called rare
events, i.e. tokens with a frequency below 10. This
is a considerable drawback, as rare events make
up more than half of the vocabulary of any cor-
pus. Another problem is the correct alignment of
multiword units like idioms. Then, differences in
word order are not modelled well by the statistical
algorithms.
In order to find solutions to these problems, we
have developed a hybrid alignment architecture: it
uses statistical information extracted directly from
a corpus, and rules or heuristics based on the lin-
guistic information as given by the corpus’ anno-
tation. Additionally, it is not necessary to compute
sentence alignment prior to aligning at the word
level. Instead, the system is capable of interac-
tively and incrementally computing sentence and
word alignment, along with alignment at the para-
graph and phrase level. The simultaneous align-
ment at different levels of granularity imposes re-
strictions on the way text alignment is computed:
we are using a constrained best-first strategy for
this purpose.
Although we are currently developing and test-
ing the alignment system for the language pair
English-German, we have made sure that it can
easily be extended to new language pairs. In fact,
we are currently adding Swedish and French to the
set of supported languages.
First performance tests have been promising,
and we are currently setting up a gold standard
</bodyText>
<page confidence="0.976321">
715
</page>
<note confidence="0.7239535">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 715–722,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996789">
of 242 manually aligned sentence pairs in English
and German for a thorough evaluation.
In the following, we give an overview on stan-
dard approaches to sentence and word alignment,
and discuss their advantages and shortcomings.
Then, we describe the design of our alignment ar-
chitecture. In the next two sections, we are de-
scribing the data on which we test our system, and
our evaluation strategy. Finally, we sum up and
describe further work.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.998159">
Research on text alignment has largely focused
on aligning either sentences or words, i.e. most
approaches either compute which sentences of a
source and a target language form a translation
pair, or they use sentence alignment as a prepro-
cessing step to align on the word level.
Additionally, emphasis was laid on the devel-
opment of language-independent algorithms. Ide-
ally, such algorithms would not be tailored to align
a specific language pair, but would be applicable to
any two languages. Language-independence has
also been favoured with respect to linguistic re-
sources in that alignment should do without e.g.
using pre-existing dictionaries. Hence there is a
dominance of purely statistical approaches.
</bodyText>
<subsectionHeader confidence="0.993616">
2.1 Sentence Alignment
</subsectionHeader>
<bodyText confidence="0.999925064516129">
Sentence alignment strategies fall roughly into
three categories: length-based approaches (Gale
and Church, 1991; Gale and Church, 1993) are
based on the assumption that the length propor-
tions of a sentence and its translation are roughly
the same. Anchor-based algorithms align sen-
tences based on cues like corpus-specific markup
and orthographic similarity (Simard et al., 1992).
The third approach uses bilingual lexical informa-
tion, e.g. estimated from the corpus (Kay and
R¨oscheisen, 1993; Fung and Church, 1994; Fung
and McKeown, 1994).
Hybrid methods (Tschorn, 2002) combine these
standard approaches such that the shortcomings of
one approach are counterbalanced by the strength
of another component: length-based methods are
very sensitive towards deletions in that a single
omission can cause the alignment to go on a wrong
track from the point where it occurred to the end
of the corpus. Strategies that assume that ortho-
graphic similarity entails translational equivalence
rely on the relatedness of the language pair in
question. In closely-related languages like English
and French, the amount of orthographically simi-
lar words that share the same meaning is higher
than in unrelated languages like English and Chi-
nese, were orthographic or even phonetic similar-
ity may only indicate translational equivalence for
names. Strategies that use system-external dictio-
naries, finally, can only be used if a large-enough
dictionary exists for a specific language pair.
</bodyText>
<subsectionHeader confidence="0.999249">
2.2 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999954516129032">
Aligning below the sentence level is usually done
using statistical models for machine translation
(Brown et al., 1991; Brown et al., 1993; Hiemstra,
1996; Vogel et al., 1999) where any word of the
target language is taken to be a possible translation
for each source language word. The probability of
some target language word to be a translation of
a source language word then depends on the fre-
quency with which both co-occur at the same or
similar positions in the parallel corpus.
The probabilities are estimated from the using
the EM-algorithm1, and a Viterbi search is car-
ried out to compute the most probable sequence
of word translation pairs. Word order differences
between the two languages are modelled by using
statistical weights, and multiword units are simi-
larly treated.
Another approach to word alignment is pre-
sented by Tiedemann (2003), where alignment
probabilities are computed using a combination of
features like e.g. co-occurrence, cognateness, syn-
tactic category membership. However, although
the alignment is partly based on linguistic fea-
tures, its computation is entirely statistical. Other
word alignment strategies (Toutanova et al., 2002;
Cherry and Lin, 2003) have also begun to in-
corporate linguistic knowledge. Unfortunately,
the basic, statistical, assumptions have not been
changed, and hence no sufficient solution to the
shortcomings of the early alignment models have
been found.
</bodyText>
<sectionHeader confidence="0.625185" genericHeader="method">
3 Shortcomings of the statistical
alignment approaches
</sectionHeader>
<bodyText confidence="0.99949475">
While sentence alignment can be done success-
fully using a combination of the existing algo-
rithms, word alignment quality suffers due to
three problematic phenomena: the amount of rare
</bodyText>
<footnote confidence="0.981849">
1see (Manning and Sch¨utze, 1999), chapter 14.2.2 for a
general introduction
</footnote>
<page confidence="0.998205">
716
</page>
<bodyText confidence="0.999618">
words typically found in corpora, word order dif-
ferences between the to languages to be aligned,
and the existence of multiword units
</bodyText>
<subsectionHeader confidence="0.998339">
3.1 Rare Words
</subsectionHeader>
<bodyText confidence="0.999983527777778">
Approximately half of a corpus’ vocabulary con-
sists of so-called hapax legomena, i.e. types that
occur exactly once in a text. Most other words fall
into the range of so-called rare events, which we
define here as types with occurrences between 2
and 10. Both hapax legomena and rare events ob-
viously do not provide sufficient information for
statistical analysis.
In the case of word alignment, it is easy to see
that they are hard to align: there is virtually no fre-
quency or co-occurrence data with which to com-
pute the alignment. On the other hand, five to ten
percent of a corpus’ vocabulary consists of highly
frequent words, i.e. words with frequencies of
100 or above. These types have the advantage of
occurring frequently enough for statistical analy-
sis, however, as they occur at virtually every posi-
tion in a corpus, they can correspond to anything
if alignment decisions are taken on the basis of
statistics only.
One solution to this problem would be to use
statistics-free rules for alignment, i.e. rules that
are insensitive to the rarity or frequency of a word.
However, this means that statistical models either
have to be abandoned completely, or that effort has
to be put in finding a means to combine both align-
ment approaches into one single, hybrid system.
An alternative would be to design a statisti-
cal alignment model that is better suited for the
Zipfian frequency distributions in the source and
target language texts. Research in this direc-
tion would greatly benefit from large amounts
of high quality example alignments, e.g. taken
from the parallel treebanks that are currently be-
ing built (Volk and Samuelsson, 2004; Neumann
and Hansen-Schirra, 2005).
</bodyText>
<subsectionHeader confidence="0.999881">
3.2 Word Order Differences
</subsectionHeader>
<bodyText confidence="0.99998021875">
Another problem that has been noticed as early
as 1993 with the first research on word alignment
(Brown et al., 1993) concerns the differences in
word order between source and target language.
While simple statistical alignment models like
IBM-1 (Brown et al., 1993) and the symmetric
alignment approach by Hiemstra (1996) treat sen-
tences as unstructured bags of words, the more so-
phisticated IBM-models by Brown et al. (1993)
approximates word order differences using a sta-
tistical distortion factor. Vogel et al. (1999), on
the other hand, treat word order differences as a
local phenomenon that can be modelled within a
window of no more than three words. Recently,
researchers like Cherry and Lin (2003) have be-
gun to use syntactic analyses to guide and restrict
the word alignment process.
The advantage of using available syntactic in-
formation for word alignment is that it helps to
overcome data sparseness: although a token may
be rare, its syntactic category may not, and hence
there may be sufficient statistical information to
align at the phrase level. Subsequently, the phrase
level information can be used to compute align-
ments for the tokens within the aligned phrases.
The syntactic function of a token as modifier, head
etc. can equally simplify and guide the align-
ment process considerably. However, it is unclear
whether such an approach performs well for lan-
guage pairs where syntactic and functional differ-
ences are greater than between e.g. English and
French.
</bodyText>
<subsectionHeader confidence="0.999387">
3.3 Multiword alignment
</subsectionHeader>
<bodyText confidence="0.999986423076923">
Like syntactic differences, n:m correspondences,
i.e. alignments that involve multiword expres-
sions, have soon been noted as being difficult for
statistical word alignment: Brown et al. (1993)
modelled fertility, as they called it, statistically in
the more sophisticated IBM-models. Other ap-
proaches adopt again a normalizing procedure: in
a preprocessing step, multiwords are either rec-
ognized as such and subsequently treated as if
they were a single token (Tiedemann, 1999), or,
reversely, the tokens they align to may be split
into their components, with the components be-
ing aligned to the parts of the corresponding mul-
tiword expression on a 1:1 basis.
The latter approach is clearly insufficient for
word alignment quality: it assumes that composi-
tionality holds for both the multiword unit and its
translation, i.e. that the meaning of the whole unit
is made up of the meaning of its part. This clearly
need not be the case, and further problems arise
when a multiword unit and its translation contain
different numbers of elements.
The former approach, i.e. of recognizing mul-
tiword units as such and treating them as a single
token, depends on the kind of recognition proce-
dure adopted, and on the way their alignment is
</bodyText>
<page confidence="0.982847">
717
</page>
<bodyText confidence="0.999787083333333">
computed: if it is based on statistics, again, the
approach will hardly perform well for rare expres-
sions.
To sum up, aligning at the sentence level can
be done with success using a combination of
language-independent methods. Word alignment,
on the other hand, still leaves room for improve-
ment: current models do not suffice to align rare
words and multiword units, and syntactic differ-
ences between source and target languages, too,
still present a challenge for most word alignment
strategies.
</bodyText>
<sectionHeader confidence="0.934691" genericHeader="method">
4 An alternative text alignment system
</sectionHeader>
<bodyText confidence="0.999993736842105">
In order to address these problems, we have de-
signed an alternative text alignment system, called
ATLAS, that computes text alignment based on a
combination of linguistically informed rules and
statistical computation. It takes a linguistically an-
notated corpus as input2. The output of the text
alignment system consists of the corpus alignment
information and a bilingual dictionary.
During the alignment process, hypotheses on
translation pairs are computed by different align-
ment modules, and assigned a confidence value.
These hypotheses may be about paragraphs, sen-
tences, words, or phrases.
All hypotheses are reused to refine and com-
plete the text alignment, and in a final filtering
step, implausible hypotheses are filtered out. The
remaining hypotheses constitute the final overall
text alignment and are used to generate a bilingual
dictionary (see figure 1 for an illustration).
</bodyText>
<subsectionHeader confidence="0.997027">
4.1 Core Component
</subsectionHeader>
<bodyText confidence="0.9544715">
The alignment process is controlled by a core
component: it manages all knowledge bases, i.e.
</bodyText>
<listItem confidence="0.9998336">
• information contained in a system-internal
dictionary,
• corpus information like the positions of to-
kens and their annotations, and
• the set of alignment hypotheses.
</listItem>
<tableCaption confidence="0.9686228">
2The linguistic annotation currently supported includes
lemmas, parts of speech, and syntactic phrases, along with
information on sentence or paragraph boundaries. The an-
notation may include sentence alignment information, and a
bilingual dictionary may be used, too.
</tableCaption>
<bodyText confidence="0.999953176470588">
Additionally, the core component triggers the dif-
ferent alignment modules depending on the type of
a hypothesis: if, for example, a hypothesis is about
a sentence pair, then the word alignment modules
of ATLAS are started in order to find translation
pairs within the sentence pair.
The alignment modules are run simultaneously,
but independently of each other, i.e. an alignment
hypothesis may be generated several times, based
on cues used by different alignment modules. A
word pair e.g. may be aligned based on ortho-
graphic similarity by one module, and based on
syntactic information by another module.
Each hypothesis is assigned a confidence value
by the alignment module that generated it, and
then returned to the core component. The confi-
dence value of each hypothesis is derived from i)
its probability or similarity value, and ii) the con-
fidence value of the parent hypothesis.
The core component may change the confidence
value of a hypothesis, e.g. if it was generated mul-
tiple times by different alignment modules, based
on different alignment cues. This multiple gen-
eration of the same hypothesis is taken as indica-
tion that the hypothesis is more reliable than if it
had been generated by only one alignment mod-
ule, and hence its confidence value is increase.
The core component adds all new information
to its knowledge bases, and hands it over to appro-
priate alignment modules for further computation.
The process is iterated until no new hypotheses
are found. Then, the core component assembles
the best hypotheses to compute a final hypothesis
set: starting with the hypothesis that has the high-
est confidence, each next-best hypothesis is tested
whether it fits into the final set; if there is a contra-
diction between the hypotheses already in the set
and the next-best, the latter is discarded from the
knowledge base. If not, then it is added to the final
set. This process is iterated until all hypotheses
have been either added to the final hypothesis set,
or have been discarded.
Cleaning-up procedures ensure that corpus
items left unaligned are either aligned to null, or
can be aligned based on a process of elimina-
tion: if two units a and b are contained within the
same textual unit, e.g. within the same paragraph,
and aligning them would not cause a contradiction
with the hypotheses in the final set, then they are
aligned. Finally, all remaining hypothesis are used
to generate the overall text alignment, and to com-
</bodyText>
<page confidence="0.979046">
718
</page>
<figure confidence="0.999816105263158">
write alignment
information
core component
alignment modules
➔ management of knowledge bases
➔ corpus,
➔system-internal dictionary,
➔ set of hypotheses
➔ task management
➔ result filtering
➔ output generation
trigger alignment
receive hypotheses
paragraph alignment strategies
sentence alignment strategies
... further alignment strategies
phrase alignment strategies
word alignment strategies
read corpus
</figure>
<figureCaption confidence="0.999651">
Figure 1: A schema of the text alignment architecture
</figureCaption>
<bodyText confidence="0.63954">
pute a bilingual dictionary.
</bodyText>
<subsectionHeader confidence="0.97807">
4.2 Alignment modules
</subsectionHeader>
<bodyText confidence="0.999975818181818">
Each alignment module receives a parent hypoth-
esis as input that covers certain units of the cor-
pus, i.e. a hypothesis on a sentence pair covers
those tokens along with their annotations that are
contained within the sentence pair. It uses this in-
formation to compute child hypotheses within the
units of the parent hypothesis, assigns each child
hypothesis a confidence value that indicates how
reliable it is, and returns the set of children hy-
potheses to the core component.
In the case of a statistics-based alignment mod-
ule, the confidence value corresponds to the proba-
bility with which a translation pair may be aligned.
In other, non-statistical alignment modules, the
confidence value is derived from the similarity
value computed for a specific translation pair.
The alignment modules that are currently used
by our the system are modules for aligning sen-
tences or paragraphs based on the strategies that
have been proposed in the literature (see overview
in section 2.1), but also strategies that we have ex-
perimented with for aligning words based on lin-
ear ordering, parts of speech, dictionary lookup
etc (see section 5). No statistical word align-
ment procedure has yet been added to the sys-
tem, but we are experimenting with using statisti-
cal co-occurrence measures for deriving word cor-
respondences. One language independent align-
ment strategy is based on inheritance: if two units
a and b are aligned, then this information is used
to derive alignment hypotheses for the elements
within a and b as well as for the textual units that
contain a and b.
</bodyText>
<sectionHeader confidence="0.679201" genericHeader="method">
5 Advantages of the hybrid architecture
</sectionHeader>
<bodyText confidence="0.99999175">
As our alignment architecture is hybrid and hence
need not rely on statistial information alone, it
can be used to successfully address word align-
ment problems. Note that although linguistically
informed alignment strategies are used, the sys-
tem is not restricted to statistics-free computation:
it is still possible to compute word co-occurrence
statistics and derive alignment hypotheses.
</bodyText>
<subsectionHeader confidence="0.955343">
5.1 Rare words
</subsectionHeader>
<bodyText confidence="0.9999702">
Linguistically-informed rules that compute align-
ments based on corpus annotation, but not on
statistics, can be used to overcome data sparse-
ness. Syntactic categories e.g. give reliable align-
ment cues as lexical categories such as nouns and
verbs are not commonly changed during the trans-
lation process. Even if category changes occur, it
is likely that the categorial class stays the same.
Ideally, a noun e.g. will be translated as a noun, or
if it is not, it is highly probable that it is translated
as an adjective or verb, but not as a functional class
member like a preposition.
Likewise, dictionary lookup may be used, and is
used by or system, to align words within sentences
or phrases. We have also implemented a module
that aligns sentences and words based on string
similarity constrained by syntactic categories: the
module exploits the part of speech annotation to
align sentences and words based on string simi-
larity between nouns, adjectives, and verbs, thus
modifying the classic approach by Simard et al
(1992). The advantage of the modification is that
the amount of cognates within lexical class words
will be considerably higher than between prepo-
sitions, determiners, etc., hence filtering by word
</bodyText>
<page confidence="0.996241">
719
</page>
<bodyText confidence="0.681345">
category yields good results.
</bodyText>
<subsectionHeader confidence="0.998367">
5.2 Word Order Differences
</subsectionHeader>
<bodyText confidence="0.999425866666667">
As ATLAS supports the alignment of phrases, mis-
matches between the linear orderings of source
and target language words become irrelevant. Ad-
ditionally, phrase alignment can considerably nar-
row down the search space within which to find
the translation of a word. If e.g. a noun phrase has
already been aligned to its equivalent in the other
language, aligning its daughter nodes on the basis
of their syntactic categories, without any further
constraints or statistical information, can be suffi-
cient.
Furthermore, if parts of the phrase can be
aligned using the system-internal dictionary,
aligning the remaining words could be done by
process of elimination.
</bodyText>
<subsectionHeader confidence="0.998037">
5.3 Multiword alignment
</subsectionHeader>
<bodyText confidence="0.999930368421053">
Multiwords are traditionally hardest to align, one
reason being that they are hard to recognize statis-
tically. With our text alignment system, however,
it is possible to write i) language-specific rules
that detect multiwords and define ii) a similar-
ity measure that aligns the detected multiwords to
their translations. This similarity measure may be
language-pair specific, or it may be defined glob-
ally, i.e. it will be used for any language pair.
We have already tested such a procedure for
aligning English nominal multiwords with their
German translations: In this procedure, English
nominals are detected based on their typical part-
of-speech patterns, and aligned to German nouns
if the two expressions are roughly of the same
length, counted in characters. The results are en-
couraging, indicating that nominals can be aligned
reliably irrespective of their frequencies in the cor-
pus (Schrader, 2006).
</bodyText>
<sectionHeader confidence="0.99353" genericHeader="method">
6 Data
</sectionHeader>
<bodyText confidence="0.999984928571429">
As development corpus, we are using Europarl,
a corpus of European Parliament debates (Koehn,
2005). Europarl consists of roughly 30 million to-
kens per language and is tokenized and sentence-
aligned. For the purposes of testing ATLAS, we
have POS-tagged and lemmatized the German,
English, and French parts of the corpus using the
freely available tree-tagger (Schmid, 1994). Addi-
tionally, we have chunked the German and English
texts with an extension of this tool (Schmid, un-
published). Table 1 shows the number of tokens
and types of the corpus for all three languages.
It also shows the percentages of hapax legomena,
rare events3, and all other types of the corpus.
</bodyText>
<sectionHeader confidence="0.987035" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999944682926829">
For evaluating of our text alignment system, we
are currently setting up an English-German gold
standard: we have randomly chosen a debate pro-
tocol of the Europarl corpus that contains approx-
imately 100,000 tokens per language (see table 2),
and we corrected its sentence alignment manually.
The correction was done by two annotaters inde-
pendently of each other, and remaining sentence
alignment differences after the corrections were
resolved.
In a second step, we have chosen 242 sentence
pairs from this reference set to create a word align-
ment gold standard. Some sentence pairs of this
set have been chosen randomly, the others are
taken from two text passages in the protocol. We
had considered choosing sentence pairs that were
distributed randomly over the reference set, how-
ever, we decided for taking complete text passages
in order to make manual annotation easier. This
way, the annotators can easily access the context
of a sentence pair to resolve alignment ambigui-
ties.
Additionally, we have created word align-
ment guidelines based on those already given by
Melamed (1998) and Merkel (1999). We have an-
notated all 242 sentence pairs twice, and annota-
tion differences are currently being resolved.
As this gold standard can only be used to eval-
uate the performance of English-German word
alignment, we will also evaluate our system on the
Stockholm parallel treebank (Volk and Samuels-
son, 2004). Evaluating against this manually con-
structed treebank has the advantage that we can
evaluate phrase alignment quality, and that we
can gather evaluation data for the language pairs
English-Swedish and Swedish-German.
We have decided to use the evaluation met-
rics precision, recall and the alignment error rate
(AER) proposed by Och and Ney (2000) in order
to compare results to those of other alignment sys-
tems.
</bodyText>
<footnote confidence="0.948226">
3We define rare events here as types occurring 2 to 10
times
</footnote>
<page confidence="0.97019">
720
</page>
<table confidence="0.9976685">
Language Tokens Types Hapax Legomena Rare Events Frequent Types
English 29.077,024 101,967 39,200 (38.44%) 35,608 (34.92%) 27,159 (26.64%)
German 27.643,792 286,330 140,826 (49.18%) 98,126 (34.27%) 47,378 (16.55%)
French 32.439,353 114,891 42,114 (36.66%) 41,194 (35.84%) 31,583 (27.49%)
</table>
<tableCaption confidence="0.994119">
Table 1: Corpus characteristics of the Europarl corpus
</tableCaption>
<table confidence="0.999306333333333">
Language Tokens Types Hapax Legomena Rare Events Frequent Types
English 111,222 7,657 3,474 (45.37%) 3,027 (39.53%) 1,156 (15.10%)
German 91,054 11,237 6,336 (56.39%) 3,973 (35.36%) 928 ( 8.26%)
</table>
<tableCaption confidence="0.998393">
Table 2: Characteristics of the evaluation suite
</tableCaption>
<sectionHeader confidence="0.986524" genericHeader="evaluation">
8 Summary
</sectionHeader>
<bodyText confidence="0.999960677419355">
Summing up, we have presented a new text
alignment architecture that makes use of multi-
ple sources of information, partly statistical, partly
linguistics-based, to align bilingual, parallel texts.
Its input is a linguistically annotated parallel cor-
pus, and corpus annotation may include informa-
tion on syntactic constituency, syntactic category
membership, lemmas, etc. Alignment is done on
various levels of granularity, i.e. the system aligns
simultaneously at the paragraph, sentence, phrase,
and word level. A constrained best-first search is
used to filter out errors, and the output of the sys-
tem is corpus alignment information along with a
bilingual dictionary, generated on the basis of the
text alignment.
As our system need not rely on statistics alone,
the alignment of hapax legomena and other rare
events is not a problem. Additionally, specific
strategies have been implemented, and further can
be added, to deal with various kinds of multiword
units. Finally, as the system allows phrase align-
ment, it stands on equal footing with other phrase
alignment approaches.
Currently, the system is tested on the English-
German parts of the Europarl corpus, but as it is
highly modular, it can easily be extended to new
language pairs, types of information, and different
alignment strategies.
First performance test have been promising, and
we are setting up a gold standard alignment for a
thorough evaluation.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="conclusions">
9 Further work
</sectionHeader>
<bodyText confidence="0.9999575">
We are currently adding Swedish and French to the
set of supported languages, such that our system
will be able to align all possible pairings with the
languages German, English, French and Swedish.
If possible, we want to conduct experiments that
involve further languages and additional kinds of
corpus annotation, like e.g. detailed morphologi-
cal information as annotated e.g. within the CroCo
project (Neumann and Hansen-Schirra, 2005).
At the same time, we are constantly extend-
ing the set of available alignment strategies, e.g.
with strategies for specific syntactic categories or
strategies that compute alignments based on statis-
tical co-occurrence.
A first evaluation of our text alignment system
will have been completed by autumn 2006, and
we plan to make our gold standard as well as our
guidelines available to the research community.
</bodyText>
<sectionHeader confidence="0.9657" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998563">
We thank Judith Degen for annotation help with
the gold standard.
</bodyText>
<sectionHeader confidence="0.998551" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.980912">
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In
Proceedings of the 29th Annual Meeting of the As-
sociation for Computational Linguistics, pages 169–
176, Berkeley, California, USA.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 88–95, Sapporo,
Japan.
Pascale Fung and Kenneth W. Church. 1994. K-vec:
a new approach for aligning parallel texts. In Pro-
ceedings of the 15th International Conference on
</reference>
<page confidence="0.98794">
721
</page>
<reference confidence="0.999728425742574">
Computational Linguistics (COLING), pages 1096–
1102, Kyoto, Japan.
Pascale Fung and Kathleen McKeown. 1994. Align-
ing noisy parallel corpora across language groups:
word pair feature matching by dynamic time warp-
ing. In Proceedings of the First Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-94), pages 81–88, Columbia, Maryland,
USA.
William A. Gale and Kenneth W. Church. 1991. A
program for aligning sentences in bilingual corpora.
In Proceedings of the 29th Annual Meeting of the As-
sociation for Computational Linguistics, pages 177–
184, Berkeley, California, USA. Reprinted 1993 in
Computational Linguistics.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75–102.
D. Hiemstra. 1996. Using statistical methods to create
a bilingual dictionary. Master’s thesis, Universiteit
Twente.
Martin Kay and Martin R¨oscheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121–142.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Jonas Kuhn. 2004. Exploiting parallel corpora for
monolingual grammar induction – a pilot study.
In Workshop proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC), pages 54–57, Lisbon, Portugal. LREC
Workshop: The Amazing Utility of Parallel and
Comparable Corpora.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, Massachusetts, Lon-
don.
I. Dan Melamed. 1998. Annotation style guide for
the BLINKER project. Technical Report 98-06, In-
stitute for Research in Cognitive Science, University
of Pennsylvania.
Magnus Merkel. 1999. Annotation style guide for the
PLUG link annotator. Technical report, Link¨oping
university, Link¨oping, March. PLUG report.
Stella Neumann and Silvia Hansen-Schirra. 2005.
The CroCo project. Cross-linguistic corpora for the
investigateon of explicitation in translation. In
Proceedings of the Corpus Linguistics Conference,
Birmingham, UK.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440–447, Hong Kong,
China.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44–49, Manchester, England.
Helmut Schmid. unpublished. The IMS Chunker. un-
published manuscript.
Bettina Schrader. 2006. Non-probabilistic alignment
of rare German and English nominal expressions. In
To appear in: Proceedings of the Fifth Language Re-
sources and Evaluation Conference (LREC), Genoa,
Italy. to appear.
Michel Simard, G. F. Foster, and P. Isabelle. 1992.
Using cognates to align sentences in bilingual cor-
pora. In Proceedings of the Fourth International
conference on theoretical and methodological is-
sues in Machine translation, pages 67–81, Montreal,
Canada.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Compu-
tational Linguistics, 22(1):1–38.
J¨org Tiedemann. 1999. Word alignment - step by step.
In Proceedings of the 12th Nordic Conference on
Computational Linguistics, pages 216–227, Trond-
heim, Norway.
J¨org Tiedemann. 2003. Combining clues for word
alignment. In Proceedings of the 10th Conference of
the European Chapter of the ACL (EACL03), pages
339 – 346, Budapest, Hungary.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to HMM-based sta-
tistical word alignment models. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2002), pages 87–94, Philadelphia, USA.
Patrick Tschorn. 2002. Automatically aligning
English-German parallel texts at sentence level us-
ing linguistic knowledge. Master’s thesis, Univer-
sit¨at Osnabr¨uck.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1999. HMM-based word alignment in sta-
tistical translation. In Proceedings of the Inter-
national Conference on Computational Linguistics,
pages 836–841, Copenhagen, Denmark.
Martin Volk and Yvonne Samuelsson. 2004. Boot-
strapping parallel treebanks. In Proceedings of
the Workshop on Linguistically Interpreted Corpora
(LINC) at COLING, Geneva, Switzerland.
</reference>
<page confidence="0.997112">
722
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942202">
<title confidence="0.981464">a new text alignment architecture</title>
<author confidence="0.999852">Bettina Schrader</author>
<affiliation confidence="0.999907">Institute of cognitive Science University of Osnabr¨uck</affiliation>
<address confidence="0.975088">49069 Osnabr¨uck</address>
<email confidence="0.992089">bschrade@uos.de</email>
<abstract confidence="0.999642583333333">We are presenting a new, hybrid alignment architecture for aligning bilingual, linguistically annotated parallel corpora. It is able to align simultaneously at paragraph, sentence, phrase and word level, using statistical and heuristic cues, along with linguistics-based rules. The system currently aligns English and German texts, and the linguistic annotation used covers POS-tags, lemmas and syntactic constitutents. However, as the system is highly modular, we can easily adapt it to new language pairs and other types of annotation. The hybrid nature of the system allows experiments with a variety of alignment cues to find solutions to word alignment problems like the correct alignment of rare words and multiwords, or how to align despite syntactic differences between two languages. First performance tests are promising, and we are setting up a gold standard for a thorough evaluation of the system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>169--176</pages>
<location>Berkeley, California, USA.</location>
<contexts>
<context position="6289" citStr="Brown et al., 1991" startWordPosition="971" endWordPosition="974"> relatedness of the language pair in question. In closely-related languages like English and French, the amount of orthographically similar words that share the same meaning is higher than in unrelated languages like English and Chinese, were orthographic or even phonetic similarity may only indicate translational equivalence for names. Strategies that use system-external dictionaries, finally, can only be used if a large-enough dictionary exists for a specific language pair. 2.2 Word Alignment Aligning below the sentence level is usually done using statistical models for machine translation (Brown et al., 1991; Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999) where any word of the target language is taken to be a possible translation for each source language word. The probability of some target language word to be a translation of a source language word then depends on the frequency with which both co-occur at the same or similar positions in the parallel corpus. The probabilities are estimated from the using the EM-algorithm1, and a Viterbi search is carried out to compute the most probable sequence of word translation pairs. Word order differences between the two languages are modelled by </context>
</contexts>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer. 1991. Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 169– 176, Berkeley, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1364" citStr="Brown et al., 1993" startWordPosition="204" endWordPosition="207"> with a variety of alignment cues to find solutions to word alignment problems like the correct alignment of rare words and multiwords, or how to align despite syntactic differences between two languages. First performance tests are promising, and we are setting up a gold standard for a thorough evaluation of the system. 1 Introduction Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. toke</context>
<context position="6309" citStr="Brown et al., 1993" startWordPosition="975" endWordPosition="978">language pair in question. In closely-related languages like English and French, the amount of orthographically similar words that share the same meaning is higher than in unrelated languages like English and Chinese, were orthographic or even phonetic similarity may only indicate translational equivalence for names. Strategies that use system-external dictionaries, finally, can only be used if a large-enough dictionary exists for a specific language pair. 2.2 Word Alignment Aligning below the sentence level is usually done using statistical models for machine translation (Brown et al., 1991; Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999) where any word of the target language is taken to be a possible translation for each source language word. The probability of some target language word to be a translation of a source language word then depends on the frequency with which both co-occur at the same or similar positions in the parallel corpus. The probabilities are estimated from the using the EM-algorithm1, and a Viterbi search is carried out to compute the most probable sequence of word translation pairs. Word order differences between the two languages are modelled by using statistical we</context>
<context position="9888" citStr="Brown et al., 1993" startWordPosition="1553" endWordPosition="1556">s to combine both alignment approaches into one single, hybrid system. An alternative would be to design a statistical alignment model that is better suited for the Zipfian frequency distributions in the source and target language texts. Research in this direction would greatly benefit from large amounts of high quality example alignments, e.g. taken from the parallel treebanks that are currently being built (Volk and Samuelsson, 2004; Neumann and Hansen-Schirra, 2005). 3.2 Word Order Differences Another problem that has been noticed as early as 1993 with the first research on word alignment (Brown et al., 1993) concerns the differences in word order between source and target language. While simple statistical alignment models like IBM-1 (Brown et al., 1993) and the symmetric alignment approach by Hiemstra (1996) treat sentences as unstructured bags of words, the more sophisticated IBM-models by Brown et al. (1993) approximates word order differences using a statistical distortion factor. Vogel et al. (1999), on the other hand, treat word order differences as a local phenomenon that can be modelled within a window of no more than three words. Recently, researchers like Cherry and Lin (2003) have begu</context>
<context position="11470" citStr="Brown et al. (1993)" startWordPosition="1803" endWordPosition="1806">he phrase level information can be used to compute alignments for the tokens within the aligned phrases. The syntactic function of a token as modifier, head etc. can equally simplify and guide the alignment process considerably. However, it is unclear whether such an approach performs well for language pairs where syntactic and functional differences are greater than between e.g. English and French. 3.3 Multiword alignment Like syntactic differences, n:m correspondences, i.e. alignments that involve multiword expressions, have soon been noted as being difficult for statistical word alignment: Brown et al. (1993) modelled fertility, as they called it, statistically in the more sophisticated IBM-models. Other approaches adopt again a normalizing procedure: in a preprocessing step, multiwords are either recognized as such and subsequently treated as if they were a single token (Tiedemann, 1999), or, reversely, the tokens they align to may be split into their components, with the components being aligned to the parts of the corresponding multiword expression on a 1:1 basis. The latter approach is clearly insufficient for word alignment quality: it assumes that compositionality holds for both the multiwor</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="7363" citStr="Cherry and Lin, 2003" startWordPosition="1140" endWordPosition="1143">h is carried out to compute the most probable sequence of word translation pairs. Word order differences between the two languages are modelled by using statistical weights, and multiword units are similarly treated. Another approach to word alignment is presented by Tiedemann (2003), where alignment probabilities are computed using a combination of features like e.g. co-occurrence, cognateness, syntactic category membership. However, although the alignment is partly based on linguistic features, its computation is entirely statistical. Other word alignment strategies (Toutanova et al., 2002; Cherry and Lin, 2003) have also begun to incorporate linguistic knowledge. Unfortunately, the basic, statistical, assumptions have not been changed, and hence no sufficient solution to the shortcomings of the early alignment models have been found. 3 Shortcomings of the statistical alignment approaches While sentence alignment can be done successfully using a combination of the existing algorithms, word alignment quality suffers due to three problematic phenomena: the amount of rare 1see (Manning and Sch¨utze, 1999), chapter 14.2.2 for a general introduction 716 words typically found in corpora, word order differe</context>
<context position="10478" citStr="Cherry and Lin (2003)" startWordPosition="1647" endWordPosition="1650"> alignment (Brown et al., 1993) concerns the differences in word order between source and target language. While simple statistical alignment models like IBM-1 (Brown et al., 1993) and the symmetric alignment approach by Hiemstra (1996) treat sentences as unstructured bags of words, the more sophisticated IBM-models by Brown et al. (1993) approximates word order differences using a statistical distortion factor. Vogel et al. (1999), on the other hand, treat word order differences as a local phenomenon that can be modelled within a window of no more than three words. Recently, researchers like Cherry and Lin (2003) have begun to use syntactic analyses to guide and restrict the word alignment process. The advantage of using available syntactic information for word alignment is that it helps to overcome data sparseness: although a token may be rare, its syntactic category may not, and hence there may be sufficient statistical information to align at the phrase level. Subsequently, the phrase level information can be used to compute alignments for the tokens within the aligned phrases. The syntactic function of a token as modifier, head etc. can equally simplify and guide the alignment process considerably</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 88–95, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kenneth W Church</author>
</authors>
<title>K-vec: a new approach for aligning parallel texts.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1096--1102</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="5191" citStr="Fung and Church, 1994" startWordPosition="804" endWordPosition="807">sting dictionaries. Hence there is a dominance of purely statistical approaches. 2.1 Sentence Alignment Sentence alignment strategies fall roughly into three categories: length-based approaches (Gale and Church, 1991; Gale and Church, 1993) are based on the assumption that the length proportions of a sentence and its translation are roughly the same. Anchor-based algorithms align sentences based on cues like corpus-specific markup and orthographic similarity (Simard et al., 1992). The third approach uses bilingual lexical information, e.g. estimated from the corpus (Kay and R¨oscheisen, 1993; Fung and Church, 1994; Fung and McKeown, 1994). Hybrid methods (Tschorn, 2002) combine these standard approaches such that the shortcomings of one approach are counterbalanced by the strength of another component: length-based methods are very sensitive towards deletions in that a single omission can cause the alignment to go on a wrong track from the point where it occurred to the end of the corpus. Strategies that assume that orthographic similarity entails translational equivalence rely on the relatedness of the language pair in question. In closely-related languages like English and French, the amount of ortho</context>
</contexts>
<marker>Fung, Church, 1994</marker>
<rawString>Pascale Fung and Kenneth W. Church. 1994. K-vec: a new approach for aligning parallel texts. In Proceedings of the 15th International Conference on Computational Linguistics (COLING), pages 1096– 1102, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aligning noisy parallel corpora across language groups: word pair feature matching by dynamic time warping.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA-94),</booktitle>
<pages>81--88</pages>
<location>Columbia, Maryland, USA.</location>
<contexts>
<context position="5216" citStr="Fung and McKeown, 1994" startWordPosition="808" endWordPosition="811">ce there is a dominance of purely statistical approaches. 2.1 Sentence Alignment Sentence alignment strategies fall roughly into three categories: length-based approaches (Gale and Church, 1991; Gale and Church, 1993) are based on the assumption that the length proportions of a sentence and its translation are roughly the same. Anchor-based algorithms align sentences based on cues like corpus-specific markup and orthographic similarity (Simard et al., 1992). The third approach uses bilingual lexical information, e.g. estimated from the corpus (Kay and R¨oscheisen, 1993; Fung and Church, 1994; Fung and McKeown, 1994). Hybrid methods (Tschorn, 2002) combine these standard approaches such that the shortcomings of one approach are counterbalanced by the strength of another component: length-based methods are very sensitive towards deletions in that a single omission can cause the alignment to go on a wrong track from the point where it occurred to the end of the corpus. Strategies that assume that orthographic similarity entails translational equivalence rely on the relatedness of the language pair in question. In closely-related languages like English and French, the amount of orthographically similar words</context>
</contexts>
<marker>Fung, McKeown, 1994</marker>
<rawString>Pascale Fung and Kathleen McKeown. 1994. Aligning noisy parallel corpora across language groups: word pair feature matching by dynamic time warping. In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA-94), pages 81–88, Columbia, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--184</pages>
<location>Berkeley, California, USA. Reprinted</location>
<note>in Computational Linguistics.</note>
<contexts>
<context position="4786" citStr="Gale and Church, 1991" startWordPosition="741" endWordPosition="744">t as a preprocessing step to align on the word level. Additionally, emphasis was laid on the development of language-independent algorithms. Ideally, such algorithms would not be tailored to align a specific language pair, but would be applicable to any two languages. Language-independence has also been favoured with respect to linguistic resources in that alignment should do without e.g. using pre-existing dictionaries. Hence there is a dominance of purely statistical approaches. 2.1 Sentence Alignment Sentence alignment strategies fall roughly into three categories: length-based approaches (Gale and Church, 1991; Gale and Church, 1993) are based on the assumption that the length proportions of a sentence and its translation are roughly the same. Anchor-based algorithms align sentences based on cues like corpus-specific markup and orthographic similarity (Simard et al., 1992). The third approach uses bilingual lexical information, e.g. estimated from the corpus (Kay and R¨oscheisen, 1993; Fung and Church, 1994; Fung and McKeown, 1994). Hybrid methods (Tschorn, 2002) combine these standard approaches such that the shortcomings of one approach are counterbalanced by the strength of another component: le</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William A. Gale and Kenneth W. Church. 1991. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 177– 184, Berkeley, California, USA. Reprinted 1993 in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1714" citStr="Gale and Church, 1993" startWordPosition="255" endWordPosition="258">parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. tokens with a frequency below 10. This is a considerable drawback, as rare events make up more than half of the vocabulary of any corpus. Another problem is the correct alignment of multiword units like idioms. Then, differences in word order are not modelled well by the statistical algorithms. In order to find solutions to these problems, we have deve</context>
<context position="4810" citStr="Gale and Church, 1993" startWordPosition="745" endWordPosition="748">ep to align on the word level. Additionally, emphasis was laid on the development of language-independent algorithms. Ideally, such algorithms would not be tailored to align a specific language pair, but would be applicable to any two languages. Language-independence has also been favoured with respect to linguistic resources in that alignment should do without e.g. using pre-existing dictionaries. Hence there is a dominance of purely statistical approaches. 2.1 Sentence Alignment Sentence alignment strategies fall roughly into three categories: length-based approaches (Gale and Church, 1991; Gale and Church, 1993) are based on the assumption that the length proportions of a sentence and its translation are roughly the same. Anchor-based algorithms align sentences based on cues like corpus-specific markup and orthographic similarity (Simard et al., 1992). The third approach uses bilingual lexical information, e.g. estimated from the corpus (Kay and R¨oscheisen, 1993; Fung and Church, 1994; Fung and McKeown, 1994). Hybrid methods (Tschorn, 2002) combine these standard approaches such that the shortcomings of one approach are counterbalanced by the strength of another component: length-based methods are v</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>William A. Gale and Kenneth W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hiemstra</author>
</authors>
<title>Using statistical methods to create a bilingual dictionary. Master’s thesis,</title>
<date>1996</date>
<institution>Universiteit Twente.</institution>
<contexts>
<context position="1419" citStr="Hiemstra, 1996" startWordPosition="211" endWordPosition="212"> alignment problems like the correct alignment of rare words and multiwords, or how to align despite syntactic differences between two languages. First performance tests are promising, and we are setting up a gold standard for a thorough evaluation of the system. 1 Introduction Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. tokens with a frequency below 10. This is a considerable dr</context>
<context position="6325" citStr="Hiemstra, 1996" startWordPosition="979" endWordPosition="980">stion. In closely-related languages like English and French, the amount of orthographically similar words that share the same meaning is higher than in unrelated languages like English and Chinese, were orthographic or even phonetic similarity may only indicate translational equivalence for names. Strategies that use system-external dictionaries, finally, can only be used if a large-enough dictionary exists for a specific language pair. 2.2 Word Alignment Aligning below the sentence level is usually done using statistical models for machine translation (Brown et al., 1991; Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999) where any word of the target language is taken to be a possible translation for each source language word. The probability of some target language word to be a translation of a source language word then depends on the frequency with which both co-occur at the same or similar positions in the parallel corpus. The probabilities are estimated from the using the EM-algorithm1, and a Viterbi search is carried out to compute the most probable sequence of word translation pairs. Word order differences between the two languages are modelled by using statistical weights, and multi</context>
<context position="10093" citStr="Hiemstra (1996)" startWordPosition="1585" endWordPosition="1586">e and target language texts. Research in this direction would greatly benefit from large amounts of high quality example alignments, e.g. taken from the parallel treebanks that are currently being built (Volk and Samuelsson, 2004; Neumann and Hansen-Schirra, 2005). 3.2 Word Order Differences Another problem that has been noticed as early as 1993 with the first research on word alignment (Brown et al., 1993) concerns the differences in word order between source and target language. While simple statistical alignment models like IBM-1 (Brown et al., 1993) and the symmetric alignment approach by Hiemstra (1996) treat sentences as unstructured bags of words, the more sophisticated IBM-models by Brown et al. (1993) approximates word order differences using a statistical distortion factor. Vogel et al. (1999), on the other hand, treat word order differences as a local phenomenon that can be modelled within a window of no more than three words. Recently, researchers like Cherry and Lin (2003) have begun to use syntactic analyses to guide and restrict the word alignment process. The advantage of using available syntactic information for word alignment is that it helps to overcome data sparseness: althoug</context>
</contexts>
<marker>Hiemstra, 1996</marker>
<rawString>D. Hiemstra. 1996. Using statistical methods to create a bilingual dictionary. Master’s thesis, Universiteit Twente.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
<author>Martin R¨oscheisen</author>
</authors>
<title>Texttranslation alignment.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<marker>Kay, R¨oscheisen, 1993</marker>
<rawString>Martin Kay and Martin R¨oscheisen. 1993. Texttranslation alignment. Computational Linguistics, 19(1):121–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="22454" citStr="Koehn, 2005" startWordPosition="3560" endWordPosition="3561">ly, i.e. it will be used for any language pair. We have already tested such a procedure for aligning English nominal multiwords with their German translations: In this procedure, English nominals are detected based on their typical partof-speech patterns, and aligned to German nouns if the two expressions are roughly of the same length, counted in characters. The results are encouraging, indicating that nominals can be aligned reliably irrespective of their frequencies in the corpus (Schrader, 2006). 6 Data As development corpus, we are using Europarl, a corpus of European Parliament debates (Koehn, 2005). Europarl consists of roughly 30 million tokens per language and is tokenized and sentencealigned. For the purposes of testing ATLAS, we have POS-tagged and lemmatized the German, English, and French parts of the corpus using the freely available tree-tagger (Schmid, 1994). Additionally, we have chunked the German and English texts with an extension of this tool (Schmid, unpublished). Table 1 shows the number of tokens and types of the corpus for all three languages. It also shows the percentages of hapax legomena, rare events3, and all other types of the corpus. 7 Evaluation For evaluating o</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Kuhn</author>
</authors>
<title>Exploiting parallel corpora for monolingual grammar induction – a pilot study.</title>
<date>2004</date>
<booktitle>In Workshop proceedings of the 4th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>54--57</pages>
<location>Lisbon,</location>
<contexts>
<context position="1502" citStr="Kuhn, 2004" startWordPosition="223" endWordPosition="224">align despite syntactic differences between two languages. First performance tests are promising, and we are setting up a gold standard for a thorough evaluation of the system. 1 Introduction Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. tokens with a frequency below 10. This is a considerable drawback, as rare events make up more than half of the vocabulary of any corpus. Anot</context>
</contexts>
<marker>Kuhn, 2004</marker>
<rawString>Jonas Kuhn. 2004. Exploiting parallel corpora for monolingual grammar induction – a pilot study. In Workshop proceedings of the 4th International Conference on Language Resources and Evaluation (LREC), pages 54–57, Lisbon, Portugal. LREC Workshop: The Amazing Utility of Parallel and Comparable Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, London.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, Massachusetts, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Annotation style guide for the BLINKER project.</title>
<date>1998</date>
<tech>Technical Report 98-06,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="24125" citStr="Melamed (1998)" startWordPosition="3832" endWordPosition="3833">osen 242 sentence pairs from this reference set to create a word alignment gold standard. Some sentence pairs of this set have been chosen randomly, the others are taken from two text passages in the protocol. We had considered choosing sentence pairs that were distributed randomly over the reference set, however, we decided for taking complete text passages in order to make manual annotation easier. This way, the annotators can easily access the context of a sentence pair to resolve alignment ambiguities. Additionally, we have created word alignment guidelines based on those already given by Melamed (1998) and Merkel (1999). We have annotated all 242 sentence pairs twice, and annotation differences are currently being resolved. As this gold standard can only be used to evaluate the performance of English-German word alignment, we will also evaluate our system on the Stockholm parallel treebank (Volk and Samuelsson, 2004). Evaluating against this manually constructed treebank has the advantage that we can evaluate phrase alignment quality, and that we can gather evaluation data for the language pairs English-Swedish and Swedish-German. We have decided to use the evaluation metrics precision, rec</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>I. Dan Melamed. 1998. Annotation style guide for the BLINKER project. Technical Report 98-06, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Merkel</author>
</authors>
<title>Annotation style guide for the PLUG link annotator.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Link¨oping university, Link¨oping,</institution>
<contexts>
<context position="24143" citStr="Merkel (1999)" startWordPosition="3835" endWordPosition="3836">airs from this reference set to create a word alignment gold standard. Some sentence pairs of this set have been chosen randomly, the others are taken from two text passages in the protocol. We had considered choosing sentence pairs that were distributed randomly over the reference set, however, we decided for taking complete text passages in order to make manual annotation easier. This way, the annotators can easily access the context of a sentence pair to resolve alignment ambiguities. Additionally, we have created word alignment guidelines based on those already given by Melamed (1998) and Merkel (1999). We have annotated all 242 sentence pairs twice, and annotation differences are currently being resolved. As this gold standard can only be used to evaluate the performance of English-German word alignment, we will also evaluate our system on the Stockholm parallel treebank (Volk and Samuelsson, 2004). Evaluating against this manually constructed treebank has the advantage that we can evaluate phrase alignment quality, and that we can gather evaluation data for the language pairs English-Swedish and Swedish-German. We have decided to use the evaluation metrics precision, recall and the alignm</context>
</contexts>
<marker>Merkel, 1999</marker>
<rawString>Magnus Merkel. 1999. Annotation style guide for the PLUG link annotator. Technical report, Link¨oping university, Link¨oping, March. PLUG report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stella Neumann</author>
<author>Silvia Hansen-Schirra</author>
</authors>
<title>The CroCo project. Cross-linguistic corpora for the investigateon of explicitation in translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Corpus Linguistics Conference,</booktitle>
<location>Birmingham, UK.</location>
<contexts>
<context position="1597" citStr="Neumann and Hansen-Schirra, 2005" startWordPosition="235" endWordPosition="238">nce tests are promising, and we are setting up a gold standard for a thorough evaluation of the system. 1 Introduction Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. tokens with a frequency below 10. This is a considerable drawback, as rare events make up more than half of the vocabulary of any corpus. Another problem is the correct alignment of multiword units like idioms. Then, differences in word </context>
<context position="9742" citStr="Neumann and Hansen-Schirra, 2005" startWordPosition="1528" endWordPosition="1531">arity or frequency of a word. However, this means that statistical models either have to be abandoned completely, or that effort has to be put in finding a means to combine both alignment approaches into one single, hybrid system. An alternative would be to design a statistical alignment model that is better suited for the Zipfian frequency distributions in the source and target language texts. Research in this direction would greatly benefit from large amounts of high quality example alignments, e.g. taken from the parallel treebanks that are currently being built (Volk and Samuelsson, 2004; Neumann and Hansen-Schirra, 2005). 3.2 Word Order Differences Another problem that has been noticed as early as 1993 with the first research on word alignment (Brown et al., 1993) concerns the differences in word order between source and target language. While simple statistical alignment models like IBM-1 (Brown et al., 1993) and the symmetric alignment approach by Hiemstra (1996) treat sentences as unstructured bags of words, the more sophisticated IBM-models by Brown et al. (1993) approximates word order differences using a statistical distortion factor. Vogel et al. (1999), on the other hand, treat word order differences </context>
</contexts>
<marker>Neumann, Hansen-Schirra, 2005</marker>
<rawString>Stella Neumann and Silvia Hansen-Schirra. 2005. The CroCo project. Cross-linguistic corpora for the investigateon of explicitation in translation. In Proceedings of the Corpus Linguistics Conference, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="24794" citStr="Och and Ney (2000)" startWordPosition="3937" endWordPosition="3940">nce pairs twice, and annotation differences are currently being resolved. As this gold standard can only be used to evaluate the performance of English-German word alignment, we will also evaluate our system on the Stockholm parallel treebank (Volk and Samuelsson, 2004). Evaluating against this manually constructed treebank has the advantage that we can evaluate phrase alignment quality, and that we can gather evaluation data for the language pairs English-Swedish and Swedish-German. We have decided to use the evaluation metrics precision, recall and the alignment error rate (AER) proposed by Och and Ney (2000) in order to compare results to those of other alignment systems. 3We define rare events here as types occurring 2 to 10 times 720 Language Tokens Types Hapax Legomena Rare Events Frequent Types English 29.077,024 101,967 39,200 (38.44%) 35,608 (34.92%) 27,159 (26.64%) German 27.643,792 286,330 140,826 (49.18%) 98,126 (34.27%) 47,378 (16.55%) French 32.439,353 114,891 42,114 (36.66%) 41,194 (35.84%) 31,583 (27.49%) Table 1: Corpus characteristics of the Europarl corpus Language Tokens Types Hapax Legomena Rare Events Frequent Types English 111,222 7,657 3,474 (45.37%) 3,027 (39.53%) 1,156 (15.</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, England.</location>
<contexts>
<context position="22728" citStr="Schmid, 1994" startWordPosition="3603" endWordPosition="3604">rman nouns if the two expressions are roughly of the same length, counted in characters. The results are encouraging, indicating that nominals can be aligned reliably irrespective of their frequencies in the corpus (Schrader, 2006). 6 Data As development corpus, we are using Europarl, a corpus of European Parliament debates (Koehn, 2005). Europarl consists of roughly 30 million tokens per language and is tokenized and sentencealigned. For the purposes of testing ATLAS, we have POS-tagged and lemmatized the German, English, and French parts of the corpus using the freely available tree-tagger (Schmid, 1994). Additionally, we have chunked the German and English texts with an extension of this tool (Schmid, unpublished). Table 1 shows the number of tokens and types of the corpus for all three languages. It also shows the percentages of hapax legomena, rare events3, and all other types of the corpus. 7 Evaluation For evaluating of our text alignment system, we are currently setting up an English-German gold standard: we have randomly chosen a debate protocol of the Europarl corpus that contains approximately 100,000 tokens per language (see table 2), and we corrected its sentence alignment manually</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing, pages 44–49, Manchester, England.</rawString>
</citation>
<citation valid="false">
<authors>
<author>unpublished</author>
</authors>
<title>The IMS Chunker.</title>
<note>unpublished manuscript.</note>
<marker>unpublished, </marker>
<rawString>Helmut Schmid. unpublished. The IMS Chunker. unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bettina Schrader</author>
</authors>
<title>Non-probabilistic alignment of rare German and English nominal expressions. In To appear in:</title>
<date>2006</date>
<booktitle>Proceedings of the Fifth Language Resources and Evaluation Conference (LREC),</booktitle>
<location>Genoa, Italy.</location>
<note>to appear.</note>
<contexts>
<context position="22346" citStr="Schrader, 2006" startWordPosition="3543" endWordPosition="3544">words to their translations. This similarity measure may be language-pair specific, or it may be defined globally, i.e. it will be used for any language pair. We have already tested such a procedure for aligning English nominal multiwords with their German translations: In this procedure, English nominals are detected based on their typical partof-speech patterns, and aligned to German nouns if the two expressions are roughly of the same length, counted in characters. The results are encouraging, indicating that nominals can be aligned reliably irrespective of their frequencies in the corpus (Schrader, 2006). 6 Data As development corpus, we are using Europarl, a corpus of European Parliament debates (Koehn, 2005). Europarl consists of roughly 30 million tokens per language and is tokenized and sentencealigned. For the purposes of testing ATLAS, we have POS-tagged and lemmatized the German, English, and French parts of the corpus using the freely available tree-tagger (Schmid, 1994). Additionally, we have chunked the German and English texts with an extension of this tool (Schmid, unpublished). Table 1 shows the number of tokens and types of the corpus for all three languages. It also shows the p</context>
</contexts>
<marker>Schrader, 2006</marker>
<rawString>Bettina Schrader. 2006. Non-probabilistic alignment of rare German and English nominal expressions. In To appear in: Proceedings of the Fifth Language Resources and Evaluation Conference (LREC), Genoa, Italy. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>G F Foster</author>
<author>P Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International conference on theoretical and methodological issues in Machine translation,</booktitle>
<pages>67--81</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1748" citStr="Simard et al., 1992" startWordPosition="261" endWordPosition="264">tting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. tokens with a frequency below 10. This is a considerable drawback, as rare events make up more than half of the vocabulary of any corpus. Another problem is the correct alignment of multiword units like idioms. Then, differences in word order are not modelled well by the statistical algorithms. In order to find solutions to these problems, we have developed a hybrid alignment architect</context>
<context position="5054" citStr="Simard et al., 1992" startWordPosition="783" endWordPosition="786">Language-independence has also been favoured with respect to linguistic resources in that alignment should do without e.g. using pre-existing dictionaries. Hence there is a dominance of purely statistical approaches. 2.1 Sentence Alignment Sentence alignment strategies fall roughly into three categories: length-based approaches (Gale and Church, 1991; Gale and Church, 1993) are based on the assumption that the length proportions of a sentence and its translation are roughly the same. Anchor-based algorithms align sentences based on cues like corpus-specific markup and orthographic similarity (Simard et al., 1992). The third approach uses bilingual lexical information, e.g. estimated from the corpus (Kay and R¨oscheisen, 1993; Fung and Church, 1994; Fung and McKeown, 1994). Hybrid methods (Tschorn, 2002) combine these standard approaches such that the shortcomings of one approach are counterbalanced by the strength of another component: length-based methods are very sensitive towards deletions in that a single omission can cause the alignment to go on a wrong track from the point where it occurred to the end of the corpus. Strategies that assume that orthographic similarity entails translational equiva</context>
<context position="20500" citStr="Simard et al (1992)" startWordPosition="3254" endWordPosition="3257">g. will be translated as a noun, or if it is not, it is highly probable that it is translated as an adjective or verb, but not as a functional class member like a preposition. Likewise, dictionary lookup may be used, and is used by or system, to align words within sentences or phrases. We have also implemented a module that aligns sentences and words based on string similarity constrained by syntactic categories: the module exploits the part of speech annotation to align sentences and words based on string similarity between nouns, adjectives, and verbs, thus modifying the classic approach by Simard et al (1992). The advantage of the modification is that the amount of cognates within lexical class words will be considerably higher than between prepositions, determiners, etc., hence filtering by word 719 category yields good results. 5.2 Word Order Differences As ATLAS supports the alignment of phrases, mismatches between the linear orderings of source and target language words become irrelevant. Additionally, phrase alignment can considerably narrow down the search space within which to find the translation of a word. If e.g. a noun phrase has already been aligned to its equivalent in the other langu</context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>Michel Simard, G. F. Foster, and P. Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proceedings of the Fourth International conference on theoretical and methodological issues in Machine translation, pages 67–81, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="1462" citStr="Smadja et al., 1996" startWordPosition="215" endWordPosition="218">lignment of rare words and multiwords, or how to align despite syntactic differences between two languages. First performance tests are promising, and we are setting up a gold standard for a thorough evaluation of the system. 1 Introduction Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. tokens with a frequency below 10. This is a considerable drawback, as rare events make up more than ha</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Word alignment - step by step.</title>
<date>1999</date>
<booktitle>In Proceedings of the 12th Nordic Conference on Computational Linguistics,</booktitle>
<pages>216--227</pages>
<location>Trondheim,</location>
<contexts>
<context position="11755" citStr="Tiedemann, 1999" startWordPosition="1848" endWordPosition="1849">ll for language pairs where syntactic and functional differences are greater than between e.g. English and French. 3.3 Multiword alignment Like syntactic differences, n:m correspondences, i.e. alignments that involve multiword expressions, have soon been noted as being difficult for statistical word alignment: Brown et al. (1993) modelled fertility, as they called it, statistically in the more sophisticated IBM-models. Other approaches adopt again a normalizing procedure: in a preprocessing step, multiwords are either recognized as such and subsequently treated as if they were a single token (Tiedemann, 1999), or, reversely, the tokens they align to may be split into their components, with the components being aligned to the parts of the corresponding multiword expression on a 1:1 basis. The latter approach is clearly insufficient for word alignment quality: it assumes that compositionality holds for both the multiword unit and its translation, i.e. that the meaning of the whole unit is made up of the meaning of its part. This clearly need not be the case, and further problems arise when a multiword unit and its translation contain different numbers of elements. The former approach, i.e. of recogn</context>
</contexts>
<marker>Tiedemann, 1999</marker>
<rawString>J¨org Tiedemann. 1999. Word alignment - step by step. In Proceedings of the 12th Nordic Conference on Computational Linguistics, pages 216–227, Trondheim, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Combining clues for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the ACL (EACL03),</booktitle>
<pages>339--346</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="7026" citStr="Tiedemann (2003)" startWordPosition="1096" endWordPosition="1097"> translation for each source language word. The probability of some target language word to be a translation of a source language word then depends on the frequency with which both co-occur at the same or similar positions in the parallel corpus. The probabilities are estimated from the using the EM-algorithm1, and a Viterbi search is carried out to compute the most probable sequence of word translation pairs. Word order differences between the two languages are modelled by using statistical weights, and multiword units are similarly treated. Another approach to word alignment is presented by Tiedemann (2003), where alignment probabilities are computed using a combination of features like e.g. co-occurrence, cognateness, syntactic category membership. However, although the alignment is partly based on linguistic features, its computation is entirely statistical. Other word alignment strategies (Toutanova et al., 2002; Cherry and Lin, 2003) have also begun to incorporate linguistic knowledge. Unfortunately, the basic, statistical, assumptions have not been changed, and hence no sufficient solution to the shortcomings of the early alignment models have been found. 3 Shortcomings of the statistical a</context>
</contexts>
<marker>Tiedemann, 2003</marker>
<rawString>J¨org Tiedemann. 2003. Combining clues for word alignment. In Proceedings of the 10th Conference of the European Chapter of the ACL (EACL03), pages 339 – 346, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>H Tolga Ilhan</author>
<author>Christopher D Manning</author>
</authors>
<title>Extensions to HMM-based statistical word alignment models.</title>
<date>2002</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>87--94</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1899" citStr="Toutanova et al., 2002" startWordPosition="285" endWordPosition="288">e of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. tokens with a frequency below 10. This is a considerable drawback, as rare events make up more than half of the vocabulary of any corpus. Another problem is the correct alignment of multiword units like idioms. Then, differences in word order are not modelled well by the statistical algorithms. In order to find solutions to these problems, we have developed a hybrid alignment architecture: it uses statistical information extracted directly from a corpus, and rules or heuristics based on the linguistic information as given by the corp</context>
<context position="7340" citStr="Toutanova et al., 2002" startWordPosition="1136" endWordPosition="1139">hm1, and a Viterbi search is carried out to compute the most probable sequence of word translation pairs. Word order differences between the two languages are modelled by using statistical weights, and multiword units are similarly treated. Another approach to word alignment is presented by Tiedemann (2003), where alignment probabilities are computed using a combination of features like e.g. co-occurrence, cognateness, syntactic category membership. However, although the alignment is partly based on linguistic features, its computation is entirely statistical. Other word alignment strategies (Toutanova et al., 2002; Cherry and Lin, 2003) have also begun to incorporate linguistic knowledge. Unfortunately, the basic, statistical, assumptions have not been changed, and hence no sufficient solution to the shortcomings of the early alignment models have been found. 3 Shortcomings of the statistical alignment approaches While sentence alignment can be done successfully using a combination of the existing algorithms, word alignment quality suffers due to three problematic phenomena: the amount of rare 1see (Manning and Sch¨utze, 1999), chapter 14.2.2 for a general introduction 716 words typically found in corp</context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Manning. 2002. Extensions to HMM-based statistical word alignment models. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 87–94, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Tschorn</author>
</authors>
<title>Automatically aligning English-German parallel texts at sentence level using linguistic knowledge. Master’s thesis,</title>
<date>2002</date>
<institution>Universit¨at Osnabr¨uck.</institution>
<contexts>
<context position="5248" citStr="Tschorn, 2002" startWordPosition="814" endWordPosition="815">ical approaches. 2.1 Sentence Alignment Sentence alignment strategies fall roughly into three categories: length-based approaches (Gale and Church, 1991; Gale and Church, 1993) are based on the assumption that the length proportions of a sentence and its translation are roughly the same. Anchor-based algorithms align sentences based on cues like corpus-specific markup and orthographic similarity (Simard et al., 1992). The third approach uses bilingual lexical information, e.g. estimated from the corpus (Kay and R¨oscheisen, 1993; Fung and Church, 1994; Fung and McKeown, 1994). Hybrid methods (Tschorn, 2002) combine these standard approaches such that the shortcomings of one approach are counterbalanced by the strength of another component: length-based methods are very sensitive towards deletions in that a single omission can cause the alignment to go on a wrong track from the point where it occurred to the end of the corpus. Strategies that assume that orthographic similarity entails translational equivalence rely on the relatedness of the language pair in question. In closely-related languages like English and French, the amount of orthographically similar words that share the same meaning is </context>
</contexts>
<marker>Tschorn, 2002</marker>
<rawString>Patrick Tschorn. 2002. Automatically aligning English-German parallel texts at sentence level using linguistic knowledge. Master’s thesis, Universit¨at Osnabr¨uck.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1874" citStr="Vogel et al., 1999" startWordPosition="281" endWordPosition="284">sing step for a range of applications, including but not limited to machine translation (Brown et al., 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al., 1996) and induction of NLP-tools (Kuhn, 2004). Aligned corpora can be also be used in translation studies (Neumann and Hansen-Schirra, 2005). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simard et al., 1992). Word alignment, however, is almost exclusively done using statistics (Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999; Toutanova et al., 2002). Hence it is difficult to align so-called rare events, i.e. tokens with a frequency below 10. This is a considerable drawback, as rare events make up more than half of the vocabulary of any corpus. Another problem is the correct alignment of multiword units like idioms. Then, differences in word order are not modelled well by the statistical algorithms. In order to find solutions to these problems, we have developed a hybrid alignment architecture: it uses statistical information extracted directly from a corpus, and rules or heuristics based on the linguistic informa</context>
<context position="6346" citStr="Vogel et al., 1999" startWordPosition="981" endWordPosition="984">y-related languages like English and French, the amount of orthographically similar words that share the same meaning is higher than in unrelated languages like English and Chinese, were orthographic or even phonetic similarity may only indicate translational equivalence for names. Strategies that use system-external dictionaries, finally, can only be used if a large-enough dictionary exists for a specific language pair. 2.2 Word Alignment Aligning below the sentence level is usually done using statistical models for machine translation (Brown et al., 1991; Brown et al., 1993; Hiemstra, 1996; Vogel et al., 1999) where any word of the target language is taken to be a possible translation for each source language word. The probability of some target language word to be a translation of a source language word then depends on the frequency with which both co-occur at the same or similar positions in the parallel corpus. The probabilities are estimated from the using the EM-algorithm1, and a Viterbi search is carried out to compute the most probable sequence of word translation pairs. Word order differences between the two languages are modelled by using statistical weights, and multiword units are simila</context>
<context position="10292" citStr="Vogel et al. (1999)" startWordPosition="1615" endWordPosition="1618">g built (Volk and Samuelsson, 2004; Neumann and Hansen-Schirra, 2005). 3.2 Word Order Differences Another problem that has been noticed as early as 1993 with the first research on word alignment (Brown et al., 1993) concerns the differences in word order between source and target language. While simple statistical alignment models like IBM-1 (Brown et al., 1993) and the symmetric alignment approach by Hiemstra (1996) treat sentences as unstructured bags of words, the more sophisticated IBM-models by Brown et al. (1993) approximates word order differences using a statistical distortion factor. Vogel et al. (1999), on the other hand, treat word order differences as a local phenomenon that can be modelled within a window of no more than three words. Recently, researchers like Cherry and Lin (2003) have begun to use syntactic analyses to guide and restrict the word alignment process. The advantage of using available syntactic information for word alignment is that it helps to overcome data sparseness: although a token may be rare, its syntactic category may not, and hence there may be sufficient statistical information to align at the phrase level. Subsequently, the phrase level information can be used t</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1999</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1999. HMM-based word alignment in statistical translation. In Proceedings of the International Conference on Computational Linguistics, pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
<author>Yvonne Samuelsson</author>
</authors>
<title>Bootstrapping parallel treebanks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Linguistically Interpreted Corpora (LINC) at COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="9707" citStr="Volk and Samuelsson, 2004" startWordPosition="1524" endWordPosition="1527">at are insensitive to the rarity or frequency of a word. However, this means that statistical models either have to be abandoned completely, or that effort has to be put in finding a means to combine both alignment approaches into one single, hybrid system. An alternative would be to design a statistical alignment model that is better suited for the Zipfian frequency distributions in the source and target language texts. Research in this direction would greatly benefit from large amounts of high quality example alignments, e.g. taken from the parallel treebanks that are currently being built (Volk and Samuelsson, 2004; Neumann and Hansen-Schirra, 2005). 3.2 Word Order Differences Another problem that has been noticed as early as 1993 with the first research on word alignment (Brown et al., 1993) concerns the differences in word order between source and target language. While simple statistical alignment models like IBM-1 (Brown et al., 1993) and the symmetric alignment approach by Hiemstra (1996) treat sentences as unstructured bags of words, the more sophisticated IBM-models by Brown et al. (1993) approximates word order differences using a statistical distortion factor. Vogel et al. (1999), on the other </context>
<context position="24446" citStr="Volk and Samuelsson, 2004" startWordPosition="3882" endWordPosition="3886">wever, we decided for taking complete text passages in order to make manual annotation easier. This way, the annotators can easily access the context of a sentence pair to resolve alignment ambiguities. Additionally, we have created word alignment guidelines based on those already given by Melamed (1998) and Merkel (1999). We have annotated all 242 sentence pairs twice, and annotation differences are currently being resolved. As this gold standard can only be used to evaluate the performance of English-German word alignment, we will also evaluate our system on the Stockholm parallel treebank (Volk and Samuelsson, 2004). Evaluating against this manually constructed treebank has the advantage that we can evaluate phrase alignment quality, and that we can gather evaluation data for the language pairs English-Swedish and Swedish-German. We have decided to use the evaluation metrics precision, recall and the alignment error rate (AER) proposed by Och and Ney (2000) in order to compare results to those of other alignment systems. 3We define rare events here as types occurring 2 to 10 times 720 Language Tokens Types Hapax Legomena Rare Events Frequent Types English 29.077,024 101,967 39,200 (38.44%) 35,608 (34.92%</context>
</contexts>
<marker>Volk, Samuelsson, 2004</marker>
<rawString>Martin Volk and Yvonne Samuelsson. 2004. Bootstrapping parallel treebanks. In Proceedings of the Workshop on Linguistically Interpreted Corpora (LINC) at COLING, Geneva, Switzerland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>