<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013264">
<title confidence="0.9871215">
Semantic vs. Syntactic vs. N-gram Structure
for Machine Translation Evaluation
</title>
<author confidence="0.891159">
Chi-kiu Lo and Dekai WU
</author>
<affiliation confidence="0.920448">
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
</affiliation>
<email confidence="0.991916">
jackielo,dekai @cs.ust.hk
</email>
<sectionHeader confidence="0.995478" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999373612903226">
We present results of an empirical study
on evaluating the utility of the machine
translation output, by assessing the accu-
racy with which human readers are able
to complete the semantic role annotation
templates. Unlike the widely-used lexi-
cal and n-gram based or syntactic based
MT evaluation metrics which are fluency-
oriented, our results show that using se-
mantic role labels to evaluate the utility
of MT output achieve higher correlation
with human judgments on adequacy. In
this study, human readers were employed
to identify the semantic role labels in the
translation. For each role, the filler is
considered an accurate translation if it ex-
presses the same meaning as that anno-
tated in the gold standard reference trans-
lation. Our SRL based f-score evaluation
metric has a 0.41 correlation coefficient
with the human judgement on adequacy,
while in contrast BLEU has only a 0.25
correlation coefficient and the syntactic
based MT evaluation metric STM has only
0.32 correlation coefficient with the hu-
man judgement on adequacy. Our results
strongly indicate that using semantic role
labels for MT evaluation can be signifi-
cantly more effective and better correlated
with human judgement on adequacy than
BLEU and STM.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933023809524">
In this paper, we show that evaluating machine
translation quality by assessing the accuracy of
human performance in reconstructing the seman-
tic frames from the MT output has a higher cor-
relation with human judgment on translation ad-
equacy than (1) the widely-used lexical n-gram
precision based MT evaluation metric, BLEU (Pa-
pineni et al., 2002), as well as (2) the best-known
syntactic tree precision based MT evaluation met-
ric, STM (Liu and Gildea, 2005). At the same
time, unlike some highly labor intensive evalua-
tion metrics such as HTER (Snover et al., 2006),
our proposed semantic metric only requires sim-
ple and minimal instructions to the human judges
involved in the evaluation cycle.
We argue that neither n-gram based metrics,
like BLEU, nor syntax-based metrics, like STM,
adequately capture the similarity in meaning be-
tween the machine translation and the reference
translation—which, ultimately, is essential for
translations to be useful.
First, n-gram based metrics assume that &amp;quot;good&amp;quot;
translations share the same lexical choices with
the reference translation. While BLEU score per-
forms well in capturing the translation fluency,
Callison-Burch et al. (2006) and Koehn and Monz
(2006) report cases where BLEU strongly dis-
agrees with human judgment on translation qual-
ity. The underlying reason is that lexical similarity
does not adequately reflect the similarity in mean-
ing.
Second, just like n-gram based metrics
such as BLEU, syntax-based metrics are still
more fluency-oriented than adequacy/accuracy-
oriented. While STM addresses the failure of
BLEU in evaluating the translation grammati-
cality, a grammatical translation can nonetheless
achieve a high STM score even if contains errors
arising from confusion of semantic roles. Syntac-
tic structure similarity still inadequately reflects
similarity of meaning.
As MT systems improve, the shortcomings of
</bodyText>
<page confidence="0.984516">
52
</page>
<note confidence="0.9841935">
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 52–60,
COLING 2010, Beijing, August 2010.
</note>
<figureCaption confidence="0.999943">
Figure 1: Example of semantic frames in Chinese input, English reference translation and MT output.
</figureCaption>
<bodyText confidence="0.999898">
lexical n-gram based and syntax-based evalua-
tion metrics are becoming more apparent. State-
of-the-art MT systems are often able to output
translations containing roughly the correct words
and being almost grammatical, but not express-
ing meaning that is close to the source input. We
adopt the outset of the principle that a good trans-
lation is one from which human readers may suc-
cessfully understand at least the basic event struc-
ture — &amp;quot;who did what to whom, when, where and
why&amp;quot; (Pradhan et al., 2004) which represents the
most important meaning of the source utterances.
Our objective is to evaluate how well the most es-
sential semantic information is being captured by
the machine translation systems from the user&apos;s
point of view.
In this paper, we describe in detail the method-
ology that underlies the new semantic machine
translation evaluation metrics we are developing.
We present the results of the study on evaluating
machine translation utility by measuring the accu-
racy with which human readers are able to com-
plete the semantic role annotation templates. Last
but not the least, we show that our proposed eval-
uation metric has a higher correlation with human
judgments on adequacy than BLEU and STM.
</bodyText>
<sectionHeader confidence="0.999599" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.998416">
2.1 Semantic models in SMT
</subsectionHeader>
<bodyText confidence="0.99996488">
Numerous recent works has been done on apply-
ing different semantic models to statistical ma-
chine translation. Word sense disambiguation
(WSD) models combine a wide range of context
features into a single lexical choice prediction, as
in the work of Carpuat and Wu (2007), Chan et al.
(2007), and Gimenez and Marquez (2007a). In
particular, Phrase Sense Disambiguation (PSD),
a generalization of the WSD approach, automat-
ically acquires fully phrasal translation lexicons
and provides a context-dependent probability dis-
tribution over the possible translation candidates
for any given phrasal lexicon (Carpuat and Wu,
2007).
Another recent research direction on semantic
SMT is applying semantic role labeling models.
Semantic role labeling (SRL) is the task of identi-
fying the semantic predicate-argument structures
within a sentence. Semantic role labels repre-
sent an abstract level of understanding in mean-
ing. There is an increasing availability of large
parallel corpora annotated with semantic role in-
formation, in particular, in the work of Palmer et
al. (2005) and Xue and Palmer (2005). As a result,
the accuracy of automatic SRL task is also rising.
</bodyText>
<page confidence="0.99781">
53
</page>
<bodyText confidence="0.979846378787879">
The best monolingual shallow semantic parser by
Fung et al. (2006) achieved an F-score of 82.01
in Chinese semantic role labeling, while the best
cross-lingual semantic verb frame argument map-
pings with accuracy of 89.3% as reported in the
same work.
The example in Figure 1 is labeled with seman-
tic roles in the Propbank convention. sre shows a
fragment of a typical Chinese source sentence that
is drawn from newswire genre of the evaluation
corpus. ref shows the corresponding fragment of
the English reference translation. MT1, MT2 and
MT3 show the three corresponding fragments of
the machine translation output from three differ-
ent MT systems.
A relevant subset of the semantic roles and
predicates has been annotated in these fragments,
using the PropBank convention of OntoNotes. In
the Chinese source sentence, there are two main
verbs marked PRED. The first verb &amp;quot; &amp;quot; (cease
of sales) has three arguments: one in ARG1 expe-
riencer role, &amp;quot; &amp;quot; (the com-
plete range of SK-II products); one in ARGM-
LOC location role, &amp;quot; &amp;quot; (in mainland
China), and one in ARGM-EXT extent role, &amp;quot;
&amp;quot; (for almost two months). The second
verb &amp;quot; &amp;quot; (resumed) also has three arguments:
two in ARG0 agent roles, &amp;quot;
&amp;quot; (the
complete range of SK-II products which sales had
ceased in mainland China for almost two months)
and &amp;quot; &amp;quot; (sales), and one in ARGM-ADV role,
&amp;quot; &amp;quot; (until then).
In the corresponding English target, there are
also two main verbs marked PRED. The first
verb (ceased) has three arguments: one in an
ARG1 experiencer role, &amp;quot;their sales&amp;quot;; one in an
ARGM-LOC role, &amp;quot;in mainland China&amp;quot;, and one
in ARGM-TMP temporal role, &amp;quot;for almost two
months&amp;quot;. The second verb (resumed) also has
three arguments: two in ARGM-TMP temporal
roles, &amp;quot;until after their sales ceased in mainland
China for almost two months&amp;quot; and &amp;quot;now&amp;quot;, and one
in ARG1 experiencer role, &amp;quot;sales of the complete
range of SK-II products&amp;quot;.
Similarly, the first two MT outputs are also an-
notated with semantic roles in the PropBank con-
vention. Since there is no verb appeared in the
third MT output, no predicate-argument structure
is annotated.
Recent work by Wu and Fung (2009a) and Wu
and Fung (2009b) has begun to apply SRL to sta-
tistical machine translation using a semantic re-
ordering model based on SRL that successfully re-
turns a better translation with fewer semantic role
confusion errors.
With recent rise of work applying semantic
model to statistical machine translation, there is
a high demand for MT evaluation metrics that
are directly sensitive to the semantic improvement
made. We believe evaluating machine translation
utility based on semantic roles should reflect se-
mantic improvement better than current widely-
used automated n-gram precision based MT eval-
uation metrics, like BLEU or fluency-oriented
syntactic MT evaluation metrics, like STM.
</bodyText>
<subsectionHeader confidence="0.924668">
2.2 STM: syntax-based MT evaluation
</subsectionHeader>
<bodyText confidence="0.999647583333333">
Liu and Gildea (2005) proposed to use syntactic
features in MT evaluation and developed subtree
metric (STM) which based on the similarity of
syntax tree of the MT output and that of the ref-
erence. It is the first proposed metric that incor-
porates syntactic features in MT evaluation and
underlies all the other recently proposed syntac-
tic MT evaluation metrics.
STM is a precision based metric that captures
the fractions of the subtree in a specific depth of
the MT output syntax tree which also appear in the
reference syntax tree. The fractions of different
depths are then average in arithmetic mean.
where is the maximum depth of subtress con-
sidered, denotes the number of times
subtree appears in the MT output&apos;s syntax tree,
and denotes the found number of
times appears in the references&apos; syntax tree, each
subtree in reference will only be found once.
Figure 2 shows the syntax tree of a reference
translation and that of the corresponding MT out-
put. For example, we set the maximum depth of
subtree considered to 4. There are seven 1-depth
subtrees in the MT output (S, NP, VP, PRP, V,
</bodyText>
<page confidence="0.996958">
54
</page>
<figureCaption confidence="0.999092">
Figure 2: Example for the computation of STM
</figureCaption>
<bodyText confidence="0.989269461538462">
NP and PRP) in which only six of them appear in
the references (S, NP, VP, PRP, V and NP). Note
that the found count of PRP should be 1 rather
than 2 because there is only one PRP in the ref-
erence translation syntax tree. For 2-depth, there
are four subtrees in the MT output (S NP VP,
NP PRP, VP V NP and NP PRP) in which
three of them appear in the reference (S NP VP,
NP PRP and VP V NP). Similarly, there are
one out of two 3-depth subtrees and zero out of
one 4-depth subtrees in the MT output found in
the reference. Therefore, the final STM score for
this example is (6/7+3/4+1/2+0/1)/4=0.527.
2.3 MT evaluation metric based on semantic
role overlap
Gim6nez and Marquez (2008) introduced ULC, a
new automatic MT evaluation metric in which a
series of linguistic features are combined together.
One of those linguistic features is shallow seman-
tic similarity on semantic role overlap. The se-
mantic role overlap metric calculates the lexical
overlapping between semantic roles of the same
type in the machine translation output and the cor-
responding reference translations and then consid-
ers the average lexical overlapping over all seman-
tic role types.
Despite the fact that the metric shows an im-
proved correlation with human judgment of trans-
lation quality (Gim6nez and Marquez, 2007b,
2008; Callison-Burch et al., 2007, 2008), it is
not commonly used in large-scale MT evaluation
campaign. The reason may lie in the high time
cost.
We believe it is important to first focus on de-
veloping simple measures to evaluate machine
translation utility, that make use of human extrac-
tion of role information. It is necessary to first un-
derstand the upper bounds of human performance
on this task, as a foundation for better design of
efficient automated metrics.
2.4 HTER: non-automated MT evaluation
metric
Human-targeted Translation Edit Rate (HTER)
in the work of Snover et al. (2006) is a non-
automatic machine translation evaluation metric
based on the number of edits required to correct
the translation hypotheses. A human annotator
edits each MT hypothesis so that it is meaning-
equivalent with the reference translation. It em-
phasizes on making the minimum possible num-
ber of edits. The Translation Edit Rate (TER) is
then calculated using the human-edited translation
as a targeted reference for the MT hypothesis.
The HTER is highly labor intensive in the
evaluation process. The human annotators are
not only required to understand the meaning ex-
pressed in the reference translation and the ma-
chine translation, but are also required to propose
minimum possible number of edits to the trans-
lation hypotheses. With such heavy-duty human
decision requirements, the cost in evaluation is
enormously increased, bottlenecking the evalua-
tion cycle. Instead, we believe that any human de-
cisions in the evaluation cycle should be reduced
to be as simple as possible.
</bodyText>
<sectionHeader confidence="0.941075" genericHeader="method">
3 Semantic role translation accuracy
</sectionHeader>
<bodyText confidence="0.9998946">
To evaluate the semantic utility of machine trans-
lation output, we conduct a comparative analysis
on the Propbank annotation templates completed
by the human readers in the machine translation
output versus the reference translation.
</bodyText>
<subsectionHeader confidence="0.988563">
3.1 Evaluation corpus
</subsectionHeader>
<bodyText confidence="0.999346333333333">
The sentences of the evaluation corpus are ran-
domly drawn from the newswire genre of the
DARPA GALE program Phase 2.5 evaluation. For
each Chinese input sentence, there are one corre-
sponding English reference translation and three
state-of-the-art machine translation systems&apos; out-
puts. The Chinese source and the English refer-
ence are annotated with gold standard semantic
role labels in Propbank style.
</bodyText>
<page confidence="0.996853">
55
</page>
<figureCaption confidence="0.979111">
Figure 3: Example given to human annotators demonstrating how to label the semantic frames.
</figureCaption>
<tableCaption confidence="0.9987695">
Table 1: List of semantic roles that human judges
are requested to label.
</tableCaption>
<table confidence="0.9984252">
Label Event Label Event
Actor who Temporal when
Action did Location where
Experiencer what Other adverbial arg. why / how
Patient whom
</table>
<bodyText confidence="0.993037142857143">
&amp;quot;Other adverbial argument&amp;quot; label. Human anno-
tators are given simple and minimal instructions
on what to label and two examples demonstrating
how to label. Table 1 shows the list of labels an-
notators are requested to annotate. Figure 3 shows
the example shown to the human annotators on
how to label semantic frames.
</bodyText>
<subsectionHeader confidence="0.761452">
3.2 Reconstruction of semantic frames in
MT output
</subsectionHeader>
<bodyText confidence="0.999939133333333">
Four groups of bilingual Chinese English human
annotators are employed to conduct the analysis.
One group of them is given the reference transla-
tion. This sanity check serves as the control con-
dition of the analysis. The other three groups of
them is given one set of the machine translation
system output. The four groups are all disjoint
such that no annotators annotate more than one
sentence from a MT-reference set to avoid con-
tamination in annotators&apos; judgments. To reduce
the effect of personal bias on annotations, each
sentence is annotated by at least two human an-
notators. The results are reported as the average
among the annotators.
With the aim of evaluating machine translation
utility from a user standpoint, we have simpli-
fied the Propbank annotation into a more intu-
itive event structure, i.e. &amp;quot;who did what to whom,
when, where, why and how&amp;quot;. Since the layman
annotators find that it is difficult to distinguish be-
tween the &amp;quot;why&amp;quot; and &amp;quot;how&amp;quot; events type, we have
combined the &amp;quot;why&amp;quot; and &amp;quot;how&amp;quot; events in to one
After reconstruction of the semantic frames,
the annotated machine translation outputs are dis-
tributed to another disjoint group of three mono-
lingual human judges. The human judges are re-
quired to match each predicate in the reference
translation with those annotated in the MT out-
put. Then, for each matched predicate, they are
required to judge whether each of the associated
argument in the reference translation is translated
and annotated in the MT output: Correct, Incor-
rect or Partial. Translations of the semantic frames
are judged Correct if they express the same mean-
ing as that of the reference translations or the orig-
inal source input. Translations of the semantic
frames are judged Incorrect if they express mean-
ing(s) that belongs in other arguments. Transla-
tion of the semantic frames may also be judged
Partial if only part of the meaning is correctly ex-
pressed. Extra meaning in the semantic frames
will not be penalized unless it belongs in another
argument. The partially correct category is de-
signed to facilitate a finer-grained measurement of
the translation utility.
</bodyText>
<page confidence="0.991624">
56
</page>
<subsectionHeader confidence="0.937119">
3.3 SRL based evaluation metric
</subsectionHeader>
<bodyText confidence="0.935641375">
Based on the comparative matrices collected from
the human judges, a precision-recall analysis of
accuracy with the reconstructed semantic frames,
reflecting the utility of each machine translation
system could be done.
and represent the number of cor-
rectly translated core arguments and adjunct argu-
ments of a matched predicate respectively while
and represent the number of par-
tially translated core arguments and adjunct ar-
guments of a matched predicate . and
represent the total number of core argu-
ments and adjunct arguments of the matched pred-
icate in the MT output and and
represent the total number of core arguments and
adjunct arguments of the matched predicate in
the reference.
and are the sum of the
portions of correctly or partial correctly trans-
lated predicate-argument structures in the MT out-
put. They can be viewed as the true positive
Table 2: SRL annotation of example 1 MT1 out-
put in figure 1 and the human judgement on trans-
lation correctness of each argument.
</bodyText>
<table confidence="0.99764875">
SRL reference MT1 decision
PRED ceased — not match
PRED resumed resume match
ARG0 — s� - ii the sale incorrect
of products in
the mainland of
China
ARG1 sales of com- sales partial
plete range of
SK - II products
TMP Until after, their So far , nearly partial
sales had ceased two months
in mainland
China for almost
two months
TMP now — incorrect
</table>
<bodyText confidence="0.998246703703704">
for precision. and are the sum of
the portion of correctly or partial correctly trans-
lated predicate-argument structures in the refer-
ence. They can be viewed as the true positive for
recall. Note that , and are the weights
for the matched predicate, core arguments and ad-
junct arguments. These weights can be viewed as
the importance of meanings in the different cat-
egories of semantic roles. In this very first pre-
liminary study, we have set them all to 1 and we
expect tuning these weights can further increase
the correlation of the evaluation metric with hu-
man judgment of translation utility.
The precision, recall and f-score of the SRL
based MT evaluation metric are defined in terms
of the translation accuracy of predicate-argument
structures. Note that is the weights for the
partially correct translated arguments. In this ex-
periment, we have arbitrarily set it to 0.5.
If all the reconstructed semantic frames in the
MT output are completely identical to the gold
standard annotation in the reference translation
and all the arguments in the reconstructed frames
express the same meaning as the corresponding
arguments in the reference translations, the f-score
of the SRL based MT evaluation metric will be
equal to 1.
</bodyText>
<subsectionHeader confidence="0.908165">
3.4 Experiment and Results
</subsectionHeader>
<bodyText confidence="0.999789">
Table 2 shows the SRL annotation of MT1 by
one of the annotators of example 1 in figure 1
</bodyText>
<page confidence="0.999609">
57
</page>
<tableCaption confidence="0.9980615">
Table 3: SRL based MT evaluation average on all
annotators and all sentences.
</tableCaption>
<table confidence="0.999397">
System Precision Recall F-score
Reference 0.75 0.73 0.73
MT1 0.39 0.35 0.36
MT2 0.37 0.31 0.33
MT3 0.34 0.30 0.30
</table>
<bodyText confidence="0.9998646">
and the human judgement on translation correct-
ness of each argument. The predicate &amp;quot;ceased&amp;quot; in
the reference translation did not match with any
predicate annotated in MT1 while the predicate
&amp;quot;resumed&amp;quot; matched with the predicate &amp;quot;resume&amp;quot;
annotated in MT1. The ARGM-TMP argument,
&amp;quot;Until after their sales had ceased in mainland
China for almost two months&amp;quot;, in the reference
translation is partially translated to ARGM-TMP
argument, &amp;quot;So far, nearly two months&amp;quot;, in MT1;
the ARG1 argument, &amp;quot;sales of the complete range
of SK - II products&amp;quot;, in the reference translation
is partially translated to ARG1 argument, &amp;quot;sales&amp;quot;,
in MT1 and the ARGM-TMP argument, &amp;quot;now&amp;quot; in
the reference translation is missing in MT1. The
SRL based f-score of this example is 0.33. The fi-
nal sentence-level SRL based MT evaluation met-
ric of MT1 is the f-score averaged on all annota-
tors. Table 3 shows the results of the SRL based
MT evaluation metric averaged on all annotators
and all sentences. Our results show that the evalu-
ation metric can successfully distinguish the trans-
lation utility of the human translation and the three
MT systems; and on system level, MT1 provides
the most accurate translation.
</bodyText>
<sectionHeader confidence="0.998275" genericHeader="method">
4 Inter-annotator Agreement
</sectionHeader>
<bodyText confidence="0.985469618181818">
We measured the inter-annotator agreement in two
tasks: role identification and role classification.
The standard f-score is used to measure the agree-
ment on SRL annotation as in Brants (2000).
For role identification, the agreement is counted
on the matching of word span in the annotated
arguments with a tolerance of 1 word in mis-
match. The tolerance is designed for the fact that
annotators are not consistent in handling the arti-
cles or punctuations at the beginning or the end of
the annotated arguments. The agreement rate on
SRL annotations in role identification of reference
translation is 76%, and that on MT output is 72%.
For role classification, in addition to the re-
quirement of matching of word span in role iden-
tification task, the agreement is counted on the
matching of the semantic role labels within two
aligned word spans. The agreement rate on SRL
annotations of reference translation and that on
MT output are 69% and 65% respectively.
The results show that with such minimal train-
ing, the layman annotators perform consistently
in identifying the semantic structure in both the
reference translation and the MT output. The re-
sults suggest that the layman annotators also hav-
ing problem in role confusion and we believe that
a slightly more detailed explanation on the role la-
bels may help to clear the confusion.
5 Correlation with human judgments on
translation adequacy
We used the Spearman&apos;s rank correlation coeffi-
cient to measure the correlation of the eval-
uation metrics with the human judgment on ad-
equacy at sentence-level and took average on the
whole data set. The human judgment on adequacy
was obtained by showing all three MT outputs to-
gether with the Chinese source input to a human
reader. The human reader was instructed to or-
der the sentences from the three MT systems ac-
cording to the accuracy of meaning in the trans-
lations. For the MT output, we ranked the sen-
tences from the three MT systems according to
the raw scores of the evaluation metrics. The
STM scores are calculated based on the syntax
tree of the reference and MT output parsed by
the Charniak parser (Charniak, 2001). Table 4
shows the raw scores of example 1 under the our
proposed SRL based evaluation metric, sentence-
level BLEU, sentence-level STM and the corre-
sponding ranks assigned to each of the systems,
together with the human ranks on adequacy.
The Spearman&apos;s rank correlation coefficient
can be calculated using the following simplified
equation:
where is the difference between the ranks of the
</bodyText>
<page confidence="0.999335">
58
</page>
<tableCaption confidence="0.997329666666667">
Table 4: Sentence-level SRL based f-score evaluation metrics average on annotators, sentence-level
BLEU, sentence-level STM, their corresponding rank assigned and the human rank on adequacy for
example 1.
</tableCaption>
<table confidence="0.9968015">
System MT output SRL rank BLEU rank STM rank Human
score score score rank
Src - - - - - - -
Ref Until after their sales had ceased in main- - - - - - - -
land China for almost two months , sales
of the complete range of SK II products
have now be resumed.
MT1 So far, nearly two months sk - ii the sale of 0.167 2 0.012 3 0.364 1 2
products in the mainland of China to resume
sales .
MT2 So far, in the mainland of China to stop sell- 0.317 1 0.013 2 0.303 3 1
ing nearly two months of SK - 2 products
sales resumed.
MT3 So far, the sale in the mainland of China for 0.000 3 0.124 1 0.344 2 3
nearly two months of SK - II line of prod-
ucts .
</table>
<tableCaption confidence="0.737259">
Table 5: Average sentence-level correlation for
the evaluation metrics.
</tableCaption>
<table confidence="0.9861006">
Metric Correlation
with human
SRL based evaluation 0.41
BLEU 0.25
STM 0.32
</table>
<bodyText confidence="0.99991655">
evaluation metrics and the human judgment over
of system and is the number of systems. The
range of possible values of correlation coefficient
is [-1,1], where 1 means the systems are ranked
in the same order as the human judgment and -1
means the systems are ranked in the reverse order
as the human judgment. The higher the value for
indicates the more similar the ranking by the
evaluation metric to the human judgment.
Our results show that the proposed SRL based
evaluation metric has a higher correlation with
the human judgment on adequacy than either the
BLEU or STM metrics. Table 5 compares the
average sentence-level for our proposed SRL
based evaluation metric, BLEU, and STM. The
correlation coefficient for the proposed SRL based
evaluation metric is 0.41, while that for BLEU is
0.25. The correlation coefficient for STM is 0.32,
significantly better than BLEU, but still far short
of our SRL based metric.
</bodyText>
<sectionHeader confidence="0.996753" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999920551724138">
We presented results of an empirical study on
evaluation the utility of MT output, by assessing
the accuracy with which human reader are able to
complete the SRL templates. The SRL based f-
score evaluation metric we proposed provided an
intuitive picture on how much information of the
original source input the machine translation users
can extract by reading the MT output. Compar-
ing to HTER where the human decision is heavy
and requires advance knowledge in how to fix the
translation with minimum change, only minimal
instructions is necessary to be given to the human
readers in our proposed metric. The human read-
ers may not necessarily be translation experts.
Our results show that using SRL in seman-
tic MT evaluation is a highly promising direction
for further research. We evaluated the proposed
SRL based metric with human judgment on ad-
equacy using Spearman&apos;s correlation coefficient.
The proposed SRL based evaluation metric was
found to be significantly better correlated to hu-
man judgment on adequacy than either BLEU or
the syntax-based evaluation metric STM.
Our current direction is to discriminatively tune
the weights within the SRL based evaluation met-
ric, so as to further increase the correlation of the
metric with human judgment.
Our other main avenue of current work is to
construct automated metrics approximating the
</bodyText>
<page confidence="0.996063">
59
</page>
<bodyText confidence="0.999968333333333">
evaluation method described here (which provides
an upper bound for automated SRL-based met-
rics). With the improving performance of shal-
low semantic parsers, we believe that the proposed
evaluation metric could be further developed into
inexpensive automatic MT evaluation metrics.
</bodyText>
<sectionHeader confidence="0.997737" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99623675">
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tract Nos. HR0011-06-C-0022 and HR0011-06-
C-0023 and by the Hong Kong Research Grants
Council (RGC) research grants GRF621008,
GRF612806, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of the Defense Advanced
Research Projects Agency.
</bodyText>
<sectionHeader confidence="0.996237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918166666667">
T. Brants. Inter-annotator agreement for a German newspa-
per corpus. In Proceedings of the 2nd International Con-
ference on Language Resources and Evaluation (LREC-
2000). Citeseer, 2000.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
Re-evaluating the role of BLEU in machine translation
research. In Proceedings of EACL 2006, pages 249-256,
2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. (Meta-) evaluation of
machine translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 136-158.
Association for Computational Linguistics, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. Further meta-
evaluation of machine translation. In Proceedings of
the Third Workshop on Statistical Machine Translation,
pages 70-106. Association for Computational Linguis-
tics, 2008.
Marine Carpuat and Dekai Wu. Improving Statistical Ma-
chine Translation using Word Sense Disambiguation. In
Conference on Empirical Methods in Natural Language
Processing and Conference on Computational Natural
Language Learning (EMNLP-CoNLL 2007), Prague, Jun
2007.
Y.S. Chan, H.T. Ng, and D. Chiang. Word sense disambigua-
tion improves statistical machine translation. In 45th An-
nual Meeting of the Association for Computational Lin-
guistics, Prague, Czech, 2007.
E. Charniak. Immediate-head parsing for language models.
In Proceedings of the 39th Annual Meeting on Association
for Computational Linguistics, page 131. Association for
Computational Linguistics, 2001.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai Wu.
Automatic Learning of Chinese English Semantic Struc-
ture Mapping. In IEEE Spoken Language Technology
Workshop, 2006, pages 230-233, 2006.
Jesus Gim6nez and Llufs Marquez. Discriminative Phrase
Selection for Statistical Machine Translation. Learning
Machine Translation, 2007.
Jesus Gim6nez and Llufs Marquez. Linguistic features for
automatic evaluation of heterogenous MT systems. In
Proceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 256-264, Prague, Czech Repub-
lic, June 2007. Association for Computational Linguis-
tics.
Jesus Gim6nez and Llufs Marquez. A smorgasbord of fea-
tures for automatic MT evaluation. In Proceedings of the
3rd Workshop on Statistical Machine Translation, pages
195-198, Columbus, OH, June 2008. Association for
Computational Linguistics.
Philipp Koehn and Christof Monz. Manual and automatic
evaluation of machine translation between european lan-
guages. In Proceedings of the Workshop on Statisti-
cal Machine Translation, pages 102-121. Association for
Computational Linguistics, 2006.
D. Liu and D. Gildea. Syntactic features for evaluation
of machine translation. ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, page 25, 2005.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. The
Proposition Bank: an Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71-106, 2005.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. BLEU: a method for automatic evaluation of ma-
chine translation. In 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311-318, 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.
Martin, and Dan Jurafsky. Shallow Semantic Parsing Us-
ing Support Vector Machines. In Proceedings of NAACL-
HLT 2004, 2004.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. A study of translation edit rate with targeted
human annotation. In Proceedings ofAssociation for Ma-
chine Translation in the Americas, pages 223-231, 2006.
Dekai Wu and Pascale Fung. Can Semantic Role Labeling
Improve SMT? In Proceedings of the 13th Annual Con-
ference of the EAMT, pages 218-225, Barcelona, Spain,
May 2009.
Dekai Wu and Pascale Fung. Semantic Roles for SMT: A
Hybrid Two-Pass Model. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Compu-
tational Linguistics, Companion Volume: Short Papers,
pages 13-16. Association for Computational Linguistics,
2009.
Nianwen Xue and Martha Palmer. Automatic Semantic Role
Labeling for Chinese Verbs. In Proceedings of the 19th
International Joint Conference on Arti�cial Intelligence,
Edinburgh, Scotland, 2005.
</reference>
<page confidence="0.998413">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.172174">
<title confidence="0.935308">Semantic vs. Syntactic vs. N-gram for Machine Translation Evaluation Dekai</title>
<author confidence="0.826972">Human Language Technology</author>
<affiliation confidence="0.996754">Department of Computer Science and</affiliation>
<abstract confidence="0.95703544117647">Hong Kong University of Science and jackielo,dekai @cs.ust.hk Abstract We present results of an empirical study on evaluating the utility of the machine translation output, by assessing the accuracy with which human readers are able to complete the semantic role annotation templates. Unlike the widely-used lexical and n-gram based or syntactic based MT evaluation metrics which are fluencyoriented, our results show that using semantic role labels to evaluate the utility of MT output achieve higher correlation with human judgments on adequacy. In this study, human readers were employed to identify the semantic role labels in the translation. For each role, the filler is considered an accurate translation if it expresses the same meaning as that annotated in the gold standard reference translation. Our SRL based f-score evaluation metric has a 0.41 correlation coefficient with the human judgement on adequacy, while in contrast BLEU has only a 0.25 correlation coefficient and the syntactic based MT evaluation metric STM has only 0.32 correlation coefficient with the human judgement on adequacy. Our results strongly indicate that using semantic role labels for MT evaluation can be significantly more effective and better correlated with human judgement on adequacy than BLEU and STM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>Inter-annotator agreement for a German newspaper corpus.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC2000). Citeseer,</booktitle>
<contexts>
<context position="20707" citStr="Brants (2000)" startWordPosition="3387" endWordPosition="3388">T evaluation metric of MT1 is the f-score averaged on all annotators. Table 3 shows the results of the SRL based MT evaluation metric averaged on all annotators and all sentences. Our results show that the evaluation metric can successfully distinguish the translation utility of the human translation and the three MT systems; and on system level, MT1 provides the most accurate translation. 4 Inter-annotator Agreement We measured the inter-annotator agreement in two tasks: role identification and role classification. The standard f-score is used to measure the agreement on SRL annotation as in Brants (2000). For role identification, the agreement is counted on the matching of word span in the annotated arguments with a tolerance of 1 word in mismatch. The tolerance is designed for the fact that annotators are not consistent in handling the articles or punctuations at the beginning or the end of the annotated arguments. The agreement rate on SRL annotations in role identification of reference translation is 76%, and that on MT output is 72%. For role classification, in addition to the requirement of matching of word span in role identification task, the agreement is counted on the matching of the</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. Inter-annotator agreement for a German newspaper corpus. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC2000). Citeseer, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>249--256</pages>
<contexts>
<context position="2690" citStr="Callison-Burch et al. (2006)" startWordPosition="414" endWordPosition="417">er et al., 2006), our proposed semantic metric only requires simple and minimal instructions to the human judges involved in the evaluation cycle. We argue that neither n-gram based metrics, like BLEU, nor syntax-based metrics, like STM, adequately capture the similarity in meaning between the machine translation and the reference translation—which, ultimately, is essential for translations to be useful. First, n-gram based metrics assume that &amp;quot;good&amp;quot; translations share the same lexical choices with the reference translation. While BLEU score performs well in capturing the translation fluency, Callison-Burch et al. (2006) and Koehn and Monz (2006) report cases where BLEU strongly disagrees with human judgment on translation quality. The underlying reason is that lexical similarity does not adequately reflect the similarity in meaning. Second, just like n-gram based metrics such as BLEU, syntax-based metrics are still more fluency-oriented than adequacy/accuracyoriented. While STM addresses the failure of BLEU in evaluating the translation grammaticality, a grammatical translation can nonetheless achieve a high STM score even if contains errors arising from confusion of semantic roles. Syntactic structure simil</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in machine translation research. In Proceedings of EACL 2006, pages 249-256, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>136--158</pages>
<publisher>Association</publisher>
<contexts>
<context position="11367" citStr="Callison-Burch et al., 2007" startWordPosition="1847" endWordPosition="1850">a new automatic MT evaluation metric in which a series of linguistic features are combined together. One of those linguistic features is shallow semantic similarity on semantic role overlap. The semantic role overlap metric calculates the lexical overlapping between semantic roles of the same type in the machine translation output and the corresponding reference translations and then considers the average lexical overlapping over all semantic role types. Despite the fact that the metric shows an improved correlation with human judgment of translation quality (Gim6nez and Marquez, 2007b, 2008; Callison-Burch et al., 2007, 2008), it is not commonly used in large-scale MT evaluation campaign. The reason may lie in the high time cost. We believe it is important to first focus on developing simple measures to evaluate machine translation utility, that make use of human extraction of role information. It is necessary to first understand the upper bounds of human performance on this task, as a foundation for better design of efficient automated metrics. 2.4 HTER: non-automated MT evaluation metric Human-targeted Translation Edit Rate (HTER) in the work of Snover et al. (2006) is a nonautomatic machine translation e</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. (Meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 136-158. Association for Computational Linguistics, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further metaevaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<publisher>Association</publisher>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further metaevaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70-106. Association for Computational Linguistics, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving Statistical Machine Translation using Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning (EMNLP-CoNLL 2007),</booktitle>
<location>Prague,</location>
<contexts>
<context position="5158" citStr="Carpuat and Wu (2007)" startWordPosition="806" endWordPosition="809">the study on evaluating machine translation utility by measuring the accuracy with which human readers are able to complete the semantic role annotation templates. Last but not the least, we show that our proposed evaluation metric has a higher correlation with human judgments on adequacy than BLEU and STM. 2 Related Work 2.1 Semantic models in SMT Numerous recent works has been done on applying different semantic models to statistical machine translation. Word sense disambiguation (WSD) models combine a wide range of context features into a single lexical choice prediction, as in the work of Carpuat and Wu (2007), Chan et al. (2007), and Gimenez and Marquez (2007a). In particular, Phrase Sense Disambiguation (PSD), a generalization of the WSD approach, automatically acquires fully phrasal translation lexicons and provides a context-dependent probability distribution over the possible translation candidates for any given phrasal lexicon (Carpuat and Wu, 2007). Another recent research direction on semantic SMT is applying semantic role labeling models. Semantic role labeling (SRL) is the task of identifying the semantic predicate-argument structures within a sentence. Semantic role labels represent an a</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. Improving Statistical Machine Translation using Word Sense Disambiguation. In Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning (EMNLP-CoNLL 2007), Prague, Jun 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
<author>D Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech,</location>
<contexts>
<context position="5178" citStr="Chan et al. (2007)" startWordPosition="810" endWordPosition="813"> machine translation utility by measuring the accuracy with which human readers are able to complete the semantic role annotation templates. Last but not the least, we show that our proposed evaluation metric has a higher correlation with human judgments on adequacy than BLEU and STM. 2 Related Work 2.1 Semantic models in SMT Numerous recent works has been done on applying different semantic models to statistical machine translation. Word sense disambiguation (WSD) models combine a wide range of context features into a single lexical choice prediction, as in the work of Carpuat and Wu (2007), Chan et al. (2007), and Gimenez and Marquez (2007a). In particular, Phrase Sense Disambiguation (PSD), a generalization of the WSD approach, automatically acquires fully phrasal translation lexicons and provides a context-dependent probability distribution over the possible translation candidates for any given phrasal lexicon (Carpuat and Wu, 2007). Another recent research direction on semantic SMT is applying semantic role labeling models. Semantic role labeling (SRL) is the task of identifying the semantic predicate-argument structures within a sentence. Semantic role labels represent an abstract level of und</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Y.S. Chan, H.T. Ng, and D. Chiang. Word sense disambiguation improves statistical machine translation. In 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, page 131. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="22640" citStr="Charniak, 2001" startWordPosition="3719" endWordPosition="3720"> human judgment on adequacy at sentence-level and took average on the whole data set. The human judgment on adequacy was obtained by showing all three MT outputs together with the Chinese source input to a human reader. The human reader was instructed to order the sentences from the three MT systems according to the accuracy of meaning in the translations. For the MT output, we ranked the sentences from the three MT systems according to the raw scores of the evaluation metrics. The STM scores are calculated based on the syntax tree of the reference and MT output parsed by the Charniak parser (Charniak, 2001). Table 4 shows the raw scores of example 1 under the our proposed SRL based evaluation metric, sentencelevel BLEU, sentence-level STM and the corresponding ranks assigned to each of the systems, together with the human ranks on adequacy. The Spearman&apos;s rank correlation coefficient can be calculated using the following simplified equation: where is the difference between the ranks of the 58 Table 4: Sentence-level SRL based f-score evaluation metrics average on annotators, sentence-level BLEU, sentence-level STM, their corresponding rank assigned and the human rank on adequacy for example 1. S</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, page 131. Association for Computational Linguistics, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Zhaojun Wu</author>
<author>Yongsheng Yang</author>
<author>Dekai Wu</author>
</authors>
<title>Automatic Learning of Chinese English Semantic Structure Mapping.</title>
<date>2006</date>
<booktitle>In IEEE Spoken Language Technology Workshop,</booktitle>
<pages>230--233</pages>
<contexts>
<context position="6116" citStr="Fung et al. (2006)" startWordPosition="952" endWordPosition="955">nd Wu, 2007). Another recent research direction on semantic SMT is applying semantic role labeling models. Semantic role labeling (SRL) is the task of identifying the semantic predicate-argument structures within a sentence. Semantic role labels represent an abstract level of understanding in meaning. There is an increasing availability of large parallel corpora annotated with semantic role information, in particular, in the work of Palmer et al. (2005) and Xue and Palmer (2005). As a result, the accuracy of automatic SRL task is also rising. 53 The best monolingual shallow semantic parser by Fung et al. (2006) achieved an F-score of 82.01 in Chinese semantic role labeling, while the best cross-lingual semantic verb frame argument mappings with accuracy of 89.3% as reported in the same work. The example in Figure 1 is labeled with semantic roles in the Propbank convention. sre shows a fragment of a typical Chinese source sentence that is drawn from newswire genre of the evaluation corpus. ref shows the corresponding fragment of the English reference translation. MT1, MT2 and MT3 show the three corresponding fragments of the machine translation output from three different MT systems. A relevant subse</context>
</contexts>
<marker>Fung, Wu, Yang, Wu, 2006</marker>
<rawString>Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai Wu. Automatic Learning of Chinese English Semantic Structure Mapping. In IEEE Spoken Language Technology Workshop, 2006, pages 230-233, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gim6nez</author>
<author>Llufs Marquez</author>
</authors>
<title>Discriminative Phrase Selection for Statistical Machine Translation. Learning Machine Translation,</title>
<date>2007</date>
<marker>Gim6nez, Marquez, 2007</marker>
<rawString>Jesus Gim6nez and Llufs Marquez. Discriminative Phrase Selection for Statistical Machine Translation. Learning Machine Translation, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gim6nez</author>
<author>Llufs Marquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogenous MT systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>256--264</pages>
<location>Prague, Czech Republic,</location>
<marker>Gim6nez, Marquez, 2007</marker>
<rawString>Jesus Gim6nez and Llufs Marquez. Linguistic features for automatic evaluation of heterogenous MT systems. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256-264, Prague, Czech Republic, June 2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gim6nez</author>
<author>Llufs Marquez</author>
</authors>
<title>A smorgasbord of features for automatic MT evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd Workshop on Statistical Machine Translation,</booktitle>
<pages>195--198</pages>
<location>Columbus, OH,</location>
<marker>Gim6nez, Marquez, 2008</marker>
<rawString>Jesus Gim6nez and Llufs Marquez. A smorgasbord of features for automatic MT evaluation. In Proceedings of the 3rd Workshop on Statistical Machine Translation, pages 195-198, Columbus, OH, June 2008. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<publisher>Association</publisher>
<contexts>
<context position="2716" citStr="Koehn and Monz (2006)" startWordPosition="419" endWordPosition="422">mantic metric only requires simple and minimal instructions to the human judges involved in the evaluation cycle. We argue that neither n-gram based metrics, like BLEU, nor syntax-based metrics, like STM, adequately capture the similarity in meaning between the machine translation and the reference translation—which, ultimately, is essential for translations to be useful. First, n-gram based metrics assume that &amp;quot;good&amp;quot; translations share the same lexical choices with the reference translation. While BLEU score performs well in capturing the translation fluency, Callison-Burch et al. (2006) and Koehn and Monz (2006) report cases where BLEU strongly disagrees with human judgment on translation quality. The underlying reason is that lexical similarity does not adequately reflect the similarity in meaning. Second, just like n-gram based metrics such as BLEU, syntax-based metrics are still more fluency-oriented than adequacy/accuracyoriented. While STM addresses the failure of BLEU in evaluating the translation grammaticality, a grammatical translation can nonetheless achieve a high STM score even if contains errors arising from confusion of semantic roles. Syntactic structure similarity still inadequately r</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. Manual and automatic evaluation of machine translation between european languages. In Proceedings of the Workshop on Statistical Machine Translation, pages 102-121. Association for Computational Linguistics, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
<author>D Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25</pages>
<contexts>
<context position="1970" citStr="Liu and Gildea, 2005" startWordPosition="306" endWordPosition="309">semantic role labels for MT evaluation can be significantly more effective and better correlated with human judgement on adequacy than BLEU and STM. 1 Introduction In this paper, we show that evaluating machine translation quality by assessing the accuracy of human performance in reconstructing the semantic frames from the MT output has a higher correlation with human judgment on translation adequacy than (1) the widely-used lexical n-gram precision based MT evaluation metric, BLEU (Papineni et al., 2002), as well as (2) the best-known syntactic tree precision based MT evaluation metric, STM (Liu and Gildea, 2005). At the same time, unlike some highly labor intensive evaluation metrics such as HTER (Snover et al., 2006), our proposed semantic metric only requires simple and minimal instructions to the human judges involved in the evaluation cycle. We argue that neither n-gram based metrics, like BLEU, nor syntax-based metrics, like STM, adequately capture the similarity in meaning between the machine translation and the reference translation—which, ultimately, is essential for translations to be useful. First, n-gram based metrics assume that &amp;quot;good&amp;quot; translations share the same lexical choices with the </context>
<context position="8916" citStr="Liu and Gildea (2005)" startWordPosition="1418" endWordPosition="1421"> on SRL that successfully returns a better translation with fewer semantic role confusion errors. With recent rise of work applying semantic model to statistical machine translation, there is a high demand for MT evaluation metrics that are directly sensitive to the semantic improvement made. We believe evaluating machine translation utility based on semantic roles should reflect semantic improvement better than current widelyused automated n-gram precision based MT evaluation metrics, like BLEU or fluency-oriented syntactic MT evaluation metrics, like STM. 2.2 STM: syntax-based MT evaluation Liu and Gildea (2005) proposed to use syntactic features in MT evaluation and developed subtree metric (STM) which based on the similarity of syntax tree of the MT output and that of the reference. It is the first proposed metric that incorporates syntactic features in MT evaluation and underlies all the other recently proposed syntactic MT evaluation metrics. STM is a precision based metric that captures the fractions of the subtree in a specific depth of the MT output syntax tree which also appear in the reference syntax tree. The fractions of different depths are then average in arithmetic mean. where is the ma</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>D. Liu and D. Gildea. Syntactic features for evaluation of machine translation. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, page 25, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: an Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--1</pages>
<contexts>
<context position="5955" citStr="Palmer et al. (2005)" startWordPosition="923" endWordPosition="926">al translation lexicons and provides a context-dependent probability distribution over the possible translation candidates for any given phrasal lexicon (Carpuat and Wu, 2007). Another recent research direction on semantic SMT is applying semantic role labeling models. Semantic role labeling (SRL) is the task of identifying the semantic predicate-argument structures within a sentence. Semantic role labels represent an abstract level of understanding in meaning. There is an increasing availability of large parallel corpora annotated with semantic role information, in particular, in the work of Palmer et al. (2005) and Xue and Palmer (2005). As a result, the accuracy of automatic SRL task is also rising. 53 The best monolingual shallow semantic parser by Fung et al. (2006) achieved an F-score of 82.01 in Chinese semantic role labeling, while the best cross-lingual semantic verb frame argument mappings with accuracy of 89.3% as reported in the same work. The example in Figure 1 is labeled with semantic roles in the Propbank convention. sre shows a fragment of a typical Chinese source sentence that is drawn from newswire genre of the evaluation corpus. ref shows the corresponding fragment of the English r</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. The Proposition Bank: an Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71-106, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1859" citStr="Papineni et al., 2002" startWordPosition="286" endWordPosition="290">nly 0.32 correlation coefficient with the human judgement on adequacy. Our results strongly indicate that using semantic role labels for MT evaluation can be significantly more effective and better correlated with human judgement on adequacy than BLEU and STM. 1 Introduction In this paper, we show that evaluating machine translation quality by assessing the accuracy of human performance in reconstructing the semantic frames from the MT output has a higher correlation with human judgment on translation adequacy than (1) the widely-used lexical n-gram precision based MT evaluation metric, BLEU (Papineni et al., 2002), as well as (2) the best-known syntactic tree precision based MT evaluation metric, STM (Liu and Gildea, 2005). At the same time, unlike some highly labor intensive evaluation metrics such as HTER (Snover et al., 2006), our proposed semantic metric only requires simple and minimal instructions to the human judges involved in the evaluation cycle. We argue that neither n-gram based metrics, like BLEU, nor syntax-based metrics, like STM, adequately capture the similarity in meaning between the machine translation and the reference translation—which, ultimately, is essential for translations to </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing Using Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACLHLT</booktitle>
<contexts>
<context position="4135" citStr="Pradhan et al., 2004" startWordPosition="638" endWordPosition="641">ing, August 2010. Figure 1: Example of semantic frames in Chinese input, English reference translation and MT output. lexical n-gram based and syntax-based evaluation metrics are becoming more apparent. Stateof-the-art MT systems are often able to output translations containing roughly the correct words and being almost grammatical, but not expressing meaning that is close to the source input. We adopt the outset of the principle that a good translation is one from which human readers may successfully understand at least the basic event structure — &amp;quot;who did what to whom, when, where and why&amp;quot; (Pradhan et al., 2004) which represents the most important meaning of the source utterances. Our objective is to evaluate how well the most essential semantic information is being captured by the machine translation systems from the user&apos;s point of view. In this paper, we describe in detail the methodology that underlies the new semantic machine translation evaluation metrics we are developing. We present the results of the study on evaluating machine translation utility by measuring the accuracy with which human readers are able to complete the semantic role annotation templates. Last but not the least, we show th</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow Semantic Parsing Using Support Vector Machines. In Proceedings of NAACLHLT 2004, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="2078" citStr="Snover et al., 2006" startWordPosition="325" endWordPosition="328">udgement on adequacy than BLEU and STM. 1 Introduction In this paper, we show that evaluating machine translation quality by assessing the accuracy of human performance in reconstructing the semantic frames from the MT output has a higher correlation with human judgment on translation adequacy than (1) the widely-used lexical n-gram precision based MT evaluation metric, BLEU (Papineni et al., 2002), as well as (2) the best-known syntactic tree precision based MT evaluation metric, STM (Liu and Gildea, 2005). At the same time, unlike some highly labor intensive evaluation metrics such as HTER (Snover et al., 2006), our proposed semantic metric only requires simple and minimal instructions to the human judges involved in the evaluation cycle. We argue that neither n-gram based metrics, like BLEU, nor syntax-based metrics, like STM, adequately capture the similarity in meaning between the machine translation and the reference translation—which, ultimately, is essential for translations to be useful. First, n-gram based metrics assume that &amp;quot;good&amp;quot; translations share the same lexical choices with the reference translation. While BLEU score performs well in capturing the translation fluency, Callison-Burch e</context>
<context position="11927" citStr="Snover et al. (2006)" startWordPosition="1940" endWordPosition="1943">im6nez and Marquez, 2007b, 2008; Callison-Burch et al., 2007, 2008), it is not commonly used in large-scale MT evaluation campaign. The reason may lie in the high time cost. We believe it is important to first focus on developing simple measures to evaluate machine translation utility, that make use of human extraction of role information. It is necessary to first understand the upper bounds of human performance on this task, as a foundation for better design of efficient automated metrics. 2.4 HTER: non-automated MT evaluation metric Human-targeted Translation Edit Rate (HTER) in the work of Snover et al. (2006) is a nonautomatic machine translation evaluation metric based on the number of edits required to correct the translation hypotheses. A human annotator edits each MT hypothesis so that it is meaningequivalent with the reference translation. It emphasizes on making the minimum possible number of edits. The Translation Edit Rate (TER) is then calculated using the human-edited translation as a targeted reference for the MT hypothesis. The HTER is highly labor intensive in the evaluation process. The human annotators are not only required to understand the meaning expressed in the reference transl</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation for Machine Translation in the Americas, pages 223-231, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Can Semantic Role Labeling Improve SMT?</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the EAMT,</booktitle>
<pages>218--225</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="8171" citStr="Wu and Fung (2009" startWordPosition="1304" endWordPosition="1307">r role, &amp;quot;their sales&amp;quot;; one in an ARGM-LOC role, &amp;quot;in mainland China&amp;quot;, and one in ARGM-TMP temporal role, &amp;quot;for almost two months&amp;quot;. The second verb (resumed) also has three arguments: two in ARGM-TMP temporal roles, &amp;quot;until after their sales ceased in mainland China for almost two months&amp;quot; and &amp;quot;now&amp;quot;, and one in ARG1 experiencer role, &amp;quot;sales of the complete range of SK-II products&amp;quot;. Similarly, the first two MT outputs are also annotated with semantic roles in the PropBank convention. Since there is no verb appeared in the third MT output, no predicate-argument structure is annotated. Recent work by Wu and Fung (2009a) and Wu and Fung (2009b) has begun to apply SRL to statistical machine translation using a semantic reordering model based on SRL that successfully returns a better translation with fewer semantic role confusion errors. With recent rise of work applying semantic model to statistical machine translation, there is a high demand for MT evaluation metrics that are directly sensitive to the semantic improvement made. We believe evaluating machine translation utility based on semantic roles should reflect semantic improvement better than current widelyused automated n-gram precision based MT evalu</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. Can Semantic Role Labeling Improve SMT? In Proceedings of the 13th Annual Conference of the EAMT, pages 218-225, Barcelona, Spain, May 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Semantic Roles for SMT: A Hybrid Two-Pass Model.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>13--16</pages>
<publisher>Association</publisher>
<contexts>
<context position="8171" citStr="Wu and Fung (2009" startWordPosition="1304" endWordPosition="1307">r role, &amp;quot;their sales&amp;quot;; one in an ARGM-LOC role, &amp;quot;in mainland China&amp;quot;, and one in ARGM-TMP temporal role, &amp;quot;for almost two months&amp;quot;. The second verb (resumed) also has three arguments: two in ARGM-TMP temporal roles, &amp;quot;until after their sales ceased in mainland China for almost two months&amp;quot; and &amp;quot;now&amp;quot;, and one in ARG1 experiencer role, &amp;quot;sales of the complete range of SK-II products&amp;quot;. Similarly, the first two MT outputs are also annotated with semantic roles in the PropBank convention. Since there is no verb appeared in the third MT output, no predicate-argument structure is annotated. Recent work by Wu and Fung (2009a) and Wu and Fung (2009b) has begun to apply SRL to statistical machine translation using a semantic reordering model based on SRL that successfully returns a better translation with fewer semantic role confusion errors. With recent rise of work applying semantic model to statistical machine translation, there is a high demand for MT evaluation metrics that are directly sensitive to the semantic improvement made. We believe evaluating machine translation utility based on semantic roles should reflect semantic improvement better than current widelyused automated n-gram precision based MT evalu</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. Semantic Roles for SMT: A Hybrid Two-Pass Model. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 13-16. Association for Computational Linguistics, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Automatic Semantic Role Labeling for Chinese Verbs.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Arti�cial Intelligence,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="5981" citStr="Xue and Palmer (2005)" startWordPosition="928" endWordPosition="931">nd provides a context-dependent probability distribution over the possible translation candidates for any given phrasal lexicon (Carpuat and Wu, 2007). Another recent research direction on semantic SMT is applying semantic role labeling models. Semantic role labeling (SRL) is the task of identifying the semantic predicate-argument structures within a sentence. Semantic role labels represent an abstract level of understanding in meaning. There is an increasing availability of large parallel corpora annotated with semantic role information, in particular, in the work of Palmer et al. (2005) and Xue and Palmer (2005). As a result, the accuracy of automatic SRL task is also rising. 53 The best monolingual shallow semantic parser by Fung et al. (2006) achieved an F-score of 82.01 in Chinese semantic role labeling, while the best cross-lingual semantic verb frame argument mappings with accuracy of 89.3% as reported in the same work. The example in Figure 1 is labeled with semantic roles in the Propbank convention. sre shows a fragment of a typical Chinese source sentence that is drawn from newswire genre of the evaluation corpus. ref shows the corresponding fragment of the English reference translation. MT1,</context>
</contexts>
<marker>Xue, Palmer, 2005</marker>
<rawString>Nianwen Xue and Martha Palmer. Automatic Semantic Role Labeling for Chinese Verbs. In Proceedings of the 19th International Joint Conference on Arti�cial Intelligence, Edinburgh, Scotland, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>