<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99337">
Learning Dense Models of Query Similarity from User Click Logs
</title>
<author confidence="0.965834">
Fabio De Bona*
</author>
<affiliation confidence="0.864258">
Friedrich Miescher Laboratory
of the Max Planck Society
</affiliation>
<address confidence="0.515574">
T¨ubingen, Germany
</address>
<email confidence="0.94236">
fabio@tuebingen.mpg.de
</email>
<author confidence="0.939961">
Massimiliano Ciaramita
</author>
<affiliation confidence="0.899922">
Google Research
</affiliation>
<address confidence="0.718389">
Z¨urich, Switzerland
</address>
<email confidence="0.997405">
massi@google.com
</email>
<author confidence="0.863866">
Stefan Riezler
</author>
<affiliation confidence="0.830023">
Google Research
</affiliation>
<address confidence="0.674215">
Z¨urich, Switzerland
</address>
<email confidence="0.979335">
riezler@google.com
</email>
<author confidence="0.988847">
Amac¸ Herdaˇgdelen*
</author>
<affiliation confidence="0.8634625">
University of Trento
Rovereto, Italy
</affiliation>
<email confidence="0.99145">
amac@herdagdelen.com
</email>
<author confidence="0.772772">
Keith Hall
</author>
<affiliation confidence="0.75324">
Google Research
</affiliation>
<address confidence="0.684057">
Z¨urich, Switzerland
</address>
<email confidence="0.971872">
kbhall@google.com
</email>
<author confidence="0.878489">
Maria Holmqvist*
</author>
<affiliation confidence="0.956078">
Linkopings University
</affiliation>
<address confidence="0.538855">
Linkopings, Sweden
</address>
<email confidence="0.995794">
marho@ida.liu.se
</email>
<sectionHeader confidence="0.996623" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998831875">
The goal of this work is to integrate query
similarity metrics as features into a dense
model that can be trained on large amounts
of query log data, in order to rank query
rewrites. We propose features that incorpo-
rate various notions of syntactic and semantic
similarity in a generalized edit distance frame-
work. We use the implicit feedback of user
clicks on search results as weak labels in train-
ing linear ranking models on large data sets.
We optimize different ranking objectives in a
stochastic gradient descent framework. Our
experiments show that a pairwise SVM ranker
trained on multipartite rank levels outperforms
other pairwise and listwise ranking methods
under a variety of evaluation metrics.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999333755102041">
Measures of query similarity are used for a wide
range of web search applications, including query
expansion, query suggestions, or listings of related
queries. Several recent approaches deploy user
query logs to learn query similarities. One set of ap-
proaches focuses on user reformulations of queries
that differ only in one phrase, e.g., Jones et al.
(2006). Such phrases are then identified as candi-
date expansion terms, and filtered by various signals
such as co-occurrence in similar sessions, or log-
likelihood ratio of original and expansion phrase.
Other approaches focus on the relation of queries
and search results, either by clustering queries based
*The work presented in this paper was done while the au-
thors were visiting Google Research, Z¨urich.
on their search results, e.g., Beeferman and Berger
(2000), or by deploying the graph of queries and re-
sults to find related queries, e.g., Sahami and Heil-
man (2006).
The approach closest to ours is that of Jones et al.
(2006). Similar to their approach, we create a train-
ing set of candidate query rewrites from user query
logs, and use it to train learners. While the dataset
used in Jones et al. (2006) is in the order of a few
thousand query-rewrite pairs, our dataset comprises
around 1 billion query-rewrite pairs. Clearly, man-
ual labeling of rewrite quality is not feasible for our
dataset, and perhaps not even desirable. Instead, our
intent is to learn from large amounts of user query
log data. Such data permit to learn smooth mod-
els because of the effectiveness of large data sets to
capture even rare aspects of language, and they also
are available as in the wild, i.e., they reflect the ac-
tual input-output behaviour that we seek to automate
(Halevy et al., 2009). We propose a technique to au-
tomatically create weak labels from co-click infor-
mation in user query logs of search engines. The
central idea is that two queries are related if they
lead to user clicks on the same documents for a large
amount of documents. A manual evaluation of a
small subset showed that a determination of positive
versus negative rewrites by thresholding the number
of co-clicks correlates well with human judgements
of similarity, thus justifying our method of eliciting
labels from co-clicks.
Similar to Jones et al. (2006), the features of our
models are not based on word identities, but instead
on general string similarity metrics. This leads to
dense rather than sparse feature spaces. The dif-
</bodyText>
<page confidence="0.985265">
474
</page>
<note confidence="0.7527665">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474–482,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99880795">
ference of our approach to Jones et al. (2006) lies
in our particular choice of string similarity metrics.
While Jones et al. (2006) deploy “syntactic” fea-
tures such as Levenshtein distance, and “semantic”
features such as log-likelihood ratio or mutual in-
formation, we combine syntactic and semantic as-
pects into generalized edit-distance features where
the cost of each edit operation is weighted by vari-
ous term probability models.
Lastly, the learners used in our approach are appli-
cable to very large datasets by an integration of lin-
ear ranking models into a stochastic gradient descent
framework for optimization. We compare several
linear ranking models, including a log-linear prob-
ability model for bipartite ranking, and pairwise and
listwise SVM rankers. We show in an experimen-
tal evaluation that a pairwise SVM ranker trained on
multipartite rank levels outperforms state-of-the-art
pairwise and listwise ranking methods under a vari-
ety of evaluation metrics.
</bodyText>
<sectionHeader confidence="0.739895" genericHeader="method">
2 Query Similarity Measures
</sectionHeader>
<subsectionHeader confidence="0.993012">
2.1 Semantic measures
</subsectionHeader>
<bodyText confidence="0.999977181818182">
In several of the similarity measures we describe be-
low, we employ pointwise mutual information (PMI)
as a measure of the association between two terms or
queries. Let wi and wj be two strings that we want
to measure the amount of association between. Let
p(wi) and p(wj) be the probability of observing wi
and wj in a given model; e.g., relative frequencies
estimated from occurrence counts in a corpus. We
also define p(wi,wj) as the joint probability of wi
and wj; i.e., the probability of the two strings occur-
ring together. We define PMI as follows:
</bodyText>
<equation confidence="0.993996">
PMI(wi, wj) = log p(wi, wj) (1)
p(wi)p(wj)
</equation>
<bodyText confidence="0.999615333333333">
PMI has been introduced by Church and Hanks
(1990) as word assosiatio ratio, and since then
been used extensively to model semantic similar-
ity. Among several desirable properties, it correlates
well with human judgments (Recchia and Jones,
2009).
</bodyText>
<subsectionHeader confidence="0.998663">
2.2 Taxonomic normalizations
</subsectionHeader>
<bodyText confidence="0.999955612903226">
As pointed out in earlier work, query transitions tend
to correlate with taxonomic relations such as gener-
alization and specialization (Lau and Horvitz, 1999;
Rieh and Xie, 2006). Boldi et al. (2009) show how
knowledge of transition types can positively impact
query reformulation. We would like to exploit this
information as well. However, rather than building a
dedicated supervised classifier for this task we try to
capture it directly at the source. First, we notice how
string features; e.g., length, and edit distance already
model this phenomenon to some extent, and in fact
are part of the features used in Boldi et al. (2009).
However, these measures are not always accurate
and it is easy to find counterexamples both at the
term level (e.g., “camping” to “outdoor activities” is
a generalization) and character level (“animal pic-
tures” to “cat pictures” is a specialization). Sec-
ondly, we propose that by manipulating PMI we can
directly model taxonomic relations to some extent.
Rather than using raw PMI values we re-
normalize them. Notice that it is not obvious in our
context how to interpret the relation between strings
co-occurring less frequently than random. Such
noisy events will yield negative PMI values since
p(wi, wj) &lt; p(wi)p(wj). We enforce zero PMI val-
ues for such cases. If PMI is thus constrained to
non-negative values, normalization will bound PMI
to the range between 0 and 1.
The first type of normalization, called joint nor-
malization, uses the negative log joint probability
and is defined as
</bodyText>
<equation confidence="0.571825">
PMI(J)(wi, wj) = PMI(wi, wj)/−log(p(wi, wj)).
</equation>
<bodyText confidence="0.999970307692308">
The jointly normalized PMI(J) is a symmetric
measure between wi and wj in the sense that
PMI(J)(wi, wj) = PMI(J)(wj, wi). Intuitively it
is a measure of the amount of shared information
between the two strings relative to the sum of indi-
vidual strings information. The advantages of the
joint normalization of PMI have been noticed be-
fore (Bouma, 2009).
To capture asymmetries in the relation between
two strings, we introduce two non-symmetric nor-
malizations which also bound the measure between
0 and 1. The second normalization is called special-
ization normalization and is defined as
</bodyText>
<equation confidence="0.519308">
PMI(S)(wi, wj) = PMI(wi, wj)/ − log(p(wi)).
</equation>
<bodyText confidence="0.998189">
The reason we call it specialization is that PMI(S)
favors pairs where the second string is a specializa-
</bodyText>
<page confidence="0.998634">
475
</page>
<bodyText confidence="0.999913714285714">
tion of the first one. For instance, PMI(S) is at its
maximum when p(wi, wj) = p(wj) and that means
the conditional probability p(wi|wj) is 1 which is an
indication of a specialization relation.
The last normalization is called the generalization
normalization and is defined in the reverse direction
as
</bodyText>
<equation confidence="0.685658666666667">
PMI(G)(wi, wj) = PMI(wi, wj)/ − log(p(wj)).
Again, PMI(G) is a measure between 0 and 1 and is
at its maximum value when p(wj|wi) is 1.
</equation>
<bodyText confidence="0.999724">
The three normalizations provide a richer rep-
resentation of the association between two strings.
Furthermore, jointly, they model in an information-
theoretic sense the generalization-specialization di-
mension directly. As an example, for the query
transition “apple” to “mac os” PMI(G)=0.2917 and
PMI(S)=0.3686; i.e., there is more evidence for a
specialization. Conversely for the query transition
“ferrari models” to “ferrari” we get PMI(G)=1 and
PMI(S)=0.5558; i.e., the target is a “perfect” gener-
alization of the source1.
</bodyText>
<subsectionHeader confidence="0.998464">
2.3 Syntactic measures
</subsectionHeader>
<bodyText confidence="0.999992681818182">
Let V be a finite vocabulary and � be the null
symbol. An edit operation: insertion, deletion or
substitution, is a pair (a, b) E {V U {�} x V U
{f}} \ {(�,�)}. An alignment between two se-
quences wi and wj is a sequence of edit oper-
ations w = (as, bs), ..., (an, bn). Given a non-
negative cost function c, the cost of an alignment is
c(w) = Eni�s c(wi). The Levenshtein distance, or
edit distance, defined over V , dV (wi,wj) between
two sequences is the cost of the least expensive se-
quence of edit operations which transforms wi into
wj (Levenshtein, 1966). The distance computation
can be performed via dynamic programming in time
O(|wi||wj|). Similarity at the string, i.e., character
or term, level is an indicator of semantic similar-
ity. Edit distance captures the amount of overlap be-
tween the queries as sequences of symbols and has
been previously used in information retrieval (Boldi
et al., 2009; Jones et al., 2006).
We use two basic Levenshtein distance models.
The first, called Edit1 (E1), employs a unit cost func-
tion for each of the three operations. That is, given
</bodyText>
<footnote confidence="0.626081">
1The values are computed from Web counts.
</footnote>
<bodyText confidence="0.981294923076923">
a finite vocabulary T containing all terms occurring
in queries:
ba, b E T, cEs(a, b) = 1 if(a =� b), 0 else.
The second, called Edit2 (E2), uses unit costs for
insertion and deletion, but computes the character-
based edit distance between two terms to decide on
the substitution cost. If two terms are very similar
at the character level, then the cost of substitution is
lower. Given a finite vocabulary T of terms and a
finite vocabulary A of characters, the cost function
is defined as:
ba, b E T, cE2(a, b) = dA(a, b) ifa n b =� �,1 else.
where dA(a, b) is linearly scaled between 0 and 1
dividing by max(|a|, |b|).
We also investigate a variant of the edit distance
algorithm in which the terms in the input sequences
are sorted, alphabetically, before the distance com-
putation. The motivation behind this variant is the
observation that linear order in queries is not always
meaningful. For example, it seems reasonable to as-
sume that “brooklyn pizza” and “pizza brooklyn”
denote roughly the same user intent. However, the
pair has an edit distance of two (delete-insert), while
the distance between “brooklyn pizza” and the less
relevant “brooklyn college” is only one (substitute).
The sorted variant relaxes the ordering constraint.
</bodyText>
<subsectionHeader confidence="0.973982">
2.4 Generalized measures
</subsectionHeader>
<bodyText confidence="0.9981982">
In this section we extend the edit distance frame-
work introduced in Section 2.3 with the semantic
similarity measures described in Section 2.1, using
the taxonomic normalizations defined in Section 2.2.
Extending the Levenshtein distance framework
to take into account semantic similarities between
terms is conceptually simple. As in the Edit2 model
above we use a modified cost function. We introduce
a cost matrix encoding individual costs for term sub-
stitution operations; the cost is defined in terms of
the normalized PMI measures of Section 2.2, recall
that these measures range between 0 and 1. Given a
normalized similarity measure f, an entry in a cost
matrix 5 for a term pair (wi, wj) is defined as:
s(wi, wj) = 2 − 2f(wi, wj) + c
</bodyText>
<page confidence="0.995397">
476
</page>
<bodyText confidence="0.993685045454545">
We call these models SEdit (SE), where S specifies
the cost matrix used. Given a finite term vocabulary
T and cost matrix 5, the cost function is defined as:
ba, b E T, cSE(a, b) = s(a, b) ifa n b =� �,1 else.
The cost function has the following properties.
Since insertion and deletion have unit cost, a term
is substituted only if a substitution is “cheaper” than
deleting and inserting another term, namely, if the
similarity between the terms is not zero. The E
correction, coupled with unit insertion and deletion
cost, guarantees that for an unrelated term pair a
combination of insertion and deletion will always be
less costly then a substitution. Thus in the compu-
tation of the optimal alignment, each operation cost
ranges between 0 and 2.
As a remark on efficiency, we notice that here the
semantic similarities are computed between terms,
rather than full queries. At the term level, caching
techniques can be applied more effectively to speed
up feature computation. The cost function is imple-
mented as a pre-calculated matrix, in the next sec-
tion we describe how the matrix is estimated.
</bodyText>
<subsectionHeader confidence="0.99849">
2.5 Cost matrix estimation
</subsectionHeader>
<bodyText confidence="0.992328055555556">
In our experiments we evaluated two different
sources to obtain the PMI-based cost matrices. In
both cases, we assumed that the cost of the substitu-
tion of a term with itself (i.e. identity substitution)
is always 0. The first technique uses a probabilis-
tic clustering model trained on queries and clicked
documents from user query logs. The second model
estimates cost matrices directly from user session
logs, consisting of approximately 1.3 billion U.S.
English queries. A session is defined as a sequence
of queries from the same user within a controlled
time interval. Let qs and qt be a query pair observed
in the session data where qt is issued immediately
after qs in the same session. Let q�s = qs \ qt and
q�t = qt \ qs, where \ is the set difference opera-
tor. The co-occurrence count of two terms wi and
wj from a query pair qs, qt is denoted by ni,j(qs, qt)
and is defined as:
</bodyText>
<footnote confidence="0.763838333333333">
{ 1 if wi = wj n wi E qs n wj E qt
1/(Jq�sJ JqtJ) if wi E q�s n wj E qt
0 else.
</footnote>
<bodyText confidence="0.985607035714286">
In other words, if a term occurs in both queries,
it has a co-occurrence count of 1. For all other term
pairs, a normalized co-occurrence count is computed
in order to make sure the sum of co-occurrence
counts for a term wi E qs sums to 1 for a given
query pair. The normalization is an attempt to avoid
the under representation of terms occurring in both
queries.
The final co-occurrence count of two arbitrary
terms wi and wj is denoted by Ni,j and it is defined
as the sum over all query pairs in the session logs,
Ni,j = Eqs,qt ni,j(qs, qt). Let N = Ewi,wj Ni,j be
the sum of co-occurrence counts over all term pairs.
Then we define a joint probability for a term pair as
p(wi,wj) = Ni,j
N . Similarly, we define the single-
occurrence counts and probabilities of the terms
by computing the marginalized sums over all term
pairs. Namely, the probability of a term wi occurring
in the source query is p(i, ·) = Ewj Ni,j/N and
similarly the probability of a term wj occurring in
the target query is p(·, A = Ewi Ni,j/N. Plugging
in these values in Eq. (1), we get the PMI(wi, wj)
for term pair wi and wj, which are further normal-
ized as described in Section 2.2.
More explanation and evaluation of the features
described in this section can be found in Ciaramita
et al. (2010).
</bodyText>
<sectionHeader confidence="0.865689" genericHeader="method">
3 Learning to Rank from Co-Click Data
</sectionHeader>
<subsectionHeader confidence="0.999345">
3.1 Extracting Weak Labels from Co-Clicks
</subsectionHeader>
<bodyText confidence="0.999996470588235">
Several studies have shown that implicit feedback
from clickstream data is a weaker signal than human
relevance judgements. Joachims (2002) or Agrawal
et al. (2009) presented techniques to convert clicks
into labels that can be used for machine learning.
Our goal is not to elicit relevance judgments from
user clicks, but rather to relate queries by pivoting on
commonly clicked search results. The hypothesis is
that two queries are related if they lead to user clicks
on the same documents for a large amount of docu-
ments. This approach is similar to the method pro-
posed by Fitzpatrick and Dent (1997) who attempt
to measure the relatedness between two queries by
using the normalized intersection of the top 200 re-
trieval results. We add click information to this
setup, thus strengthening the preference for preci-
sion over recall in the extraction of related queries.
</bodyText>
<equation confidence="0.829516">
ni,j(qs, qt) =
</equation>
<page confidence="0.997666">
477
</page>
<tableCaption confidence="0.999729">
Table 1: Statistics of co-click data sets.
</tableCaption>
<table confidence="0.987461333333333">
train dev test
number of queries 250,000 2,500 100
average number of 4,500 4,500 30
rewrites per query
percentage of rewrites 0.2 0.2 43
with ≥ 10 coclicks
</table>
<bodyText confidence="0.99973030952381">
In our experiments we created two ground-truth
ranking scenarios from the co-click signals. In a first
scenario, called bipartite ranking, we extract a set
of positive and a set of negative query-rewrite pairs
from the user logs data. We define positive pairs as
queries that have been co-clicked with at least 10 dif-
ferent results, and negative pairs as query pairs with
fewer than 10 co-clicks. In a second scenario, called
multipartite ranking, we define a hierarchy of levels
of “goodness”, by combining rewrites with the same
number of co-clicks at the same level, with increas-
ing ranks for higher number of co-clicks. Statistics
on the co-click data prepared for our experiments are
given in Table 1.
For training and development, we collected
query-rewrite pairs from user query logs that con-
tained at least one positive rewrite. The training set
consists of about 1 billion of query-rewrite pairs; the
development set contains 10 million query-rewrite
pairs. The average number of rewrites per query is
around 4,500 for the training and development set,
with a very small amount of 0.2% positive rewrites
per query. In order to confirm the validity of our co-
click hypothesis, and for final evaluation, we held
out another sample of query-rewrite pairs for man-
ual evaluation. This dataset contains 100 queries for
each of which we sampled 30 rewrites in descending
order of co-clicks, resulting in a high percentage of
43% positive rewrites per query. The query-rewrite
pairs were annotated by 3 raters as follows: First the
raters were asked to rank the rewrites in descend-
ing order of relevance using a graphical user inter-
face. Second the raters assigned rank labels and bi-
nary relevance scores to the ranked list of rewrites.
This labeling strategy is similar to the labeling strat-
egy for synonymy judgements proposed by Ruben-
stein and Goodenough (1965). Inter-rater agree-
ments on binary relevance judgements, and agree-
ment between rounded averaged human relevance
scores and assignments of positive/negative labels
by the co-click threshold of 10 produced a Kappa
value of 0.65 (Siegel and Castellan, 1988).
</bodyText>
<subsectionHeader confidence="0.993853">
3.2 Learning-to-Rank Query Rewrites
3.2.1 Notation
</subsectionHeader>
<bodyText confidence="0.99370368">
Let 5 = {(xq, yq)}nq�1 be a training sample
of queries, each represented by a set of rewrites
xq = {xq1, ... , xq,n(q)}, and set of rank labels
yq = {yq1, ... , yq,n(q)}, where n(q) is the num-
ber of rewrites for query q. For full rankings of
all rewrites for a query, a total order on rewrites is
assumed, with rank labels taking on values yqi ∈
{1, ... , n(q)}. Rewrites of equivalent rank can be
specified by assuming a partial order on rewrites,
where a multipartite ranking involves r &lt; n(q) rele-
vance levels such that yqi ∈ {1, ... , r} , and a bipar-
tite ranking involves two rank values yqi ∈ {1, 2}
with relevant rewrites at rank 1 and non-relevant
rewrites at rank 2.
Let the rewrites in xq be identified by the integers
{1, 2, ... , n(q)}, and let a permutation 7rq on xq be
defined as a bijection from {1, 2,... , n(q)} onto it-
self. Let IIq denote the set of all possible permuta-
tions on xq, and let 7rqi denote the rank position of
xqi. Furthermore, let (i, j) denote a pair of rewrites
in xq and let Pq be the set of all pairs in xq.
We associate a feature function O(xqi) with each
rewrite i = 1, ... , n(q) for each query q. Further-
more, a partial-order feature map as used in Yue et
al. (2007) is created for each rewrite set as follows:
</bodyText>
<equation confidence="0.990188666666667">
1 1 1
O(xq, 7rq) =
(i,j)EPq
</equation>
<bodyText confidence="0.999878083333333">
The goal of learning a ranking over the rewrites
xq for a query q can be achieved either by sorting the
rewrites according to the rewrite-level ranking func-
tion f(xqi) = hw, O(xqi)i, or by finding the permu-
tation that scores highest according to a query-level
ranking function f(xq, 7rq) = hw, O(xq, 7rq)i.
In the following, we will describe a variety
of well-known ranking objectives, and extensions
thereof, that are used in our experiments. Optimiza-
tion is done in a stochastic gradient descent (SGD)
framework. We minimize an empirical loss objec-
tive
</bodyText>
<equation confidence="0.9760025">
�min
w
xq,yq
|Pq|
e(w)
�
O(xqi)−O(xqj)sgn( − ).
7rqi 7rqj
</equation>
<page confidence="0.991865">
478
</page>
<bodyText confidence="0.960218333333333">
Note that the ranking scenario is in this case bipartite
with yqi ∈ {1, 2}.
The derivatives for `lh are as follows:
</bodyText>
<equation confidence="0.909554333333333">
∂ `lh = { 0 if (�w, φ(xq, yq) − φ(xq, π*q)))
∂wk &gt; L(yq,π*q),
−(φk(xq, yq) − φk(xq, π*q)) else.
</equation>
<bodyText confidence="0.9913542">
SGD optimization involves computing π* q for each
feature and each query, which can be done effi-
ciently using the greedy algorithm proposed by Yue
et al. (2007). We will refer to this method as the
SVM-MAP model.
</bodyText>
<footnote confidence="0.442901">
3.2.3 Pairwise Hinge Loss for Bipartite and
Multipartite Ranking
</footnote>
<bodyText confidence="0.996267571428571">
Joachims (2002) proposed an SVM method that
defines the ranking problem as a pairwise classifi-
cation problem. Cortes et al. (2007) extended this
method to a magnitude-preserving version by penal-
izing a pairwise misranking by the magnitude of the
difference in preference labels. A position-sensitive
penalty for pairwise ranking SVMs was proposed
by Riezler and De Bona (2009) and Chapelle and
Keerthi (2010), and earlier for perceptrons by Shen
and Joshi (2005). In the latter approaches, the mag-
nitude of the difference in inverted ranks is accrued
for each misranked pair. The idea is to impose an
increased penalty for misrankings at the top of the
list, and for misrankings that involve a difference of
several rank levels.
Similar to the listwise case, we can view the
penalty as a prediction loss function, and incor-
porate it into the hinge loss function by rescaling
the margin by a pairwise prediction loss function
L(yqi, yqj). In our experiments we used a position-
sensitive prediction loss function
</bodyText>
<equation confidence="0.9951355">
1
L(yqi, yqj) = |
</equation>
<bodyText confidence="0.991772666666667">
defined on the difference of inverted ranks. The
margin-rescaled pairwise hinge loss is then defined
as follows:
</bodyText>
<equation confidence="0.970884375">
yqi
−
1|
yqj
by stochastic updating
wt+1 = wt − ηtgt
where ηt is a learning rate, and gt is the gradient
gt = ∇`(w)
</equation>
<bodyText confidence="0.693871">
where
</bodyText>
<equation confidence="0.965602">
∇`(w)_� ∂ ∂ ∂
aw1 `(w), ∂w2 `(w), ... ∂wn
</equation>
<subsectionHeader confidence="0.920724">
3.2.2 Listwise Hinge Loss
</subsectionHeader>
<bodyText confidence="0.999767642857143">
Standard ranking evaluation metrics such as
(Mean) Average Precision (Manning et al., 2008)
are defined on permutations of whole lists and are
not decomposable over instances. Joachims (2005),
Yue et al. (2007), or Chakrabarti et al. (2008) have
proposed multivariate SVM models to optimize such
listwise evaluation metrics. The central idea is to
formalize the evaluation metric as a prediction loss
function L, and incorporate L via margin rescal-
ing into the hinge loss function, such that an up-
per bound on the prediction loss is achieved (see
Tsochantaridis et al. (2004), Proposition 2).
The loss function is given by the following list-
wise hinge loss:
</bodyText>
<equation confidence="0.8456405">
`lh(w) = (L(yq, π*q)−�w, φ(xq, yq) − φ(xq, π*q)))+
where π* q is the maximizer of the
maxπqEΠq\yq L(yq, π*q) + �w, φ(xq, π*q))
LAP(yq, πq) = 1 − AP(yq, πq)
where AP is defined as follows:
AP(yq, πq) = E�=1 Prec(j) · ( |yqj − 2 |)
Prec(j) = E k:πqkG (|yqk − 2|) .
πqj
</equation>
<bodyText confidence="0.979205666666667">
2We slightly abuse the notation yq to denote the permutation
on xq that is induced by the rank labels. In case of full rankings,
the permutation irq corresponding to ranking yq is unique. For
multipartite and bipartite rankings, there is more than one pos-
sible permutation for a given ranking, so that we let irq denote
a permutation that is consistent with ranking yq.
</bodyText>
<equation confidence="0.748962">
)`(w) .
</equation>
<bodyText confidence="0.980991833333333">
ex-
pression, (z)+ = max{0, z} and L(yq, πq) ∈ [0, 1]
denotes a prediction loss of a predicted ranking πq
compared to the ground-truth ranking yq.2
In this paper, we use Average Precision (AP) as
prediction loss function s.t.
</bodyText>
<equation confidence="0.744309777777778">
En(q) ,
j=1 (|yqj − 2|)
`ph(w) = � (L(yqi, yqj) −
(i,j)EPq
0 if (�w, φ(xq, yq) − φ(xq, π*q))) −
1))+
yqj
hw, φ(xqi) − φ(xqj)i sgn( 1
yqi
</equation>
<page confidence="0.998525">
479
</page>
<tableCaption confidence="0.998705">
Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,
SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.
</tableCaption>
<table confidence="0.9808184">
MAP NDCG@10 AUC Prec@1 Prec@3 Prec@5
Random 51.8 48.7 50.4 45.6 45.6 46.6
Best-feature 71.9 70.2 74.5 70.2 68.1 68.7
SVM-bipart. 73.7 73.7 74.7 79.4 70.1 70.1
SVM-MAP 74.3 75.2 75.3 76.3 71.8 72.0
Log-linear 74.7 75.1 75.7 75.3 72.2 71.3
SVM-pos.-sens. 75.7 76.0 76.6 82.5 72.9 73.0
SVM-multipart. 76.5 77.3 77.2 83.5 74.2 73.6
The derivative of `ph is calculated as follows:
∂ `lp = { 0 if ((w, φ(xqi) − φ(xqj))
∂wk sgn( yqi 1 − 1
yqj )) &gt; L(yqi, yqj),
−(φk(xqi) − φk(xqj))sgn(yqi 1 − 1
yqj )
else.
</table>
<bodyText confidence="0.999932875">
Note that the effect of inducing a position-
sensitive penalty on pairwise misrankings applies
only in case of full rankings on n(q) rank levels,
or in case of multipartite rankings involving 2 &lt;
r &lt; n(q) rank levels. Henceforth we will refer to
margin-rescaled pairwise hinge loss for multipartite
rankings as the SVM-pos.-sens. method.
Bipartite ranking is a special case where
L(yqi, yqj) is constant so that margin rescaling does
not have the effect of inducing position-sensitivity.
This method will be referred to as the SVM-bipartite
model.
Also note that for full ranking or multipartite
ranking, predicting a low ranking for an instance
that is ranked high in the ground truth has a domino
effect of accruing an additional penalty at each
rank level. This effect is independent of margin-
rescaling. The method of pairwise hinge loss
for multipartite ranking with constant margin will
henceforth be referred to as the SVM-multipartite
model.
Computation in SGD optimization is dominated
by the number of pairwise comparisons |Pq |for
each query. For full ranking, a comparison of
</bodyText>
<equation confidence="0.889759">
|Pq |= (n2
</equation>
<bodyText confidence="0.997157222222222">
q)) pairs has to be done. In the case
of multipartite ranking at r rank levels, each in-
cluding |li |rewrites, pairwise comparisons between
rewrites at the same rank level can be ignored.
This reduces the number of comparisons to |Pq |=
Er�1 Er j=i+1 |li||lj|. For bipartite ranking of p
i=1
positive and n negative instances, |Pq |= p · n com-
parisons are necessary.
</bodyText>
<subsubsectionHeader confidence="0.539043">
3.2.4 Log-linear Models for Bipartite Ranking
</subsubsectionHeader>
<bodyText confidence="0.999689875">
A probabilistic model for bipartite ranking can be
defined as the conditional probability of the set of
relevant rewrites, i.e., rewrites at rank level 1, given
all rewrites at rank levels 1 and 2. A formalization in
the family of log-linear models yields the following
logistic loss function `llm that was used for discrim-
inative estimation from sets of partially labeled data
in Riezler et al. (2002):
</bodyText>
<subsectionHeader confidence="0.493814">
ExqiExq e�w,φ(xqi))
</subsectionHeader>
<bodyText confidence="0.998838">
The gradient of `llm is calculated as a difference be-
tween two expectations:
</bodyText>
<equation confidence="0.543413">
`llm = −pw [φk|xq� yqi = 1] + pw [φk|xq] .
∂wk
</equation>
<bodyText confidence="0.99982425">
The SGD computation for the log-linear model is
dominated by the computation of expectations for
each query. The logistic loss for bipartite ranking is
henceforth referred to as the log-linear model.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999920555555555">
In the experiments reported in this paper, we trained
linear ranking models on 1 billion query-rewrite
pairs using 60 dense features, combined of the build-
ing blocks of syntactic and semantic similarity met-
rics under different estimations of cost matrices. De-
velopment testing was done on a data set that was
held-out from the training set. Final testing was car-
ried out on the manually labeled dataset. Data statis-
tics for all sets are given in Table 1.
</bodyText>
<equation confidence="0.973489333333333">
ExqiExq|yqi=1 e(w,φ(xqi))
`llm(w) = −log
∂
</equation>
<page confidence="0.998634">
480
</page>
<tableCaption confidence="0.99974">
Table 3: P-values computed by approximate randomization test for 15 pairwise comparisons of result differences.
</tableCaption>
<table confidence="0.887698428571429">
Best-feature SVM-bipart. SVM-MAP Log-linear SVM-pos.-sens. SVM-multipart.
Best-feature - &lt; 0.005 &lt; 0.005 &lt; 0.005 &lt; 0.005 &lt; 0.005
SVM-bipart. - - 0.324 &lt; 0.005 &lt; 0.005 &lt; 0.005
SVM-MAP - - - 0.374 &lt; 0.005 &lt; 0.005
Log-linear - - - - 0.053 &lt; 0.005
SVM-pos.-sens. - - - - - &lt; 0.005
SVM-multipart. - - - - - -
</table>
<bodyText confidence="0.999768027777777">
Model selection was performed by adjusting
meta-parameters on the development set. We
trained each model at constant learning rates 77 E
11, 0.5, 0.1, 0.01, 0.0011, and evaluated each variant
after every fifth out of 100 passes over the training
set. The variant with the highest MAP score on the
development set was chosen and evaluated on the
test set. This early stopping routine also served for
regularization.
Evaluation results for the systems are reported in
Table 2. We evaluate all models according to the fol-
lowing evaluation metrics: Mean Average Precision
(MAP), Normalized Discounted Cumulative Gain
with a cutoff at rank 10 (NDCG@10), Area-under-
the-ROC-curve (AUC), Precision@n3. As baselines
we report a random permutation of rewrites (ran-
dom), and the single dense feature that performed
best on the development set (best-feature). The latter
is the log-probability assigned to the query-rewrite
pair by the probabilistic clustering model used for
cost matrix estimation (see Section 2.5). P-values
are reported in Table 3 for all pairwise compar-
isons of systems (except the random baseline) us-
ing an Approximate Randomization test where strat-
ified shuffling is applied to results on the query level
(see Noreen (1989)). The rows in Tables 2 and 3
are ranked according to MAP values of the systems.
SVM-multipartite outperforms all other ranking sys-
tems under all evaluation metrics at a significance
level &gt; 0.995. For all other pairwise comparisons
of result differences, we find result differences of
systems ranked next to each other to be not statis-
tically significant. All systems outperform the ran-
dom and best-feature baselines with statistically sig-
nificant result differences. The distinctive advantage
of the SVM-multipartite models lies in the possibil-
</bodyText>
<footnote confidence="0.65291">
3For a definition of these metrics see Manning et al. (2008)
</footnote>
<bodyText confidence="0.999783125">
ity to rank rewrites with very high co-click num-
bers even higher than rewrites with reasonable num-
bers of co-clicks. This preference for ranking the
top co-clicked rewrites high seems the best avenue
for transferring co-click information to the human
judgements encoded in the manually labeled test set.
Position-sensitive margin rescaling does not seem to
help, but rather seems to hurt.
</bodyText>
<sectionHeader confidence="0.999767" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999992538461539">
We presented an approach to learn rankings of query
rewrites from large amounts of user query log data.
We showed how to use the implicit co-click feed-
back about rewrite quality in user log data to train
ranking models that perform well on ranking query
rewrites according to human quality standards. We
presented large-scale experiments using SGD opti-
mization for linear ranking models. Our experimen-
tal results show that an SVM model for multipartite
ranking outperforms other linear ranking models un-
der several evaluation metrics. In future work, we
would like to extend our approach to other models,
e.g., sparse combinations of lexicalized features.
</bodyText>
<sectionHeader confidence="0.997949" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997587666666667">
R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra,
and P. Tsaparas. 2009. Generating labels from clicks.
In Proceedings of the 2nd ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain.
Doug Beeferman and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD’00), Boston, MA.
P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009.
From ’Dango’ to ’Japanese cakes’: Query reformula-
</reference>
<page confidence="0.992364">
481
</page>
<reference confidence="0.999760742857143">
tion models and patterns. In Proceedings of Web Intel-
ligence. IEEE Cs Press.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Proceedings of
GSCL.
Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and
Chiru Bhattacharayya. 2008. Structured learning for
non-smooth ranking losses. In Proceedings of the 14th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD’08), Las Vegas, NV.
Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22–29.
Massimiliano Ciaramita, Amac¸ Herdaˇgdelen, Daniel
Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler,
and Enrique Alfonseca. 2010. Generalized syntactic
and semantic models of query reformulation. In Pro-
ceedings of the 33rd ACM SIGIR Conference, Geneva,
Switzerland.
Corinna Cortes, Mehryar Mohri, and Asish Rastogi.
2007. Magnitude-preserving ranking algorithms. In
Proceedings of the 24th International Conference on
Machine Learning (ICML’07), Corvallis, OR.
Larry Fitzpatrick and Mei Dent. 1997. Automatic feed-
back using past queries: Social searching? In Pro-
ceedings of the 20th Annual International ACM SIGIR
Conference, Philadelphia, PA.
Alon Halevy, Peter Norvig, and Fernando Pereira. 2009.
The unreasonable effectiveness of data. IEEE Intelli-
gent Systems, 24:8–12.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD’08), New York, NY.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd International Conference on Machine Learn-
ing (ICML’05), Bonn, Germany.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th International World Wide Web
conference (WWW’06), Edinburgh, Scotland.
T. Lau and E. Horvitz. 1999. Patterns of search: analyz-
ing and modeling web query refinement. In Proceed-
ings of the seventh international conference on User
modeling, pages 119–128. Springer-Verlag New York,
Inc.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707–710.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
G. Recchia and M.N. Jones. 2009. More data trumps
smarter algorithms: comparing pointwise mutual in-
formation with latent semantic analysis. Behavioral
Research Methods, 41(3):647–656.
S.Y. Rieh and H. Xie. 2006. Analysis of multiple query
reformulations on the web: the interactive information
retrieval context. Inf. Process. Manage., 42(3):751–
768.
Stefan Riezler and Fabio De Bona. 2009. Simple risk
bounds for position-sensitive max-margin ranking al-
gorithms. In Proceedings of the Workshop on Ad-
vances in Ranking at the 23rd Annual Conference
on Neural Information Processing Systems (NIPS’09),
Whistler, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL’02), Philadelphia, PA.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 10(3):627–633.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th Inter-
national World Wide Web conference (WWW’06), Ed-
inburgh, Scotland.
Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73–96.
Sidney Siegel and John Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. Second Edition.
MacGraw-Hill, Boston, MA.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning (ICML’04), Banff,
Canada.
Yisong Yue, Thomas Finley, Filip Radlinski, and
Thorsten Joachims. 2007. A support vector method
for optimizing average precision. In Proceedings of
the 30th Annual International ACM SIGIR Confer-
ence, Amsterdam, The Netherlands.
</reference>
<page confidence="0.998465">
482
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.089250">
<title confidence="0.966935">Learning Dense Models of Query Similarity from User Click Logs De</title>
<author confidence="0.652237">Friedrich Miescher of the Max Planck T¨ubingen</author>
<author confidence="0.652237">Germany</author>
<email confidence="0.998763">fabio@tuebingen.mpg.de</email>
<author confidence="0.997662">Massimiliano Ciaramita</author>
<affiliation confidence="0.99745">Google Research</affiliation>
<address confidence="0.670397">Z¨urich, Switzerland</address>
<email confidence="0.999297">massi@google.com</email>
<author confidence="0.985752">Stefan</author>
<affiliation confidence="0.87944">Google</affiliation>
<address confidence="0.900855">Z¨urich, Switzerland</address>
<email confidence="0.999286">riezler@google.com</email>
<affiliation confidence="0.999788">University of Trento</affiliation>
<address confidence="0.965651">Rovereto, Italy</address>
<email confidence="0.9997">amac@herdagdelen.com</email>
<author confidence="0.808873">Keith</author>
<affiliation confidence="0.750692">Google</affiliation>
<address confidence="0.897986">Z¨urich, Switzerland</address>
<email confidence="0.999824">kbhall@google.com</email>
<affiliation confidence="0.999571">Linkopings University</affiliation>
<address confidence="0.90138">Linkopings, Sweden</address>
<email confidence="0.974267">marho@ida.liu.se</email>
<abstract confidence="0.999731588235294">The goal of this work is to integrate query similarity metrics as features into a dense model that can be trained on large amounts of query log data, in order to rank query rewrites. We propose features that incorporate various notions of syntactic and semantic similarity in a generalized edit distance framework. We use the implicit feedback of user clicks on search results as weak labels in training linear ranking models on large data sets. We optimize different ranking objectives in a stochastic gradient descent framework. Our experiments show that a pairwise SVM ranker trained on multipartite rank levels outperforms other pairwise and listwise ranking methods under a variety of evaluation metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>A Halverson</author>
<author>K Kenthapadi</author>
<author>N Mishra</author>
<author>P Tsaparas</author>
</authors>
<title>Generating labels from clicks.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd ACM International Conference on Web Search and Data Mining,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="15900" citStr="Agrawal et al. (2009)" startWordPosition="2652" endWordPosition="2655"> p(i, ·) = Ewj Ni,j/N and similarly the probability of a term wj occurring in the target query is p(·, A = Ewi Ni,j/N. Plugging in these values in Eq. (1), we get the PMI(wi, wj) for term pair wi and wj, which are further normalized as described in Section 2.2. More explanation and evaluation of the features described in this section can be found in Ciaramita et al. (2010). 3 Learning to Rank from Co-Click Data 3.1 Extracting Weak Labels from Co-Clicks Several studies have shown that implicit feedback from clickstream data is a weaker signal than human relevance judgements. Joachims (2002) or Agrawal et al. (2009) presented techniques to convert clicks into labels that can be used for machine learning. Our goal is not to elicit relevance judgments from user clicks, but rather to relate queries by pivoting on commonly clicked search results. The hypothesis is that two queries are related if they lead to user clicks on the same documents for a large amount of documents. This approach is similar to the method proposed by Fitzpatrick and Dent (1997) who attempt to measure the relatedness between two queries by using the normalized intersection of the top 200 retrieval results. We add click information to t</context>
</contexts>
<marker>Agrawal, Halverson, Kenthapadi, Mishra, Tsaparas, 2009</marker>
<rawString>R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra, and P. Tsaparas. 2009. Generating labels from clicks. In Proceedings of the 2nd ACM International Conference on Web Search and Data Mining, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
</authors>
<title>Agglomerative clustering of a search engine query log.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’00),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2098" citStr="Beeferman and Berger (2000)" startWordPosition="303" endWordPosition="306">user query logs to learn query similarities. One set of approaches focuses on user reformulations of queries that differ only in one phrase, e.g., Jones et al. (2006). Such phrases are then identified as candidate expansion terms, and filtered by various signals such as co-occurrence in similar sessions, or loglikelihood ratio of original and expansion phrase. Other approaches focus on the relation of queries and search results, either by clustering queries based *The work presented in this paper was done while the authors were visiting Google Research, Z¨urich. on their search results, e.g., Beeferman and Berger (2000), or by deploying the graph of queries and results to find related queries, e.g., Sahami and Heilman (2006). The approach closest to ours is that of Jones et al. (2006). Similar to their approach, we create a training set of candidate query rewrites from user query logs, and use it to train learners. While the dataset used in Jones et al. (2006) is in the order of a few thousand query-rewrite pairs, our dataset comprises around 1 billion query-rewrite pairs. Clearly, manual labeling of rewrite quality is not feasible for our dataset, and perhaps not even desirable. Instead, our intent is to le</context>
</contexts>
<marker>Beeferman, Berger, 2000</marker>
<rawString>Doug Beeferman and Adam Berger. 2000. Agglomerative clustering of a search engine query log. In Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’00), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boldi</author>
<author>F Bonchi</author>
<author>C Castillo</author>
<author>S Vigna</author>
</authors>
<title>From ’Dango’ to ’Japanese cakes’: Query reformulation models and patterns.</title>
<date>2009</date>
<booktitle>In Proceedings of Web Intelligence.</booktitle>
<publisher>IEEE Cs Press.</publisher>
<contexts>
<context position="6037" citStr="Boldi et al. (2009)" startWordPosition="949" endWordPosition="952">of wi and wj; i.e., the probability of the two strings occurring together. We define PMI as follows: PMI(wi, wj) = log p(wi, wj) (1) p(wi)p(wj) PMI has been introduced by Church and Hanks (1990) as word assosiatio ratio, and since then been used extensively to model semantic similarity. Among several desirable properties, it correlates well with human judgments (Recchia and Jones, 2009). 2.2 Taxonomic normalizations As pointed out in earlier work, query transitions tend to correlate with taxonomic relations such as generalization and specialization (Lau and Horvitz, 1999; Rieh and Xie, 2006). Boldi et al. (2009) show how knowledge of transition types can positively impact query reformulation. We would like to exploit this information as well. However, rather than building a dedicated supervised classifier for this task we try to capture it directly at the source. First, we notice how string features; e.g., length, and edit distance already model this phenomenon to some extent, and in fact are part of the features used in Boldi et al. (2009). However, these measures are not always accurate and it is easy to find counterexamples both at the term level (e.g., “camping” to “outdoor activities” is a gener</context>
<context position="10047" citStr="Boldi et al., 2009" startWordPosition="1613" endWordPosition="1616"> cost function c, the cost of an alignment is c(w) = Eni�s c(wi). The Levenshtein distance, or edit distance, defined over V , dV (wi,wj) between two sequences is the cost of the least expensive sequence of edit operations which transforms wi into wj (Levenshtein, 1966). The distance computation can be performed via dynamic programming in time O(|wi||wj|). Similarity at the string, i.e., character or term, level is an indicator of semantic similarity. Edit distance captures the amount of overlap between the queries as sequences of symbols and has been previously used in information retrieval (Boldi et al., 2009; Jones et al., 2006). We use two basic Levenshtein distance models. The first, called Edit1 (E1), employs a unit cost function for each of the three operations. That is, given 1The values are computed from Web counts. a finite vocabulary T containing all terms occurring in queries: ba, b E T, cEs(a, b) = 1 if(a =� b), 0 else. The second, called Edit2 (E2), uses unit costs for insertion and deletion, but computes the characterbased edit distance between two terms to decide on the substitution cost. If two terms are very similar at the character level, then the cost of substitution is lower. Gi</context>
</contexts>
<marker>Boldi, Bonchi, Castillo, Vigna, 2009</marker>
<rawString>P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009. From ’Dango’ to ’Japanese cakes’: Query reformulation models and patterns. In Proceedings of Web Intelligence. IEEE Cs Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of GSCL.</booktitle>
<contexts>
<context position="7767" citStr="Bouma, 2009" startWordPosition="1236" endWordPosition="1237">constrained to non-negative values, normalization will bound PMI to the range between 0 and 1. The first type of normalization, called joint normalization, uses the negative log joint probability and is defined as PMI(J)(wi, wj) = PMI(wi, wj)/−log(p(wi, wj)). The jointly normalized PMI(J) is a symmetric measure between wi and wj in the sense that PMI(J)(wi, wj) = PMI(J)(wj, wi). Intuitively it is a measure of the amount of shared information between the two strings relative to the sum of individual strings information. The advantages of the joint normalization of PMI have been noticed before (Bouma, 2009). To capture asymmetries in the relation between two strings, we introduce two non-symmetric normalizations which also bound the measure between 0 and 1. The second normalization is called specialization normalization and is defined as PMI(S)(wi, wj) = PMI(wi, wj)/ − log(p(wi)). The reason we call it specialization is that PMI(S) favors pairs where the second string is a specializa475 tion of the first one. For instance, PMI(S) is at its maximum when p(wi, wj) = p(wj) and that means the conditional probability p(wi|wj) is 1 which is an indication of a specialization relation. The last normaliz</context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>G. Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. In Proceedings of GSCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
<author>Rajiv Khanna</author>
<author>Uma Sawant</author>
<author>Chiru Bhattacharayya</author>
</authors>
<title>Structured learning for non-smooth ranking losses.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD’08),</booktitle>
<location>Las Vegas, NV.</location>
<contexts>
<context position="22957" citStr="Chakrabarti et al. (2008)" startWordPosition="3878" endWordPosition="3881">xperiments we used a positionsensitive prediction loss function 1 L(yqi, yqj) = | defined on the difference of inverted ranks. The margin-rescaled pairwise hinge loss is then defined as follows: yqi − 1| yqj by stochastic updating wt+1 = wt − ηtgt where ηt is a learning rate, and gt is the gradient gt = ∇`(w) where ∇`(w)_� ∂ ∂ ∂ aw1 `(w), ∂w2 `(w), ... ∂wn 3.2.2 Listwise Hinge Loss Standard ranking evaluation metrics such as (Mean) Average Precision (Manning et al., 2008) are defined on permutations of whole lists and are not decomposable over instances. Joachims (2005), Yue et al. (2007), or Chakrabarti et al. (2008) have proposed multivariate SVM models to optimize such listwise evaluation metrics. The central idea is to formalize the evaluation metric as a prediction loss function L, and incorporate L via margin rescaling into the hinge loss function, such that an upper bound on the prediction loss is achieved (see Tsochantaridis et al. (2004), Proposition 2). The loss function is given by the following listwise hinge loss: `lh(w) = (L(yq, π*q)−�w, φ(xq, yq) − φ(xq, π*q)))+ where π* q is the maximizer of the maxπqEΠq\yq L(yq, π*q) + �w, φ(xq, π*q)) LAP(yq, πq) = 1 − AP(yq, πq) where AP is defined as fol</context>
</contexts>
<marker>Chakrabarti, Khanna, Sawant, Bhattacharayya, 2008</marker>
<rawString>Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and Chiru Bhattacharayya. 2008. Structured learning for non-smooth ranking losses. In Proceedings of the 14th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD’08), Las Vegas, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>S Sathiya Keerthi</author>
</authors>
<title>Efficient algorithms for ranking with SVMs.</title>
<date>2010</date>
<journal>Information Retrieval Journal.</journal>
<contexts>
<context position="21794" citStr="Chapelle and Keerthi (2010)" startWordPosition="3676" endWordPosition="3679">nd each query, which can be done efficiently using the greedy algorithm proposed by Yue et al. (2007). We will refer to this method as the SVM-MAP model. 3.2.3 Pairwise Hinge Loss for Bipartite and Multipartite Ranking Joachims (2002) proposed an SVM method that defines the ranking problem as a pairwise classification problem. Cortes et al. (2007) extended this method to a magnitude-preserving version by penalizing a pairwise misranking by the magnitude of the difference in preference labels. A position-sensitive penalty for pairwise ranking SVMs was proposed by Riezler and De Bona (2009) and Chapelle and Keerthi (2010), and earlier for perceptrons by Shen and Joshi (2005). In the latter approaches, the magnitude of the difference in inverted ranks is accrued for each misranked pair. The idea is to impose an increased penalty for misrankings at the top of the list, and for misrankings that involve a difference of several rank levels. Similar to the listwise case, we can view the penalty as a prediction loss function, and incorporate it into the hinge loss function by rescaling the margin by a pairwise prediction loss function L(yqi, yqj). In our experiments we used a positionsensitive prediction loss functio</context>
</contexts>
<marker>Chapelle, Keerthi, 2010</marker>
<rawString>Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient algorithms for ranking with SVMs. Information Retrieval Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="5612" citStr="Church and Hanks (1990)" startWordPosition="885" endWordPosition="888">ity measures we describe below, we employ pointwise mutual information (PMI) as a measure of the association between two terms or queries. Let wi and wj be two strings that we want to measure the amount of association between. Let p(wi) and p(wj) be the probability of observing wi and wj in a given model; e.g., relative frequencies estimated from occurrence counts in a corpus. We also define p(wi,wj) as the joint probability of wi and wj; i.e., the probability of the two strings occurring together. We define PMI as follows: PMI(wi, wj) = log p(wi, wj) (1) p(wi)p(wj) PMI has been introduced by Church and Hanks (1990) as word assosiatio ratio, and since then been used extensively to model semantic similarity. Among several desirable properties, it correlates well with human judgments (Recchia and Jones, 2009). 2.2 Taxonomic normalizations As pointed out in earlier work, query transitions tend to correlate with taxonomic relations such as generalization and specialization (Lau and Horvitz, 1999; Rieh and Xie, 2006). Boldi et al. (2009) show how knowledge of transition types can positively impact query reformulation. We would like to exploit this information as well. However, rather than building a dedicated</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Patrick Hanks. 1990. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Amac¸ Herdaˇgdelen</author>
<author>Daniel Mahler</author>
<author>Maria Holmqvist</author>
<author>Keith Hall</author>
<author>Stefan Riezler</author>
<author>Enrique Alfonseca</author>
</authors>
<title>Generalized syntactic and semantic models of query reformulation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd ACM SIGIR Conference,</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Ciaramita, Herdaˇgdelen, Mahler, Holmqvist, Hall, Riezler, Alfonseca, 2010</marker>
<rawString>Massimiliano Ciaramita, Amac¸ Herdaˇgdelen, Daniel Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler, and Enrique Alfonseca. 2010. Generalized syntactic and semantic models of query reformulation. In Proceedings of the 33rd ACM SIGIR Conference, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Mehryar Mohri</author>
<author>Asish Rastogi</author>
</authors>
<title>Magnitude-preserving ranking algorithms.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning (ICML’07),</booktitle>
<location>Corvallis, OR.</location>
<contexts>
<context position="21516" citStr="Cortes et al. (2007)" startWordPosition="3634" endWordPosition="3637">Note that the ranking scenario is in this case bipartite with yqi ∈ {1, 2}. The derivatives for `lh are as follows: ∂ `lh = { 0 if (�w, φ(xq, yq) − φ(xq, π*q))) ∂wk &gt; L(yq,π*q), −(φk(xq, yq) − φk(xq, π*q)) else. SGD optimization involves computing π* q for each feature and each query, which can be done efficiently using the greedy algorithm proposed by Yue et al. (2007). We will refer to this method as the SVM-MAP model. 3.2.3 Pairwise Hinge Loss for Bipartite and Multipartite Ranking Joachims (2002) proposed an SVM method that defines the ranking problem as a pairwise classification problem. Cortes et al. (2007) extended this method to a magnitude-preserving version by penalizing a pairwise misranking by the magnitude of the difference in preference labels. A position-sensitive penalty for pairwise ranking SVMs was proposed by Riezler and De Bona (2009) and Chapelle and Keerthi (2010), and earlier for perceptrons by Shen and Joshi (2005). In the latter approaches, the magnitude of the difference in inverted ranks is accrued for each misranked pair. The idea is to impose an increased penalty for misrankings at the top of the list, and for misrankings that involve a difference of several rank levels. S</context>
</contexts>
<marker>Cortes, Mohri, Rastogi, 2007</marker>
<rawString>Corinna Cortes, Mehryar Mohri, and Asish Rastogi. 2007. Magnitude-preserving ranking algorithms. In Proceedings of the 24th International Conference on Machine Learning (ICML’07), Corvallis, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Fitzpatrick</author>
<author>Mei Dent</author>
</authors>
<title>Automatic feedback using past queries: Social searching?</title>
<date>1997</date>
<booktitle>In Proceedings of the 20th Annual International ACM SIGIR Conference,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="16340" citStr="Fitzpatrick and Dent (1997)" startWordPosition="2728" endWordPosition="2731"> Labels from Co-Clicks Several studies have shown that implicit feedback from clickstream data is a weaker signal than human relevance judgements. Joachims (2002) or Agrawal et al. (2009) presented techniques to convert clicks into labels that can be used for machine learning. Our goal is not to elicit relevance judgments from user clicks, but rather to relate queries by pivoting on commonly clicked search results. The hypothesis is that two queries are related if they lead to user clicks on the same documents for a large amount of documents. This approach is similar to the method proposed by Fitzpatrick and Dent (1997) who attempt to measure the relatedness between two queries by using the normalized intersection of the top 200 retrieval results. We add click information to this setup, thus strengthening the preference for precision over recall in the extraction of related queries. ni,j(qs, qt) = 477 Table 1: Statistics of co-click data sets. train dev test number of queries 250,000 2,500 100 average number of 4,500 4,500 30 rewrites per query percentage of rewrites 0.2 0.2 43 with ≥ 10 coclicks In our experiments we created two ground-truth ranking scenarios from the co-click signals. In a first scenario, </context>
</contexts>
<marker>Fitzpatrick, Dent, 1997</marker>
<rawString>Larry Fitzpatrick and Mei Dent. 1997. Automatic feedback using past queries: Social searching? In Proceedings of the 20th Annual International ACM SIGIR Conference, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Halevy</author>
<author>Peter Norvig</author>
<author>Fernando Pereira</author>
</authors>
<title>The unreasonable effectiveness of data.</title>
<date>2009</date>
<journal>IEEE Intelligent Systems,</journal>
<pages>24--8</pages>
<contexts>
<context position="3018" citStr="Halevy et al., 2009" startWordPosition="467" endWordPosition="470">. While the dataset used in Jones et al. (2006) is in the order of a few thousand query-rewrite pairs, our dataset comprises around 1 billion query-rewrite pairs. Clearly, manual labeling of rewrite quality is not feasible for our dataset, and perhaps not even desirable. Instead, our intent is to learn from large amounts of user query log data. Such data permit to learn smooth models because of the effectiveness of large data sets to capture even rare aspects of language, and they also are available as in the wild, i.e., they reflect the actual input-output behaviour that we seek to automate (Halevy et al., 2009). We propose a technique to automatically create weak labels from co-click information in user query logs of search engines. The central idea is that two queries are related if they lead to user clicks on the same documents for a large amount of documents. A manual evaluation of a small subset showed that a determination of positive versus negative rewrites by thresholding the number of co-clicks correlates well with human judgements of similarity, thus justifying our method of eliciting labels from co-clicks. Similar to Jones et al. (2006), the features of our models are not based on word ide</context>
</contexts>
<marker>Halevy, Norvig, Pereira, 2009</marker>
<rawString>Alon Halevy, Peter Norvig, and Fernando Pereira. 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24:8–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD’08),</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="15875" citStr="Joachims (2002)" startWordPosition="2649" endWordPosition="2650">the source query is p(i, ·) = Ewj Ni,j/N and similarly the probability of a term wj occurring in the target query is p(·, A = Ewi Ni,j/N. Plugging in these values in Eq. (1), we get the PMI(wi, wj) for term pair wi and wj, which are further normalized as described in Section 2.2. More explanation and evaluation of the features described in this section can be found in Ciaramita et al. (2010). 3 Learning to Rank from Co-Click Data 3.1 Extracting Weak Labels from Co-Clicks Several studies have shown that implicit feedback from clickstream data is a weaker signal than human relevance judgements. Joachims (2002) or Agrawal et al. (2009) presented techniques to convert clicks into labels that can be used for machine learning. Our goal is not to elicit relevance judgments from user clicks, but rather to relate queries by pivoting on commonly clicked search results. The hypothesis is that two queries are related if they lead to user clicks on the same documents for a large amount of documents. This approach is similar to the method proposed by Fitzpatrick and Dent (1997) who attempt to measure the relatedness between two queries by using the normalized intersection of the top 200 retrieval results. We a</context>
<context position="21401" citStr="Joachims (2002)" startWordPosition="3617" endWordPosition="3618">mework. We minimize an empirical loss objective �min w xq,yq |Pq| e(w) � O(xqi)−O(xqj)sgn( − ). 7rqi 7rqj 478 Note that the ranking scenario is in this case bipartite with yqi ∈ {1, 2}. The derivatives for `lh are as follows: ∂ `lh = { 0 if (�w, φ(xq, yq) − φ(xq, π*q))) ∂wk &gt; L(yq,π*q), −(φk(xq, yq) − φk(xq, π*q)) else. SGD optimization involves computing π* q for each feature and each query, which can be done efficiently using the greedy algorithm proposed by Yue et al. (2007). We will refer to this method as the SVM-MAP model. 3.2.3 Pairwise Hinge Loss for Bipartite and Multipartite Ranking Joachims (2002) proposed an SVM method that defines the ranking problem as a pairwise classification problem. Cortes et al. (2007) extended this method to a magnitude-preserving version by penalizing a pairwise misranking by the magnitude of the difference in preference labels. A position-sensitive penalty for pairwise ranking SVMs was proposed by Riezler and De Bona (2009) and Chapelle and Keerthi (2010), and earlier for perceptrons by Shen and Joshi (2005). In the latter approaches, the magnitude of the difference in inverted ranks is accrued for each misranked pair. The idea is to impose an increased pena</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the 8th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD’08), New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>A support vector method for multivariate performance measures.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning (ICML’05),</booktitle>
<location>Bonn, Germany.</location>
<contexts>
<context position="22908" citStr="Joachims (2005)" startWordPosition="3871" endWordPosition="3872">ion loss function L(yqi, yqj). In our experiments we used a positionsensitive prediction loss function 1 L(yqi, yqj) = | defined on the difference of inverted ranks. The margin-rescaled pairwise hinge loss is then defined as follows: yqi − 1| yqj by stochastic updating wt+1 = wt − ηtgt where ηt is a learning rate, and gt is the gradient gt = ∇`(w) where ∇`(w)_� ∂ ∂ ∂ aw1 `(w), ∂w2 `(w), ... ∂wn 3.2.2 Listwise Hinge Loss Standard ranking evaluation metrics such as (Mean) Average Precision (Manning et al., 2008) are defined on permutations of whole lists and are not decomposable over instances. Joachims (2005), Yue et al. (2007), or Chakrabarti et al. (2008) have proposed multivariate SVM models to optimize such listwise evaluation metrics. The central idea is to formalize the evaluation metric as a prediction loss function L, and incorporate L via margin rescaling into the hinge loss function, such that an upper bound on the prediction loss is achieved (see Tsochantaridis et al. (2004), Proposition 2). The loss function is given by the following listwise hinge loss: `lh(w) = (L(yq, π*q)−�w, φ(xq, yq) − φ(xq, π*q)))+ where π* q is the maximizer of the maxπqEΠq\yq L(yq, π*q) + �w, φ(xq, π*q)) LAP(yq</context>
</contexts>
<marker>Joachims, 2005</marker>
<rawString>Thorsten Joachims. 2005. A support vector method for multivariate performance measures. In Proceedings of the 22nd International Conference on Machine Learning (ICML’05), Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Benjamin Rey</author>
<author>Omid Madani</author>
<author>Wiley Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th International World Wide Web conference (WWW’06),</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1637" citStr="Jones et al. (2006)" startWordPosition="231" endWordPosition="234">t ranking objectives in a stochastic gradient descent framework. Our experiments show that a pairwise SVM ranker trained on multipartite rank levels outperforms other pairwise and listwise ranking methods under a variety of evaluation metrics. 1 Introduction Measures of query similarity are used for a wide range of web search applications, including query expansion, query suggestions, or listings of related queries. Several recent approaches deploy user query logs to learn query similarities. One set of approaches focuses on user reformulations of queries that differ only in one phrase, e.g., Jones et al. (2006). Such phrases are then identified as candidate expansion terms, and filtered by various signals such as co-occurrence in similar sessions, or loglikelihood ratio of original and expansion phrase. Other approaches focus on the relation of queries and search results, either by clustering queries based *The work presented in this paper was done while the authors were visiting Google Research, Z¨urich. on their search results, e.g., Beeferman and Berger (2000), or by deploying the graph of queries and results to find related queries, e.g., Sahami and Heilman (2006). The approach closest to ours i</context>
<context position="3564" citStr="Jones et al. (2006)" startWordPosition="557" endWordPosition="560">ual input-output behaviour that we seek to automate (Halevy et al., 2009). We propose a technique to automatically create weak labels from co-click information in user query logs of search engines. The central idea is that two queries are related if they lead to user clicks on the same documents for a large amount of documents. A manual evaluation of a small subset showed that a determination of positive versus negative rewrites by thresholding the number of co-clicks correlates well with human judgements of similarity, thus justifying our method of eliciting labels from co-clicks. Similar to Jones et al. (2006), the features of our models are not based on word identities, but instead on general string similarity metrics. This leads to dense rather than sparse feature spaces. The dif474 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474–482, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ference of our approach to Jones et al. (2006) lies in our particular choice of string similarity metrics. While Jones et al. (2006) deploy “syntactic” features such as Levenshtein distance, and “semantic” features such as </context>
<context position="10068" citStr="Jones et al., 2006" startWordPosition="1617" endWordPosition="1620">e cost of an alignment is c(w) = Eni�s c(wi). The Levenshtein distance, or edit distance, defined over V , dV (wi,wj) between two sequences is the cost of the least expensive sequence of edit operations which transforms wi into wj (Levenshtein, 1966). The distance computation can be performed via dynamic programming in time O(|wi||wj|). Similarity at the string, i.e., character or term, level is an indicator of semantic similarity. Edit distance captures the amount of overlap between the queries as sequences of symbols and has been previously used in information retrieval (Boldi et al., 2009; Jones et al., 2006). We use two basic Levenshtein distance models. The first, called Edit1 (E1), employs a unit cost function for each of the three operations. That is, given 1The values are computed from Web counts. a finite vocabulary T containing all terms occurring in queries: ba, b E T, cEs(a, b) = 1 if(a =� b), 0 else. The second, called Edit2 (E2), uses unit costs for insertion and deletion, but computes the characterbased edit distance between two terms to decide on the substitution cost. If two terms are very similar at the character level, then the cost of substitution is lower. Given a finite vocabula</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proceedings of the 15th International World Wide Web conference (WWW’06), Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lau</author>
<author>E Horvitz</author>
</authors>
<title>Patterns of search: analyzing and modeling web query refinement.</title>
<date>1999</date>
<booktitle>In Proceedings of the seventh international conference on User modeling,</booktitle>
<pages>119--128</pages>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="5995" citStr="Lau and Horvitz, 1999" startWordPosition="941" endWordPosition="944">so define p(wi,wj) as the joint probability of wi and wj; i.e., the probability of the two strings occurring together. We define PMI as follows: PMI(wi, wj) = log p(wi, wj) (1) p(wi)p(wj) PMI has been introduced by Church and Hanks (1990) as word assosiatio ratio, and since then been used extensively to model semantic similarity. Among several desirable properties, it correlates well with human judgments (Recchia and Jones, 2009). 2.2 Taxonomic normalizations As pointed out in earlier work, query transitions tend to correlate with taxonomic relations such as generalization and specialization (Lau and Horvitz, 1999; Rieh and Xie, 2006). Boldi et al. (2009) show how knowledge of transition types can positively impact query reformulation. We would like to exploit this information as well. However, rather than building a dedicated supervised classifier for this task we try to capture it directly at the source. First, we notice how string features; e.g., length, and edit distance already model this phenomenon to some extent, and in fact are part of the features used in Boldi et al. (2009). However, these measures are not always accurate and it is easy to find counterexamples both at the term level (e.g., “c</context>
</contexts>
<marker>Lau, Horvitz, 1999</marker>
<rawString>T. Lau and E. Horvitz. 1999. Patterns of search: analyzing and modeling web query refinement. In Proceedings of the seventh international conference on User modeling, pages 119–128. Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="9699" citStr="Levenshtein, 1966" startWordPosition="1560" endWordPosition="1561">rfect” generalization of the source1. 2.3 Syntactic measures Let V be a finite vocabulary and � be the null symbol. An edit operation: insertion, deletion or substitution, is a pair (a, b) E {V U {�} x V U {f}} \ {(�,�)}. An alignment between two sequences wi and wj is a sequence of edit operations w = (as, bs), ..., (an, bn). Given a nonnegative cost function c, the cost of an alignment is c(w) = Eni�s c(wi). The Levenshtein distance, or edit distance, defined over V , dV (wi,wj) between two sequences is the cost of the least expensive sequence of edit operations which transforms wi into wj (Levenshtein, 1966). The distance computation can be performed via dynamic programming in time O(|wi||wj|). Similarity at the string, i.e., character or term, level is an indicator of semantic similarity. Edit distance captures the amount of overlap between the queries as sequences of symbols and has been previously used in information retrieval (Boldi et al., 2009; Jones et al., 2006). We use two basic Levenshtein distance models. The first, called Edit1 (E1), employs a unit cost function for each of the three operations. That is, given 1The values are computed from Web counts. a finite vocabulary T containing </context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V.I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="29536" citStr="Noreen (1989)" startWordPosition="4991" endWordPosition="4992">utoff at rank 10 (NDCG@10), Area-underthe-ROC-curve (AUC), Precision@n3. As baselines we report a random permutation of rewrites (random), and the single dense feature that performed best on the development set (best-feature). The latter is the log-probability assigned to the query-rewrite pair by the probabilistic clustering model used for cost matrix estimation (see Section 2.5). P-values are reported in Table 3 for all pairwise comparisons of systems (except the random baseline) using an Approximate Randomization test where stratified shuffling is applied to results on the query level (see Noreen (1989)). The rows in Tables 2 and 3 are ranked according to MAP values of the systems. SVM-multipartite outperforms all other ranking systems under all evaluation metrics at a significance level &gt; 0.995. For all other pairwise comparisons of result differences, we find result differences of systems ranked next to each other to be not statistically significant. All systems outperform the random and best-feature baselines with statistically significant result differences. The distinctive advantage of the SVM-multipartite models lies in the possibil3For a definition of these metrics see Manning et al. </context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Recchia</author>
<author>M N Jones</author>
</authors>
<title>More data trumps smarter algorithms: comparing pointwise mutual information with latent semantic analysis.</title>
<date>2009</date>
<journal>Behavioral Research Methods,</journal>
<volume>41</volume>
<issue>3</issue>
<contexts>
<context position="5807" citStr="Recchia and Jones, 2009" startWordPosition="914" endWordPosition="917">e amount of association between. Let p(wi) and p(wj) be the probability of observing wi and wj in a given model; e.g., relative frequencies estimated from occurrence counts in a corpus. We also define p(wi,wj) as the joint probability of wi and wj; i.e., the probability of the two strings occurring together. We define PMI as follows: PMI(wi, wj) = log p(wi, wj) (1) p(wi)p(wj) PMI has been introduced by Church and Hanks (1990) as word assosiatio ratio, and since then been used extensively to model semantic similarity. Among several desirable properties, it correlates well with human judgments (Recchia and Jones, 2009). 2.2 Taxonomic normalizations As pointed out in earlier work, query transitions tend to correlate with taxonomic relations such as generalization and specialization (Lau and Horvitz, 1999; Rieh and Xie, 2006). Boldi et al. (2009) show how knowledge of transition types can positively impact query reformulation. We would like to exploit this information as well. However, rather than building a dedicated supervised classifier for this task we try to capture it directly at the source. First, we notice how string features; e.g., length, and edit distance already model this phenomenon to some exten</context>
</contexts>
<marker>Recchia, Jones, 2009</marker>
<rawString>G. Recchia and M.N. Jones. 2009. More data trumps smarter algorithms: comparing pointwise mutual information with latent semantic analysis. Behavioral Research Methods, 41(3):647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Y Rieh</author>
<author>H Xie</author>
</authors>
<title>Analysis of multiple query reformulations on the web: the interactive information retrieval context.</title>
<date>2006</date>
<journal>Inf. Process. Manage.,</journal>
<volume>42</volume>
<issue>3</issue>
<pages>768</pages>
<contexts>
<context position="6016" citStr="Rieh and Xie, 2006" startWordPosition="945" endWordPosition="948">he joint probability of wi and wj; i.e., the probability of the two strings occurring together. We define PMI as follows: PMI(wi, wj) = log p(wi, wj) (1) p(wi)p(wj) PMI has been introduced by Church and Hanks (1990) as word assosiatio ratio, and since then been used extensively to model semantic similarity. Among several desirable properties, it correlates well with human judgments (Recchia and Jones, 2009). 2.2 Taxonomic normalizations As pointed out in earlier work, query transitions tend to correlate with taxonomic relations such as generalization and specialization (Lau and Horvitz, 1999; Rieh and Xie, 2006). Boldi et al. (2009) show how knowledge of transition types can positively impact query reformulation. We would like to exploit this information as well. However, rather than building a dedicated supervised classifier for this task we try to capture it directly at the source. First, we notice how string features; e.g., length, and edit distance already model this phenomenon to some extent, and in fact are part of the features used in Boldi et al. (2009). However, these measures are not always accurate and it is easy to find counterexamples both at the term level (e.g., “camping” to “outdoor a</context>
</contexts>
<marker>Rieh, Xie, 2006</marker>
<rawString>S.Y. Rieh and H. Xie. 2006. Analysis of multiple query reformulations on the web: the interactive information retrieval context. Inf. Process. Manage., 42(3):751– 768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Fabio De Bona</author>
</authors>
<title>Simple risk bounds for position-sensitive max-margin ranking algorithms.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Advances in Ranking at the 23rd Annual Conference on Neural Information Processing Systems (NIPS’09),</booktitle>
<location>Whistler, Canada.</location>
<marker>Riezler, De Bona, 2009</marker>
<rawString>Stefan Riezler and Fabio De Bona. 2009. Simple risk bounds for position-sensitive max-margin ranking algorithms. In Proceedings of the Workshop on Advances in Ranking at the 23rd Annual Conference on Neural Information Processing Systems (NIPS’09), Whistler, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="27015" citStr="Riezler et al. (2002)" startWordPosition="4575" endWordPosition="4578">educes the number of comparisons to |Pq |= Er�1 Er j=i+1 |li||lj|. For bipartite ranking of p i=1 positive and n negative instances, |Pq |= p · n comparisons are necessary. 3.2.4 Log-linear Models for Bipartite Ranking A probabilistic model for bipartite ranking can be defined as the conditional probability of the set of relevant rewrites, i.e., rewrites at rank level 1, given all rewrites at rank levels 1 and 2. A formalization in the family of log-linear models yields the following logistic loss function `llm that was used for discriminative estimation from sets of partially labeled data in Riezler et al. (2002): ExqiExq e�w,φ(xqi)) The gradient of `llm is calculated as a difference between two expectations: `llm = −pw [φk|xq� yqi = 1] + pw [φk|xq] . ∂wk The SGD computation for the log-linear model is dominated by the computation of expectations for each query. The logistic loss for bipartite ranking is henceforth referred to as the log-linear model. 4 Experimental Results In the experiments reported in this paper, we trained linear ranking models on 1 billion query-rewrite pairs using 60 dense features, combined of the building blocks of syntactic and semantic similarity metrics under different esti</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="18691" citStr="Rubenstein and Goodenough (1965)" startWordPosition="3116" endWordPosition="3120">mple of query-rewrite pairs for manual evaluation. This dataset contains 100 queries for each of which we sampled 30 rewrites in descending order of co-clicks, resulting in a high percentage of 43% positive rewrites per query. The query-rewrite pairs were annotated by 3 raters as follows: First the raters were asked to rank the rewrites in descending order of relevance using a graphical user interface. Second the raters assigned rank labels and binary relevance scores to the ranked list of rewrites. This labeling strategy is similar to the labeling strategy for synonymy judgements proposed by Rubenstein and Goodenough (1965). Inter-rater agreements on binary relevance judgements, and agreement between rounded averaged human relevance scores and assignments of positive/negative labels by the co-click threshold of 10 produced a Kappa value of 0.65 (Siegel and Castellan, 1988). 3.2 Learning-to-Rank Query Rewrites 3.2.1 Notation Let 5 = {(xq, yq)}nq�1 be a training sample of queries, each represented by a set of rewrites xq = {xq1, ... , xq,n(q)}, and set of rank labels yq = {yq1, ... , yq,n(q)}, where n(q) is the number of rewrites for query q. For full rankings of all rewrites for a query, a total order on rewrites</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 10(3):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A webbased kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th International World Wide Web conference (WWW’06),</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="2205" citStr="Sahami and Heilman (2006)" startWordPosition="322" endWordPosition="326">that differ only in one phrase, e.g., Jones et al. (2006). Such phrases are then identified as candidate expansion terms, and filtered by various signals such as co-occurrence in similar sessions, or loglikelihood ratio of original and expansion phrase. Other approaches focus on the relation of queries and search results, either by clustering queries based *The work presented in this paper was done while the authors were visiting Google Research, Z¨urich. on their search results, e.g., Beeferman and Berger (2000), or by deploying the graph of queries and results to find related queries, e.g., Sahami and Heilman (2006). The approach closest to ours is that of Jones et al. (2006). Similar to their approach, we create a training set of candidate query rewrites from user query logs, and use it to train learners. While the dataset used in Jones et al. (2006) is in the order of a few thousand query-rewrite pairs, our dataset comprises around 1 billion query-rewrite pairs. Clearly, manual labeling of rewrite quality is not feasible for our dataset, and perhaps not even desirable. Instead, our intent is to learn from large amounts of user query log data. Such data permit to learn smooth models because of the effec</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D. Heilman. 2006. A webbased kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th International World Wide Web conference (WWW’06), Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>Ranking and reranking with perceptron.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>60--1</pages>
<contexts>
<context position="21848" citStr="Shen and Joshi (2005)" startWordPosition="3685" endWordPosition="3688">y algorithm proposed by Yue et al. (2007). We will refer to this method as the SVM-MAP model. 3.2.3 Pairwise Hinge Loss for Bipartite and Multipartite Ranking Joachims (2002) proposed an SVM method that defines the ranking problem as a pairwise classification problem. Cortes et al. (2007) extended this method to a magnitude-preserving version by penalizing a pairwise misranking by the magnitude of the difference in preference labels. A position-sensitive penalty for pairwise ranking SVMs was proposed by Riezler and De Bona (2009) and Chapelle and Keerthi (2010), and earlier for perceptrons by Shen and Joshi (2005). In the latter approaches, the magnitude of the difference in inverted ranks is accrued for each misranked pair. The idea is to impose an increased penalty for misrankings at the top of the list, and for misrankings that involve a difference of several rank levels. Similar to the listwise case, we can view the penalty as a prediction loss function, and incorporate it into the hinge loss function by rescaling the margin by a pairwise prediction loss function L(yqi, yqj). In our experiments we used a positionsensitive prediction loss function 1 L(yqi, yqj) = | defined on the difference of inver</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2005. Ranking and reranking with perceptron. Journal of Machine Learning Research, 60(1-3):73–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>John Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences. Second Edition.</title>
<date>1988</date>
<publisher>MacGraw-Hill,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="18945" citStr="Siegel and Castellan, 1988" startWordPosition="3154" endWordPosition="3157">otated by 3 raters as follows: First the raters were asked to rank the rewrites in descending order of relevance using a graphical user interface. Second the raters assigned rank labels and binary relevance scores to the ranked list of rewrites. This labeling strategy is similar to the labeling strategy for synonymy judgements proposed by Rubenstein and Goodenough (1965). Inter-rater agreements on binary relevance judgements, and agreement between rounded averaged human relevance scores and assignments of positive/negative labels by the co-click threshold of 10 produced a Kappa value of 0.65 (Siegel and Castellan, 1988). 3.2 Learning-to-Rank Query Rewrites 3.2.1 Notation Let 5 = {(xq, yq)}nq�1 be a training sample of queries, each represented by a set of rewrites xq = {xq1, ... , xq,n(q)}, and set of rank labels yq = {yq1, ... , yq,n(q)}, where n(q) is the number of rewrites for query q. For full rankings of all rewrites for a query, a total order on rewrites is assumed, with rank labels taking on values yqi ∈ {1, ... , n(q)}. Rewrites of equivalent rank can be specified by assuming a partial order on rewrites, where a multipartite ranking involves r &lt; n(q) relevance levels such that yqi ∈ {1, ... , r} , and</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Sidney Siegel and John Castellan. 1988. Nonparametric Statistics for the Behavioral Sciences. Second Edition. MacGraw-Hill, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning (ICML’04),</booktitle>
<location>Banff, Canada.</location>
<contexts>
<context position="23292" citStr="Tsochantaridis et al. (2004)" startWordPosition="3933" endWordPosition="3936">w1 `(w), ∂w2 `(w), ... ∂wn 3.2.2 Listwise Hinge Loss Standard ranking evaluation metrics such as (Mean) Average Precision (Manning et al., 2008) are defined on permutations of whole lists and are not decomposable over instances. Joachims (2005), Yue et al. (2007), or Chakrabarti et al. (2008) have proposed multivariate SVM models to optimize such listwise evaluation metrics. The central idea is to formalize the evaluation metric as a prediction loss function L, and incorporate L via margin rescaling into the hinge loss function, such that an upper bound on the prediction loss is achieved (see Tsochantaridis et al. (2004), Proposition 2). The loss function is given by the following listwise hinge loss: `lh(w) = (L(yq, π*q)−�w, φ(xq, yq) − φ(xq, π*q)))+ where π* q is the maximizer of the maxπqEΠq\yq L(yq, π*q) + �w, φ(xq, π*q)) LAP(yq, πq) = 1 − AP(yq, πq) where AP is defined as follows: AP(yq, πq) = E�=1 Prec(j) · ( |yqj − 2 |) Prec(j) = E k:πqkG (|yqk − 2|) . πqj 2We slightly abuse the notation yq to denote the permutation on xq that is induced by the rank labels. In case of full rankings, the permutation irq corresponding to ranking yq is unique. For multipartite and bipartite rankings, there is more than on</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the 21st International Conference on Machine Learning (ICML’04), Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yisong Yue</author>
<author>Thomas Finley</author>
<author>Filip Radlinski</author>
<author>Thorsten Joachims</author>
</authors>
<title>A support vector method for optimizing average precision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th Annual International ACM SIGIR Conference,</booktitle>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="20206" citStr="Yue et al. (2007)" startWordPosition="3403" endWordPosition="3406">s yqi ∈ {1, 2} with relevant rewrites at rank 1 and non-relevant rewrites at rank 2. Let the rewrites in xq be identified by the integers {1, 2, ... , n(q)}, and let a permutation 7rq on xq be defined as a bijection from {1, 2,... , n(q)} onto itself. Let IIq denote the set of all possible permutations on xq, and let 7rqi denote the rank position of xqi. Furthermore, let (i, j) denote a pair of rewrites in xq and let Pq be the set of all pairs in xq. We associate a feature function O(xqi) with each rewrite i = 1, ... , n(q) for each query q. Furthermore, a partial-order feature map as used in Yue et al. (2007) is created for each rewrite set as follows: 1 1 1 O(xq, 7rq) = (i,j)EPq The goal of learning a ranking over the rewrites xq for a query q can be achieved either by sorting the rewrites according to the rewrite-level ranking function f(xqi) = hw, O(xqi)i, or by finding the permutation that scores highest according to a query-level ranking function f(xq, 7rq) = hw, O(xq, 7rq)i. In the following, we will describe a variety of well-known ranking objectives, and extensions thereof, that are used in our experiments. Optimization is done in a stochastic gradient descent (SGD) framework. We minimize </context>
<context position="22927" citStr="Yue et al. (2007)" startWordPosition="3873" endWordPosition="3876"> L(yqi, yqj). In our experiments we used a positionsensitive prediction loss function 1 L(yqi, yqj) = | defined on the difference of inverted ranks. The margin-rescaled pairwise hinge loss is then defined as follows: yqi − 1| yqj by stochastic updating wt+1 = wt − ηtgt where ηt is a learning rate, and gt is the gradient gt = ∇`(w) where ∇`(w)_� ∂ ∂ ∂ aw1 `(w), ∂w2 `(w), ... ∂wn 3.2.2 Listwise Hinge Loss Standard ranking evaluation metrics such as (Mean) Average Precision (Manning et al., 2008) are defined on permutations of whole lists and are not decomposable over instances. Joachims (2005), Yue et al. (2007), or Chakrabarti et al. (2008) have proposed multivariate SVM models to optimize such listwise evaluation metrics. The central idea is to formalize the evaluation metric as a prediction loss function L, and incorporate L via margin rescaling into the hinge loss function, such that an upper bound on the prediction loss is achieved (see Tsochantaridis et al. (2004), Proposition 2). The loss function is given by the following listwise hinge loss: `lh(w) = (L(yq, π*q)−�w, φ(xq, yq) − φ(xq, π*q)))+ where π* q is the maximizer of the maxπqEΠq\yq L(yq, π*q) + �w, φ(xq, π*q)) LAP(yq, πq) = 1 − AP(yq, </context>
</contexts>
<marker>Yue, Finley, Radlinski, Joachims, 2007</marker>
<rawString>Yisong Yue, Thomas Finley, Filip Radlinski, and Thorsten Joachims. 2007. A support vector method for optimizing average precision. In Proceedings of the 30th Annual International ACM SIGIR Conference, Amsterdam, The Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>