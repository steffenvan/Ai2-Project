<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.679115">
Recognizing Named Entities in Tweets
</title>
<author confidence="0.799415">
Xiaohua Liu t †, Shaodian Zhang* §, Furu Wei †, Ming Zhou †
</author>
<affiliation confidence="0.916015">
$School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
§Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, 200240, China
†Microsoft Research Asia
Beijing, 100190, China
</affiliation>
<email confidence="0.9391765">
†{xiaoliu, fuwei, mingzhou}@microsoft.com
§ zhangsd.sjtu@gmail.com
</email>
<sectionHeader confidence="0.998301" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999656944444445">
The challenges of Named Entities Recogni-
tion (NER) for tweets lie in the insufficient
information in a tweet and the unavailabil-
ity of training data. We propose to com-
bine a K-Nearest Neighbors (KNN) classi-
fier with a linear Conditional Random Fields
(CRF) model under a semi-supervised learn-
ing framework to tackle these challenges. The
KNN based classifier conducts pre-labeling to
collect global coarse evidence across tweets
while the CRF model conducts sequential la-
beling to capture fine-grained information en-
coded in a tweet. The semi-supervised learn-
ing plus the gazetteers alleviate the lack of
training data. Extensive experiments show the
advantages of our method over the baselines
as well as the effectiveness of KNN and semi-
supervised learning.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998791818181818">
Named Entities Recognition (NER) is generally un-
derstood as the task of identifying mentions of rigid
designators from text belonging to named-entity
types such as persons, organizations and locations
(Nadeau and Sekine, 2007). Proposed solutions to
NER fall into three categories: 1) The rule-based
(Krupka and Hausman, 1998); 2) the machine learn-
ing based (Finkel and Manning, 2009; Singh et al.,
2010) ; and 3) hybrid methods (Jansche and Abney,
2002). With the availability of annotated corpora,
such as ACE05, Enron (Minkov et al., 2005) and
</bodyText>
<footnote confidence="0.93024">
∗ This work has been done while the author was visiting
Microsoft Research Asia.
</footnote>
<bodyText confidence="0.982112529411765">
CoNLL03 (Tjong Kim Sang and De Meulder, 2003),
the data driven methods now become the dominating
methods.
However, current NER mainly focuses on for-
mal text such as news articles (Mccallum and Li,
2003; Etzioni et al., 2005). Exceptions include stud-
ies on informal text such as emails, blogs, clini-
cal notes (Wang, 2009). Because of the domain
mismatch, current systems trained on non-tweets
perform poorly on tweets, a new genre of text,
which are short, informal, ungrammatical and noise
prone. For example, the average F1 of the Stan-
ford NER (Finkel et al., 2005) , which is trained
on the CoNLL03 shared task data set and achieves
state-of-the-art performance on that task, drops from
90.8% (Ratinov and Roth, 2009) to 45.8% on tweets.
Thus, building a domain specific NER for tweets
is necessary, which requires a lot of annotated tweets
or rules. However, manually creating them is tedious
and prohibitively unaffordable. Proposed solutions
to alleviate this issue include: 1) Domain adaption,
which aims to reuse the knowledge of the source do-
main in a target domain. Two recent examples are
Wu et al. (2009), which uses data that is informa-
tive about the target domain and also easy to be la-
beled to bridge the two domains, and Chiticariu et
al. (2010), which introduces a high-level rule lan-
guage, called NERL, to build the general and do-
main specific NER systems; and 2) semi-supervised
learning, which aims to use the abundant unlabeled
data to compensate for the lack of annotated data.
Suzuki and Isozaki (2008) is one such example.
Another challenge is the limited information in
tweet. Two factors contribute to this difficulty. One
</bodyText>
<page confidence="0.984541">
359
</page>
<note confidence="0.9795445">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 359–367,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.998411695652174">
is the tweet’s informal nature, making conventional
features such as part-of-speech (POS) and capital-
ization not reliable. The performance of current
NLP tools drops sharply on tweets. For example,
OpenNLP 1, the state-of-the-art POS tagger, gets
only an accuracy of 74.0% on our test data set. The
other is the tweet’s short nature, leading to the ex-
cessive abbreviations or shorthand in tweets, and
the availability of very limited context information.
Tackling this challenge, ideally, requires adapting
related NLP tools to fit tweets, or normalizing tweets
to accommodate existing tools, both of which are
hard tasks.
We propose a novel NER system to address these
challenges. Firstly, a K-Nearest Neighbors (KNN)
based classifier is adopted to conduct word level
classification, leveraging the similar and recently
labeled tweets. Following the two-stage predic-
tion aggregation methods (Krishnan and Manning,
2006), such pre-labeled results, together with other
conventional features used by the state-of-the-art
NER systems, are fed into a linear Conditional Ran-
dom Fields (CRF) (Lafferty et al., 2001) model,
which conducts fine-grained tweet level NER. Fur-
thermore, the KNN and CRF model are repeat-
edly retrained with an incrementally augmented
training set, into which high confidently labeled
tweets are added. Indeed, it is the combination of
KNN and CRF under a semi-supervised learning
framework that differentiates ours from the exist-
ing. Finally, following Lev Ratinov and Dan Roth
(2009), 30 gazetteers are used, which cover com-
mon names, countries, locations, temporal expres-
sions, etc. These gazetteers represent general knowl-
edge across domains. The underlying idea of our
method is to combine global evidence from KNN
and the gazetteers with local contextual information,
and to use common knowledge and unlabeled tweets
to make up for the lack of training data.
12,245 tweets are manually annotated as the test
data set. Experimental results show that our method
outperforms the baselines. It is also demonstrated
that integrating KNN classified results into the CRF
model and semi-supervised learning considerably
boost the performance.
Our contributions are summarized as follows.
</bodyText>
<footnote confidence="0.978858">
1http://sourceforge.net/projects/opennlp/
</footnote>
<listItem confidence="0.664161428571428">
1. We propose to a novel method that combines
a KNN classifier with a conventional CRF
based labeler under a semi-supervised learning
framework to combat the lack of information in
tweet and the unavailability of training data.
2. We evaluate our method on a human anno-
tated data set, and show that our method outper-
</listItem>
<bodyText confidence="0.950441222222222">
forms the baselines and that both the combina-
tion with KNN and the semi-supervised learn-
ing strategy are effective.
The rest of our paper is organized as follows. In
the next section, we introduce related work. In Sec-
tion 3, we formally define the task and present the
challenges. In Section 4, we detail our method. In
Section 5, we evaluate our method. Finally, Section
6 concludes our work.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99584325">
Related work can be roughly divided into three cat-
egories: NER on tweets, NER on non-tweets (e.g.,
news, bio-logical medicine, and clinical notes), and
semi-supervised learning for NER.
</bodyText>
<subsectionHeader confidence="0.993914">
2.1 NER on Tweets
</subsectionHeader>
<bodyText confidence="0.9998489">
Finin et al. (2010) use Amazons Mechanical Turk
service 2 and CrowdFlower 3 to annotate named en-
tities in tweets and train a CRF model to evaluate
the effectiveness of human labeling. In contrast, our
work aims to build a system that can automatically
identify named entities in tweets. To achieve this,
a KNN classifier with a CRF model is combined
to leverage cross tweets information, and the semi-
supervised learning is adopted to leverage unlabeled
tweets.
</bodyText>
<subsectionHeader confidence="0.99973">
2.2 NER on Non-Tweets
</subsectionHeader>
<bodyText confidence="0.999117625">
NER has been extensively studied on formal text,
such as news, and various approaches have been pro-
posed. For example, Krupka and Hausman (1998)
use manual rules to extract entities of predefined
types; Zhou and Ju (2002) adopt Hidden Markov
Models (HMM) while Finkel et al. (2005) use CRF
to train a sequential NE labeler, in which the BIO
(meaning Beginning, the Inside and the Outside of
</bodyText>
<footnote confidence="0.999943">
2https://www.mturk.com/mturk/
3http://crowdflower.com/
</footnote>
<page confidence="0.994366">
360
</page>
<bodyText confidence="0.9987999375">
an entity, respectively) schema is applied. Other confidently labeled but is also informative to its
methods, such as classification based on Maximum training set, which is used to re-train its model. Jiang
Entropy models and sequential application of Per- and Zhai (2007) propose a balanced bootstrapping
ceptron or Winnow (Collins, 2002), are also prac- algorithm and successfully apply it to NER. Their
ticed. The state-of-the-art system, e.g., the Stanford method is based on instance re-weighting, which
NER, can achieve an F1 score of over 92.0% on its allows the small amount of the bootstrapped train-
test set. ing sets to have an equal weight to the large source
Biomedical NER represents another line of active domain training set. Wu et al. (2009) propose an-
research. Machine learning based systems are com- other bootstrapping algorithm that selects bridging
monly used and outperform the rule based systems. instances from an unlabeled target domain, which
A state-of-the-art biomedical NER system (Yoshida are informative about the target domain and are also
and Tsujii, 2007) uses lexical features, orthographic easy to be correctly labeled. We adopt bootstrapping
features, semantic features and syntactic features, as well, but use human labeled tweets as seeds.
such as part-of-speech (POS) and shallow parsing. Another representative of semi-supervised learn-
A handful of work on other domains exists. For ing is learning a robust representation of the input
example, Wang (2009) introduces NER on clinical from unlabeled data. Miller et al. (2004) use word
notes. A data set is manually annotated and a linear clusters (Brown et al., 1992) learned from unla-
CRF model is trained, which achieves an F-score of beled text, resulting in a performance improvement
81.48% on their test data set; Downey et al. (2007) of NER. Guo et al. (2009) introduce Latent Seman-
employ capitalization cues and n-gram statistics to tic Association (LSA) for NER. In our pilot study of
locate names of a variety of classes in web text; NER for tweets, we adopt bag-of-words models to
most recently, Chiticariu et al. (2010) design and im- represent a word in tweet, to concentrate our efforts
plement a high-level language NERL that is tuned on combining global evidence with local informa-
to simplify the process of building, understanding, tion and semi-supervised learning. We leave it to
and customizing complex rule-based named-entity our future work to explore which is the best input
annotators for different domains. representation for our task.
Ratinov and Roth (2009) systematically study 3 Task Definition
the challenges in NER, compare several solutions We first introduce some background about tweets,
and report some interesting findings. For exam- then give a formal definition of the task.
ple, they show that a conditional model that does 3.1 The Tweets
not consider interactions at the output level per- A tweet is a short text message containing no
forms comparably to beam search or Viterbi, and more than 140 characters in Twitter, the biggest
that the BILOU (Beginning, the Inside and the Last micro-blog service. Here is an example of
tokens of multi-token chunks as well as Unit-length tweets: “mycraftingworld: #Win Microsoft Of-
chunks) encoding scheme significantly outperforms fice 2010 Home and Student *2Winners* #Con-
the BIO schema (Beginning, the Inside and Outside test from @office and @momtobedby8 #Giveaway
of a chunk). http://bit.ly/bCsLOr ends 11/14”, where ”mycraft-
In contrast to the above work, our study focuses ingworld” is the name of the user who published
on NER for tweets, a new genre of texts, which are this tweet. Words beginning with the “#” char-
short, noise prone and ungrammatical. acter, like “”#Win”, “#Contest” and “#Giveaway”,
2.3 Semi-supervised Learning for NER are hash tags, usually indicating the topics of the
Semi-supervised learning exploits both labeled and tweet; words starting with “@”, like “@office”
un-labeled data. It proves useful when labeled data and “@momtobedby8”, represent user names, and
is scarce and hard to construct while unlabeled data “http://bit.ly/bCsLOr” is a shortened link.
is abundant and easy to access. Twitter users are interested in named entities, such
Bootstrapping is a typical semi-supervised learn-
ing method. It iteratively adds data that has been
361
</bodyText>
<subsectionHeader confidence="0.561055">
4.1 Method Overview
</subsectionHeader>
<bodyText confidence="0.818092804878049">
NER task can be naturally divided into two sub-
tasks, i.e., boundary detection and type classifica-
tion. Following the common practice , we adopt
a sequential labeling approach to jointly resolve
these sub-tasks, i.e., for each word in the input
tweet, a label is assigned to it, indicating both the
boundary and entity type. Inspired by Ratinov and
Roth (2009), we use the BILOU schema.
Algorithm 1 outlines our method, where: trains
and traink denote two machine learning processes
to get the CRF labeler and the KNN classifier, re-
spectively; repr,,, converts a word in a tweet into a
bag-of-words vector; the reprt function transforms
a tweet into a feature matrix that is later fed into the
CRF model; the knn function predicts the class of
a word; the update function applies the predicted
class by KNN to the inputted tweet; the crf function
conducts word level NE labeling;τ and γ represent
the minimum labeling confidence of KNN and CRF,
respectively, which are experimentally set to 0.1 and
0.001; N (1,000 in our work) denotes the maximum
number of new accumulated training data.
Our method, as illustrated in Algorithm 1, repeat-
edly adds the new confidently labeled tweets to the
training set 4 and retrains itself once the number
of new accumulated training data goes above the
threshold N. Algorithm 1 also demonstrates one
striking characteristic of our method: A KNN clas-
sifier is applied to determine the label of the current
word before the CRF model. The labels of the words
that confidently assigned by the KNN classifier are
treated as visible variables for the CRF model.
Figure 1: Portion of different types of named entities in
tweets. This is based on an investigation of 12,245 ran-
domly sampled tweets, which are manually labeled.
as person names, organization names and product
names, as evidenced by the abundant named entities
in tweets. According to our investigation on 12,245
randomly sampled tweets that are manually labeled,
about 46.8% have at least one named entity. Figure
1 shows the portion of named entities of different
types.
3.2 The Task
Given a tweet as input, our task is to identify both the
boundary and the class of each mention of entities of
predefined types. We focus on four types of entities
in our study, i.e., persons, organizations, products,
and locations, which, according to our investigation
as shown in Figure 1, account for 89.0% of all the
named entities.
Here is an example illustrating our task.
The input is “...Me without you is like an
iphone without apps, Justin Bieber without
his hair, Lady gaga without her telephone, it
just wouldn...” The expected output is as fol-
lows:“...Me without you is like an &lt;PRODUCT
&gt;iphone&lt;/PRODUCT&gt;without apps,
&lt;PERSON&gt;Justin Bieber&lt;/PERSON&gt;without his
hair,&lt;PERSON&gt;Lady gaga&lt;/PERSON&gt; without
her telephone, it just wouldn...”, meaning that
“iphone” is a product, while “Justin Bieber” and
“Lady gaga” are persons.
4 Our Method
Now we present our solution to the challenging task
of NER for tweets. An overview of our method
is first given, followed by detailed discussion of its
core components.
362
4.2 Model
Our model is hybrid in the sense that a KNN clas-
sifier and a CRF model are sequentially applied to
the target tweet, with the goal that the KNN classi-
fier captures global coarse evidence while the CRF
model fine-grained information encoded in a single
tweet and in the gazetteers. Algorithm 2 outlines the
training process of KNN, which records the labeled
word vector for every type of label.
Algorithm 3 describes how the KNN classifier
4The training set is has a maximum allowable number of
items, which is 10,000 in our work. Adding an item into it will
cause the oldest one being removed if it is full.
Algorithm 1 NER for Tweets.
</bodyText>
<listItem confidence="0.876467307692308">
Require: Tweet stream i; output stream o.
Require: Training tweets ts; gazetteers ga.
1: Initialize ls, the CRF labeler: ls = trains(ts).
2: Initialize lk, the KNN classifier: lk = traink(ts).
3: Initialize n, the # of new training tweets: n = 0.
4: while Pop a tweet t from i and t =� null do
5: for Each word w E t do
6: Get the feature vector ⃗w: w⃗ =
reprw(w, t).
7: Classify w⃗ with knn: (c, cf) =
knn(lk, ⃗w).
8: if cf &gt; T then
9: Pre-label: t = update(t, w, c).
</listItem>
<figure confidence="0.821058176470588">
10: end if
11: end for
12: Get the feature vector ⃗t: t⃗ = reprt(t, ga).
13: Label t⃗with crf: (t,cf) = crf(ls,⃗t).
14: Put labeled result (t, cf) into o.
15: if cf &gt; y then
16: Add labeled result t to ts , n = n + 1.
17: end if
18: if n &gt; N then
19: Retrain ls: ls = trains(ts).
20: Retrain lk: lk = traink(ts).
21: n = 0.
22: end if
23: end while
24: return o.
Algorithm 2 KNN Training.
Require: Training tweets ts.
</figure>
<listItem confidence="0.984527">
1: Initialize the classifier lk:lk = ∅.
2: for Each tweet t E ts do
3: for Each word,label pair (w, c) E t do
4: Get the feature vector ⃗w: w⃗ =
reprw(w, t).
5: Add the w⃗ and c pair to the classifier: lk =
lk U 1(⃗w,c)}.
6: end for
7: end for
8: return KNN classifier lk.
</listItem>
<bodyText confidence="0.999593571428571">
predicts the label of the word. In our work, K is
experimentally set to 20, which yields the best per-
formance.
Two desirable properties of KNN make it stand
out from its alternatives: 1) It can straightforwardly
incorporate evidence from new labeled tweets and
retraining is fast; and 2) combining with a CRF
</bodyText>
<figure confidence="0.8509934">
Algorithm 3 KNN predication.
Require: KNN classifier lk ;word vector ⃗w.
1: Initialize nb, the neighbors of ⃗w: nb =
neigbors(lk, ⃗w).
2: Calculate the predicted class c*: c* =
∑
argmaxc (⃗w′ ,c′ )Enb S(c, c′) · cos(⃗w, ⃗w′).
3: Calculate the labeling confidence cf: cf =
∑( ⃗w′ ,c′ )∈nb δ(c,c′ )·cos(⃗w,⃗w′)
∑( ⃗w′ ,c′ )∈nb cos(⃗w,⃗w′ ) .
</figure>
<figureCaption confidence="0.405454">
4: return The predicted label c* and its confidence cf.
</figureCaption>
<bodyText confidence="0.999953375">
model, which is good at encoding the subtle interac-
tions between words and their labels, compensates
for KNN’s incapability to capture fine-grained evi-
dence involving multiple decision points.
The Linear CRF model is used as the fine model,
with the following considerations: 1) It is well-
studied and has been successfully used in state-of-
the-art NER systems (Finkel et al., 2005; Wang,
2009); 2) it can output the probability of a label
sequence, which can be used as the labeling con-
fidence that is necessary for the semi-supervised
learning framework.
In our experiments, the CRF++ 5 toolkit is used to
train a linear CRF model. We have written a Viterbi
decoder that can incorporate partially observed la-
bels to implement the crf function in Algorithm 1.
</bodyText>
<subsectionHeader confidence="0.950974">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.999944133333333">
Given a word in a tweet, the KNN classifier consid-
ers a text window of size 5 with the word in the mid-
dle (Zhang and Johnson, 2003), and extracts bag-of-
word features from the window as features. For each
word, our CRF model extracts similar features as
Wang (2009) and Ratinov and Roth (2009), namely,
orthographic features, lexical features and gazetteers
related features. In our work, we use the gazetteers
provided by Ratinov and Roth (2009).
Two points are worth noting here. One is that
before feature extraction for either the KNN or the
CRF, stop words are removed. The stop words
used here are mainly from a set of frequently-used
words 6. The other is that tweet meta data is normal-
ized, that is, every link becomes *LINK* and every
</bodyText>
<footnote confidence="0.965821333333333">
5http://crfpp.sourceforge.net/
6http://www.textfixer.com/resources/common-english-
words.txt
</footnote>
<page confidence="0.999235">
363
</page>
<bodyText confidence="0.9996106">
account name becomes *ACCOUNT*. Hash tags
are treated as common words.
outperforms the baselines. The contributions of the
combination of KNN and CRF as well as the semi-
supervised learning are studied, respectively.
</bodyText>
<subsectionHeader confidence="0.954489">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999992205128205">
We now discuss several design considerations re-
lated to the performance of our method, i.e., addi-
tional features, gazetteers and alternative models.
Additional Features. Features related to chunking
and parsing are not adopted in our final system, be-
cause they give only a slight performance improve-
ment while a lot of computing resources are required
to extract such features. The ineffectiveness of these
features is linked to the noisy and informal nature of
tweets. Word class (Brown et al., 1992) features are
not used either, which prove to be unhelpful for our
system. We are interested in exploring other tweet
representations, which may fit our NER task, for ex-
ample the LSA models (Guo et al., 2009).
Gazetteers. In our work, gazetteers prove to be sub-
stantially useful, which is consistent with the obser-
vation of Ratinov and Roth (2009). However, the
gazetteers used in our work contain noise, which
hurts the performance. Moreover, they are static,
directly from Ratinov and Roth (2009), thus with
a relatively lower coverage, especially for person
names and product names in tweets. We are devel-
oping tools to clean the gazetteers. In future, we plan
to feed the fresh entities correctly identified from
tweets back into the gazetteers. The correctness of
an entity can rely on its frequency or other evidence.
Alternative Models. We have replaced KNN by
other classifiers, such as those based on Maximum
Entropy and Support Vector Machines, respectively.
KNN consistently yields comparable performance,
while enjoying a faster retraining speed. Similarly,
to study the effectiveness of the CRF model, it is re-
placed by its alternations, such as the HMM labeler
and a beam search plus a maximum entropy based
classifier. In contrast to what is reported by Ratinov
and Roth (2009), it turns out that the CRF model
gives remarkably better results than its competitors.
Note that all these evaluations are on the same train-
ing and testing data sets as described in Section 5.1.
</bodyText>
<sectionHeader confidence="0.999756" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.996842">
In this section, we evaluate our method on a man-
ually annotated data set and show that our system
</bodyText>
<subsectionHeader confidence="0.980212">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999978933333333">
We use the Twigg SDK 7 to crawl all tweets
from April 201h 2010 to April 251h 2010, then drop
non-English tweets and get about 11,371,389, from
which 15,800 tweets are randomly sampled, and are
then labeled by two independent annotators, so that
the beginning and the end of each named entity are
marked with &lt;TYPE&gt; and &lt;/TYPE&gt;, respectively.
Here TYPE is PERSON, PRODUCT, ORGANIZA-
TION or LOCATION. 3555 tweets are dropped be-
cause of inconsistent annotation. Finally we get
12,245 tweets, forming the gold-standard data set.
Figure 1 shows the portion of named entities of dif-
ferent types. On average, a named entity has 1.2
words. The gold-standard data set is evenly split into
two parts: One for training and the other for testing.
</bodyText>
<subsectionHeader confidence="0.998258">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999967666666667">
For every type of named entity, Precision (Pre.), re-
call (Rec.) and F1 are used as the evaluation met-
rics. Precision is a measure of what percentage the
output labels are correct, and recall tells us to what
percentage the labels in the gold-standard data set
are correctly labeled, while F1 is the harmonic mean
of precision and recall. For the overall performance,
we use the average Precision, Recall and F1, where
the weight of each name entity type is proportional
to the number of entities of that type. These metrics
are widely used by existing NER systems to evaluate
their performance.
</bodyText>
<subsectionHeader confidence="0.998605">
5.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999606555555556">
Two systems are used as baselines: One is the
dictionary look-up system based on the gazetteers;
the other is the modified version of our system
without KNN and semi-supervised learning. Here-
after these two baselines are called NERDIC and
NERBA, respectively. The OpenNLP and the Stan-
ford parser (Klein and Manning, 2003) are used to
extract linguistic features for the baselines and our
method.
</bodyText>
<footnote confidence="0.9996085">
7It is developed by the Bing social search team, and cur-
rently is only internally available.
</footnote>
<page confidence="0.991786">
364
</page>
<table confidence="0.999894">
System Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NERBA 83.6 68.6 75.4
NERDIC 32.6 25.4 28.6
</table>
<tableCaption confidence="0.978988">
Table 1: Overall experimental results.
</tableCaption>
<table confidence="0.99996075">
System Pre.(%) Rec.(%) F1(%)
NERCB 78.4 74.5 76.4
NERBA 83.6 68.4 75.2
NERDIC 37.1 29.7 33.0
</table>
<tableCaption confidence="0.964208">
Table 2: Experimental results on PERSON.
</tableCaption>
<table confidence="0.99996325">
System Pre.(%) Rec.(%) F1(%)
NERCB 80.3 77.5 78.9
NERBA 81.6 69.7 75.2
NERDIC 30.2 30.0 30.1
</table>
<tableCaption confidence="0.985426">
Table 4: Experimental results on LOCATION.
</tableCaption>
<table confidence="0.99995825">
System Pre.(%) Rec.(%) F1(%)
NERCB 83.2 60.4 70.0
NERBA 87.6 52.5 65.7
NERDIC 54.5 11.8 19.4
</table>
<tableCaption confidence="0.999532">
Table 5: Experimental results on ORGANIZATION.
</tableCaption>
<subsectionHeader confidence="0.997568">
5.4 Basic Results
</subsectionHeader>
<bodyText confidence="0.9999041">
Table 1 shows the overall results for the baselines
and ours with the name NERCB. Here our sys-
tem is trained as described in Algorithm 1, combin-
ing a KNN classifier and a CRF labeler, with semi-
supervised learning enabled. As can be seen from
Table 1, on the whole, our method significantly out-
performs (with p &lt; 0.001) the baselines. Tables 2-5
report the results on each entity type, indicating that
our method consistently yields better results on all
entity types.
</bodyText>
<subsectionHeader confidence="0.998052">
5.5 Effects of KNN Classifier
</subsectionHeader>
<bodyText confidence="0.999022384615385">
Table 6 shows the performance of our method
without combining the KNN classifier, denoted by
NERCB−KNN. A drop in performance is observed
then. We further check the confidently predicted la-
bels of the KNN classifier, which account for about
22.2% of all predications, and find that its F1 is as
high as 80.2% while the baseline system based on
the CRF model achieves only an F1 of 75.4%. This
largely explains why the KNN classifier helps the
CRF labeler. The KNN classifier is replaced with
its competitors, and only a slight difference in per-
formance is observed. We do observe that retraining
KNN is obviously faster.
</bodyText>
<table confidence="0.999706">
System Pre.(%) Rec.(%) F1(%)
NERCB 81.3 65.4 72.5
NERBA 82.5 58.4 68.4
NERDIC 8.2 6.1 7.0
</table>
<tableCaption confidence="0.999866">
Table 3: Experimental results on PRODUCT.
</tableCaption>
<subsectionHeader confidence="0.974299">
5.6 Effects of the CRF Labeler
</subsectionHeader>
<bodyText confidence="0.999981384615385">
Similarly, the CRF model is replaced by its alterna-
tives. As is opposite to the finding of Ratinov and
Roth (2009), the CRF model gives remarkably bet-
ter results, i.e., 2.1% higher in F1 than its best fol-
lowers (with p &lt; 0.001). Table 7 shows the overall
performance of the CRF labeler with various feature
set combinations, where Fo, Fl and Fg denote the
orthographic features, the lexical features and the
gazetteers related features, respectively. It can be
seen from Table 7 that the lexical and gazetteer re-
lated features are helpful. Other advanced features
such as chunking are also explored but with no sig-
nificant improvement.
</bodyText>
<subsectionHeader confidence="0.997946">
5.7 Effects of Semi-supervised Learning
</subsectionHeader>
<bodyText confidence="0.9999485">
Table 8 compares our method with its modified ver-
sion without semi-supervised learning, suggesting
that semi-supervised learning considerably boosts
the performance. To get more details about self-
training, we evenly divide the test data into 10 parts
and feed them into our method sequentially; we
record the average F1 score on each part, as shown
in Figure 2.
</bodyText>
<subsectionHeader confidence="0.695793">
5.8 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999973571428571">
Errors made by our system on the test set fall into
three categories. The first kind of error, accounting
for 35.5% of all errors, is largely related to slang ex-
pressions and informal abbreviations. For example,
our method identifies “Cali”, which actually means
“California”, as a PERSON in the tweet “i love Cali
so much”. In future, we can design a normalization
</bodyText>
<page confidence="0.995568">
365
</page>
<table confidence="0.999803333333333">
System Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NERCB_KNN 82.6 74.8 78.5
</table>
<tableCaption confidence="0.9623595">
Table 6: Overall performance of our system with and
without the KNN classifier, respectively.
</tableCaption>
<table confidence="0.9999374">
Features Pre.(%) Rec.(%) F1(%)
Fo 71.3 42.8 53.5
Fo + Fl 76.2 44.2 55.9
Fo + Fg 80.5 66.2 72.7
Fo + Fl + Fg 82.6 74.8 78.5
</table>
<tableCaption confidence="0.9972495">
Table 7: Overview performance of the CRF labeler (com-
bined with KNN) with different feature sets.
</tableCaption>
<bodyText confidence="0.99931075">
component to handle such slang expressions and in-
formal abbreviations.
The second kind of error, accounting for 37.2%
of all errors, is mainly attributed to the data sparse-
ness. For example, for this tweet “come to see jaxon
someday”, our method mistakenly labels “jaxon”
as a LOCATION, which actually denotes a PER-
SON. This error is understandable somehow, since
this tweet is one of the earliest tweets that mention
“jaxon”, and at that time there was no strong evi-
dence supporting that it represents a person. Possi-
ble solutions to these errors include continually en-
riching the gazetteers and aggregating additional ex-
ternal knowledge from other channels such as tradi-
tional news.
The last kind of error, which represents 27.3%
of all errors, somehow links to the noise prone na-
ture of tweets. Consider this tweet “wesley snipes
ws cought 4 nt payin tax coz ths celebz dnt take it
cirus.”, in which “wesley snipes” is not identified
as a PERSON but simply ignored by our method,
because this tweet is too noisy to provide effective
features. Tweet normalization technology seems a
possible solution to alleviate this kind of error.
</bodyText>
<table confidence="0.999551666666667">
Features Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NER′CB 82.1 71.9 76.7
</table>
<tableCaption confidence="0.9317265">
Table 8: Performance of our system with and without
semi-supervised learning, respectively.
</tableCaption>
<figureCaption confidence="0.76932625">
Figure 2: F1 score on 10 test data sets sequentially fed
into the system, each with 600 instances. Horizontal and
vertical axes represent the sequential number of the test
data set and the averaged F1 score (%), respectively.
</figureCaption>
<sectionHeader confidence="0.998803" genericHeader="conclusions">
6 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.9999778125">
We propose a novel NER system for tweets, which
combines a KNN classifier with a CRF labeler under
a semi-supervised learning framework. The KNN
classifier collects global information across recently
labeled tweets while the CRF labeler exploits infor-
mation from a single tweet and from the gazetteers.
A serials of experiments show the effectiveness of
our method, and particularly, show the positive ef-
fects of KNN and semi-supervised learning.
In future, we plan to further improve the per-
formance of our method through two directions.
Firstly, we hope to develop tweet normalization
technology to make tweets friendlier to the NER
task. Secondly, we are interested in integrating
new entities from tweets or other channels into the
gazetteers.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999837166666667">
We thank Long Jiang, Changning Huang, Yunbo
Cao, Dongdong Zhang, Zaiqing Nie for helpful dis-
cussions, and the anonymous reviewers for their
valuable comments. We also thank Matt Callcut for
his careful proofreading of an early draft of this pa-
per.
</bodyText>
<sectionHeader confidence="0.989489" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.55510475">
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467–479.
</bodyText>
<page confidence="0.996584">
366
</page>
<reference confidence="0.997428195652174">
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP, pages
1002–1012.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1–8.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating Complex Named Entities in Web Text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91–134.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In CSLDAMT, pages 80–88.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In EMNLP, pages
141–150.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL, pages 363–370.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL, pages 281–289.
Martin Jansche and Steven P. Abney. 2002. Informa-
tion extraction from voicemail transcripts. In EMNLP,
pages 320–327.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In ACL, pages 264–
271.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423–430.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In ACL, pages
1121–1128.
George R. Krupka and Kevin Hausman. 1998. Isoquest:
Description of the netowlTM extractor system as used
in muc-7. In MUC-7.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282–289.
Andrew Mccallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In HLT-NAACL, pages 188–191.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In HLT-NAACL, pages 337–342.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In HLT,
pages 443–450.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Linguisti-
cae Investigationes, 30:3–26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147–155.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In HLT-NAACL, pages 73–81.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In ACL, pages 665–673.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL, pages 142–147.
Yefeng Wang. 2009. Annotating and recognising named
entities in clinical notes. In ACL-IJCNLP, pages 18–
26.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In EMNLP, pages 1523–1532.
Kazuhiro Yoshida and Jun’ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In BioNLP,
pages 209–216.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition system.
In HLT-NAACL, pages 204–207.
GuoDong Zhou and Jian Su. 2002. Named entity recog-
nition using an hmm-based chunk tagger. In ACL,
pages 473–480.
</reference>
<page confidence="0.998299">
367
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.292239">
<title confidence="0.999909">Recognizing Named Entities in Tweets</title>
<author confidence="0.999927">Liu Shaodian Furu Wei Ming Zhou</author>
<affiliation confidence="0.978699">of Computer Science and Harbin Institute of Technology, Harbin, 150001, of Computer Science and</affiliation>
<address confidence="0.667595333333333">Shanghai Jiao Tong University, Shanghai, 200240, Research Beijing, 100190,</address>
<email confidence="0.9904">fuwei,</email>
<abstract confidence="0.997328210526316">The challenges of Named Entities Recognition (NER) for tweets lie in the insufficient information in a tweet and the unavailability of training data. We propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to tackle these challenges. The KNN based classifier conducts pre-labeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet. The semi-supervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semisupervised learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Chiticariu</author>
<author>Rajasekar Krishnamurthy</author>
<author>Yunyao Li</author>
<author>Frederick Reiss</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Domain adaptation of rule-based annotators for named-entity recognition tasks.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>1002--1012</pages>
<contexts>
<context position="3084" citStr="Chiticariu et al. (2010)" startWordPosition="486" endWordPosition="489">tate-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary, which requires a lot of annotated tweets or rules. However, manually creating them is tedious and prohibitively unaffordable. Proposed solutions to alleviate this issue include: 1) Domain adaption, which aims to reuse the knowledge of the source domain in a target domain. Two recent examples are Wu et al. (2009), which uses data that is informative about the target domain and also easy to be labeled to bridge the two domains, and Chiticariu et al. (2010), which introduces a high-level rule language, called NERL, to build the general and domain specific NER systems; and 2) semi-supervised learning, which aims to use the abundant unlabeled data to compensate for the lack of annotated data. Suzuki and Isozaki (2008) is one such example. Another challenge is the limited information in tweet. Two factors contribute to this difficulty. One 359 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 359–367, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics is the tweet’s info</context>
<context position="9893" citStr="Chiticariu et al. (2010)" startWordPosition="1553" endWordPosition="1556">) introduces NER on clinical from unlabeled data. Miller et al. (2004) use word notes. A data set is manually annotated and a linear clusters (Brown et al., 1992) learned from unlaCRF model is trained, which achieves an F-score of beled text, resulting in a performance improvement 81.48% on their test data set; Downey et al. (2007) of NER. Guo et al. (2009) introduce Latent Semanemploy capitalization cues and n-gram statistics to tic Association (LSA) for NER. In our pilot study of locate names of a variety of classes in web text; NER for tweets, we adopt bag-of-words models to most recently, Chiticariu et al. (2010) design and im- represent a word in tweet, to concentrate our efforts plement a high-level language NERL that is tuned on combining global evidence with local informato simplify the process of building, understanding, tion and semi-supervised learning. We leave it to and customizing complex rule-based named-entity our future work to explore which is the best input annotators for different domains. representation for our task. Ratinov and Roth (2009) systematically study 3 Task Definition the challenges in NER, compare several solutions We first introduce some background about tweets, and repor</context>
</contexts>
<marker>Chiticariu, Krishnamurthy, Li, Reiss, Vaithyanathan, 2010</marker>
<rawString>Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan. 2010. Domain adaptation of rule-based annotators for named-entity recognition tasks. In EMNLP, pages 1002–1012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8113" citStr="Collins, 2002" startWordPosition="1269" endWordPosition="1270">tities of predefined types; Zhou and Ju (2002) adopt Hidden Markov Models (HMM) while Finkel et al. (2005) use CRF to train a sequential NE labeler, in which the BIO (meaning Beginning, the Inside and the Outside of 2https://www.mturk.com/mturk/ 3http://crowdflower.com/ 360 an entity, respectively) schema is applied. Other confidently labeled but is also informative to its methods, such as classification based on Maximum training set, which is used to re-train its model. Jiang Entropy models and sequential application of Per- and Zhai (2007) propose a balanced bootstrapping ceptron or Winnow (Collins, 2002), are also prac- algorithm and successfully apply it to NER. Their ticed. The state-of-the-art system, e.g., the Stanford method is based on instance re-weighting, which NER, can achieve an F1 score of over 92.0% on its allows the small amount of the bootstrapped traintest set. ing sets to have an equal weight to the large source Biomedical NER represents another line of active domain training set. Wu et al. (2009) propose anresearch. Machine learning based systems are com- other bootstrapping algorithm that selects bridging monly used and outperform the rule based systems. instances from an u</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Locating Complex Named Entities in Web Text.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="9602" citStr="Downey et al. (2007)" startWordPosition="1503" endWordPosition="1506">nd syntactic features, as well, but use human labeled tweets as seeds. such as part-of-speech (POS) and shallow parsing. Another representative of semi-supervised learnA handful of work on other domains exists. For ing is learning a robust representation of the input example, Wang (2009) introduces NER on clinical from unlabeled data. Miller et al. (2004) use word notes. A data set is manually annotated and a linear clusters (Brown et al., 1992) learned from unlaCRF model is trained, which achieves an F-score of beled text, resulting in a performance improvement 81.48% on their test data set; Downey et al. (2007) of NER. Guo et al. (2009) introduce Latent Semanemploy capitalization cues and n-gram statistics to tic Association (LSA) for NER. In our pilot study of locate names of a variety of classes in web text; NER for tweets, we adopt bag-of-words models to most recently, Chiticariu et al. (2010) design and im- represent a word in tweet, to concentrate our efforts plement a high-level language NERL that is tuned on combining global evidence with local informato simplify the process of building, understanding, tion and semi-supervised learning. We leave it to and customizing complex rule-based named-</context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>Doug Downey, Matthew Broadhead, and Oren Etzioni. 2007. Locating Complex Named Entities in Web Text. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artif. Intell.,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="2048" citStr="Etzioni et al., 2005" startWordPosition="311" endWordPosition="314">posed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new genre of text, which are short, informal, ungrammatical and noise prone. For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary, which requires a lot o</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artif. Intell., 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>Will Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in twitter data with crowdsourcing. In</title>
<date>2010</date>
<booktitle>CSLDAMT,</booktitle>
<pages>80--88</pages>
<contexts>
<context position="6860" citStr="Finin et al. (2010)" startWordPosition="1069" endWordPosition="1072">elines and that both the combination with KNN and the semi-supervised learning strategy are effective. The rest of our paper is organized as follows. In the next section, we introduce related work. In Section 3, we formally define the task and present the challenges. In Section 4, we detail our method. In Section 5, we evaluate our method. Finally, Section 6 concludes our work. 2 Related Work Related work can be roughly divided into three categories: NER on tweets, NER on non-tweets (e.g., news, bio-logical medicine, and clinical notes), and semi-supervised learning for NER. 2.1 NER on Tweets Finin et al. (2010) use Amazons Mechanical Turk service 2 and CrowdFlower 3 to annotate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. In contrast, our work aims to build a system that can automatically identify named entities in tweets. To achieve this, a KNN classifier with a CRF model is combined to leverage cross tweets information, and the semisupervised learning is adopted to leverage unlabeled tweets. 2.2 NER on Non-Tweets NER has been extensively studied on formal text, such as news, and various approaches have been proposed. For example, Krupka and Hausma</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in twitter data with crowdsourcing. In CSLDAMT, pages 80–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Nested named entity recognition.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="1579" citStr="Finkel and Manning, 2009" startWordPosition="231" endWordPosition="234">The semi-supervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semisupervised learning. 1 Introduction Named Entities Recognition (NER) is generally understood as the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Proposed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, c</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Nested named entity recognition. In EMNLP, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="2390" citStr="Finkel et al., 2005" startWordPosition="368" endWordPosition="371">hile the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new genre of text, which are short, informal, ungrammatical and noise prone. For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary, which requires a lot of annotated tweets or rules. However, manually creating them is tedious and prohibitively unaffordable. Proposed solutions to alleviate this issue include: 1) Domain adaption, which aims to reuse the knowledge of the source domain in a target domain. Two recent examples are Wu et al. (2009), which uses data that is informative about the tar</context>
<context position="7605" citStr="Finkel et al. (2005)" startWordPosition="1193" endWordPosition="1196">te the effectiveness of human labeling. In contrast, our work aims to build a system that can automatically identify named entities in tweets. To achieve this, a KNN classifier with a CRF model is combined to leverage cross tweets information, and the semisupervised learning is adopted to leverage unlabeled tweets. 2.2 NER on Non-Tweets NER has been extensively studied on formal text, such as news, and various approaches have been proposed. For example, Krupka and Hausman (1998) use manual rules to extract entities of predefined types; Zhou and Ju (2002) adopt Hidden Markov Models (HMM) while Finkel et al. (2005) use CRF to train a sequential NE labeler, in which the BIO (meaning Beginning, the Inside and the Outside of 2https://www.mturk.com/mturk/ 3http://crowdflower.com/ 360 an entity, respectively) schema is applied. Other confidently labeled but is also informative to its methods, such as classification based on Maximum training set, which is used to re-train its model. Jiang Entropy models and sequential application of Per- and Zhai (2007) propose a balanced bootstrapping ceptron or Winnow (Collins, 2002), are also prac- algorithm and successfully apply it to NER. Their ticed. The state-of-the-a</context>
<context position="18130" citStr="Finkel et al., 2005" startWordPosition="2961" endWordPosition="2964">icted class c*: c* = ∑ argmaxc (⃗w′ ,c′ )Enb S(c, c′) · cos(⃗w, ⃗w′). 3: Calculate the labeling confidence cf: cf = ∑( ⃗w′ ,c′ )∈nb δ(c,c′ )·cos(⃗w,⃗w′) ∑( ⃗w′ ,c′ )∈nb cos(⃗w,⃗w′ ) . 4: return The predicted label c* and its confidence cf. model, which is good at encoding the subtle interactions between words and their labels, compensates for KNN’s incapability to capture fine-grained evidence involving multiple decision points. The Linear CRF model is used as the fine model, with the following considerations: 1) It is wellstudied and has been successfully used in state-ofthe-art NER systems (Finkel et al., 2005; Wang, 2009); 2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. In our experiments, the CRF++ 5 toolkit is used to train a linear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. 4.3 Features Given a word in a tweet, the KNN classifier considers a text window of size 5 with the word in the middle (Zhang and Johnson, 2003), and extracts bag-ofword features from the window as features. For each wor</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Guo</author>
<author>Huijia Zhu</author>
<author>Zhili Guo</author>
<author>Xiaoxun Zhang</author>
<author>Xian Wu</author>
<author>Zhong Su</author>
</authors>
<title>Domain adaptation with latent semantic association for named entity recognition.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>281--289</pages>
<contexts>
<context position="9628" citStr="Guo et al. (2009)" startWordPosition="1509" endWordPosition="1512">l, but use human labeled tweets as seeds. such as part-of-speech (POS) and shallow parsing. Another representative of semi-supervised learnA handful of work on other domains exists. For ing is learning a robust representation of the input example, Wang (2009) introduces NER on clinical from unlabeled data. Miller et al. (2004) use word notes. A data set is manually annotated and a linear clusters (Brown et al., 1992) learned from unlaCRF model is trained, which achieves an F-score of beled text, resulting in a performance improvement 81.48% on their test data set; Downey et al. (2007) of NER. Guo et al. (2009) introduce Latent Semanemploy capitalization cues and n-gram statistics to tic Association (LSA) for NER. In our pilot study of locate names of a variety of classes in web text; NER for tweets, we adopt bag-of-words models to most recently, Chiticariu et al. (2010) design and im- represent a word in tweet, to concentrate our efforts plement a high-level language NERL that is tuned on combining global evidence with local informato simplify the process of building, understanding, tion and semi-supervised learning. We leave it to and customizing complex rule-based named-entity our future work to </context>
<context position="20301" citStr="Guo et al., 2009" startWordPosition="3316" endWordPosition="3319"> additional features, gazetteers and alternative models. Additional Features. Features related to chunking and parsing are not adopted in our final system, because they give only a slight performance improvement while a lot of computing resources are required to extract such features. The ineffectiveness of these features is linked to the noisy and informal nature of tweets. Word class (Brown et al., 1992) features are not used either, which prove to be unhelpful for our system. We are interested in exploring other tweet representations, which may fit our NER task, for example the LSA models (Guo et al., 2009). Gazetteers. In our work, gazetteers prove to be substantially useful, which is consistent with the observation of Ratinov and Roth (2009). However, the gazetteers used in our work contain noise, which hurts the performance. Moreover, they are static, directly from Ratinov and Roth (2009), thus with a relatively lower coverage, especially for person names and product names in tweets. We are developing tools to clean the gazetteers. In future, we plan to feed the fresh entities correctly identified from tweets back into the gazetteers. The correctness of an entity can rely on its frequency or </context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Wu, Su, 2009</marker>
<rawString>Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang, Xian Wu, and Zhong Su. 2009. Domain adaptation with latent semantic association for named entity recognition. In NAACL, pages 281–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jansche</author>
<author>Steven P Abney</author>
</authors>
<title>Information extraction from voicemail transcripts.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="1650" citStr="Jansche and Abney, 2002" startWordPosition="244" endWordPosition="247">training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semisupervised learning. 1 Introduction Named Entities Recognition (NER) is generally understood as the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Proposed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new ge</context>
</contexts>
<marker>Jansche, Abney, 2002</marker>
<rawString>Martin Jansche and Steven P. Abney. 2002. Information extraction from voicemail transcripts. In EMNLP, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>264--271</pages>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In ACL, pages 264– 271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="23402" citStr="Klein and Manning, 2003" startWordPosition="3832" endWordPosition="3835">c mean of precision and recall. For the overall performance, we use the average Precision, Recall and F1, where the weight of each name entity type is proportional to the number of entities of that type. These metrics are widely used by existing NER systems to evaluate their performance. 5.3 Baselines Two systems are used as baselines: One is the dictionary look-up system based on the gazetteers; the other is the modified version of our system without KNN and semi-supervised learning. Hereafter these two baselines are called NERDIC and NERBA, respectively. The OpenNLP and the Stanford parser (Klein and Manning, 2003) are used to extract linguistic features for the baselines and our method. 7It is developed by the Bing social search team, and currently is only internally available. 364 System Pre.(%) Rec.(%) F1(%) NERCB 81.6 78.8 80.2 NERBA 83.6 68.6 75.4 NERDIC 32.6 25.4 28.6 Table 1: Overall experimental results. System Pre.(%) Rec.(%) F1(%) NERCB 78.4 74.5 76.4 NERBA 83.6 68.4 75.2 NERDIC 37.1 29.7 33.0 Table 2: Experimental results on PERSON. System Pre.(%) Rec.(%) F1(%) NERCB 80.3 77.5 78.9 NERBA 81.6 69.7 75.2 NERDIC 30.2 30.0 30.1 Table 4: Experimental results on LOCATION. System Pre.(%) Rec.(%) F1(</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay Krishnan</author>
<author>Christopher D Manning</author>
</authors>
<title>An effective two-stage model for exploiting non-local dependencies in named entity recognition.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>1121--1128</pages>
<contexts>
<context position="4585" citStr="Krishnan and Manning, 2006" startWordPosition="711" endWordPosition="714">. The other is the tweet’s short nature, leading to the excessive abbreviations or shorthand in tweets, and the availability of very limited context information. Tackling this challenge, ideally, requires adapting related NLP tools to fit tweets, or normalizing tweets to accommodate existing tools, both of which are hard tasks. We propose a novel NER system to address these challenges. Firstly, a K-Nearest Neighbors (KNN) based classifier is adopted to conduct word level classification, leveraging the similar and recently labeled tweets. Following the two-stage prediction aggregation methods (Krishnan and Manning, 2006), such pre-labeled results, together with other conventional features used by the state-of-the-art NER systems, are fed into a linear Conditional Random Fields (CRF) (Lafferty et al., 2001) model, which conducts fine-grained tweet level NER. Furthermore, the KNN and CRF model are repeatedly retrained with an incrementally augmented training set, into which high confidently labeled tweets are added. Indeed, it is the combination of KNN and CRF under a semi-supervised learning framework that differentiates ours from the existing. Finally, following Lev Ratinov and Dan Roth (2009), 30 gazetteers </context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Vijay Krishnan and Christopher D. Manning. 2006. An effective two-stage model for exploiting non-local dependencies in named entity recognition. In ACL, pages 1121–1128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George R Krupka</author>
<author>Kevin Hausman</author>
</authors>
<title>Isoquest: Description of the netowlTM extractor system as used in muc-7.</title>
<date>1998</date>
<booktitle>In MUC-7.</booktitle>
<contexts>
<context position="1522" citStr="Krupka and Hausman, 1998" startWordPosition="221" endWordPosition="224">g to capture fine-grained information encoded in a tweet. The semi-supervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semisupervised learning. 1 Introduction Named Entities Recognition (NER) is generally understood as the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Proposed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clini</context>
<context position="7468" citStr="Krupka and Hausman (1998)" startWordPosition="1170" endWordPosition="1173">nin et al. (2010) use Amazons Mechanical Turk service 2 and CrowdFlower 3 to annotate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. In contrast, our work aims to build a system that can automatically identify named entities in tweets. To achieve this, a KNN classifier with a CRF model is combined to leverage cross tweets information, and the semisupervised learning is adopted to leverage unlabeled tweets. 2.2 NER on Non-Tweets NER has been extensively studied on formal text, such as news, and various approaches have been proposed. For example, Krupka and Hausman (1998) use manual rules to extract entities of predefined types; Zhou and Ju (2002) adopt Hidden Markov Models (HMM) while Finkel et al. (2005) use CRF to train a sequential NE labeler, in which the BIO (meaning Beginning, the Inside and the Outside of 2https://www.mturk.com/mturk/ 3http://crowdflower.com/ 360 an entity, respectively) schema is applied. Other confidently labeled but is also informative to its methods, such as classification based on Maximum training set, which is used to re-train its model. Jiang Entropy models and sequential application of Per- and Zhai (2007) propose a balanced bo</context>
</contexts>
<marker>Krupka, Hausman, 1998</marker>
<rawString>George R. Krupka and Kevin Hausman. 1998. Isoquest: Description of the netowlTM extractor system as used in muc-7. In MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4774" citStr="Lafferty et al., 2001" startWordPosition="739" endWordPosition="742">, requires adapting related NLP tools to fit tweets, or normalizing tweets to accommodate existing tools, both of which are hard tasks. We propose a novel NER system to address these challenges. Firstly, a K-Nearest Neighbors (KNN) based classifier is adopted to conduct word level classification, leveraging the similar and recently labeled tweets. Following the two-stage prediction aggregation methods (Krishnan and Manning, 2006), such pre-labeled results, together with other conventional features used by the state-of-the-art NER systems, are fed into a linear Conditional Random Fields (CRF) (Lafferty et al., 2001) model, which conducts fine-grained tweet level NER. Furthermore, the KNN and CRF model are repeatedly retrained with an incrementally augmented training set, into which high confidently labeled tweets are added. Indeed, it is the combination of KNN and CRF under a semi-supervised learning framework that differentiates ours from the existing. Finally, following Lev Ratinov and Dan Roth (2009), 30 gazetteers are used, which cover common names, countries, locations, temporal expressions, etc. These gazetteers represent general knowledge across domains. The underlying idea of our method is to com</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Mccallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>188--191</pages>
<contexts>
<context position="2025" citStr="Mccallum and Li, 2003" startWordPosition="307" endWordPosition="310"> and Sekine, 2007). Proposed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new genre of text, which are short, informal, ungrammatical and noise prone. For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary,</context>
</contexts>
<marker>Mccallum, Li, 2003</marker>
<rawString>Andrew Mccallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In HLT-NAACL, pages 188–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>337--342</pages>
<contexts>
<context position="9339" citStr="Miller et al. (2004)" startWordPosition="1457" endWordPosition="1460">ed target domain, which A state-of-the-art biomedical NER system (Yoshida are informative about the target domain and are also and Tsujii, 2007) uses lexical features, orthographic easy to be correctly labeled. We adopt bootstrapping features, semantic features and syntactic features, as well, but use human labeled tweets as seeds. such as part-of-speech (POS) and shallow parsing. Another representative of semi-supervised learnA handful of work on other domains exists. For ing is learning a robust representation of the input example, Wang (2009) introduces NER on clinical from unlabeled data. Miller et al. (2004) use word notes. A data set is manually annotated and a linear clusters (Brown et al., 1992) learned from unlaCRF model is trained, which achieves an F-score of beled text, resulting in a performance improvement 81.48% on their test data set; Downey et al. (2007) of NER. Guo et al. (2009) introduce Latent Semanemploy capitalization cues and n-gram statistics to tic Association (LSA) for NER. In our pilot study of locate names of a variety of classes in web text; NER for tweets, we adopt bag-of-words models to most recently, Chiticariu et al. (2010) design and im- represent a word in tweet, to </context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In HLT-NAACL, pages 337–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Extracting personal names from email: applying named entity recognition to informal text.</title>
<date>2005</date>
<booktitle>In HLT,</booktitle>
<pages>443--450</pages>
<contexts>
<context position="1738" citStr="Minkov et al., 2005" startWordPosition="258" endWordPosition="261"> well as the effectiveness of KNN and semisupervised learning. 1 Introduction Named Entities Recognition (NER) is generally understood as the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Proposed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new genre of text, which are short, informal, ungrammatical and noise prone. For example, the </context>
</contexts>
<marker>Minkov, Wang, Cohen, 2005</marker>
<rawString>Einat Minkov, Richard C. Wang, and William W. Cohen. 2005. Extracting personal names from email: applying named entity recognition to informal text. In HLT, pages 443–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<booktitle>Linguisticae Investigationes,</booktitle>
<pages>30--3</pages>
<contexts>
<context position="1422" citStr="Nadeau and Sekine, 2007" startWordPosition="206" endWordPosition="209">ing to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet. The semi-supervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semisupervised learning. 1 Introduction Named Entities Recognition (NER) is generally understood as the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Proposed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30:3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In CoNLL,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="2543" citStr="Ratinov and Roth, 2009" startWordPosition="393" endWordPosition="396"> methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new genre of text, which are short, informal, ungrammatical and noise prone. For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary, which requires a lot of annotated tweets or rules. However, manually creating them is tedious and prohibitively unaffordable. Proposed solutions to alleviate this issue include: 1) Domain adaption, which aims to reuse the knowledge of the source domain in a target domain. Two recent examples are Wu et al. (2009), which uses data that is informative about the target domain and also easy to be labeled to bridge the two domains, and Chiticariu et al. (2010), which introduces a high-level rule language, called NERL,</context>
<context position="10346" citStr="Ratinov and Roth (2009)" startWordPosition="1622" endWordPosition="1625">) for NER. In our pilot study of locate names of a variety of classes in web text; NER for tweets, we adopt bag-of-words models to most recently, Chiticariu et al. (2010) design and im- represent a word in tweet, to concentrate our efforts plement a high-level language NERL that is tuned on combining global evidence with local informato simplify the process of building, understanding, tion and semi-supervised learning. We leave it to and customizing complex rule-based named-entity our future work to explore which is the best input annotators for different domains. representation for our task. Ratinov and Roth (2009) systematically study 3 Task Definition the challenges in NER, compare several solutions We first introduce some background about tweets, and report some interesting findings. For exam- then give a formal definition of the task. ple, they show that a conditional model that does 3.1 The Tweets not consider interactions at the output level per- A tweet is a short text message containing no forms comparably to beam search or Viterbi, and more than 140 characters in Twitter, the biggest that the BILOU (Beginning, the Inside and the Last micro-blog service. Here is an example of tokens of multi-tok</context>
<context position="12499" citStr="Ratinov and Roth (2009)" startWordPosition="1960" endWordPosition="1963">abeled data “http://bit.ly/bCsLOr” is a shortened link. is abundant and easy to access. Twitter users are interested in named entities, such Bootstrapping is a typical semi-supervised learning method. It iteratively adds data that has been 361 4.1 Method Overview NER task can be naturally divided into two subtasks, i.e., boundary detection and type classification. Following the common practice , we adopt a sequential labeling approach to jointly resolve these sub-tasks, i.e., for each word in the input tweet, a label is assigned to it, indicating both the boundary and entity type. Inspired by Ratinov and Roth (2009), we use the BILOU schema. Algorithm 1 outlines our method, where: trains and traink denote two machine learning processes to get the CRF labeler and the KNN classifier, respectively; repr,,, converts a word in a tweet into a bag-of-words vector; the reprt function transforms a tweet into a feature matrix that is later fed into the CRF model; the knn function predicts the class of a word; the update function applies the predicted class by KNN to the inputted tweet; the crf function conducts word level NE labeling;τ and γ represent the minimum labeling confidence of KNN and CRF, respectively, w</context>
<context position="18815" citStr="Ratinov and Roth (2009)" startWordPosition="3083" endWordPosition="3086">uence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. In our experiments, the CRF++ 5 toolkit is used to train a linear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. 4.3 Features Given a word in a tweet, the KNN classifier considers a text window of size 5 with the word in the middle (Zhang and Johnson, 2003), and extracts bag-ofword features from the window as features. For each word, our CRF model extracts similar features as Wang (2009) and Ratinov and Roth (2009), namely, orthographic features, lexical features and gazetteers related features. In our work, we use the gazetteers provided by Ratinov and Roth (2009). Two points are worth noting here. One is that before feature extraction for either the KNN or the CRF, stop words are removed. The stop words used here are mainly from a set of frequently-used words 6. The other is that tweet meta data is normalized, that is, every link becomes *LINK* and every 5http://crfpp.sourceforge.net/ 6http://www.textfixer.com/resources/common-englishwords.txt 363 account name becomes *ACCOUNT*. Hash tags are treated </context>
<context position="20440" citStr="Ratinov and Roth (2009)" startWordPosition="3339" endWordPosition="3342">d in our final system, because they give only a slight performance improvement while a lot of computing resources are required to extract such features. The ineffectiveness of these features is linked to the noisy and informal nature of tweets. Word class (Brown et al., 1992) features are not used either, which prove to be unhelpful for our system. We are interested in exploring other tweet representations, which may fit our NER task, for example the LSA models (Guo et al., 2009). Gazetteers. In our work, gazetteers prove to be substantially useful, which is consistent with the observation of Ratinov and Roth (2009). However, the gazetteers used in our work contain noise, which hurts the performance. Moreover, they are static, directly from Ratinov and Roth (2009), thus with a relatively lower coverage, especially for person names and product names in tweets. We are developing tools to clean the gazetteers. In future, we plan to feed the fresh entities correctly identified from tweets back into the gazetteers. The correctness of an entity can rely on its frequency or other evidence. Alternative Models. We have replaced KNN by other classifiers, such as those based on Maximum Entropy and Support Vector Ma</context>
<context position="25530" citStr="Ratinov and Roth (2009)" startWordPosition="4193" endWordPosition="4196">d that its F1 is as high as 80.2% while the baseline system based on the CRF model achieves only an F1 of 75.4%. This largely explains why the KNN classifier helps the CRF labeler. The KNN classifier is replaced with its competitors, and only a slight difference in performance is observed. We do observe that retraining KNN is obviously faster. System Pre.(%) Rec.(%) F1(%) NERCB 81.3 65.4 72.5 NERBA 82.5 58.4 68.4 NERDIC 8.2 6.1 7.0 Table 3: Experimental results on PRODUCT. 5.6 Effects of the CRF Labeler Similarly, the CRF model is replaced by its alternatives. As is opposite to the finding of Ratinov and Roth (2009), the CRF model gives remarkably better results, i.e., 2.1% higher in F1 than its best followers (with p &lt; 0.001). Table 7 shows the overall performance of the CRF labeler with various feature set combinations, where Fo, Fl and Fg denote the orthographic features, the lexical features and the gazetteers related features, respectively. It can be seen from Table 7 that the lexical and gazetteer related features are helpful. Other advanced features such as chunking are also explored but with no significant improvement. 5.7 Effects of Semi-supervised Learning Table 8 compares our method with its m</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Dustin Hillard</author>
<author>Chris Leggetter</author>
</authors>
<title>Minimally-supervised extraction of entities from text advertisements.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>73--81</pages>
<contexts>
<context position="1600" citStr="Singh et al., 2010" startWordPosition="235" endWordPosition="238">ng plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semisupervised learning. 1 Introduction Named Entities Recognition (NER) is generally understood as the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Proposed solutions to NER fall into three categories: 1) The rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, current systems traine</context>
</contexts>
<marker>Singh, Hillard, Leggetter, 2010</marker>
<rawString>Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010. Minimally-supervised extraction of entities from text advertisements. In HLT-NAACL, pages 73–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>665--673</pages>
<contexts>
<context position="3348" citStr="Suzuki and Isozaki (2008)" startWordPosition="529" endWordPosition="532"> prohibitively unaffordable. Proposed solutions to alleviate this issue include: 1) Domain adaption, which aims to reuse the knowledge of the source domain in a target domain. Two recent examples are Wu et al. (2009), which uses data that is informative about the target domain and also easy to be labeled to bridge the two domains, and Chiticariu et al. (2010), which introduces a high-level rule language, called NERL, to build the general and domain specific NER systems; and 2) semi-supervised learning, which aims to use the abundant unlabeled data to compensate for the lack of annotated data. Suzuki and Isozaki (2008) is one such example. Another challenge is the limited information in tweet. Two factors contribute to this difficulty. One 359 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 359–367, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics is the tweet’s informal nature, making conventional features such as part-of-speech (POS) and capitalization not reliable. The performance of current NLP tools drops sharply on tweets. For example, OpenNLP 1, the state-of-the-art POS tagger, gets only an accuracy of 74.0% on our tes</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In ACL, pages 665–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>142--147</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: languageindependent named entity recognition. In HLTNAACL, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yefeng Wang</author>
</authors>
<title>Annotating and recognising named entities in clinical notes.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP,</booktitle>
<pages>18--26</pages>
<contexts>
<context position="2144" citStr="Wang, 2009" startWordPosition="329" endWordPosition="330">chine learning based (Finkel and Manning, 2009; Singh et al., 2010) ; and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and ∗ This work has been done while the author was visiting Microsoft Research Asia. CoNLL03 (Tjong Kim Sang and De Meulder, 2003), the data driven methods now become the dominating methods. However, current NER mainly focuses on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). Exceptions include studies on informal text such as emails, blogs, clinical notes (Wang, 2009). Because of the domain mismatch, current systems trained on non-tweets perform poorly on tweets, a new genre of text, which are short, informal, ungrammatical and noise prone. For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary, which requires a lot of annotated tweets or rules. However, manually creating them is tedious and prohibitively unaffo</context>
<context position="9270" citStr="Wang (2009)" startWordPosition="1448" endWordPosition="1449">outperform the rule based systems. instances from an unlabeled target domain, which A state-of-the-art biomedical NER system (Yoshida are informative about the target domain and are also and Tsujii, 2007) uses lexical features, orthographic easy to be correctly labeled. We adopt bootstrapping features, semantic features and syntactic features, as well, but use human labeled tweets as seeds. such as part-of-speech (POS) and shallow parsing. Another representative of semi-supervised learnA handful of work on other domains exists. For ing is learning a robust representation of the input example, Wang (2009) introduces NER on clinical from unlabeled data. Miller et al. (2004) use word notes. A data set is manually annotated and a linear clusters (Brown et al., 1992) learned from unlaCRF model is trained, which achieves an F-score of beled text, resulting in a performance improvement 81.48% on their test data set; Downey et al. (2007) of NER. Guo et al. (2009) introduce Latent Semanemploy capitalization cues and n-gram statistics to tic Association (LSA) for NER. In our pilot study of locate names of a variety of classes in web text; NER for tweets, we adopt bag-of-words models to most recently, C</context>
<context position="18143" citStr="Wang, 2009" startWordPosition="2965" endWordPosition="2966">∑ argmaxc (⃗w′ ,c′ )Enb S(c, c′) · cos(⃗w, ⃗w′). 3: Calculate the labeling confidence cf: cf = ∑( ⃗w′ ,c′ )∈nb δ(c,c′ )·cos(⃗w,⃗w′) ∑( ⃗w′ ,c′ )∈nb cos(⃗w,⃗w′ ) . 4: return The predicted label c* and its confidence cf. model, which is good at encoding the subtle interactions between words and their labels, compensates for KNN’s incapability to capture fine-grained evidence involving multiple decision points. The Linear CRF model is used as the fine model, with the following considerations: 1) It is wellstudied and has been successfully used in state-ofthe-art NER systems (Finkel et al., 2005; Wang, 2009); 2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. In our experiments, the CRF++ 5 toolkit is used to train a linear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. 4.3 Features Given a word in a tweet, the KNN classifier considers a text window of size 5 with the word in the middle (Zhang and Johnson, 2003), and extracts bag-ofword features from the window as features. For each word, our CRF mo</context>
</contexts>
<marker>Wang, 2009</marker>
<rawString>Yefeng Wang. 2009. Annotating and recognising named entities in clinical notes. In ACL-IJCNLP, pages 18– 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Wu</author>
<author>Wee Sun Lee</author>
<author>Nan Ye</author>
<author>Hai Leong Chieu</author>
</authors>
<title>Domain adaptive bootstrapping for named entity recognition.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>1523--1532</pages>
<contexts>
<context position="2939" citStr="Wu et al. (2009)" startWordPosition="458" endWordPosition="461">r example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-art performance on that task, drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets. Thus, building a domain specific NER for tweets is necessary, which requires a lot of annotated tweets or rules. However, manually creating them is tedious and prohibitively unaffordable. Proposed solutions to alleviate this issue include: 1) Domain adaption, which aims to reuse the knowledge of the source domain in a target domain. Two recent examples are Wu et al. (2009), which uses data that is informative about the target domain and also easy to be labeled to bridge the two domains, and Chiticariu et al. (2010), which introduces a high-level rule language, called NERL, to build the general and domain specific NER systems; and 2) semi-supervised learning, which aims to use the abundant unlabeled data to compensate for the lack of annotated data. Suzuki and Isozaki (2008) is one such example. Another challenge is the limited information in tweet. Two factors contribute to this difficulty. One 359 Proceedings of the 49th Annual Meeting of the Association for C</context>
<context position="8531" citStr="Wu et al. (2009)" startWordPosition="1338" endWordPosition="1341"> Maximum training set, which is used to re-train its model. Jiang Entropy models and sequential application of Per- and Zhai (2007) propose a balanced bootstrapping ceptron or Winnow (Collins, 2002), are also prac- algorithm and successfully apply it to NER. Their ticed. The state-of-the-art system, e.g., the Stanford method is based on instance re-weighting, which NER, can achieve an F1 score of over 92.0% on its allows the small amount of the bootstrapped traintest set. ing sets to have an equal weight to the large source Biomedical NER represents another line of active domain training set. Wu et al. (2009) propose anresearch. Machine learning based systems are com- other bootstrapping algorithm that selects bridging monly used and outperform the rule based systems. instances from an unlabeled target domain, which A state-of-the-art biomedical NER system (Yoshida are informative about the target domain and are also and Tsujii, 2007) uses lexical features, orthographic easy to be correctly labeled. We adopt bootstrapping features, semantic features and syntactic features, as well, but use human labeled tweets as seeds. such as part-of-speech (POS) and shallow parsing. Another representative of se</context>
</contexts>
<marker>Wu, Lee, Ye, Chieu, 2009</marker>
<rawString>Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu. 2009. Domain adaptive bootstrapping for named entity recognition. In EMNLP, pages 1523–1532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhiro Yoshida</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Reranking for biomedical named-entity recognition. In BioNLP,</title>
<date>2007</date>
<pages>209--216</pages>
<marker>Yoshida, Tsujii, 2007</marker>
<rawString>Kazuhiro Yoshida and Jun’ichi Tsujii. 2007. Reranking for biomedical named-entity recognition. In BioNLP, pages 209–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>David Johnson</author>
</authors>
<title>A robust risk minimization based named entity recognition system.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>204--207</pages>
<contexts>
<context position="18654" citStr="Zhang and Johnson, 2003" startWordPosition="3056" endWordPosition="3059">It is wellstudied and has been successfully used in state-ofthe-art NER systems (Finkel et al., 2005; Wang, 2009); 2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. In our experiments, the CRF++ 5 toolkit is used to train a linear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. 4.3 Features Given a word in a tweet, the KNN classifier considers a text window of size 5 with the word in the middle (Zhang and Johnson, 2003), and extracts bag-ofword features from the window as features. For each word, our CRF model extracts similar features as Wang (2009) and Ratinov and Roth (2009), namely, orthographic features, lexical features and gazetteers related features. In our work, we use the gazetteers provided by Ratinov and Roth (2009). Two points are worth noting here. One is that before feature extraction for either the KNN or the CRF, stop words are removed. The stop words used here are mainly from a set of frequently-used words 6. The other is that tweet meta data is normalized, that is, every link becomes *LINK</context>
</contexts>
<marker>Zhang, Johnson, 2003</marker>
<rawString>Tong Zhang and David Johnson. 2003. A robust risk minimization based named entity recognition system. In HLT-NAACL, pages 204–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Jian Su</author>
</authors>
<title>Named entity recognition using an hmm-based chunk tagger.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>473--480</pages>
<marker>Zhou, Su, 2002</marker>
<rawString>GuoDong Zhou and Jian Su. 2002. Named entity recognition using an hmm-based chunk tagger. In ACL, pages 473–480.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>