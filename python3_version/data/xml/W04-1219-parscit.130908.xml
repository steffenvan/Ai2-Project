<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005783">
<title confidence="0.756799">
Exploring Deep Knowledge Resources in Biomedical Name Recognition
</title>
<author confidence="0.979481">
ZHOU GuoDong SU Jian
</author>
<affiliation confidence="0.98305">
Institute for Infocomm Research
</affiliation>
<address confidence="0.976717">
21 Heng Mui Keng Terrace
Singapore 119613
</address>
<email confidence="0.993071">
Email: {zhougd, sujian}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.997342" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999591">
In this paper, we present a named entity
recognition system in the biomedical domain. In
order to deal with the special phenomena in the
biomedical domain, various evidential features are
proposed and integrated through a Hidden
Markov Model (HMM). In addition, a Support
Vector Machine (SVM) plus sigmoid is proposed
to resolve the data sparseness problem in our
system. Besides the widely used lexical-level
features, such as word formation pattern,
morphological pattern, out-domain POS and
semantic trigger, we also explore the name alias
phenomenon, the cascaded entity name
phenomenon, the use of both a closed dictionary
from the training corpus and an open dictionary
from the database term list SwissProt and the alias
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus.
</bodyText>
<sectionHeader confidence="0.980229" genericHeader="keywords">
1. The Baseline System
</sectionHeader>
<subsectionHeader confidence="0.994734">
1.1 Hidden Markov Model
</subsectionHeader>
<bodyText confidence="0.999906666666667">
In this paper, we use the Hidden Markov Model
(HMM) as described in Zhou et al (2002). Given
an output sequence O n = o1o2 ...o , the system finds
</bodyText>
<sectionHeader confidence="0.508085" genericHeader="introduction">
1 n
</sectionHeader>
<bodyText confidence="0.691296">
the most likely state sequence S n = s1s2 ... s that
</bodyText>
<equation confidence="0.982403714285714">
1 n
maximizes P Sn O n
( 1  |1 ) as follows:
n
(1)
)
1
</equation>
<bodyText confidence="0.790967">
From Equation (1), we can see that:
</bodyText>
<listItem confidence="0.992882375">
• The first term can be computed by applying
chain rules. In ngram modeling (Chen et al 1996),
each tag is assumed to be dependent on the N-1
previous tags.
• The second term is the summation of log
probabilities of all the individual tags.
• The third term corresponds to the “lexical”
component (dictionary) of the tagger.
</listItem>
<bodyText confidence="0.999853923076923">
The idea behind the model is that it tries to
assign each output an appropriate tag (state), which
contains boundary and class information. For
example, “TCF 1 binds stronger than NF kB to
TCEd DNA”. The tag assigned to token “TCF”
should indicate that it is at the beginning of an
entity name and it belongs to the “Protein” class;
and the tag assigned to token “binds” should
indicate that it does not belong to an entity name.
Here, the Viterbi algorithm (Viterbi 1967) is
implemented to find the most likely tag sequence.
The problem with the above HMM lies in the
data sparseness problem raised by P (  |1 )
</bodyText>
<equation confidence="0.9912565">
n
si O in the
</equation>
<bodyText confidence="0.965575666666667">
third term of Equation (1). In this paper, a Support
Vector Machine (SVM) plus sigmoid is proposed
to resolve this problem in our system.
</bodyText>
<subsectionHeader confidence="0.998472">
1.2 Support Vector Machine plus Sigmoid
</subsectionHeader>
<bodyText confidence="0.999854166666667">
Support Vector Machines (SVMs) are a popular
machine learning approach first presented by
Vapnik (1995). Based on the structural risk
minimization of statistical learning theory, SVMs
seek an optimal separating hyper-plane to divide
the training examples into two classes and make
decisions based on support vectors which are
selected as the only effective examples in the
training set. However, SVMs produce an un-
calibrated value that is not probability. That is, the
unthresholded output of an SVM can be
represented as
</bodyText>
<equation confidence="0.98840375">
f (x) ai yi k(xi , x) b
= ∑ - - + (2)
i SV
∈
</equation>
<bodyText confidence="0.99992275">
To map the SVM output into the probability, we
train an additional sigmoid model(Platt 1999):
Basically, SVMs are binary classifiers.
Therefore, we must extend SVMs to multi-class
(e.g. K) classifiers. For efficiency, we apply the
one vs. others strategy, which builds K classifiers
so as to separate one class from all others, instead
of the pairwise strategy, which builds K*(K-1)/2
classifiers considering all pairs of classes.
Moreover, we only apply the simple linear kernel,
although other kernels (e.g. polynomial kernel) and
pairwise strategy can have better performance.
</bodyText>
<equation confidence="0.990480380952381">
(y=1  |f) =
1 exp(
+ Af B
+
p
1
) (3)
O;) = log P(S;) − ∑ log P(si )
1
n i =
log
( |
P S
n
1
logP(si |O1
n
+
∑
i
=
</equation>
<page confidence="0.964304">
96
</page>
<subsectionHeader confidence="0.719668">
1.3 Features
</subsectionHeader>
<bodyText confidence="0.9991805">
Various widely used lexical-level features are
explored in the baseline system.
</bodyText>
<listItem confidence="0.948890038461539">
• Word Formation Pattern (FWFP): The purpose
of this feature is to capture capitalization,
digitalization and other word formation
information. In this paper, the same feature as in
Shen et al 2003 is used.
• Morphological Pattern (FMP): Morphological
information, such as prefix and suffix, is
considered as an important cue for terminology
identification. Same as Shen et al 2003, we use a
statistical method to get the most useful
prefixes/suffixes from the training data.
• Part-of-Speech (FPOS): Since many of the
words in biomedical entity names are in lowercase,
capitalization information in the biomedical
domain is not as evidential as that in the newswire
domain. Moreover, many biomedical entity names
are descriptive and very long. Therefore, POS may
provide useful evidence about the boundaries of
biomedical entity names. In the baseline system, an
out-domain POS using the PENN TreeBank is
applied.
• Head Noun Trigger (FHEAD): The head noun,
which is the major noun of a noun phrase, often
describes the function or the property of the noun
phrase. In this paper, we automatically extract
unigram and bigram head nouns from the training
</listItem>
<bodyText confidence="0.93644">
data, and rank them by frequency. For each entity
class, we select 50% of top ranked head nouns as
head noun triggers.
</bodyText>
<sectionHeader confidence="0.957746" genericHeader="method">
2. Deep Knowledge Resources
</sectionHeader>
<bodyText confidence="0.999900875">
Besides the widely used lexical-level features as
described above, we also explore the name alias
phenomenon, the cascaded entity name
phenomenon, the use of both a closed dictionary
from the training corpus and an open dictionary
from the database term list SwissProt and the alias
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus.
</bodyText>
<subsectionHeader confidence="0.897723">
2.1 Name Alias Resolution
</subsectionHeader>
<bodyText confidence="0.999937228571429">
A novel name alias feature is proposed to resolve
the name alias phenomenon. The intuition behind
this feature is the name alias phenomenon that
relevant entities will be referred to in many ways
throughout a given text and thus success of named
entity recognition is conditional on success at
determining when one noun phrase refers to the
very same entity as another noun phrase.
During decoding, the entity names already
recognized from the previous sentences of the
document are stored in a list. When the system
encounters an entity name candidate (e.g. a word
with a special word formation pattern), a name
alias algorithm (similar to Schwartz et al 2003) is
invoked to first dynamically determine whether the
entity name candidate might be alias for a
previously recognized name in the recognized list.
The name alias feature FALIAS is represented as
ENTITYnLm (L indicates the locality of the name
alias phenomenon). Here ENTITY indicates the
class of the recognized entity name and n indicates
the number of the words in the recognized entity
name while m indicates the number of the words in
the recognized entity name from which the name
alias candidate is formed. For example, when the
decoding process encounters the word “TCF”, the
word “TCF” is proposed as an entity name
candidate and the name alias algorithm is invoked
to check if the word “TCF” is an alias of a
recognized named entity. If “T cell Factor” is a
“Protein” name recognized earlier in the
document, the word “TCF” is determined as an
alias of “T cell Factor” with the name alias feature
Protein3L3 by taking the three initial letters of the
three-word “protein” name “T cell Factor”.
</bodyText>
<subsectionHeader confidence="0.996372">
2.2 Cascaded Entity Name Resolution
</subsectionHeader>
<bodyText confidence="0.99547975">
It is found (Shen et al 2003) that 16.57% of entity
names in GENIA V3.0 have cascaded
constructions, e.g.
&lt;RNA&gt;&lt;DNA&gt;CIITA&lt;/DNA&gt; mRNA&lt;/RNA&gt;.
Therefore, it is important to resolve such
phenomenon.
Here, a pattern-based module is proposed to
resolve the cascaded entity names while the above
HMM is applied to recognize embedded entity
names and non-cascaded entity names. In the
GENIA corpus, we find that there are six useful
patterns of cascaded entity name constructions:
</bodyText>
<listItem confidence="0.929594083333333">
• &lt;ENTITY&gt; := &lt;ENTITY&gt; + head noun, e.g.
&lt;PROTEIN&gt; binding motif4&lt;DNA&gt;
• &lt;ENTITY&gt; := &lt;ENTITY&gt; + &lt;ENTITY&gt;
• &lt;ENTITY&gt; := modifier + &lt;ENTITY&gt;, e.g.
anti &lt;Protein&gt;4&lt;Protein&gt;
• &lt;ENTITY&gt; := &lt;ENTITY&gt; + word +
&lt;ENTITY&gt;
• &lt;ENTITY&gt; := modifier + &lt;ENTITY&gt; + head
noun
•
&lt;ENTITY&gt; := &lt;ENTITY&gt; + &lt;ENTITY&gt; +
head noun
</listItem>
<bodyText confidence="0.998105333333333">
In our experiments, all the rules of above six
patterns are extracted from the cascaded entity
names in the GENIA V3.0 to deal with the
</bodyText>
<page confidence="0.999084">
97
</page>
<bodyText confidence="0.999384">
cascaded entity name phenomenon where the
&lt;ENTITY&gt; above is restricted to the five
categories in the shared task: Protein, DNA, RNA,
CellLine, CellType.
</bodyText>
<subsectionHeader confidence="0.999131">
2.3 Abbreviation Resolution
</subsectionHeader>
<bodyText confidence="0.999995135135135">
While the name alias feature is useful to detect the
inter-sentential name alias phenomenon, it is
unable to identify the inner-sentential name alias
phenomenon: the inner-sentential abbreviation.
Such abbreviations widely occur in the biomedical
domain.
In our system, we present an effective and
efficient algorithm to recognize the inner-sentential
abbreviations more accurately by mapping them to
their full expanded forms. In the GENIA corpus,
we observe that the expanded form and its
abbreviation often occur together via parentheses.
Generally, there are two patterns: “expanded form
(abbreviation)” and “abbreviation (expanded
form)”.
Our algorithm is based on the fact that it is
much harder to classify an abbreviation than its
expanded form. Generally, the expanded form is
more evidential than its abbreviation to determine
its class. The algorithm works as follows: Given a
sentence with parentheses, we use a similar
algorithm as in Schwartz et al (2003) to determine
whether it is an abbreviation with parentheses. If
yes, we remove the abbreviation and the
parentheses from the sentence. After the sentence
is processed, we restore the abbreviation with
parentheses to its original position in the sentence.
Then, the abbreviation is classified as the same
class of the expanded form, if the expanded form is
recognized as an entity name. In the meanwhile,
we also adjust the boundaries of the expanded form
according to the abbreviation, if necessary. Finally,
the expanded form and its abbreviation are stored
in the recognized list of biomedical entity names
from the document to help the resolution of
forthcoming occurrences of the same abbreviation
in the document.
</bodyText>
<subsectionHeader confidence="0.977412">
2.4 Dictionary
</subsectionHeader>
<bodyText confidence="0.999977941176471">
In our system, two different features are explored
to capture the existence of an entity name in a
closed dictionary and an open dictionary. Here, the
closed dictionary is constructed by extracting all
entity names from the training data while the open
dictionary (~700,000 entries) is combined from the
database term list Swissport and the alias list
LocusLink. The closed dictionary feature is
represented as ClosedENTITYn (Here ENTITY
indicates the class of the entity name and n
indicates the number of the words in the entity
name) while the open dictionary feature is
represented as Openn (Here n indicates the number
of the words in the entity name. We don’t
differentiate the class of the entity name since the
open dictionary only contains protein/gene names
and their aliases).
</bodyText>
<subsectionHeader confidence="0.865049">
2.5 In-domain POS
</subsectionHeader>
<bodyText confidence="0.9998052">
We also examine the impact of an in-domain POS
feature instead of an out-domain POS feature
which is trained on PENN TreeBank. Here, the in-
domain POS is trained on the GENIA corpus
V3.02p.
</bodyText>
<sectionHeader confidence="0.993253" genericHeader="evaluation">
3. Evaluation
</sectionHeader>
<bodyText confidence="0.9985436">
Table 1 shows the performance of the baseline
system and the impact of deep knowledge
resources while Table 2-4 show the detailed
performance using the provided scoring algorithm.
Table 1 shows that:
</bodyText>
<listItem confidence="0.904144035714286">
• The baseline system achieves F-measure of
60.3 while incorporation of deep knowledge
resources can improve the performance by 12.2 to
72.5 in F-measure.
• The replacement of the out-domain POS with
in-domain POS improves the performance by 3.8
in F-measure. This suggests in-domain POS can
much improve the performance.
• The name alias feature in name alias resolution
slightly improves the performance by 0.9 in F-
measure.
• The cascaded entity name resolution improves
the performance by 3.1 in F-measure. This
suggests that the cascaded entity name resolution is
very useful due to the fact that about 16% of entity
names have cascaded constructions.
• The abbreviation resolution improves the
performance by 2.1 in F-measure.
• The small closed dictionary improves the
performance by 1.5 in F-measure. In the
meanwhile, the large open dictionary improves the
performance by 1.2 in F-measure largely due to the
performance improvement for the protein class. It
is interesting that the small closed dictionary
contributes more than the large open dictionary
does. This may be due to the high ambiguity in the
open dictionary and that the open dictionary only
contains protein and gene names.
</listItem>
<tableCaption confidence="0.999149">
Table 1: Impact of Deep Knowledge Resources
</tableCaption>
<table confidence="0.7557875">
Performance F
Baseline 60.3
</table>
<page confidence="0.85953">
98
</page>
<table confidence="0.999910142857143">
+In-domain POS +3.8
+Name Alias Feature +0.9
+Cascaded Entity Name Res. +3.1
+Abbreviation Resolution +2.1
+Small Closed Dictionary +1.5
+Large Open Dictionary +1.2
+All Deep Knowledge Resources +12.2
</table>
<tableCaption confidence="0.939947">
Table 2: Final Detailed Performance: full correct
answer
</tableCaption>
<table confidence="0.99963775">
(# of correct P R F
answers)
Protein (4015) 69.01 79.24 73.77
DNA (772) 66.84 73.11 69.83
RNA (75) 64.66 63.56 64.10
Cell Line (329) 53.85 65.80 59.23
Cell Type (1391) 78.06 72.41 75.13
Overall (6582) 69.42 75.99 72.55
</table>
<tableCaption confidence="0.9792455">
Table 3: Final Detailed Performance: correct left
boundary with correct class information
</tableCaption>
<table confidence="0.9995555">
(# of correct P R F
answers)
Protein (4239) 72.86 83.66 77.89
DNA (798) 69.09 75.57 72.18
RNA (76) 65.52 64.41 64.96
Cell Line (346) 56.63 69.20 62.29
Cell Type (1418) 79.57 73.82 76.59
Overall (6877) 72.53 79.39 75.80
</table>
<tableCaption confidence="0.983847">
Table 4: Final Detailed Performance: correct right
boundary with correct class information
</tableCaption>
<table confidence="0.999599375">
(# of correct P R F
answers)
Protein (4285) 73.65 84.57 78.73
DNA (854) 73.94 80.87 77.25
RNA (83) 71.55 70.34 70.94
Cell Line (383) 62.68 76.60 68.95
Cell Type (1532) 85.97 79.75 82.74
Overall (7137) 75.27 82.39 78.67
</table>
<sectionHeader confidence="0.998142" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999980833333333">
In the paper, we have explored various deep
knowledge resources such as the name alias
phenomenon, the cascaded entity name
phenomenon, the use of both a closed dictionary
from the training corpus and an open dictionary
from the database term list SwissProt and the alias
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus.
In the near future, we will further improve the
performance by investigating more on conjunction
and disjunction construction and the combination
of coreference resolution.
</bodyText>
<sectionHeader confidence="0.976213" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998868666666667">
We thank ZHANG Zhuo for providing the
database entity name list SwissProt and the alias
list LocusLink.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998424416666667">
Chen and Goodman. 1996. An Empirical Study of
Smoothing Technniques for Language
Modeling. In Proceedings of the 34th Annual
Meeting of the Association of Computational
Linguistics (ACL’1996). pp310-318. Santa Cruz,
California, USA.
Ohta T., Tateisi Y., Kim J., Mima H., and Tsujii J.
2002. The GENIA corpus: An annotated
research abstract corpus in molecular biology
domain. In Proc. of HLT 2002.
Platt J. 1999. Probabilistic Outputs for Support
Vector Machines and comparisions to
regularized Likelihood Methods. MIT Press.
Schwartz A.S. and Hearst M.A. 2003. A Simple
Algorithm for Identifying Abbreviation
Definitions in Biomedical Text. In Proc. of the
Pacific Symposium on Biocomputing (PSB
2003) Kauai.
Shen Dan, Zhang Jie, Zhou GuoDong, Su Jian and
Tan Chew Lim, Effective Adaptation of a
Hidden Markov Model-based Named Entity
Recognizer for Biomedical Domain,
Proceedings of ACL’2003 Workshop on Natural
Language Processing in Biomedicine, Sapporo,
Japan, 11 July 2003. pp49-56.
Vapnik V. 1995. The Nature of Statistical
Learning Theory. NY, USA: Springer-Verlag.
Viterbi A.J. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information
Theory, 260-269.
Zhou G.D. and Su J. 2002. Named Entity
Recognition using an HMM-based Chunk
Tagger. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), 473-480.
</reference>
<page confidence="0.998938">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000323">
<title confidence="0.999631">Exploring Deep Knowledge Resources in Biomedical Name Recognition</title>
<author confidence="0.99366">ZHOU GuoDong SU</author>
<affiliation confidence="0.998078">Institute for Infocomm</affiliation>
<address confidence="0.993468">21 Heng Mui Keng Singapore</address>
<email confidence="0.997036">zhougd,sujian}@i2r.a-star.edu.sg</email>
<abstract confidence="0.99846994498382">In this paper, we present a named entity recognition system in the biomedical domain. In order to deal with the special phenomena in the biomedical domain, various evidential features are proposed and integrated through a Hidden Markov Model (HMM). In addition, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve the data sparseness problem in our system. Besides the widely used lexical-level features, such as word formation pattern, morphological pattern, out-domain POS and semantic trigger, we also explore the name alias phenomenon, the cascaded entity name phenomenon, the use of both a closed dictionary from the training corpus and an open dictionary from the database term list SwissProt and the alias list LocusLink, the abbreviation resolution and indomain POS using the GENIA corpus. 1. The Baseline System 1.1 Hidden Markov Model In this paper, we use the Hidden Markov Model (HMM) as described in Zhou et al (2002). Given output sequence the system finds most likely state sequence ... O n follows: n (1) ) 1 From Equation (1), we can see that: • The first term can be computed by applying chain rules. In ngram modeling (Chen et al 1996), each tag is assumed to be dependent on the N-1 previous tags. • The second term is the summation of log probabilities of all the individual tags. • The third term corresponds to the “lexical” component (dictionary) of the tagger. The idea behind the model is that it tries to assign each output an appropriate tag (state), which contains boundary and class information. For 1binds stronger than kBto The tag assigned to token should indicate that it is at the beginning of an name and it belongs to the the tag assigned to token indicate that it does not belong to an entity name. Here, the Viterbi algorithm (Viterbi 1967) is implemented to find the most likely tag sequence. The problem with the above HMM lies in the sparseness problem raised by | n O the third term of Equation (1). In this paper, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve this problem in our system. 1.2 Support Vector Machine plus Sigmoid Support Vector Machines (SVMs) are a popular machine learning approach first presented by Vapnik (1995). Based on the structural risk minimization of statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective examples in the training set. However, SVMs produce an uncalibrated value that is not probability. That is, the unthresholded output of an SVM can be represented as , - + i SV ∈ To map the SVM output into the probability, we train an additional sigmoid model(Platt 1999): Basically, SVMs are binary classifiers. Therefore, we must extend SVMs to multi-class (e.g. K) classifiers. For efficiency, we apply the vs. others which builds K classifiers so as to separate one class from all others, instead the which builds K*(K-1)/2 classifiers considering all pairs of classes. Moreover, we only apply the simple linear kernel, although other kernels (e.g. polynomial kernel) and pairwise strategy can have better performance. | 1 exp( B + p 1 1 n i = log ( | P S n 1 logP(si |O1 + ∑ i = 96 1.3 Features Various widely used lexical-level features are explored in the baseline system. Word Formation Pattern The purpose of this feature is to capture capitalization, digitalization and other word formation information. In this paper, the same feature as in Shen et al 2003 is used. Morphological Pattern Morphological information, such as prefix and suffix, is considered as an important cue for terminology identification. Same as Shen et al 2003, we use a statistical method to get the most useful prefixes/suffixes from the training data. Part-of-Speech Since many of the words in biomedical entity names are in lowercase, capitalization information in the biomedical domain is not as evidential as that in the newswire domain. Moreover, many biomedical entity names are descriptive and very long. Therefore, POS may provide useful evidence about the boundaries of biomedical entity names. In the baseline system, an out-domain POS using the PENN TreeBank is applied. Head Noun Trigger The head noun, which is the major noun of a noun phrase, often describes the function or the property of the noun phrase. In this paper, we automatically extract unigram and bigram head nouns from the training data, and rank them by frequency. For each entity class, we select 50% of top ranked head nouns as head noun triggers. 2. Deep Knowledge Resources Besides the widely used lexical-level features as described above, we also explore the name alias phenomenon, the cascaded entity name phenomenon, the use of both a closed dictionary from the training corpus and an open dictionary from the database term list SwissProt and the alias list LocusLink, the abbreviation resolution and indomain POS using the GENIA corpus. 2.1 Name Alias Resolution A novel name alias feature is proposed to resolve the name alias phenomenon. The intuition behind this feature is the name alias phenomenon that relevant entities will be referred to in many ways throughout a given text and thus success of named entity recognition is conditional on success at determining when one noun phrase refers to the very same entity as another noun phrase. During decoding, the entity names already recognized from the previous sentences of the document are stored in a list. When the system encounters an entity name candidate (e.g. a word with a special word formation pattern), a name alias algorithm (similar to Schwartz et al 2003) is invoked to first dynamically determine whether the entity name candidate might be alias for a previously recognized name in the recognized list. name alias feature is represented as indicates the locality of the name phenomenon). Here the of the recognized entity name and the number of the words in the recognized entity while the number of the words in the recognized entity name from which the name alias candidate is formed. For example, when the process encounters the word the is proposed as an entity name candidate and the name alias algorithm is invoked check if the word is an alias of a named entity. If cell is a recognized earlier in the the word is determined as an of cell with the name alias feature taking the three initial letters of the cell 2.2 Cascaded Entity Name Resolution It is found (Shen et al 2003) that 16.57% of entity names in GENIA V3.0 have cascaded constructions, e.g. Therefore, it is important to resolve such phenomenon. Here, a pattern-based module is proposed to resolve the cascaded entity names while the above HMM is applied to recognize embedded entity names and non-cascaded entity names. In the GENIA corpus, we find that there are six useful patterns of cascaded entity name constructions: • &lt;ENTITY&gt; := &lt;ENTITY&gt; + head noun, e.g. binding • &lt;ENTITY&gt; := &lt;ENTITY&gt; + &lt;ENTITY&gt; • &lt;ENTITY&gt; := modifier + &lt;ENTITY&gt;, e.g. • &lt;ENTITY&gt; := &lt;ENTITY&gt; + word + &lt;ENTITY&gt; • &lt;ENTITY&gt; := modifier + &lt;ENTITY&gt; + head noun &lt;ENTITY&gt; := &lt;ENTITY&gt; + &lt;ENTITY&gt; + head noun In our experiments, all the rules of above six patterns are extracted from the cascaded entity names in the GENIA V3.0 to deal with the 97 cascaded entity name phenomenon where the &lt;ENTITY&gt; above is restricted to the five categories in the shared task: Protein, DNA, RNA, CellLine, CellType. 2.3 Abbreviation Resolution While the name alias feature is useful to detect the inter-sentential name alias phenomenon, it is unable to identify the inner-sentential name alias phenomenon: the inner-sentential abbreviation. Such abbreviations widely occur in the biomedical domain. In our system, we present an effective and efficient algorithm to recognize the inner-sentential abbreviations more accurately by mapping them to their full expanded forms. In the GENIA corpus, we observe that the expanded form and its abbreviation often occur together via parentheses. Generally, there are two patterns: “expanded form (abbreviation)” and “abbreviation (expanded form)”. Our algorithm is based on the fact that it is much harder to classify an abbreviation than its expanded form. Generally, the expanded form is more evidential than its abbreviation to determine its class. The algorithm works as follows: Given a sentence with parentheses, we use a similar algorithm as in Schwartz et al (2003) to determine whether it is an abbreviation with parentheses. If yes, we remove the abbreviation and the parentheses from the sentence. After the sentence is processed, we restore the abbreviation with parentheses to its original position in the sentence. Then, the abbreviation is classified as the same class of the expanded form, if the expanded form is recognized as an entity name. In the meanwhile, we also adjust the boundaries of the expanded form according to the abbreviation, if necessary. Finally, the expanded form and its abbreviation are stored in the recognized list of biomedical entity names from the document to help the resolution of forthcoming occurrences of the same abbreviation in the document. 2.4 Dictionary In our system, two different features are explored to capture the existence of an entity name in a closed dictionary and an open dictionary. Here, the closed dictionary is constructed by extracting all entity names from the training data while the open dictionary (~700,000 entries) is combined from the database term list Swissport and the alias list LocusLink. The closed dictionary feature is as the class of the entity name and indicates the number of the words in the entity name) while the open dictionary feature is as the number of the words in the entity name. We don’t differentiate the class of the entity name since the open dictionary only contains protein/gene names and their aliases). 2.5 In-domain POS We also examine the impact of an in-domain POS feature instead of an out-domain POS feature which is trained on PENN TreeBank. Here, the indomain POS is trained on the GENIA corpus V3.02p. 3. Evaluation Table 1 shows the performance of the baseline system and the impact of deep knowledge resources while Table 2-4 show the detailed performance using the provided scoring algorithm. Table 1 shows that: • The baseline system achieves F-measure of 60.3 while incorporation of deep knowledge resources can improve the performance by 12.2 to 72.5 in F-measure. • The replacement of the out-domain POS with in-domain POS improves the performance by 3.8 in F-measure. This suggests in-domain POS can much improve the performance. • The name alias feature in name alias resolution slightly improves the performance by 0.9 in Fmeasure. • The cascaded entity name resolution improves the performance by 3.1 in F-measure. This suggests that the cascaded entity name resolution is very useful due to the fact that about 16% of entity names have cascaded constructions. • The abbreviation resolution improves the performance by 2.1 in F-measure. • The small closed dictionary improves the performance by 1.5 in F-measure. In the meanwhile, the large open dictionary improves the performance by 1.2 in F-measure largely due to the performance improvement for the protein class. It is interesting that the small closed dictionary contributes more than the large open dictionary does. This may be due to the high ambiguity in the open dictionary and that the open dictionary only contains protein and gene names.</abstract>
<note confidence="0.926512682926829">Table 1: Impact of Deep Knowledge Resources Performance F Baseline 60.3 98 +In-domain POS +3.8 +Name Alias Feature +0.9 +Cascaded Entity Name Res. +3.1 +Abbreviation Resolution +2.1 +Small Closed Dictionary +1.5 +Large Open Dictionary +1.2 +All Deep Knowledge Resources +12.2 Table 2: Final Detailed Performance: full correct answer (# of P R F answers) Protein (4015) 69.01 79.24 73.77 DNA (772) 66.84 73.11 69.83 RNA (75) 64.66 63.56 64.10 Cell Line (329) 53.85 65.80 59.23 Cell Type (1391) 78.06 72.41 75.13 Overall (6582) 69.42 75.99 72.55 Table 3: Final Detailed Performance: correct left boundary with correct class information (# of P R F answers) Protein (4239) 72.86 83.66 77.89 DNA (798) 69.09 75.57 72.18 RNA (76) 65.52 64.41 64.96 Cell Line (346) 56.63 69.20 62.29 Cell Type (1418) 79.57 73.82 76.59 Overall (6877) 72.53 79.39 75.80 Table 4: Final Detailed Performance: correct right boundary with correct class information (# of P R F answers) Protein (4285) 73.65 84.57 78.73 DNA (854) 73.94 80.87 77.25 RNA (83) 71.55 70.34 70.94 Cell Line (383) 62.68 76.60 68.95 Cell Type (1532) 85.97 79.75 82.74 Overall (7137) 75.27 82.39 78.67</note>
<abstract confidence="0.982634294117647">4. Conclusion In the paper, we have explored various deep knowledge resources such as the name alias phenomenon, the cascaded entity name phenomenon, the use of both a closed dictionary from the training corpus and an open dictionary from the database term list SwissProt and the alias list LocusLink, the abbreviation resolution and indomain POS using the GENIA corpus. In the near future, we will further improve the performance by investigating more on conjunction and disjunction construction and the combination of coreference resolution. Acknowledgement We thank ZHANG Zhuo for providing the database entity name list SwissProt and the alias list LocusLink.</abstract>
<note confidence="0.7499115">References Chen and Goodman. 1996. An Empirical Study of</note>
<title confidence="0.852459">Smoothing Technniques for Language</title>
<affiliation confidence="0.526187666666667">In of the 34th Annual Meeting of the Association of Computational pp310-318. Santa Cruz,</affiliation>
<address confidence="0.974453">California, USA.</address>
<note confidence="0.7649325">Ohta T., Tateisi Y., Kim J., Mima H., and Tsujii J. 2002. The GENIA corpus: An annotated research abstract corpus in molecular biology In of HLT Platt J. 1999. Probabilistic Outputs for Support Vector Machines and comparisions to Likelihood Methods. Press. Schwartz A.S. and Hearst M.A. 2003. A Simple</note>
<title confidence="0.477110833333333">Algorithm for Identifying Abbreviation in Biomedical Text. In of the Pacific Symposium on Biocomputing (PSB Shen Dan, Zhang Jie, Zhou GuoDong, Su Jian and Tan Chew Lim, Effective Adaptation of a Hidden Markov Model-based Named Entity</title>
<note confidence="0.947689714285714">Recognizer for Biomedical Domain, Proceedings of ACL’2003 Workshop on Natural Processing in Sapporo, Japan, 11 July 2003. pp49-56. V. The Nature of Statistical NY, USA: Springer-Verlag. Viterbi A.J. 1967. Error bounds for convolutional</note>
<title confidence="0.5377405">codes and an asymptotically optimum decoding Transactions on Information</title>
<note confidence="0.990125666666667">260-269. Zhou G.D. and Su J. 2002. Named Entity Recognition using an HMM-based Chunk In of the 40th Annual Meeting of the Association for Computational Linguistics 473-480.</note>
<intro confidence="0.644816">99</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chen</author>
<author>Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Technniques for Language Modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association of Computational Linguistics (ACL’1996).</booktitle>
<pages>310--318</pages>
<location>Santa Cruz, California, USA.</location>
<marker>Chen, Goodman, 1996</marker>
<rawString>Chen and Goodman. 1996. An Empirical Study of Smoothing Technniques for Language Modeling. In Proceedings of the 34th Annual Meeting of the Association of Computational Linguistics (ACL’1996). pp310-318. Santa Cruz, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Kim</author>
<author>H Mima</author>
<author>J Tsujii</author>
</authors>
<title>The GENIA corpus: An annotated research abstract corpus in molecular biology domain.</title>
<date>2002</date>
<booktitle>In Proc. of HLT</booktitle>
<marker>Ohta, Tateisi, Kim, Mima, Tsujii, 2002</marker>
<rawString>Ohta T., Tateisi Y., Kim J., Mima H., and Tsujii J. 2002. The GENIA corpus: An annotated research abstract corpus in molecular biology domain. In Proc. of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Probabilistic Outputs for Support Vector Machines and comparisions to regularized Likelihood Methods.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3154" citStr="Platt 1999" startWordPosition="544" endWordPosition="545">hine learning approach first presented by Vapnik (1995). Based on the structural risk minimization of statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective examples in the training set. However, SVMs produce an uncalibrated value that is not probability. That is, the unthresholded output of an SVM can be represented as f (x) ai yi k(xi , x) b = ∑ - - + (2) i SV ∈ To map the SVM output into the probability, we train an additional sigmoid model(Platt 1999): Basically, SVMs are binary classifiers. Therefore, we must extend SVMs to multi-class (e.g. K) classifiers. For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes. Moreover, we only apply the simple linear kernel, although other kernels (e.g. polynomial kernel) and pairwise strategy can have better performance. (y=1 |f) = 1 exp( + Af B + p 1 ) (3) O;) = log P(S;) − ∑ log P(si ) 1 n i = log ( | P S n 1 logP(si |O1 n + ∑ i </context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>Platt J. 1999. Probabilistic Outputs for Support Vector Machines and comparisions to regularized Likelihood Methods. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Schwartz</author>
<author>M A Hearst</author>
</authors>
<title>A Simple Algorithm for Identifying Abbreviation Definitions in Biomedical Text.</title>
<date>2003</date>
<booktitle>In Proc. of the Pacific Symposium on Biocomputing (PSB 2003) Kauai.</booktitle>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Schwartz A.S. and Hearst M.A. 2003. A Simple Algorithm for Identifying Abbreviation Definitions in Biomedical Text. In Proc. of the Pacific Symposium on Biocomputing (PSB 2003) Kauai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Dan</author>
<author>Zhang Jie</author>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
<author>Tan Chew Lim</author>
</authors>
<title>Effective Adaptation of a Hidden Markov Model-based Named Entity Recognizer for Biomedical Domain,</title>
<date>2003</date>
<booktitle>Proceedings of ACL’2003 Workshop on Natural Language Processing in Biomedicine,</booktitle>
<pages>49--56</pages>
<location>Sapporo,</location>
<marker>Dan, Jie, GuoDong, Jian, Lim, 2003</marker>
<rawString>Shen Dan, Zhang Jie, Zhou GuoDong, Su Jian and Tan Chew Lim, Effective Adaptation of a Hidden Markov Model-based Named Entity Recognizer for Biomedical Domain, Proceedings of ACL’2003 Workshop on Natural Language Processing in Biomedicine, Sapporo, Japan, 11 July 2003. pp49-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag.</publisher>
<location>NY, USA:</location>
<contexts>
<context position="2598" citStr="Vapnik (1995)" startWordPosition="446" endWordPosition="447">and it belongs to the “Protein” class; and the tag assigned to token “binds” should indicate that it does not belong to an entity name. Here, the Viterbi algorithm (Viterbi 1967) is implemented to find the most likely tag sequence. The problem with the above HMM lies in the data sparseness problem raised by P ( |1 ) n si O in the third term of Equation (1). In this paper, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve this problem in our system. 1.2 Support Vector Machine plus Sigmoid Support Vector Machines (SVMs) are a popular machine learning approach first presented by Vapnik (1995). Based on the structural risk minimization of statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective examples in the training set. However, SVMs produce an uncalibrated value that is not probability. That is, the unthresholded output of an SVM can be represented as f (x) ai yi k(xi , x) b = ∑ - - + (2) i SV ∈ To map the SVM output into the probability, we train an additional sigmoid model(Platt 1999): Basically, SVMs are binary classifiers. Th</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik V. 1995. The Nature of Statistical Learning Theory. NY, USA: Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>260--269</pages>
<contexts>
<context position="2163" citStr="Viterbi 1967" startWordPosition="369" endWordPosition="370">mmation of log probabilities of all the individual tags. • The third term corresponds to the “lexical” component (dictionary) of the tagger. The idea behind the model is that it tries to assign each output an appropriate tag (state), which contains boundary and class information. For example, “TCF 1 binds stronger than NF kB to TCEd DNA”. The tag assigned to token “TCF” should indicate that it is at the beginning of an entity name and it belongs to the “Protein” class; and the tag assigned to token “binds” should indicate that it does not belong to an entity name. Here, the Viterbi algorithm (Viterbi 1967) is implemented to find the most likely tag sequence. The problem with the above HMM lies in the data sparseness problem raised by P ( |1 ) n si O in the third term of Equation (1). In this paper, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve this problem in our system. 1.2 Support Vector Machine plus Sigmoid Support Vector Machines (SVMs) are a popular machine learning approach first presented by Vapnik (1995). Based on the structural risk minimization of statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two clas</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi A.J. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, 260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>J Su</author>
</authors>
<title>Named Entity Recognition using an HMM-based Chunk Tagger.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>473--480</pages>
<marker>Zhou, Su, 2002</marker>
<rawString>Zhou G.D. and Su J. 2002. Named Entity Recognition using an HMM-based Chunk Tagger. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), 473-480.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>