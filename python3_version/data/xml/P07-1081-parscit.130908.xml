<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004072">
<title confidence="0.991113">
Corpus Effects on the Evaluation of Automated Transliteration Systems
</title>
<author confidence="0.995525">
Sarvnaz Karimi Andrew Turpin Falk Scholer
</author>
<affiliation confidence="0.8269915">
School of Computer Science and Information Technology
RMIT University, GPO Box 2476V, Melbourne 3001, Australia
</affiliation>
<email confidence="0.998683">
{sarvnaz,aht,fscholer}@cs.rmit.edu.au
</email>
<sectionHeader confidence="0.994794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905318181819">
Most current machine transliteration sys-
tems employ a corpus of known source-
target word pairs to train their system, and
typically evaluate their systems on a similar
corpus. In this paper we explore the perfor-
mance of transliteration systems on corpora
that are varied in a controlled way. In partic-
ular, we control the number, and prior lan-
guage knowledge of human transliterators
used to construct the corpora, and the origin
of the source words that make up the cor-
pora. We find that the word accuracy of au-
tomated transliteration systems can vary by
up to 30% (in absolute terms) depending on
the corpus on which they are run. We con-
clude that at least four human transliterators
should be used to construct corpora for eval-
uating automated transliteration systems;
and that although absolute word accuracy
metrics may not translate across corpora, the
relative rankings of system performance re-
mains stable across differing corpora.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936173913044">
Machine transliteration is the process of transform-
ing a word written in a source language into a word
in a target language without the aid of a bilingual
dictionary. Word pronunciation is preserved, as far
as possible, but the script used to render the target
word is different from that of the source language.
Transliteration is applied to proper nouns and out-
of-vocabulary terms as part of machine translation
and cross-lingual information retrieval (CLIR) (Ab-
dulJaleel and Larkey, 2003; Pirkola et al., 2006).
Several transliteration methods are reported in the
literature for a variety of languages, with their per-
formance being evaluated on multilingual corpora.
Source-target pairs are either extracted from bilin-
gual documents or dictionaries (AbdulJaleel and
Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi,
2006; Zelenko and Aone, 2006), or gathered ex-
plicitly from human transliterators (Al-Onaizan and
Knight, 2002; Zelenko and Aone, 2006). Some eval-
uations of transliteration methods depend on a single
unique transliteration for each source word, while
others take multiple target words for a single source
word into account. In their work on transliterating
English to Persian, Karimi et al. (2006) observed
that the content of the corpus used for evaluating
systems could have dramatic affects on the reported
accuracy of methods.
The effects of corpus composition on the evalua-
tion of transliteration systems has not been specif-
ically studied, with only implicit experiments or
claims made in the literature such as introduc-
ing the effects of different transliteration mod-
els (AbdulJaleel and Larkey, 2003), language fam-
ilies (Lind´en, 2005) or application based (CLIR)
evaluation (Pirkola et al., 2006). In this paper, we re-
port our experiments designed to explicitly examine
the effect that varying the underlying corpus used in
both training and testing systems has on translitera-
tion accuracy. Specifically, we vary the number of
human transliterators that are used to construct the
corpus; and the origin of the English words used in
the corpus.
Our experiments show that the word accuracy of
automated transliteration systems can vary by up to
30% (in absolute terms), depending on the corpus
used. Despite the wide range of absolute values
</bodyText>
<page confidence="0.969608">
640
</page>
<note confidence="0.9254665">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 640–647,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998014">
in performance, the ranking of our two translitera-
tion systems was preserved on all corpora. We also
find that a human’s confidence in the language from
which they are transliterating can affect the corpus
in such a way that word accuracy rates are altered.
</bodyText>
<sectionHeader confidence="0.989576" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9995236">
Machine transliteration methods are divided into
grapheme-based (AbdulJaleel and Larkey, 2003;
Lind´en, 2005), phoneme-based (Jung et al., 2000;
Virga and Khudanpur, 2003) and combined tech-
niques (Bilac and Tanaka, 2005; Oh and Choi,
2006). Grapheme-based methods derive transforma-
tion rules for character combinations in the source
text from a training data set, while phoneme-based
methods use an intermediate phonetic transforma-
tion. In this paper, we use two grapheme-based
methods for English to Persian transliteration. Dur-
ing a training phase, both methods derive rules for
transforming character combinations (segments) in
the source language into character combinations in
the target language with some probability.
During transliteration, the source word si is seg-
mented and rules are chosen and applied to each seg-
ment according to heuristics. The probability of a
resulting word is the product of the probabilities of
the applied rules. The result is a list of target words
sorted by their associated probabilities, Li.
The first system we use (SYS-1) is an n-gram
approach that uses the last character of the previ-
ous source segment to condition the choice of the
rule for the current source segment. This system has
been shown to outperform other n-gram based meth-
ods for English to Persian transliteration (Karimi et
al., 2006).
The second system we employ (SYS-2) makes
use of some explicit knowledge of our chosen lan-
guage pair, English and Persian, and is also on
the collapsed-vowel scheme presented by Karimi et
al. (2006). In particular, it exploits the tendency for
runs of English vowels to be collapsed into a single
Persian character, or perhaps omitted from the Per-
sian altogether. As such, segments are chosen based
on surrounding consonants and vowels. The full de-
tails of this system are not important for this paper;
here we focus on the performance evaluation of sys-
tems, not the systems themselves.
</bodyText>
<subsectionHeader confidence="0.994645">
2.1 System Evaluation
</subsectionHeader>
<bodyText confidence="0.968293">
In order to evaluate the list Li of target words pro-
duced by a transliteration system for source word si,
a test corpus is constructed. The test corpus con-
sists of a source word, si, and a list of possible target
words {tij}, where 1 &lt; j &lt; di, the number of dis-
tinct target words for source word si. Associated
with each tij is a count nij which is the number of
human transliterators who transliterated si into tij.
Often the test corpus is a proportion of a larger
corpus, the remainder of which has been used for
training the system’s rule base. In this work we
adopt the standard ten-fold cross validation tech-
nique for all of our results, where 90% of a corpus
is used for training and 10% for testing. The pro-
cess is repeated ten times, and the mean result taken.
Forthwith, we use the term corpus to refer to the sin-
gle corpus from which both training and test sets are
drawn in this fashion.
Once the corpus is decided upon, a metric to mea-
sure the system’s accuracy is required. The appro-
priate metric depends on the scenario in which the
transliteration system is to be used. For example,
in a machine translation application where only one
target word can be inserted in the text to represent a
source word, it is important that the word at the top
of the system generated list of target words (by def-
inition the most probable) is one of the words gen-
erated by a human in the corpus. More formally,
the first word generated for source word si, Li1, must
be one of tij,1 &lt; j &lt; di. It may even be desirable
that this is the target word most commonly used for
this source word; that is, Li1 = tij such that nij &gt;_ nik,
for all 1 &lt; k &lt; di. Alternately, in a CLIR appli-
cation, all variants of a source word might be re-
quired. For example, if a user searches for an En-
glish term “Tom” in Persian documents, the search
engine should try and locate documents that contain
both “ÐL&amp;quot;” (3 letters:u-@-Ð) and ”Õç�”(2 letters:�H-Ð),
two possible transliterations of “Tom” that would be
generated by human transliterators. In this case, a
metric that counts the number of tij that appear in
the top di elements of the system generated list, Li,
might be appropriate.
In this paper we focus on the “Top-1” case, where
it is important for the most probable target word gen-
erated by the system, Li1 to be either the most pop-
</bodyText>
<page confidence="0.99789">
641
</page>
<bodyText confidence="0.994492517241379">
ular tij (labeled the Majority, with ties broken ar-
bitrarily), or just one of the tij’s (labeled Uniform
because all possible transliterations are equally re-
warded). A third scheme (labeled Weighted) is also
possible where the reward for tij appearing as Li1
is nij/∑di
j=1 nij; here, each target word is given a
weight proportional to how often a human translit-
erator chose that target word. Due to space consid-
erations, we focus on the first two variants only.
In general, there are two commonly used met-
rics for transliteration evaluation: word accuracy
(WA) and character accuracy (CA) (Hall and Dowl-
ing, 1980). In all of our experiments, CA based
metrics closely mirrored WA based metrics, and
so conclusions drawn from the data would be the
same whether WA metrics or CA metrics were used.
Hence we only discuss and report WA based metrics
in this paper.
For each source word in the test corpus of K
words, word accuracy calculates the percentage of
correctly transliterated terms. Hence for the major-
ity case, where every source word in the corpus only
has one target word, the word accuracy is defined as
MWA =|{si|Li1 = ti1,1 ≤ i ≤ K}|/K,
and for the Uniform case, where every target variant
is included with equal weight in the corpus, the word
accuracy is defined as
UWA = |{si|Li1 ∈ {tij},1 ≤ i ≤ K,1 ≤ j ≤ di}|/K.
</bodyText>
<subsectionHeader confidence="0.996987">
2.2 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.9978369">
To evaluate the level of agreement between translit-
erators, we use an agreement measure based on Mun
and Eye (2004).
For any source word si, there are di different
transliterations made by the ni human translitera-
tors (ni = ∑di
j=1 nij, where nij is the number of times
source word si was transliterated into target word
tij). When any two transliterators agree on the
same target word, there are two agreements being
made: transliterator one agrees with transliterator
two, and vice versa. In general, therefore, the to-
tal number of agreements made on source word si is
∑ di
j=1 ni j(ni j − 1). Hence the total number of actual
agreements made on the entire corpus of K words is
nij(nij −1).
The total number of possible agreements (that is,
when all human transliterators agree on a single tar-
get word for each source word), is
</bodyText>
<equation confidence="0.998174666666667">
K
Aposs = ∑ ni(ni − 1).
i=1
</equation>
<bodyText confidence="0.954226">
The proportion of overall agreement is therefore
</bodyText>
<figure confidence="0.76373">
Aact
.
Aposs
</figure>
<subsectionHeader confidence="0.986415">
2.3 Corpora
</subsectionHeader>
<bodyText confidence="0.999992135135135">
Seven transliterators (T1, T2, ..., T7: all native Per-
sian speakers from Iran) were recruited to transliter-
ate 1500 proper names that we provided. The names
were taken from lists of names written in English on
English Web sites. Five hundred of these names also
appeared in lists of names on Arabic Web sites, and
five hundred on Dutch name lists. The transliterators
were not told of the origin of each word. The en-
tire corpus, therefore, was easily separated into three
sub-corpora of 500 words each based on the origin
of each word. To distinguish these collections, we
use E7, A7 and D7 to denote the English, Arabic and
Dutch sub-corpora, respectively. The whole 1500
word corpus is referred to as EDA7.
Dutch and Arabic were chosen with an assump-
tion that most Iranian Persian speakers have little
knowledge of Dutch, while their familiarity with
Arabic should be in the second rank after English.
All of the participants held at least a Bachelors de-
gree. Table 1 summarizes the information about
the transliterators and their perception of the given
task. Participants were asked to scale the difficulty
of the transliteration of each sub-corpus, indicated
as a scale from 1 (hard) to 3 (easy). Similarly, the
participants’ confidence in performing the task was
rated from 1 (no confidence) to 3 (quite confident).
The level of familiarity with second languages was
also reported based on a scale of zero (not familiar)
to 3 (excellent knowledge).
The information provided by participants con-
firms our assumption of transliterators knowledge
of second languages: high familiarity with English,
some knowledge of Arabic, and little or no prior
knowledge of Dutch. Also, the majority of them
found the transliteration of English terms of medium
difficulty, Dutch was considered mostly hard, and
Arabic as easy to medium.
</bodyText>
<equation confidence="0.995940625">
K
∑
i=1
Aact =
di
∑
j=1
PA =
</equation>
<page confidence="0.984889">
642
</page>
<table confidence="0.999174555555556">
Transliterator Second Language Knowledge Difficulty,Confidence
English Dutch Arabic Other English Dutch Arabic
1 2 0 1 - 1,1 1,2 2,3
2 2 0 2 - 2,2 2,3 3,3
3 2 0 1 - 2,2 1,2 2,2
4 2 0 1 - 2,2 2,1 3,3
5 2 0 2 Turkish 2,2 1,1 3,2
6 2 0 1 - 2,2 1,1 3,3
7 2 0 1 - 2,2 1,1 2,2
</table>
<tableCaption confidence="0.946684">
Table 1: Transliterator’s language knowledge (0=not familiar to 3=excellent knowledge), perception of
difficulty (1=hard to 3=easy) and confidence (1=no confidence to 3=quite confident) in creating the corpus.
</tableCaption>
<figureCaption confidence="0.967885666666667">
Figure 1: Comparison of the two evaluation metrics
using the two systems on four corpora. (Lines were
added for clarity, and do not represent data points.)
</figureCaption>
<figure confidence="0.9967289">
100
80
60
UWA (SYS-2)
UWA (SYS-1)
MWA (SYS-2)
MWA (SYS-1)
0
0 20 40 60 80 100
Corpus
</figure>
<figureCaption confidence="0.909089666666667">
Figure 2: Comparison of the two evaluation metrics
using the two systems on 100 randomly generated
sub-corpora.
</figureCaption>
<sectionHeader confidence="0.999863" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999625854166667">
Figure 1 shows the values of UWA and MWA for
E7, A7, D7 and EDA7 using the two transliteration
systems. Immediately obvious is that varying the
corpora (x-axis) results in different values for word
accuracy, whether by the UWA or MWA method. For
example, if you chose to evaluate SYS-2 with the
UWA metric on the D7 corpus, you would obtain a
result of 82%, but if you chose to evaluate it with the
A7 corpus you would receive a result of only 73%.
This makes comparing systems that report results
obtained on different corpora very difficult. Encour-
agingly, however, SYS-2 consistently outperforms
the SYS-1 on all corpora for both metrics except
MWA on E7. This implies that ranking system per-
formance on the same corpus most likely yields a
system ranking that is transferable to other corpora.
To further investigate this, we randomly extracted
100 corpora of 500 word pairs from EDA7 and ran
the two systems on them and evaluated the results
using both MWA and UWA. Both of the measures
ranked the systems consistently using all these cor-
pora (Figure 2).
As expected, the UWA metric is consistently
higher than the MWA metric; it allows for the top
transliteration to appear in any of the possible vari-
ants for that word in the corpus, unlike the MWA
metric which insists upon a single target word. For
example, for the E7 corpus using the SYS-2 ap-
proach, UWA is 76.4% and MWA is 47.0%.
Each of the three sub-corpora can be further di-
vided based on the seven individual transliterators,
in different combinations. That is, construct a sub-
corpus from T1’s transliterations, T2’s, and so on;
then take all combinations of two transliterators,
then three, and so on. In general we can construct
7Cr such corpora from r transliterators in this fash-
ion, all of which have 500 source words, but may
have between one to seven different transliterations
for each of those words.
Figure 3 shows the MWA for these sub-corpora.
The x-axis shows the number of transliterators used
to form the sub-corpora. For example, when x = 3,
the performance figures plotted are achieved on cor-
pora when taking all triples of the seven translitera-
tor’s transliterations.
From the boxplots it can be seen that performance
varies considerably when the number of transliter-
ators used to determine a majority vote is varied.
</bodyText>
<figure confidence="0.97509025925926">
100
UWA (SYS-2)
UWA (SYS-1)
MWA (SYS-2)
MWA (SYS-1)
0
E7 D7 A7 EDA7
Corpus
80
60
Word Accuracy (%)
40
20
Word Accuracy (%)
40
20
643
Word Accuracy (%)
Word Accuracy (%)
20 30 40 50 60
20 30 40 50 60
EDA7
E7
D7
A7
1 2 3 4 5 6 7 1 2 3 4 5 6 7
Number of Transliterators Number of Transliterators
</figure>
<figureCaption confidence="0.851986">
Figure 3: Performance on sub-corpora derived by combining the number of transliterators shown on the x-
axis. Boxes show the 25th and 75th percentile of the MWA for all 7Cx combinations of transliterators using
SYS-2, with whiskers showing extreme values.
</figureCaption>
<bodyText confidence="0.999765846153846">
However, the changes do not follow a fixed trend
across the languages. For E7, the range of accuracies
achieved is high when only two or three translitera-
tors are involved, ranging from 37.0% to 50.6% in
SYS-2 method and from 33.8% to 48.0% in SYS-1
(not shown) when only two transliterators’ data are
available. When more than three transliterators are
used, the range of performance is noticeably smaller.
Hence if at least four transliterators are used, then it
is more likely that a system’s MWA will be stable.
This finding is supported by Papineni et al. (2002)
who recommend that four people should be used for
collecting judgments for machine translation exper-
iments.
The corpora derived from A7 show consistent me-
dian increases as the number of transliterators in-
creases, but the median accuracy is lower than for
other languages. The D7 collection does not show
any stable results until at least six transliterator’s are
used.
The results indicate that creating a collection used
for the evaluation of transliteration systems, based
on a “gold standard” created by only one human
transliterator may lead to word accuracy results that
could show a 10% absolute difference compared to
results on a corpus derived using a different translit-
</bodyText>
<figure confidence="0.6182515">
E7 D7 A7 EDA7
Corpus
</figure>
<figureCaption confidence="0.992001">
Figure 4: Word accuracy on the sub-corpora using
only a single transliterator’s transliterations.
</figureCaption>
<bodyText confidence="0.9983341875">
erator. This is evidenced by the leftmost box in each
panel of the figure which has a wide range of results.
Figure 4 shows this box in more detail for each
collection, plotting the word accuracy for each
user for all sub-corpora for SYS-2. The accuracy
achieved varies significantly between translitera-
tors; for example, for E7 collections, word accuracy
varies from 37.2% for T1 to 50.0% for T5. This
variance is more obvious for the D7 dataset where
the difference ranges from 23.2% for T1 to 56.2%
for T3. Origin language also has an effect: accuracy
for the Arabic collection (A7) is generally less than
that of English (E7). The Dutch collection (D7),
shows an unstable trend across transliterators. In
other words, accuracy differs in a narrower range for
Arabic and English, but in wider range for Dutch.
</bodyText>
<figure confidence="0.996857230769231">
Word Accuracy (%)
40
20
60
0
SYS-2
T1
T2
T3
T4
T5
T6
T7
</figure>
<page confidence="0.904855">
644
</page>
<bodyText confidence="0.931110666666667">
which Ht = 0.79.
The expected entropy can be used to obtain a sin-
gle entropy value over the whole corpus,
This is likely due to the fact that most transliterators
found Dutch a difficult language to work with, as
reported in Table 1.
</bodyText>
<subsectionHeader confidence="0.998187">
3.1 Transliterator Consistency
</subsectionHeader>
<bodyText confidence="0.999947985294118">
To investigate the effect of invididual transliterator
consistency on system accuracy, we consider the
number of Persian characters used by each transliter-
ator on each sub-corpus, and the average number of
rules generated by SYS-2 on the ten training sets de-
rived in the ten-fold cross validation process, which
are shown in Table 2. For example, when translit-
erating words from E7 into Persian, T3 only ever
used 21 out of 32 characters available in the Persian
alphabet; T7, on the other hand, used 24 different
Persian characters. It is expected that an increase in
number of characters or rules provides more “noise”
for the automated system, hence may lead to lower
accuracy. Superficially the opposite seems true for
rules: the mean number of rules generated by SYS-
2 is much higher for the EDA7 corpus than for the A7
corpus, and yet Figure 1 shows that word accuracy
is higher on the EDA7 corpus. A correlation test,
however, reveals that there is no significant relation-
ship between either the number of characters used,
nor the number of rules generated, and the result-
ing word accuracy of SYS-2 (Spearman correlation,
p = 0.09 (characters) and p = 0.98 (rules)).
A better indication of “noise” in the corpus may
be given by the consistency with which a translit-
erator applies a certain rule. For example, a large
number of rules generated from a particular translit-
erator’s corpus may not be problematic if many of
the rules get applied with a low probability. If, on
the other hand, there were many rules with approx-
imately equal probabilities, the system may have
difficulty distinguishing when to apply some rules,
and not others. One way to quantify this effect
is to compute the self entropy of the rule distribu-
tion for each segment in the corpus for an indi-
vidual. If pij is the probability of applying rule
1 &lt; j &lt; m when confronted with source segment
i, then Hi = −∑ m j=1 pij log2 pij is the entropy of the
probability distribution for that rule. H is maximized
when the probabilities pij are all equal, and mini-
mized when the probabilities are very skewed (Shan-
non, 1948). As an example, consider the rules:
t —*&lt;u,0.5 &gt;, t —*&lt;-.6,0.3 &gt; and t —*&lt;.),0.2 &gt;; for
where Hi is the entropy of the rule probabilities for
segment i, R is the total number of segments, fi is
the frequency with which segment i occurs at any
position in all source words in the corpus, and S is
the sum of all fi.
The expected entropy for each transliterator is
shown in Figure 5, separated by corpus. Compar-
ison of this graph with Figure 4 shows that gen-
erally transliterators that have used rules inconsis-
tently generate a corpus that leads to low accuracy
for the systems. For example, T1 who has the low-
est accuracy for all the collections in both methods,
also has the highest expected entropy of rules for
all the collections. For the E7 collection, the max-
imum accuracy of 50.0%, belongs to T5 who has
the minimum expected entropy. The same applies
to the D7 collection, where the maximum accuracy
of 56.2% and the minimum expected entropy both
belong to T3. These observations are confirmed
by a statistically significant Spearman correlation
between expected rule entropy and word accuracy
(r = −0.54, p = 0.003). Therefore, the consistency
with which transliterators employ their own internal
rules in developing a corpus has a direct effect on
system performance measures.
</bodyText>
<subsectionHeader confidence="0.999671">
3.2 Inter-Transliterator Agreement and
Perceived Difficulty
</subsectionHeader>
<bodyText confidence="0.999780769230769">
Here we present various agreement proportions (PA
from Section 2.2), which give a measure of consis-
tency in the corpora across all users, as opposed to
the entropy measure which gives a consistency mea-
sure for a single user. For E7, PA was 33.6%, for
A7 it was 33.3% and for D7, agreement was 15.5%.
In general, humans agree less than 33% of the time
when transliterating English to Persian.
In addition, we examined agreement among
transliterators based on their perception of the task
difficulty shown in Table 1. For A7, agreement
among those who found the task easy was higher
(22.3%) than those who found it in medium level
</bodyText>
<equation confidence="0.934141857142857">
fi
S
E = −
Hi,
R
∑
i=1
</equation>
<page confidence="0.997303">
645
</page>
<table confidence="0.8548973">
E7 D7 A7 EDA7
Char Rules Char Rules Char Rules Char Rules
T1 23 523 23 623 28 330 31 1075
T2 22 487 25 550 29 304 32 956
T3 21 466 20 500 28 280 31 870
T4 23 497 22 524 28 307 30 956
T5 21 492 22 508 28 296 29 896
T6 24 493 21 563 25 313 29 968
T7 24 495 21 529 28 299 30 952
Mean 23 493 22 542 28 304 30 953
</table>
<tableCaption confidence="0.998827">
Table 2: Number of characters used and rules generated using SYS-2, per transliterator.
</tableCaption>
<bodyText confidence="0.999527380952381">
(18.8%). PA is 12.0% for those who found the
D7 collection hard to transliterate; while the six
transliterators who found the E7 collection difficulty
medium had PA = 30.2%. Hence, the harder par-
ticipants rated the transliteration task, the lower the
agreement scores tend to be for the derived corpus.
Finally, in Table 3 we show word accuracy results
for the two systems on corpora derived from translit-
erators grouped by perceived level of difficulty on
A7. It is readily apparent that SYS-2 outperforms
SYS-1 on the corpus comprised of human translit-
erations from people who saw the task as easy with
both word accuracy metrics; the relative improve-
ment of over 50% is statistically significant (paired
t-test on ten-fold cross validation runs). However,
on the corpus composed of transliterations that were
perceived as more difficult, “Medium”, the advan-
tage of SYS-2 is significantly eroded, but is still
statistically significant for UWA. Here again, using
only one transliteration, AIWA, did not distinguish
the performance of each system.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="discussions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999933857142857">
We have evaluated two English to Persian translit-
eration systems on a variety of controlled corpora
using evaluation metrics that appear in previous
transliteration studies. Varying the evaluation cor-
pus in a controlled fashion has revealed several in-
teresting facts.
We report that human agreement on the English
to Persian transliteration task is about 33%. The ef-
fect that this level of disagreement on the evalua-
tion of systems has, can be seen in Figure 4, where
word accuracy is computed on corpora derived from
single transliterators. Accuracy can vary by up to
30% in absolute terms depending on the translitera-
tor chosen. To our knowledge, this is the first paper
</bodyText>
<figure confidence="0.994157909090909">
0.6
T1
T2
T3
T4
T5
T6
T7
0.0
E7 D7 A7 EDA7
Corpus
</figure>
<figureCaption confidence="0.7981435">
Figure 5: Entropy of the generated segments based
on the collections created by different transliterators.
</figureCaption>
<bodyText confidence="0.999402230769231">
to report human agreement, and examine its effects
on transliteration accuracy.
In order to alleviate some of these effects on the
stability of word accuracy measures across corpora,
we recommend that at least four transliterators are
used to construct a corpus. Figure 3 shows that con-
structing a corpus with four or more transliterators,
the range of possible word accuracies achieved is
less than that of using fewer transliterators.
Some past studies do not use more than a sin-
gle target word for every source word in the cor-
pus (Bilac and Tanaka, 2005; Oh and Choi, 2006).
Our results indicate that it is unlikely that these re-
sults would translate onto a corpus other than the
one used in these studies, except in rare cases where
human transliterators are in 100% agreement for a
given language pair.
Given the nature of the English language, an En-
glish corpus can contain English words from a vari-
ety of different origins. In this study we have used
English words from an Arabic and Dutch origin to
show that word accuracy of the systems can vary by
up to 25% (in absolute terms) depending on the ori-
gin of English words in the corpus, as demonstrated
in Figure 1.
In addition to computing agreement, we also in-
</bodyText>
<figure confidence="0.988708333333333">
Entropy
0.4
0.2
</figure>
<page confidence="0.973263">
646
</page>
<table confidence="0.984556166666667">
Relative
Perception SYS-1 SYS-2 Improvement (%)
UWA Easy 33.4 55.4 54.4 (p &lt; 0.001)
Medium 44.6 48.4 8.52 (p &lt; 0.001)
MWA Easy 23.2 36.2 56.0 (p &lt; 0.001)
Medium 30.6 37.4 22.2 (p = 0.038)
</table>
<tableCaption confidence="0.9034935">
Table 3: System performance when A7 is split into sub-corpora based on transliterators perception of the
task (Easy or Medium).
</tableCaption>
<bodyText confidence="0.999826103448276">
vestigated the transliterator’s perception of difficulty
of the transliteration task with the ensuing word ac-
curacy of the systems. Interestingly, when using cor-
pora built from transliterators that perceive the task
to be easy, there is a large difference in the word
accuracy between the two systems, but on corpora
built from transliterators who perceive the task to be
more difficult, the gap between the systems narrows.
Hence, a corpus applied for evaluation of transliter-
ation should either be made carefully with translit-
erators with a variety of backgrounds, or should be
large enough and be gathered from various sources
so as to simulate different expectations of its ex-
pected non-homogeneous users.
The self entropy of rule probability distributions
derived by the automated transliteration system can
be used to measure the consistency with which in-
dividual transliterators apply their own rules in con-
structing a corpus. It was demonstrated that when
systems are evaluated on corpora built by transliter-
ators who are less consistent in their application of
transliteration rules, word accuracy is reduced.
Given the large variations in system accuracy that
are demonstrated by the varying corpora used in this
study, we recommend that extreme care be taken
when constructing corpora for evaluating translitera-
tion systems. Studies should also give details of their
corpora that would allow any of the effects observed
in this paper to be taken into account.
</bodyText>
<sectionHeader confidence="0.998831" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9903075">
This work was supported in part by the Australian
government IPRS program (SK).
</bodyText>
<sectionHeader confidence="0.998723" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922625">
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical
transliteration for English-Arabic cross-language informa-
tion retrieval. In Conference on Information and Knowledge
Management, pages 139–146.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of the ACL-
02 workshop on Computational approaches to semitic lan-
guages, pages 1–13.
Slaven Bilac and Hozumi Tanaka. 2005. Direct combination
of spelling and pronunciation information for robust back-
transliteration. In Conference on Computational Linguistics
and Intelligent Text Processing, pages 413–424.
Patrick A. V. Hall and Geoff R. Dowling. 1980. Approximate
string matching. ACM Computing Survey, 12(4):381–402.
Sung Young Jung, Sung Lim Hong, and Eunok Paek. 2000. An
English to Korean transliteration model of extended Markov
window. In Conference on Computational Linguistics, pages
383–389.
Sarvnaz Karimi, Andrew Turpin, and Falk Scholer. 2006. En-
glish to Persian transliteration. In String Processing and In-
formation Retrieval, pages 255–266.
Krister Lind´en. 2005. Multilingual modeling of cross-lingual
spelling variants. Information Retrieval, 9(3):295–310.
Eun Young Mun and Alexander Von Eye, 2004. Analyzing
Rater Agreement: Manifest Variable Methods. Lawrence
Erlbaum Associates.
Jong-Hoon Oh and Key-Sun Choi. 2006. An ensemble of
transliteration models for information retrieval. Information
Processing Management, 42(4):980–1002.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In The 40th Annual Meeting ofAssoci-
ation for Computational Linguistics, pages 311–318.
Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo, and Kalervo
J¨arvelin. 2006. FITE-TRT: a high quality translation tech-
nique for OOV words. In Proceedings of the 2006 ACM
Symposium on Applied Computing, pages 1043–1049.
Claude Elwood Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal, 27:379–
423.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of
proper names in cross-language applications. In ACMSIGIR
Conference on Research and Development on Information
Retrieval, pages 365–366.
Dmitry Zelenko and Chinatsu Aone. 2006. Discriminative
methods for transliteration. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language Process-
ing, pages 612–617.
</reference>
<page confidence="0.99819">
647
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.826188">
<title confidence="0.99967">Corpus Effects on the Evaluation of Automated Transliteration Systems</title>
<author confidence="0.999586">Sarvnaz Karimi Andrew Turpin Falk Scholer</author>
<affiliation confidence="0.999961">School of Computer Science and Information Technology</affiliation>
<address confidence="0.839921">RMIT University, GPO Box 2476V, Melbourne 3001, Australia</address>
<abstract confidence="0.999315043478261">Most current machine transliteration systems employ a corpus of known sourcetarget word pairs to train their system, and typically evaluate their systems on a similar corpus. In this paper we explore the performance of transliteration systems on corpora that are varied in a controlled way. In particular, we control the number, and prior language knowledge of human transliterators used to construct the corpora, and the origin of the source words that make up the corpora. We find that the word accuracy of automated transliteration systems can vary by up to 30% (in absolute terms) depending on the corpus on which they are run. We conclude that at least four human transliterators should be used to construct corpora for evaluating automated transliteration systems; and that although absolute word accuracy metrics may not translate across corpora, the relative rankings of system performance remains stable across differing corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nasreen AbdulJaleel</author>
<author>Leah S Larkey</author>
</authors>
<title>Statistical transliteration for English-Arabic cross-language information retrieval.</title>
<date>2003</date>
<booktitle>In Conference on Information and Knowledge Management,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="1715" citStr="AbdulJaleel and Larkey, 2003" startWordPosition="263" endWordPosition="267">ics may not translate across corpora, the relative rankings of system performance remains stable across differing corpora. 1 Introduction Machine transliteration is the process of transforming a word written in a source language into a word in a target language without the aid of a bilingual dictionary. Word pronunciation is preserved, as far as possible, but the script used to render the target word is different from that of the source language. Transliteration is applied to proper nouns and outof-vocabulary terms as part of machine translation and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Pirkola et al., 2006). Several transliteration methods are reported in the literature for a variety of languages, with their performance being evaluated on multilingual corpora. Source-target pairs are either extracted from bilingual documents or dictionaries (AbdulJaleel and Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi, 2006; Zelenko and Aone, 2006), or gathered explicitly from human transliterators (Al-Onaizan and Knight, 2002; Zelenko and Aone, 2006). Some evaluations of transliteration methods depend on a single unique transliteration for each source word, while others take multiple</context>
<context position="4047" citStr="AbdulJaleel and Larkey, 2003" startWordPosition="622" endWordPosition="625"> depending on the corpus used. Despite the wide range of absolute values 640 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 640–647, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics in performance, the ranking of our two transliteration systems was preserved on all corpora. We also find that a human’s confidence in the language from which they are transliterating can affect the corpus in such a way that word accuracy rates are altered. 2 Background Machine transliteration methods are divided into grapheme-based (AbdulJaleel and Larkey, 2003; Lind´en, 2005), phoneme-based (Jung et al., 2000; Virga and Khudanpur, 2003) and combined techniques (Bilac and Tanaka, 2005; Oh and Choi, 2006). Grapheme-based methods derive transformation rules for character combinations in the source text from a training data set, while phoneme-based methods use an intermediate phonetic transformation. In this paper, we use two grapheme-based methods for English to Persian transliteration. During a training phase, both methods derive rules for transforming character combinations (segments) in the source language into character combinations in the target </context>
</contexts>
<marker>AbdulJaleel, Larkey, 2003</marker>
<rawString>Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical transliteration for English-Arabic cross-language information retrieval. In Conference on Information and Knowledge Management, pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine transliteration of names in Arabic text.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 workshop on Computational approaches to semitic languages,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="2154" citStr="Al-Onaizan and Knight, 2002" startWordPosition="328" endWordPosition="331"> language. Transliteration is applied to proper nouns and outof-vocabulary terms as part of machine translation and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Pirkola et al., 2006). Several transliteration methods are reported in the literature for a variety of languages, with their performance being evaluated on multilingual corpora. Source-target pairs are either extracted from bilingual documents or dictionaries (AbdulJaleel and Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi, 2006; Zelenko and Aone, 2006), or gathered explicitly from human transliterators (Al-Onaizan and Knight, 2002; Zelenko and Aone, 2006). Some evaluations of transliteration methods depend on a single unique transliteration for each source word, while others take multiple target words for a single source word into account. In their work on transliterating English to Persian, Karimi et al. (2006) observed that the content of the corpus used for evaluating systems could have dramatic affects on the reported accuracy of methods. The effects of corpus composition on the evaluation of transliteration systems has not been specifically studied, with only implicit experiments or claims made in the literature s</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine transliteration of names in Arabic text. In Proceedings of the ACL02 workshop on Computational approaches to semitic languages, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Direct combination of spelling and pronunciation information for robust backtransliteration.</title>
<date>2005</date>
<booktitle>In Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>413--424</pages>
<contexts>
<context position="2030" citStr="Bilac and Tanaka, 2005" startWordPosition="309" endWordPosition="312">on is preserved, as far as possible, but the script used to render the target word is different from that of the source language. Transliteration is applied to proper nouns and outof-vocabulary terms as part of machine translation and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Pirkola et al., 2006). Several transliteration methods are reported in the literature for a variety of languages, with their performance being evaluated on multilingual corpora. Source-target pairs are either extracted from bilingual documents or dictionaries (AbdulJaleel and Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi, 2006; Zelenko and Aone, 2006), or gathered explicitly from human transliterators (Al-Onaizan and Knight, 2002; Zelenko and Aone, 2006). Some evaluations of transliteration methods depend on a single unique transliteration for each source word, while others take multiple target words for a single source word into account. In their work on transliterating English to Persian, Karimi et al. (2006) observed that the content of the corpus used for evaluating systems could have dramatic affects on the reported accuracy of methods. The effects of corpus composition on the evaluation of </context>
<context position="4173" citStr="Bilac and Tanaka, 2005" startWordPosition="641" endWordPosition="644">on of Computational Linguistics, pages 640–647, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics in performance, the ranking of our two transliteration systems was preserved on all corpora. We also find that a human’s confidence in the language from which they are transliterating can affect the corpus in such a way that word accuracy rates are altered. 2 Background Machine transliteration methods are divided into grapheme-based (AbdulJaleel and Larkey, 2003; Lind´en, 2005), phoneme-based (Jung et al., 2000; Virga and Khudanpur, 2003) and combined techniques (Bilac and Tanaka, 2005; Oh and Choi, 2006). Grapheme-based methods derive transformation rules for character combinations in the source text from a training data set, while phoneme-based methods use an intermediate phonetic transformation. In this paper, we use two grapheme-based methods for English to Persian transliteration. During a training phase, both methods derive rules for transforming character combinations (segments) in the source language into character combinations in the target language with some probability. During transliteration, the source word si is segmented and rules are chosen and applied to ea</context>
<context position="25505" citStr="Bilac and Tanaka, 2005" startWordPosition="4376" endWordPosition="4379">ts based on the collections created by different transliterators. to report human agreement, and examine its effects on transliteration accuracy. In order to alleviate some of these effects on the stability of word accuracy measures across corpora, we recommend that at least four transliterators are used to construct a corpus. Figure 3 shows that constructing a corpus with four or more transliterators, the range of possible word accuracies achieved is less than that of using fewer transliterators. Some past studies do not use more than a single target word for every source word in the corpus (Bilac and Tanaka, 2005; Oh and Choi, 2006). Our results indicate that it is unlikely that these results would translate onto a corpus other than the one used in these studies, except in rare cases where human transliterators are in 100% agreement for a given language pair. Given the nature of the English language, an English corpus can contain English words from a variety of different origins. In this study we have used English words from an Arabic and Dutch origin to show that word accuracy of the systems can vary by up to 25% (in absolute terms) depending on the origin of English words in the corpus, as demonstra</context>
</contexts>
<marker>Bilac, Tanaka, 2005</marker>
<rawString>Slaven Bilac and Hozumi Tanaka. 2005. Direct combination of spelling and pronunciation information for robust backtransliteration. In Conference on Computational Linguistics and Intelligent Text Processing, pages 413–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick A V Hall</author>
<author>Geoff R Dowling</author>
</authors>
<title>Approximate string matching.</title>
<date>1980</date>
<journal>ACM Computing Survey,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="8843" citStr="Hall and Dowling, 1980" startWordPosition="1460" endWordPosition="1464">t pop641 ular tij (labeled the Majority, with ties broken arbitrarily), or just one of the tij’s (labeled Uniform because all possible transliterations are equally rewarded). A third scheme (labeled Weighted) is also possible where the reward for tij appearing as Li1 is nij/∑di j=1 nij; here, each target word is given a weight proportional to how often a human transliterator chose that target word. Due to space considerations, we focus on the first two variants only. In general, there are two commonly used metrics for transliteration evaluation: word accuracy (WA) and character accuracy (CA) (Hall and Dowling, 1980). In all of our experiments, CA based metrics closely mirrored WA based metrics, and so conclusions drawn from the data would be the same whether WA metrics or CA metrics were used. Hence we only discuss and report WA based metrics in this paper. For each source word in the test corpus of K words, word accuracy calculates the percentage of correctly transliterated terms. Hence for the majority case, where every source word in the corpus only has one target word, the word accuracy is defined as MWA =|{si|Li1 = ti1,1 ≤ i ≤ K}|/K, and for the Uniform case, where every target variant is included w</context>
</contexts>
<marker>Hall, Dowling, 1980</marker>
<rawString>Patrick A. V. Hall and Geoff R. Dowling. 1980. Approximate string matching. ACM Computing Survey, 12(4):381–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Young Jung</author>
<author>Sung Lim Hong</author>
<author>Eunok Paek</author>
</authors>
<title>An English to Korean transliteration model of extended Markov window.</title>
<date>2000</date>
<booktitle>In Conference on Computational Linguistics,</booktitle>
<pages>383--389</pages>
<contexts>
<context position="4097" citStr="Jung et al., 2000" startWordPosition="629" endWordPosition="632">lute values 640 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 640–647, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics in performance, the ranking of our two transliteration systems was preserved on all corpora. We also find that a human’s confidence in the language from which they are transliterating can affect the corpus in such a way that word accuracy rates are altered. 2 Background Machine transliteration methods are divided into grapheme-based (AbdulJaleel and Larkey, 2003; Lind´en, 2005), phoneme-based (Jung et al., 2000; Virga and Khudanpur, 2003) and combined techniques (Bilac and Tanaka, 2005; Oh and Choi, 2006). Grapheme-based methods derive transformation rules for character combinations in the source text from a training data set, while phoneme-based methods use an intermediate phonetic transformation. In this paper, we use two grapheme-based methods for English to Persian transliteration. During a training phase, both methods derive rules for transforming character combinations (segments) in the source language into character combinations in the target language with some probability. During translitera</context>
</contexts>
<marker>Jung, Hong, Paek, 2000</marker>
<rawString>Sung Young Jung, Sung Lim Hong, and Eunok Paek. 2000. An English to Korean transliteration model of extended Markov window. In Conference on Computational Linguistics, pages 383–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarvnaz Karimi</author>
<author>Andrew Turpin</author>
<author>Falk Scholer</author>
</authors>
<title>English to Persian transliteration.</title>
<date>2006</date>
<booktitle>In String Processing and Information Retrieval,</booktitle>
<pages>255--266</pages>
<contexts>
<context position="2441" citStr="Karimi et al. (2006)" startWordPosition="373" endWordPosition="376">anguages, with their performance being evaluated on multilingual corpora. Source-target pairs are either extracted from bilingual documents or dictionaries (AbdulJaleel and Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi, 2006; Zelenko and Aone, 2006), or gathered explicitly from human transliterators (Al-Onaizan and Knight, 2002; Zelenko and Aone, 2006). Some evaluations of transliteration methods depend on a single unique transliteration for each source word, while others take multiple target words for a single source word into account. In their work on transliterating English to Persian, Karimi et al. (2006) observed that the content of the corpus used for evaluating systems could have dramatic affects on the reported accuracy of methods. The effects of corpus composition on the evaluation of transliteration systems has not been specifically studied, with only implicit experiments or claims made in the literature such as introducing the effects of different transliteration models (AbdulJaleel and Larkey, 2003), language families (Lind´en, 2005) or application based (CLIR) evaluation (Pirkola et al., 2006). In this paper, we report our experiments designed to explicitly examine the effect that var</context>
<context position="5296" citStr="Karimi et al., 2006" startWordPosition="819" endWordPosition="822">y. During transliteration, the source word si is segmented and rules are chosen and applied to each segment according to heuristics. The probability of a resulting word is the product of the probabilities of the applied rules. The result is a list of target words sorted by their associated probabilities, Li. The first system we use (SYS-1) is an n-gram approach that uses the last character of the previous source segment to condition the choice of the rule for the current source segment. This system has been shown to outperform other n-gram based methods for English to Persian transliteration (Karimi et al., 2006). The second system we employ (SYS-2) makes use of some explicit knowledge of our chosen language pair, English and Persian, and is also on the collapsed-vowel scheme presented by Karimi et al. (2006). In particular, it exploits the tendency for runs of English vowels to be collapsed into a single Persian character, or perhaps omitted from the Persian altogether. As such, segments are chosen based on surrounding consonants and vowels. The full details of this system are not important for this paper; here we focus on the performance evaluation of systems, not the systems themselves. 2.1 System </context>
</contexts>
<marker>Karimi, Turpin, Scholer, 2006</marker>
<rawString>Sarvnaz Karimi, Andrew Turpin, and Falk Scholer. 2006. English to Persian transliteration. In String Processing and Information Retrieval, pages 255–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krister Lind´en</author>
</authors>
<title>Multilingual modeling of cross-lingual spelling variants.</title>
<date>2005</date>
<journal>Information Retrieval,</journal>
<volume>9</volume>
<issue>3</issue>
<marker>Lind´en, 2005</marker>
<rawString>Krister Lind´en. 2005. Multilingual modeling of cross-lingual spelling variants. Information Retrieval, 9(3):295–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eun Young Mun</author>
<author>Alexander Von Eye</author>
</authors>
<title>Analyzing Rater Agreement: Manifest Variable Methods. Lawrence Erlbaum Associates.</title>
<date>2004</date>
<marker>Mun, Von Eye, 2004</marker>
<rawString>Eun Young Mun and Alexander Von Eye, 2004. Analyzing Rater Agreement: Manifest Variable Methods. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>An ensemble of transliteration models for information retrieval.</title>
<date>2006</date>
<journal>Information Processing Management,</journal>
<volume>42</volume>
<issue>4</issue>
<contexts>
<context position="2049" citStr="Oh and Choi, 2006" startWordPosition="313" endWordPosition="316">as possible, but the script used to render the target word is different from that of the source language. Transliteration is applied to proper nouns and outof-vocabulary terms as part of machine translation and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Pirkola et al., 2006). Several transliteration methods are reported in the literature for a variety of languages, with their performance being evaluated on multilingual corpora. Source-target pairs are either extracted from bilingual documents or dictionaries (AbdulJaleel and Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi, 2006; Zelenko and Aone, 2006), or gathered explicitly from human transliterators (Al-Onaizan and Knight, 2002; Zelenko and Aone, 2006). Some evaluations of transliteration methods depend on a single unique transliteration for each source word, while others take multiple target words for a single source word into account. In their work on transliterating English to Persian, Karimi et al. (2006) observed that the content of the corpus used for evaluating systems could have dramatic affects on the reported accuracy of methods. The effects of corpus composition on the evaluation of transliteration sys</context>
<context position="4193" citStr="Oh and Choi, 2006" startWordPosition="645" endWordPosition="648">uistics, pages 640–647, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics in performance, the ranking of our two transliteration systems was preserved on all corpora. We also find that a human’s confidence in the language from which they are transliterating can affect the corpus in such a way that word accuracy rates are altered. 2 Background Machine transliteration methods are divided into grapheme-based (AbdulJaleel and Larkey, 2003; Lind´en, 2005), phoneme-based (Jung et al., 2000; Virga and Khudanpur, 2003) and combined techniques (Bilac and Tanaka, 2005; Oh and Choi, 2006). Grapheme-based methods derive transformation rules for character combinations in the source text from a training data set, while phoneme-based methods use an intermediate phonetic transformation. In this paper, we use two grapheme-based methods for English to Persian transliteration. During a training phase, both methods derive rules for transforming character combinations (segments) in the source language into character combinations in the target language with some probability. During transliteration, the source word si is segmented and rules are chosen and applied to each segment according</context>
<context position="25525" citStr="Oh and Choi, 2006" startWordPosition="4380" endWordPosition="4383">ons created by different transliterators. to report human agreement, and examine its effects on transliteration accuracy. In order to alleviate some of these effects on the stability of word accuracy measures across corpora, we recommend that at least four transliterators are used to construct a corpus. Figure 3 shows that constructing a corpus with four or more transliterators, the range of possible word accuracies achieved is less than that of using fewer transliterators. Some past studies do not use more than a single target word for every source word in the corpus (Bilac and Tanaka, 2005; Oh and Choi, 2006). Our results indicate that it is unlikely that these results would translate onto a corpus other than the one used in these studies, except in rare cases where human transliterators are in 100% agreement for a given language pair. Given the nature of the English language, an English corpus can contain English words from a variety of different origins. In this study we have used English words from an Arabic and Dutch origin to show that word accuracy of the systems can vary by up to 25% (in absolute terms) depending on the origin of English words in the corpus, as demonstrated in Figure 1. In </context>
</contexts>
<marker>Oh, Choi, 2006</marker>
<rawString>Jong-Hoon Oh and Key-Sun Choi. 2006. An ensemble of transliteration models for information retrieval. Information Processing Management, 42(4):980–1002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In The 40th Annual Meeting ofAssociation for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="16628" citStr="Papineni et al. (2002)" startWordPosition="2835" endWordPosition="2838">ors using SYS-2, with whiskers showing extreme values. However, the changes do not follow a fixed trend across the languages. For E7, the range of accuracies achieved is high when only two or three transliterators are involved, ranging from 37.0% to 50.6% in SYS-2 method and from 33.8% to 48.0% in SYS-1 (not shown) when only two transliterators’ data are available. When more than three transliterators are used, the range of performance is noticeably smaller. Hence if at least four transliterators are used, then it is more likely that a system’s MWA will be stable. This finding is supported by Papineni et al. (2002) who recommend that four people should be used for collecting judgments for machine translation experiments. The corpora derived from A7 show consistent median increases as the number of transliterators increases, but the median accuracy is lower than for other languages. The D7 collection does not show any stable results until at least six transliterator’s are used. The results indicate that creating a collection used for the evaluation of transliteration systems, based on a “gold standard” created by only one human transliterator may lead to word accuracy results that could show a 10% absolu</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In The 40th Annual Meeting ofAssociation for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ari Pirkola</author>
<author>Jarmo Toivonen</author>
<author>Heikki Keskustalo</author>
<author>Kalervo J¨arvelin</author>
</authors>
<title>FITE-TRT: a high quality translation technique for OOV words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 ACM Symposium on Applied Computing,</booktitle>
<pages>1043--1049</pages>
<marker>Pirkola, Toivonen, Keskustalo, J¨arvelin, 2006</marker>
<rawString>Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo, and Kalervo J¨arvelin. 2006. FITE-TRT: a high quality translation technique for OOV words. In Proceedings of the 2006 ACM Symposium on Applied Computing, pages 1043–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Elwood Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>423</pages>
<contexts>
<context position="20658" citStr="Shannon, 1948" startWordPosition="3529" endWordPosition="3531">he other hand, there were many rules with approximately equal probabilities, the system may have difficulty distinguishing when to apply some rules, and not others. One way to quantify this effect is to compute the self entropy of the rule distribution for each segment in the corpus for an individual. If pij is the probability of applying rule 1 &lt; j &lt; m when confronted with source segment i, then Hi = −∑ m j=1 pij log2 pij is the entropy of the probability distribution for that rule. H is maximized when the probabilities pij are all equal, and minimized when the probabilities are very skewed (Shannon, 1948). As an example, consider the rules: t —*&lt;u,0.5 &gt;, t —*&lt;-.6,0.3 &gt; and t —*&lt;.),0.2 &gt;; for where Hi is the entropy of the rule probabilities for segment i, R is the total number of segments, fi is the frequency with which segment i occurs at any position in all source words in the corpus, and S is the sum of all fi. The expected entropy for each transliterator is shown in Figure 5, separated by corpus. Comparison of this graph with Figure 4 shows that generally transliterators that have used rules inconsistently generate a corpus that leads to low accuracy for the systems. For example, T1 who ha</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude Elwood Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379– 423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Virga</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Transliteration of proper names in cross-language applications.</title>
<date>2003</date>
<booktitle>In ACMSIGIR Conference on Research and Development on Information Retrieval,</booktitle>
<pages>365--366</pages>
<contexts>
<context position="4125" citStr="Virga and Khudanpur, 2003" startWordPosition="633" endWordPosition="636">ceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 640–647, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics in performance, the ranking of our two transliteration systems was preserved on all corpora. We also find that a human’s confidence in the language from which they are transliterating can affect the corpus in such a way that word accuracy rates are altered. 2 Background Machine transliteration methods are divided into grapheme-based (AbdulJaleel and Larkey, 2003; Lind´en, 2005), phoneme-based (Jung et al., 2000; Virga and Khudanpur, 2003) and combined techniques (Bilac and Tanaka, 2005; Oh and Choi, 2006). Grapheme-based methods derive transformation rules for character combinations in the source text from a training data set, while phoneme-based methods use an intermediate phonetic transformation. In this paper, we use two grapheme-based methods for English to Persian transliteration. During a training phase, both methods derive rules for transforming character combinations (segments) in the source language into character combinations in the target language with some probability. During transliteration, the source word si is </context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of proper names in cross-language applications. In ACMSIGIR Conference on Research and Development on Information Retrieval, pages 365–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
</authors>
<title>Discriminative methods for transliteration.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>612--617</pages>
<contexts>
<context position="2074" citStr="Zelenko and Aone, 2006" startWordPosition="317" endWordPosition="320">e script used to render the target word is different from that of the source language. Transliteration is applied to proper nouns and outof-vocabulary terms as part of machine translation and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Pirkola et al., 2006). Several transliteration methods are reported in the literature for a variety of languages, with their performance being evaluated on multilingual corpora. Source-target pairs are either extracted from bilingual documents or dictionaries (AbdulJaleel and Larkey, 2003; Bilac and Tanaka, 2005; Oh and Choi, 2006; Zelenko and Aone, 2006), or gathered explicitly from human transliterators (Al-Onaizan and Knight, 2002; Zelenko and Aone, 2006). Some evaluations of transliteration methods depend on a single unique transliteration for each source word, while others take multiple target words for a single source word into account. In their work on transliterating English to Persian, Karimi et al. (2006) observed that the content of the corpus used for evaluating systems could have dramatic affects on the reported accuracy of methods. The effects of corpus composition on the evaluation of transliteration systems has not been specifi</context>
</contexts>
<marker>Zelenko, Aone, 2006</marker>
<rawString>Dmitry Zelenko and Chinatsu Aone. 2006. Discriminative methods for transliteration. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 612–617.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>