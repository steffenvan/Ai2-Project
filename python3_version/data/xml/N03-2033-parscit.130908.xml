<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002785">
<title confidence="0.878272">
Automatically Predicting Information Quality in News Documents
</title>
<note confidence="0.907669555555555">
Rong Tang Kwong Bor Ng Tomek Strzalkowski Paul B. Kantor
School of Information Graduate School of ILS Institute School of Communication
Science and Policy Library and Information Univerity at Albany Information and Library
University at Albany Studies, Queens 1400 Washington Ave Studies
135 Western Avenue College, CUNY. Albany, NY 12222 Rutgers University
Albany, NY 12222 New York, NY 11367 tomek@albany.edu New Brunswick, NJ 08901
tangr@albany.edu kbng@qc.edu kan-
tor@scils.rutgers.e
du
</note>
<sectionHeader confidence="0.993914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999553">
We report here empirical results of a series of
studies aimed at automatically predicting in-
formation quality in news documents. Multiple
research methods and data analysis techniques
enabled a good level of machine prediction of
information quality. Procedures regarding user
experiments and statistical analysis are de-
scribed.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937888888889">
As a part of a large-scale multi-institutional project
HITIQA (High-quality Interactive Question Answer-
ing), we worked on developing an extended model for
classifying information by quality, in addition to, and as
an extension of the traditional notion of relevance. The
project involves Computer and Information Science
researchers from University at Albany and Rutgers Uni-
versity. Our serving clientele are intelligent analysts,
and the documents that we targeted were news articles.
</bodyText>
<sectionHeader confidence="0.993301" genericHeader="method">
2 Research Approach
</sectionHeader>
<bodyText confidence="0.999744">
The term “Quality” is defined by International Organi-
zation of Standards (1986) as “the totality of
characteristics of an entity that bear on its ability to
satisfy stated and implied need” (Standard 8402, 3.1).
Among numerous study on classification of information
quality, Wang and Strong (1996) proposed four dimen-
sions of qualities as detailed in Table 1: intrinsic,
contextual, representational, and accessibility.
</bodyText>
<figure confidence="0.4505144">
Categories Elements
Intrinsic IQ Accuracy, Objectivity, Believ-
ability, Reputation
Accessibility IQ Accessibility, Security
Contextual IQ Relevancy, Value-added,
Timeliness, Completeness,
Amount of Information
Representa- Interpretability, Ease of Under-
tional IQ standing, Concise Representa-
tion, Consistent Representation
</figure>
<tableCaption confidence="0.8796295">
Table 1. Information Quality Dimensions (Source:
Strong, Lee, Wang, 1997, p.39)
</tableCaption>
<bodyText confidence="0.999716769230769">
Empirical attempts to assess quality have primarily
focused on counting hyperlinks in a networked envi-
ronment. Representative studies include the work by
Amento and his colleagues (Amento, Terveen, &amp; Hills,
2000), Price and Hersh (1999), and Zhu and Gauch
(2000). However, as a whole, previous studies were
only able to produce algorithmic measures for Web
documents based on link counts and with a limited
number of quality aspects such as popularity. Our ap-
proach is to record actual users’ quality assessments of
news articles and conduct advanced statistical models of
association between users’ quality scoring and occur-
rence and prevalence of certain textual features.
</bodyText>
<sectionHeader confidence="0.994062" genericHeader="method">
3 Methodology and Results
</sectionHeader>
<bodyText confidence="0.99954169047619">
Multiple research methods were used. Firstly, we con-
ducted focus-group sessions to elicit key quality aspects
from news analysts. Secondly, we performed experts
and students quality judgment experimental sessions.
Thirdly, we identified a set of textual features, ran pro-
grams to generate counts of the features, and performed
statistical analysis to establish the correlation between
features and users’ quality ratings.
Two focus group sessions were conducted during
March and April of 2002. Participants included journal-
ism faculty members, professional editors, and a num-
ber of journalists from a local newspaper Albany Times
Union. Nine information quality criteria were consid-
ered to be salient to the context of news analysis: Accu-
racy, Source reliability, Objectivity, Depth, Author
credibility, Readability, Conciseness, Grammatically
Correctness, and Multiple Viewpoints.
A computerized quality judgment system that incor-
porated the nine quality aspects was developed. One
thousand medium-sized (100 to 2500 words) news arti-
cles were selected from the TREC collection (Voorhees,
2001) with 25 relevant documents each from five TREC
Q&amp;A topics.
We recruited expert and student participants for
judgment experiments. Expert sessions were performed
first and ten documents judged by experts were selected
and used as the training and testing material for the stu-
dent participants. The entire judgment experiment pe-
riod ran from May to August of 2002. As a result, each
of the 1,000 documents was rated twice, by two differ-
ent judges, one at Albany, and one at Rutgers.
There were high inter-judge agreements between Al-
bany and Rutgers. Figure 1 is the normality plot of the
difference between scores assigned by Rutgers’ judges
and Albany’s judges on the variable of “accuracy,” with
a mean almost equals to zero (with range from – 9 to +
9). The curves of the other eight quality variables are
similar to the one below, indicating a very insignificant
disagreement in judgments.
The second component (the upper one) consists of
“grammar”, “readability”, and “verbose and concise-
ness”. Together they explain 58% of the variance.
</bodyText>
<figure confidence="0.9908943">
a_verbos a_readab
a_gramma
a_credit
a_accura
a_source
a_object
a_multiv
a_depth
-1.0 -.5 0.0 .5 1.0
Component 1
</figure>
<figureCaption confidence="0.999446333333333">
Figure 2. PCA of Judgment data, in rotated space.
Rotation method: Oblimin with Kaiser Normaliza-
tion. Rotation converged in 5 iterations.
</figureCaption>
<bodyText confidence="0.999282375">
We recoded users’ scores 1 to 5 as low and scores 6
to 10 as high. We split the 1,000 documents into two
halves by random selection. In our training round the
first half was used to estimate the parameters that would
give best discriminant and logistic regression functions.
In our testing round, we applied the functions to the
other half to predict the quality criteria of the docu-
ments.
</bodyText>
<figure confidence="0.990100333333333">
1.0
.5
0.0
Component 2
-.5
-1.0
</figure>
<table confidence="0.953570916666667">
Discriminant Logistic
Analysis Cor- Regression
rect-Rate Correct-Rate
Accuracy 75.8% 75.9%
Source Reliability 67.8% 68.5%
Objectivity 70.6% 73.8%
Depth 77.4% 77.9%
Author Credibility 69.3% 71.7%
Readability 81.3% 83.0%
Conciseness 70.5% 70.9%
Grammar 74.9% 75.1%
Multi-view 82.1% 82.2%
</table>
<figure confidence="0.906346">
Expected Normal Value
-2
-4
-6
-8
8
6
4
2
0
-8 -6 -4 -2 0 2 4 6 8 Table 2. Performance of prediction (based on split-
Observed Value half training and testing) by two methods
</figure>
<figureCaption confidence="0.991531">
Figure 1. Normality Plot of differences in quality
judgments on the aspect of “Accuracy”
</figureCaption>
<bodyText confidence="0.985908153846154">
Principle component analysis (PCA) revealed the
same two components from Albany data as from Rut-
gers data. As shown in Figure 2, one component (the
lower one) consists of “credibility”, “source reliability”,
“accuracy”, “multi-view”, “depth”, and “objectivity.”
We then employed stepwise discriminant analysis to
select the dominant predictive variables from a range of
104 textual features. These features included elements
of punctuations, special symbols, length of document
segments, upper case, quotations, key terms, POS, and
entities. Our further analysis suggested that certain text
features are highly correlated with each of the nine as-
pects.
</bodyText>
<table confidence="0.958068">
Quality As- Textual Feature Pearson
pects correlation
(2 tails)
Accuracy Personal Pronoun 0.0002
Source Distinct organization 0.0048
Objectivity Pronoun 0.0001
Depth Document length 0.0000
Author Date unit, e.g. day, 0.0000
Credibility week
Readability Closing parenthesis 0.0099
</table>
<sectionHeader confidence="0.980209" genericHeader="method">
4 Summary
</sectionHeader>
<bodyText confidence="0.999663222222222">
In this study, we were able to identify important quality
criteria relevant to intelligent analysts’ work and we
were also able to generate automatic quality metrics of
news documents using users’ quality judgments. Our
next step is to apply our machine prediction method to
produce measures of a new set of documents and have
users to verify and modify machines’ scoring. We hope
that through this, we can collect new data to test our
quality metrics and to further improve its’ performance.
</bodyText>
<table confidence="0.996576166666667">
Conciseness Subordinating prepo- 0.0003
sition or conjunction
Multi-view Past tense verb 0.0000
Acknowledgement
Grammatical Average length of 0.0016
correctness paragraph in words
</table>
<tableCaption confidence="0.9854325">
Table 3. Highly correlated textual features and
quality aspects
</tableCaption>
<bodyText confidence="0.999859875">
At this point, we are able to produce good prediction
of several aspects of information quality, including
Depth, Objectivity, Multi-view, and Readability. The
prediction testing and training for the remaining quality
aspects are currently in progress. Tables 4 and 5 illus-
trate the results of training versus testing classification
for the criteria of “objectivity” and “depth,” with rat-
ings grouped into high and low categories.
</bodyText>
<table confidence="0.869986125">
Objectivity Predicted Group
Membership
Low High
Training Low 58.7% 41.3%
Original
Cases High 12.7% 87.3%
Testing Original Low 45.5% 54.5%
Cases High 23.5% 76.5%
</table>
<tableCaption confidence="0.741712">
Table 4. Classification result of “objectivity.”
75.5% of training cases correctly classified, 63.5%
of testing cases correctly classified
</tableCaption>
<table confidence="0.998116142857143">
Depth Predicted Group
Membership
Low High
Low 64.5% 35.5%
High 11.9% 88.1%
Low 51.0% 49.0%
High 22.6% 75.4%
</table>
<tableCaption confidence="0.99413">
Table 5. Classification result of “depth.” 74.5% of
training cases correctly classified, 61.6% of testing
cases correctly classified
</tableCaption>
<bodyText confidence="0.9928616">
This paper is based on work supported by the Advanced
Research and Development Activity (ARDA)’s Ad-
vanced Question Answering for Intelligence
(AQUAINT) Program under contract number 2002-
H790400-000.
</bodyText>
<sectionHeader confidence="0.999374" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998929178571429">
Amendo, B., Terveen, L., &amp; Hill, W. (2000). Does “au-
thority” mean quality? Predicting expert quality rat-
ings of Web documents. Proceedings of the Twenty-
Third Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, 296-303.
Price, S. L., &amp; Hersh, W. R. (1999). Filtering Web
pages for quality indicators: An empirical approach
to finding high quality consumer health information
on the World Wide Web. Proceedings of the AMIA
1999 Annual Symposium. 911-915.
Voorhees, E. (2001). Overview of TREC 2001. In E.
Voorhees (ed.) NIST Special Publication 500-250:
The Tenth Text REtrieval Conference, pp. 1 – 15.
Washington, D.C.
Strong, D., Lee, Y., &amp; Wang, R. Y. (1997). 10 potholes
in the road to information quality. IEEE Computer,
30(8), 38-46.
Wang, R. Y., &amp; Strong, D. M. (1996). Beyond accu-
racy: What data quality means to data consumers.
Journal of Management Information Systems, 12(4),
5-34.
Zhu, X., &amp; Gauch, S. (2000). Incorporating quality met-
rics in centralized/distributed information retrieval on
the World Wide Web. Proceedings of the Twenty-
Third Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, 288-295.
</reference>
<figure confidence="0.99414025">
Training Original
Cases
Testing Original
Cases
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.306082">
<title confidence="0.999646">Automatically Predicting Information Quality in News Documents</title>
<author confidence="0.999989">Rong Tang Kwong Bor Ng Tomek Strzalkowski Paul B Kantor</author>
<affiliation confidence="0.998538">School of Information Graduate School of ILS Institute School of Communication Science and Policy Library and Information Univerity at Albany Information and Library</affiliation>
<address confidence="0.750346333333333">University at Albany Studies, Queens 1400 Washington Ave Studies 135 Western Avenue College, CUNY. Albany, NY 12222 Rutgers University Albany, NY New York, NY tomek@albany.edu New Brunswick, NJ 08901 kan-tor@scils.rutgers.e du</address>
<email confidence="0.997892">tangr@albany.edukbng@qc.edu</email>
<abstract confidence="0.975674333333333">We report here empirical results of a series of studies aimed at automatically predicting information quality in news documents. Multiple research methods and data analysis techniques enabled a good level of machine prediction of information quality. Procedures regarding user experiments and statistical analysis are described.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Amendo</author>
<author>L Terveen</author>
<author>W Hill</author>
</authors>
<title>Does “authority” mean quality? Predicting expert quality ratings of Web documents.</title>
<date>2000</date>
<booktitle>Proceedings of the TwentyThird Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>296--303</pages>
<marker>Amendo, Terveen, Hill, 2000</marker>
<rawString>Amendo, B., Terveen, L., &amp; Hill, W. (2000). Does “authority” mean quality? Predicting expert quality ratings of Web documents. Proceedings of the TwentyThird Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 296-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Price</author>
<author>W R Hersh</author>
</authors>
<title>Filtering Web pages for quality indicators: An empirical approach to finding high quality consumer health information on the World Wide Web.</title>
<date>1999</date>
<booktitle>Proceedings of the AMIA 1999 Annual Symposium.</booktitle>
<pages>911--915</pages>
<contexts>
<context position="2470" citStr="Price and Hersh (1999)" startWordPosition="338" endWordPosition="341">nts Intrinsic IQ Accuracy, Objectivity, Believability, Reputation Accessibility IQ Accessibility, Security Contextual IQ Relevancy, Value-added, Timeliness, Completeness, Amount of Information Representa- Interpretability, Ease of Undertional IQ standing, Concise Representation, Consistent Representation Table 1. Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39) Empirical attempts to assess quality have primarily focused on counting hyperlinks in a networked environment. Representative studies include the work by Amento and his colleagues (Amento, Terveen, &amp; Hills, 2000), Price and Hersh (1999), and Zhu and Gauch (2000). However, as a whole, previous studies were only able to produce algorithmic measures for Web documents based on link counts and with a limited number of quality aspects such as popularity. Our approach is to record actual users’ quality assessments of news articles and conduct advanced statistical models of association between users’ quality scoring and occurrence and prevalence of certain textual features. 3 Methodology and Results Multiple research methods were used. Firstly, we conducted focus-group sessions to elicit key quality aspects from news analysts. Secon</context>
</contexts>
<marker>Price, Hersh, 1999</marker>
<rawString>Price, S. L., &amp; Hersh, W. R. (1999). Filtering Web pages for quality indicators: An empirical approach to finding high quality consumer health information on the World Wide Web. Proceedings of the AMIA 1999 Annual Symposium. 911-915.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of TREC</title>
<date>2001</date>
<booktitle>NIST Special Publication 500-250: The Tenth Text REtrieval Conference, pp. 1 – 15.</booktitle>
<editor>In E. Voorhees (ed.)</editor>
<location>Washington, D.C.</location>
<contexts>
<context position="4029" citStr="Voorhees, 2001" startWordPosition="569" endWordPosition="570">April of 2002. Participants included journalism faculty members, professional editors, and a number of journalists from a local newspaper Albany Times Union. Nine information quality criteria were considered to be salient to the context of news analysis: Accuracy, Source reliability, Objectivity, Depth, Author credibility, Readability, Conciseness, Grammatically Correctness, and Multiple Viewpoints. A computerized quality judgment system that incorporated the nine quality aspects was developed. One thousand medium-sized (100 to 2500 words) news articles were selected from the TREC collection (Voorhees, 2001) with 25 relevant documents each from five TREC Q&amp;A topics. We recruited expert and student participants for judgment experiments. Expert sessions were performed first and ten documents judged by experts were selected and used as the training and testing material for the student participants. The entire judgment experiment period ran from May to August of 2002. As a result, each of the 1,000 documents was rated twice, by two different judges, one at Albany, and one at Rutgers. There were high inter-judge agreements between Albany and Rutgers. Figure 1 is the normality plot of the difference be</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Voorhees, E. (2001). Overview of TREC 2001. In E. Voorhees (ed.) NIST Special Publication 500-250: The Tenth Text REtrieval Conference, pp. 1 – 15. Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Strong</author>
<author>Y Lee</author>
<author>R Y Wang</author>
</authors>
<title>10 potholes in the road to information quality.</title>
<date>1997</date>
<journal>IEEE Computer,</journal>
<volume>30</volume>
<issue>8</issue>
<pages>38--46</pages>
<marker>Strong, Lee, Wang, 1997</marker>
<rawString>Strong, D., Lee, Y., &amp; Wang, R. Y. (1997). 10 potholes in the road to information quality. IEEE Computer, 30(8), 38-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Y Wang</author>
<author>D M Strong</author>
</authors>
<title>Beyond accuracy: What data quality means to data consumers.</title>
<date>1996</date>
<journal>Journal of Management Information Systems,</journal>
<volume>12</volume>
<issue>4</issue>
<pages>5--34</pages>
<contexts>
<context position="1709" citStr="Wang and Strong (1996)" startWordPosition="241" endWordPosition="244"> quality, in addition to, and as an extension of the traditional notion of relevance. The project involves Computer and Information Science researchers from University at Albany and Rutgers University. Our serving clientele are intelligent analysts, and the documents that we targeted were news articles. 2 Research Approach The term “Quality” is defined by International Organization of Standards (1986) as “the totality of characteristics of an entity that bear on its ability to satisfy stated and implied need” (Standard 8402, 3.1). Among numerous study on classification of information quality, Wang and Strong (1996) proposed four dimensions of qualities as detailed in Table 1: intrinsic, contextual, representational, and accessibility. Categories Elements Intrinsic IQ Accuracy, Objectivity, Believability, Reputation Accessibility IQ Accessibility, Security Contextual IQ Relevancy, Value-added, Timeliness, Completeness, Amount of Information Representa- Interpretability, Ease of Undertional IQ standing, Concise Representation, Consistent Representation Table 1. Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39) Empirical attempts to assess quality have primarily focused on counting hyp</context>
</contexts>
<marker>Wang, Strong, 1996</marker>
<rawString>Wang, R. Y., &amp; Strong, D. M. (1996). Beyond accuracy: What data quality means to data consumers. Journal of Management Information Systems, 12(4), 5-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>S Gauch</author>
</authors>
<title>Incorporating quality metrics in centralized/distributed information retrieval on the World Wide Web.</title>
<date>2000</date>
<booktitle>Proceedings of the TwentyThird Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>288--295</pages>
<contexts>
<context position="2496" citStr="Zhu and Gauch (2000)" startWordPosition="343" endWordPosition="346">bjectivity, Believability, Reputation Accessibility IQ Accessibility, Security Contextual IQ Relevancy, Value-added, Timeliness, Completeness, Amount of Information Representa- Interpretability, Ease of Undertional IQ standing, Concise Representation, Consistent Representation Table 1. Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39) Empirical attempts to assess quality have primarily focused on counting hyperlinks in a networked environment. Representative studies include the work by Amento and his colleagues (Amento, Terveen, &amp; Hills, 2000), Price and Hersh (1999), and Zhu and Gauch (2000). However, as a whole, previous studies were only able to produce algorithmic measures for Web documents based on link counts and with a limited number of quality aspects such as popularity. Our approach is to record actual users’ quality assessments of news articles and conduct advanced statistical models of association between users’ quality scoring and occurrence and prevalence of certain textual features. 3 Methodology and Results Multiple research methods were used. Firstly, we conducted focus-group sessions to elicit key quality aspects from news analysts. Secondly, we performed experts </context>
</contexts>
<marker>Zhu, Gauch, 2000</marker>
<rawString>Zhu, X., &amp; Gauch, S. (2000). Incorporating quality metrics in centralized/distributed information retrieval on the World Wide Web. Proceedings of the TwentyThird Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 288-295.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>