<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.125343">
<title confidence="0.9822675">
Contrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback for
Rapport
</title>
<author confidence="0.998139">
Siwei Wang
</author>
<affiliation confidence="0.998674">
Department of Psychology
University of Chicago
</affiliation>
<address confidence="0.557673">
Chicago, IL 60637 USA
</address>
<email confidence="0.998792">
siweiw@cs.uchicago.edu
</email>
<author confidence="0.983195">
Gina-Anne Levow
</author>
<affiliation confidence="0.997184">
Department of Linguistics
University of Washington
</affiliation>
<address confidence="0.878529">
Seattle, WA 98195 USA
</address>
<email confidence="0.999396">
levow@uw.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988830625">
Verbal feedback is an important information
source in establishing interactional rapport.
However, predicting verbal feedback across
languages is challenging due to language-
specific differences, inter-speaker variation,
and the relative sparseness and optionality of
verbal feedback. In this paper, we employ an
approach combining classifier weighting and
SMOTE algorithm oversampling to improve
verbal feedback prediction in Arabic, English,
and Spanish dyadic conversations. This ap-
proach improves the prediction of verbal feed-
back, up to 6-fold, while maintaining a high
overall accuracy. Analyzing highly weighted
features highlights widespread use of pitch,
with more varied use of intensity and duration.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859428571429">
Culture-specific aspects of speech and nonverbal be-
havior enable creation and maintenance of a sense of
rapport. Rapport is important because it is known to
enhance goal-directed interactions and also to pro-
mote learning. Previous work has identified cross-
cultural differences in a variety of behaviors, for
example, nodding (Maynard, 1990), facial expres-
sion (Matsumoto et al., 2005), gaze (Watson, 1970),
cues to vocal back-channel (Ward and Tsukuhara,
2000; Ward and Al Bayyari, 2007; Rivera and
Ward, 2007), nonverbal back-channel (Bertrand et
al., 2007)), and coverbal gesturing (Kendon, 2004).
Here we focus on the automatic prediction of lis-
tener verbal feedback in dyadic unrehearsed story-
telling to elucidate the similarities and differences
in three language/cultural groups: Iraqi Arabic-,
Mexican Spanish-, and American English-speaking
cultures. (Tickle-Degnen and Rosenthal, 1990)
identified coordination, along with positive emo-
tion and mutual attention, as a key element of in-
teractional rapport. In the verbal channel, this co-
ordination manifests in the timing of contributions
from the conversational participants, through turn-
taking and back-channels. (Duncan, 1972) pro-
posed an analysis of turn-taking as rule-governed,
supported by a range of prosodic and non-verbal
cues. Several computational approaches have inves-
tigated prosodic and verbal cues to these phenom-
ena. (Shriberg et al., 2001) found that prosodic cues
could aid in the identification of jump-in points in
multi-party meetings. (Cathcart et al., 2003) em-
ployed features such as pause duration and part-of-
speech (POS) tag sequences for back-channel pre-
diction. (Gravano and Hirschberg, 2009) investi-
gated back-channel-inviting cues in task-oriented di-
alog, identifying increases in pitch and intensity as
well as certain POS patterns as key contributors. In
multi-lingual comparisons, (Ward and Tsukuhara,
2000; Ward and Al Bayyari, 2007; Rivera and Ward,
2007) found pitch patterns, including periods of low
pitch or drops in pitch, to be associated with elic-
iting back-channels across Japanese, English, Ara-
bic, and Spanish. (Herrera et al., 2010) collected a
corpus of multi-party interactions among American
English, Mexican Spanish, and Arabic speakers to
investigate cross-cultural differences in proxemics,
gaze, and turn-taking. (Levow et al., 2010) identi-
fied contrasts in narrative length and rate of verbal
feedback in recordings of American English-, Mexi-
</bodyText>
<page confidence="0.977339">
614
</page>
<note confidence="0.608459">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 614–619,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.98498">
can Spanish-, and Iraqi Arabic-speaking dyads. This
work also identified reductions in pitch and intensity
associated with instances of verbal feedback as com-
mon, but not uniform, across these groups.
ings by exploring automatic recognition of speaker
prosodic contexts associated with listener verbal
feedback.
</bodyText>
<sectionHeader confidence="0.918445" genericHeader="introduction">
2 Multi-modal Rapport Corpus
</sectionHeader>
<bodyText confidence="0.999890804878049">
To enable a more controlled comparison of listener
behavior, we collected a multi-modal dyadic corpus
of unrehearsed story-telling. We audio- and video-
recorded pairs of individuals who were close ac-
quaintances or family members with, we assumed,
well-established rapport. One participant viewed a
six minute film, the “Pear Film” (Chafe, 1975), de-
veloped for language-independent elicitation. In the
role of Speaker, this participant then related the story
to the active and engaged Listener, who understood
that they would need to retell the story themselves
later. We have collected 114 elicitations: 45 Arabic,
32 Mexican Spanish, and 37 American English.
All recordings have been fully transcribed and
time-aligned to the audio using a semi-automated
procedure. We convert an initial manual coarse tran-
scription at the phrase level to a full word and phone
alignment using CUSonic (Pellom et al., 2001), ap-
plying its language porting functionality to Spanish
and Arabic. In addition, word and phrase level En-
glish glosses were manually created for the Span-
ish and Arabic data. Manual annotation of a broad
range of nonverbal cues, including gaze, blink, head
nod and tilt, fidget, and coverbal gestures, is under-
way. For the experiments presented in the remainder
of this paper, we employ a set of 45 vetted dyads, 15
in each language.
Analysis of cross-cultural differences in narrative
length, rate of listener verbal contributions, and the
use of pitch and intensity in eliciting listener vocal-
izations appears in (Levow et al., 2010). That work
found that the American English-speaking dyads
produced significantly longer narratives than the
other language/cultural groups, while Arabic listen-
ers provided a significantly higher rate of verbal con-
tributions than those in the other groups. Finally, all
three groups exhibited significantly lower speaker
pitch preceding listener verbal feedback than in
other contexts, while only English and Spanish ex-
hibited significant reductions in intensity. The cur-
rent paper aims to extend and enhance these find-
</bodyText>
<sectionHeader confidence="0.924416" genericHeader="method">
3 Challenges in Predicting Verbal
Feedback
</sectionHeader>
<bodyText confidence="0.999911193548387">
Predicting verbal feedback in dyadic rapport in di-
verse language/cultural groups presents a number of
challenges. In addition to the cross-linguistic, cross-
cultural differences which are the focus of our study,
it is also clear that there are substantial inter-speaker
differences in verbal feedback, both in frequency
and, we expect, in signalling. Furthermore, while
the rate of verbal feedback differs across language
and speaker, it is, overall, a relatively infrequent
phenomenon, occurring in as little as zero percent
of pausal intervals for some dyads and only at an av-
erage of 13-30% of pausal intervals across the three
languages. As a result, the substantial class imbal-
ance and relative sparsity of listener verbal feedback
present challenges for data-driven machine learn-
ing methods. Finally, as prior researchers have ob-
served, provision of verbal feedback can be viewed
as optional. The presence of feedback, we assume,
indicates the presence of a suitable context; the ab-
sence of feedback, however, does not guarantee that
feedback would have been inappropriate, only that
the conversant did not provide it.
We address each of these issues in our experi-
mental process. We employ a leave-one-dyad-out
cross-validation framework that allows us to deter-
mine overall accuracy while highlighting the differ-
ent characteristics of the dyads. We employ and
evaluate both an oversampling technique (Chawla
et al., 2002) and class weighting to compensate for
class imbalance. Finally, we tune our classification
for the recognition of the feedback class.
</bodyText>
<sectionHeader confidence="0.998119" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<bodyText confidence="0.999661875">
We define a Speaker pausal region as an interval in
the Speaker’s channel annotated with a contiguous
span of silence and/or non-speech sounds. These
Speaker pausal regions are tagged as ’Feedback
(FB)’ if the participant in the Listener role initi-
ates verbal feedback during that interval and as ’No
Feedback (NoFB)’ if the Listener does not. We aim
to characterize and automatically classify each such
</bodyText>
<page confidence="0.991205">
615
</page>
<table confidence="0.996161">
Arabic English Spanish
0.30 (0.21) 0.152 (0.10) 0.136 (0.12)
</table>
<tableCaption confidence="0.995654">
Table 1: Mean and standard deviation of proportion of
pausal regions associated with listener verbal feedback
</tableCaption>
<bodyText confidence="0.999953833333333">
region. We group the dyads by language/cultural
group to contrast the prosodic characteristics of the
speech that elicit listener feedback and to assess the
effectiveness of these prosodic cues for classifica-
tion. The proportion of regions with listener feed-
back for each language appears in Table 1.
</bodyText>
<subsectionHeader confidence="0.991792">
4.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999939333333333">
For each Speaker pausal region, we extract fea-
tures from the Speaker’s words immediately preced-
ing and following the non-speech interval, as well
as computing differences between some of these
measures. We extract a set of 39 prosodic fea-
tures motivated by (Shriberg et al., 2001), using
Praat’s (Boersma, 2001) “To Pitch...” and “To In-
tensity...”. All durational measures and word posi-
tions are based on the semi-automatic alignment de-
scribed above. All measures are log-scaled and z-
score normalized per speaker. The full feature set
appears in Table 2.
</bodyText>
<subsectionHeader confidence="0.986047">
4.2 Classification and Analysis
</subsectionHeader>
<bodyText confidence="0.999742458333333">
For classification, we employ Support Vector Ma-
chines (SVM), using the LibSVM implementation
(C-C.Cheng and Lin, 2001) with an RBF kernel. For
each language/cultural group, we perform ’leave-
one-dyad-out’ cross-validation based on F-measure
as implemented in that toolkit. For each fold, train-
ing on 14 dyads and testing on the last, we determine
not only accuracy but also the weight-based ranking
of each feature described above.
Managing Class Imbalance Since listener verbal
feedback occurs in only 14-30% of candidate posi-
tions, classification often predicts only the majority
’NoFB’ class. To compensate for this imbalance, we
apply two strategies: reweighting and oversampling.
We explore increasing the weight on the minority
class in the classifier by a factor of two or four. We
also apply SMOTE (Chawla et al., 2002) oversam-
pling to double or quadruple the number of minority
class training instances. SMOTE oversampling cre-
ates new synthetic minority class instances by iden-
tifying k = 3 nearest neighbors and inducing a new
instance by taking the difference between a sample
and its neighbor, multiplying by a factor between 0
and 1, and adding that value to the original instance.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999967368421053">
Table 4 presents the classification accuracy for dis-
tinguishing FB and NoFB contexts. We present the
overall class distribution for each language. We then
contrast the minority FB class and overall accuracy
under each of three weighting and oversampling set-
tings. The second row has no weighting or over-
sampling; the third has no weighting with quadru-
ple oversampling on all folds, a setting in which the
largest number of Arabic dyads achieves their best
performance. The last row indicates the oracle per-
formance when the best weighting and oversampling
setting is chosen for each fold.
We find that the use of reweighting and over-
sampling dramatically improves the recognition of
the minority class, with only small reductions in
overall accuracy of 3-7%. Under a uniform set-
ting of quadruple oversampling and no reweight-
ing, the number of correctly recognized Arabic and
English FB samples nearly triples, while the num-
ber of Spanish FB samples doubles. We further
see that if we can dynamically select the optimal
training settings, we can achieve even greater im-
provements. Here the number of correctly recog-
nized FB examples increases between 3- (Spanish)
and 6-fold (Arabic) with only a reduction of 1-4%
in overall accuracy. These accuracy levels corre-
spond to recognizing between 38% (English, Span-
ish) and 73% (Arabic) of the FB instances. Even un-
der these tuned conditions, the sparseness and vari-
ability of the English and Spanish data continue to
present challenges.
Finally, Table 3 illustrates the impact of the full
range of reweighting and oversampling conditions.
Each cell indicates the number of folds in each of
Arabic, English, and Spanish respectively, for which
that training condition yields the highest accuracy.
We can see that the different dyads achieve optimal
results under a wide range of training conditions.
</bodyText>
<page confidence="0.997829">
616
</page>
<table confidence="0.999755571428571">
Feature Type Description Feature IDs
Pitch 5 uniform points across word pre 0,pre 0.25,pre 0.5,pre 0.75,pre 1
post 0,post 0.25,post 0.5,post 0.75,post 1
Maximum, minimum, mean pre pmax, pre pmin, pre pmean
Differences in max, min, mean post pmax, post pmin, post pmean
diff pmax, diff pmin, diff pmean
Difference b/t boundaries diff pitch endbeg
Start and end slope pre bslope, pre eslope, post bslope, post eslope
Difference b/t slopes diff slope endbeg
Intensity Maximum, minimum, mean pre imax, pre imin, pre imean
Difference in maxima post imax,post imin, post imean
diff imax
Duration Last rhyme, last vowel, pause pre rdur, pre vdur, post rdur, post vdur, pause dur
Voice Quality Doubling &amp; halving pre doub, pre half,post doub,post half
</table>
<tableCaption confidence="0.993713">
Table 2: Prosodic features for classification and analysis. Features tagged ’pre’ are extracted from the word immedi-
ately preceding the Speaker pausal region; those tagged ’post’ are extracted from the word immediatey following.
</tableCaption>
<table confidence="0.999935333333333">
Arabic English Spanish
Overall 478 (1405) 395 (2659) 173 (1226)
Baseline 53 (950) 23 (2167) 23 (1066)
S=2, W=1 145 (878) 67 (2120) 47 (1023)
Oracle 347 (918) 152 (2033) 68 (1059)
weight 1 2 4
no SMOTE 1,2,3 2,2,2 1,0,3
SMOTE Double 1,0,2 1,2,0 2,2,1
SMOTE Quad 3,0,0 1,2,2 3,6,2
</table>
<tableCaption confidence="0.9927195">
Table 3: Varying SVM weight and SMOTE ratio. Each
cell shows # dyads in each language (Arabic, English,
Spanish) with their best performance with this setting.
Table 4: Row 1: Class distribution: # FB instances (#
total instances). Rows 2-4: Recognition under different
settings: # FB correctly recognized (total # correct)
</tableCaption>
<sectionHeader confidence="0.964637" genericHeader="method">
6 Discussion: Feature Analysis
</sectionHeader>
<bodyText confidence="0.999991722222222">
To investigate the cross-language variation in
speaker cues eliciting listener verbal feedback, we
conduct a feature analysis. Table 5 presents the
features with highest average weight for each lan-
guage assigned by the classifier across folds, as well
as those distinctive features highly ranked for only
one language.
We find that the Arabic dyads make extensive
and distinctive use of pitch in cuing verbal feed-
back, from both preceding and following words,
while placing little weight on other feature types.
In contrast, both English and Spanish dyads exploit
both pitch and intensity features from surrounding
words. Spanish alone makes significant use of both
vocalic and pause duration. We also observe that, al-
though there is substantial variation in feature rank-
ing across speakers, the highly ranked features are
robustly employed across almost all folds.
</bodyText>
<sectionHeader confidence="0.993771" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999870571428571">
Because of the size of our data set, it may be pre-
mature to draw firm conclusion about differences
between these three language groups based on this
analysis. The SVM weighting and SMOTE over-
sampling strategy discussed here is promising for
improving recognition on imbalanced class data.
This strategy substantially improves the prediction
</bodyText>
<page confidence="0.994684">
617
</page>
<table confidence="0.999750333333333">
Most Important Features
Arabic English Spanish
pre pmax pre pmean pre min
pre pmean post pmean post 0.5
pre 0.25 post 0.5 post 0.75
pre 0.5 post 0.75 post 1
pre 0.75 post 1 pre imax
pre 1 diff pmin pre imean
post pmin pre imax post imax
post bslope pre imean pause dur
diff pmin post imean pre vdur
Most Distinctive Features
Arabic English Spanish
post pmin post pmean post 0
post bslope post 0.25 post eslope
pre 0.25 pre eslope
pre 0.5 post vdur
pre 1 pre imean
</table>
<tableCaption confidence="0.957663">
Table 5: Highest ranked and distinctive features for each
language/cultural group
</tableCaption>
<bodyText confidence="0.999922928571429">
of verbal feedback. The resulting feature ranking
also provides insight into the contrasts in the use of
prosodic cues among these language cultural groups,
while highlighting the widespread, robust use of
pitch features.
In future research, we would like to extend our
work to exploit sequential learning frameworks to
predict verbal feedback. We also plan to explore the
fusion of multi-modal features to enhance recogni-
tion and increase our understanding of multi-modal
rapport behavior. We will also work to analyze how
quickly people can establish rapport, as the short du-
ration of our Spanish dyads poses substantial chal-
lenges.
</bodyText>
<sectionHeader confidence="0.998262" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9976095">
We would like to thank our team of annota-
tor/analysts for their efforts in creating this corpus,
and Danial Parvaz for the development of the Arabic
transliteration tool. We are grateful for the insights
of Susan Duncan, David McNeill, and Dan Loehr.
This work was supported by NSF BCS#: 0729515.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.991402" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997986595744681">
R. Bertrand, G. Ferre, P. Blache, R. Espesser, and
S. Rauzy. 2007. Backchannels revisited from a mul-
timodal perspective. In Auditory-visual Speech Pro-
cessing, The Netherlands. Hilvarenbeek.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9–10):341–345.
C-C.Cheng and C-J. Lin. 2001. LIBSVM:a library
for support vector machines. Software available at:
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
N. Cathcart, J. Carletta, and E. Klein. 2003. A shallow
model of backchannel continuers in spoken dialogue.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Linguis-
tics - Volume 1, pages 51–58.
W. Chafe. 1975. The Pear Film.
Nitesh Chawla, Kevin Bowyer, Lawrence O. Hall, and
W. Philip Legelmeyer. 2002. SMOTE: Synthetic mi-
nority over-sampling technique. Journal of Artificial
Intelligence Research, 16:321–357.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283–292.
A. Gravano and J. Hirschberg. 2009. Backchannel-
inviting cues in task-oriented dialogue. In Proceedings
of Interspeech 2009, pages 1019–1022.
David Herrera, David Novick, Dusan Jan, and David
Traum. 2010. The UTEP-ICT cross-cultural mul-
tiparty multimodal dialog corpus. In Proceedings of
the Multimodal Corpora Workshop: Advances in Cap-
turing, Coding and Analyzing Multimodality (MMC
2010).
A. Kendon. 2004. Gesture: Visible Action as Utterance.
Cambridge University Press.
G.-A. Levow, S. Duncan, and E. King. 2010. Cross-
cultural investigation of prosody in verbal feedback in
interactional rapport. In Proceedings of Interspeech
2010.
D. Matsumoto, S. H. Yoo, S. Hirayama, and G. Petrova.
2005. Validation of an individual-level measure of
display rules: The display rule assessment inventory
(DRAI). Emotion, 5:23–40.
S. Maynard. 1990. Conversation management in con-
trast: listener response in Japanese and American En-
glish. Journal of Pragmatics, 14:397–412.
B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang,
X. Yu, and S. Pradhan. 2001. University of Colorado
dialog systems for travel and navigation.
</reference>
<page confidence="0.977666">
618
</page>
<reference confidence="0.999854761904762">
A. Rivera and N. Ward. 2007. Three prosodic features
that cue back-channel in Northern Mexican Span-
ish. Technical Report UTEP-CS-07-12, University of
Texas, El Paso.
E. Shriberg, A. Stolcke, and D. Baron. 2001. Can
prosody aid the automatic processing of multi-party
meetings? evidence from predicting punctuation, dis-
fluencies, and overlapping speech. In Proc. of ISCA
Tutorial and Research Workshop on Prosody in Speech
Recognition and Understanding.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
nature of rapport and its nonverbal correlates. Psycho-
logical Inquiry, 1(4):285–293.
N. Ward and Y. Al Bayyari. 2007. A prosodic feature
that invites back-channels in Egyptian Arabic. Per-
spectives in Arabic Linguistics XX.
N. Ward and W. Tsukuhara. 2000. Prosodic fea-
tures which cue back-channel responses in English and
Japanese. Journal of Pragmatics, 32(8):1177–1207.
O. M. Watson. 1970. Proxemic Behavior: A Cross-
cultural Study. Mouton, The Hague.
</reference>
<page confidence="0.998663">
619
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636975">
<title confidence="0.971424">Contrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback for Rapport</title>
<author confidence="0.946971">Siwei</author>
<affiliation confidence="0.9995035">Department of University of</affiliation>
<address confidence="0.973562">Chicago, IL 60637</address>
<email confidence="0.999667">siweiw@cs.uchicago.edu</email>
<author confidence="0.74932">Gina-Anne</author>
<affiliation confidence="0.9993205">Department of University of</affiliation>
<address confidence="0.999864">Seattle, WA 98195</address>
<email confidence="0.999813">levow@uw.edu</email>
<abstract confidence="0.998016588235294">Verbal feedback is an important information source in establishing interactional rapport. However, predicting verbal feedback across languages is challenging due to languagespecific differences, inter-speaker variation, and the relative sparseness and optionality of verbal feedback. In this paper, we employ an approach combining classifier weighting and SMOTE algorithm oversampling to improve verbal feedback prediction in Arabic, English, and Spanish dyadic conversations. This approach improves the prediction of verbal feedback, up to 6-fold, while maintaining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bertrand</author>
<author>G Ferre</author>
<author>P Blache</author>
<author>R Espesser</author>
<author>S Rauzy</author>
</authors>
<title>Backchannels revisited from a multimodal perspective. In Auditory-visual Speech Processing, The Netherlands.</title>
<date>2007</date>
<location>Hilvarenbeek.</location>
<contexts>
<context position="1577" citStr="Bertrand et al., 2007" startWordPosition="216" endWordPosition="219"> with more varied use of intensity and duration. 1 Introduction Culture-specific aspects of speech and nonverbal behavior enable creation and maintenance of a sense of rapport. Rapport is important because it is known to enhance goal-directed interactions and also to promote learning. Previous work has identified crosscultural differences in a variety of behaviors, for example, nodding (Maynard, 1990), facial expression (Matsumoto et al., 2005), gaze (Watson, 1970), cues to vocal back-channel (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007), nonverbal back-channel (Bertrand et al., 2007)), and coverbal gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key element of interactional rapport. In the verbal channel, this coordination manifests in the timing of contributions from the conversational participants, through turntaking and ba</context>
</contexts>
<marker>Bertrand, Ferre, Blache, Espesser, Rauzy, 2007</marker>
<rawString>R. Bertrand, G. Ferre, P. Blache, R. Espesser, and S. Rauzy. 2007. Backchannels revisited from a multimodal perspective. In Auditory-visual Speech Processing, The Netherlands. Hilvarenbeek.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boersma</author>
</authors>
<title>Praat, a system for doing phonetics by computer.</title>
<date>2001</date>
<journal>Glot International,</journal>
<pages>5--9</pages>
<contexts>
<context position="8918" citStr="Boersma, 2001" startWordPosition="1323" endWordPosition="1324">dyads by language/cultural group to contrast the prosodic characteristics of the speech that elicit listener feedback and to assess the effectiveness of these prosodic cues for classification. The proportion of regions with listener feedback for each language appears in Table 1. 4.1 Feature Extraction For each Speaker pausal region, we extract features from the Speaker’s words immediately preceding and following the non-speech interval, as well as computing differences between some of these measures. We extract a set of 39 prosodic features motivated by (Shriberg et al., 2001), using Praat’s (Boersma, 2001) “To Pitch...” and “To Intensity...”. All durational measures and word positions are based on the semi-automatic alignment described above. All measures are log-scaled and zscore normalized per speaker. The full feature set appears in Table 2. 4.2 Classification and Analysis For classification, we employ Support Vector Machines (SVM), using the LibSVM implementation (C-C.Cheng and Lin, 2001) with an RBF kernel. For each language/cultural group, we perform ’leaveone-dyad-out’ cross-validation based on F-measure as implemented in that toolkit. For each fold, training on 14 dyads and testing on t</context>
</contexts>
<marker>Boersma, 2001</marker>
<rawString>P. Boersma. 2001. Praat, a system for doing phonetics by computer. Glot International, 5(9–10):341–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Cheng</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM:a library for support vector machines. Software available at: http://www.csie.ntu.edu.tw/ cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="9312" citStr="Cheng and Lin, 2001" startWordPosition="1382" endWordPosition="1385">ely preceding and following the non-speech interval, as well as computing differences between some of these measures. We extract a set of 39 prosodic features motivated by (Shriberg et al., 2001), using Praat’s (Boersma, 2001) “To Pitch...” and “To Intensity...”. All durational measures and word positions are based on the semi-automatic alignment described above. All measures are log-scaled and zscore normalized per speaker. The full feature set appears in Table 2. 4.2 Classification and Analysis For classification, we employ Support Vector Machines (SVM), using the LibSVM implementation (C-C.Cheng and Lin, 2001) with an RBF kernel. For each language/cultural group, we perform ’leaveone-dyad-out’ cross-validation based on F-measure as implemented in that toolkit. For each fold, training on 14 dyads and testing on the last, we determine not only accuracy but also the weight-based ranking of each feature described above. Managing Class Imbalance Since listener verbal feedback occurs in only 14-30% of candidate positions, classification often predicts only the majority ’NoFB’ class. To compensate for this imbalance, we apply two strategies: reweighting and oversampling. We explore increasing the weight o</context>
</contexts>
<marker>Cheng, Lin, 2001</marker>
<rawString>C-C.Cheng and C-J. Lin. 2001. LIBSVM:a library for support vector machines. Software available at: http://www.csie.ntu.edu.tw/ cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cathcart</author>
<author>J Carletta</author>
<author>E Klein</author>
</authors>
<title>A shallow model of backchannel continuers in spoken dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>51--58</pages>
<contexts>
<context position="2556" citStr="Cathcart et al., 2003" startWordPosition="356" endWordPosition="359">ion, along with positive emotion and mutual attention, as a key element of interactional rapport. In the verbal channel, this coordination manifests in the timing of contributions from the conversational participants, through turntaking and back-channels. (Duncan, 1972) proposed an analysis of turn-taking as rule-governed, supported by a range of prosodic and non-verbal cues. Several computational approaches have investigated prosodic and verbal cues to these phenomena. (Shriberg et al., 2001) found that prosodic cues could aid in the identification of jump-in points in multi-party meetings. (Cathcart et al., 2003) employed features such as pause duration and part-ofspeech (POS) tag sequences for back-channel prediction. (Gravano and Hirschberg, 2009) investigated back-channel-inviting cues in task-oriented dialog, identifying increases in pitch and intensity as well as certain POS patterns as key contributors. In multi-lingual comparisons, (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007) found pitch patterns, including periods of low pitch or drops in pitch, to be associated with eliciting back-channels across Japanese, English, Arabic, and Spanish. (Herrera et al., 2010) co</context>
</contexts>
<marker>Cathcart, Carletta, Klein, 2003</marker>
<rawString>N. Cathcart, J. Carletta, and E. Klein. 2003. A shallow model of backchannel continuers in spoken dialogue. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume 1, pages 51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chafe</author>
</authors>
<title>The Pear Film. Nitesh Chawla,</title>
<date>1975</date>
<location>Kevin Bowyer, Lawrence</location>
<contexts>
<context position="4360" citStr="Chafe, 1975" startWordPosition="616" endWordPosition="617"> reductions in pitch and intensity associated with instances of verbal feedback as common, but not uniform, across these groups. ings by exploring automatic recognition of speaker prosodic contexts associated with listener verbal feedback. 2 Multi-modal Rapport Corpus To enable a more controlled comparison of listener behavior, we collected a multi-modal dyadic corpus of unrehearsed story-telling. We audio- and videorecorded pairs of individuals who were close acquaintances or family members with, we assumed, well-established rapport. One participant viewed a six minute film, the “Pear Film” (Chafe, 1975), developed for language-independent elicitation. In the role of Speaker, this participant then related the story to the active and engaged Listener, who understood that they would need to retell the story themselves later. We have collected 114 elicitations: 45 Arabic, 32 Mexican Spanish, and 37 American English. All recordings have been fully transcribed and time-aligned to the audio using a semi-automated procedure. We convert an initial manual coarse transcription at the phrase level to a full word and phone alignment using CUSonic (Pellom et al., 2001), applying its language porting funct</context>
</contexts>
<marker>Chafe, 1975</marker>
<rawString>W. Chafe. 1975. The Pear Film. Nitesh Chawla, Kevin Bowyer, Lawrence O. Hall, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Philip Legelmeyer</author>
</authors>
<title>SMOTE: Synthetic minority over-sampling technique.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>16--321</pages>
<marker>Legelmeyer, 2002</marker>
<rawString>W. Philip Legelmeyer. 2002. SMOTE: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Duncan</author>
</authors>
<title>Some signals and rules for taking speaking turns in conversations.</title>
<date>1972</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="2204" citStr="Duncan, 1972" startWordPosition="304" endWordPosition="305">l gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key element of interactional rapport. In the verbal channel, this coordination manifests in the timing of contributions from the conversational participants, through turntaking and back-channels. (Duncan, 1972) proposed an analysis of turn-taking as rule-governed, supported by a range of prosodic and non-verbal cues. Several computational approaches have investigated prosodic and verbal cues to these phenomena. (Shriberg et al., 2001) found that prosodic cues could aid in the identification of jump-in points in multi-party meetings. (Cathcart et al., 2003) employed features such as pause duration and part-ofspeech (POS) tag sequences for back-channel prediction. (Gravano and Hirschberg, 2009) investigated back-channel-inviting cues in task-oriented dialog, identifying increases in pitch and intensit</context>
</contexts>
<marker>Duncan, 1972</marker>
<rawString>S. Duncan. 1972. Some signals and rules for taking speaking turns in conversations. Journal of Personality and Social Psychology, 23(2):283–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gravano</author>
<author>J Hirschberg</author>
</authors>
<title>Backchannelinviting cues in task-oriented dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>1019--1022</pages>
<contexts>
<context position="2695" citStr="Gravano and Hirschberg, 2009" startWordPosition="377" endWordPosition="380">nation manifests in the timing of contributions from the conversational participants, through turntaking and back-channels. (Duncan, 1972) proposed an analysis of turn-taking as rule-governed, supported by a range of prosodic and non-verbal cues. Several computational approaches have investigated prosodic and verbal cues to these phenomena. (Shriberg et al., 2001) found that prosodic cues could aid in the identification of jump-in points in multi-party meetings. (Cathcart et al., 2003) employed features such as pause duration and part-ofspeech (POS) tag sequences for back-channel prediction. (Gravano and Hirschberg, 2009) investigated back-channel-inviting cues in task-oriented dialog, identifying increases in pitch and intensity as well as certain POS patterns as key contributors. In multi-lingual comparisons, (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007) found pitch patterns, including periods of low pitch or drops in pitch, to be associated with eliciting back-channels across Japanese, English, Arabic, and Spanish. (Herrera et al., 2010) collected a corpus of multi-party interactions among American English, Mexican Spanish, and Arabic speakers to investigate cross-cultural dif</context>
</contexts>
<marker>Gravano, Hirschberg, 2009</marker>
<rawString>A. Gravano and J. Hirschberg. 2009. Backchannelinviting cues in task-oriented dialogue. In Proceedings of Interspeech 2009, pages 1019–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Herrera</author>
<author>David Novick</author>
<author>Dusan Jan</author>
<author>David Traum</author>
</authors>
<title>The UTEP-ICT cross-cultural multiparty multimodal dialog corpus.</title>
<date>2010</date>
<booktitle>In Proceedings of the Multimodal Corpora Workshop: Advances in Capturing, Coding and Analyzing Multimodality (MMC</booktitle>
<contexts>
<context position="3153" citStr="Herrera et al., 2010" startWordPosition="446" endWordPosition="449"> (Cathcart et al., 2003) employed features such as pause duration and part-ofspeech (POS) tag sequences for back-channel prediction. (Gravano and Hirschberg, 2009) investigated back-channel-inviting cues in task-oriented dialog, identifying increases in pitch and intensity as well as certain POS patterns as key contributors. In multi-lingual comparisons, (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007) found pitch patterns, including periods of low pitch or drops in pitch, to be associated with eliciting back-channels across Japanese, English, Arabic, and Spanish. (Herrera et al., 2010) collected a corpus of multi-party interactions among American English, Mexican Spanish, and Arabic speakers to investigate cross-cultural differences in proxemics, gaze, and turn-taking. (Levow et al., 2010) identified contrasts in narrative length and rate of verbal feedback in recordings of American English-, Mexi614 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 614–619, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics can Spanish-, and Iraqi Arabic-speaking dyads. This work also identified redu</context>
</contexts>
<marker>Herrera, Novick, Jan, Traum, 2010</marker>
<rawString>David Herrera, David Novick, Dusan Jan, and David Traum. 2010. The UTEP-ICT cross-cultural multiparty multimodal dialog corpus. In Proceedings of the Multimodal Corpora Workshop: Advances in Capturing, Coding and Analyzing Multimodality (MMC 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Gesture: Visible Action as Utterance.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1617" citStr="Kendon, 2004" startWordPosition="223" endWordPosition="224">1 Introduction Culture-specific aspects of speech and nonverbal behavior enable creation and maintenance of a sense of rapport. Rapport is important because it is known to enhance goal-directed interactions and also to promote learning. Previous work has identified crosscultural differences in a variety of behaviors, for example, nodding (Maynard, 1990), facial expression (Matsumoto et al., 2005), gaze (Watson, 1970), cues to vocal back-channel (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007), nonverbal back-channel (Bertrand et al., 2007)), and coverbal gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key element of interactional rapport. In the verbal channel, this coordination manifests in the timing of contributions from the conversational participants, through turntaking and back-channels. (Duncan, 1972) proposed an </context>
</contexts>
<marker>Kendon, 2004</marker>
<rawString>A. Kendon. 2004. Gesture: Visible Action as Utterance. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-A Levow</author>
<author>S Duncan</author>
<author>E King</author>
</authors>
<title>Crosscultural investigation of prosody in verbal feedback in interactional rapport.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<contexts>
<context position="3361" citStr="Levow et al., 2010" startWordPosition="473" endWordPosition="476">k-oriented dialog, identifying increases in pitch and intensity as well as certain POS patterns as key contributors. In multi-lingual comparisons, (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007) found pitch patterns, including periods of low pitch or drops in pitch, to be associated with eliciting back-channels across Japanese, English, Arabic, and Spanish. (Herrera et al., 2010) collected a corpus of multi-party interactions among American English, Mexican Spanish, and Arabic speakers to investigate cross-cultural differences in proxemics, gaze, and turn-taking. (Levow et al., 2010) identified contrasts in narrative length and rate of verbal feedback in recordings of American English-, Mexi614 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 614–619, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics can Spanish-, and Iraqi Arabic-speaking dyads. This work also identified reductions in pitch and intensity associated with instances of verbal feedback as common, but not uniform, across these groups. ings by exploring automatic recognition of speaker prosodic contexts associated with</context>
<context position="5558" citStr="Levow et al., 2010" startWordPosition="806" endWordPosition="809">anguage porting functionality to Spanish and Arabic. In addition, word and phrase level English glosses were manually created for the Spanish and Arabic data. Manual annotation of a broad range of nonverbal cues, including gaze, blink, head nod and tilt, fidget, and coverbal gestures, is underway. For the experiments presented in the remainder of this paper, we employ a set of 45 vetted dyads, 15 in each language. Analysis of cross-cultural differences in narrative length, rate of listener verbal contributions, and the use of pitch and intensity in eliciting listener vocalizations appears in (Levow et al., 2010). That work found that the American English-speaking dyads produced significantly longer narratives than the other language/cultural groups, while Arabic listeners provided a significantly higher rate of verbal contributions than those in the other groups. Finally, all three groups exhibited significantly lower speaker pitch preceding listener verbal feedback than in other contexts, while only English and Spanish exhibited significant reductions in intensity. The current paper aims to extend and enhance these find3 Challenges in Predicting Verbal Feedback Predicting verbal feedback in dyadic r</context>
</contexts>
<marker>Levow, Duncan, King, 2010</marker>
<rawString>G.-A. Levow, S. Duncan, and E. King. 2010. Crosscultural investigation of prosody in verbal feedback in interactional rapport. In Proceedings of Interspeech 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Matsumoto</author>
<author>S H Yoo</author>
<author>S Hirayama</author>
<author>G Petrova</author>
</authors>
<title>Validation of an individual-level measure of display rules: The display rule assessment inventory (DRAI).</title>
<date>2005</date>
<journal>Emotion,</journal>
<pages>5--23</pages>
<contexts>
<context position="1403" citStr="Matsumoto et al., 2005" startWordPosition="190" endWordPosition="193">ach improves the prediction of verbal feedback, up to 6-fold, while maintaining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration. 1 Introduction Culture-specific aspects of speech and nonverbal behavior enable creation and maintenance of a sense of rapport. Rapport is important because it is known to enhance goal-directed interactions and also to promote learning. Previous work has identified crosscultural differences in a variety of behaviors, for example, nodding (Maynard, 1990), facial expression (Matsumoto et al., 2005), gaze (Watson, 1970), cues to vocal back-channel (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007), nonverbal back-channel (Bertrand et al., 2007)), and coverbal gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key eleme</context>
</contexts>
<marker>Matsumoto, Yoo, Hirayama, Petrova, 2005</marker>
<rawString>D. Matsumoto, S. H. Yoo, S. Hirayama, and G. Petrova. 2005. Validation of an individual-level measure of display rules: The display rule assessment inventory (DRAI). Emotion, 5:23–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maynard</author>
</authors>
<title>Conversation management in contrast: listener response in Japanese and American English.</title>
<date>1990</date>
<journal>Journal of Pragmatics,</journal>
<pages>14--397</pages>
<contexts>
<context position="1359" citStr="Maynard, 1990" startWordPosition="185" endWordPosition="186">sh dyadic conversations. This approach improves the prediction of verbal feedback, up to 6-fold, while maintaining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration. 1 Introduction Culture-specific aspects of speech and nonverbal behavior enable creation and maintenance of a sense of rapport. Rapport is important because it is known to enhance goal-directed interactions and also to promote learning. Previous work has identified crosscultural differences in a variety of behaviors, for example, nodding (Maynard, 1990), facial expression (Matsumoto et al., 2005), gaze (Watson, 1970), cues to vocal back-channel (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007), nonverbal back-channel (Bertrand et al., 2007)), and coverbal gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive </context>
</contexts>
<marker>Maynard, 1990</marker>
<rawString>S. Maynard. 1990. Conversation management in contrast: listener response in Japanese and American English. Journal of Pragmatics, 14:397–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pellom</author>
<author>W Ward</author>
<author>J Hansen</author>
<author>K Hacioglu</author>
<author>J Zhang</author>
<author>X Yu</author>
<author>S Pradhan</author>
</authors>
<title>University of Colorado dialog systems for travel and navigation.</title>
<date>2001</date>
<contexts>
<context position="4923" citStr="Pellom et al., 2001" startWordPosition="702" endWordPosition="705">t viewed a six minute film, the “Pear Film” (Chafe, 1975), developed for language-independent elicitation. In the role of Speaker, this participant then related the story to the active and engaged Listener, who understood that they would need to retell the story themselves later. We have collected 114 elicitations: 45 Arabic, 32 Mexican Spanish, and 37 American English. All recordings have been fully transcribed and time-aligned to the audio using a semi-automated procedure. We convert an initial manual coarse transcription at the phrase level to a full word and phone alignment using CUSonic (Pellom et al., 2001), applying its language porting functionality to Spanish and Arabic. In addition, word and phrase level English glosses were manually created for the Spanish and Arabic data. Manual annotation of a broad range of nonverbal cues, including gaze, blink, head nod and tilt, fidget, and coverbal gestures, is underway. For the experiments presented in the remainder of this paper, we employ a set of 45 vetted dyads, 15 in each language. Analysis of cross-cultural differences in narrative length, rate of listener verbal contributions, and the use of pitch and intensity in eliciting listener vocalizati</context>
</contexts>
<marker>Pellom, Ward, Hansen, Hacioglu, Zhang, Yu, Pradhan, 2001</marker>
<rawString>B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang, X. Yu, and S. Pradhan. 2001. University of Colorado dialog systems for travel and navigation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rivera</author>
<author>N Ward</author>
</authors>
<title>Three prosodic features that cue back-channel in Northern Mexican Spanish.</title>
<date>2007</date>
<tech>Technical Report UTEP-CS-07-12,</tech>
<institution>University of Texas, El Paso.</institution>
<contexts>
<context position="1529" citStr="Rivera and Ward, 2007" startWordPosition="210" endWordPosition="213">ted features highlights widespread use of pitch, with more varied use of intensity and duration. 1 Introduction Culture-specific aspects of speech and nonverbal behavior enable creation and maintenance of a sense of rapport. Rapport is important because it is known to enhance goal-directed interactions and also to promote learning. Previous work has identified crosscultural differences in a variety of behaviors, for example, nodding (Maynard, 1990), facial expression (Matsumoto et al., 2005), gaze (Watson, 1970), cues to vocal back-channel (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007), nonverbal back-channel (Bertrand et al., 2007)), and coverbal gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key element of interactional rapport. In the verbal channel, this coordination manifests in the timing of contributions from the conver</context>
<context position="2965" citStr="Rivera and Ward, 2007" startWordPosition="416" endWordPosition="419">es have investigated prosodic and verbal cues to these phenomena. (Shriberg et al., 2001) found that prosodic cues could aid in the identification of jump-in points in multi-party meetings. (Cathcart et al., 2003) employed features such as pause duration and part-ofspeech (POS) tag sequences for back-channel prediction. (Gravano and Hirschberg, 2009) investigated back-channel-inviting cues in task-oriented dialog, identifying increases in pitch and intensity as well as certain POS patterns as key contributors. In multi-lingual comparisons, (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007) found pitch patterns, including periods of low pitch or drops in pitch, to be associated with eliciting back-channels across Japanese, English, Arabic, and Spanish. (Herrera et al., 2010) collected a corpus of multi-party interactions among American English, Mexican Spanish, and Arabic speakers to investigate cross-cultural differences in proxemics, gaze, and turn-taking. (Levow et al., 2010) identified contrasts in narrative length and rate of verbal feedback in recordings of American English-, Mexi614 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:sh</context>
</contexts>
<marker>Rivera, Ward, 2007</marker>
<rawString>A. Rivera and N. Ward. 2007. Three prosodic features that cue back-channel in Northern Mexican Spanish. Technical Report UTEP-CS-07-12, University of Texas, El Paso.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Baron</author>
</authors>
<title>Can prosody aid the automatic processing of multi-party meetings? evidence from predicting punctuation, disfluencies, and overlapping speech.</title>
<date>2001</date>
<booktitle>In Proc. of ISCA Tutorial and Research Workshop on Prosody in Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="2432" citStr="Shriberg et al., 2001" startWordPosition="337" endWordPosition="340"> Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key element of interactional rapport. In the verbal channel, this coordination manifests in the timing of contributions from the conversational participants, through turntaking and back-channels. (Duncan, 1972) proposed an analysis of turn-taking as rule-governed, supported by a range of prosodic and non-verbal cues. Several computational approaches have investigated prosodic and verbal cues to these phenomena. (Shriberg et al., 2001) found that prosodic cues could aid in the identification of jump-in points in multi-party meetings. (Cathcart et al., 2003) employed features such as pause duration and part-ofspeech (POS) tag sequences for back-channel prediction. (Gravano and Hirschberg, 2009) investigated back-channel-inviting cues in task-oriented dialog, identifying increases in pitch and intensity as well as certain POS patterns as key contributors. In multi-lingual comparisons, (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007) found pitch patterns, including periods of low pitch or drops in p</context>
<context position="8887" citStr="Shriberg et al., 2001" startWordPosition="1317" endWordPosition="1320">r verbal feedback region. We group the dyads by language/cultural group to contrast the prosodic characteristics of the speech that elicit listener feedback and to assess the effectiveness of these prosodic cues for classification. The proportion of regions with listener feedback for each language appears in Table 1. 4.1 Feature Extraction For each Speaker pausal region, we extract features from the Speaker’s words immediately preceding and following the non-speech interval, as well as computing differences between some of these measures. We extract a set of 39 prosodic features motivated by (Shriberg et al., 2001), using Praat’s (Boersma, 2001) “To Pitch...” and “To Intensity...”. All durational measures and word positions are based on the semi-automatic alignment described above. All measures are log-scaled and zscore normalized per speaker. The full feature set appears in Table 2. 4.2 Classification and Analysis For classification, we employ Support Vector Machines (SVM), using the LibSVM implementation (C-C.Cheng and Lin, 2001) with an RBF kernel. For each language/cultural group, we perform ’leaveone-dyad-out’ cross-validation based on F-measure as implemented in that toolkit. For each fold, traini</context>
</contexts>
<marker>Shriberg, Stolcke, Baron, 2001</marker>
<rawString>E. Shriberg, A. Stolcke, and D. Baron. 2001. Can prosody aid the automatic processing of multi-party meetings? evidence from predicting punctuation, disfluencies, and overlapping speech. In Proc. of ISCA Tutorial and Research Workshop on Prosody in Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linda Tickle-Degnen</author>
<author>Robert Rosenthal</author>
</authors>
<title>The nature of rapport and its nonverbal correlates.</title>
<date>1990</date>
<journal>Psychological Inquiry,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="1913" citStr="Tickle-Degnen and Rosenthal, 1990" startWordPosition="260" endWordPosition="263">ural differences in a variety of behaviors, for example, nodding (Maynard, 1990), facial expression (Matsumoto et al., 2005), gaze (Watson, 1970), cues to vocal back-channel (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007), nonverbal back-channel (Bertrand et al., 2007)), and coverbal gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key element of interactional rapport. In the verbal channel, this coordination manifests in the timing of contributions from the conversational participants, through turntaking and back-channels. (Duncan, 1972) proposed an analysis of turn-taking as rule-governed, supported by a range of prosodic and non-verbal cues. Several computational approaches have investigated prosodic and verbal cues to these phenomena. (Shriberg et al., 2001) found that prosodic cues could aid in the identification of jump-in points in mu</context>
</contexts>
<marker>Tickle-Degnen, Rosenthal, 1990</marker>
<rawString>Linda Tickle-Degnen and Robert Rosenthal. 1990. The nature of rapport and its nonverbal correlates. Psychological Inquiry, 1(4):285–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>Y Al Bayyari</author>
</authors>
<title>A prosodic feature that invites back-channels in Egyptian Arabic. Perspectives in Arabic Linguistics XX.</title>
<date>2007</date>
<marker>Ward, Bayyari, 2007</marker>
<rawString>N. Ward and Y. Al Bayyari. 2007. A prosodic feature that invites back-channels in Egyptian Arabic. Perspectives in Arabic Linguistics XX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>W Tsukuhara</author>
</authors>
<title>Prosodic features which cue back-channel responses in English and Japanese.</title>
<date>2000</date>
<journal>Journal of Pragmatics,</journal>
<volume>32</volume>
<issue>8</issue>
<contexts>
<context position="1478" citStr="Ward and Tsukuhara, 2000" startWordPosition="201" endWordPosition="204">ining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration. 1 Introduction Culture-specific aspects of speech and nonverbal behavior enable creation and maintenance of a sense of rapport. Rapport is important because it is known to enhance goal-directed interactions and also to promote learning. Previous work has identified crosscultural differences in a variety of behaviors, for example, nodding (Maynard, 1990), facial expression (Matsumoto et al., 2005), gaze (Watson, 1970), cues to vocal back-channel (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007), nonverbal back-channel (Bertrand et al., 2007)), and coverbal gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key element of interactional rapport. In the verbal channel, this coordination manif</context>
<context position="2914" citStr="Ward and Tsukuhara, 2000" startWordPosition="407" endWordPosition="410">c and non-verbal cues. Several computational approaches have investigated prosodic and verbal cues to these phenomena. (Shriberg et al., 2001) found that prosodic cues could aid in the identification of jump-in points in multi-party meetings. (Cathcart et al., 2003) employed features such as pause duration and part-ofspeech (POS) tag sequences for back-channel prediction. (Gravano and Hirschberg, 2009) investigated back-channel-inviting cues in task-oriented dialog, identifying increases in pitch and intensity as well as certain POS patterns as key contributors. In multi-lingual comparisons, (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007) found pitch patterns, including periods of low pitch or drops in pitch, to be associated with eliciting back-channels across Japanese, English, Arabic, and Spanish. (Herrera et al., 2010) collected a corpus of multi-party interactions among American English, Mexican Spanish, and Arabic speakers to investigate cross-cultural differences in proxemics, gaze, and turn-taking. (Levow et al., 2010) identified contrasts in narrative length and rate of verbal feedback in recordings of American English-, Mexi614 Proceedings of the 49th Annual Meeting </context>
</contexts>
<marker>Ward, Tsukuhara, 2000</marker>
<rawString>N. Ward and W. Tsukuhara. 2000. Prosodic features which cue back-channel responses in English and Japanese. Journal of Pragmatics, 32(8):1177–1207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O M Watson</author>
</authors>
<title>Proxemic Behavior: A Crosscultural Study.</title>
<date>1970</date>
<location>Mouton, The Hague.</location>
<contexts>
<context position="1424" citStr="Watson, 1970" startWordPosition="195" endWordPosition="196">verbal feedback, up to 6-fold, while maintaining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration. 1 Introduction Culture-specific aspects of speech and nonverbal behavior enable creation and maintenance of a sense of rapport. Rapport is important because it is known to enhance goal-directed interactions and also to promote learning. Previous work has identified crosscultural differences in a variety of behaviors, for example, nodding (Maynard, 1990), facial expression (Matsumoto et al., 2005), gaze (Watson, 1970), cues to vocal back-channel (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007), nonverbal back-channel (Bertrand et al., 2007)), and coverbal gesturing (Kendon, 2004). Here we focus on the automatic prediction of listener verbal feedback in dyadic unrehearsed storytelling to elucidate the similarities and differences in three language/cultural groups: Iraqi Arabic-, Mexican Spanish-, and American English-speaking cultures. (Tickle-Degnen and Rosenthal, 1990) identified coordination, along with positive emotion and mutual attention, as a key element of interactional r</context>
</contexts>
<marker>Watson, 1970</marker>
<rawString>O. M. Watson. 1970. Proxemic Behavior: A Crosscultural Study. Mouton, The Hague.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>