<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001595">
<title confidence="0.985086">
Learning language through pictures
</title>
<author confidence="0.674908">
Grzegorz Chrupała ´Akos K´ad´ar Afra Alishahi
</author>
<email confidence="0.643732">
g.chrupala@uvt.nl a.kadar@uvt.nl a.alishahi@uvt.nl
</email>
<affiliation confidence="0.840838">
Tilburg Center for Cognition and Communication
Tilburg University
</affiliation>
<sectionHeader confidence="0.97565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999825">
We propose IMAGINET, a model of learn-
ing visually grounded representations of
language from coupled textual and visual
input. The model consists of two Gated
Recurrent Unit networks with shared word
embeddings, and uses a multi-task objec-
tive by receiving a textual description of
a scene and trying to concurrently predict
its visual representation and the next word
in the sentence. Mimicking an important
aspect of human language learning, it ac-
quires meaning representations for indi-
vidual words from descriptions of visual
scenes. Moreover, it learns to effectively
use sequential structure in semantic inter-
pretation of multi-word phrases.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999993403846154">
Vision is the most important sense for humans
and visual sensory input plays an important role
in language acquisition by grounding meanings of
words and phrases in perception. Similarly, in
practical applications processing multimodal data
where text is accompanied by images or videos is
increasingly important. In this paper we propose
a novel model of learning visually-grounded rep-
resentations of language from paired textual and
visual input. The model learns language through
comprehension and production, by receiving a tex-
tual description of a scene and trying to “imagine”
a visual representation of it, while predicting the
next word at the same time.
The full model, which we dub IMAGINET, con-
sists of two Gated Recurrent Unit (GRU) networks
coupled via shared word embeddings. IMAGINET
uses a multi-task Caruana (1997) objective: both
networks read the sentence word-by-word in par-
allel; one of them predicts the feature represen-
tation of the image depicting the described scene
after reading the whole sentence, while the other
one predicts the next word at each position in the
word sequence. The importance of the visual and
textual objectives can be traded off, and either of
them can be switched off entirely, enabling us to
investigate the impact of visual vs textual infor-
mation on the learned language representations.
Our approach to modeling human language
learning has connections to recent models of im-
age captioning (see Section 2). Unlike in many of
these models, in IMAGINET the image is the target
to predict rather then the input, and the model can
build a visually-grounded representation of a sen-
tence independently of an image. We can directly
compare the performance of IMAGINET against a
simple multivariate linear regression model with
bag-of-words features and thus quantify the con-
tribution of the added expressive power of a recur-
rent neural network.
We evaluate our model’s knowledge of word
meaning and sentence structure through simulat-
ing human judgments of word similarity, retriev-
ing images corresponding to single words as well
as full sentences, and retrieving paraphrases of im-
age captions. In all these tasks the model outper-
forms the baseline; the model significantly corre-
lates with human ratings of word similarity, and
predicts appropriate visual interpretations of sin-
gle and multi-word phrases. The acquired knowl-
edge of sentence structure boosts the model’s per-
formance in both image and caption retrieval.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.995946625">
Several computational models have been proposed
to study early language acquisition. The acqui-
sition of word meaning has been mainly mod-
eled using connectionist networks that learn to
associate word forms with semantic or percep-
tual features (e.g., Li et al., 2004; Coventry et al.,
2005; Regier, 2005), and rule-based or proba-
bilistic implementations which use statistical reg-
</bodyText>
<page confidence="0.919926">
112
</page>
<bodyText confidence="0.973900294117647">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112–118,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
ularities observed in the input to detect associa-
tions between linguistic labels and visual features
or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly
et al., 2010). These models either use toy lan-
guages as input (e.g., Siskind, 1996), or child-
directed utterances from the CHILDES database
(MacWhinney, 2014) paired with artificially gen-
erated semantic information. Some models have
investigated the acquisition of terminology for vi-
sual concepts from simple videos (Fleischman
and Roy, 2005; Skocaj et al., 2011). Lazaridou
et al. (2015) adapt the skip-gram word-embedding
model (Mikolov et al., 2013) for learning word
representations via a multi-task objective similar
to ours, learning from a dataset where some words
are individually aligned with corresponding im-
ages. All these models ignore sentence structure
and treat inputs as bags of words.
A few models have looked at the concurrent ac-
quisition of words and some aspect of sentence
structure, such as lexical categories (Alishahi and
Chrupala, 2012) or syntactic properties (Howell
et al., 2005; Kwiatkowski et al., 2012), from utter-
ances paired with an artificially generated repre-
sentation of their meaning. To our knowledge, no
existing model has been proposed for concurrent
learning of grounded word meanings and sentence
structure from large scale data and realistic visual
input.
Recently, the engineering task of generating
captions for images has received a lot of atten-
tion (Karpathy and Fei-Fei, 2014; Mao et al.,
2014; Kiros et al., 2014; Donahue et al., 2014;
Vinyals et al., 2014; Venugopalan et al., 2014;
Chen and Zitnick, 2014; Fang et al., 2014). From
the point of view of modeling, the research most
relevant to our interests is that of Chen and Zitnick
(2014). They develop a model based on a context-
dependent recurrent neural network (Mikolov and
Zweig, 2012) which simultaneously processes tex-
tual and visual input and updates two parallel hid-
den states. Unlike theirs, our model receives the
visual target only at the end of the sentence and is
thus encouraged to store in the final hidden state
of the visual pathway all aspects of the sentence
needed to predict the image features successfully.
Our setup is more suitable for the goal of learning
representations of complete sentences.
</bodyText>
<sectionHeader confidence="0.985478" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.563922">
IMAGINET consists of two parallel recurrent path-
</bodyText>
<figureCaption confidence="0.999681">
Figure 1: Structure of IMAGINET
</figureCaption>
<bodyText confidence="0.994974777777778">
ways coupled via shared word embeddings. Both
pathways are composed of Gated Recurrent Units
(GRU) first introduced by Cho et al. (2014) and
Chung et al. (2014). GRUs are related to the
Long Short-Term Memory units (Hochreiter and
Schmidhuber, 1997), but do not employ a sepa-
rate memory cell. In a GRU, activation at time t is
the linear combination of previous activation, and
candidate activation:
</bodyText>
<equation confidence="0.988087">
ht = (1 − zt) � ht−1 + zt � ˜ht (1)
</equation>
<bodyText confidence="0.998194666666667">
where � is elementwise multiplication. The up-
date gate determines how much the activation is
updated:
</bodyText>
<equation confidence="0.9669536">
zt = as(Wzxt + Uzht−1) (2)
The candidate activation is computed as:
˜ht = a(Wxt + U(rt � ht−1)) (3)
The reset gate is defined as:
rt = as(Wrxt + Urht−1) (4)
</equation>
<bodyText confidence="0.963712">
Our gated recurrent units use steep sigmoids for
gate activations:
</bodyText>
<equation confidence="0.998641">
1
as(z) =
1 + exp(−3.75z)
</equation>
<bodyText confidence="0.999194">
and rectified linear units clipped between 0 and 5
for the unit activations:
</bodyText>
<equation confidence="0.997838">
a(z) = clip(0.5(z + abs(z)), 0, 5)
</equation>
<bodyText confidence="0.995367428571429">
Figure 1 illustrates the structure of the network.
The word embeddings is a matrix of learned pa-
rameters We with each column corresponding to a
vector for a particular word. The input word sym-
bol St of sentence S at each step t indexes into the
embeddings matrix and the vector xt forms input
to both GRU networks:
</bodyText>
<equation confidence="0.874529">
xt = We[:, St] (5)
</equation>
<page confidence="0.984402">
113
</page>
<bodyText confidence="0.999765">
This input is mapped into two parallel hidden
states, hVt along the visual pathway, and hTt along
the textual pathway:
</bodyText>
<equation confidence="0.999291">
hVt = GRUV (hVt−1, xt) (6)
hTt = GRUT (hTt−1, xt) (7)
</equation>
<bodyText confidence="0.99988175">
The final hidden state along the visual pathway hVτ
is then mapped to the predicted target image rep-
resentation ˆi by the fully connected layer with pa-
rameters V and the clipped rectifier activation:
</bodyText>
<equation confidence="0.995419">
ˆi = �(VhVτ ) (8)
</equation>
<bodyText confidence="0.999673333333333">
Each hidden state along the textual pathway hTt is
used to predict the next symbol in the sentence S
via a softmax layer with parameters L:
</bodyText>
<equation confidence="0.999485">
p(St+1|S1:t) = softmax(LhTt ) (9)
</equation>
<bodyText confidence="0.9990468">
The loss function whose gradient is backpropa-
gated through time to the GRUs and the embed-
dings is a composite objective with terms penaliz-
ing error on the visual and the textual targets si-
multaneously:
</bodyText>
<equation confidence="0.997917">
L(B) = αLT (B) + (1 − α)LV (B) (10)
</equation>
<bodyText confidence="0.914474">
where B is the set of all IMAGINET parameters. LT
is the cross entropy function:
</bodyText>
<equation confidence="0.9971115">
log p(St|S1:t) (11)
while LV is the mean squared error:
K (ˆZk − Zk)2 (12)
LV (B) = 1 1
K
k=1
</equation>
<bodyText confidence="0.999316444444444">
By setting α to 0 we can switch the whole textual
pathway off and obtain the VISUAL model vari-
ant. Analogously, setting α to 1 gives the TEX-
TUAL model. Intermediate values of α (in the ex-
periments below we use 0.1) give the full MUL-
TITASK version. Finally, as baseline for some of
the tasks we use a simple linear regression model
LINREG with a bag-of-words representation of the
sentence:
</bodyText>
<equation confidence="0.992698">
ˆi = Ax + b (13)
</equation>
<bodyText confidence="0.9982092">
where ˆi is the vector of the predicted image fea-
tures, x is the vector of word counts for the in-
put sentence and (A, b) the parameters of the
linear model estimated via L2-penalized sum-of-
squared-errors loss.
</bodyText>
<table confidence="0.999258">
SimLex MEN 3K
VISUAL 0.32 0.57
MULTITASK 0.39 0.63
TEXTUAL 0.31 0.53
LINREG 0.18 0.23
</table>
<tableCaption confidence="0.703846666666667">
Table 1: Word similarity correlations with human
judgments measured by Spearman’s p (all correla-
tions are significant at level p &lt; 0.01).
</tableCaption>
<sectionHeader confidence="0.99865" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9997582">
Settings The model was implemented in Theano
(Bastien et al., 2012; Bergstra et al., 2010) and op-
timized by Adam (Kingma and Ba, 2014).1 The
fixed 4096-dimensional target image representa-
tion come from the pre-softmax layer of the 16-
layer CNN (Simonyan and Zisserman, 2014). We
used 1024 dimensions for the embeddings and for
the hidden states of each of the GRU networks. We
ran 8 iterations of training, and we report either
full learning curves, or the results for each model
after iteration 7 (where they performed best for the
image retrieval task). For training we use the stan-
dard MS-COCO training data. For validation and
test, we take a sample of 5000 images each from
the validation data.
</bodyText>
<subsectionHeader confidence="0.995877">
4.1 Word representations
</subsectionHeader>
<bodyText confidence="0.99995119047619">
We assess the quality of the learned embeddings
for single words via two tasks: (i) we measure
similarity between embeddings of word pairs and
compare them to elicited human ratings; (ii) we
examine how well the model learns visual repre-
sentations of words by projecting word embed-
dings into the visual space, and retrieving images
of single concepts from ImageNet.
Word similarity judgment For similarity judg-
ment correlations, we selected two existing bench-
marks that have the largest vocabulary overlap
with our data: MEN 3K (Bruni et al., 2014) and
SimLex-999 (Hill et al., 2014). We measure the
similarity between word pairs by computing the
cosine similarity between their embeddings from
three versions of our model, VISUAL, MULTI-
TASK and TEXTUAL, and the baseline LINREG.
Table 1 summarizes the results. All IMAGINET
models significantly correlate with human simi-
larity judgments, and outperform LINREG. Ex-
amples of word pairs for which MULTITASK cap-
</bodyText>
<equation confidence="0.6601218">
1Code available at github.com/gchrupala/imaginet.
LT (B) = −1
Ir
�τ
t=1
</equation>
<page confidence="0.909593">
114
</page>
<table confidence="0.8184145">
VISUAL MULTITASK LINREG
0.38 0.38 0.33
</table>
<tableCaption confidence="0.9928835">
Table 2: Accuracy@5 of retrieving images with
compatible labels from ImageNet.
</tableCaption>
<bodyText confidence="0.998238266666667">
tures human similarity judgments better than VI-
SUAL include antonyms (dusk, dawn), colloca-
tions (sexy, smile), or related but not visually sim-
ilar words (college, exhibition).
Single-word image retrieval In order to visual-
ize the acquired meaning for individual words, we
use images from the ILSVRC2012 subset of Im-
ageNet (Russakovsky et al., 2014) as benchmark.
Labels of the images in ImageNet are synsets from
WordNet, which identify a single concept in the
image rather than providing descriptions of its
full content. Since the synset labels in ImageNet
are much more precise than the descriptions pro-
vided in the captions in our training data (e.g.,
elkhound), we use synset hypernyms from Word-
Net as substitute labels when the original labels
are not in our vocabulary.
We extracted the features from the 50,000 im-
ages of the ImageNet validation set. The labels
in this set result in 393 distinct (original or hyper-
nym) words from our vocabulary. Each word was
projected to the visual space by feeding it through
the model as a one-word sentence. We ranked
the vectors corresponding to all 50,000 images
based on their similarity to the predicted vector,
and measured the accuracy of retrieving an image
with the correct label among the top 5 ranked im-
ages (Accuracy@5). Table 2 summarizes the re-
sults: VISUAL and MULTITASK learn more accu-
rate word meaning representations than LINREG.
</bodyText>
<subsectionHeader confidence="0.999561">
4.2 Sentence structure
</subsectionHeader>
<bodyText confidence="0.99084792">
In the following experiments, we examine the
knowledge of sentence structure learned by IMAG-
INET, and its impact on the model performance on
image and paraphrase retrieval.
Image retrieval We retrieve images based on
the similarity of their vectors with those predicted
by IMAGINET in two conditions: sentences are fed
to the model in their original order, or scrambled.
Figure 2 (left) shows the proportion of sentences
for which the correct image was in the top 5 high-
est ranked images for each model, as a function of
the number of training iterations: both models out-
Figure 2: Left: Accuracy@5 of image retrieval
with original versus scrambled captions. Right:
Recall@4 of paraphrase retrieval with original
vs scrambled captions.
perform the baseline. MULTITASK is initially bet-
ter in retrieving the correct image, but eventually
the gap disappears. Both models perform substan-
tially better when tested on the original captions
compared to the scrambled ones, indicating that
models learn to exploit aspects of sentence struc-
ture. This ability is to be expected for MULTI-
TASK, but the VISUAL model shows a similar ef-
fect to some extent. In the case of VISUAL, this
sensitivity to structural aspects of sentence mean-
ing is entirely driven by how they are reflected in
the image, as this models only receives the visual
supervision signal.
Qualitative analysis of the role of sequential
structure suggests that the models are sensitive
to the fact that periods terminate a sentence, that
sentences tend not to start with conjunctions, that
topics appear in sentence-initial position, and that
words have different importance as modifiers ver-
sus heads. Figure 3 shows an example; see supple-
mentary material for more.
IMAGINET vs captioning systems While it is
not our goal to engineer a state-of-the-art image
retrieval system, we want to situate IMAGINET’s
performance within the landscape of image re-
trieval results on captioned images. As most of
these are on Flickr30K (Young et al., 2014), we
ran MULTITASK on it and got an accuracy@5 of
32%, within the range of numbers reported in pre-
vious work: 29.8% (Socher et al., 2014), 31.2%
(Mao et al., 2014), 34% (Kiros et al., 2014) and
37.7% (Karpathy and Fei-Fei, 2014). Karpathy
and Fei-Fei (2014) report 29.6% on MS-COCO,
but with additional training data.
</bodyText>
<page confidence="0.998034">
115
</page>
<bodyText confidence="0.99951925">
Original a couple of horses UNK their head over a rock pile
rank 1 two brown horses hold their heads above a rocky wall.
rank 2 two horses looking over a short stone wall .
Scrambled rock couple their head pile a a UNK over of horses
rank 1 an image of a man on a couple of horses
rank 2 looking in to a straw lined pen of cows
Original a cute baby playing with a cell phone
rank 1 small baby smiling at camera and talking on phone .
rank 2 a smiling baby holding a cell phone up to ear.
Scrambled phone playing cute cell a with baby a
rank 1 someone is using their phone to send a text or play a game.
rank 2 a camera is placed next to a cellular phone .
</bodyText>
<tableCaption confidence="0.950498">
Table 3: Examples of two nearest neighbors retrieved by MULTITASK for original and scrambled cap-
tions.
</tableCaption>
<figure confidence="0.467558">
“ a variety of kitchen utensils hanging from a UNK board .”
“kitchen offrom hanging UNK variety a board utensils a .”
</figure>
<figureCaption confidence="0.653466">
Figure 3: For the original caption MULTITASK un-
</figureCaption>
<bodyText confidence="0.996609333333333">
derstands kitchen as a modifier of headword uten-
sils, which is the topic. For the scrambled sen-
tence, the model thinks kitchen is the topic.
Paraphrase retrieval In our dataset each image
is paired with five different captions, which can
be seen as paraphrases. This affords us the op-
portunity to test IMAGINET’s sentence represen-
tations on a non-visual task. Although all mod-
els receive one caption-image pair at a time, the
co-occurrence with the same image can lead the
model to learn structural similarities between cap-
tions that are different on the surface. We feed
the whole set of validation captions through the
trained model and record the final hidden visual
state hVτ . For each caption we rank all others ac-
cording to cosine similarity and measure the pro-
portion of the ones associated with the same image
among the top four highest ranked. For the scram-
bled condition, we rank original captions against
a scrambled one. Figure 2 (right) summarizes the
results: both models outperform the baseline on
ordered captions, but not on scrambled ones. As
expected, MULTITASK is more affected by manip-
ulating word order, because it is more sensitive to
structure. Table 3 shows concrete examples of the
effect of scrambling words in what sentences are
retrieved.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999958294117647">
IMAGINET is a novel model of grounded lan-
guage acquisition which simultaneously learns
word meaning representations and knowledge of
sentence structure from captioned images. It
acquires meaning representations for individual
words from descriptions of visual scenes, mim-
icking an important aspect of human language
learning, and can effectively use sentence structure
in semantic interpretation of multi-word phrases.
In future we plan to upgrade the current word-
prediction pathway to a sentence reconstruction
and/or sentence paraphrasing task in order to en-
courage the formation of representations of full
sentences. We also want to explore the acquired
structure further, especially for generalizing the
grounded meanings to those words for which vi-
sual data is not available.
</bodyText>
<sectionHeader confidence="0.97021" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99981225">
The authors would like to thank Angeliki Lazari-
dou and Marco Baroni for their many insightful
comments on the research presented in this pa-
per.
</bodyText>
<sectionHeader confidence="0.992741" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989706888888889">
Afra Alishahi and Grzegorz Chrupała. 2012. Concur-
rent acquisition of word meaning and lexical cate-
gories. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 643–654. Association for Compu-
tational Linguistics.
Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud Berg-
</reference>
<page confidence="0.997525">
116
</page>
<note confidence="0.970671">
eron, Nicolas Bouchard, and Yoshua Bengio. 2012.
Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning
NIPS 2012 Workshop.
</note>
<reference confidence="0.995628471153846">
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy). Oral Presentation.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research (JAIR), 49:1–47.
Rich Caruana. 1997. Multitask learning. Machine
learning, 28(1):41–75.
Xinlei Chen and C Lawrence Zitnick. 2014. Learning
a recurrent visual representation for image caption
generation. arXiv preprint arXiv:1411.5654.
Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation (SSST-8).
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In NIPS 2014 Deep Learning and Representa-
tion Learning Workshop.
Kenny R. Coventry, Angelo Cangelosi, Rohanna Ra-
japakse, Alison Bacon, Stephen Newstead, Dan
Joyce, and Lynn V. Richards. 2005. Spatial preposi-
tions and vague quantifiers: Implementing the func-
tional geometric framework. In Christian Freksa,
Markus Knauff, Bernd Krieg-Br¨uckner, Bernhard
Nebel, and Thomas Barkowsky, editors, Spatial
Cognition IV. Reasoning, Action, Interaction, vol-
ume 3343 of Lecture Notes in Computer Science,
pages 98–110. Springer Berlin Heidelberg.
Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2014. Long-
term recurrent convolutional networks for vi-
sual recognition and description. arXiv preprint
arXiv:1411.4389.
Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Sri-
vastava, Li Deng, Piotr Doll´ar, Jianfeng Gao, Xi-
aodong He, Margaret Mitchell, John Platt, et al.
2014. From captions to visual concepts and back.
arXiv preprint arXiv:1411.4952.
Afsaneh Fazly, Afra Alishahi, and Suzanen Steven-
son. 2010. A probabilistic computational model of
cross-situational word learning. Cognitive Science:
A Multidisciplinary Journal, 34(6):1017–1063.
Michael Fleischman and Deb Roy. 2005. Intentional
context in situated natural language learning. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning, pages 104–111. Asso-
ciation for Computational Linguistics.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with
(genuine) similarity estimation. arXiv preprint
arXiv:1408.3456.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Steve R Howell, Damian Jankowicz, and Suzanna
Becker. 2005. A model of grounded language ac-
quisition: Sensorimotor features improve lexical and
grammatical learning. Journal of Memory and Lan-
guage, 53(2):258–276.
Andrej Karpathy and Li Fei-Fei. 2014. Deep visual-
semantic alignments for generating image descrip-
tions. arXiv preprint arXiv:1412.2306.
Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S
Zemel. 2014. Unifying visual-semantic embeddings
with multimodal neural language models. arXiv
preprint arXiv:1411.2539.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-
moyer, and Mark Steedman. 2012. A probabilis-
tic model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 234–244. Association for Computa-
tional Linguistics.
Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with
a multimodal skip-gram model. In Proceedings of
NAACL HLT 2015 (2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies).
Ping Li, Igor Farkas, and Brian MacWhinney. 2004.
Early lexical development in a self-organizing neu-
ral network. Neural Networks, 17:1345–1362.
Brian MacWhinney. 2014. The CHILDES project:
Tools for analyzing talk, Volume I: Transcription for-
mat and programs. Psychology Press.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L
Yuille. 2014. Explain images with multimodal recur-
rent neural networks. In NIPS 2014 Deep Learning
Workshop.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
</reference>
<page confidence="0.981865">
117
</page>
<reference confidence="0.99987502">
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In SLT, pages 234–239.
Terry Regier. 2005. The emergence of words: Atten-
tional learning in form and meaning. Cognitive Sci-
ence: A Multidisciplinary Journal, 29:819–865.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2014. ImageNet
Large Scale Visual Recognition Challenge.
K. Simonyan and A. Zisserman. 2014. Very deep con-
volutional networks for large-scale image recogni-
tion. CoRR, abs/1409.1556.
Jeffrey M. Siskind. 1996. A computational study of
cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39–91.
Danijel Skocaj, Matej Kristan, Alen Vrecko, Marko
Mahnic, Miroslav Janicek, Geert-Jan M Krui-
jff, Marc Hanheide, Nick Hawes, Thomas Keller,
Michael Zillich, et al. 2011. A system for interac-
tive learning in dialogue with a tutor. In Intelligent
Robots and Systems (IROS), 2011 IEEE/RSJ Inter-
national Conference on, pages 3387–3394. IEEE.
Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.
Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,
Marcus Rohrbach, Raymond Mooney, and Kate
Saenko. 2014. Translating videos to natural lan-
guage using deep recurrent neural networks. arXiv
preprint arXiv:1412.4729.
Oriol Vinyals, Alexander Toshev, Samy Bengio,
and Dumitru Erhan. 2014. Show and tell: A
neural image caption generator. arXiv preprint
arXiv:1411.4555.
Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to
visual denotations: New similarity metrics for se-
mantic inference over event descriptions. Transac-
tions of the Association for Computational Linguis-
tics, 2:67–78.
Chen Yu. 2008. A statistical associative account of vo-
cabulary growth in early word learning. Language
Learning and Development, 4(1):32–62.
</reference>
<page confidence="0.99619">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.603306">
<title confidence="0.999946">Learning language through pictures</title>
<author confidence="0.994062">´Akos K´ad´ar Afra Alishahi</author>
<email confidence="0.630906">g.chrupala@uvt.nla.kadar@uvt.nla.alishahi@uvt.nl</email>
<affiliation confidence="0.9929775">Tilburg Center for Cognition and Tilburg University</affiliation>
<abstract confidence="0.998565235294118">propose a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Afra Alishahi</author>
<author>Grzegorz Chrupała</author>
</authors>
<title>Concurrent acquisition of word meaning and lexical categories.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>643--654</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Alishahi, Chrupała, 2012</marker>
<rawString>Afra Alishahi and Grzegorz Chrupała. 2012. Concurrent acquisition of word meaning and lexical categories. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 643–654. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>James Bergstra</author>
<author>Ian J Goodfellow</author>
</authors>
<location>Arnaud Berg-</location>
<marker>Bastien, Lamblin, Pascanu, Bergstra, Goodfellow, </marker>
<rawString>Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Berg-</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation.</booktitle>
<contexts>
<context position="9526" citStr="Bergstra et al., 2010" startWordPosition="1560" endWordPosition="1563">n model LINREG with a bag-of-words representation of the sentence: ˆi = Ax + b (13) where ˆi is the vector of the predicted image features, x is the vector of word counts for the input sentence and (A, b) the parameters of the linear model estimated via L2-penalized sum-ofsquared-errors loss. SimLex MEN 3K VISUAL 0.32 0.57 MULTITASK 0.39 0.63 TEXTUAL 0.31 0.53 LINREG 0.18 0.23 Table 1: Word similarity correlations with human judgments measured by Spearman’s p (all correlations are significant at level p &lt; 0.01). 4 Experiments Settings The model was implemented in Theano (Bastien et al., 2012; Bergstra et al., 2010) and optimized by Adam (Kingma and Ba, 2014).1 The fixed 4096-dimensional target image representation come from the pre-softmax layer of the 16- layer CNN (Simonyan and Zisserman, 2014). We used 1024 dimensions for the embeddings and for the hidden states of each of the GRU networks. We ran 8 iterations of training, and we report either full learning curves, or the results for each model after iteration 7 (where they performed best for the image retrieval task). For training we use the standard MS-COCO training data. For validation and test, we take a sample of 5000 images each from the valida</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>49--1</pages>
<contexts>
<context position="10710" citStr="Bruni et al., 2014" startWordPosition="1757" endWordPosition="1760">of 5000 images each from the validation data. 4.1 Word representations We assess the quality of the learned embeddings for single words via two tasks: (i) we measure similarity between embeddings of word pairs and compare them to elicited human ratings; (ii) we examine how well the model learns visual representations of words by projecting word embeddings into the visual space, and retrieving images of single concepts from ImageNet. Word similarity judgment For similarity judgment correlations, we selected two existing benchmarks that have the largest vocabulary overlap with our data: MEN 3K (Bruni et al., 2014) and SimLex-999 (Hill et al., 2014). We measure the similarity between word pairs by computing the cosine similarity between their embeddings from three versions of our model, VISUAL, MULTITASK and TEXTUAL, and the baseline LINREG. Table 1 summarizes the results. All IMAGINET models significantly correlate with human similarity judgments, and outperform LINREG. Examples of word pairs for which MULTITASK cap1Code available at github.com/gchrupala/imaginet. LT (B) = −1 Ir �τ t=1 114 VISUAL MULTITASK LINREG 0.38 0.38 0.33 Table 2: Accuracy@5 of retrieving images with compatible labels from ImageN</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research (JAIR), 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine learning,</booktitle>
<pages>28--1</pages>
<contexts>
<context position="1699" citStr="Caruana (1997)" startWordPosition="249" endWordPosition="250">cessing multimodal data where text is accompanied by images or videos is increasingly important. In this paper we propose a novel model of learning visually-grounded representations of language from paired textual and visual input. The model learns language through comprehension and production, by receiving a textual description of a scene and trying to “imagine” a visual representation of it, while predicting the next word at the same time. The full model, which we dub IMAGINET, consists of two Gated Recurrent Unit (GRU) networks coupled via shared word embeddings. IMAGINET uses a multi-task Caruana (1997) objective: both networks read the sentence word-by-word in parallel; one of them predicts the feature representation of the image depicting the described scene after reading the whole sentence, while the other one predicts the next word at each position in the word sequence. The importance of the visual and textual objectives can be traded off, and either of them can be switched off entirely, enabling us to investigate the impact of visual vs textual information on the learned language representations. Our approach to modeling human language learning has connections to recent models of image </context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask learning. Machine learning, 28(1):41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinlei Chen</author>
<author>C Lawrence Zitnick</author>
</authors>
<title>Learning a recurrent visual representation for image caption generation. arXiv preprint arXiv:1411.5654.</title>
<date>2014</date>
<contexts>
<context position="5608" citStr="Chen and Zitnick, 2014" startWordPosition="855" endWordPosition="858">Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspects of the sentence needed to predict the image features successfully. Our setup is more suita</context>
</contexts>
<marker>Chen, Zitnick, 2014</marker>
<rawString>Xinlei Chen and C Lawrence Zitnick. 2014. Learning a recurrent visual representation for image caption generation. arXiv preprint arXiv:1411.5654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merri¨enboer</author>
<author>Dzmitry Bahdanau</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the properties of neural machine translation: Encoder-decoder approaches.</title>
<date>2014</date>
<booktitle>In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8).</booktitle>
<marker>Cho, van Merri¨enboer, Bahdanau, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyoung Chung</author>
<author>Caglar Gulcehre</author>
<author>KyungHyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Empirical evaluation of gated recurrent neural networks on sequence modeling.</title>
<date>2014</date>
<booktitle>In NIPS 2014 Deep Learning and Representation Learning Workshop.</booktitle>
<contexts>
<context position="6525" citStr="Chung et al. (2014)" startWordPosition="1008" endWordPosition="1011">updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspects of the sentence needed to predict the image features successfully. Our setup is more suitable for the goal of learning representations of complete sentences. 3 Models IMAGINET consists of two parallel recurrent pathFigure 1: Structure of IMAGINET ways coupled via shared word embeddings. Both pathways are composed of Gated Recurrent Units (GRU) first introduced by Cho et al. (2014) and Chung et al. (2014). GRUs are related to the Long Short-Term Memory units (Hochreiter and Schmidhuber, 1997), but do not employ a separate memory cell. In a GRU, activation at time t is the linear combination of previous activation, and candidate activation: ht = (1 − zt) � ht−1 + zt � ˜ht (1) where � is elementwise multiplication. The update gate determines how much the activation is updated: zt = as(Wzxt + Uzht−1) (2) The candidate activation is computed as: ˜ht = a(Wxt + U(rt � ht−1)) (3) The reset gate is defined as: rt = as(Wrxt + Urht−1) (4) Our gated recurrent units use steep sigmoids for gate activations</context>
</contexts>
<marker>Chung, Gulcehre, Cho, Bengio, 2014</marker>
<rawString>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS 2014 Deep Learning and Representation Learning Workshop.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kenny R Coventry</author>
<author>Angelo Cangelosi</author>
<author>Rohanna Rajapakse</author>
<author>Alison Bacon</author>
<author>Stephen Newstead</author>
<author>Dan Joyce</author>
<author>Lynn V Richards</author>
</authors>
<title>Spatial prepositions and vague quantifiers: Implementing the functional geometric framework.</title>
<date>2005</date>
<booktitle>Spatial Cognition IV. Reasoning, Action, Interaction,</booktitle>
<volume>3343</volume>
<pages>98--110</pages>
<editor>In Christian Freksa, Markus Knauff, Bernd Krieg-Br¨uckner, Bernhard Nebel, and Thomas Barkowsky, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="3633" citStr="Coventry et al., 2005" startWordPosition="556" endWordPosition="559">l these tasks the model outperforms the baseline; the model significantly correlates with human ratings of word similarity, and predicts appropriate visual interpretations of single and multi-word phrases. The acquired knowledge of sentence structure boosts the model’s performance in both image and caption retrieval. 2 Related work Several computational models have been proposed to study early language acquisition. The acquisition of word meaning has been mainly modeled using connectionist networks that learn to associate word forms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical reg112 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1</context>
</contexts>
<marker>Coventry, Cangelosi, Rajapakse, Bacon, Newstead, Joyce, Richards, 2005</marker>
<rawString>Kenny R. Coventry, Angelo Cangelosi, Rohanna Rajapakse, Alison Bacon, Stephen Newstead, Dan Joyce, and Lynn V. Richards. 2005. Spatial prepositions and vague quantifiers: Implementing the functional geometric framework. In Christian Freksa, Markus Knauff, Bernd Krieg-Br¨uckner, Bernhard Nebel, and Thomas Barkowsky, editors, Spatial Cognition IV. Reasoning, Action, Interaction, volume 3343 of Lecture Notes in Computer Science, pages 98–110. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Donahue</author>
<author>Lisa Anne Hendricks</author>
<author>Sergio Guadarrama</author>
<author>Marcus Rohrbach</author>
<author>Subhashini Venugopalan</author>
<author>Kate Saenko</author>
<author>Trevor Darrell</author>
</authors>
<title>Longterm recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389.</title>
<date>2014</date>
<contexts>
<context position="5536" citStr="Donahue et al., 2014" startWordPosition="843" endWordPosition="846">ds and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspects of the sentence ne</context>
</contexts>
<marker>Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, Darrell, 2014</marker>
<rawString>Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2014. Longterm recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Fang</author>
<author>Saurabh Gupta</author>
<author>Forrest Iandola</author>
<author>Rupesh Srivastava</author>
<author>Li Deng</author>
<author>Piotr Doll´ar</author>
</authors>
<title>From captions to visual concepts and back. arXiv preprint arXiv:1411.4952.</title>
<date>2014</date>
<location>Jianfeng Gao, Xiaodong He, Margaret Mitchell, John</location>
<marker>Fang, Gupta, Iandola, Srivastava, Deng, Doll´ar, 2014</marker>
<rawString>Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Doll´ar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John Platt, et al. 2014. From captions to visual concepts and back. arXiv preprint arXiv:1411.4952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Afra Alishahi</author>
<author>Suzanen Stevenson</author>
</authors>
<title>A probabilistic computational model of cross-situational word learning.</title>
<date>2010</date>
<journal>Cognitive Science: A Multidisciplinary Journal,</journal>
<volume>34</volume>
<issue>6</issue>
<contexts>
<context position="4167" citStr="Fazly et al., 2010" startWordPosition="631" endWordPosition="634">ms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical reg112 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with correspondin</context>
</contexts>
<marker>Fazly, Alishahi, Stevenson, 2010</marker>
<rawString>Afsaneh Fazly, Afra Alishahi, and Suzanen Stevenson. 2010. A probabilistic computational model of cross-situational word learning. Cognitive Science: A Multidisciplinary Journal, 34(6):1017–1063.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Deb Roy</author>
</authors>
<title>Intentional context in situated natural language learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4494" citStr="Fleischman and Roy, 2005" startWordPosition="679" endWordPosition="682">anguage Processing (Short Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from u</context>
</contexts>
<marker>Fleischman, Roy, 2005</marker>
<rawString>Michael Fleischman and Deb Roy. 2005. Intentional context in situated natural language learning. In Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 104–111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456.</title>
<date>2014</date>
<contexts>
<context position="10745" citStr="Hill et al., 2014" startWordPosition="1763" endWordPosition="1766">ion data. 4.1 Word representations We assess the quality of the learned embeddings for single words via two tasks: (i) we measure similarity between embeddings of word pairs and compare them to elicited human ratings; (ii) we examine how well the model learns visual representations of words by projecting word embeddings into the visual space, and retrieving images of single concepts from ImageNet. Word similarity judgment For similarity judgment correlations, we selected two existing benchmarks that have the largest vocabulary overlap with our data: MEN 3K (Bruni et al., 2014) and SimLex-999 (Hill et al., 2014). We measure the similarity between word pairs by computing the cosine similarity between their embeddings from three versions of our model, VISUAL, MULTITASK and TEXTUAL, and the baseline LINREG. Table 1 summarizes the results. All IMAGINET models significantly correlate with human similarity judgments, and outperform LINREG. Examples of word pairs for which MULTITASK cap1Code available at github.com/gchrupala/imaginet. LT (B) = −1 Ir �τ t=1 114 VISUAL MULTITASK LINREG 0.38 0.38 0.33 Table 2: Accuracy@5 of retrieving images with compatible labels from ImageNet. tures human similarity judgment</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="6614" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1021" endWordPosition="1024">isual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspects of the sentence needed to predict the image features successfully. Our setup is more suitable for the goal of learning representations of complete sentences. 3 Models IMAGINET consists of two parallel recurrent pathFigure 1: Structure of IMAGINET ways coupled via shared word embeddings. Both pathways are composed of Gated Recurrent Units (GRU) first introduced by Cho et al. (2014) and Chung et al. (2014). GRUs are related to the Long Short-Term Memory units (Hochreiter and Schmidhuber, 1997), but do not employ a separate memory cell. In a GRU, activation at time t is the linear combination of previous activation, and candidate activation: ht = (1 − zt) � ht−1 + zt � ˜ht (1) where � is elementwise multiplication. The update gate determines how much the activation is updated: zt = as(Wzxt + Uzht−1) (2) The candidate activation is computed as: ˜ht = a(Wxt + U(rt � ht−1)) (3) The reset gate is defined as: rt = as(Wrxt + Urht−1) (4) Our gated recurrent units use steep sigmoids for gate activations: 1 as(z) = 1 + exp(−3.75z) and rectified linear units clipped between 0 and 5 for the un</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve R Howell</author>
<author>Damian Jankowicz</author>
<author>Suzanna Becker</author>
</authors>
<title>A model of grounded language acquisition: Sensorimotor features improve lexical and grammatical learning.</title>
<date>2005</date>
<journal>Journal of Memory and Language,</journal>
<volume>53</volume>
<issue>2</issue>
<contexts>
<context position="5059" citStr="Howell et al., 2005" startWordPosition="767" endWordPosition="770"> concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of mod</context>
</contexts>
<marker>Howell, Jankowicz, Becker, 2005</marker>
<rawString>Steve R Howell, Damian Jankowicz, and Suzanna Becker. 2005. A model of grounded language acquisition: Sensorimotor features improve lexical and grammatical learning. Journal of Memory and Language, 53(2):258–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Li Fei-Fei</author>
</authors>
<title>Deep visualsemantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306.</title>
<date>2014</date>
<contexts>
<context position="5476" citStr="Karpathy and Fei-Fei, 2014" startWordPosition="831" endWordPosition="834">rds. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidde</context>
<context position="14953" citStr="Karpathy and Fei-Fei, 2014" startWordPosition="2445" endWordPosition="2448">words have different importance as modifiers versus heads. Figure 3 shows an example; see supplementary material for more. IMAGINET vs captioning systems While it is not our goal to engineer a state-of-the-art image retrieval system, we want to situate IMAGINET’s performance within the landscape of image retrieval results on captioned images. As most of these are on Flickr30K (Young et al., 2014), we ran MULTITASK on it and got an accuracy@5 of 32%, within the range of numbers reported in previous work: 29.8% (Socher et al., 2014), 31.2% (Mao et al., 2014), 34% (Kiros et al., 2014) and 37.7% (Karpathy and Fei-Fei, 2014). Karpathy and Fei-Fei (2014) report 29.6% on MS-COCO, but with additional training data. 115 Original a couple of horses UNK their head over a rock pile rank 1 two brown horses hold their heads above a rocky wall. rank 2 two horses looking over a short stone wall . Scrambled rock couple their head pile a a UNK over of horses rank 1 an image of a man on a couple of horses rank 2 looking in to a straw lined pen of cows Original a cute baby playing with a cell phone rank 1 small baby smiling at camera and talking on phone . rank 2 a smiling baby holding a cell phone up to ear. Scrambled phone pl</context>
</contexts>
<marker>Karpathy, Fei-Fei, 2014</marker>
<rawString>Andrej Karpathy and Li Fei-Fei. 2014. Deep visualsemantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diederik P Kingma</author>
<author>Jimmy Ba</author>
</authors>
<title>Adam: A method for stochastic optimization.</title>
<date>2014</date>
<location>CoRR, abs/1412.6980.</location>
<contexts>
<context position="9570" citStr="Kingma and Ba, 2014" startWordPosition="1569" endWordPosition="1572">ion of the sentence: ˆi = Ax + b (13) where ˆi is the vector of the predicted image features, x is the vector of word counts for the input sentence and (A, b) the parameters of the linear model estimated via L2-penalized sum-ofsquared-errors loss. SimLex MEN 3K VISUAL 0.32 0.57 MULTITASK 0.39 0.63 TEXTUAL 0.31 0.53 LINREG 0.18 0.23 Table 1: Word similarity correlations with human judgments measured by Spearman’s p (all correlations are significant at level p &lt; 0.01). 4 Experiments Settings The model was implemented in Theano (Bastien et al., 2012; Bergstra et al., 2010) and optimized by Adam (Kingma and Ba, 2014).1 The fixed 4096-dimensional target image representation come from the pre-softmax layer of the 16- layer CNN (Simonyan and Zisserman, 2014). We used 1024 dimensions for the embeddings and for the hidden states of each of the GRU networks. We ran 8 iterations of training, and we report either full learning curves, or the results for each model after iteration 7 (where they performed best for the image retrieval task). For training we use the standard MS-COCO training data. For validation and test, we take a sample of 5000 images each from the validation data. 4.1 Word representations We asses</context>
</contexts>
<marker>Kingma, Ba, 2014</marker>
<rawString>Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard S Zemel</author>
</authors>
<title>Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539.</title>
<date>2014</date>
<contexts>
<context position="5514" citStr="Kiros et al., 2014" startWordPosition="839" endWordPosition="842">t acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspe</context>
<context position="14914" citStr="Kiros et al., 2014" startWordPosition="2439" endWordPosition="2442">nce-initial position, and that words have different importance as modifiers versus heads. Figure 3 shows an example; see supplementary material for more. IMAGINET vs captioning systems While it is not our goal to engineer a state-of-the-art image retrieval system, we want to situate IMAGINET’s performance within the landscape of image retrieval results on captioned images. As most of these are on Flickr30K (Young et al., 2014), we ran MULTITASK on it and got an accuracy@5 of 32%, within the range of numbers reported in previous work: 29.8% (Socher et al., 2014), 31.2% (Mao et al., 2014), 34% (Kiros et al., 2014) and 37.7% (Karpathy and Fei-Fei, 2014). Karpathy and Fei-Fei (2014) report 29.6% on MS-COCO, but with additional training data. 115 Original a couple of horses UNK their head over a rock pile rank 1 two brown horses hold their heads above a rocky wall. rank 2 two horses looking over a short stone wall . Scrambled rock couple their head pile a a UNK over of horses rank 1 an image of a man on a couple of horses rank 2 looking in to a straw lined pen of cows Original a cute baby playing with a cell phone rank 1 small baby smiling at camera and talking on phone . rank 2 a smiling baby holding a c</context>
</contexts>
<marker>Kiros, Salakhutdinov, Zemel, 2014</marker>
<rawString>Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. 2014. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Sharon Goldwater</author>
<author>Luke Zettlemoyer</author>
<author>Mark Steedman</author>
</authors>
<title>A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>234--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5086" citStr="Kwiatkowski et al., 2012" startWordPosition="771" endWordPosition="774"> videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most re</context>
</contexts>
<marker>Kwiatkowski, Goldwater, Zettlemoyer, Steedman, 2012</marker>
<rawString>Tom Kwiatkowski, Sharon Goldwater, Luke Zettlemoyer, and Mark Steedman. 2012. A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234–244. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>Combining language and vision with a multimodal skip-gram model.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL HLT 2015 (2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies).</booktitle>
<contexts>
<context position="4541" citStr="Lazaridou et al. (2015)" startWordPosition="687" endWordPosition="690">, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated</context>
</contexts>
<marker>Lazaridou, Pham, Baroni, 2015</marker>
<rawString>Angeliki Lazaridou, Nghia The Pham, and Marco Baroni. 2015. Combining language and vision with a multimodal skip-gram model. In Proceedings of NAACL HLT 2015 (2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Igor Farkas</author>
<author>Brian MacWhinney</author>
</authors>
<title>Early lexical development in a self-organizing neural network. Neural Networks,</title>
<date>2004</date>
<pages>17--1345</pages>
<contexts>
<context position="3610" citStr="Li et al., 2004" startWordPosition="552" endWordPosition="555">e captions. In all these tasks the model outperforms the baseline; the model significantly correlates with human ratings of word similarity, and predicts appropriate visual interpretations of single and multi-word phrases. The acquired knowledge of sentence structure boosts the model’s performance in both image and caption retrieval. 2 Related work Several computational models have been proposed to study early language acquisition. The acquisition of word meaning has been mainly modeled using connectionist networks that learn to associate word forms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical reg112 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as </context>
</contexts>
<marker>Li, Farkas, MacWhinney, 2004</marker>
<rawString>Ping Li, Igor Farkas, and Brian MacWhinney. 2004. Early lexical development in a self-organizing neural network. Neural Networks, 17:1345–1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk, Volume I: Transcription format and programs.</title>
<date>2014</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="4311" citStr="MacWhinney, 2014" startWordPosition="655" endWordPosition="656">ons which use statistical reg112 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition o</context>
</contexts>
<marker>MacWhinney, 2014</marker>
<rawString>Brian MacWhinney. 2014. The CHILDES project: Tools for analyzing talk, Volume I: Transcription format and programs. Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhua Mao</author>
<author>Wei Xu</author>
<author>Yi Yang</author>
<author>Jiang Wang</author>
<author>Alan L Yuille</author>
</authors>
<title>Explain images with multimodal recurrent neural networks.</title>
<date>2014</date>
<booktitle>In NIPS 2014 Deep Learning Workshop.</booktitle>
<contexts>
<context position="5494" citStr="Mao et al., 2014" startWordPosition="835" endWordPosition="838">d at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the vis</context>
<context position="14888" citStr="Mao et al., 2014" startWordPosition="2434" endWordPosition="2437">t topics appear in sentence-initial position, and that words have different importance as modifiers versus heads. Figure 3 shows an example; see supplementary material for more. IMAGINET vs captioning systems While it is not our goal to engineer a state-of-the-art image retrieval system, we want to situate IMAGINET’s performance within the landscape of image retrieval results on captioned images. As most of these are on Flickr30K (Young et al., 2014), we ran MULTITASK on it and got an accuracy@5 of 32%, within the range of numbers reported in previous work: 29.8% (Socher et al., 2014), 31.2% (Mao et al., 2014), 34% (Kiros et al., 2014) and 37.7% (Karpathy and Fei-Fei, 2014). Karpathy and Fei-Fei (2014) report 29.6% on MS-COCO, but with additional training data. 115 Original a couple of horses UNK their head over a rock pile rank 1 two brown horses hold their heads above a rocky wall. rank 2 two horses looking over a short stone wall . Scrambled rock couple their head pile a a UNK over of horses rank 1 an image of a man on a couple of horses rank 2 looking in to a straw lined pen of cows Original a cute baby playing with a cell phone rank 1 small baby smiling at camera and talking on phone . rank 2 </context>
</contexts>
<marker>Mao, Xu, Yang, Wang, Yuille, 2014</marker>
<rawString>Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L Yuille. 2014. Explain images with multimodal recurrent neural networks. In NIPS 2014 Deep Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="4605" citStr="Mikolov et al., 2013" startWordPosition="696" endWordPosition="699">tional Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Context dependent recurrent neural network language model.</title>
<date>2012</date>
<booktitle>In SLT,</booktitle>
<pages>234--239</pages>
<contexts>
<context position="5845" citStr="Mikolov and Zweig, 2012" startWordPosition="896" endWordPosition="899">roposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspects of the sentence needed to predict the image features successfully. Our setup is more suitable for the goal of learning representations of complete sentences. 3 Models IMAGINET consists of two parallel recurrent pathFigure 1: Structure of IMAGINET ways coupled via shared word embeddings. Both pathways are composed of Gated Rec</context>
</contexts>
<marker>Mikolov, Zweig, 2012</marker>
<rawString>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In SLT, pages 234–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>The emergence of words: Attentional learning in form and meaning.</title>
<date>2005</date>
<journal>Cognitive Science: A Multidisciplinary Journal,</journal>
<pages>29--819</pages>
<contexts>
<context position="3648" citStr="Regier, 2005" startWordPosition="560" endWordPosition="561"> outperforms the baseline; the model significantly correlates with human ratings of word similarity, and predicts appropriate visual interpretations of single and multi-word phrases. The acquired knowledge of sentence structure boosts the model’s performance in both image and caption retrieval. 2 Related work Several computational models have been proposed to study early language acquisition. The acquisition of word meaning has been mainly modeled using connectionist networks that learn to associate word forms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical reg112 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childd</context>
</contexts>
<marker>Regier, 2005</marker>
<rawString>Terry Regier. 2005. The emergence of words: Attentional learning in form and meaning. Cognitive Science: A Multidisciplinary Journal, 29:819–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Russakovsky</author>
<author>Jia Deng</author>
<author>Hao Su</author>
<author>Jonathan Krause</author>
<author>Sanjeev Satheesh</author>
<author>Sean Ma</author>
<author>Zhiheng Huang</author>
<author>Andrej Karpathy</author>
<author>Aditya Khosla</author>
<author>Michael Bernstein</author>
<author>Alexander C Berg</author>
<author>Li Fei-Fei</author>
</authors>
<title>ImageNet Large Scale Visual Recognition Challenge.</title>
<date>2014</date>
<contexts>
<context position="11662" citStr="Russakovsky et al., 2014" startWordPosition="1903" endWordPosition="1906">ty judgments, and outperform LINREG. Examples of word pairs for which MULTITASK cap1Code available at github.com/gchrupala/imaginet. LT (B) = −1 Ir �τ t=1 114 VISUAL MULTITASK LINREG 0.38 0.38 0.33 Table 2: Accuracy@5 of retrieving images with compatible labels from ImageNet. tures human similarity judgments better than VISUAL include antonyms (dusk, dawn), collocations (sexy, smile), or related but not visually similar words (college, exhibition). Single-word image retrieval In order to visualize the acquired meaning for individual words, we use images from the ILSVRC2012 subset of ImageNet (Russakovsky et al., 2014) as benchmark. Labels of the images in ImageNet are synsets from WordNet, which identify a single concept in the image rather than providing descriptions of its full content. Since the synset labels in ImageNet are much more precise than the descriptions provided in the captions in our training data (e.g., elkhound), we use synset hypernyms from WordNet as substitute labels when the original labels are not in our vocabulary. We extracted the features from the 50,000 images of the ImageNet validation set. The labels in this set result in 393 distinct (original or hypernym) words from our vocabu</context>
</contexts>
<marker>Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, Fei-Fei, 2014</marker>
<rawString>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2014. ImageNet Large Scale Visual Recognition Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simonyan</author>
<author>A Zisserman</author>
</authors>
<title>Very deep convolutional networks for large-scale image recognition.</title>
<date>2014</date>
<location>CoRR, abs/1409.1556.</location>
<contexts>
<context position="9711" citStr="Simonyan and Zisserman, 2014" startWordPosition="1590" endWordPosition="1593"> input sentence and (A, b) the parameters of the linear model estimated via L2-penalized sum-ofsquared-errors loss. SimLex MEN 3K VISUAL 0.32 0.57 MULTITASK 0.39 0.63 TEXTUAL 0.31 0.53 LINREG 0.18 0.23 Table 1: Word similarity correlations with human judgments measured by Spearman’s p (all correlations are significant at level p &lt; 0.01). 4 Experiments Settings The model was implemented in Theano (Bastien et al., 2012; Bergstra et al., 2010) and optimized by Adam (Kingma and Ba, 2014).1 The fixed 4096-dimensional target image representation come from the pre-softmax layer of the 16- layer CNN (Simonyan and Zisserman, 2014). We used 1024 dimensions for the embeddings and for the hidden states of each of the GRU networks. We ran 8 iterations of training, and we report either full learning curves, or the results for each model after iteration 7 (where they performed best for the image retrieval task). For training we use the standard MS-COCO training data. For validation and test, we take a sample of 5000 images each from the validation data. 4.1 Word representations We assess the quality of the learned embeddings for single words via two tasks: (i) we measure similarity between embeddings of word pairs and compar</context>
</contexts>
<marker>Simonyan, Zisserman, 2014</marker>
<rawString>K. Simonyan and A. Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey M Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning word-tomeaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="4136" citStr="Siskind, 1996" startWordPosition="627" endWordPosition="628">arn to associate word forms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical reg112 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individ</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Jeffrey M. Siskind. 1996. A computational study of cross-situational techniques for learning word-tomeaning mappings. Cognition, 61(1-2):39–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danijel Skocaj</author>
<author>Matej Kristan</author>
<author>Alen Vrecko</author>
<author>Marko Mahnic</author>
<author>Miroslav Janicek</author>
<author>Geert-Jan M Kruijff</author>
<author>Marc Hanheide</author>
<author>Nick Hawes</author>
<author>Thomas Keller</author>
<author>Michael Zillich</author>
</authors>
<title>A system for interactive learning in dialogue with a tutor.</title>
<date>2011</date>
<booktitle>In Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on,</booktitle>
<pages>3387--3394</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4516" citStr="Skocaj et al., 2011" startWordPosition="683" endWordPosition="686">Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with </context>
</contexts>
<marker>Skocaj, Kristan, Vrecko, Mahnic, Janicek, Kruijff, Hanheide, Hawes, Keller, Zillich, 2011</marker>
<rawString>Danijel Skocaj, Matej Kristan, Alen Vrecko, Marko Mahnic, Miroslav Janicek, Geert-Jan M Kruijff, Marc Hanheide, Nick Hawes, Thomas Keller, Michael Zillich, et al. 2011. A system for interactive learning in dialogue with a tutor. In Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on, pages 3387–3394. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--207</pages>
<contexts>
<context position="14862" citStr="Socher et al., 2014" startWordPosition="2429" endWordPosition="2432"> start with conjunctions, that topics appear in sentence-initial position, and that words have different importance as modifiers versus heads. Figure 3 shows an example; see supplementary material for more. IMAGINET vs captioning systems While it is not our goal to engineer a state-of-the-art image retrieval system, we want to situate IMAGINET’s performance within the landscape of image retrieval results on captioned images. As most of these are on Flickr30K (Young et al., 2014), we ran MULTITASK on it and got an accuracy@5 of 32%, within the range of numbers reported in previous work: 29.8% (Socher et al., 2014), 31.2% (Mao et al., 2014), 34% (Kiros et al., 2014) and 37.7% (Karpathy and Fei-Fei, 2014). Karpathy and Fei-Fei (2014) report 29.6% on MS-COCO, but with additional training data. 115 Original a couple of horses UNK their head over a rock pile rank 1 two brown horses hold their heads above a rocky wall. rank 2 two horses looking over a short stone wall . Scrambled rock couple their head pile a a UNK over of horses rank 1 an image of a man on a couple of horses rank 2 looking in to a straw lined pen of cows Original a cute baby playing with a cell phone rank 1 small baby smiling at camera and </context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhashini Venugopalan</author>
<author>Huijuan Xu</author>
<author>Jeff Donahue</author>
<author>Marcus Rohrbach</author>
<author>Raymond Mooney</author>
<author>Kate Saenko</author>
</authors>
<title>Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729.</title>
<date>2014</date>
<contexts>
<context position="5584" citStr="Venugopalan et al., 2014" startWordPosition="851" endWordPosition="854">ch as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspects of the sentence needed to predict the image features successfully.</context>
</contexts>
<marker>Venugopalan, Xu, Donahue, Rohrbach, Mooney, Saenko, 2014</marker>
<rawString>Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. 2014. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Alexander Toshev</author>
<author>Samy Bengio</author>
<author>Dumitru Erhan</author>
</authors>
<title>Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555.</title>
<date>2014</date>
<contexts>
<context position="5558" citStr="Vinyals et al., 2014" startWordPosition="847" endWordPosition="850">sentence structure, such as lexical categories (Alishahi and Chrupala, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspects of the sentence needed to predict the im</context>
</contexts>
<marker>Vinyals, Toshev, Bengio, Erhan, 2014</marker>
<rawString>Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2014. Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--67</pages>
<contexts>
<context position="14725" citStr="Young et al., 2014" startWordPosition="2403" endWordPosition="2406">of sequential structure suggests that the models are sensitive to the fact that periods terminate a sentence, that sentences tend not to start with conjunctions, that topics appear in sentence-initial position, and that words have different importance as modifiers versus heads. Figure 3 shows an example; see supplementary material for more. IMAGINET vs captioning systems While it is not our goal to engineer a state-of-the-art image retrieval system, we want to situate IMAGINET’s performance within the landscape of image retrieval results on captioned images. As most of these are on Flickr30K (Young et al., 2014), we ran MULTITASK on it and got an accuracy@5 of 32%, within the range of numbers reported in previous work: 29.8% (Socher et al., 2014), 31.2% (Mao et al., 2014), 34% (Kiros et al., 2014) and 37.7% (Karpathy and Fei-Fei, 2014). Karpathy and Fei-Fei (2014) report 29.6% on MS-COCO, but with additional training data. 115 Original a couple of horses UNK their head over a rock pile rank 1 two brown horses hold their heads above a rocky wall. rank 2 two horses looking over a short stone wall . Scrambled rock couple their head pile a a UNK over of horses rank 1 an image of a man on a couple of hors</context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
</authors>
<title>A statistical associative account of vocabulary growth in early word learning.</title>
<date>2008</date>
<journal>Language Learning and Development,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="4146" citStr="Yu, 2008" startWordPosition="629" endWordPosition="630">e word forms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical reg112 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 112–118, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually alig</context>
</contexts>
<marker>Yu, 2008</marker>
<rawString>Chen Yu. 2008. A statistical associative account of vocabulary growth in early word learning. Language Learning and Development, 4(1):32–62.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>