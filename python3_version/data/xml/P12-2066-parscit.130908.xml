<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018889">
<title confidence="0.999347">
Identifying High-Impact Sub-Structures for Convolution Kernels in
Document-level Sentiment Classification
</title>
<author confidence="0.937706">
Zhaopeng Tu† Yifan He&amp;quot; Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin†
</author>
<affiliation confidence="0.85516">
†Key Lab. of Intelligent Info. Processing #Computer Science Department §School of Computing
Institute of Computing Technology, CAS New York University Dublin City University
</affiliation>
<email confidence="0.9861775">
†ftuzhaopeng,liuqun,sxlin}@ict.ac.cn,
tyhe@cs.nyu.edu, §fjfoster,josef}@computing.dcu.ie
</email>
<sectionHeader confidence="0.995521" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999462117647059">
Convolution kernels support the modeling of
complex syntactic information in machine-
learning tasks. However, such models are
highly sensitive to the type and size of syntac-
tic structure used. It is therefore an importan-
t challenge to automatically identify high im-
pact sub-structures relevant to a given task. In
this paper we present a systematic study inves-
tigating (combinations of) sequence and con-
volution kernels using different types of sub-
structures in document-level sentiment classi-
fication. We show that minimal sub-structures
extracted from constituency and dependency
trees guided by a polarity lexicon show 1.45
point absolute improvement in accuracy over a
bag-of-words classifier on a widely used sen-
timent corpus.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999655625">
An important subtask in sentiment analysis is sen-
timent classification. Sentiment classification in-
volves the identification of positive and negative
opinions from a text segment at various levels of
granularity including document-level, paragraph-
level, sentence-level and phrase-level. This paper
focuses on document-level sentiment classification.
There has been a substantial amount of work
on document-level sentiment classification. In ear-
ly pioneering work, Pang and Lee (2004) use a
flat feature vector (e.g., a bag-of-words) to rep-
resent the documents. A bag-of-words approach,
however, cannot capture important information ob-
tained from structural linguistic analysis of the doc-
uments. More recently, there have been several ap-
proaches which employ features based on deep lin-
guistic analysis with encouraging results including
Joshi and Penstein-Rose (2009) and Liu and Senef-
f (2009). However, as they select features manually,
these methods would require additional labor when
ported to other languages and domains.
In this paper, we study and evaluate diverse lin-
guistic structures encoded as convolution kernels for
the document-level sentiment classification prob-
lem, in order to utilize syntactic structures without
defining explicit linguistic rules. While the applica-
tion of kernel methods could seem intuitive for many
tasks, it is non-trivial to apply convolution kernels
to document-level sentiment classification: previous
work has already shown that categorically using the
entire syntactic structure of a single sentence would
produce too many features for a convolution ker-
nel (Zhang et al., 2006; Moschitti et al., 2008). We
expect the situation to be worse for our task as we
work with documents that tend to comprise dozens
of sentences.
It is therefore necessary to choose appropriate
substructures of a sentence as opposed to using the
whole structure in order to effectively use convolu-
tion kernels in our task. It has been observed that
not every part of a document is equally informa-
tive for identifying the polarity of the whole doc-
ument (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Koppel and Schler, 2005; Ferguson et
al., 2009): a film review often uses lengthy objective
paragraphs to simply describe the plot. Such objec-
tive portions do not contain the author’s opinion and
are irrelevant with respect to the sentiment classifi-
</bodyText>
<page confidence="0.979529">
338
</page>
<note confidence="0.6855135">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338–343,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999566684210526">
cation task. Indeed, separating objective sentences
from subjective sentences in a document produces
encouraging results (Yu and Hatzivassiloglou, 2003;
Pang and Lee, 2004; Koppel and Schler, 2005; Fer-
guson et al., 2009). Our research is inspired by these
observations. Unlike in the previous work, however,
we focus on syntactic substructures (rather than en-
tire paragraphs or sentences) that contain subjective
words.
More specifically, we use the terms in the lexi-
con constructed from (Wilson et al., 2005) as the
indicators to identify the substructures for the con-
volution kernels, and extract different sub-structures
according to these indicators for various types of
parse trees (Section 3). An empirical evaluation on
a widely used sentiment corpus shows an improve-
ment of 1.45 point in accuracy over the baseline
resulting from a combination of bag-of-words and
high-impact parse features (Section 4).
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999729351351351">
Our research builds on previous work in the field
of sentiment classification and convolution kernel-
s. For sentiment classification, the design of lexi-
cal and syntactic features is an important first step.
Several approaches propose feature-based learning
algorithms for this problem. Pang and Lee (2004)
and Dave et al. (2003) represent a document as a
bag-of-words; Matsumoto et al., (2005) extract fre-
quently occurring connected subtrees from depen-
dency parsing; Joshi and Penstein-Rose (2009) use
a transformation of dependency relation triples; Liu
and Seneff (2009) extract adverb-adjective-noun re-
lations from dependency parser output.
Previous research has convincingly demonstrat-
ed a kernel’s ability to generate large feature set-
s, which is useful to quickly model new and not
well understood linguistic phenomena in machine
learning, and has led to improvements in various
NLP tasks, including relation extraction (Bunescu
and Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al., 2006; Nguyen et al., 2009), question
answering (Moschitti and Quarteroni, 2008), seman-
tic role labeling (Moschitti et al., 2008).
Convolution kernels have been used before in sen-
timent analysis: Wiegand and Klakow (2010) use
convolution kernels for opinion holder extraction,
Johansson and Moschitti (2010) for opinion expres-
sion detection and Agarwal et al. (2011) for sen-
timent analysis of Twitter data. Wiegand and K-
lakow (2010) use e.g. noun phrases as possible can-
didate opinion holders, in our work we extract any
minimal syntactic context containing a subjective
word. Johansson and Moschitti (2010) and Agarwal
et al. (2011) process sentences and tweets respec-
tively. However, as these are considerably shorter
than documents, their feature space is less complex,
and pruning is not as pertinent.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="method">
3 Kernels for Sentiment Classification
</sectionHeader>
<subsectionHeader confidence="0.997959">
3.1 Linguistic Representations
</subsectionHeader>
<bodyText confidence="0.999683">
We explore both sequence and convolution kernels
to exploit information on surface and syntactic lev-
els. For sequence kernels, we make use of lexical
words with some syntactic information in the form
of part-of-speech (POS) tags. More specifically, we
define three types of sequences:
</bodyText>
<listItem confidence="0.997709142857143">
• SW, a sequence of lexical words, e.g.: A tragic
waste of talent and incredible visual effects.
• SP, a sequence of POS tags, e.g.: DT JJ NN IN
NN CC JJ JJ NNS.
• SWP, a sequence of words and POS tags,
e.g.: A/DT tragic/JJ waste/NN of/IN talent/NN
and/CC incredible/JJ visual/JJ effects/NNS.
</listItem>
<bodyText confidence="0.999940363636364">
In addition, we experiment with constituency tree
kernels (CON), and dependency tree kernels (D),
which capture hierarchical constituency structure
and labeled dependency relations between words,
respectively. For dependency kernels, we test with
word (DW), POS (DP), and combined word-and-
POS settings (DWP), and similarly for simple se-
quence kernels (SW, SP and SWP). We also use a
vector kernel (VK) in a bag-of-words baseline. Fig-
ure 1 shows the constituent and dependency struc-
ture for the above sentence.
</bodyText>
<subsectionHeader confidence="0.998937">
3.2 Settings
</subsectionHeader>
<bodyText confidence="0.9996325">
As kernel-based algorithms inherently explore the
whole feature space to weight the features, it is im-
portant to choose appropriate substructures to re-
move unnecessary features as much as possible.
</bodyText>
<page confidence="0.990981">
339
</page>
<figure confidence="0.999540717391304">
NP
waste
PP
NP
(a)
NP
DT JJ NN
A tragic waste
CC
NN
and
talent
NP NP
JJ JJ NNS
incredible visual effect
IN
of
DT
waste
det amod prep of
A tragic talent
conj and
effects
amod amod
incredible visual
waste
det amod prep of
DT JJ NN
conj and
NNS
amod amod
JJ JJ
det amod prep of
NN
talent
conj and
NNS
effects
amod amod
JJ
visual
visual
incredible
JJ
A tragic
(b) (c) (d)
</figure>
<figureCaption confidence="0.967996666666667">
Figure 1: Illustration of the different tree structures employed for convolution kernels. (a) Constituent parse tree
(CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b)
with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP).
</figureCaption>
<figure confidence="0.999587357142857">
NP
waste
amod
tragic
(b)
JJ
DT JJ NN
A tragic waste
(a)
Scopes Trees Size
Document 32 24
Subjective Sentences 22 27
Constituent Substructures 30 10
Dependency Substructures 40 3
</figure>
<figureCaption confidence="0.982221">
Figure 2: Illustration of the different settings on con-
stituency (CON) and dependency (DWP) parse trees with
tragic as the indicator word.
</figureCaption>
<bodyText confidence="0.997836933333333">
Unfortunately, in our task there exist several cues
indicating the polarity of the document, which are
distributed in different sentences. To solve this prob-
lem, we define the indicators in this task as subjec-
tive words in a polarity lexicon (Wilson et al., 2005).
For each polarity indicator, we define the “scope”
(the minimal syntactic structure containing at least
one subjective word) of each indicator for different
representations as follows:
For a constituent tree, a node and its children
correspond to a grammatical production. There-
fore, considering the terminal node tragic in the con-
stituent structure tree in Figure 1(a), we extract the
subtree rooted at the grandparent of the terminal, see
Figure 2(a). We also use the corresponding sequence
</bodyText>
<tableCaption confidence="0.956988">
Table 1: The detail of the corpus. Here Trees denotes the
average number of trees, and Size denotes the averaged
number of words in each tree.
</tableCaption>
<bodyText confidence="0.9997515">
of words in the subtree for the sequential kernel.
For a dependency tree, we only consider the sub-
tree containing the lexical items that are directly
connected to the subjective word. For instance, giv-
en the node tragic in Figure 1(d), we will extract its
direct parent waste integrated with dependency rela-
tions and (possibly) POS, as in Figure 2(b).
We further add two background scopes, one be-
ing subjective sentences (the sentences that contain
subjective words), and the entire document.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.959728">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999399">
We carried out experiments on the movie review
dataset (Pang and Lee, 2004), which consists of
</bodyText>
<page confidence="0.993983">
340
</page>
<bodyText confidence="0.999843523809524">
1000 positive reviews and 1000 negative reviews.
To obtain constituency trees, we parsed the docu-
ment using the Stanford Parser (Klein and Man-
ning, 2003). To obtain dependency trees, we passed
the Stanford constituency trees through the Stanford
constituency-to-dependency converter (de Marneffe
and Manning, 2008).
We exploited Subset Tree (SST) (Collins and
Duffy, 2001) and Partial Tree (PT) kernels (Mos-
chitti, 2006) for constituent and dependency parse
trees1, respectively. A sequential kernel is applied
for lexical sequences. Kernels were combined using
plain (unweighted) summation. Corpus statistics are
provided in Table 1.
We use a manually constructed polarity lexicon
(Wilson et al., 2005), in which each entry is annotat-
ed with its degree of subjectivity (strong, weak), as
well as its sentiment polarity (positive, negative and
neutral). We only take into account the subjective
terms with the degree of strong subjectivity.
We consider two baselines:
</bodyText>
<listItem confidence="0.9791798">
• VK: bag-of-words features using a vector ker-
nel (Pang and Lee, 2004; Ng et al., 2006)
• Rand: a number of randomly selected sub-
structures similar to the number of extracted
substructures defined in Section 3.2
</listItem>
<bodyText confidence="0.99628275">
All experiments were carried out using the SVM-
Light-TK toolkit2 with default parameter settings.
All results reported are based on 10-fold cross vali-
dation.
</bodyText>
<subsectionHeader confidence="0.651801">
4.2 Results and Discussions
</subsectionHeader>
<bodyText confidence="0.84271676923077">
Table 2 lists the results of the different kernel type
combinations. The best performance is obtained by
combining VK and DW kernels, gaining a signifi-
cant improvement of 1.45 point in accuracy. As far
as PT kernels are concerned, we find dependency
trees with simple words (DW) outperform both de-
pendency trees with POS (DP) and those with both
words and POS (DWP). We conjecture that in this
case, as syntactic information is already captured by
1A SubSet Tree is a structure that satisfies the constraint that
grammatical rules cannot be broken, while a Partial Tree is a
more general form of substructures obtained by the application
of partial production rules of the grammar.
</bodyText>
<footnote confidence="0.994892">
2available at http://disi.unitn.it/moschitti/
</footnote>
<table confidence="0.998927111111111">
Kernels Doc Sent Rand Sub
VK 87.05
VK + SW 87.25 86.95 87.25 87.40
VK + SP 87.35 86.95 87.45 87.35
VK + SWP 87.30 87.45 87.30 88.15*
VK + CON 87.45 87.65 87.45 88.30**
VK + DW 87.35 87.50 87.30 88.50**
VK + DP 87.75* 87.20 87.35 87.75
VK + DWP 87.70* 87.30 87.65 87.80*
</table>
<tableCaption confidence="0.932022666666667">
Table 2: Results of kernels. Here Doc denotes the whole
document of the text, Sent denotes the sentences that con-
tains subjective terms in the lexicon, Rand denotes ran-
</tableCaption>
<bodyText confidence="0.968708444444445">
domly selected substructures, and Sub denotes the sub-
structures defined in Section 3.2. We use “*” and “**” to
denote a result is better than baseline VK significantly at
p &lt; 0.05 and p &lt; 0.01 (sign test), respectively.
the dependency representation, POS tags can intro-
duce little new information, and will add unneces-
sary complexity. For example, given the substruc-
ture (waste (amod (JJ (tragic)))), the PT kernel will
use both (waste (amod (JJ))) and (waste (amod (JJ
(tragic)))). We can see that the former is adding no
value to the model, as the JJ tag could indicate ei-
ther positive words (e.g. good) or negative words
(e.g. tragic). In contrast, words are good indicators
for sentiment polarity.
The results in Table 2 confirm two of our hy-
potheses. Firstly, it clearly demonstrates the val-
ue of incorporating syntactic information into the
document-level sentiment classifier, as the tree k-
ernels (CON and D*) generally outperforms vector
and sequence kernels (VK and S*). More impor-
tantly, it also shows the necessity of extracting ap-
propriate substructures when using convolution ker-
nels in our task: when using the dependency kernel
(VK+DW), the result on lexicon guided substruc-
tures (Sub) outperforms the results on document,
sentence, or randomly selected substructures, with
statistical significance (p&lt;0.05).
</bodyText>
<sectionHeader confidence="0.979157" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999966">
We studied the impact of syntactic information on
document-level sentiment classification using con-
volution kernels, and reduced the complexity of the
kernels by extracting minimal high-impact substruc-
tures, guided by a polarity lexicon. Experiments
</bodyText>
<page confidence="0.996808">
341
</page>
<bodyText confidence="0.999961222222222">
show that our method outperformed a bag-of-words
baseline with a statistically significant gain of 1.45
absolute point in accuracy.
Our research focuses on identifying and using
high-impact substructures for convolution kernels in
document-level sentiment classification. We expect
our method to be complementary with sophisticated
methods used in state-of-the-art sentiment classifica-
tion systems, which is to be explored in future work.
</bodyText>
<sectionHeader confidence="0.735206" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998224">
The authors were supported by 863 State Key
Project No. 2006AA010108, the EuroMatrixPlus F-
P7 EU project (grant No 231720) and Science Foun-
dation Ireland (Grant No. 07/CE/I1142). Part of the
research was done while Zhaopeng Tu was visiting,
and Yifan He was at the Centre for Next Generation
Localisation (www.cngl.ie), School of Computing,
Dublin City University. We thank the anonymous
reviewers for their insightful comments. We are al-
so grateful to Junhui Li for his helpful feedback.
</bodyText>
<sectionHeader confidence="0.990272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979057038461538">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, pages 30–38. Association
for Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005a. A
Shortest Path Dependency Kernel for Relation Extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Methods
in Natural Language Processing, pages 724–731, Van-
couver, British Columbia, Canada, oct. Association for
Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005b. Sub-
sequence Kernels for Relation Extraction. In Y Weis-
s, B Sch o lkopf, and J Platt, editors, Proceedings of
the 19th Conference on Neural Information Processing
Systems, pages 171–178, Cambridge, MA. MIT Press.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems, pages 625–632.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation, Manchester, August.
Paul Ferguson, Neil O’Hare, Michael Davy, Adam
Bermingham, Paraic Sheridan, Cathal Gurrin, and
Alan F. Smeaton. 2009. Exploring the use of
paragraph-level annotations for sentiment analysis of
financial blogs. In Proceedings of the Workshop on
Opinion Mining and Sentiment Analysis.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67–76, Uppsala, Sweden, July.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing Dependency Features for Opinion Mining.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 313–316, Suntec, Singapore, jul.
Suntec, Singapore.
Dan Klein and Christopher D Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan, jul. As-
sociation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2005. Using neutral
examples for learning polarity. In Proceedings of In-
ternational Joint Conferences on Artificial Intelligence
(IJCAI) 2005, pages 1616–1616.
Steve Lawrence Kushal Dave and David Pennock. 2003.
Mining the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proceed-
ings of the 12th International Conference on World
Wide Web, pages 519–528, ACM. ACM.
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Paradigm.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 161–
169, Singapore, aug. Singapore.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. Proceed-
ings of PAKDD’05, the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
3518/2005:21–32.
Alessandro Moschitti and Silvia Quarteroni. 2008. K-
ernels on Linguistic Structures for Answer Extraction.
In Proceedings of ACL-08: HLT, Short Papers, pages
113–116, Columbus, Ohio, jun. Association for Com-
putational Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193–224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning, pages 318–329, Berlin, Germany,
</reference>
<page confidence="0.980382">
342
</page>
<reference confidence="0.999878916666667">
sep. Machine Learning: ECML 2006, 17th European
Conference on Machine Learning, Proceedings.
Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin. 2006.
Examining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 611–618,
Sydney, Australia, jul. Sydney, Australia.
Truc-Vien T Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1378–1387.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 271–278, Barcelona, S-
pain, jun. Barcelona, Spain.
Michael Wiegand and Dietrich Klakow. 2010. Convolu-
tion Kernels for Opinion Holder Extraction. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 795–803, Los An-
geles, California, jun. Los Angeles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347–354, Vancouver, British Columbia, Cana-
da, oct. Association for Computational Linguistics.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Toward-
s answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
pages 129–136, Association for Computational Lin-
guistics. Association for Computational Linguistics.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Features.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 825–832, Sydney, Australia, jul. Association for
Computational Linguistics.
</reference>
<page confidence="0.99937">
343
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928332">
<title confidence="0.994431">Identifying High-Impact Sub-Structures for Convolution Kernels Document-level Sentiment Classification</title>
<author confidence="0.991141">van</author>
<affiliation confidence="0.9864545">Lab. of Intelligent Info. Processing Science Department of Computing Institute of Computing Technology, CAS New York University Dublin City</affiliation>
<abstract confidence="0.997865444444445">Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>30--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6061" citStr="Agarwal et al. (2011)" startWordPosition="887" endWordPosition="890">ts, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and convolution kernels to exploit information on surface and syn</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, pages 30–38. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A Shortest Path Dependency Kernel for Relation Extraction.</title>
<date>2005</date>
<journal>Association for Computational Linguistics.</journal>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="5652" citStr="Bunescu and Mooney, 2005" startWordPosition="826" endWordPosition="829">) represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005a. A Shortest Path Dependency Kernel for Relation Extraction. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724–731, Vancouver, British Columbia, Canada, oct. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Subsequence Kernels for Relation Extraction. In</title>
<date>2005</date>
<booktitle>Proceedings of the 19th Conference on Neural Information Processing Systems,</booktitle>
<pages>171--178</pages>
<editor>Y Weiss, B Sch o lkopf, and J Platt, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5652" citStr="Bunescu and Mooney, 2005" startWordPosition="826" endWordPosition="829">) represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005b. Subsequence Kernels for Relation Extraction. In Y Weiss, B Sch o lkopf, and J Platt, editors, Proceedings of the 19th Conference on Neural Information Processing Systems, pages 171–178, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems,</booktitle>
<pages>625--632</pages>
<contexts>
<context position="10786" citStr="Collins and Duffy, 2001" startWordPosition="1657" endWordPosition="1660">dd two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document. 4 Experiments 4.1 Setup We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of 340 1000 positive reviews and 1000 negative reviews. To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1, respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). We only take into account the subjective terms with the degree of strong subjectivity. We consider two baselines: • VK</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of Neural Information Processing Systems, pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<location>Manchester,</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation, Manchester, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ferguson</author>
<author>Neil O’Hare</author>
<author>Michael Davy</author>
<author>Adam Bermingham</author>
<author>Paraic Sheridan</author>
<author>Cathal Gurrin</author>
<author>Alan F Smeaton</author>
</authors>
<title>Exploring the use of paragraph-level annotations for sentiment analysis of financial blogs.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Opinion Mining and Sentiment Analysis.</booktitle>
<marker>Ferguson, O’Hare, Davy, Bermingham, Sheridan, Gurrin, Smeaton, 2009</marker>
<rawString>Paul Ferguson, Neil O’Hare, Michael Davy, Adam Bermingham, Paraic Sheridan, Cathal Gurrin, and Alan F. Smeaton. 2009. Exploring the use of paragraph-level annotations for sentiment analysis of financial blogs. In Proceedings of the Workshop on Opinion Mining and Sentiment Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic and semantic structure for opinion expression detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>67--76</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6002" citStr="Johansson and Moschitti (2010)" startWordPosition="877" endWordPosition="880">ncingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and co</context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010. Syntactic and semantic structure for opinion expression detection. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 67–76, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Carolyn Penstein-Rose</author>
</authors>
<title>Generalizing Dependency Features for Opinion Mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>313--316</pages>
<publisher>Suntec,</publisher>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2072" citStr="Joshi and Penstein-Rose (2009)" startWordPosition="279" endWordPosition="282">, paragraphlevel, sentence-level and phrase-level. This paper focuses on document-level sentiment classification. There has been a substantial amount of work on document-level sentiment classification. In early pioneering work, Pang and Lee (2004) use a flat feature vector (e.g., a bag-of-words) to represent the documents. A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the documents. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. In this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown </context>
<context position="5198" citStr="Joshi and Penstein-Rose (2009)" startWordPosition="760" endWordPosition="763">in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is an important first step. Several approaches propose feature-based learning algorithms for this problem. Pang and Lee (2004) and Dave et al. (2003) represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labelin</context>
</contexts>
<marker>Joshi, Penstein-Rose, 2009</marker>
<rawString>Mahesh Joshi and Carolyn Penstein-Rose. 2009. Generalizing Dependency Features for Opinion Mining. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 313–316, Suntec, Singapore, jul. Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<journal>Association for Computational Linguistics.</journal>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="10567" citStr="Klein and Manning, 2003" startWordPosition="1627" endWordPosition="1631">ctly connected to the subjective word. For instance, given the node tragic in Figure 1(d), we will extract its direct parent waste integrated with dependency relations and (possibly) POS, as in Figure 2(b). We further add two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document. 4 Experiments 4.1 Setup We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of 340 1000 positive reviews and 1000 negative reviews. To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1, respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degree</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan, jul. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
</authors>
<title>Using neutral examples for learning polarity.</title>
<date>2005</date>
<booktitle>In Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI)</booktitle>
<pages>1616--1616</pages>
<contexts>
<context position="3351" citStr="Koppel and Schler, 2005" startWordPosition="482" endWordPosition="485">of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe the plot. Such objective portions do not contain the author’s opinion and are irrelevant with respect to the sentiment classifi338 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338–343, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Kop</context>
</contexts>
<marker>Koppel, Schler, 2005</marker>
<rawString>Moshe Koppel and Jonathan Schler. 2005. Using neutral examples for learning polarity. In Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI) 2005, pages 1616–1616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Lawrence Kushal Dave</author>
<author>David Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th International Conference on World Wide Web,</booktitle>
<pages>519--528</pages>
<publisher>ACM. ACM.</publisher>
<marker>Dave, Pennock, 2003</marker>
<rawString>Steve Lawrence Kushal Dave and David Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of the 12th International Conference on World Wide Web, pages 519–528, ACM. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Liu</author>
<author>Stephanie Seneff</author>
</authors>
<title>Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>161--169</pages>
<contexts>
<context position="2098" citStr="Liu and Seneff (2009)" startWordPosition="284" endWordPosition="288">d phrase-level. This paper focuses on document-level sentiment classification. There has been a substantial amount of work on document-level sentiment classification. In early pioneering work, Pang and Lee (2004) use a flat feature vector (e.g., a bag-of-words) to represent the documents. A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the documents. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. In this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using t</context>
<context position="5273" citStr="Liu and Seneff (2009)" startWordPosition="771" endWordPosition="774">impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is an important first step. Several approaches propose feature-based learning algorithms for this problem. Pang and Lee (2004) and Dave et al. (2003) represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in se</context>
</contexts>
<marker>Liu, Seneff, 2009</marker>
<rawString>Jingjing Liu and Stephanie Seneff. 2009. Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161– 169, Singapore, aug. Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentiment classification using word sub-sequences and dependency sub-trees.</title>
<date>2005</date>
<booktitle>Proceedings of PAKDD’05, the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining,</booktitle>
<pages>3518--2005</pages>
<contexts>
<context position="5094" citStr="Matsumoto et al., (2005)" startWordPosition="746" endWordPosition="749"> 3). An empirical evaluation on a widely used sentiment corpus shows an improvement of 1.45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is an important first step. Several approaches propose feature-based learning algorithms for this problem. Pang and Lee (2004) and Dave et al. (2003) represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al.</context>
</contexts>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Shotaro Matsumoto, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment classification using word sub-sequences and dependency sub-trees. Proceedings of PAKDD’05, the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, 3518/2005:21–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
</authors>
<title>Kernels on Linguistic Structures for Answer Extraction.</title>
<date>2008</date>
<journal>Association for Computational Linguistics.</journal>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>113--116</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5775" citStr="Moschitti and Quarteroni, 2008" startWordPosition="844" endWordPosition="847">rom dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. How</context>
</contexts>
<marker>Moschitti, Quarteroni, 2008</marker>
<rawString>Alessandro Moschitti and Silvia Quarteroni. 2008. Kernels on Linguistic Structures for Answer Extraction. In Proceedings of ACL-08: HLT, Short Papers, pages 113–116, Columbus, Ohio, jun. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="2850" citStr="Moschitti et al., 2008" startWordPosition="395" endWordPosition="398">ins. In this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe </context>
<context position="5824" citStr="Moschitti et al., 2008" startWordPosition="852" endWordPosition="855">e a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than docu</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>In Proceedings of the 17th European Conference on Machine Learning,</booktitle>
<pages>318--329</pages>
<location>Berlin, Germany,</location>
<contexts>
<context position="10834" citStr="Moschitti, 2006" startWordPosition="1666" endWordPosition="1668"> (the sentences that contain subjective words), and the entire document. 4 Experiments 4.1 Setup We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of 340 1000 positive reviews and 1000 negative reviews. To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1, respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). We only take into account the subjective terms with the degree of strong subjectivity. We consider two baselines: • VK: bag-of-words features using a vector kernel (P</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of the 17th European Conference on Machine Learning, pages 318–329, Berlin, Germany, sep. Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Sajib Dasgupta</author>
<author>S M Niaz Arifin</author>
</authors>
<title>Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>611--618</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="11469" citStr="Ng et al., 2006" startWordPosition="1764" endWordPosition="1767">d dependency parse trees1, respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). We only take into account the subjective terms with the degree of strong subjectivity. We consider two baselines: • VK: bag-of-words features using a vector kernel (Pang and Lee, 2004; Ng et al., 2006) • Rand: a number of randomly selected substructures similar to the number of extracted substructures defined in Section 3.2 All experiments were carried out using the SVMLight-TK toolkit2 with default parameter settings. All results reported are based on 10-fold cross validation. 4.2 Results and Discussions Table 2 lists the results of the different kernel type combinations. The best performance is obtained by combining VK and DW kernels, gaining a significant improvement of 1.45 point in accuracy. As far as PT kernels are concerned, we find dependency trees with simple words (DW) outperform </context>
</contexts>
<marker>Ng, Dasgupta, Arifin, 2006</marker>
<rawString>Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin. 2006. Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 611–618, Sydney, Australia, jul. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1378--1387</pages>
<contexts>
<context position="5722" citStr="Nguyen et al., 2009" startWordPosition="838" endWordPosition="841"> frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. </context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution kernels on constituent, dependency and sequential structures for relation extraction. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1689" citStr="Pang and Lee (2004)" startWordPosition="223" endWordPosition="226">xicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus. 1 Introduction An important subtask in sentiment analysis is sentiment classification. Sentiment classification involves the identification of positive and negative opinions from a text segment at various levels of granularity including document-level, paragraphlevel, sentence-level and phrase-level. This paper focuses on document-level sentiment classification. There has been a substantial amount of work on document-level sentiment classification. In early pioneering work, Pang and Lee (2004) use a flat feature vector (e.g., a bag-of-words) to represent the documents. A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the documents. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. In this paper, we study and evaluate diverse linguistic s</context>
<context position="3326" citStr="Pang and Lee, 2004" startWordPosition="478" endWordPosition="481">syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe the plot. Such objective portions do not contain the author’s opinion and are irrelevant with respect to the sentiment classifi338 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338–343, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003</context>
<context position="5006" citStr="Pang and Lee (2004)" startWordPosition="731" endWordPosition="734">-structures according to these indicators for various types of parse trees (Section 3). An empirical evaluation on a widely used sentiment corpus shows an improvement of 1.45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is an important first step. Several approaches propose feature-based learning algorithms for this problem. Pang and Lee (2004) and Dave et al. (2003) represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including</context>
<context position="10390" citStr="Pang and Lee, 2004" startWordPosition="1599" endWordPosition="1602">ber of words in each tree. of words in the subtree for the sequential kernel. For a dependency tree, we only consider the subtree containing the lexical items that are directly connected to the subjective word. For instance, given the node tragic in Figure 1(d), we will extract its direct parent waste integrated with dependency relations and (possibly) POS, as in Figure 2(b). We further add two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document. 4 Experiments 4.1 Setup We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of 340 1000 positive reviews and 1000 negative reviews. To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1, respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweigh</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 271–278, Barcelona, Spain, jun. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Dietrich Klakow</author>
</authors>
<title>Convolution Kernels for Opinion Holder Extraction. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>795--803</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="5916" citStr="Wiegand and Klakow (2010)" startWordPosition="866" endWordPosition="869">jective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for S</context>
</contexts>
<marker>Wiegand, Klakow, 2010</marker>
<rawString>Michael Wiegand and Dietrich Klakow. 2010. Convolution Kernels for Opinion Holder Extraction. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 795–803, Los Angeles, California, jun. Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis.</title>
<date>2005</date>
<journal>Association for Computational Linguistics.</journal>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="4284" citStr="Wilson et al., 2005" startWordPosition="622" endWordPosition="625">tics, pages 338–343, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009). Our research is inspired by these observations. Unlike in the previous work, however, we focus on syntactic substructures (rather than entire paragraphs or sentences) that contain subjective words. More specifically, we use the terms in the lexicon constructed from (Wilson et al., 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). An empirical evaluation on a widely used sentiment corpus shows an improvement of 1.45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is a</context>
<context position="9160" citStr="Wilson et al., 2005" startWordPosition="1399" endWordPosition="1402">h POS tags inserted before words (DWP). NP waste amod tragic (b) JJ DT JJ NN A tragic waste (a) Scopes Trees Size Document 32 24 Subjective Sentences 22 27 Constituent Substructures 30 10 Dependency Substructures 40 3 Figure 2: Illustration of the different settings on constituency (CON) and dependency (DWP) parse trees with tragic as the indicator word. Unfortunately, in our task there exist several cues indicating the polarity of the document, which are distributed in different sentences. To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al., 2005). For each polarity indicator, we define the “scope” (the minimal syntactic structure containing at least one subjective word) of each indicator for different representations as follows: For a constituent tree, a node and its children correspond to a grammatical production. Therefore, considering the terminal node tragic in the constituent structure tree in Figure 1(a), we extract the subtree rooted at the grandparent of the terminal, see Figure 2(a). We also use the corresponding sequence Table 1: The detail of the corpus. Here Trees denotes the average number of trees, and Size denotes the a</context>
<context position="11117" citStr="Wilson et al., 2005" startWordPosition="1705" endWordPosition="1708">arsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1, respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). We only take into account the subjective terms with the degree of strong subjectivity. We consider two baselines: • VK: bag-of-words features using a vector kernel (Pang and Lee, 2004; Ng et al., 2006) • Rand: a number of randomly selected substructures similar to the number of extracted substructures defined in Section 3.2 All experiments were carried out using the SVMLight-TK toolkit2 with default parameter settings. All results reported are b</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada, oct. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<institution>Association for Computational Linguistics. Association for Computational Linguistics.</institution>
<contexts>
<context position="3306" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="474" endWordPosition="477">categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe the plot. Such objective portions do not contain the author’s opinion and are irrelevant with respect to the sentiment classifi338 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338–343, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Ha</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 129–136, Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features.</title>
<date>2006</date>
<journal>Association for Computational Linguistics.</journal>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>825--832</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2825" citStr="Zhang et al., 2006" startWordPosition="391" endWordPosition="394">r languages and domains. In this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective parag</context>
<context position="5700" citStr="Zhang et al., 2006" startWordPosition="834" endWordPosition="837"> al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (201</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 825–832, Sydney, Australia, jul. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>