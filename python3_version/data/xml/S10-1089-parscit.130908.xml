<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.996998">
TreeMatch: A Fully Unsupervised WSD System Using Dependency
Knowledge on a Specific Domain
</title>
<author confidence="0.988835">
Andrew Tran Chris Bowes David Brown Ping Chen
</author>
<affiliation confidence="0.96109">
University of Houston-Downtown
</affiliation>
<author confidence="0.99504">
Max Choly Wei Ding
</author>
<affiliation confidence="0.995828">
University of Massachusetts-Boston
</affiliation>
<sectionHeader confidence="0.978665" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996738">
Word sense disambiguation (WSD) is one of
the main challenges in Computational
Linguistics. TreeMatch is a WSD system
originally developed using data from SemEval
2007 Task 7 (Coarse-grained English All-
words Task) that has been adapted for use in
SemEval 2010 Task 17 (All-words Word
Sense Disambiguation on a Specific Domain).
The system is based on a fully unsupervised
method using dependency knowledge drawn
from a domain specific knowledge base that
was built for this task. When evaluated on the
task, the system precision performs above the
First Sense Baseline.
</bodyText>
<sectionHeader confidence="0.998732" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999780452380952">
There are many words within natural
languages that can have multiple meanings or
senses depending on its usage. These words are
called homographs. Word sense disambiguation
is the process of determining which sense of a
homograph is correct in a given context. Most
WSD systems use supervised methods to
identify senses and tend to achieve the best
results. However, supervised systems rely on
manually annotated training corpora.
Availability of manually tagged corpora is
limited and generating these corpora is costly
and time consuming. With our TreeMatch
system, we use a fully unsupervised domain-
independent method that only requires a
dictionary (WordNet, Fallbaum, 1998.) and
unannotated text as input (Chen et.al, 2009).
WSD systems trained on general corpora tend
to perform worse when disambiguating words
from a document on a specific domain. The
SemEval 2010 WSD-domain task (Agirre et. al.,
2010) addresses this issue by testing participant
systems on documents from the environment
domain. The environment domain specific
corpus for this task
was built from documents contributed by the
European Centre for Nature Conservation
(ECNC) and the World Wildlife Fund (WWF) .
We adapted our existing TreeMatch system from
running on a general context knowledge base to
one targeted at the environment domain.
This paper is organized as follows. Section 2
will detail the construction of the knowledge
base. In Section 3 the WSD algorithm will be
explained. The construction procedure and WSD
algorithm described in these two sections are
similar to the procedure presented in our
NAACL 2009 paper (Chen et.al, 2009). In
Section 4 we present our experiments and
results, and Section 5 discusses related work on
WSD. Section 6 finishes the paper with
conclusions.
</bodyText>
<sectionHeader confidence="0.994363" genericHeader="method">
2 Context Knowledge Acquisition and
Representation
</sectionHeader>
<figureCaption confidence="0.680688666666667">
Figure 1 shows an overview of our context
knowledge acquisition process. The collected
knowledge is saved in a local knowledge base.
Here are some details about each step.
Figure 1: Context Knowledge Acquisition and
Representation Process
</figureCaption>
<subsectionHeader confidence="0.9979">
2.1 Corpus Building Through Web Search
</subsectionHeader>
<bodyText confidence="0.99991">
The goal of this step is to collect as many
valid sample sentences as possible that contain
instances of the target word. Preferably these
instances are also diverse enough to contain all
the different glosses of a word.
</bodyText>
<page confidence="0.987158">
396
</page>
<bodyText confidence="0.976909486486487">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 396‚Äì401,
Uppsala, Sweden, 15-16 July 2010. cÔøΩ2010 Association for Computational Linguistics
The World Wide Web is a boundless source
of textual information that can be utilized for
corpus building. This huge dynamic text
collection represents a wide cross section of
writing backgrounds that may not be represented
in other corpora and may be able to better
represent common human knowledge.
However, because the content on the internet
is not necessarily checked for grammatical or
factual accuracy, concerns may arise about the
use of a corpus built from it. The quality of
context knowledge will be affected by sentences
of poor linguistic and poor word usage but from
our experience these kind of errors are negligible
when weighted against the staggering volume of
valid content also retrieved.
To start the acquisition process, words that are
candidates for disambiguation are compiled and
saved in a text file as seeds for search queries.
Each single word is submitted to a Web search
engine as a query. Several search engines
provide API‚Äôs for research communities to
automatically retrieve large number of Web
pages. In our experiments we used MSN Bing!
API (Bing!, 2010) to retrieve up to 1,000 Web
pages and PDF documents for each to-be-
disambiguated word. Collected Web pages are
cleaned first, e.g., control characters and HTML
tags are removed. Then sentences are segmented
simply based on punctuation (e.g., ?, !, .). PDF
files undergo a similar cleaning process, except
that they are converted from PDF to HMTL
beforehand. Sentences that contain the instances
of a specific word are extracted and saved into a
local repository.
</bodyText>
<subsectionHeader confidence="0.998944">
2.2 Parsing
</subsectionHeader>
<bodyText confidence="0.999994941176471">
After the sentences have been cleaned and
segmented they are sent to the dependency
parser Minipar (Lin, 1998). After parsing,
sentences are converted to parsing trees and
saved into files. The files contain the weights of
all connections between all words existing
within the knowledge base. Parsing tends to take
the most time in the entire WSD process.
Depending on the initial size of the corpus,
parsing can take weeks. The long parsing time
can be attributed to Minipar‚Äôs execution through
system calls and also to the lack of
multithreading used. However, we only need to
parse the corpus once to construct the knowledge
base. Any further parsing is only done on the
input sentences from the words to-be-
disambiguated, and the glosses of those words.
</bodyText>
<subsectionHeader confidence="0.997857">
2.3 Merging dependency relations
</subsectionHeader>
<bodyText confidence="0.998536384615385">
After parsing, dependency relations from
different sentences are merged and saved in a
context knowledge base. The merging process is
straightforward. A dependency relation includes
one head word/node and one dependent
word/node. Nodes from different dependency
relations are merged into one as long as they
represent the same word. An example is shown
in Figure 2, which merges the following two
sentences:
‚ÄúComputer programmers write software.‚Äù
‚ÄúMany companies hire computer
programmers.‚Äù
</bodyText>
<figureCaption confidence="0.99625">
Figure 2: Merging two parsing trees. The number
beside each edge is the number of occurrences of this
dependency relation existing in the context
knowledge base.
</figureCaption>
<bodyText confidence="0.999993578947368">
In a dependency relation ‚Äúword1 -&gt; word2‚Äù,
word1 is the head word, and word2 is the
dependent word. After merging dependency
relations, we will obtain a weighted directed
graph with a word as a node, a dependency
relation as an edge, and the number of
occurrences of dependency relation as weight of
an edge. This weight indicates the strength of
semantic relevancy of head word and dependent
word. This graph will be used in the following
WSD process as our context knowledge base. As
a fully automatic knowledge acquisition process,
it is inevitable to include erroneous dependency
relations in the knowledge base. However, since
in a large text collection valid dependency
relations tend to repeat far more times than
invalid ones, these erroneous edges only have
minimal impact on the disambiguation quality as
shown in our evaluation results.
</bodyText>
<page confidence="0.996494">
397
</page>
<sectionHeader confidence="0.996149" genericHeader="method">
3 WSD Algorithm
</sectionHeader>
<bodyText confidence="0.998190736842105">
Our WSD approach is based on the following
insight:
If a word is semantically coherent with its
context, then at least one sense of this word is
semantically coherent with its context.
Assuming that the documents given are
semantically coherent, if we replace a targeted
to-be-disambiguated word with its glosses one
by one, eventually one of the glosses will have
semantic coherence within the context of its
sentence. From that idea we can show the
overview of our WSD procedure in Figure 3. For
a given to-be-disambiguated word, its glosses
from WordNet are parsed one by one along with
the original sentence of the target word. The
semantic coherency between the parse tree of
each individual gloss and the parse tree of the
original sentence are compared one by one to
determine which sense is the most relevant.
</bodyText>
<figureCaption confidence="0.999397">
Figure 3: WSD Procedure
</figureCaption>
<bodyText confidence="0.755160125">
To measure the semantic coherence we use
the following hypotheses (assume word1 is the
to-be-disambiguated word):
‚Ä¢ If in a sentence word1 is dependent on word2,
and we denote the gloss of the correct sense
of word1 as g1i, then g1i contains the most
semantically coherent words that are
dependent on word2;
</bodyText>
<listItem confidence="0.900412">
‚Ä¢ If in a sentence a set of words DEP1 are
dependent on word1, and we denote the gloss
of the correct sense of word1 as g1i, then g1i
contains the most semantically coherent
words that DEP1 are dependent on.
</listItem>
<bodyText confidence="0.963783444444445">
These hypotheses are used for the functions in
Figure 4. The TreeMatching function uses what
we call dependency matching to ascertain the
correct sense of the to-be-disambiguated word.
NodeMatching function is an extension from
Lesk algorithm (Lesk, 1986).
Input: Glosses from WordNet;
S: the to-be-disambiguated sentence;
G: the knowledge base generated in Section 2;
</bodyText>
<listItem confidence="0.999643928571428">
1. Input a sentence S, W = {w |w‚Äôs part of speech
is noun, verb, adjective, or adverb, w ‚àà S};
2. Parse S with a dependency parser, generate
parsing tree TS;
3. For each w ‚ààW {
4. Input all w‚Äôs glosses from WordNet;
5. For each gloss wi {
6. Parse wi, get a parsing tree Twi;
7. scored = TreeMatching(TS, Twi);
Scoren = NodeMatching(TS, Twi);
}
8. If the highest scored and Scoren indicate
the sense, choose this sense;
9. Otherwise, choose the first sense.
</listItem>
<figure confidence="0.5800793">
10. }
TreeMatching(TS, Twi)
11. For each node nSi ‚ààTS {
12. Assign weight wSi = 1
ùëôùëôùëôùëôùëôùëô , lSi is the
length between nSi and wi in TS;
13. }
14. For each node nwi ‚àà Twi {
15. Load its dependent words Dwi from G;
16. Assign weight wwi = 1
</figure>
<bodyText confidence="0.896581">
ùëôùëôùëôùëôùëôùëô, lwi is the
level number of nwi in Twi;
</bodyText>
<listItem confidence="0.95901175">
17. For each nSj {
18. If nSj ‚àà Dwi
19. calculate connection strength sji
between nSj and nwi;
20. score = score + wSi √ó wwi √ó sji;
21. }
}
22. Return score;
</listItem>
<subsectionHeader confidence="0.422907">
NodeMatching (TS, Twi)
</subsectionHeader>
<bodyText confidence="0.72894875">
23. For each node nSi ‚ààTS {
24. Assign weight wwi = 1
ùëôùëôùëôùëôùëôùëô, lwi is the
level number of nwi in Twi;
</bodyText>
<listItem confidence="0.535395333333333">
25. For each nSj {
28. If nSi == wwi
29. score = score + wSi √ó wwi
</listItem>
<figure confidence="0.547662">
}
}
</figure>
<figureCaption confidence="0.998517">
Figure 4: WSD Algorithm
</figureCaption>
<sectionHeader confidence="0.998449" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999652">
The WSD-domain task for SemEval 2010
focused on the environment domain. To prepare
for the tests, we constructed a new domain
specific knowledge base.
</bodyText>
<page confidence="0.99658">
398
</page>
<table confidence="0.999732166666667">
System Precision Recall
1sense 0.505 0.505
TreeMatch-1 0.506 0.493
TreeMatch-2 0.504 0.491
TreeMatch-3 0.492 0.479
Random 0.23 0.23
</table>
<tableCaption confidence="0.96291">
Table 1: Fine-Grained SemEval 2010 Task 17
Disambiguation Scores
</tableCaption>
<bodyText confidence="0.999956311111111">
Since we knew the task‚Äôs domain specific
corpus would be derived from ECNC and WWF
materials, we produced our query list from the
same source. A web crawl starting from both the
ECNC and WWF main web pages was
performed that retrieved 772 PDF documents.
Any words that were in the PDFs and also had
more than one gloss in WordNet were retained
for Bing! search queries to start the acquisition
process as described in section 2. 10779 unique
words were obtained in this manner.
Using the 10779 unique words for search
queries, the web page and PDF retrieval step
took 35 days, collecting over 3 TB of raw html
and PDF files, and the cleaning and sentence
extraction step took 2 days, reducing it down to
3 GB of relevant sentences, while running on 5
machines. Parsing took 26 days and merging
took 6 days on 9 machines. From the parse trees
we obtained 2202295 total nodes with an
average of 87 connections and 13 dependents per
node.
Each machine was a 2.66 GHz dual core PC
with 2 GB of memory with a total of 10
machines used throughout the process.
There were 3 test documents provided by the
task organizers with about 6000 total words and
1398 to-be-disambiguated words.
Disambiguation of the target words took 1.5
hours for each complete run. Each run used the
same WSD procedure with different parameters.
The overall disambiguation results are shown
in Table 1. The precision of our best submission
edged out the First Sense Baseline (1sense)
baseline by .001 and is ahead of the Random
selection baseline by .276.
The recall of our submissions is lower than
the precision because of our reliance on Minipar
for the part of speech and lemma information of
the target words. Sometimes Minipar would give
an incorrect lemma which at times cannot be
found in WordNet and thus our system would
not attempt to disambiguate the words. Previous
tasks provided the lemma and part of speech for
target words so we were able to bypass that step.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.9869015">
Generally WSD techniques can be divided into
four categories (Agirre, 2006),
</bodyText>
<listItem confidence="0.907757416666667">
‚Ä¢ Dictionary and knowledge based methods.
These methods use lexical knowledge bases
(LKB) such as dictionaries and thesauri, and
extract knowledge from word definitions
(Lesk, 1986) and relations among
words/senses. Recently, several graph-based
WSD methods were proposed. In these
approaches, first a graph is built with senses
as nodes and relations among words/senses
(e.g., synonymy, antonymy) as edges, and
the relations are usually acquired from a
LKB (e.g., Wordnet). Then a ranking
algorithm is conducted over the graph, and
senses ranked the highest are assigned to the
corresponding words. Different relations and
ranking algorithms were experimented with
these methods, such as TexRank algorithm
(Mihalcea, 2005), personalized PageRank
algorithm (Agirre, 2009), a two-stage
searching algorithm (Navigli, 2007),
Structural Semantic Interconnections
algorithm (Navigli, 2005), centrality
algorithms (Sinha, 2009).
‚Ä¢ Supervised methods. A supervised method
</listItem>
<bodyText confidence="0.972483523809524">
includes a training phase and a testing phase.
In the training phase, a sense-annotated
training corpus is required, from which
syntactic and semantic features are extracted
to build a classifier using machine learning
techniques, such as Support Vector Machine
(Novisch, 2007). In the following testing
phase, the classifier picks the best sense for a
word based on its surrounding words
(Mihalcea, 2002). Currently supervised
methods achieved the best disambiguation
quality (about 80% in precision and recall
for coarse-grained WSD in the most recent
WSD evaluation conference SemEval 2007
(Novisch, 2007). Nevertheless, since training
corpora are manually annotated and
expensive, supervised methods are often
brittle due to data scarcity, and it is
impractical to manually annotate huge
number of words existing in a natural
language.
</bodyText>
<listItem confidence="0.767896">
‚Ä¢ Semi-supervised methods. To overcome the
knowledge acquisition bottleneck suffered in
supervised methods, semi-supervised
methods make use of a small annotated
corpus as seed data in a bootstrapping
process (Hearst, 1991) (Yarowsky, 1995). A
</listItem>
<page confidence="0.997355">
399
</page>
<bodyText confidence="0.93593856">
word-aligned bilingual corpus can also serve
as seed data (Zhong, 2009).
‚Ä¢ Unsupervised methods. These methods
acquire knowledge from unannotated raw
text, and induce senses using similarity
measures (Lin, 1997). Unsupervised
methods overcome the problem of
knowledge acquisition bottleneck, but none
of existing methods can outperform the most
frequent sense baseline, which makes them
not useful at all in practice. The best
unsupervised systems only achieved about
70% in precision and 50% in recall in the
SemEval 2007 (Navigli, 2007). One recent
study utilized automatically acquired
dependency knowledge and achieved 73% in
precision and recall (Chen, 2009), which is
still below the most-frequent-sense baseline
(78.89% in precision and recall in the
SemEval 2007 Task 07).
Additionally there exist some ‚Äúmeta-
disambiguation‚Äù methods that ensemble multiple
disambiguation algorithms following the ideas of
bagging or boosting in supervised learning
(Brody, 2006).
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998943">
This paper has described a WSD system
which has been adapted for use in a specific
domain for SemEval 2010 Task 17: All-Words
Word Sense Disambiguation on a Specific
Domain. Our system has shown that domain
adaptation can be handled by unsupervised
systems without the brittleness of supervised
methods by utilizing readily available
unannotated text from internet sources and still
achieve viable results.
</bodyText>
<sectionHeader confidence="0.997674" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994624">
This work is partially funded by National
Science Foundation grants CNS 0851984 and
DHS #2009-ST-061-C10001.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999548308823529">
E. Agirre, Philip Edmonds, editors. Word Sense
Disambiguation: Algorithms and Applications,
Springer. 2006.
E. Agirre, O. Lopez de Lacalle, C. Fellbaum, S.
Hsieh, M. Tesconi, P. Vossen, and R. Segers.
SemEval-2010 Task 17: All-words Word Sense
Disambiguation on a Specific Domain. In
Proceedings of the 5th International Workshop on
Semantic Evaluations(SemEval-2010),
Association for Computational Linguistics,
Uppsala, Sweden. 2010.
E. Agirre, A. Soroa. Personalizing pagerank for word
sense disambiguation. In Proceedings of the 12th
conference of the European chapter of the
Association for Computational Linguistics
(EACL-2009).
Bing! API, available at msdn.microsoft.com
A. Brody, R. Navigli, M. Lapata, Ensemble Methods
For Unsupervised WSD, COLING-ACL, 2006
P. Chen, W. Ding, C. Bowes, D. Brown. 2009. A
Fully Unsupervised Word Sense Disambiguation
Method and Its Evaluation on Coarse-grained All-
words Task, NAACL 2009.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database, MIT press, 1998
M. Hearst. Noun Homograph Disambiguation Using
Local Context in Large Text Corpora. Proc. 7th
Annual Conference of the Univ. of Waterloo
Center for the New OED and Text Research,
Oxford. 1991.
M. Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th Annual international Conference on
Systems Documentation (Toronto, Ontario,
Canada). V. DeBuys, Ed. SIGDOC ‚Äô86.
D. Lin. Using syntactic dependency as local context
to resolve word sense ambiguity. In Proceedings
of the 35th Annual Meeting of the Association For
Computational Linguistics and Eighth Conference
of the European Chapter of the Association For
Computational Linguistics. 1997.
D. Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the LREC Workshop
on the Evaluation of Parsing Systems, pages 234‚Äì
241, Granada, Spain.
R. Mihalcea. Unsupervised Large-Vocabulary Word
Sense Disambiguation with Graph-based
Algorithms for Sequence Data Labeling, in
Proceedings of the Joint Conference on Human
Language Technology Empirical Methods in
Natural Language Processing (HLT/EMNLP),
Vancouver, October, 2005.
R. Mihalcea. Instance based learning with automatic
feature selection applied to word sense
disambiguation. In Proceedings of the 19th
International Conference on Computational
linguistics. 2002.
R. Navigli, Mirella Lapata. Graph Connectivity
Measures for Unsupervised Word Sense
Disambiguation. IJCAI 2007
R. Navigli, Paola Velardi. Structural semantic
interconnections: a knowledge-based approach to
word sense disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence
(PAMI), 27(7):1063-1074. 2005.
A. Novischi, Muirathnam Srikanth, and Andrew
Bennett. Lcc-wsd: System description for English
</reference>
<page confidence="0.963638">
400
</page>
<reference confidence="0.999108315789474">
coarse grained all words task at semeval 2007.
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages
223--226, Prague, Czech Republic. 2007.
R. Sinha, Rada Mihalcea. Unsupervised Graph-based
Word Sense Disambiguation, in ‚ÄúCurrent Issues in
Linguistic Theory: Recent Advances in Natural
Language Processing‚Äù, Editors Nicolas Nicolov
and Ruslan Mitkov, John Benjamins, 2009.
D. Yarowsky. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting on
Association For Computational Linguistics,
Cambridge, Massachusetts, 1995.
Z. Zhong, Hwee Tou Ng. Word Sense
Disambiguation for All Words without Hard
Labor. In Proceeding of the Twenty-first
International Joint Conference on Artificial
Intelligence. 2009.
</reference>
<page confidence="0.998643">
401
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.703272">
<title confidence="0.9984015">TreeMatch: A Fully Unsupervised WSD System Using Dependency Knowledge on a Specific Domain</title>
<author confidence="0.999771">Andrew Tran Chris Bowes David Brown Ping Chen</author>
<affiliation confidence="0.996">University of Houston-Downtown</affiliation>
<author confidence="0.951967">Max Choly Wei Ding</author>
<affiliation confidence="0.999154">University of Massachusetts-Boston</affiliation>
<abstract confidence="0.981967333333333">Word sense disambiguation (WSD) is one of the main challenges in Computational Linguistics. TreeMatch is a WSD system originally developed using data from SemEval 2007 Task 7 (Coarse-grained English Allwords Task) that has been adapted for use in SemEval 2010 Task 17 (All-words Word Sense Disambiguation on a Specific Domain). The system is based on a fully unsupervised method using dependency knowledge drawn from a domain specific knowledge base that was built for this task. When evaluated on the task, the system precision performs above the First Sense Baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2006</date>
<booktitle>Word Sense Disambiguation: Algorithms and Applications,</booktitle>
<editor>E. Agirre, Philip Edmonds, editors.</editor>
<publisher>Springer.</publisher>
<marker>2006</marker>
<rawString>E. Agirre, Philip Edmonds, editors. Word Sense Disambiguation: Algorithms and Applications, Springer. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O Lopez de Lacalle</author>
<author>C Fellbaum</author>
<author>S Hsieh</author>
<author>M Tesconi</author>
<author>P Vossen</author>
<author>R Segers</author>
</authors>
<title>SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations(SemEval-2010), Association for Computational Linguistics,</booktitle>
<location>Uppsala,</location>
<marker>Agirre, de Lacalle, Fellbaum, Hsieh, Tesconi, Vossen, Segers, 2010</marker>
<rawString>E. Agirre, O. Lopez de Lacalle, C. Fellbaum, S. Hsieh, M. Tesconi, P. Vossen, and R. Segers. SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain. In Proceedings of the 5th International Workshop on Semantic Evaluations(SemEval-2010), Association for Computational Linguistics, Uppsala, Sweden. 2010.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<booktitle>In Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL-2009).</booktitle>
<marker>Agirre, Soroa, </marker>
<rawString>E. Agirre, A. Soroa. Personalizing pagerank for word sense disambiguation. In Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL-2009).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bing API</author>
</authors>
<note>available at msdn.microsoft.com</note>
<marker>API, </marker>
<rawString>Bing! API, available at msdn.microsoft.com</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Brody</author>
<author>R Navigli</author>
<author>M Lapata</author>
</authors>
<title>Ensemble Methods For Unsupervised WSD, COLING-ACL,</title>
<date>2006</date>
<marker>Brody, Navigli, Lapata, 2006</marker>
<rawString>A. Brody, R. Navigli, M. Lapata, Ensemble Methods For Unsupervised WSD, COLING-ACL, 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chen</author>
<author>W Ding</author>
<author>C Bowes</author>
<author>D Brown</author>
</authors>
<title>A Fully Unsupervised Word Sense Disambiguation Method and Its Evaluation on Coarse-grained Allwords Task,</title>
<date>2009</date>
<location>NAACL</location>
<marker>Chen, Ding, Bowes, Brown, 2009</marker>
<rawString>P. Chen, W. Ding, C. Bowes, D. Brown. 2009. A Fully Unsupervised Word Sense Disambiguation Method and Its Evaluation on Coarse-grained Allwords Task, NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<publisher>MIT press,</publisher>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database, MIT press, 1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Noun Homograph Disambiguation Using Local Context in Large Text Corpora.</title>
<date>1991</date>
<booktitle>Proc. 7th Annual Conference of the Univ. of Waterloo Center for the New OED and Text Research,</booktitle>
<location>Oxford.</location>
<contexts>
<context position="14471" citStr="Hearst, 1991" startWordPosition="2354" endWordPosition="2355">est disambiguation quality (about 80% in precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Novisch, 2007). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is impractical to manually annotate huge number of words existing in a natural language. ‚Ä¢ Semi-supervised methods. To overcome the knowledge acquisition bottleneck suffered in supervised methods, semi-supervised methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A 399 word-aligned bilingual corpus can also serve as seed data (Zhong, 2009). ‚Ä¢ Unsupervised methods. These methods acquire knowledge from unannotated raw text, and induce senses using similarity measures (Lin, 1997). Unsupervised methods overcome the problem of knowledge acquisition bottleneck, but none of existing methods can outperform the most frequent sense baseline, which makes them not useful at all in practice. The best unsupervised systems only achieved about 70% in precision and 50% in recall in the SemEval 2007 (Navigli, 2007). One recent study utilized automatic</context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>M. Hearst. Noun Homograph Disambiguation Using Local Context in Large Text Corpora. Proc. 7th Annual Conference of the Univ. of Waterloo Center for the New OED and Text Research, Oxford. 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<journal>SIGDOC</journal>
<booktitle>In Proceedings of the 5th Annual international Conference on Systems Documentation</booktitle>
<volume>86</volume>
<location>(Toronto, Ontario, Canada).</location>
<contexts>
<context position="8806" citStr="Lesk, 1986" startWordPosition="1404" endWordPosition="1405">, and we denote the gloss of the correct sense of word1 as g1i, then g1i contains the most semantically coherent words that are dependent on word2; ‚Ä¢ If in a sentence a set of words DEP1 are dependent on word1, and we denote the gloss of the correct sense of word1 as g1i, then g1i contains the most semantically coherent words that DEP1 are dependent on. These hypotheses are used for the functions in Figure 4. The TreeMatching function uses what we call dependency matching to ascertain the correct sense of the to-be-disambiguated word. NodeMatching function is an extension from Lesk algorithm (Lesk, 1986). Input: Glosses from WordNet; S: the to-be-disambiguated sentence; G: the knowledge base generated in Section 2; 1. Input a sentence S, W = {w |w‚Äôs part of speech is noun, verb, adjective, or adverb, w ‚àà S}; 2. Parse S with a dependency parser, generate parsing tree TS; 3. For each w ‚ààW { 4. Input all w‚Äôs glosses from WordNet; 5. For each gloss wi { 6. Parse wi, get a parsing tree Twi; 7. scored = TreeMatching(TS, Twi); Scoren = NodeMatching(TS, Twi); } 8. If the highest scored and Scoren indicate the sense, choose this sense; 9. Otherwise, choose the first sense. 10. } TreeMatching(TS, Twi) </context>
<context position="12624" citStr="Lesk, 1986" startWordPosition="2093" endWordPosition="2094">n Minipar for the part of speech and lemma information of the target words. Sometimes Minipar would give an incorrect lemma which at times cannot be found in WordNet and thus our system would not attempt to disambiguate the words. Previous tasks provided the lemma and part of speech for target words so we were able to bypass that step. 5 Related work Generally WSD techniques can be divided into four categories (Agirre, 2006), ‚Ä¢ Dictionary and knowledge based methods. These methods use lexical knowledge bases (LKB) such as dictionaries and thesauri, and extract knowledge from word definitions (Lesk, 1986) and relations among words/senses. Recently, several graph-based WSD methods were proposed. In these approaches, first a graph is built with senses as nodes and relations among words/senses (e.g., synonymy, antonymy) as edges, and the relations are usually acquired from a LKB (e.g., Wordnet). Then a ranking algorithm is conducted over the graph, and senses ranked the highest are assigned to the corresponding words. Different relations and ranking algorithms were experimented with these methods, such as TexRank algorithm (Mihalcea, 2005), personalized PageRank algorithm (Agirre, 2009), a two-st</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual international Conference on Systems Documentation (Toronto, Ontario, Canada). V. DeBuys, Ed. SIGDOC ‚Äô86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association For Computational Linguistics and Eighth Conference of the European Chapter of the Association For Computational Linguistics.</booktitle>
<contexts>
<context position="14707" citStr="Lin, 1997" startWordPosition="2388" endWordPosition="2389">rvised methods are often brittle due to data scarcity, and it is impractical to manually annotate huge number of words existing in a natural language. ‚Ä¢ Semi-supervised methods. To overcome the knowledge acquisition bottleneck suffered in supervised methods, semi-supervised methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A 399 word-aligned bilingual corpus can also serve as seed data (Zhong, 2009). ‚Ä¢ Unsupervised methods. These methods acquire knowledge from unannotated raw text, and induce senses using similarity measures (Lin, 1997). Unsupervised methods overcome the problem of knowledge acquisition bottleneck, but none of existing methods can outperform the most frequent sense baseline, which makes them not useful at all in practice. The best unsupervised systems only achieved about 70% in precision and 50% in recall in the SemEval 2007 (Navigli, 2007). One recent study utilized automatically acquired dependency knowledge and achieved 73% in precision and recall (Chen, 2009), which is still below the most-frequent-sense baseline (78.89% in precision and recall in the SemEval 2007 Task 07). Additionally there exist some </context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>D. Lin. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of the 35th Annual Meeting of the Association For Computational Linguistics and Eighth Conference of the European Chapter of the Association For Computational Linguistics. 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC Workshop on the Evaluation of Parsing Systems,</booktitle>
<pages>234--241</pages>
<location>Granada,</location>
<contexts>
<context position="4992" citStr="Lin, 1998" startWordPosition="781" endWordPosition="782">N Bing! API (Bing!, 2010) to retrieve up to 1,000 Web pages and PDF documents for each to-bedisambiguated word. Collected Web pages are cleaned first, e.g., control characters and HTML tags are removed. Then sentences are segmented simply based on punctuation (e.g., ?, !, .). PDF files undergo a similar cleaning process, except that they are converted from PDF to HMTL beforehand. Sentences that contain the instances of a specific word are extracted and saved into a local repository. 2.2 Parsing After the sentences have been cleaned and segmented they are sent to the dependency parser Minipar (Lin, 1998). After parsing, sentences are converted to parsing trees and saved into files. The files contain the weights of all connections between all words existing within the knowledge base. Parsing tends to take the most time in the entire WSD process. Depending on the initial size of the corpus, parsing can take weeks. The long parsing time can be attributed to Minipar‚Äôs execution through system calls and also to the lack of multithreading used. However, we only need to parse the corpus once to construct the knowledge base. Any further parsing is only done on the input sentences from the words to-be</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the LREC Workshop on the Evaluation of Parsing Systems, pages 234‚Äì 241, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised Large-Vocabulary Word Sense Disambiguation with Graph-based Algorithms for Sequence Data Labeling,</title>
<date>2005</date>
<booktitle>in Proceedings of the Joint Conference on Human Language Technology Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<location>Vancouver,</location>
<contexts>
<context position="13166" citStr="Mihalcea, 2005" startWordPosition="2172" endWordPosition="2173">es and thesauri, and extract knowledge from word definitions (Lesk, 1986) and relations among words/senses. Recently, several graph-based WSD methods were proposed. In these approaches, first a graph is built with senses as nodes and relations among words/senses (e.g., synonymy, antonymy) as edges, and the relations are usually acquired from a LKB (e.g., Wordnet). Then a ranking algorithm is conducted over the graph, and senses ranked the highest are assigned to the corresponding words. Different relations and ranking algorithms were experimented with these methods, such as TexRank algorithm (Mihalcea, 2005), personalized PageRank algorithm (Agirre, 2009), a two-stage searching algorithm (Navigli, 2007), Structural Semantic Interconnections algorithm (Navigli, 2005), centrality algorithms (Sinha, 2009). ‚Ä¢ Supervised methods. A supervised method includes a training phase and a testing phase. In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to build a classifier using machine learning techniques, such as Support Vector Machine (Novisch, 2007). In the following testing phase, the classifier picks the best sense for a word </context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>R. Mihalcea. Unsupervised Large-Vocabulary Word Sense Disambiguation with Graph-based Algorithms for Sequence Data Labeling, in Proceedings of the Joint Conference on Human Language Technology Empirical Methods in Natural Language Processing (HLT/EMNLP), Vancouver, October, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational linguistics.</booktitle>
<contexts>
<context position="13813" citStr="Mihalcea, 2002" startWordPosition="2261" endWordPosition="2262">m (Agirre, 2009), a two-stage searching algorithm (Navigli, 2007), Structural Semantic Interconnections algorithm (Navigli, 2005), centrality algorithms (Sinha, 2009). ‚Ä¢ Supervised methods. A supervised method includes a training phase and a testing phase. In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to build a classifier using machine learning techniques, such as Support Vector Machine (Novisch, 2007). In the following testing phase, the classifier picks the best sense for a word based on its surrounding words (Mihalcea, 2002). Currently supervised methods achieved the best disambiguation quality (about 80% in precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Novisch, 2007). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is impractical to manually annotate huge number of words existing in a natural language. ‚Ä¢ Semi-supervised methods. To overcome the knowledge acquisition bottleneck suffered in supervised methods, semi-supervised methods make use of a small annotated cor</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>R. Mihalcea. Instance based learning with automatic feature selection applied to word sense disambiguation. In Proceedings of the 19th International Conference on Computational linguistics. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Mirella Lapata. Graph Connectivity Measures for Unsupervised Word Sense Disambiguation. IJCAI</title>
<date>2007</date>
<contexts>
<context position="13263" citStr="Navigli, 2007" startWordPosition="2183" endWordPosition="2184">s/senses. Recently, several graph-based WSD methods were proposed. In these approaches, first a graph is built with senses as nodes and relations among words/senses (e.g., synonymy, antonymy) as edges, and the relations are usually acquired from a LKB (e.g., Wordnet). Then a ranking algorithm is conducted over the graph, and senses ranked the highest are assigned to the corresponding words. Different relations and ranking algorithms were experimented with these methods, such as TexRank algorithm (Mihalcea, 2005), personalized PageRank algorithm (Agirre, 2009), a two-stage searching algorithm (Navigli, 2007), Structural Semantic Interconnections algorithm (Navigli, 2005), centrality algorithms (Sinha, 2009). ‚Ä¢ Supervised methods. A supervised method includes a training phase and a testing phase. In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to build a classifier using machine learning techniques, such as Support Vector Machine (Novisch, 2007). In the following testing phase, the classifier picks the best sense for a word based on its surrounding words (Mihalcea, 2002). Currently supervised methods achieved the best d</context>
<context position="15034" citStr="Navigli, 2007" startWordPosition="2438" endWordPosition="2439">seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A 399 word-aligned bilingual corpus can also serve as seed data (Zhong, 2009). ‚Ä¢ Unsupervised methods. These methods acquire knowledge from unannotated raw text, and induce senses using similarity measures (Lin, 1997). Unsupervised methods overcome the problem of knowledge acquisition bottleneck, but none of existing methods can outperform the most frequent sense baseline, which makes them not useful at all in practice. The best unsupervised systems only achieved about 70% in precision and 50% in recall in the SemEval 2007 (Navigli, 2007). One recent study utilized automatically acquired dependency knowledge and achieved 73% in precision and recall (Chen, 2009), which is still below the most-frequent-sense baseline (78.89% in precision and recall in the SemEval 2007 Task 07). Additionally there exist some ‚Äúmetadisambiguation‚Äù methods that ensemble multiple disambiguation algorithms following the ideas of bagging or boosting in supervised learning (Brody, 2006). 6 Conclusion This paper has described a WSD system which has been adapted for use in a specific domain for SemEval 2010 Task 17: All-Words Word Sense Disambiguation on </context>
</contexts>
<marker>Navigli, 2007</marker>
<rawString>R. Navigli, Mirella Lapata. Graph Connectivity Measures for Unsupervised Word Sense Disambiguation. IJCAI 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: a knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),</journal>
<pages>27--7</pages>
<marker>Navigli, Velardi, 2005</marker>
<rawString>R. Navigli, Paola Velardi. Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 27(7):1063-1074. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Novischi</author>
<author>Muirathnam Srikanth</author>
<author>Andrew Bennett</author>
</authors>
<title>Lcc-wsd: System description for English coarse grained all words task at semeval</title>
<date>2007</date>
<booktitle>Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>223--226</pages>
<location>Prague, Czech Republic.</location>
<marker>Novischi, Srikanth, Bennett, 2007</marker>
<rawString>A. Novischi, Muirathnam Srikanth, and Andrew Bennett. Lcc-wsd: System description for English coarse grained all words task at semeval 2007. Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 223--226, Prague, Czech Republic. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sinha</author>
</authors>
<title>Rada Mihalcea. Unsupervised Graph-based Word Sense Disambiguation, in ‚ÄúCurrent Issues in Linguistic Theory: Recent Advances in Natural Language Processing‚Äù, Editors Nicolas Nicolov and Ruslan Mitkov,</title>
<date>2009</date>
<location>John Benjamins,</location>
<contexts>
<context position="13364" citStr="Sinha, 2009" startWordPosition="2193" endWordPosition="2194">s built with senses as nodes and relations among words/senses (e.g., synonymy, antonymy) as edges, and the relations are usually acquired from a LKB (e.g., Wordnet). Then a ranking algorithm is conducted over the graph, and senses ranked the highest are assigned to the corresponding words. Different relations and ranking algorithms were experimented with these methods, such as TexRank algorithm (Mihalcea, 2005), personalized PageRank algorithm (Agirre, 2009), a two-stage searching algorithm (Navigli, 2007), Structural Semantic Interconnections algorithm (Navigli, 2005), centrality algorithms (Sinha, 2009). ‚Ä¢ Supervised methods. A supervised method includes a training phase and a testing phase. In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to build a classifier using machine learning techniques, such as Support Vector Machine (Novisch, 2007). In the following testing phase, the classifier picks the best sense for a word based on its surrounding words (Mihalcea, 2002). Currently supervised methods achieved the best disambiguation quality (about 80% in precision and recall for coarse-grained WSD in the most recent WS</context>
</contexts>
<marker>Sinha, 2009</marker>
<rawString>R. Sinha, Rada Mihalcea. Unsupervised Graph-based Word Sense Disambiguation, in ‚ÄúCurrent Issues in Linguistic Theory: Recent Advances in Natural Language Processing‚Äù, Editors Nicolas Nicolov and Ruslan Mitkov, John Benjamins, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting on Association For Computational Linguistics,</booktitle>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="14488" citStr="Yarowsky, 1995" startWordPosition="2356" endWordPosition="2357">ion quality (about 80% in precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Novisch, 2007). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is impractical to manually annotate huge number of words existing in a natural language. ‚Ä¢ Semi-supervised methods. To overcome the knowledge acquisition bottleneck suffered in supervised methods, semi-supervised methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A 399 word-aligned bilingual corpus can also serve as seed data (Zhong, 2009). ‚Ä¢ Unsupervised methods. These methods acquire knowledge from unannotated raw text, and induce senses using similarity measures (Lin, 1997). Unsupervised methods overcome the problem of knowledge acquisition bottleneck, but none of existing methods can outperform the most frequent sense baseline, which makes them not useful at all in practice. The best unsupervised systems only achieved about 70% in precision and 50% in recall in the SemEval 2007 (Navigli, 2007). One recent study utilized automatically acquired dep</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting on Association For Computational Linguistics, Cambridge, Massachusetts, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhong</author>
</authors>
<title>Hwee Tou Ng. Word Sense Disambiguation for All Words without Hard Labor.</title>
<date>2009</date>
<booktitle>In Proceeding of the Twenty-first International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="14567" citStr="Zhong, 2009" startWordPosition="2369" endWordPosition="2370">recent WSD evaluation conference SemEval 2007 (Novisch, 2007). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is impractical to manually annotate huge number of words existing in a natural language. ‚Ä¢ Semi-supervised methods. To overcome the knowledge acquisition bottleneck suffered in supervised methods, semi-supervised methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A 399 word-aligned bilingual corpus can also serve as seed data (Zhong, 2009). ‚Ä¢ Unsupervised methods. These methods acquire knowledge from unannotated raw text, and induce senses using similarity measures (Lin, 1997). Unsupervised methods overcome the problem of knowledge acquisition bottleneck, but none of existing methods can outperform the most frequent sense baseline, which makes them not useful at all in practice. The best unsupervised systems only achieved about 70% in precision and 50% in recall in the SemEval 2007 (Navigli, 2007). One recent study utilized automatically acquired dependency knowledge and achieved 73% in precision and recall (Chen, 2009), which </context>
</contexts>
<marker>Zhong, 2009</marker>
<rawString>Z. Zhong, Hwee Tou Ng. Word Sense Disambiguation for All Words without Hard Labor. In Proceeding of the Twenty-first International Joint Conference on Artificial Intelligence. 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>