<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000518">
<title confidence="0.579204">
Features for unsupervised document classification
</title>
<author confidence="0.28201">
S H Srinivasan
</author>
<affiliation confidence="0.2764295">
Applied Research Group
Satyam Computer Services Ltd.
</affiliation>
<address confidence="0.886584">
14 Langford Avenue, Lalbagh Road
Bangalore - 560 025, INDIA
</address>
<email confidence="0.996697">
SH_Srinivasan@satyam.com
</email>
<sectionHeader confidence="0.979881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999634333333333">
Unsupervised document classification is an im-
portant problem in practical text mining since
training data is seldom available. In this pa-
per we study the problem of term selection and
the performance of various features for unsuper-
vised text classification. The features studied
are: principal components, independent com-
ponents, and non-negative components. The
clustering algorithm used is based on bipartite
graph partitioning (Zha et al., 2001). The eval-
uation is performed using the newsgroups cor-
pus.
</bodyText>
<sectionHeader confidence="0.996304" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998553412698413">
Many natural language processing applications
require text classification. Labeled data for
training is typically unavailable in many situa-
tions. So unsupervised classification techniques
are called for. Classification — both supervised
and unsupervised — is usually done in two steps:
feature extraction and classification. Feature
extraction is basically a change of representa-
tion of the raw data.
The most common representation used in
document retrieval is the vector representation
or the &amp;quot;bag-of-words&amp;quot; feature representation
(Salton, 1989). Each document is represented
as a vector in the term space. Let ti, t2, • • • ,t
be the terms we use to represent the docu-
ments.&apos; This collection usually ignores very
high and very low frequency terms. The terms
can be arranged in some convenient order. The
documents are represented as vectors of dimen-
sion n. Let v be the vector corresponding to a
document d. We set v, to 1 if term t, occurs
in d. Otherwise v, is set to 0. This represen-
tation uses information about presence or ab-
&apos;Terms are are obtained from words after stemming.
sence of terms in the document. All the other
information is ignored. Some representations
use frequency information. The vectors corre-
sponding to several documents can be arranged
in a matrix — the so called &amp;quot;term-document&amp;quot;
matrix. See (Berry et al., 1999) for a review of
vector space representation. Other representa-
tion schemes can be in the vector space frame-
work.
Classification is done in the feature space.
Several techniques exist for classification — naive
bayes, support vector machines, k-means clus-
tering, neural networks etc. See (Sebastiani,
2002) for a comprehensive survey. In this paper,
we use the bipartite graph matching technique
proposed in (Zha et al., 2001) for classification.
See also (Shi and Malik, 1997). Bipartite graph
matching (also known as spectral clustering)
has several attractive properties including the
property that if the data has a &amp;quot;good&amp;quot; cluster-
ing, the algorithm will find an &amp;quot;optimal&amp;quot; one.
See (Kannan et al., 2000) for definitions and
details.
The term-by-document matrix can be consid-
ered as a representation of a weighted bipartite
graph — the vertex sets being terms (T) and
documents (D). The clustering technique par-
titions the vertices into disjoint sets (T = TUT&apos;
and D = D1UD) such that intra cluster weight
(between T1 &amp; Di and Tf &amp; DO is maximized
and inter cluster weight (between T1 &amp; D and
Tic &amp; Di) is minimized.
The overall classification scheme is shown in
figure 1. It can be seen that the final clus-
tering scheme depends on the connectivity and
weights of the bipartite graph. Any scheme
which changes the connectivity or weights is
likely to change the clustering results. In this
paper we explore the following aspects.
</bodyText>
<table confidence="0.8461638">
Documents Term Change of Bipartite Clustering
_,.. Selection Basis -1,.. Graph
Representation
-1,.. -I.
Classes
</table>
<figureCaption confidence="0.987894">
Figure 1: Document classification scheme.
</figureCaption>
<bodyText confidence="0.999069185185185">
term selection: The terms used for represent-
ing the documents are crucial to the suc-
cess of the classification scheme. Several
techniques exist for term selection: docu-
ment frequency, information gain, etc (Se-
bastiani, 2002). In this paper, we use an
iterative technique in conjunction with in-
formation gain.
basis selection: Terms form the basis of the
original representation. These bases may
not be optimal for the classification task.
We explore the following representation
schemes: singular vectors, independent
vectors, and non-negative parts.
While these techniques have been applied to
document processing (for example, (Dumais et
al., 1988), (Kolenda and Hansen, 2000), (Lee
and Seung, 1999)), there is no systematic com-
parison of these techniques for unsupervised
document classification. The same clustering al-
gorithm — bipartite graph partitioning — is used
in the final stage.
The paper is organized as follows. Section 2
introduces the representation schemes used, sec-
tion 3 describes the bipartite graph partitioning,
and section 4 lists the experimental setting and
results.
</bodyText>
<sectionHeader confidence="0.952247" genericHeader="method">
2 Feature representation schemes
</sectionHeader>
<bodyText confidence="0.999961533333333">
The term-by-document matrix is a representa-
tion of the documents using one type of basis:
terms. It is possible to extract other underlying
bases which may aid classification. For example,
matrix diagonalization (Strang, 1980) produces
an orthogonal collection of basis vectors that are
ordered according to their &amp;quot;importance&amp;quot;.
In this section, we provide a brief descrip-
tion of singular value decomposition, indepen-
dent component analysis, and non-negative ma-
trix factorization. In the following discussion,
let A be the m x n document-by-term matrix.
Here m is the number of documents and n is the
number of words (or terms) used in the repre-
sentation.
</bodyText>
<subsectionHeader confidence="0.918731">
2.1 Singular value decomposition
</subsectionHeader>
<bodyText confidence="0.999920666666667">
Matrix diagonalization can be performed only
on square matrices. In contrast, singular value
decomposition (SVD) exists for rectangular ma-
trices also. SVD is a generalization of matrix
diagonalization. The SVD of A can be written
as
</bodyText>
<equation confidence="0.617607">
A = UEVT
</equation>
<bodyText confidence="0.999960666666667">
where U and V are m x m and n x n orthog-
onal matrices and E is amxn diagonal ma-
trix (Strang, 1980). The (non-zero) diagonal
entries of E are called singular values. The
columns of U corresponding to the singular val-
ues form a basis for the column space of A and
those of V form a basis for the row space of A.
Since the document vectors lie in the row space,
the columns of V form a basis for the document
vectors. The use of SVD in document retrieval
is known as latent semantic analysis (Dumais et
al., 1988). LSA has been applied for the reso-
lution of synonymy and polysemy in document
retrieval.
In this paper, we take the columns of V as
the basis for the document space. We repre-
sent the document vectors in terms of these and
then perform the clustering. In other words, the
matrix VAT is subjected to bipartite matching.
Since this matrix can have negative entries, we
use the absolute values.
</bodyText>
<subsectionHeader confidence="0.962233">
2.2 Independent component analysis
</subsectionHeader>
<bodyText confidence="0.981694107142857">
Independent Component Analysis (ICA) is a
generalization of stochastic interpretation of
matrix diagonalization. Diagonalization, when
applied to stochastic vectors, produces mutually
uncorrelated basis. ICA produces a statistically
independent basis. Thus the independent com-
ponents of a matrix A can be thought of a col-
lection of statistically independent sources for
the rows (or columns) of A (Lee et al., 1998).
The decomposition reveals the sources as well
as mixing coefficients. The m x n matrix A is
decomposed as
A = WS+N
where S is the r x n source signal matrix, W is
the m x r mixing matrix, and N is the matrix of
noise signals. Here r is the number of indepen-
dent sources. The above decomposition can be
performed for any number of independent com-
ponents and the sizes of W and S vary accord-
ingly. We subject the matrix SAT to clustering.
(Absolute values are used, as before.) In this
case, the documents are represented in terms
of r independent components instead of words.
We use the Fast ICA algorithm for performing
the decomposition (Hyvarinen, 1999).
ICA has been used for dimensionality re-
duction and representation of word histograms
(Kolenda and Hansen, 2000).
</bodyText>
<subsectionHeader confidence="0.974607">
2.3 Non-negative matrix factorization
</subsectionHeader>
<bodyText confidence="0.99217424137931">
In the above matrix factorizations, the com-
ponents or basis vectors can have negative en-
tries. The term-by-document matrix itself does
not have negative entries. Thus negative fac-
tors are difficult to interpret. In particular,
we cannot interpret them as parts of objects
like documents. Non-negative matrix factoriza-
tion (NMF) attempts a factorization in which
the components have non-negative entries. The
NMF of A is given by
A = W H
where the factors W and H contain only non-
negative entries. The interpretation in (Lee and
Seung, 1999) for the above decomposition is as
follows: The columns of the m x n matrix A
are the signals, the columns of the m x r ma-
trix W are the basis signals, and the r x n ma-
trix H is the mixing matrix. (Here r is the
number of parts or non-negative components.)
If we use the columns of W as the new basis,
the new bipartite graph can be represented as
WTA. [In our case, the the signals of interest
are documents and documents form the rows of
A. Hence we subject B = AT to factorization.
Let B = WH. The matrix used for clustering
is WTB = WTAT.]
Non-negative matrix factorization has been
shown to discover semantic features (Lee and
Seung, 1999).
</bodyText>
<sectionHeader confidence="0.883034" genericHeader="method">
3 Bipartite graph partitioning
</sectionHeader>
<bodyText confidence="0.992356909090909">
Consider a weighted bipartite graph such as the
term-document graph. Let the vertex sets be
X and Y and the weight matrix be W. A par-
tition of the graph into two subgraphs can be
represented by a vertex partition of X and Y.
Let A C X and B C Y. Then the partition is
given by the subgraphs induced by A &amp; B and
Ac &amp; BC. The cut between A and B is defined
as
cut (A, B) = W (A, BC) + W (Ac, B)
where
</bodyText>
<equation confidence="0.95746075">
W (A, B) =
iEAJEB
The normalized cut, Ncut, is defined as (Shi and
Malik, 1997) (Zha et al., 2001)
cut(A, B)
W (A,Y) + W (X, B)
cut(Ac, BC)
+ W (Ac,Y) + W (X, Bc)
</equation>
<bodyText confidence="0.999772285714286">
The problem of finding partition with mini-
mum Ncut can be posed as an eigenvalue prob-
lem (Shi and Malik, 1997) (Zha et al., 2001).
The following is the algorithm derived in (Zha
et al., 2001).
Let e be a vector of all is (of appropriate
dimension).
</bodyText>
<listItem confidence="0.9951831">
1. Compute diagonal matrices Dx and Dy as
We = Dxe,WTe = Dye
2. Let W = Dx1i2WDy—li2
3. Compute the second largest left and right
singular vectors of W,&amp;quot;±&amp;quot; and
4. Find cut points cx and cy for x = D xli2 &amp;quot;X&amp;quot;
and y = Dyll y.
2
5. Let A ={i: xi &gt; cx}, Ac ={i: xi &lt; cx},
B= {j : yj &gt; cy}, and BC = {j : y j &lt; cy},
</listItem>
<bodyText confidence="0.9897135">
The graphs G(A, B) and G(Ac, BC) can be fur-
ther partitioned.
</bodyText>
<sectionHeader confidence="0.998016" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999976">
To test the effectiveness of different feature rep-
resentations, we use the scheme of (Zha et al.,
</bodyText>
<equation confidence="0.604011">
Ncut(A, B)
</equation>
<bodyText confidence="0.989476714285714">
2001). The task used is binary clustering - dis-
crimination between two news groups. The cor-
pus used is the 20 newsgroups database.2 The
newsgroups and the associated labeling scheme
of (Zha et al., 2001) are listed below:
The information gain between terms and
documents is defined as
</bodyText>
<figure confidence="0.986843434782609">
p(t, d) )
I G (t , d) = E Ep(t,d) log (p(t)p(d) )
d t
NG1: alt.atheism
NG2: comp.graphics
NG3: comp.os.ms-windows.misc
NG4: comp.sys.ibm.pc.hardware
NG5: comp.sys.mac.hardware
NG6: comp.windows.x
NG7: misc.forsale
NG8: rec.autos
NG9: rec.motorcycles
NG10: rec.sport.baseball
NG11: rec.sport.hockey
NG12: sci.crypt
NG13: sci.electronics
NG14: sci.med
NG15: sci.space
NG16: soc.religion.christian
NG17: talk.politics.guns
NG18: talk.politics.mideast
NG19: talk.politics.misc
NG20: talk.religion.misc
</figure>
<bodyText confidence="0.99943975">
We consider three binary classification tasks:
between NG1 &amp; NG2, NG10 &amp; NG11, and
NG18 &amp; NG19. It can be seen that NG1 and
NG2 are well-separated, NG10 and NG11 have
some overlap, and NG18 and NG19 have more
overlap. To compare with the results of (Zha et
al., 2001), we perform clustering for four differ-
ent sample sizes: 50, 100, 150, 200 and 250 files
for the second class.3 The first class always has
50 files.
Each run of the experiment was performed as
follows.
</bodyText>
<listItem confidence="0.9554315">
1. The required number of files were chosen
randomly from each class.
</listItem>
<bodyText confidence="0.6955175">
2. The documents were represented using N
terms. (In the experiments reported below,
N = 200.) The terms were chosen accord-
ing to maximum information gain criterion.
</bodyText>
<footnote confidence="0.9974422">
2The database is available from
http://www.cs.cmu.edu/afs/cs/project/theo-11/
www/naive-bayes.html.
3(Zha et al., 2001) use the sample sizes; 50, 100, 150,
and 200.
</footnote>
<equation confidence="0.938806">
(1)
</equation>
<bodyText confidence="0.871592944444444">
where t is term and d is document. This
was done using the bow toolkit.4
3. The documents were represented using the
N terms. We call this representation in
term basis.
4. The document vectors were also repre-
sented using other bases - singular vec-
tors, independent components, and non-
negative components.
5. The resulting matrices were subjected to
bipartite graph partitioning. The best
partitions were calculated by choosing cx
and cy which minimize the objective func-
tion (Shi and Malik, 1997).
The results of the experiments are shown in ta-
ble 1. The results of (Zha et al., 2001) are shown
in table 2 for comparison. The differences in re-
sults can be attributed to
</bodyText>
<listItem confidence="0.9943456">
1. Zha et al. (2001) use maximum mutual in-
formation to choose terms.
2. While we choose 200 terms in all experi-
ments, number of terms used by Zha et al.
(2001) is not available.
</listItem>
<bodyText confidence="0.996458133333333">
It can be seen that the performance confirms
to our initial expectation: classification accura-
cies are smaller when the classes are not well-
separated. It is surprising that the default term-
basis performs better than other derived bases.
In all the derived bases, we have an extra
degree of freedom: the number of components
chosen. In the previous set of experiments, we
used 200 independent components and 100 non-
negative components. We reduced the number
of independent components and non-negative
components by half and the results are shown in
table 3. ICA performs better in the presence of
uncertainty (NG18/NG19 and unbalanced data
sets for NG10/NG11).
</bodyText>
<footnote confidence="0.997577666666667">
4The toolkit is also available from
http://www.cs.cmu.edu/afs/cs/project/theo-11/
www/naive-bayes.html.
</footnote>
<table confidence="0.981109714285714">
Newsgroups: NG1/NG2
Mixture Terms SVD ICA NMF
50/50 95.16 ± 0.14 93.00 ± 0.81 86.26 ± 0.31 93.32 ± 0.62
50/100 93.51 ± 0.16 87.47 ± 1.58 88.42 ± 0.42 91.16 ± 0.73
50/150 93.33 ± 0.09 87.57 ± 0.23 86.90 ± 1.18 87.00 ± 0.30
50/200 88.18 ± 0.57 84.50 ± 0.55 86.02 ± 1.93 80.74 ± 0.34
50/250 82.24 ± 0.59 78.41 ± 0.26 79.14 ± 2.01 78.16 ± 0.28
Newsgroups: NG10/NG11
Mixture Terms SVD ICA NMF
50/50 75.42 ± 1.39 84.68 ± 0.58 60.74 ± 0.41 78.79 ± 1.69
50/100 81.02 ± 0.82 74.74 ± 1.56 68.49 ± 0.56 63.02 ± 0.98
50/150 74.40 ± 1.45 63.10 ± 0.66 61.13 ± 0.19 60.35 ± 0.18
50/200 74.29 ± 0.85 59.92 ± 0.33 61.12 ± 0.27 61.87 ± 0.18
50/250 71.56 ± 1.06 58.37 ± 0.20 59.72 ± 0.29 65.68 ± 0.18
Newsgroups: NG18/NG19
Mixture Terms SVD ICA NMF
50/50 62.95 ± 0.34 60.89 ± 0.26 62.11 ± 0.37 59.16 ± 0.31
50/100 69.26 ± 0.48 66.89 ± 0.37 64.22 ± 0.25 64.22 ± 0.27
50/150 68.58 ± 0.42 66.55 ± 0.30 67.50 ± 0.34 66.29 ± 0.13
50/200 67.41 ± 0.91 68.12 ± 0.15 69.62 ± 0.25 68.14 ± 0.18
50/250 70.08 ± 1.43 67.94 ± 0.18 68.71 ± 0.10 68.15 ± 0.10
</table>
<tableCaption confidence="0.9966365">
Table 1: Experimental results for three binary classification tasks. Percent correct figures are
reported. The statistics were measure over 20 runs for each case.
</tableCaption>
<table confidence="0.999956">
Mixture NG1/NG2 NG10/NG11 NG18/NG19
50/50 92.12 ± 3.52% 74.56 ± 8.93% 73.66 ± 10.53%
50/100 90.57 ± 3.11% 67.13 ± 7.17% 67.23 ± 7.84%
50/150 88.04 ± 3.90% 58.30 ± 5.99% 65.83 ± 2.79%
50/200 82.77 ± 5.24% 57.55 ± 5.69% 61.23 ± 9.88%
</table>
<tableCaption confidence="0.998959">
Table 2: The results of (Zha et al., 2001).
</tableCaption>
<subsectionHeader confidence="0.990493">
4.1 Term selection
</subsectionHeader>
<bodyText confidence="0.999738235294118">
Note that when the initial terms selection is
done using equation 1, the documents are not
labeled. But after first round of classification,
we have tentative class labels available. This
can be used to refine the choice of terms. For
this we use class labels instead of document la-
bels in equation 1. The overall scheme is shown
in figure 2. Figure 3 shows how the scores im-
prove with this iterative term selection.
We can use this idea to improve classification
accuracy. Table 4 gives the best case results.
The figures are obtained by using the knowledge
of classes when choosing terms using equation I.
These can be viewed classification accuracies
when best term selection is performed. Fig-
ure 4 shows the actual improvement of scores
for a particular case.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="conclusions">
5 Related work
</sectionHeader>
<bodyText confidence="0.9999324">
There have been several attempts to improve
document clustering accuracy. (Moore et al,
1999) use hypergraph partitioning and show
that the resulting clusters have smaller en-
tropy compared to other clustering techniques.
(Strehl et al., ) study the effect of various sim-
ilarity measures and clustering algorithms on
clustering web pages. They conclude that cosine
and extended Jaccard similarities and weighted
graph partitioning give good results. (Dhillon
</bodyText>
<figure confidence="0.999171827586207">
1
0.06
iteration 1
iteration 5
NG1/NG2
NG10/NG11
NG18/NG19
0.05
0.95
0.04
0.9
0.03
0.85
0.02
0.8
0.0
1
0.75
0
0.7
-0.0
1
0.65
-0.02
0.6
-0.03
0.55
0 2 4 6 8 10 12 14 16 18 20
0 10 20 30 40 50 60 70 80 90 100
</figure>
<table confidence="0.963066388888889">
Newsgroups: NG1/NG2
Mixture Terms SVD ICA NMF
50/50 99.30 ± 0.00 99.30 ± 0.00 97.40 ± 0.01 99.70 ± 0.00
50/100 99.08 ± 0.01 99.50 ± 0.01 97.83 ± 0.01 97.33 ± 0.04
50/150 92.88 ± 2.39 92.31 ± 2.23 92.94 ± 1.76 86.75 ± 0.75
50/250 99.07 ± 0.00 96.18 ± 0.07 98.67 ± 0.00 83.16 ± 0.13
Newsgroups: NG10/NG11
Mixture Terms SVD ICA NMF
50/50 98.00 ± 0.04 95.50 ± 0.07 96.80 ± 0.11 98.20 ± 0.03
50/100 97.80 ± 0.01 93.47 ± 0.15 96.87 ± 0.04 92.07 ± 0.08
50/150 97.50 ± 0.01 91.25 ± 0.25 97.50 ± 0.02 84.05 ± 0.40
50/250 97.36 ± 0.01 88.04 ± 0.22 97.92 ± 0.01 83.92 ± 0.85
Newsgroups: NG18/NG19
Mixture Terms SVD ICA NMF
50/50 90.80 ± 0.47 73.00 ± 0.99 91.70 ± 0.09 79.90 ± 0.86
50/100 94.07 ± 0.07 85.19 ± 0.31 94.52 ± 0.03 86.89 ± 0.33
50/150 91.70 ± 0.23 86.65 ± 0.37 94.80 ± 0.03 85.95 ± 0.24
50/250 91.48 ± 0.17 81.92 ± 1.43 93.00 ± 0.02 85.40 ± 0.15
</table>
<tableCaption confidence="0.991933">
Table 4: Results for &amp;quot;best case&amp;quot; term selection.
</tableCaption>
<sectionHeader confidence="0.986842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999333211538462">
M W Berry, Z Drmac, and E R Jessup. 1999.
Matrices, vector spaces, and information re-
trieval. SIAM Review, 41:335-362.
I S Dhillon and D S Modha. 2001. Concept de-
compositions for large sparse text data using
clustering. Machine Learning, 42(4143-175.
S T Dumais, G W Furnas, T K Landauer, and
S Deerwester. 1988. Using latent semantic
analysis to improve information retrieval. In
Proceedings of CHI&apos;88.
A Hyvarinen. 1999. Fast and robust fixed-point
algorithms for indepdendent component anal-
ysis. IEEE Transactions on Neural Networks,
10(3):626-634.
R Kannan, S Vempala, and A Vetta. 2000. On
clusterings: Good, bad, and spectral. In Pro-
ceedings of FOCS, pages 367-377.
T Kolenda and L Hansen. 2000. Indepen-
dent components in text. In Mark Girolami,
editor, Advances in Independent Component
Analysis, chapter 13. Springer Verlag.
D D Lee and S Seung. 1999. Learning parts of
objects by non-negative matrix factorization.
Nature, 401:788-791.
T.-W Lee, M Girolami, A J Bell, and T J
Sejnowski. 1998. A unifying information-
theoretic framework for independent com-
ponent analysis. International Journal on
Mathematical and Computer Modeling.
J Moore et al. 1999. Web page categoriza-
tion and feature selection using associate rule
and principal component clustering. Decision
Support Systems, 27(3):329-341.
G Salton. 1989. Automatic text processing: The
transformation, analysis, and retrieval of in-
formation by computer. Addison-Weslsy.
F Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing
Surveys.
J Shi and J Malik. 1997. Normalized cuts
and image segmentation. In Proc of IEEE
Conference on Computer Vision and Pattern
Recognition, June.
Gilbert Strang. 1980. Linear algebra and its ap-
plications. Academic Press.
A Strehl, J Ghosh, and R Mooney. Impact of
similarity measures on web-page clustering.
In Workshop of Artificial Intelligence for Web
Search (AAAI 2000). AAAI.
H Zha, X He, C Ding, H Simon, and M Gu.
2001. Bipartite graph partitioning and data
clustering. In Proc CKIM 01, Nov 5-10.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.277863">
<title confidence="0.998945">Features for unsupervised document classification</title>
<author confidence="0.43951">H</author>
<affiliation confidence="0.9040415">Applied Research Satyam Computer Services</affiliation>
<address confidence="0.901509">14 Langford Avenue, Lalbagh Bangalore - 560 025,</address>
<email confidence="0.999757">SH_Srinivasan@satyam.com</email>
<abstract confidence="0.981343769230769">Unsupervised document classification is an important problem in practical text mining since training data is seldom available. In this paper we study the problem of term selection and the performance of various features for unsupervised text classification. The features studied are: principal components, independent components, and non-negative components. The clustering algorithm used is based on bipartite graph partitioning (Zha et al., 2001). The evaluation is performed using the newsgroups corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>Z Drmac</author>
<author>E R Jessup</author>
</authors>
<title>Matrices, vector spaces, and information retrieval.</title>
<date>1999</date>
<journal>SIAM Review,</journal>
<pages>41--335</pages>
<contexts>
<context position="2075" citStr="Berry et al., 1999" startWordPosition="319" endWordPosition="322">nd very low frequency terms. The terms can be arranged in some convenient order. The documents are represented as vectors of dimension n. Let v be the vector corresponding to a document d. We set v, to 1 if term t, occurs in d. Otherwise v, is set to 0. This representation uses information about presence or ab&apos;Terms are are obtained from words after stemming. sence of terms in the document. All the other information is ignored. Some representations use frequency information. The vectors corresponding to several documents can be arranged in a matrix — the so called &amp;quot;term-document&amp;quot; matrix. See (Berry et al., 1999) for a review of vector space representation. Other representation schemes can be in the vector space framework. Classification is done in the feature space. Several techniques exist for classification — naive bayes, support vector machines, k-means clustering, neural networks etc. See (Sebastiani, 2002) for a comprehensive survey. In this paper, we use the bipartite graph matching technique proposed in (Zha et al., 2001) for classification. See also (Shi and Malik, 1997). Bipartite graph matching (also known as spectral clustering) has several attractive properties including the property that</context>
</contexts>
<marker>Berry, Drmac, Jessup, 1999</marker>
<rawString>M W Berry, Z Drmac, and E R Jessup. 1999. Matrices, vector spaces, and information retrieval. SIAM Review, 41:335-362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Dhillon</author>
<author>D S Modha</author>
</authors>
<title>Concept decompositions for large sparse text data using clustering.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--4143</pages>
<marker>Dhillon, Modha, 2001</marker>
<rawString>I S Dhillon and D S Modha. 2001. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(4143-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>S Deerwester</author>
</authors>
<title>Using latent semantic analysis to improve information retrieval.</title>
<date>1988</date>
<booktitle>In Proceedings of CHI&apos;88.</booktitle>
<contexts>
<context position="4310" citStr="Dumais et al., 1988" startWordPosition="673" endWordPosition="676"> for representing the documents are crucial to the success of the classification scheme. Several techniques exist for term selection: document frequency, information gain, etc (Sebastiani, 2002). In this paper, we use an iterative technique in conjunction with information gain. basis selection: Terms form the basis of the original representation. These bases may not be optimal for the classification task. We explore the following representation schemes: singular vectors, independent vectors, and non-negative parts. While these techniques have been applied to document processing (for example, (Dumais et al., 1988), (Kolenda and Hansen, 2000), (Lee and Seung, 1999)), there is no systematic comparison of these techniques for unsupervised document classification. The same clustering algorithm — bipartite graph partitioning — is used in the final stage. The paper is organized as follows. Section 2 introduces the representation schemes used, section 3 describes the bipartite graph partitioning, and section 4 lists the experimental setting and results. 2 Feature representation schemes The term-by-document matrix is a representation of the documents using one type of basis: terms. It is possible to extract ot</context>
<context position="6225" citStr="Dumais et al., 1988" startWordPosition="994" endWordPosition="997">lar matrices also. SVD is a generalization of matrix diagonalization. The SVD of A can be written as A = UEVT where U and V are m x m and n x n orthogonal matrices and E is amxn diagonal matrix (Strang, 1980). The (non-zero) diagonal entries of E are called singular values. The columns of U corresponding to the singular values form a basis for the column space of A and those of V form a basis for the row space of A. Since the document vectors lie in the row space, the columns of V form a basis for the document vectors. The use of SVD in document retrieval is known as latent semantic analysis (Dumais et al., 1988). LSA has been applied for the resolution of synonymy and polysemy in document retrieval. In this paper, we take the columns of V as the basis for the document space. We represent the document vectors in terms of these and then perform the clustering. In other words, the matrix VAT is subjected to bipartite matching. Since this matrix can have negative entries, we use the absolute values. 2.2 Independent component analysis Independent Component Analysis (ICA) is a generalization of stochastic interpretation of matrix diagonalization. Diagonalization, when applied to stochastic vectors, produce</context>
</contexts>
<marker>Dumais, Furnas, Landauer, Deerwester, 1988</marker>
<rawString>S T Dumais, G W Furnas, T K Landauer, and S Deerwester. 1988. Using latent semantic analysis to improve information retrieval. In Proceedings of CHI&apos;88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hyvarinen</author>
</authors>
<title>Fast and robust fixed-point algorithms for indepdendent component analysis.</title>
<date>1999</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<pages>10--3</pages>
<contexts>
<context position="7724" citStr="Hyvarinen, 1999" startWordPosition="1250" endWordPosition="1251">s well as mixing coefficients. The m x n matrix A is decomposed as A = WS+N where S is the r x n source signal matrix, W is the m x r mixing matrix, and N is the matrix of noise signals. Here r is the number of independent sources. The above decomposition can be performed for any number of independent components and the sizes of W and S vary accordingly. We subject the matrix SAT to clustering. (Absolute values are used, as before.) In this case, the documents are represented in terms of r independent components instead of words. We use the Fast ICA algorithm for performing the decomposition (Hyvarinen, 1999). ICA has been used for dimensionality reduction and representation of word histograms (Kolenda and Hansen, 2000). 2.3 Non-negative matrix factorization In the above matrix factorizations, the components or basis vectors can have negative entries. The term-by-document matrix itself does not have negative entries. Thus negative factors are difficult to interpret. In particular, we cannot interpret them as parts of objects like documents. Non-negative matrix factorization (NMF) attempts a factorization in which the components have non-negative entries. The NMF of A is given by A = W H where the </context>
</contexts>
<marker>Hyvarinen, 1999</marker>
<rawString>A Hyvarinen. 1999. Fast and robust fixed-point algorithms for indepdendent component analysis. IEEE Transactions on Neural Networks, 10(3):626-634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kannan</author>
<author>S Vempala</author>
<author>A Vetta</author>
</authors>
<title>On clusterings: Good, bad, and spectral.</title>
<date>2000</date>
<booktitle>In Proceedings of FOCS,</booktitle>
<pages>367--377</pages>
<contexts>
<context position="2780" citStr="Kannan et al., 2000" startWordPosition="429" endWordPosition="432">the vector space framework. Classification is done in the feature space. Several techniques exist for classification — naive bayes, support vector machines, k-means clustering, neural networks etc. See (Sebastiani, 2002) for a comprehensive survey. In this paper, we use the bipartite graph matching technique proposed in (Zha et al., 2001) for classification. See also (Shi and Malik, 1997). Bipartite graph matching (also known as spectral clustering) has several attractive properties including the property that if the data has a &amp;quot;good&amp;quot; clustering, the algorithm will find an &amp;quot;optimal&amp;quot; one. See (Kannan et al., 2000) for definitions and details. The term-by-document matrix can be considered as a representation of a weighted bipartite graph — the vertex sets being terms (T) and documents (D). The clustering technique partitions the vertices into disjoint sets (T = TUT&apos; and D = D1UD) such that intra cluster weight (between T1 &amp; Di and Tf &amp; DO is maximized and inter cluster weight (between T1 &amp; D and Tic &amp; Di) is minimized. The overall classification scheme is shown in figure 1. It can be seen that the final clustering scheme depends on the connectivity and weights of the bipartite graph. Any scheme which ch</context>
</contexts>
<marker>Kannan, Vempala, Vetta, 2000</marker>
<rawString>R Kannan, S Vempala, and A Vetta. 2000. On clusterings: Good, bad, and spectral. In Proceedings of FOCS, pages 367-377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kolenda</author>
<author>L Hansen</author>
</authors>
<title>Independent components in text.</title>
<date>2000</date>
<booktitle>Advances in Independent Component Analysis, chapter 13.</booktitle>
<editor>In Mark Girolami, editor,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="4338" citStr="Kolenda and Hansen, 2000" startWordPosition="677" endWordPosition="680">ocuments are crucial to the success of the classification scheme. Several techniques exist for term selection: document frequency, information gain, etc (Sebastiani, 2002). In this paper, we use an iterative technique in conjunction with information gain. basis selection: Terms form the basis of the original representation. These bases may not be optimal for the classification task. We explore the following representation schemes: singular vectors, independent vectors, and non-negative parts. While these techniques have been applied to document processing (for example, (Dumais et al., 1988), (Kolenda and Hansen, 2000), (Lee and Seung, 1999)), there is no systematic comparison of these techniques for unsupervised document classification. The same clustering algorithm — bipartite graph partitioning — is used in the final stage. The paper is organized as follows. Section 2 introduces the representation schemes used, section 3 describes the bipartite graph partitioning, and section 4 lists the experimental setting and results. 2 Feature representation schemes The term-by-document matrix is a representation of the documents using one type of basis: terms. It is possible to extract other underlying bases which m</context>
<context position="7837" citStr="Kolenda and Hansen, 2000" startWordPosition="1265" endWordPosition="1268">signal matrix, W is the m x r mixing matrix, and N is the matrix of noise signals. Here r is the number of independent sources. The above decomposition can be performed for any number of independent components and the sizes of W and S vary accordingly. We subject the matrix SAT to clustering. (Absolute values are used, as before.) In this case, the documents are represented in terms of r independent components instead of words. We use the Fast ICA algorithm for performing the decomposition (Hyvarinen, 1999). ICA has been used for dimensionality reduction and representation of word histograms (Kolenda and Hansen, 2000). 2.3 Non-negative matrix factorization In the above matrix factorizations, the components or basis vectors can have negative entries. The term-by-document matrix itself does not have negative entries. Thus negative factors are difficult to interpret. In particular, we cannot interpret them as parts of objects like documents. Non-negative matrix factorization (NMF) attempts a factorization in which the components have non-negative entries. The NMF of A is given by A = W H where the factors W and H contain only nonnegative entries. The interpretation in (Lee and Seung, 1999) for the above decom</context>
</contexts>
<marker>Kolenda, Hansen, 2000</marker>
<rawString>T Kolenda and L Hansen. 2000. Independent components in text. In Mark Girolami, editor, Advances in Independent Component Analysis, chapter 13. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lee</author>
<author>S Seung</author>
</authors>
<title>Learning parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<pages>401--788</pages>
<contexts>
<context position="4361" citStr="Lee and Seung, 1999" startWordPosition="681" endWordPosition="684">success of the classification scheme. Several techniques exist for term selection: document frequency, information gain, etc (Sebastiani, 2002). In this paper, we use an iterative technique in conjunction with information gain. basis selection: Terms form the basis of the original representation. These bases may not be optimal for the classification task. We explore the following representation schemes: singular vectors, independent vectors, and non-negative parts. While these techniques have been applied to document processing (for example, (Dumais et al., 1988), (Kolenda and Hansen, 2000), (Lee and Seung, 1999)), there is no systematic comparison of these techniques for unsupervised document classification. The same clustering algorithm — bipartite graph partitioning — is used in the final stage. The paper is organized as follows. Section 2 introduces the representation schemes used, section 3 describes the bipartite graph partitioning, and section 4 lists the experimental setting and results. 2 Feature representation schemes The term-by-document matrix is a representation of the documents using one type of basis: terms. It is possible to extract other underlying bases which may aid classification. </context>
<context position="8417" citStr="Lee and Seung, 1999" startWordPosition="1358" endWordPosition="1361">rd histograms (Kolenda and Hansen, 2000). 2.3 Non-negative matrix factorization In the above matrix factorizations, the components or basis vectors can have negative entries. The term-by-document matrix itself does not have negative entries. Thus negative factors are difficult to interpret. In particular, we cannot interpret them as parts of objects like documents. Non-negative matrix factorization (NMF) attempts a factorization in which the components have non-negative entries. The NMF of A is given by A = W H where the factors W and H contain only nonnegative entries. The interpretation in (Lee and Seung, 1999) for the above decomposition is as follows: The columns of the m x n matrix A are the signals, the columns of the m x r matrix W are the basis signals, and the r x n matrix H is the mixing matrix. (Here r is the number of parts or non-negative components.) If we use the columns of W as the new basis, the new bipartite graph can be represented as WTA. [In our case, the the signals of interest are documents and documents form the rows of A. Hence we subject B = AT to factorization. Let B = WH. The matrix used for clustering is WTB = WTAT.] Non-negative matrix factorization has been shown to disc</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>D D Lee and S Seung. 1999. Learning parts of objects by non-negative matrix factorization. Nature, 401:788-791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T-W Lee</author>
<author>M Girolami</author>
<author>A J Bell</author>
<author>T J Sejnowski</author>
</authors>
<title>A unifying informationtheoretic framework for independent component analysis.</title>
<date>1998</date>
<journal>International Journal on Mathematical and Computer Modeling.</journal>
<contexts>
<context position="7067" citStr="Lee et al., 1998" startWordPosition="1127" endWordPosition="1130">en perform the clustering. In other words, the matrix VAT is subjected to bipartite matching. Since this matrix can have negative entries, we use the absolute values. 2.2 Independent component analysis Independent Component Analysis (ICA) is a generalization of stochastic interpretation of matrix diagonalization. Diagonalization, when applied to stochastic vectors, produces mutually uncorrelated basis. ICA produces a statistically independent basis. Thus the independent components of a matrix A can be thought of a collection of statistically independent sources for the rows (or columns) of A (Lee et al., 1998). The decomposition reveals the sources as well as mixing coefficients. The m x n matrix A is decomposed as A = WS+N where S is the r x n source signal matrix, W is the m x r mixing matrix, and N is the matrix of noise signals. Here r is the number of independent sources. The above decomposition can be performed for any number of independent components and the sizes of W and S vary accordingly. We subject the matrix SAT to clustering. (Absolute values are used, as before.) In this case, the documents are represented in terms of r independent components instead of words. We use the Fast ICA alg</context>
</contexts>
<marker>Lee, Girolami, Bell, Sejnowski, 1998</marker>
<rawString>T.-W Lee, M Girolami, A J Bell, and T J Sejnowski. 1998. A unifying informationtheoretic framework for independent component analysis. International Journal on Mathematical and Computer Modeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Moore</author>
</authors>
<title>Web page categorization and feature selection using associate rule and principal component clustering. Decision Support Systems,</title>
<date>1999</date>
<pages>27--3</pages>
<marker>Moore, 1999</marker>
<rawString>J Moore et al. 1999. Web page categorization and feature selection using associate rule and principal component clustering. Decision Support Systems, 27(3):329-341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic text processing: The transformation, analysis, and retrieval of information by computer.</title>
<date>1989</date>
<publisher>Addison-Weslsy.</publisher>
<contexts>
<context position="1281" citStr="Salton, 1989" startWordPosition="178" endWordPosition="179">aluation is performed using the newsgroups corpus. 1 Introduction Many natural language processing applications require text classification. Labeled data for training is typically unavailable in many situations. So unsupervised classification techniques are called for. Classification — both supervised and unsupervised — is usually done in two steps: feature extraction and classification. Feature extraction is basically a change of representation of the raw data. The most common representation used in document retrieval is the vector representation or the &amp;quot;bag-of-words&amp;quot; feature representation (Salton, 1989). Each document is represented as a vector in the term space. Let ti, t2, • • • ,t be the terms we use to represent the documents.&apos; This collection usually ignores very high and very low frequency terms. The terms can be arranged in some convenient order. The documents are represented as vectors of dimension n. Let v be the vector corresponding to a document d. We set v, to 1 if term t, occurs in d. Otherwise v, is set to 0. This representation uses information about presence or ab&apos;Terms are are obtained from words after stemming. sence of terms in the document. All the other information is ig</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>G Salton. 1989. Automatic text processing: The transformation, analysis, and retrieval of information by computer. Addison-Weslsy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys.</journal>
<contexts>
<context position="2380" citStr="Sebastiani, 2002" startWordPosition="367" endWordPosition="368">r ab&apos;Terms are are obtained from words after stemming. sence of terms in the document. All the other information is ignored. Some representations use frequency information. The vectors corresponding to several documents can be arranged in a matrix — the so called &amp;quot;term-document&amp;quot; matrix. See (Berry et al., 1999) for a review of vector space representation. Other representation schemes can be in the vector space framework. Classification is done in the feature space. Several techniques exist for classification — naive bayes, support vector machines, k-means clustering, neural networks etc. See (Sebastiani, 2002) for a comprehensive survey. In this paper, we use the bipartite graph matching technique proposed in (Zha et al., 2001) for classification. See also (Shi and Malik, 1997). Bipartite graph matching (also known as spectral clustering) has several attractive properties including the property that if the data has a &amp;quot;good&amp;quot; clustering, the algorithm will find an &amp;quot;optimal&amp;quot; one. See (Kannan et al., 2000) for definitions and details. The term-by-document matrix can be considered as a representation of a weighted bipartite graph — the vertex sets being terms (T) and documents (D). The clustering techni</context>
<context position="3884" citStr="Sebastiani, 2002" startWordPosition="612" endWordPosition="614">t the final clustering scheme depends on the connectivity and weights of the bipartite graph. Any scheme which changes the connectivity or weights is likely to change the clustering results. In this paper we explore the following aspects. Documents Term Change of Bipartite Clustering _,.. Selection Basis -1,.. Graph Representation -1,.. -I. Classes Figure 1: Document classification scheme. term selection: The terms used for representing the documents are crucial to the success of the classification scheme. Several techniques exist for term selection: document frequency, information gain, etc (Sebastiani, 2002). In this paper, we use an iterative technique in conjunction with information gain. basis selection: Terms form the basis of the original representation. These bases may not be optimal for the classification task. We explore the following representation schemes: singular vectors, independent vectors, and non-negative parts. While these techniques have been applied to document processing (for example, (Dumais et al., 1988), (Kolenda and Hansen, 2000), (Lee and Seung, 1999)), there is no systematic comparison of these techniques for unsupervised document classification. The same clustering algo</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shi</author>
<author>J Malik</author>
</authors>
<title>Normalized cuts and image segmentation.</title>
<date>1997</date>
<booktitle>In Proc of IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<contexts>
<context position="2551" citStr="Shi and Malik, 1997" startWordPosition="393" endWordPosition="396">ion. The vectors corresponding to several documents can be arranged in a matrix — the so called &amp;quot;term-document&amp;quot; matrix. See (Berry et al., 1999) for a review of vector space representation. Other representation schemes can be in the vector space framework. Classification is done in the feature space. Several techniques exist for classification — naive bayes, support vector machines, k-means clustering, neural networks etc. See (Sebastiani, 2002) for a comprehensive survey. In this paper, we use the bipartite graph matching technique proposed in (Zha et al., 2001) for classification. See also (Shi and Malik, 1997). Bipartite graph matching (also known as spectral clustering) has several attractive properties including the property that if the data has a &amp;quot;good&amp;quot; clustering, the algorithm will find an &amp;quot;optimal&amp;quot; one. See (Kannan et al., 2000) for definitions and details. The term-by-document matrix can be considered as a representation of a weighted bipartite graph — the vertex sets being terms (T) and documents (D). The clustering technique partitions the vertices into disjoint sets (T = TUT&apos; and D = D1UD) such that intra cluster weight (between T1 &amp; Di and Tf &amp; DO is maximized and inter cluster weight (b</context>
<context position="9573" citStr="Shi and Malik, 1997" startWordPosition="1595" endWordPosition="1598"> WTAT.] Non-negative matrix factorization has been shown to discover semantic features (Lee and Seung, 1999). 3 Bipartite graph partitioning Consider a weighted bipartite graph such as the term-document graph. Let the vertex sets be X and Y and the weight matrix be W. A partition of the graph into two subgraphs can be represented by a vertex partition of X and Y. Let A C X and B C Y. Then the partition is given by the subgraphs induced by A &amp; B and Ac &amp; BC. The cut between A and B is defined as cut (A, B) = W (A, BC) + W (Ac, B) where W (A, B) = iEAJEB The normalized cut, Ncut, is defined as (Shi and Malik, 1997) (Zha et al., 2001) cut(A, B) W (A,Y) + W (X, B) cut(Ac, BC) + W (Ac,Y) + W (X, Bc) The problem of finding partition with minimum Ncut can be posed as an eigenvalue problem (Shi and Malik, 1997) (Zha et al., 2001). The following is the algorithm derived in (Zha et al., 2001). Let e be a vector of all is (of appropriate dimension). 1. Compute diagonal matrices Dx and Dy as We = Dxe,WTe = Dye 2. Let W = Dx1i2WDy—li2 3. Compute the second largest left and right singular vectors of W,&amp;quot;±&amp;quot; and 4. Find cut points cx and cy for x = D xli2 &amp;quot;X&amp;quot; and y = Dyll y. 2 5. Let A ={i: xi &gt; cx}, Ac ={i: xi &lt; cx},</context>
<context position="12554" citStr="Shi and Malik, 1997" startWordPosition="2105" endWordPosition="2108">.cmu.edu/afs/cs/project/theo-11/ www/naive-bayes.html. 3(Zha et al., 2001) use the sample sizes; 50, 100, 150, and 200. (1) where t is term and d is document. This was done using the bow toolkit.4 3. The documents were represented using the N terms. We call this representation in term basis. 4. The document vectors were also represented using other bases - singular vectors, independent components, and nonnegative components. 5. The resulting matrices were subjected to bipartite graph partitioning. The best partitions were calculated by choosing cx and cy which minimize the objective function (Shi and Malik, 1997). The results of the experiments are shown in table 1. The results of (Zha et al., 2001) are shown in table 2 for comparison. The differences in results can be attributed to 1. Zha et al. (2001) use maximum mutual information to choose terms. 2. While we choose 200 terms in all experiments, number of terms used by Zha et al. (2001) is not available. It can be seen that the performance confirms to our initial expectation: classification accuracies are smaller when the classes are not wellseparated. It is surprising that the default termbasis performs better than other derived bases. In all the </context>
</contexts>
<marker>Shi, Malik, 1997</marker>
<rawString>J Shi and J Malik. 1997. Normalized cuts and image segmentation. In Proc of IEEE Conference on Computer Vision and Pattern Recognition, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilbert Strang</author>
</authors>
<title>Linear algebra and its applications.</title>
<date>1980</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="5011" citStr="Strang, 1980" startWordPosition="780" endWordPosition="781">of these techniques for unsupervised document classification. The same clustering algorithm — bipartite graph partitioning — is used in the final stage. The paper is organized as follows. Section 2 introduces the representation schemes used, section 3 describes the bipartite graph partitioning, and section 4 lists the experimental setting and results. 2 Feature representation schemes The term-by-document matrix is a representation of the documents using one type of basis: terms. It is possible to extract other underlying bases which may aid classification. For example, matrix diagonalization (Strang, 1980) produces an orthogonal collection of basis vectors that are ordered according to their &amp;quot;importance&amp;quot;. In this section, we provide a brief description of singular value decomposition, independent component analysis, and non-negative matrix factorization. In the following discussion, let A be the m x n document-by-term matrix. Here m is the number of documents and n is the number of words (or terms) used in the representation. 2.1 Singular value decomposition Matrix diagonalization can be performed only on square matrices. In contrast, singular value decomposition (SVD) exists for rectangular ma</context>
</contexts>
<marker>Strang, 1980</marker>
<rawString>Gilbert Strang. 1980. Linear algebra and its applications. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Strehl</author>
<author>J Ghosh</author>
<author>R Mooney</author>
</authors>
<title>Impact of similarity measures on web-page clustering.</title>
<date>2000</date>
<booktitle>In Workshop of Artificial Intelligence for Web Search (AAAI</booktitle>
<publisher>AAAI.</publisher>
<marker>Strehl, Ghosh, Mooney, 2000</marker>
<rawString>A Strehl, J Ghosh, and R Mooney. Impact of similarity measures on web-page clustering. In Workshop of Artificial Intelligence for Web Search (AAAI 2000). AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zha</author>
<author>X He</author>
<author>C Ding</author>
<author>H Simon</author>
<author>M Gu</author>
</authors>
<title>Bipartite graph partitioning and data clustering.</title>
<date>2001</date>
<booktitle>In Proc CKIM 01,</booktitle>
<pages>5--10</pages>
<contexts>
<context position="660" citStr="Zha et al., 2001" startWordPosition="89" endWordPosition="92">cation S H Srinivasan Applied Research Group Satyam Computer Services Ltd. 14 Langford Avenue, Lalbagh Road Bangalore - 560 025, INDIA SH_Srinivasan@satyam.com Abstract Unsupervised document classification is an important problem in practical text mining since training data is seldom available. In this paper we study the problem of term selection and the performance of various features for unsupervised text classification. The features studied are: principal components, independent components, and non-negative components. The clustering algorithm used is based on bipartite graph partitioning (Zha et al., 2001). The evaluation is performed using the newsgroups corpus. 1 Introduction Many natural language processing applications require text classification. Labeled data for training is typically unavailable in many situations. So unsupervised classification techniques are called for. Classification — both supervised and unsupervised — is usually done in two steps: feature extraction and classification. Feature extraction is basically a change of representation of the raw data. The most common representation used in document retrieval is the vector representation or the &amp;quot;bag-of-words&amp;quot; feature represen</context>
<context position="2500" citStr="Zha et al., 2001" startWordPosition="385" endWordPosition="388">red. Some representations use frequency information. The vectors corresponding to several documents can be arranged in a matrix — the so called &amp;quot;term-document&amp;quot; matrix. See (Berry et al., 1999) for a review of vector space representation. Other representation schemes can be in the vector space framework. Classification is done in the feature space. Several techniques exist for classification — naive bayes, support vector machines, k-means clustering, neural networks etc. See (Sebastiani, 2002) for a comprehensive survey. In this paper, we use the bipartite graph matching technique proposed in (Zha et al., 2001) for classification. See also (Shi and Malik, 1997). Bipartite graph matching (also known as spectral clustering) has several attractive properties including the property that if the data has a &amp;quot;good&amp;quot; clustering, the algorithm will find an &amp;quot;optimal&amp;quot; one. See (Kannan et al., 2000) for definitions and details. The term-by-document matrix can be considered as a representation of a weighted bipartite graph — the vertex sets being terms (T) and documents (D). The clustering technique partitions the vertices into disjoint sets (T = TUT&apos; and D = D1UD) such that intra cluster weight (between T1 &amp; Di a</context>
<context position="9592" citStr="Zha et al., 2001" startWordPosition="1599" endWordPosition="1602">atrix factorization has been shown to discover semantic features (Lee and Seung, 1999). 3 Bipartite graph partitioning Consider a weighted bipartite graph such as the term-document graph. Let the vertex sets be X and Y and the weight matrix be W. A partition of the graph into two subgraphs can be represented by a vertex partition of X and Y. Let A C X and B C Y. Then the partition is given by the subgraphs induced by A &amp; B and Ac &amp; BC. The cut between A and B is defined as cut (A, B) = W (A, BC) + W (Ac, B) where W (A, B) = iEAJEB The normalized cut, Ncut, is defined as (Shi and Malik, 1997) (Zha et al., 2001) cut(A, B) W (A,Y) + W (X, B) cut(Ac, BC) + W (Ac,Y) + W (X, Bc) The problem of finding partition with minimum Ncut can be posed as an eigenvalue problem (Shi and Malik, 1997) (Zha et al., 2001). The following is the algorithm derived in (Zha et al., 2001). Let e be a vector of all is (of appropriate dimension). 1. Compute diagonal matrices Dx and Dy as We = Dxe,WTe = Dye 2. Let W = Dx1i2WDy—li2 3. Compute the second largest left and right singular vectors of W,&amp;quot;±&amp;quot; and 4. Find cut points cx and cy for x = D xli2 &amp;quot;X&amp;quot; and y = Dyll y. 2 5. Let A ={i: xi &gt; cx}, Ac ={i: xi &lt; cx}, B= {j : yj &gt; cy}, </context>
<context position="11451" citStr="Zha et al., 2001" startWordPosition="1927" endWordPosition="1930"> NG5: comp.sys.mac.hardware NG6: comp.windows.x NG7: misc.forsale NG8: rec.autos NG9: rec.motorcycles NG10: rec.sport.baseball NG11: rec.sport.hockey NG12: sci.crypt NG13: sci.electronics NG14: sci.med NG15: sci.space NG16: soc.religion.christian NG17: talk.politics.guns NG18: talk.politics.mideast NG19: talk.politics.misc NG20: talk.religion.misc We consider three binary classification tasks: between NG1 &amp; NG2, NG10 &amp; NG11, and NG18 &amp; NG19. It can be seen that NG1 and NG2 are well-separated, NG10 and NG11 have some overlap, and NG18 and NG19 have more overlap. To compare with the results of (Zha et al., 2001), we perform clustering for four different sample sizes: 50, 100, 150, 200 and 250 files for the second class.3 The first class always has 50 files. Each run of the experiment was performed as follows. 1. The required number of files were chosen randomly from each class. 2. The documents were represented using N terms. (In the experiments reported below, N = 200.) The terms were chosen according to maximum information gain criterion. 2The database is available from http://www.cs.cmu.edu/afs/cs/project/theo-11/ www/naive-bayes.html. 3(Zha et al., 2001) use the sample sizes; 50, 100, 150, and 20</context>
<context position="12748" citStr="Zha et al. (2001)" startWordPosition="2144" endWordPosition="2147">3. The documents were represented using the N terms. We call this representation in term basis. 4. The document vectors were also represented using other bases - singular vectors, independent components, and nonnegative components. 5. The resulting matrices were subjected to bipartite graph partitioning. The best partitions were calculated by choosing cx and cy which minimize the objective function (Shi and Malik, 1997). The results of the experiments are shown in table 1. The results of (Zha et al., 2001) are shown in table 2 for comparison. The differences in results can be attributed to 1. Zha et al. (2001) use maximum mutual information to choose terms. 2. While we choose 200 terms in all experiments, number of terms used by Zha et al. (2001) is not available. It can be seen that the performance confirms to our initial expectation: classification accuracies are smaller when the classes are not wellseparated. It is surprising that the default termbasis performs better than other derived bases. In all the derived bases, we have an extra degree of freedom: the number of components chosen. In the previous set of experiments, we used 200 independent components and 100 nonnegative components. We redu</context>
<context position="15131" citStr="Zha et al., 2001" startWordPosition="2581" endWordPosition="2584">27 50/150 68.58 ± 0.42 66.55 ± 0.30 67.50 ± 0.34 66.29 ± 0.13 50/200 67.41 ± 0.91 68.12 ± 0.15 69.62 ± 0.25 68.14 ± 0.18 50/250 70.08 ± 1.43 67.94 ± 0.18 68.71 ± 0.10 68.15 ± 0.10 Table 1: Experimental results for three binary classification tasks. Percent correct figures are reported. The statistics were measure over 20 runs for each case. Mixture NG1/NG2 NG10/NG11 NG18/NG19 50/50 92.12 ± 3.52% 74.56 ± 8.93% 73.66 ± 10.53% 50/100 90.57 ± 3.11% 67.13 ± 7.17% 67.23 ± 7.84% 50/150 88.04 ± 3.90% 58.30 ± 5.99% 65.83 ± 2.79% 50/200 82.77 ± 5.24% 57.55 ± 5.69% 61.23 ± 9.88% Table 2: The results of (Zha et al., 2001). 4.1 Term selection Note that when the initial terms selection is done using equation 1, the documents are not labeled. But after first round of classification, we have tentative class labels available. This can be used to refine the choice of terms. For this we use class labels instead of document labels in equation 1. The overall scheme is shown in figure 2. Figure 3 shows how the scores improve with this iterative term selection. We can use this idea to improve classification accuracy. Table 4 gives the best case results. The figures are obtained by using the knowledge of classes when choo</context>
</contexts>
<marker>Zha, He, Ding, Simon, Gu, 2001</marker>
<rawString>H Zha, X He, C Ding, H Simon, and M Gu. 2001. Bipartite graph partitioning and data clustering. In Proc CKIM 01, Nov 5-10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>