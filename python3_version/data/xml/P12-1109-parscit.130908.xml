<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.998615">
A Broad-Coverage Normalization System for Social Media Language
</title>
<author confidence="0.992169">
Fei Liu Fuliang Weng Xiao Jiang
</author>
<affiliation confidence="0.8002075">
Research and Technology Center
Robert Bosch LLC
</affiliation>
<email confidence="0.979836">
{fei.liu, fuliang.weng}@us.bosch.com
{fixed-term.xiao.jiang}@us.bosch.com
</email>
<sectionHeader confidence="0.998509" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9965378">
Social media language contains huge amount
and wide variety of nonstandard tokens, cre-
ated both intentionally and unintentionally by
the users. It is of crucial importance to nor-
malize the noisy nonstandard tokens before
applying other NLP techniques. A major
challenge facing this task is the system cov-
erage, i.e., for any user-created nonstandard
term, the system should be able to restore the
correct word within its top n output candi-
dates. In this paper, we propose a cognitively-
driven normalization system that integrates
different human perspectives in normalizing
the nonstandard tokens, including the en-
hanced letter transformation, visual priming,
and string/phonetic similarity. The system
was evaluated on both word- and message-
level using four SMS and Twitter data sets.
Results show that our system achieves over
90% word-coverage across all data sets (a
10% absolute increase compared to state-of-
the-art); the broad word-coverage can also
successfully translate into message-level per-
formance gain, yielding 6% absolute increase
compared to the best prior approach.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863477272727">
The amount of user generated content has increased
drastically in the past few years, driven by the pros-
perous development of the social media websites
such as Twitter, Facebook, and Google+. As of June
2011, Twitter has attracted over 300 million users
and produces more than 2 billion tweets per week
(Twitter, 2011). In a broader sense, Twitter mes-
sages, SMS messages, Facebook updates, chat logs,
Emails, etc. can all be considered as “social text”,
which is significantly different from the traditional
news text due to the informal writing style and the
conversational nature. The social text serves as a
very valuable information source for many NLP ap-
plications, such as the information extraction (Ritter
et al., 2011), retrieval (Subramaniam et al., 2009),
summarization (Liu et al., 2011a), sentiment analy-
sis (Celikyilmaz et al., 2010), etc. Yet existing sys-
tems often perform poorly in this domain due the
to extensive use of the nonstandard tokens, emoti-
cons, incomplete and ungrammatical sentences, etc.
It is reported that the Stanford named entity recog-
nizer (NER) experienced a performance drop from
90.8% to 45.8% on tweets (Liu et al., 2011c); the
part-of-speech (POS) tagger and dependency parser
degraded 12.2% and 20.65% respectively on tweets
(Foster et al., 2011). It is therefore of great impor-
tance to normalize the social text before applying the
standard NLP techniques. Text normalization is also
crucial for building robust text-to-speech (TTS) sys-
tems, which need to determine the pronunciations
for nonstandard words in the social text.
The goal of this work is to automatically con-
vert the noisy nonstandard tokens observed in the
social text into standard English words. We aim
for a robust text normalization system with “broad
coverage”, i.e., for any user-created nonstandard to-
ken, the system should be able to restore the correct
word within its top n candidates (n = 1, 3,10...).
This is a very challenging task due to two facts:
first, there exists huge amount and a wide variety
of nonstandard tokens. (Liu et al., 2011b) found
more than 4 million distinct out-of-vocabulary to-
kens in the Edinburgh Twitter corpus (Petrovic et
al., 2010); second, the nonstandard tokens consist
</bodyText>
<page confidence="0.952976">
1035
</page>
<note confidence="0.979396">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<equation confidence="0.992554">
2gether(6326) togetha(919) tgthr(250) togeda (20)
2getha(1266) togather(207) t0gether (57) toqethaa (10)
2gthr (178) togehter (94) togeter (49) 2getter (10)
u (3240535) ya (460963) yo (252274) yaa (17015)
yaaa (7740) yew (7591) yuo (467) youz (426)
yoooooou (186) youy (105) yoiu (128) yoooouuuu (82)
</equation>
<tableCaption confidence="0.915635">
Table 1: Nonstandard tokens and their frequencies in the
Edinburgh Twitter corpus. The corresponding standard
words are “together” and “you”, respectively.
</tableCaption>
<bodyText confidence="0.999815777777778">
of a mixture of both unintentional misspellings and
intentionally-created tokens for various reasons1, in-
cluding the needs for speed, ease of typing (Crystal,
2009), sentiment expressing (e.g., “coooool” (Brody
and Diakopoulos, 2011)), intimacy and social pur-
pose (Thurlow, 2003), etc., making it even harder to
decipher the social messages. Table 1 shows some
example nonstandard tokens.
Existing spell checkers and normalization sys-
tems rely heavily on lexical/phonetic similarity to
select the correct candidate words. This may not
work well since a good portion of the correct words
lie outside the specified similarity threshold (e.g.,
(tomorrow, “tmrw”)2), yet the number of candidates
increases dramatically as the system strives to in-
crease the coverage by enlarging the threshold. (Han
and Baldwin, 2011) reported an average of 127 can-
didates per nonstandard token with the correct-word
coverage of 84%. The low coverage score also en-
forces an undesirable performance ceiling for the
candidate reranking approaches. Different from pre-
vious work, we tackle the text normalization prob-
lem from a cognitive-sensitive perspective and in-
vestigate the human rationales for normalizing the
nonstandard tokens. We argue that there exists a set
of letter transformation patterns that humans use to
decipher the nonstandard tokens. Moreover, the “vi-
sual priming” effect may play an important role in
human comprehension of the noisy tokens. “Prim-
ing” represents an implicit memory effect. For ex-
ample, if a person reads a list of words including the
word table, and is later asked to complete a word
starting with tab-, it is very likely that he answers
table since the person is primed.
In this paper, we propose a broad-coverage nor-
malization system by integrating three human per-
</bodyText>
<footnote confidence="0.9966012">
1For this reason, we will use the term “nonstandard tokens”
instead of “ill-formed tokens” throughout the paper.
2We use the form (standard word, “nonstandard token”) to
denote an example nonstandard token and its corresponding
standard word.
</footnote>
<bodyText confidence="0.999972533333333">
spectives, including the enhanced letter transforma-
tion, visual priming, and the string and phonetic
similarity. For an arbitrary nonstandard token, the
three subnormalizers each suggest their most con-
fident candidates from a different perspective. The
candidates can then be heuristically combined or
reranked using a message-level decoding process.
We evaluate the system on both word- and message-
level using four SMS and Twitter data sets. Results
show that our system can achieve over 90% word-
coverage with limited number of candidates and the
broad word-coverage can be successfully translated
into message-level performance gain. In addition,
our system requires no human annotations, therefore
can be easily adapted to different domains.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99867271875">
Text normalization, in its traditional sense, is the
first step of a speech synthesis system, where the
numbers, dates, acronyms, etc. found in the real-
world text were converted into standard dictionary
words, so that the system can pronounce them cor-
rectly. Spell checking plays an important role in this
process. (Church and Gale, 1991; Mays et al., 1991;
Brill and Moore, 2000) proposed to use the noisy
channel framework to generate a list of corrections
for any misspelled word, ranked by the correspond-
ing posterior probabilities. (Sproat et al., 2001) en-
hanced this framework by calculating the likelihood
probability as the chance of a noisy token and its as-
sociated tag being generated by a specific word.
With the rapid growth of SMS and social me-
dia content, text normalization system has drawn in-
creasing attention in the recent decade, where the
focus is on converting the noisy nonstandard tokens
in the informal text into standard dictionary words.
(Choudhury et al., 2007) modeled each standard En-
glish word as a hidden Markov model (HMM) and
calculated the probability of observing the noisy-
token under each of the HMM models; (Cook and
Stevenson, 2009) calculated the sum of the probabil-
ities of a noisy token being generated by a specific
word and a word formation process; (Beaufort et al.,
2010) employed the weighted finite-state machines
(FSMs) and rewriting rules for normalizing French
SMS; (Pennell and Liu, 2010) focused on tweets cre-
ated by handsets and developed a CRF tagger for
deletion-based abbreviation. The text normalization
problem was also tackled under the machine transla-
</bodyText>
<page confidence="0.990667">
1036
</page>
<bodyText confidence="0.999967404761905">
tion (MT) or speech recognition (ASR) framework.
(Aw et al., 2006) adapted a phrase-based MT model
for normalizing SMS and achieved satisfying per-
formance. (Kobus et al., 2008) showed that using a
statistical MT system in combination with an anal-
ogy of the ASR system improved performance in
French SMS normalization. (Pennell and Liu, 2011)
proposed a two-phase character-level MT system for
expanding the abbreviations into standard text.
Recent work also focuses on normalizing the
Twitter messages, which is generally considered a
more challenging task. (Han and Baldwin, 2011) de-
veloped classifiers for detecting the ill-formed words
and generated corrections based on the morpho-
phonemic similarity. (Liu et al., 2011b) proposed
to normalize the nonstandard tokens without explic-
itly categorizing them. (Xue et al., 2011) adopted
the noisy-channel framework and incorporated or-
thographic, phonetic, contextual, and acronym ex-
pansion factors in calculating the likelihood proba-
bilities. (Gouws et al., 2011) revealed that different
populations exhibit different shortening styles.
Most of the above systems limit their processing
scope to certain categories (e.g., deletion-based ab-
breviations, misspellings) or require large-scale hu-
man annotated corpus for training, which greatly
hinders the scalability of the system. In this paper,
we propose a novel cognitively-driven text normal-
ization system that robustly tackle both the unin-
tentional misspellings and the intentionally-created
noisy tokens. We propose a global context-based
approach to purify the automatically collected train-
ing data and learn the letter transformation pat-
terns without human supervision. We also propose
a cognitively-grounded “visual priming” approach
that leverages the “priming” effect to suggest the
candidate words. By integrating different perspec-
tives, our system can successfully mimic the hu-
man rationales and yield broad word-coverage on
both SMS and Twitter messages. To the best of our
knowledge, we are the first to integrate these human
perspectives in the text normalization system.
</bodyText>
<sectionHeader confidence="0.915709" genericHeader="method">
3 Broad-Coverage Normalization System
</sectionHeader>
<bodyText confidence="0.96220475">
In this section, we describe our broad-coverage nor-
malization system, which consists of four key com-
ponents. For a standard/nonstandard token, three
subnormalizers each suggest their most confident
</bodyText>
<figureCaption confidence="0.987912">
Figure 1: Examples f of o r e v nonstandard e r tokens generated by
</figureCaption>
<bodyText confidence="0.942723315789474">
h u b b y s o m e o n e
performing letter transformation on the dictionary words.
h b b i 4 1
candidates from a different perspective3: “Enhanced
Letter Transformation” automatically learns a set
of letter transformation patterns and is most effec-
tive in normalizing the intentionally created non-
standard tokens through letter insertion, repetition,
deletion, and substitution (Section 3.1); “Visual
Priming” proposes candidates based on the visual
cues and a primed perspective (Section 3.2); “Spell
Checker” corrects the misspellings (Section 3.3).
The fourth component, “Candidate Combination”
introduces various strategies to combine the candi-
dates with or without the local context (Section 3.4).
Note that it is crucial to integrate different human
perspectives so that the system is flexible in pro-
cessing both unintentional misspellings and various
intentionally-created noisy tokens.
</bodyText>
<subsectionHeader confidence="0.998582">
3.1 Enhanced Letter Transformation
</subsectionHeader>
<bodyText confidence="0.999672">
Given a noisy token ti seen in the text, the letter
transformation subnormalizer produces a list of cor-
rection candidates si under the noisy channel model:
</bodyText>
<equation confidence="0.997738">
s� = argmaxsip(silti) = arg maxsip(ti|si)p(si)
</equation>
<bodyText confidence="0.9984506">
where we assume each nonstandard token ti is de-
pendent on only one English word si, that is, we
are not considering acronyms (e.g., “bbl” for “be
back later”) in this study. p(si) can be calculated
as the unigram count from a background corpus. We
formulate the process of generating a nonstandard
token ti from the dictionary word si using a letter
transformation model, and use the model confidence
as the probability p(ti|si). Figure 1 shows several
example (word, token) pairs.
To form a nonstandard token, each letter in the
dictionary word can be labeled with: (a) one of the
0-9 digits; (b) one of the 26 characters including it-
self; (c) the null character “-”; (d) a letter combi-
nation4. This transformation process from dictio-
</bodyText>
<footnote confidence="0.7904194">
3For the dictionary word, we allow the subnormalizers to
either return the word itself or candidates that are the possibly
intended words in the given context (e.g., (with, “wit”)).
4The set of letter combinations used in this work are {ah, ai,
aw, ay, ck, ea, ey, ie, ou, te, wh}
</footnote>
<figure confidence="0.996481625">
(1) birthday --&gt; bday (2) photos --&gt; fotoz
b i r t h d a y
b - - - - d a y
p h o t o s
f - o t o z
(3) nothing --&gt; nuthin
n o t h i n g
n u t h i n -
</figure>
<page confidence="0.983442">
1037
</page>
<bodyText confidence="0.999848">
nary words to nonstandard tokens will be learned
by a character-level sequence labeling system us-
ing the automatically collected (word, token) pairs.
Next, we create a large lookup table by applying the
character-level labeling system to the standard dic-
tionary words and generate multiple variations for
each word using the n-best labeling output, the la-
beling confidence is used as p(ti|si). During testing,
we search this lookup table to find the best candidate
words for the nonstandard tokens. For tokens with
letter repetition, we first generate a set of variants
by varying the repetitive letters (e.g. Ci = {“pleas”,
“pleeas”, “pleaas”, “pleeaas”, ‘pleeeaas”} for ti =
{“pleeeaas”}), then select the maximum posterior
probability among all the variants:
</bodyText>
<equation confidence="0.9911485">
p(ti|si) = max
�tiECi
</equation>
<bodyText confidence="0.999865636363636">
Different from the work in (Liu et al., 2011b), we
enhanced the letter transformation process with two
novel aspects: first, we devise a set of phoneme-,
syllable-, morpheme- and word-boundary based fea-
tures that effectively characterize the formation pro-
cess of the nonstandard tokens; second, we propose
a global context-aware approach to purify the auto-
matically collected training (word, token) pairs, re-
sulting system yielded similar performance but with
only one ninth of the original data. We name this
subnormalizer “Enhanced Letter Transformation”.
</bodyText>
<subsectionHeader confidence="0.931504">
3.1.1 Context-Aware Training Pair Selection
</subsectionHeader>
<bodyText confidence="0.999986757575757">
Manual annotation of the noisy nonstandard to-
kens takes a lot of time and effort. (Liu et al., 2011b)
proposed to use Google search engine to automati-
cally collect large amount of training pairs. Yet the
resulting (work, token) pairs are often noisy, con-
taining pairs such as (events, “ents”), (downtown,
“downto”), etc. The ideal training data should con-
sist of the most frequent nonstandard tokens paired
with the corresponding corrections, so that the sys-
tem can learn from the most representative letter
transformation patterns.
Motivated by research on word sense disambigua-
tion (WSD) (Mihalcea, 2007), we hypothesize the
nonstandard token and the standard word share a lot
of common terms in their global context. For exam-
ple, “luv” and “love” share “i”, “you”, “u”, “it”, etc.
among their top context words. Based on this find-
ing, we propose to filter out the low-quality train-
ing pairs by evaluating the global contextual simi-
larity between the word and token. To the best of
our knowledge, we are the first to explore this global
contextual similarity for the text normalization task.
Given a noisy (word, token) pair, we construct
two context vectors vi and vj by collecting the
most frequent terms appearing before or after the
work/token. We consider two terms on each side
of the word/token as context and restrict the vector
length to the top 100 terms. The frequency informa-
tion were calculated using a large background cor-
pus; stopwords were not excluded from the context
vector. The contextual similarity of the (word, to-
ken) pair is defined as the cosine similarity between
the context vectors vi and vj:
</bodyText>
<equation confidence="0.9938695">
n ��n
k=1 w2 i,k � k=1 w2 j,k
</equation>
<bodyText confidence="0.993509">
where wi,k is the weight of term tk within the con-
text of term ti. The term weights are defined using a
normalized TF-IDF method:
</bodyText>
<equation confidence="0.923882">
TFi,k N
TFi x log(DFk )
</equation>
<bodyText confidence="0.970889933333334">
where T Fi,k is the count of term tk appearing within
the context of term ti; TFi is the total count of ti in
TFi,k
the corpus. is therefore the relative frequency
TFi
of tk appearing in the context of ti; log(DFk�) de-
notes the inverse document frequency of tk, calcu-
lated as the logarithm of total tweets (N) divided by
the number of tweets containing tk.
To select the most representative (word, token)
pairs for training, we rank the automatically col-
lected 46,288 pairs by the token frequency, filter
out pairs whose contextual similarity lower than a
threshold 0 (set empirically at 0.0003), and retain
only the top portion (5,000 pairs) for experiments.
</bodyText>
<subsectionHeader confidence="0.758672">
3.1.2 Character-level Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.999767666666667">
For a dictionary word si, we use the conditional
random fields (CRF) model to perform character-
level labeling to generate its variant ti. In the train-
ing stage, we align the collected (word, token) pairs
at the character level (Liu et al., 2011b), then con-
struct a feature vector for each letter of the dictio-
nary word, using its mapped character as the ref-
erence label. This aligned data set is used to train
a CRF model (Lafferty et al., 2001; Kudo, 2005)
</bodyText>
<equation confidence="0.9634844">
p(�ti|si)
�n S
Contextim(vi, vj) =
=1 wi,k x wj,k
wi,k =
</equation>
<page confidence="0.894674">
1038
</page>
<table confidence="0.879992272727273">
Character a d v e r t i s e m e n t s sponding syllable/morpheme/word.
Phoneme AE D V ER ER T AY Z M AH N T S We use the aforementioned features to train the
CRF model, then apply the model on dictionary
words si to generate multiple variations ti for each
word. When a nonstandard token is seen during test-
ing, we apply the noisy channel to generate a list of
best candidate words: s� = arg maxsip(ti|si)p(si).
3.2 Visual Priming Approach
A second key component of the broad-coverage nor-
malization system is a novel “Visual Priming” sub-
normalizer. It is built on a cognitively-driven “prim-
</table>
<bodyText confidence="0.922644892857143">
ing” effect, which has not been explored by other
studies yet was shown to be effective across all our
data sets.
“Priming”7 is an implicit memory effect caused
by spreading neural networks (Tulving and Stark,
1982). As an example, in the word-stem comple-
tion task, participants are given a list of study words,
and then asked to complete word “stems” consisting
of first 3 letters. A priming effect is observed when
participants complete stems with words on the study
list more often than with the novel words. The study
list activates parts of the human brain right before
the stem completion task, later when a word stem is
seen, less additional activation is needed for one to
choose a word from the study list.
We argue that the “priming” effect may play an
important role in human comprehension of the noisy
tokens. A person familiarized with the “social talk”
is highly primed with the most commonly used
words; later when a nonstandard token shows only
minor visual cues or visual stimulus, it can still be
quickly recognized by the person. In this process,
the first letter or first few letters of the word serve
as a very important visual stimulus. Based on this
assumption, we introduce the “priming” subnormal-
izer based only on the word frequency and the minor
visual stimulus. Concretely, this approach proposes
candidate words based on the following equation:
</bodyText>
<table confidence="0.875723636363636">
VisualPrim(si|ti) = len(LCS(ti, si))x log(TF(si))
len(ti)
Where TF(si) is the term frequency of si as in the
background social text corpus; log(TF(si)) primes
the system with the most common words in the so-
cial text; LCS(·) means the longest common char-
acter subsequence; len(·) denotes the length of the
Phoneme boundary O O O B1 L1 O O O O O O O O O
Syllable boundary B L B I L B I I L B I I I L
Morpheme boundary B I I I I I I I L B I I L U
Word boundary B I I I I I I I I I I I I L
</table>
<tableCaption confidence="0.534289">
Table 2: Example boundary tags for word “advertise-
</tableCaption>
<bodyText confidence="0.932076379310345">
ments” on the phoneme-, syllable-, morpheme-, and
word-level, labeled with the “BILOU” encoding scheme.
with L-BFGS optimization. We use the charac-
ter/phoneme n-gram and binary vowel features as in
(Liu et al., 2011b), but develop a set of boundary
features to effectively characterize the letter trans-
formation process.
We notice that in creating the nonstandard tokens,
humans tend to drop certain letter units from the
word or replace them with other letters. For exam-
ple, in abbreviating “advertisements” to “ads”, hu-
mans may first break the word into smaller units
“ad-ver-tise-ment-s”, then drop the middle parts.
This also conforms with the word construction the-
ory where a word is composed of smaller units and
construction rules. Based on this assumption, we
decompose the dictionary words on the phoneme-,
syllable-, morpheme-, and word-level5 and use the
“BILOU” tagging scheme (Ratinov and Roth, 2009)
to represent the unit boundary, where “BILOU”
stands for B(egin), I(nside), L(ast), O(utside), and
U(nit-length) of the corresponding unit6. Example
“BILOU” boundary tags were shown in Table 2.
On top of the boundary tags, we develop a set of
conjunction features to accurately pinpoint the cur-
rent character position. We consider conjunction
features formed by concatenating character position
in syllable and current syllable position in the word
(e.g., conjunction feature “L B” for the letter “d” in
</bodyText>
<figureCaption confidence="0.914593272727273">
Table 2). A similar set of features are also devel-
oped on morpheme level. We consider conjunction
of character/vowel feature and their boundary tags
on the syllable/morpheme/word level; conjunction
of phoneme and phoneme boundary tags, and ab-
solute position of current character within the corre-
5Phoneme decomposition is generated using the (Jiampo-
jamarn et al., 2007) algorithm to map up to two letters to
phonemes (2-to-2 alignment); syllable boundary acquired by
the hyphenation algorithm (Liang, 1983); morpheme boundary
determined by toolkit Morfessor 1.0 (Creutz and Lagus, 2005).
</figureCaption>
<table confidence="0.551516714285714">
6For phoneme boundary, we use “B1” and “L1” to represent
two different characters aligned to one phoneme and “B2”, “L2”
represent same characters aligned to one phoneme.
7http://en.wikipedia.org/wiki/Priming (psychology)
1039
character sequence. Together len(LCS(ti,si))
len(ti) pro-
</table>
<bodyText confidence="0.999857652173913">
vides the minor visual stimulus from ti. Note that
the first character has been shown to be a crucial vi-
sual cue for the brain to understand jumbled words
(Davis, ), we therefore consider as candidates only
those words si that start with the same character as
ti. In the case that the nonstandard token ti starts
with a digit (e.g., “2moro”), we use the mostly likely
corresponding letter to search the candidates (those
starting with letter “t”). This setting also effectively
reduces the candidate search space.
The “visual priming” subnormalizer promotes the
candidate words that are frequently used in the so-
cial talk and also bear visual similarity with the
given noisy token. It slightly deviates from the tradi-
tional “priming” notion in that the frequency infor-
mation were acquired from the global corpus rather
than from the prior context. This approach also in-
herently follows the noisy channel framework, with
p(ti|si) represents the visual stimulus and p(si) be-
ing the logarithm of frequency. The candidate words
are ranked by s� = arg maxsiV isualPrim(si|ti).
We show that the “priming” subnormalizer is robust
across data sets abide its simplistic representation.
</bodyText>
<subsectionHeader confidence="0.999691">
3.3 Spell Checker
</subsectionHeader>
<bodyText confidence="0.999955222222222">
The third subnormalizer is the spell checker, which
combines the string and phonetic similarity algo-
rithms and is most effective in normalizing the mis-
spellings. We use the Jazzy spell checker (Idzelis,
2005) that integrates the DoubleMetaphone phonetic
matching algorithm and the Levenshtein distance us-
ing the near-miss strategy, which enables the in-
terchange of two adjacent letters, and the replac-
ing/deleting/adding of letters.
</bodyText>
<subsectionHeader confidence="0.976054">
3.4 Candidate Combination
</subsectionHeader>
<bodyText confidence="0.999937324324324">
Each of the three subnormalizers is a stand-alone
system and can suggest corrections for the nonstan-
dard tokens. Yet we show that each subnormal-
izer mimics a different perspective that humans use
to decode the nonstandard tokens, as a result, our
broad-coverage normalization system is built by in-
tegrating candidates from the three subnormalizers
using various strategies.
For a noisy token seen in the informal text, the
most convenient way of system combination is to
harvest up to n candidates from each of the sub-
normalizers, and use the pool of candidates (up to
3n) as the system output. This sets an upper bound
for other candidate combination strategies, and we
name this approach “Oracle”.
A second combination strategy is to give higher
priority to candidates from high-precision subsys-
tems. Both “Letter Transformation” and “Spell
Checker” have been shown to have high precision in
suggesting corrections (Liu et al., 2011b), while “Vi-
sual Priming” may not yield high precision due to
its definition. We therefore take the top-3 candidates
from each of the “Letter Tran.” and “Spell Checker”
subsystems, but put candidates from “Letter Tran.”
ahead of “Spell Checker” if the confidence of the
best candidate is greater than a threshold A and vice
versa. The list of candidates is then compensated us-
ing the “Visual Priming” output until the total num-
ber reaches n. We name this approach “Word-level”
combination since no message-level context infor-
mation is involved.
Based on the “Word-level” combination output,
we can further rerank all the candidates using a
message-level Viterbi decoding process (Pennell and
Liu, 2011) where the local context information is
used to select the best candidate. This approach is
named “Message-level” combination.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995549">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9998981875">
We use four SMS and Twitter data sets to evaluate
the system effectiveness. Statistics of these data sets
are summarized in Table 3. Data set (1) to (3) are
used for word-level evaluation; data set (4) for both
word- and message-level evaluation. In Table 3, we
also present the number of distinct nonstandard to-
kens found in each data set, and notice that only a
small portion of the nonstandard tokens correspond
to multiple standard words. We calculate the dic-
tionary coverage of the manually annotated words
since this sets an upper bound for any normaliza-
tion system. We use the Edinburgh Twitter corpus
(Petrovic et al., 2010) as the background corpus for
frequency calculation, and a dictionary containing
82,324 words.8 The nonstandard tokens may consist
of both numbers/characters and apostrophe.
</bodyText>
<footnote confidence="0.99531975">
8The dictionary is created by combining the CMU (CMU,
2007) and Aspell (Atkinson, 2006) dictionaries and dropping
words with frequency &lt; 20 in the background corpus. “rt” and
all single characters except “a” and “i” are excluded.
</footnote>
<page confidence="0.891842">
1040
</page>
<table confidence="0.999516">
Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference
Tokens %Nonstan. Tkns of cands
w/ Multi-cands
SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007)
Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011)
SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011)
Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011)
</table>
<tableCaption confidence="0.999861">
Table 3: Statistics of different SMS and Twitter data sets.
</tableCaption>
<bodyText confidence="0.9994767">
The goal of word-level normalization is to convert
the list of distinct nonstandard tokens into standard
words. For each nonstandard token, the system is
considered correct if any of the corresponding stan-
dard words is among the n-best output from the sys-
tem. We adopt this word-level n-best accuracy to
make our results comparable to other state-of-the-art
systems. On message-level, we evaluate the 1-best
system output using precision, recall, and f-score,
calculated respective to the nonstandard tokens.
</bodyText>
<subsectionHeader confidence="0.984515">
4.2 Word-level Results
</subsectionHeader>
<bodyText confidence="0.9998484">
The word-level results are presented in Table 4, 5,
and 6, evaluated on data set (1), (2), (3) respectively.
We present the n-best accuracy (n = 1, 3, 10, 20) of
the system as well as the “Oracle” results generated
by pooling the top-20 candidates from each of the
three subnormalizers. The best prior results on the
data sets are also included in the tables.
We notice that the broad-coverage system outper-
forms all other systems on the reported data sets.
It achieves about 90% word-level accuracy on data
set (1) and (2) with the top-10 candidates (an aver-
age 10% performance gain compared to (Liu et al.,
2011b)). This is of crucial importance to a normal-
ization system, since the high accuracy and limited
number of candidates will enable more sophisticated
reranking or supervised learning techniques to se-
lect the best candidate. We also observe the “Ora-
cle” system has averagely only 5% gap to the dic-
tionary coverage. A detailed analysis shows that the
human annotators perform many semantic/grammar
corrections as well as inconsistent annotations, e.g.,
(sleepy, “zzz”), (disliked, “unliked”). These are out
of the capabilities of the current text normalization
system and partly explains the remaining 5% gap.
Regarding the subnormalizer performance, the
spell checker yields only 50% to 60% accuracy on
all data sets, indicating that the vast amount of the
intentionally created nonstandard tokens can hardly
be tackled by a system relies solely on the lexi-
cal/phonetic similarity. The “Visual Priming” sub-
</bodyText>
<table confidence="0.999439363636364">
SMS Dataset Word Level Accuracy (%)
(303 pairs)
1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 43.89 55.45 56.77 56.77 n/a
Visual Priming 54.13 74.92 84.82 87.13 n/a
Enhanced Letter Tran. 61.06 74.92 80.86 82.51 n/a
Broad-Cov. System 64.36 80.20 89.77 91.75 94.06
(Pennell et al., 2011)* 60.39 74.58 75.57 75.57 n/a
(Liu et al., 2011) 62.05 75.91 81.19 81.19 n/a
(Cook et al., 2009) 59.4 n/a 83.8 87.8 n/a
(Choudhury et al., 2007)* 59.9 n/a 84.3 88.7 n/a
</table>
<tableCaption confidence="0.994732">
Table 4: Word-level results on data set (1). * denotes
system requires human annotations for training.
</tableCaption>
<table confidence="0.999732125">
Twitter Dataset Word Level Accuracy (%)
(3802 pairs)
1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 47.19 56.92 59.13 59.18 n/a
Visual Priming 54.34 70.59 80.83 84.74 n/a
Enhanced Letter Tran. 61.05 70.07 74.04 74.75 n/a
Broad-Cov. System 69.81 82.51 92.24 93.79 95.71
(Liu et al., 2011) 68.88 78.27 80.93 81.17 n/a
</table>
<tableCaption confidence="0.998969">
Table 5: Word-level results on data set (2).
</tableCaption>
<bodyText confidence="0.999928368421053">
normalizer performs surprisingly well and shows ro-
bust performance across all data sets. A minor side-
effect is that the candidates were restricted to have
the same first letter with the noisy token, this sets
the upper bound of the approach to 89.77%, 92.45%,
and 93.51%, respectively on data set (1), (2), and (3).
Compared to other subnormalizers, the “Enhanced
Letter Tran.” is effective at normalizing intention-
ally created tokens and has better precision regard-
ing its top candidate (n = 1). We demonstrate the
context-aware training pair selection results in Fig-
ure 2, by plotting the learning curve using different
amounts of training data, ranging from 1,000 (word,
token) pairs to the total 46,288 pairs. We notice that
the system can effectively learn the letter transfor-
mation patterns from a small number of high quality
training pairs. The final system was trained using the
top 5,000 pairs and the lookup table was created by
generating 50 variations for each dictionary word.
</bodyText>
<subsectionHeader confidence="0.998777">
4.3 Message-level Results
</subsectionHeader>
<bodyText confidence="0.999641333333333">
The goal of message-level normalization is to re-
place each occurrence of the nonstandard token with
the candidate word that best fits the local context.
</bodyText>
<page confidence="0.944973">
1041
</page>
<table confidence="0.999614125">
SMS/Twitter Dataset Word Level Accuracy (%)
(2404 pairs)
1-best 3-best 10-best 20-best Oracle
Jazzy Spell Checker 39.89 46.51 48.54 48.67 n/a
Visual Priming 54.12 68.59 78.83 83.11 n/a
Enhanced Letter Tran. 57.65 67.18 71.01 71.88 n/a
Broad-Cov. System 64.39 78.29 86.56 88.69 91.60
(Pennell et al., 2011)* 37.40 n/a n/a 72.38 n/a
</table>
<tableCaption confidence="0.9974585">
Table 6: Word-level results on data set (3). * denotes
system requires human annotations for training.
</tableCaption>
<figure confidence="0.996445625">
76
74
72
70
68
66
64
1K
</figure>
<figureCaption confidence="0.999355">
Figure 2: Learning curve of the enhanced letter transfor-
mation system using random training pair selection or the
context-aware approach. Evaluated on data set (2).
</figureCaption>
<bodyText confidence="0.999992142857143">
We use the word-level “Broad-Cov. System” for
candidate suggestion and the Viterbi algorithm for
message-level decoding. The system is evaluated on
data set (4) and results shown in Table 7. Following
research in (Han and Baldwin, 2011), we focus on
the the normalization task and assume perfect non-
standard token detection.
The “Word-level w/o Context” results are gen-
erated by replacing each nonstandard token using
the 1-best word-level candidate. Although the re-
placement process is static, it results in 70.97% f-
score due to the high performance of the word-level
system. We explore two language models (LM)
for the Viterbi decoding process. First, a bigram
LM is trained using the Edinburgh Twitter corpus
(53,794,549 English tweets) with the SRILM toolkit
(Stolcke, 2002) and Kneser-Ney smoothing; second,
we retrieve the bigram probabilities from the Mi-
crosoft Web N-gram API (Wang et al., 2010) since
this represents a more comprehensive web-based
corpus. During decoding, we use the “VisualPrim”
score as the emission probability, since this score
best fits the log scale and applies to all candidates.
For the Twitter LM, we apply a scaling factor of
0.5 to the “VisualPrim” score to make it compara-
ble in scale to the LM probabilities. We use the 3-
best word-level candidates for Viterbi decoding. In
addition, we add the commonly used corrections for
</bodyText>
<table confidence="0.999138857142857">
Twitter Dataset Message-level P/R/F
(549 Tweets)
Precision (%) Recall (%) F-score (%)
Word-level w/o Context 75.69 66.81 70.97
w/ Context Web LM 79.12 77.11 78.10
Twitter LM 84.13 78.38 81.15
(Han and Baldwin, 2011)* 75.30 75.30 75.30
</table>
<tableCaption confidence="0.9973715">
Table 7: Message-level results on data set (4). * denotes
system requires human annotations for training.
</tableCaption>
<bodyText confidence="0.999459083333333">
16 single-characters, e.g., for “r”, “c”, we add “are”,
“see” to the candidate list if they are not already pre-
sented. A default “VisualPrim” score (77 = 25) is
used for these candidates. As seen from Table 7,
both Web LM and Twitter LM achieve better perfor-
mance than the best prior results, with Twitter LM
outperforms the Web LM, yielding a f-score of 81%.
This shows that a vanilla Viterbi decoding process is
able to outperform the fine-tuned supervised system
given competitive word-level candidates. In future,
we will investigate other comprehensive message-
level candidate reranking process.
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999983277777778">
In this paper, we propose a broad-coverage normal-
ization system for the social media language with-
out using the human annotations. It integrates three
key components: the enhanced letter transformation,
visual priming, and string/phonetic similarity. The
system was evaluated on both word- and message-
level using four SMS and Twitter data sets. We show
that our system achieves over 90% word-coverage
across all data sets and the broad word-coverage can
be successfully translated into message-level perfor-
mance gain. We observe that the social media is an
emotion-rich language, therefore future normaliza-
tion system will need to address various sentiment-
related expressions, such as emoticons (“:d”, “X-
8”), interjections (“bwahaha”, “brrrr”), acronyms
(“lol”, “lmao”), etc., whether and how these expres-
sions should be normalized is an unaddressed issue
and worths future investigation.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994515">
We thank the three anonymous reviewers for their
insightful comments and valuable input. We thank
Prof. Yang Liu, Deana Pennell, Bo Han, and Prof.
Tim Baldwin for sharing the annotated data and the
useful discussions. Part of this work was done while
Xiao Jiang was a research intern in Bosch Research.
</bodyText>
<page confidence="0.994842">
1042
</page>
<sectionHeader confidence="0.996037" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996227588652482">
Kevin Atkinson. 2006. Gnu aspell. http://aspell.net/.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of COLING/ACL, pages 33–40.
Richard Beaufort, Sophie Roekhaut, Louise-Am´elie
Cougnon, and C´edrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings ofACL, pages 770–
779.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings ofACL.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! Using word
lengthening to detect sentiment in microblogs. In Pro-
ceedings of EMNLP, pages 562–570.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng.
2010. Probabilistic model-based sentiment analysis of
twitter messages. In Proceedings of the IEEE Work-
shop on Spoken Language Technology, pages 79–84.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157–174.
Kenneth W. Church and William A. Gale. 1991. Prob-
ability scoring for spelling correction. Statistics and
Computing, 1:93–103.
CMU. 2007. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text messages normalization. In Pro-
ceedings of the NAACL HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71–
78.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. In Computer
and Information Science, Report A81, Helsinki Uni-
versity of Technology.
David Crystal. 2009. Txtng: The gr8 db8. Oxford Uni-
versity Press.
Matt Davis. Reading jumbled texts. http://www.mrc-
cbu.cam.ac.uk/personal/matt.davis/Cmabrigde/.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS tagging and parsing the twitterverse. In
Proceedings of the AAAI Workshop on Analyzing Mi-
crotext, pages 20–25.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011. Contextual bearing on linguistic
variation in social media. In Proceedings of the ACL
Workshop on Language in Social Media, pages 20–29.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
Proceedings ofACL, pages 368–378.
Mindaugas Idzelis. 2005. Jazzy: The java open source
spell checker. http://jazzy.sourceforge.net/.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proceedings of HLT/NAACL, pages 372–
379.
Catherine Kobus, Franc¸ois Yvon, and G´eraldine
Damnati. 2008. Normalizing sms: Are two metaphors
better than one? In Proceedings of COLING, pages
441–448.
Taku Kudo. 2005. CRF++: Yet another CRF took kit.
http://crfpp.sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282–289.
Franklin Mark Liang. 1983. Word hy-phen-a-tion by
com-put-er. In PhD Dissertation, Stanford University.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why
is ”sxsw” trending? Exploring multiple text sources
for twitter topic summarization. In Proceedings of the
ACL Workshop on Language in Social Media (LSM),
pages 66–75.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011b. Insertion, deletion, or substitution? Normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings ofACL, pages 71–76.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011c. Recognizing named entities in tweets.
In Proceedings ofACL, pages 359–367.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Information
Processing and Management: An International Jour-
nal, 27(5):517–522.
Rada Mihalcea. 2007. Using wikipedia for auto-
matic word sense disambiguation. In Proceedings of
NAACL, pages 196–203.
Deana L. Pennell and Yang Liu. 2010. Normalization
of text messages for text-to-speech. In Proceedings of
ICASSP, pages 4842–4845.
Deana L. Pennell and Yang Liu. 2011. A character-
level machine translation approach for normalization
of sms abbreviations. In Proceedings of the 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 974–982.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
1043
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25–26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL, pages 147–155.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In Proceedings of EMNLP.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287–333.
Andreas Stolcke. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901–904.
L. Venkata Subramaniam, Shourya Roy, Tanveer A.
Faruquie, and Sumit Negi. 2009. A survey of types
of text noise and techniques to handle noisy text. In
Proceedings of AND.
Crispin Thurlow. 2003. Generation txt? the sociolin-
guistics of young people’s text-messaging. Discourse
Analysis Online.
Endel Tulving and Daniel L. Schacter; Heather A. Stark.
1982. Priming effects in word fragment comple-
tion are independent of recognition memory. Journal
of Experimental Psychology: Learning, Memory and
Cognition, 8(4).
Twitter. 2011. http://en.wikipedia.org/wiki/Twitter.
Kuansan Wang, Christopher Thrasher, Evelyne Viegas,
Xiaolong Li, and Bo june (Paul) Hsu. 2010. An
overview of microsoft web n-gram corpus and appli-
cations. In Proceedings of NAACL-HLT, pages 45–48.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI
Workshop on Analyzing Microtext, pages 74–79.
</reference>
<page confidence="0.99355">
1044
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.363436">
<title confidence="0.999813">A Broad-Coverage Normalization System for Social Media Language</title>
<author confidence="0.798488">Fei Liu Fuliang Weng Xiao</author>
<title confidence="0.715401">Research and Technology</title>
<author confidence="0.998031">Robert Bosch</author>
<abstract confidence="0.987560153846154">Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. A major challenge facing this task is the system covi.e., for nonstandard term, the system should be able to restore the word within its top candidates. In this paper, we propose a cognitivelydriven normalization system that integrates in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. The system was evaluated on both wordand messagelevel using four SMS and Twitter data sets. Results show that our system achieves over 90% word-coverage across all data sets (a 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kevin Atkinson</author>
</authors>
<date>2006</date>
<note>Gnu aspell. http://aspell.net/.</note>
<contexts>
<context position="26840" citStr="Atkinson, 2006" startWordPosition="4305" endWordPosition="4306"> number of distinct nonstandard tokens found in each data set, and notice that only a small portion of the nonstandard tokens correspond to multiple standard words. We calculate the dictionary coverage of the manually annotated words since this sets an upper bound for any normalization system. We use the Edinburgh Twitter corpus (Petrovic et al., 2010) as the background corpus for frequency calculation, and a dictionary containing 82,324 words.8 The nonstandard tokens may consist of both numbers/characters and apostrophe. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and Twitter data sets. The goal</context>
</contexts>
<marker>Atkinson, 2006</marker>
<rawString>Kevin Atkinson. 2006. Gnu aspell. http://aspell.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for sms text normalization.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="8669" citStr="Aw et al., 2006" startWordPosition="1334" endWordPosition="1337">ted the probability of observing the noisytoken under each of the HMM models; (Cook and Stevenson, 2009) calculated the sum of the probabilities of a noisy token being generated by a specific word and a word formation process; (Beaufort et al., 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine transla1036 tion (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections b</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for sms text normalization. In Proceedings of COLING/ACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Beaufort</author>
<author>Sophie Roekhaut</author>
<author>Louise-Am´elie Cougnon</author>
<author>C´edrick Fairon</author>
</authors>
<title>A hybrid rule/model-based finite-state framework for normalizing sms messages.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>770--779</pages>
<contexts>
<context position="8303" citStr="Beaufort et al., 2010" startWordPosition="1279" endWordPosition="1282"> by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. (Choudhury et al., 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisytoken under each of the HMM models; (Cook and Stevenson, 2009) calculated the sum of the probabilities of a noisy token being generated by a specific word and a word formation process; (Beaufort et al., 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine transla1036 tion (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French S</context>
</contexts>
<marker>Beaufort, Roekhaut, Cougnon, Fairon, 2010</marker>
<rawString>Richard Beaufort, Sophie Roekhaut, Louise-Am´elie Cougnon, and C´edrick Fairon. 2010. A hybrid rule/model-based finite-state framework for normalizing sms messages. In Proceedings ofACL, pages 770– 779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="7367" citStr="Brill and Moore, 2000" startWordPosition="1122" endWordPosition="1125"> number of candidates and the broad word-coverage can be successfully translated into message-level performance gain. In addition, our system requires no human annotations, therefore can be easily adapted to different domains. 2 Related work Text normalization, in its traditional sense, is the first step of a speech synthesis system, where the numbers, dates, acronyms, etc. found in the realworld text were converted into standard dictionary words, so that the system can pronounce them correctly. Spell checking plays an important role in this process. (Church and Gale, 1991; Mays et al., 1991; Brill and Moore, 2000) proposed to use the noisy channel framework to generate a list of corrections for any misspelled word, ranked by the corresponding posterior probabilities. (Sproat et al., 2001) enhanced this framework by calculating the likelihood probability as the chance of a noisy token and its associated tag being generated by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. (Choudhury et al</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! Using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>562--570</pages>
<contexts>
<context position="4436" citStr="Brody and Diakopoulos, 2011" startWordPosition="666" endWordPosition="669"> togather(207) t0gether (57) toqethaa (10) 2gthr (178) togehter (94) togeter (49) 2getter (10) u (3240535) ya (460963) yo (252274) yaa (17015) yaaa (7740) yew (7591) yuo (467) youz (426) yoooooou (186) youy (105) yoiu (128) yoooouuuu (82) Table 1: Nonstandard tokens and their frequencies in the Edinburgh Twitter corpus. The corresponding standard words are “together” and “you”, respectively. of a mixture of both unintentional misspellings and intentionally-created tokens for various reasons1, including the needs for speed, ease of typing (Crystal, 2009), sentiment expressing (e.g., “coooool” (Brody and Diakopoulos, 2011)), intimacy and social purpose (Thurlow, 2003), etc., making it even harder to decipher the social messages. Table 1 shows some example nonstandard tokens. Existing spell checkers and normalization systems rely heavily on lexical/phonetic similarity to select the correct candidate words. This may not work well since a good portion of the correct words lie outside the specified similarity threshold (e.g., (tomorrow, “tmrw”)2), yet the number of candidates increases dramatically as the system strives to increase the coverage by enlarging the threshold. (Han and Baldwin, 2011) reported an average</context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! Using word lengthening to detect sentiment in microblogs. In Proceedings of EMNLP, pages 562–570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
<author>Junlan Feng</author>
</authors>
<title>Probabilistic model-based sentiment analysis of twitter messages.</title>
<date>2010</date>
<booktitle>In Proceedings of the IEEE Workshop on Spoken Language Technology,</booktitle>
<pages>79--84</pages>
<contexts>
<context position="2171" citStr="Celikyilmaz et al., 2010" startWordPosition="321" endWordPosition="324">ttracted over 300 million users and produces more than 2 billion tweets per week (Twitter, 2011). In a broader sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, which is significantly different from the traditional news text due to the informal writing style and the conversational nature. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al., 2011), retrieval (Subramaniam et al., 2009), summarization (Liu et al., 2011a), sentiment analysis (Celikyilmaz et al., 2010), etc. Yet existing systems often perform poorly in this domain due the to extensive use of the nonstandard tokens, emoticons, incomplete and ungrammatical sentences, etc. It is reported that the Stanford named entity recognizer (NER) experienced a performance drop from 90.8% to 45.8% on tweets (Liu et al., 2011c); the part-of-speech (POS) tagger and dependency parser degraded 12.2% and 20.65% respectively on tweets (Foster et al., 2011). It is therefore of great importance to normalize the social text before applying the standard NLP techniques. Text normalization is also crucial for building</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, Feng, 2010</marker>
<rawString>Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng. 2010. Probabilistic model-based sentiment analysis of twitter messages. In Proceedings of the IEEE Workshop on Spoken Language Technology, pages 79–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="7975" citStr="Choudhury et al., 2007" startWordPosition="1222" endWordPosition="1225">nd Moore, 2000) proposed to use the noisy channel framework to generate a list of corrections for any misspelled word, ranked by the corresponding posterior probabilities. (Sproat et al., 2001) enhanced this framework by calculating the likelihood probability as the chance of a noisy token and its associated tag being generated by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. (Choudhury et al., 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisytoken under each of the HMM models; (Cook and Stevenson, 2009) calculated the sum of the probabilities of a noisy token being generated by a specific word and a word formation process; (Beaufort et al., 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled un</context>
<context position="27160" citStr="Choudhury et al., 2007" startWordPosition="4355" endWordPosition="4358">rgh Twitter corpus (Petrovic et al., 2010) as the background corpus for frequency calculation, and a dictionary containing 82,324 words.8 The nonstandard tokens may consist of both numbers/characters and apostrophe. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and Twitter data sets. The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words. For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system. We adopt this word-level n-best accuracy to make our results compara</context>
<context position="29922" citStr="Choudhury et al., 2007" startWordPosition="4805" endWordPosition="4808">ating that the vast amount of the intentionally created nonstandard tokens can hardly be tackled by a system relies solely on the lexical/phonetic similarity. The “Visual Priming” subSMS Dataset Word Level Accuracy (%) (303 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell Checker 43.89 55.45 56.77 56.77 n/a Visual Priming 54.13 74.92 84.82 87.13 n/a Enhanced Letter Tran. 61.06 74.92 80.86 82.51 n/a Broad-Cov. System 64.36 80.20 89.77 91.75 94.06 (Pennell et al., 2011)* 60.39 74.58 75.57 75.57 n/a (Liu et al., 2011) 62.05 75.91 81.19 81.19 n/a (Cook et al., 2009) 59.4 n/a 83.8 87.8 n/a (Choudhury et al., 2007)* 59.9 n/a 84.3 88.7 n/a Table 4: Word-level results on data set (1). * denotes system requires human annotations for training. Twitter Dataset Word Level Accuracy (%) (3802 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell Checker 47.19 56.92 59.13 59.18 n/a Visual Priming 54.34 70.59 80.83 84.74 n/a Enhanced Letter Tran. 61.05 70.07 74.04 74.75 n/a Broad-Cov. System 69.81 82.51 92.24 93.79 95.71 (Liu et al., 2011) 68.88 78.27 80.93 81.17 n/a Table 5: Word-level results on data set (2). normalizer performs surprisingly well and shows robust performance across all data sets. A minor side</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal on Document Analysis and Recognition, 10(3):157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>Probability scoring for spelling correction.</title>
<date>1991</date>
<journal>Statistics and Computing,</journal>
<pages>1--93</pages>
<contexts>
<context position="7324" citStr="Church and Gale, 1991" startWordPosition="1114" endWordPosition="1117">achieve over 90% wordcoverage with limited number of candidates and the broad word-coverage can be successfully translated into message-level performance gain. In addition, our system requires no human annotations, therefore can be easily adapted to different domains. 2 Related work Text normalization, in its traditional sense, is the first step of a speech synthesis system, where the numbers, dates, acronyms, etc. found in the realworld text were converted into standard dictionary words, so that the system can pronounce them correctly. Spell checking plays an important role in this process. (Church and Gale, 1991; Mays et al., 1991; Brill and Moore, 2000) proposed to use the noisy channel framework to generate a list of corrections for any misspelled word, ranked by the corresponding posterior probabilities. (Sproat et al., 2001) enhanced this framework by calculating the likelihood probability as the chance of a noisy token and its associated tag being generated by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into </context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth W. Church and William A. Gale. 1991. Probability scoring for spelling correction. Statistics and Computing, 1:93–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CMU</author>
</authors>
<title>The cmu pronouncing dictionary.</title>
<date>2007</date>
<note>http://www.speech.cs.cmu.edu/cgi-bin/cmudict.</note>
<contexts>
<context position="26812" citStr="CMU, 2007" startWordPosition="4301" endWordPosition="4302"> 3, we also present the number of distinct nonstandard tokens found in each data set, and notice that only a small portion of the nonstandard tokens correspond to multiple standard words. We calculate the dictionary coverage of the manually annotated words since this sets an upper bound for any normalization system. We use the Edinburgh Twitter corpus (Petrovic et al., 2010) as the background corpus for frequency calculation, and a dictionary containing 82,324 words.8 The nonstandard tokens may consist of both numbers/characters and apostrophe. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and</context>
</contexts>
<marker>CMU, 2007</marker>
<rawString>CMU. 2007. The cmu pronouncing dictionary. http://www.speech.cs.cmu.edu/cgi-bin/cmudict.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text messages normalization.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="8157" citStr="Cook and Stevenson, 2009" startWordPosition="1253" endWordPosition="1256">et al., 2001) enhanced this framework by calculating the likelihood probability as the chance of a noisy token and its associated tag being generated by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. (Choudhury et al., 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisytoken under each of the HMM models; (Cook and Stevenson, 2009) calculated the sum of the probabilities of a noisy token being generated by a specific word and a word formation process; (Beaufort et al., 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine transla1036 tion (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text messages normalization. In Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 71– 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised morpheme segmentation and morphology induction from text corpora using morfessor 1.0.</title>
<date>2005</date>
<booktitle>In Computer and Information Science, Report A81,</booktitle>
<institution>Helsinki University of Technology.</institution>
<contexts>
<context position="22208" citStr="Creutz and Lagus, 2005" startWordPosition="3576" endWordPosition="3579">n feature “L B” for the letter “d” in Table 2). A similar set of features are also developed on morpheme level. We consider conjunction of character/vowel feature and their boundary tags on the syllable/morpheme/word level; conjunction of phoneme and phoneme boundary tags, and absolute position of current character within the corre5Phoneme decomposition is generated using the (Jiampojamarn et al., 2007) algorithm to map up to two letters to phonemes (2-to-2 alignment); syllable boundary acquired by the hyphenation algorithm (Liang, 1983); morpheme boundary determined by toolkit Morfessor 1.0 (Creutz and Lagus, 2005). 6For phoneme boundary, we use “B1” and “L1” to represent two different characters aligned to one phoneme and “B2”, “L2” represent same characters aligned to one phoneme. 7http://en.wikipedia.org/wiki/Priming (psychology) 1039 character sequence. Together len(LCS(ti,si)) len(ti) provides the minor visual stimulus from ti. Note that the first character has been shown to be a crucial visual cue for the brain to understand jumbled words (Davis, ), we therefore consider as candidates only those words si that start with the same character as ti. In the case that the nonstandard token ti starts wit</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005. Unsupervised morpheme segmentation and morphology induction from text corpora using morfessor 1.0. In Computer and Information Science, Report A81, Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Crystal</author>
</authors>
<title>Txtng: The gr8 db8.</title>
<date>2009</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="4367" citStr="Crystal, 2009" startWordPosition="660" endWordPosition="661">(6326) togetha(919) tgthr(250) togeda (20) 2getha(1266) togather(207) t0gether (57) toqethaa (10) 2gthr (178) togehter (94) togeter (49) 2getter (10) u (3240535) ya (460963) yo (252274) yaa (17015) yaaa (7740) yew (7591) yuo (467) youz (426) yoooooou (186) youy (105) yoiu (128) yoooouuuu (82) Table 1: Nonstandard tokens and their frequencies in the Edinburgh Twitter corpus. The corresponding standard words are “together” and “you”, respectively. of a mixture of both unintentional misspellings and intentionally-created tokens for various reasons1, including the needs for speed, ease of typing (Crystal, 2009), sentiment expressing (e.g., “coooool” (Brody and Diakopoulos, 2011)), intimacy and social purpose (Thurlow, 2003), etc., making it even harder to decipher the social messages. Table 1 shows some example nonstandard tokens. Existing spell checkers and normalization systems rely heavily on lexical/phonetic similarity to select the correct candidate words. This may not work well since a good portion of the correct words lie outside the specified similarity threshold (e.g., (tomorrow, “tmrw”)2), yet the number of candidates increases dramatically as the system strives to increase the coverage by</context>
</contexts>
<marker>Crystal, 2009</marker>
<rawString>David Crystal. 2009. Txtng: The gr8 db8. Oxford University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Matt Davis</author>
</authors>
<title>Reading jumbled texts.</title>
<note>http://www.mrccbu.cam.ac.uk/personal/matt.davis/Cmabrigde/.</note>
<marker>Davis, </marker>
<rawString>Matt Davis. Reading jumbled texts. http://www.mrccbu.cam.ac.uk/personal/matt.davis/Cmabrigde/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Stephen Hogan</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>hardtoparse: POS tagging and parsing the twitterverse.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI Workshop on Analyzing Microtext,</booktitle>
<pages>20--25</pages>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Hogan, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011. #hardtoparse: POS tagging and parsing the twitterverse. In Proceedings of the AAAI Workshop on Analyzing Microtext, pages 20–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Donald Metzler</author>
<author>Congxing Cai</author>
<author>Eduard Hovy</author>
</authors>
<title>Contextual bearing on linguistic variation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Language in Social Media,</booktitle>
<pages>20--29</pages>
<contexts>
<context position="9614" citStr="Gouws et al., 2011" startWordPosition="1473" endWordPosition="1476"> expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion factors in calculating the likelihood probabilities. (Gouws et al., 2011) revealed that different populations exhibit different shortening styles. Most of the above systems limit their processing scope to certain categories (e.g., deletion-based abbreviations, misspellings) or require large-scale human annotated corpus for training, which greatly hinders the scalability of the system. In this paper, we propose a novel cognitively-driven text normalization system that robustly tackle both the unintentional misspellings and the intentionally-created noisy tokens. We propose a global context-based approach to purify the automatically collected training data and learn </context>
</contexts>
<marker>Gouws, Metzler, Cai, Hovy, 2011</marker>
<rawString>Stephan Gouws, Donald Metzler, Congxing Cai, and Eduard Hovy. 2011. Contextual bearing on linguistic variation in social media. In Proceedings of the ACL Workshop on Language in Social Media, pages 20–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>368--378</pages>
<contexts>
<context position="5016" citStr="Han and Baldwin, 2011" startWordPosition="754" endWordPosition="757">., “coooool” (Brody and Diakopoulos, 2011)), intimacy and social purpose (Thurlow, 2003), etc., making it even harder to decipher the social messages. Table 1 shows some example nonstandard tokens. Existing spell checkers and normalization systems rely heavily on lexical/phonetic similarity to select the correct candidate words. This may not work well since a good portion of the correct words lie outside the specified similarity threshold (e.g., (tomorrow, “tmrw”)2), yet the number of candidates increases dramatically as the system strives to increase the coverage by enlarging the threshold. (Han and Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%. The low coverage score also enforces an undesirable performance ceiling for the candidate reranking approaches. Different from previous work, we tackle the text normalization problem from a cognitive-sensitive perspective and investigate the human rationales for normalizing the nonstandard tokens. We argue that there exists a set of letter transformation patterns that humans use to decipher the nonstandard tokens. Moreover, the “visual priming” effect may play an important role in human comprehe</context>
<context position="9184" citStr="Han and Baldwin, 2011" startWordPosition="1412" endWordPosition="1415">so tackled under the machine transla1036 tion (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion factors in calculating the likelihood probabilities. (Gouws et al., 2011) revealed that different populations exhibit different shortening styles. Most of the above systems limit their processing scope to certain categories (e.g., deletion-bas</context>
<context position="27371" citStr="Han and Baldwin, 2011" startWordPosition="4394" endWordPosition="4397">he. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and Twitter data sets. The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words. For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system. We adopt this word-level n-best accuracy to make our results comparable to other state-of-the-art systems. On message-level, we evaluate the 1-best system output using precision, recall, and f-score, calculated respective to the nonstandard tokens. 4.2 Word-level Results The wor</context>
<context position="32455" citStr="Han and Baldwin, 2011" startWordPosition="5215" endWordPosition="5218">oad-Cov. System 64.39 78.29 86.56 88.69 91.60 (Pennell et al., 2011)* 37.40 n/a n/a 72.38 n/a Table 6: Word-level results on data set (3). * denotes system requires human annotations for training. 76 74 72 70 68 66 64 1K Figure 2: Learning curve of the enhanced letter transformation system using random training pair selection or the context-aware approach. Evaluated on data set (2). We use the word-level “Broad-Cov. System” for candidate suggestion and the Viterbi algorithm for message-level decoding. The system is evaluated on data set (4) and results shown in Table 7. Following research in (Han and Baldwin, 2011), we focus on the the normalization task and assume perfect nonstandard token detection. The “Word-level w/o Context” results are generated by replacing each nonstandard token using the 1-best word-level candidate. Although the replacement process is static, it results in 70.97% fscore due to the high performance of the word-level system. We explore two language models (LM) for the Viterbi decoding process. First, a bigram LM is trained using the Edinburgh Twitter corpus (53,794,549 English tweets) with the SRILM toolkit (Stolcke, 2002) and Kneser-Ney smoothing; second, we retrieve the bigram </context>
<context position="33799" citStr="Han and Baldwin, 2011" startWordPosition="5431" endWordPosition="5434">orpus. During decoding, we use the “VisualPrim” score as the emission probability, since this score best fits the log scale and applies to all candidates. For the Twitter LM, we apply a scaling factor of 0.5 to the “VisualPrim” score to make it comparable in scale to the LM probabilities. We use the 3- best word-level candidates for Viterbi decoding. In addition, we add the commonly used corrections for Twitter Dataset Message-level P/R/F (549 Tweets) Precision (%) Recall (%) F-score (%) Word-level w/o Context 75.69 66.81 70.97 w/ Context Web LM 79.12 77.11 78.10 Twitter LM 84.13 78.38 81.15 (Han and Baldwin, 2011)* 75.30 75.30 75.30 Table 7: Message-level results on data set (4). * denotes system requires human annotations for training. 16 single-characters, e.g., for “r”, “c”, we add “are”, “see” to the candidate list if they are not already presented. A default “VisualPrim” score (77 = 25) is used for these candidates. As seen from Table 7, both Web LM and Twitter LM achieve better performance than the best prior results, with Twitter LM outperforms the Web LM, yielding a f-score of 81%. This shows that a vanilla Viterbi decoding process is able to outperform the fine-tuned supervised system given co</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proceedings ofACL, pages 368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mindaugas Idzelis</author>
</authors>
<title>Jazzy: The java open source spell checker.</title>
<date>2005</date>
<note>http://jazzy.sourceforge.net/.</note>
<contexts>
<context position="23895" citStr="Idzelis, 2005" startWordPosition="3841" endWordPosition="3842">from the global corpus rather than from the prior context. This approach also inherently follows the noisy channel framework, with p(ti|si) represents the visual stimulus and p(si) being the logarithm of frequency. The candidate words are ranked by s� = arg maxsiV isualPrim(si|ti). We show that the “priming” subnormalizer is robust across data sets abide its simplistic representation. 3.3 Spell Checker The third subnormalizer is the spell checker, which combines the string and phonetic similarity algorithms and is most effective in normalizing the misspellings. We use the Jazzy spell checker (Idzelis, 2005) that integrates the DoubleMetaphone phonetic matching algorithm and the Levenshtein distance using the near-miss strategy, which enables the interchange of two adjacent letters, and the replacing/deleting/adding of letters. 3.4 Candidate Combination Each of the three subnormalizers is a stand-alone system and can suggest corrections for the nonstandard tokens. Yet we show that each subnormalizer mimics a different perspective that humans use to decode the nonstandard tokens, as a result, our broad-coverage normalization system is built by integrating candidates from the three subnormalizers u</context>
</contexts>
<marker>Idzelis, 2005</marker>
<rawString>Mindaugas Idzelis. 2005. Jazzy: The java open source spell checker. http://jazzy.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>372--379</pages>
<contexts>
<context position="21991" citStr="Jiampojamarn et al., 2007" startWordPosition="3544" endWordPosition="3548">junction features to accurately pinpoint the current character position. We consider conjunction features formed by concatenating character position in syllable and current syllable position in the word (e.g., conjunction feature “L B” for the letter “d” in Table 2). A similar set of features are also developed on morpheme level. We consider conjunction of character/vowel feature and their boundary tags on the syllable/morpheme/word level; conjunction of phoneme and phoneme boundary tags, and absolute position of current character within the corre5Phoneme decomposition is generated using the (Jiampojamarn et al., 2007) algorithm to map up to two letters to phonemes (2-to-2 alignment); syllable boundary acquired by the hyphenation algorithm (Liang, 1983); morpheme boundary determined by toolkit Morfessor 1.0 (Creutz and Lagus, 2005). 6For phoneme boundary, we use “B1” and “L1” to represent two different characters aligned to one phoneme and “B2”, “L2” represent same characters aligned to one phoneme. 7http://en.wikipedia.org/wiki/Priming (psychology) 1039 character sequence. Together len(LCS(ti,si)) len(ti) provides the minor visual stimulus from ti. Note that the first character has been shown to be a cruci</context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion. In Proceedings of HLT/NAACL, pages 372– 379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Kobus</author>
<author>Franc¸ois Yvon</author>
<author>G´eraldine Damnati</author>
</authors>
<title>Normalizing sms: Are two metaphors better than one?</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>441--448</pages>
<contexts>
<context position="8779" citStr="Kobus et al., 2008" startWordPosition="1351" endWordPosition="1354">lculated the sum of the probabilities of a noisy token being generated by a specific word and a word formation process; (Beaufort et al., 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine transla1036 tion (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens withou</context>
</contexts>
<marker>Kobus, Yvon, Damnati, 2008</marker>
<rawString>Catherine Kobus, Franc¸ois Yvon, and G´eraldine Damnati. 2008. Normalizing sms: Are two metaphors better than one? In Proceedings of COLING, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<date>2005</date>
<note>CRF++: Yet another CRF took kit. http://crfpp.sourceforge.net/.</note>
<contexts>
<context position="17642" citStr="Kudo, 2005" startWordPosition="2795" endWordPosition="2796">er than a threshold 0 (set empirically at 0.0003), and retain only the top portion (5,000 pairs) for experiments. 3.1.2 Character-level Sequence Labeling For a dictionary word si, we use the conditional random fields (CRF) model to perform characterlevel labeling to generate its variant ti. In the training stage, we align the collected (word, token) pairs at the character level (Liu et al., 2011b), then construct a feature vector for each letter of the dictionary word, using its mapped character as the reference label. This aligned data set is used to train a CRF model (Lafferty et al., 2001; Kudo, 2005) p(�ti|si) �n S Contextim(vi, vj) = =1 wi,k x wj,k wi,k = 1038 Character a d v e r t i s e m e n t s sponding syllable/morpheme/word. Phoneme AE D V ER ER T AY Z M AH N T S We use the aforementioned features to train the CRF model, then apply the model on dictionary words si to generate multiple variations ti for each word. When a nonstandard token is seen during testing, we apply the noisy channel to generate a list of best candidate words: s� = arg maxsip(ti|si)p(si). 3.2 Visual Priming Approach A second key component of the broad-coverage normalization system is a novel “Visual Priming” sub</context>
</contexts>
<marker>Kudo, 2005</marker>
<rawString>Taku Kudo. 2005. CRF++: Yet another CRF took kit. http://crfpp.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="17629" citStr="Lafferty et al., 2001" startWordPosition="2791" endWordPosition="2794">ntextual similarity lower than a threshold 0 (set empirically at 0.0003), and retain only the top portion (5,000 pairs) for experiments. 3.1.2 Character-level Sequence Labeling For a dictionary word si, we use the conditional random fields (CRF) model to perform characterlevel labeling to generate its variant ti. In the training stage, we align the collected (word, token) pairs at the character level (Liu et al., 2011b), then construct a feature vector for each letter of the dictionary word, using its mapped character as the reference label. This aligned data set is used to train a CRF model (Lafferty et al., 2001; Kudo, 2005) p(�ti|si) �n S Contextim(vi, vj) = =1 wi,k x wj,k wi,k = 1038 Character a d v e r t i s e m e n t s sponding syllable/morpheme/word. Phoneme AE D V ER ER T AY Z M AH N T S We use the aforementioned features to train the CRF model, then apply the model on dictionary words si to generate multiple variations ti for each word. When a nonstandard token is seen during testing, we apply the noisy channel to generate a list of best candidate words: s� = arg maxsip(ti|si)p(si). 3.2 Visual Priming Approach A second key component of the broad-coverage normalization system is a novel “Visual</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franklin Mark Liang</author>
</authors>
<title>Word hy-phen-a-tion by com-put-er.</title>
<date>1983</date>
<institution>In PhD Dissertation, Stanford University.</institution>
<contexts>
<context position="22128" citStr="Liang, 1983" startWordPosition="3567" endWordPosition="3568"> syllable and current syllable position in the word (e.g., conjunction feature “L B” for the letter “d” in Table 2). A similar set of features are also developed on morpheme level. We consider conjunction of character/vowel feature and their boundary tags on the syllable/morpheme/word level; conjunction of phoneme and phoneme boundary tags, and absolute position of current character within the corre5Phoneme decomposition is generated using the (Jiampojamarn et al., 2007) algorithm to map up to two letters to phonemes (2-to-2 alignment); syllable boundary acquired by the hyphenation algorithm (Liang, 1983); morpheme boundary determined by toolkit Morfessor 1.0 (Creutz and Lagus, 2005). 6For phoneme boundary, we use “B1” and “L1” to represent two different characters aligned to one phoneme and “B2”, “L2” represent same characters aligned to one phoneme. 7http://en.wikipedia.org/wiki/Priming (psychology) 1039 character sequence. Together len(LCS(ti,si)) len(ti) provides the minor visual stimulus from ti. Note that the first character has been shown to be a crucial visual cue for the brain to understand jumbled words (Davis, ), we therefore consider as candidates only those words si that start wit</context>
</contexts>
<marker>Liang, 1983</marker>
<rawString>Franklin Mark Liang. 1983. Word hy-phen-a-tion by com-put-er. In PhD Dissertation, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
<author>Fuliang Weng</author>
</authors>
<title>Why is ”sxsw” trending? Exploring multiple text sources for twitter topic summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Language in Social Media (LSM),</booktitle>
<pages>66--75</pages>
<contexts>
<context position="2122" citStr="Liu et al., 2011" startWordPosition="314" endWordPosition="317"> Google+. As of June 2011, Twitter has attracted over 300 million users and produces more than 2 billion tweets per week (Twitter, 2011). In a broader sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, which is significantly different from the traditional news text due to the informal writing style and the conversational nature. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al., 2011), retrieval (Subramaniam et al., 2009), summarization (Liu et al., 2011a), sentiment analysis (Celikyilmaz et al., 2010), etc. Yet existing systems often perform poorly in this domain due the to extensive use of the nonstandard tokens, emoticons, incomplete and ungrammatical sentences, etc. It is reported that the Stanford named entity recognizer (NER) experienced a performance drop from 90.8% to 45.8% on tweets (Liu et al., 2011c); the part-of-speech (POS) tagger and dependency parser degraded 12.2% and 20.65% respectively on tweets (Foster et al., 2011). It is therefore of great importance to normalize the social text before applying the standard NLP techniques</context>
<context position="3386" citStr="Liu et al., 2011" startWordPosition="520" endWordPosition="523">g robust text-to-speech (TTS) systems, which need to determine the pronunciations for nonstandard words in the social text. The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. We aim for a robust text normalization system with “broad coverage”, i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3,10...). This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. (Liu et al., 2011b) found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (Petrovic et al., 2010); second, the nonstandard tokens consist 1035 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2gether(6326) togetha(919) tgthr(250) togeda (20) 2getha(1266) togather(207) t0gether (57) toqethaa (10) 2gthr (178) togehter (94) togeter (49) 2getter (10) u (3240535) ya (460963) yo (252274) yaa (17015) yaaa (7740) yew (7591) yuo (467) yo</context>
<context position="9325" citStr="Liu et al., 2011" startWordPosition="1433" endWordPosition="1436">malizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion factors in calculating the likelihood probabilities. (Gouws et al., 2011) revealed that different populations exhibit different shortening styles. Most of the above systems limit their processing scope to certain categories (e.g., deletion-based abbreviations, misspellings) or require large-scale human annotated corpus for training, which greatly hinders the scalability of the syst</context>
<context position="14120" citStr="Liu et al., 2011" startWordPosition="2203" endWordPosition="2206">-level labeling system to the standard dictionary words and generate multiple variations for each word using the n-best labeling output, the labeling confidence is used as p(ti|si). During testing, we search this lookup table to find the best candidate words for the nonstandard tokens. For tokens with letter repetition, we first generate a set of variants by varying the repetitive letters (e.g. Ci = {“pleas”, “pleeas”, “pleaas”, “pleeaas”, ‘pleeeaas”} for ti = {“pleeeaas”}), then select the maximum posterior probability among all the variants: p(ti|si) = max �tiECi Different from the work in (Liu et al., 2011b), we enhanced the letter transformation process with two novel aspects: first, we devise a set of phoneme-, syllable-, morpheme- and word-boundary based features that effectively characterize the formation process of the nonstandard tokens; second, we propose a global context-aware approach to purify the automatically collected training (word, token) pairs, resulting system yielded similar performance but with only one ninth of the original data. We name this subnormalizer “Enhanced Letter Transformation”. 3.1.1 Context-Aware Training Pair Selection Manual annotation of the noisy nonstandard</context>
<context position="17429" citStr="Liu et al., 2011" startWordPosition="2753" endWordPosition="2756">ber of tweets containing tk. To select the most representative (word, token) pairs for training, we rank the automatically collected 46,288 pairs by the token frequency, filter out pairs whose contextual similarity lower than a threshold 0 (set empirically at 0.0003), and retain only the top portion (5,000 pairs) for experiments. 3.1.2 Character-level Sequence Labeling For a dictionary word si, we use the conditional random fields (CRF) model to perform characterlevel labeling to generate its variant ti. In the training stage, we align the collected (word, token) pairs at the character level (Liu et al., 2011b), then construct a feature vector for each letter of the dictionary word, using its mapped character as the reference label. This aligned data set is used to train a CRF model (Lafferty et al., 2001; Kudo, 2005) p(�ti|si) �n S Contextim(vi, vj) = =1 wi,k x wj,k wi,k = 1038 Character a d v e r t i s e m e n t s sponding syllable/morpheme/word. Phoneme AE D V ER ER T AY Z M AH N T S We use the aforementioned features to train the CRF model, then apply the model on dictionary words si to generate multiple variations ti for each word. When a nonstandard token is seen during testing, we apply the</context>
<context position="20420" citStr="Liu et al., 2011" startWordPosition="3303" endWordPosition="3306">pus; log(TF(si)) primes the system with the most common words in the social text; LCS(·) means the longest common character subsequence; len(·) denotes the length of the Phoneme boundary O O O B1 L1 O O O O O O O O O Syllable boundary B L B I L B I I L B I I I L Morpheme boundary B I I I I I I I L B I I L U Word boundary B I I I I I I I I I I I I L Table 2: Example boundary tags for word “advertisements” on the phoneme-, syllable-, morpheme-, and word-level, labeled with the “BILOU” encoding scheme. with L-BFGS optimization. We use the character/phoneme n-gram and binary vowel features as in (Liu et al., 2011b), but develop a set of boundary features to effectively characterize the letter transformation process. We notice that in creating the nonstandard tokens, humans tend to drop certain letter units from the word or replace them with other letters. For example, in abbreviating “advertisements” to “ads”, humans may first break the word into smaller units “ad-ver-tise-ment-s”, then drop the middle parts. This also conforms with the word construction theory where a word is composed of smaller units and construction rules. Based on this assumption, we decompose the dictionary words on the phoneme-,</context>
<context position="25080" citStr="Liu et al., 2011" startWordPosition="4023" endWordPosition="4026">from the three subnormalizers using various strategies. For a noisy token seen in the informal text, the most convenient way of system combination is to harvest up to n candidates from each of the subnormalizers, and use the pool of candidates (up to 3n) as the system output. This sets an upper bound for other candidate combination strategies, and we name this approach “Oracle”. A second combination strategy is to give higher priority to candidates from high-precision subsystems. Both “Letter Transformation” and “Spell Checker” have been shown to have high precision in suggesting corrections (Liu et al., 2011b), while “Visual Priming” may not yield high precision due to its definition. We therefore take the top-3 candidates from each of the “Letter Tran.” and “Spell Checker” subsystems, but put candidates from “Letter Tran.” ahead of “Spell Checker” if the confidence of the best candidate is greater than a threshold A and vice versa. The list of candidates is then compensated using the “Visual Priming” output until the total number reaches n. We name this approach “Word-level” combination since no message-level context information is involved. Based on the “Word-level” combination output, we can f</context>
<context position="27230" citStr="Liu et al., 2011" startWordPosition="4369" endWordPosition="4372">uency calculation, and a dictionary containing 82,324 words.8 The nonstandard tokens may consist of both numbers/characters and apostrophe. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and Twitter data sets. The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words. For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system. We adopt this word-level n-best accuracy to make our results comparable to other state-of-the-art systems. On message-level, we evaluate t</context>
<context position="28577" citStr="Liu et al., 2011" startWordPosition="4593" endWordPosition="4596">ts The word-level results are presented in Table 4, 5, and 6, evaluated on data set (1), (2), (3) respectively. We present the n-best accuracy (n = 1, 3, 10, 20) of the system as well as the “Oracle” results generated by pooling the top-20 candidates from each of the three subnormalizers. The best prior results on the data sets are also included in the tables. We notice that the broad-coverage system outperforms all other systems on the reported data sets. It achieves about 90% word-level accuracy on data set (1) and (2) with the top-10 candidates (an average 10% performance gain compared to (Liu et al., 2011b)). This is of crucial importance to a normalization system, since the high accuracy and limited number of candidates will enable more sophisticated reranking or supervised learning techniques to select the best candidate. We also observe the “Oracle” system has averagely only 5% gap to the dictionary coverage. A detailed analysis shows that the human annotators perform many semantic/grammar corrections as well as inconsistent annotations, e.g., (sleepy, “zzz”), (disliked, “unliked”). These are out of the capabilities of the current text normalization system and partly explains the remaining </context>
<context position="29826" citStr="Liu et al., 2011" startWordPosition="4787" endWordPosition="4790">zer performance, the spell checker yields only 50% to 60% accuracy on all data sets, indicating that the vast amount of the intentionally created nonstandard tokens can hardly be tackled by a system relies solely on the lexical/phonetic similarity. The “Visual Priming” subSMS Dataset Word Level Accuracy (%) (303 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell Checker 43.89 55.45 56.77 56.77 n/a Visual Priming 54.13 74.92 84.82 87.13 n/a Enhanced Letter Tran. 61.06 74.92 80.86 82.51 n/a Broad-Cov. System 64.36 80.20 89.77 91.75 94.06 (Pennell et al., 2011)* 60.39 74.58 75.57 75.57 n/a (Liu et al., 2011) 62.05 75.91 81.19 81.19 n/a (Cook et al., 2009) 59.4 n/a 83.8 87.8 n/a (Choudhury et al., 2007)* 59.9 n/a 84.3 88.7 n/a Table 4: Word-level results on data set (1). * denotes system requires human annotations for training. Twitter Dataset Word Level Accuracy (%) (3802 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell Checker 47.19 56.92 59.13 59.18 n/a Visual Priming 54.34 70.59 80.83 84.74 n/a Enhanced Letter Tran. 61.05 70.07 74.04 74.75 n/a Broad-Cov. System 69.81 82.51 92.24 93.79 95.71 (Liu et al., 2011) 68.88 78.27 80.93 81.17 n/a Table 5: Word-level results on data set (2). norma</context>
</contexts>
<marker>Liu, Liu, Weng, 2011</marker>
<rawString>Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is ”sxsw” trending? Exploring multiple text sources for twitter topic summarization. In Proceedings of the ACL Workshop on Language in Social Media (LSM), pages 66–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Bingqing Wang</author>
<author>Yang Liu</author>
</authors>
<title>Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>71--76</pages>
<contexts>
<context position="2122" citStr="Liu et al., 2011" startWordPosition="314" endWordPosition="317"> Google+. As of June 2011, Twitter has attracted over 300 million users and produces more than 2 billion tweets per week (Twitter, 2011). In a broader sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, which is significantly different from the traditional news text due to the informal writing style and the conversational nature. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al., 2011), retrieval (Subramaniam et al., 2009), summarization (Liu et al., 2011a), sentiment analysis (Celikyilmaz et al., 2010), etc. Yet existing systems often perform poorly in this domain due the to extensive use of the nonstandard tokens, emoticons, incomplete and ungrammatical sentences, etc. It is reported that the Stanford named entity recognizer (NER) experienced a performance drop from 90.8% to 45.8% on tweets (Liu et al., 2011c); the part-of-speech (POS) tagger and dependency parser degraded 12.2% and 20.65% respectively on tweets (Foster et al., 2011). It is therefore of great importance to normalize the social text before applying the standard NLP techniques</context>
<context position="3386" citStr="Liu et al., 2011" startWordPosition="520" endWordPosition="523">g robust text-to-speech (TTS) systems, which need to determine the pronunciations for nonstandard words in the social text. The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. We aim for a robust text normalization system with “broad coverage”, i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3,10...). This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. (Liu et al., 2011b) found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (Petrovic et al., 2010); second, the nonstandard tokens consist 1035 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2gether(6326) togetha(919) tgthr(250) togeda (20) 2getha(1266) togather(207) t0gether (57) toqethaa (10) 2gthr (178) togehter (94) togeter (49) 2getter (10) u (3240535) ya (460963) yo (252274) yaa (17015) yaaa (7740) yew (7591) yuo (467) yo</context>
<context position="9325" citStr="Liu et al., 2011" startWordPosition="1433" endWordPosition="1436">malizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion factors in calculating the likelihood probabilities. (Gouws et al., 2011) revealed that different populations exhibit different shortening styles. Most of the above systems limit their processing scope to certain categories (e.g., deletion-based abbreviations, misspellings) or require large-scale human annotated corpus for training, which greatly hinders the scalability of the syst</context>
<context position="14120" citStr="Liu et al., 2011" startWordPosition="2203" endWordPosition="2206">-level labeling system to the standard dictionary words and generate multiple variations for each word using the n-best labeling output, the labeling confidence is used as p(ti|si). During testing, we search this lookup table to find the best candidate words for the nonstandard tokens. For tokens with letter repetition, we first generate a set of variants by varying the repetitive letters (e.g. Ci = {“pleas”, “pleeas”, “pleaas”, “pleeaas”, ‘pleeeaas”} for ti = {“pleeeaas”}), then select the maximum posterior probability among all the variants: p(ti|si) = max �tiECi Different from the work in (Liu et al., 2011b), we enhanced the letter transformation process with two novel aspects: first, we devise a set of phoneme-, syllable-, morpheme- and word-boundary based features that effectively characterize the formation process of the nonstandard tokens; second, we propose a global context-aware approach to purify the automatically collected training (word, token) pairs, resulting system yielded similar performance but with only one ninth of the original data. We name this subnormalizer “Enhanced Letter Transformation”. 3.1.1 Context-Aware Training Pair Selection Manual annotation of the noisy nonstandard</context>
<context position="17429" citStr="Liu et al., 2011" startWordPosition="2753" endWordPosition="2756">ber of tweets containing tk. To select the most representative (word, token) pairs for training, we rank the automatically collected 46,288 pairs by the token frequency, filter out pairs whose contextual similarity lower than a threshold 0 (set empirically at 0.0003), and retain only the top portion (5,000 pairs) for experiments. 3.1.2 Character-level Sequence Labeling For a dictionary word si, we use the conditional random fields (CRF) model to perform characterlevel labeling to generate its variant ti. In the training stage, we align the collected (word, token) pairs at the character level (Liu et al., 2011b), then construct a feature vector for each letter of the dictionary word, using its mapped character as the reference label. This aligned data set is used to train a CRF model (Lafferty et al., 2001; Kudo, 2005) p(�ti|si) �n S Contextim(vi, vj) = =1 wi,k x wj,k wi,k = 1038 Character a d v e r t i s e m e n t s sponding syllable/morpheme/word. Phoneme AE D V ER ER T AY Z M AH N T S We use the aforementioned features to train the CRF model, then apply the model on dictionary words si to generate multiple variations ti for each word. When a nonstandard token is seen during testing, we apply the</context>
<context position="20420" citStr="Liu et al., 2011" startWordPosition="3303" endWordPosition="3306">pus; log(TF(si)) primes the system with the most common words in the social text; LCS(·) means the longest common character subsequence; len(·) denotes the length of the Phoneme boundary O O O B1 L1 O O O O O O O O O Syllable boundary B L B I L B I I L B I I I L Morpheme boundary B I I I I I I I L B I I L U Word boundary B I I I I I I I I I I I I L Table 2: Example boundary tags for word “advertisements” on the phoneme-, syllable-, morpheme-, and word-level, labeled with the “BILOU” encoding scheme. with L-BFGS optimization. We use the character/phoneme n-gram and binary vowel features as in (Liu et al., 2011b), but develop a set of boundary features to effectively characterize the letter transformation process. We notice that in creating the nonstandard tokens, humans tend to drop certain letter units from the word or replace them with other letters. For example, in abbreviating “advertisements” to “ads”, humans may first break the word into smaller units “ad-ver-tise-ment-s”, then drop the middle parts. This also conforms with the word construction theory where a word is composed of smaller units and construction rules. Based on this assumption, we decompose the dictionary words on the phoneme-,</context>
<context position="25080" citStr="Liu et al., 2011" startWordPosition="4023" endWordPosition="4026">from the three subnormalizers using various strategies. For a noisy token seen in the informal text, the most convenient way of system combination is to harvest up to n candidates from each of the subnormalizers, and use the pool of candidates (up to 3n) as the system output. This sets an upper bound for other candidate combination strategies, and we name this approach “Oracle”. A second combination strategy is to give higher priority to candidates from high-precision subsystems. Both “Letter Transformation” and “Spell Checker” have been shown to have high precision in suggesting corrections (Liu et al., 2011b), while “Visual Priming” may not yield high precision due to its definition. We therefore take the top-3 candidates from each of the “Letter Tran.” and “Spell Checker” subsystems, but put candidates from “Letter Tran.” ahead of “Spell Checker” if the confidence of the best candidate is greater than a threshold A and vice versa. The list of candidates is then compensated using the “Visual Priming” output until the total number reaches n. We name this approach “Word-level” combination since no message-level context information is involved. Based on the “Word-level” combination output, we can f</context>
<context position="27230" citStr="Liu et al., 2011" startWordPosition="4369" endWordPosition="4372">uency calculation, and a dictionary containing 82,324 words.8 The nonstandard tokens may consist of both numbers/characters and apostrophe. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and Twitter data sets. The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words. For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system. We adopt this word-level n-best accuracy to make our results comparable to other state-of-the-art systems. On message-level, we evaluate t</context>
<context position="28577" citStr="Liu et al., 2011" startWordPosition="4593" endWordPosition="4596">ts The word-level results are presented in Table 4, 5, and 6, evaluated on data set (1), (2), (3) respectively. We present the n-best accuracy (n = 1, 3, 10, 20) of the system as well as the “Oracle” results generated by pooling the top-20 candidates from each of the three subnormalizers. The best prior results on the data sets are also included in the tables. We notice that the broad-coverage system outperforms all other systems on the reported data sets. It achieves about 90% word-level accuracy on data set (1) and (2) with the top-10 candidates (an average 10% performance gain compared to (Liu et al., 2011b)). This is of crucial importance to a normalization system, since the high accuracy and limited number of candidates will enable more sophisticated reranking or supervised learning techniques to select the best candidate. We also observe the “Oracle” system has averagely only 5% gap to the dictionary coverage. A detailed analysis shows that the human annotators perform many semantic/grammar corrections as well as inconsistent annotations, e.g., (sleepy, “zzz”), (disliked, “unliked”). These are out of the capabilities of the current text normalization system and partly explains the remaining </context>
<context position="29826" citStr="Liu et al., 2011" startWordPosition="4787" endWordPosition="4790">zer performance, the spell checker yields only 50% to 60% accuracy on all data sets, indicating that the vast amount of the intentionally created nonstandard tokens can hardly be tackled by a system relies solely on the lexical/phonetic similarity. The “Visual Priming” subSMS Dataset Word Level Accuracy (%) (303 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell Checker 43.89 55.45 56.77 56.77 n/a Visual Priming 54.13 74.92 84.82 87.13 n/a Enhanced Letter Tran. 61.06 74.92 80.86 82.51 n/a Broad-Cov. System 64.36 80.20 89.77 91.75 94.06 (Pennell et al., 2011)* 60.39 74.58 75.57 75.57 n/a (Liu et al., 2011) 62.05 75.91 81.19 81.19 n/a (Cook et al., 2009) 59.4 n/a 83.8 87.8 n/a (Choudhury et al., 2007)* 59.9 n/a 84.3 88.7 n/a Table 4: Word-level results on data set (1). * denotes system requires human annotations for training. Twitter Dataset Word Level Accuracy (%) (3802 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell Checker 47.19 56.92 59.13 59.18 n/a Visual Priming 54.34 70.59 80.83 84.74 n/a Enhanced Letter Tran. 61.05 70.07 74.04 74.75 n/a Broad-Cov. System 69.81 82.51 92.24 93.79 95.71 (Liu et al., 2011) 68.88 78.27 80.93 81.17 n/a Table 5: Word-level results on data set (2). norma</context>
</contexts>
<marker>Liu, Weng, Wang, Liu, 2011</marker>
<rawString>Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu. 2011b. Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision. In Proceedings ofACL, pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Shaodian Zhang</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
</authors>
<title>Recognizing named entities in tweets.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>359--367</pages>
<contexts>
<context position="2122" citStr="Liu et al., 2011" startWordPosition="314" endWordPosition="317"> Google+. As of June 2011, Twitter has attracted over 300 million users and produces more than 2 billion tweets per week (Twitter, 2011). In a broader sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, which is significantly different from the traditional news text due to the informal writing style and the conversational nature. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al., 2011), retrieval (Subramaniam et al., 2009), summarization (Liu et al., 2011a), sentiment analysis (Celikyilmaz et al., 2010), etc. Yet existing systems often perform poorly in this domain due the to extensive use of the nonstandard tokens, emoticons, incomplete and ungrammatical sentences, etc. It is reported that the Stanford named entity recognizer (NER) experienced a performance drop from 90.8% to 45.8% on tweets (Liu et al., 2011c); the part-of-speech (POS) tagger and dependency parser degraded 12.2% and 20.65% respectively on tweets (Foster et al., 2011). It is therefore of great importance to normalize the social text before applying the standard NLP techniques</context>
<context position="3386" citStr="Liu et al., 2011" startWordPosition="520" endWordPosition="523">g robust text-to-speech (TTS) systems, which need to determine the pronunciations for nonstandard words in the social text. The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. We aim for a robust text normalization system with “broad coverage”, i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3,10...). This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. (Liu et al., 2011b) found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (Petrovic et al., 2010); second, the nonstandard tokens consist 1035 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2gether(6326) togetha(919) tgthr(250) togeda (20) 2getha(1266) togather(207) t0gether (57) toqethaa (10) 2gthr (178) togehter (94) togeter (49) 2getter (10) u (3240535) ya (460963) yo (252274) yaa (17015) yaaa (7740) yew (7591) yuo (467) yo</context>
<context position="9325" citStr="Liu et al., 2011" startWordPosition="1433" endWordPosition="1436">malizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion factors in calculating the likelihood probabilities. (Gouws et al., 2011) revealed that different populations exhibit different shortening styles. Most of the above systems limit their processing scope to certain categories (e.g., deletion-based abbreviations, misspellings) or require large-scale human annotated corpus for training, which greatly hinders the scalability of the syst</context>
<context position="14120" citStr="Liu et al., 2011" startWordPosition="2203" endWordPosition="2206">-level labeling system to the standard dictionary words and generate multiple variations for each word using the n-best labeling output, the labeling confidence is used as p(ti|si). During testing, we search this lookup table to find the best candidate words for the nonstandard tokens. For tokens with letter repetition, we first generate a set of variants by varying the repetitive letters (e.g. Ci = {“pleas”, “pleeas”, “pleaas”, “pleeaas”, ‘pleeeaas”} for ti = {“pleeeaas”}), then select the maximum posterior probability among all the variants: p(ti|si) = max �tiECi Different from the work in (Liu et al., 2011b), we enhanced the letter transformation process with two novel aspects: first, we devise a set of phoneme-, syllable-, morpheme- and word-boundary based features that effectively characterize the formation process of the nonstandard tokens; second, we propose a global context-aware approach to purify the automatically collected training (word, token) pairs, resulting system yielded similar performance but with only one ninth of the original data. We name this subnormalizer “Enhanced Letter Transformation”. 3.1.1 Context-Aware Training Pair Selection Manual annotation of the noisy nonstandard</context>
<context position="17429" citStr="Liu et al., 2011" startWordPosition="2753" endWordPosition="2756">ber of tweets containing tk. To select the most representative (word, token) pairs for training, we rank the automatically collected 46,288 pairs by the token frequency, filter out pairs whose contextual similarity lower than a threshold 0 (set empirically at 0.0003), and retain only the top portion (5,000 pairs) for experiments. 3.1.2 Character-level Sequence Labeling For a dictionary word si, we use the conditional random fields (CRF) model to perform characterlevel labeling to generate its variant ti. In the training stage, we align the collected (word, token) pairs at the character level (Liu et al., 2011b), then construct a feature vector for each letter of the dictionary word, using its mapped character as the reference label. This aligned data set is used to train a CRF model (Lafferty et al., 2001; Kudo, 2005) p(�ti|si) �n S Contextim(vi, vj) = =1 wi,k x wj,k wi,k = 1038 Character a d v e r t i s e m e n t s sponding syllable/morpheme/word. Phoneme AE D V ER ER T AY Z M AH N T S We use the aforementioned features to train the CRF model, then apply the model on dictionary words si to generate multiple variations ti for each word. When a nonstandard token is seen during testing, we apply the</context>
<context position="20420" citStr="Liu et al., 2011" startWordPosition="3303" endWordPosition="3306">pus; log(TF(si)) primes the system with the most common words in the social text; LCS(·) means the longest common character subsequence; len(·) denotes the length of the Phoneme boundary O O O B1 L1 O O O O O O O O O Syllable boundary B L B I L B I I L B I I I L Morpheme boundary B I I I I I I I L B I I L U Word boundary B I I I I I I I I I I I I L Table 2: Example boundary tags for word “advertisements” on the phoneme-, syllable-, morpheme-, and word-level, labeled with the “BILOU” encoding scheme. with L-BFGS optimization. We use the character/phoneme n-gram and binary vowel features as in (Liu et al., 2011b), but develop a set of boundary features to effectively characterize the letter transformation process. We notice that in creating the nonstandard tokens, humans tend to drop certain letter units from the word or replace them with other letters. For example, in abbreviating “advertisements” to “ads”, humans may first break the word into smaller units “ad-ver-tise-ment-s”, then drop the middle parts. This also conforms with the word construction theory where a word is composed of smaller units and construction rules. Based on this assumption, we decompose the dictionary words on the phoneme-,</context>
<context position="25080" citStr="Liu et al., 2011" startWordPosition="4023" endWordPosition="4026">from the three subnormalizers using various strategies. For a noisy token seen in the informal text, the most convenient way of system combination is to harvest up to n candidates from each of the subnormalizers, and use the pool of candidates (up to 3n) as the system output. This sets an upper bound for other candidate combination strategies, and we name this approach “Oracle”. A second combination strategy is to give higher priority to candidates from high-precision subsystems. Both “Letter Transformation” and “Spell Checker” have been shown to have high precision in suggesting corrections (Liu et al., 2011b), while “Visual Priming” may not yield high precision due to its definition. We therefore take the top-3 candidates from each of the “Letter Tran.” and “Spell Checker” subsystems, but put candidates from “Letter Tran.” ahead of “Spell Checker” if the confidence of the best candidate is greater than a threshold A and vice versa. The list of candidates is then compensated using the “Visual Priming” output until the total number reaches n. We name this approach “Word-level” combination since no message-level context information is involved. Based on the “Word-level” combination output, we can f</context>
<context position="27230" citStr="Liu et al., 2011" startWordPosition="4369" endWordPosition="4372">uency calculation, and a dictionary containing 82,324 words.8 The nonstandard tokens may consist of both numbers/characters and apostrophe. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and Twitter data sets. The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words. For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system. We adopt this word-level n-best accuracy to make our results comparable to other state-of-the-art systems. On message-level, we evaluate t</context>
<context position="28577" citStr="Liu et al., 2011" startWordPosition="4593" endWordPosition="4596">ts The word-level results are presented in Table 4, 5, and 6, evaluated on data set (1), (2), (3) respectively. We present the n-best accuracy (n = 1, 3, 10, 20) of the system as well as the “Oracle” results generated by pooling the top-20 candidates from each of the three subnormalizers. The best prior results on the data sets are also included in the tables. We notice that the broad-coverage system outperforms all other systems on the reported data sets. It achieves about 90% word-level accuracy on data set (1) and (2) with the top-10 candidates (an average 10% performance gain compared to (Liu et al., 2011b)). This is of crucial importance to a normalization system, since the high accuracy and limited number of candidates will enable more sophisticated reranking or supervised learning techniques to select the best candidate. We also observe the “Oracle” system has averagely only 5% gap to the dictionary coverage. A detailed analysis shows that the human annotators perform many semantic/grammar corrections as well as inconsistent annotations, e.g., (sleepy, “zzz”), (disliked, “unliked”). These are out of the capabilities of the current text normalization system and partly explains the remaining </context>
<context position="29826" citStr="Liu et al., 2011" startWordPosition="4787" endWordPosition="4790">zer performance, the spell checker yields only 50% to 60% accuracy on all data sets, indicating that the vast amount of the intentionally created nonstandard tokens can hardly be tackled by a system relies solely on the lexical/phonetic similarity. The “Visual Priming” subSMS Dataset Word Level Accuracy (%) (303 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell Checker 43.89 55.45 56.77 56.77 n/a Visual Priming 54.13 74.92 84.82 87.13 n/a Enhanced Letter Tran. 61.06 74.92 80.86 82.51 n/a Broad-Cov. System 64.36 80.20 89.77 91.75 94.06 (Pennell et al., 2011)* 60.39 74.58 75.57 75.57 n/a (Liu et al., 2011) 62.05 75.91 81.19 81.19 n/a (Cook et al., 2009) 59.4 n/a 83.8 87.8 n/a (Choudhury et al., 2007)* 59.9 n/a 84.3 88.7 n/a Table 4: Word-level results on data set (1). * denotes system requires human annotations for training. Twitter Dataset Word Level Accuracy (%) (3802 pairs) 1-best 3-best 10-best 20-best Oracle Jazzy Spell Checker 47.19 56.92 59.13 59.18 n/a Visual Priming 54.34 70.59 80.83 84.74 n/a Enhanced Letter Tran. 61.05 70.07 74.04 74.75 n/a Broad-Cov. System 69.81 82.51 92.24 93.79 95.71 (Liu et al., 2011) 68.88 78.27 80.93 81.17 n/a Table 5: Word-level results on data set (2). norma</context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011c. Recognizing named entities in tweets. In Proceedings ofACL, pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Mays</author>
<author>Fred J Damerau</author>
<author>Robert L Mercer</author>
</authors>
<title>Context based spelling correction.</title>
<date>1991</date>
<booktitle>Information Processing and Management: An International Journal,</booktitle>
<pages>27--5</pages>
<contexts>
<context position="7343" citStr="Mays et al., 1991" startWordPosition="1118" endWordPosition="1121">verage with limited number of candidates and the broad word-coverage can be successfully translated into message-level performance gain. In addition, our system requires no human annotations, therefore can be easily adapted to different domains. 2 Related work Text normalization, in its traditional sense, is the first step of a speech synthesis system, where the numbers, dates, acronyms, etc. found in the realworld text were converted into standard dictionary words, so that the system can pronounce them correctly. Spell checking plays an important role in this process. (Church and Gale, 1991; Mays et al., 1991; Brill and Moore, 2000) proposed to use the noisy channel framework to generate a list of corrections for any misspelled word, ranked by the corresponding posterior probabilities. (Sproat et al., 2001) enhanced this framework by calculating the likelihood probability as the chance of a noisy token and its associated tag being generated by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary</context>
</contexts>
<marker>Mays, Damerau, Mercer, 1991</marker>
<rawString>Eric Mays, Fred J. Damerau, and Robert L. Mercer. 1991. Context based spelling correction. Information Processing and Management: An International Journal, 27(5):517–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Using wikipedia for automatic word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>196--203</pages>
<contexts>
<context position="15284" citStr="Mihalcea, 2007" startWordPosition="2380" endWordPosition="2381">election Manual annotation of the noisy nonstandard tokens takes a lot of time and effort. (Liu et al., 2011b) proposed to use Google search engine to automatically collect large amount of training pairs. Yet the resulting (work, token) pairs are often noisy, containing pairs such as (events, “ents”), (downtown, “downto”), etc. The ideal training data should consist of the most frequent nonstandard tokens paired with the corresponding corrections, so that the system can learn from the most representative letter transformation patterns. Motivated by research on word sense disambiguation (WSD) (Mihalcea, 2007), we hypothesize the nonstandard token and the standard word share a lot of common terms in their global context. For example, “luv” and “love” share “i”, “you”, “u”, “it”, etc. among their top context words. Based on this finding, we propose to filter out the low-quality training pairs by evaluating the global contextual similarity between the word and token. To the best of our knowledge, we are the first to explore this global contextual similarity for the text normalization task. Given a noisy (word, token) pair, we construct two context vectors vi and vj by collecting the most frequent ter</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Rada Mihalcea. 2007. Using wikipedia for automatic word sense disambiguation. In Proceedings of NAACL, pages 196–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana L Pennell</author>
<author>Yang Liu</author>
</authors>
<title>Normalization of text messages for text-to-speech.</title>
<date>2010</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>4842--4845</pages>
<contexts>
<context position="8426" citStr="Pennell and Liu, 2010" startWordPosition="1296" endWordPosition="1299">attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. (Choudhury et al., 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisytoken under each of the HMM models; (Cook and Stevenson, 2009) calculated the sum of the probabilities of a noisy token being generated by a specific word and a word formation process; (Beaufort et al., 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine transla1036 tion (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations in</context>
</contexts>
<marker>Pennell, Liu, 2010</marker>
<rawString>Deana L. Pennell and Yang Liu. 2010. Normalization of text messages for text-to-speech. In Proceedings of ICASSP, pages 4842–4845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana L Pennell</author>
<author>Yang Liu</author>
</authors>
<title>A characterlevel machine translation approach for normalization of sms abbreviations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>974--982</pages>
<contexts>
<context position="8944" citStr="Pennell and Liu, 2011" startWordPosition="1378" endWordPosition="1381">ted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine transla1036 tion (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion fac</context>
<context position="25783" citStr="Pennell and Liu, 2011" startWordPosition="4135" endWordPosition="4138"> therefore take the top-3 candidates from each of the “Letter Tran.” and “Spell Checker” subsystems, but put candidates from “Letter Tran.” ahead of “Spell Checker” if the confidence of the best candidate is greater than a threshold A and vice versa. The list of candidates is then compensated using the “Visual Priming” output until the total number reaches n. We name this approach “Word-level” combination since no message-level context information is involved. Based on the “Word-level” combination output, we can further rerank all the candidates using a message-level Viterbi decoding process (Pennell and Liu, 2011) where the local context information is used to select the best candidate. This approach is named “Message-level” combination. 4 Experiments 4.1 Experimental Setup We use four SMS and Twitter data sets to evaluate the system effectiveness. Statistics of these data sets are summarized in Table 3. Data set (1) to (3) are used for word-level evaluation; data set (4) for both word- and message-level evaluation. In Table 3, we also present the number of distinct nonstandard tokens found in each data set, and notice that only a small portion of the nonstandard tokens correspond to multiple standard </context>
<context position="27298" citStr="Pennell and Liu, 2011" startWordPosition="4380" endWordPosition="4383">he nonstandard tokens may consist of both numbers/characters and apostrophe. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 – Feb 2010 6150 3802 3.87% 99.34% (Liu et al., 2011) SMS/Twitter Aug 2009 4660 2040 2.41% 96.84% (Pennell and Liu, 2011) Twitter Aug 2010 – Oct 2010 549 558 2.87% 99.10% (Han and Baldwin, 2011) Table 3: Statistics of different SMS and Twitter data sets. The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words. For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system. We adopt this word-level n-best accuracy to make our results comparable to other state-of-the-art systems. On message-level, we evaluate the 1-best system output using precision, recall, and f-score, calcul</context>
</contexts>
<marker>Pennell, Liu, 2011</marker>
<rawString>Deana L. Pennell and Yang Liu. 2011. A characterlevel machine translation approach for normalization of sms abbreviations. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 974–982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>The edinburgh twitter corpus.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Computational Linguistics in a World of Social Media,</booktitle>
<pages>25--26</pages>
<contexts>
<context position="3504" citStr="Petrovic et al., 2010" startWordPosition="538" endWordPosition="541">ocial text. The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. We aim for a robust text normalization system with “broad coverage”, i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3,10...). This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. (Liu et al., 2011b) found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (Petrovic et al., 2010); second, the nonstandard tokens consist 1035 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2gether(6326) togetha(919) tgthr(250) togeda (20) 2getha(1266) togather(207) t0gether (57) toqethaa (10) 2gthr (178) togehter (94) togeter (49) 2getter (10) u (3240535) ya (460963) yo (252274) yaa (17015) yaaa (7740) yew (7591) yuo (467) youz (426) yoooooou (186) youy (105) yoiu (128) yoooouuuu (82) Table 1: Nonstandard tokens and their frequencies in the </context>
<context position="26579" citStr="Petrovic et al., 2010" startWordPosition="4266" endWordPosition="4269">r SMS and Twitter data sets to evaluate the system effectiveness. Statistics of these data sets are summarized in Table 3. Data set (1) to (3) are used for word-level evaluation; data set (4) for both word- and message-level evaluation. In Table 3, we also present the number of distinct nonstandard tokens found in each data set, and notice that only a small portion of the nonstandard tokens correspond to multiple standard words. We calculate the dictionary coverage of the manually annotated words since this sets an upper bound for any normalization system. We use the Edinburgh Twitter corpus (Petrovic et al., 2010) as the background corpus for frequency calculation, and a dictionary containing 82,324 words.8 The nonstandard tokens may consist of both numbers/characters and apostrophe. 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency &lt; 20 in the background corpus. “rt” and all single characters except “a” and “i” are excluded. 1040 Index Domain Time Period #Uniq Nonstan. #Msgs %Dict cov. Reference Tokens %Nonstan. Tkns of cands w/ Multi-cands SMS Around 2007 n/a 303 1.32% 100% (Choudhury et al., 2007) Twitter Nov 2009 –</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2010</marker>
<rawString>Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. The edinburgh twitter corpus. In Proceedings of the NAACL HLT Workshop on Computational Linguistics in a World of Social Media, pages 25–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="21118" citStr="Ratinov and Roth, 2009" startWordPosition="3412" endWordPosition="3415">letter transformation process. We notice that in creating the nonstandard tokens, humans tend to drop certain letter units from the word or replace them with other letters. For example, in abbreviating “advertisements” to “ads”, humans may first break the word into smaller units “ad-ver-tise-ment-s”, then drop the middle parts. This also conforms with the word construction theory where a word is composed of smaller units and construction rules. Based on this assumption, we decompose the dictionary words on the phoneme-, syllable-, morpheme-, and word-level5 and use the “BILOU” tagging scheme (Ratinov and Roth, 2009) to represent the unit boundary, where “BILOU” stands for B(egin), I(nside), L(ast), O(utside), and U(nit-length) of the corresponding unit6. Example “BILOU” boundary tags were shown in Table 2. On top of the boundary tags, we develop a set of conjunction features to accurately pinpoint the current character position. We consider conjunction features formed by concatenating character position in syllable and current syllable position in the word (e.g., conjunction feature “L B” for the letter “d” in Table 2). A similar set of features are also developed on morpheme level. We consider conjuncti</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of CoNLL, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2051" citStr="Ritter et al., 2011" startWordPosition="304" endWordPosition="307">ous development of the social media websites such as Twitter, Facebook, and Google+. As of June 2011, Twitter has attracted over 300 million users and produces more than 2 billion tweets per week (Twitter, 2011). In a broader sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, which is significantly different from the traditional news text due to the informal writing style and the conversational nature. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al., 2011), retrieval (Subramaniam et al., 2009), summarization (Liu et al., 2011a), sentiment analysis (Celikyilmaz et al., 2010), etc. Yet existing systems often perform poorly in this domain due the to extensive use of the nonstandard tokens, emoticons, incomplete and ungrammatical sentences, etc. It is reported that the Stanford named entity recognizer (NER) experienced a performance drop from 90.8% to 45.8% on tweets (Liu et al., 2011c); the part-of-speech (POS) tagger and dependency parser degraded 12.2% and 20.65% respectively on tweets (Foster et al., 2011). It is therefore of great importance t</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Alan W Black</author>
<author>Stanley Chen</author>
<author>Shankar Kumar</author>
<author>Mari Ostendorf</author>
<author>Christopher Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="7545" citStr="Sproat et al., 2001" startWordPosition="1150" endWordPosition="1153">re can be easily adapted to different domains. 2 Related work Text normalization, in its traditional sense, is the first step of a speech synthesis system, where the numbers, dates, acronyms, etc. found in the realworld text were converted into standard dictionary words, so that the system can pronounce them correctly. Spell checking plays an important role in this process. (Church and Gale, 1991; Mays et al., 1991; Brill and Moore, 2000) proposed to use the noisy channel framework to generate a list of corrections for any misspelled word, ranked by the corresponding posterior probabilities. (Sproat et al., 2001) enhanced this framework by calculating the likelihood probability as the chance of a noisy token and its associated tag being generated by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. (Choudhury et al., 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisytoken under each of the HMM models; (Cook and Stev</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Richard Sproat, Alan W. Black, Stanley Chen, Shankar Kumar, Mari Ostendorf, and Christopher Richards. 2001. Normalization of non-standard words. Computer Speech and Language, 15(3):287–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="32997" citStr="Stolcke, 2002" startWordPosition="5302" endWordPosition="5303"> results shown in Table 7. Following research in (Han and Baldwin, 2011), we focus on the the normalization task and assume perfect nonstandard token detection. The “Word-level w/o Context” results are generated by replacing each nonstandard token using the 1-best word-level candidate. Although the replacement process is static, it results in 70.97% fscore due to the high performance of the word-level system. We explore two language models (LM) for the Viterbi decoding process. First, a bigram LM is trained using the Edinburgh Twitter corpus (53,794,549 English tweets) with the SRILM toolkit (Stolcke, 2002) and Kneser-Ney smoothing; second, we retrieve the bigram probabilities from the Microsoft Web N-gram API (Wang et al., 2010) since this represents a more comprehensive web-based corpus. During decoding, we use the “VisualPrim” score as the emission probability, since this score best fits the log scale and applies to all candidates. For the Twitter LM, we apply a scaling factor of 0.5 to the “VisualPrim” score to make it comparable in scale to the LM probabilities. We use the 3- best word-level candidates for Viterbi decoding. In addition, we add the commonly used corrections for Twitter Datas</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of ICSLP, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Venkata Subramaniam</author>
<author>Shourya Roy</author>
<author>Tanveer A Faruquie</author>
<author>Sumit Negi</author>
</authors>
<title>A survey of types of text noise and techniques to handle noisy text.</title>
<date>2009</date>
<booktitle>In Proceedings of AND.</booktitle>
<contexts>
<context position="2089" citStr="Subramaniam et al., 2009" startWordPosition="309" endWordPosition="312">ia websites such as Twitter, Facebook, and Google+. As of June 2011, Twitter has attracted over 300 million users and produces more than 2 billion tweets per week (Twitter, 2011). In a broader sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, which is significantly different from the traditional news text due to the informal writing style and the conversational nature. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al., 2011), retrieval (Subramaniam et al., 2009), summarization (Liu et al., 2011a), sentiment analysis (Celikyilmaz et al., 2010), etc. Yet existing systems often perform poorly in this domain due the to extensive use of the nonstandard tokens, emoticons, incomplete and ungrammatical sentences, etc. It is reported that the Stanford named entity recognizer (NER) experienced a performance drop from 90.8% to 45.8% on tweets (Liu et al., 2011c); the part-of-speech (POS) tagger and dependency parser degraded 12.2% and 20.65% respectively on tweets (Foster et al., 2011). It is therefore of great importance to normalize the social text before app</context>
</contexts>
<marker>Subramaniam, Roy, Faruquie, Negi, 2009</marker>
<rawString>L. Venkata Subramaniam, Shourya Roy, Tanveer A. Faruquie, and Sumit Negi. 2009. A survey of types of text noise and techniques to handle noisy text. In Proceedings of AND.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Crispin Thurlow</author>
</authors>
<title>Generation txt? the sociolinguistics of young people’s text-messaging. Discourse Analysis Online.</title>
<date>2003</date>
<contexts>
<context position="4482" citStr="Thurlow, 2003" startWordPosition="675" endWordPosition="676">ter (94) togeter (49) 2getter (10) u (3240535) ya (460963) yo (252274) yaa (17015) yaaa (7740) yew (7591) yuo (467) youz (426) yoooooou (186) youy (105) yoiu (128) yoooouuuu (82) Table 1: Nonstandard tokens and their frequencies in the Edinburgh Twitter corpus. The corresponding standard words are “together” and “you”, respectively. of a mixture of both unintentional misspellings and intentionally-created tokens for various reasons1, including the needs for speed, ease of typing (Crystal, 2009), sentiment expressing (e.g., “coooool” (Brody and Diakopoulos, 2011)), intimacy and social purpose (Thurlow, 2003), etc., making it even harder to decipher the social messages. Table 1 shows some example nonstandard tokens. Existing spell checkers and normalization systems rely heavily on lexical/phonetic similarity to select the correct candidate words. This may not work well since a good portion of the correct words lie outside the specified similarity threshold (e.g., (tomorrow, “tmrw”)2), yet the number of candidates increases dramatically as the system strives to increase the coverage by enlarging the threshold. (Han and Baldwin, 2011) reported an average of 127 candidates per nonstandard token with </context>
</contexts>
<marker>Thurlow, 2003</marker>
<rawString>Crispin Thurlow. 2003. Generation txt? the sociolinguistics of young people’s text-messaging. Discourse Analysis Online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Endel Tulving</author>
<author>Daniel L Schacter</author>
<author>Heather A Stark</author>
</authors>
<title>Priming effects in word fragment completion are independent of recognition memory.</title>
<date>1982</date>
<journal>Journal of Experimental Psychology: Learning, Memory and Cognition,</journal>
<volume>8</volume>
<issue>4</issue>
<marker>Tulving, Schacter, Stark, 1982</marker>
<rawString>Endel Tulving and Daniel L. Schacter; Heather A. Stark. 1982. Priming effects in word fragment completion are independent of recognition memory. Journal of Experimental Psychology: Learning, Memory and Cognition, 8(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Twitter</author>
</authors>
<date>2011</date>
<note>http://en.wikipedia.org/wiki/Twitter.</note>
<contexts>
<context position="1642" citStr="Twitter, 2011" startWordPosition="241" endWordPosition="242">t our system achieves over 90% word-coverage across all data sets (a 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach. 1 Introduction The amount of user generated content has increased drastically in the past few years, driven by the prosperous development of the social media websites such as Twitter, Facebook, and Google+. As of June 2011, Twitter has attracted over 300 million users and produces more than 2 billion tweets per week (Twitter, 2011). In a broader sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, which is significantly different from the traditional news text due to the informal writing style and the conversational nature. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al., 2011), retrieval (Subramaniam et al., 2009), summarization (Liu et al., 2011a), sentiment analysis (Celikyilmaz et al., 2010), etc. Yet existing systems often perform poorly in this domain due the</context>
</contexts>
<marker>Twitter, 2011</marker>
<rawString>Twitter. 2011. http://en.wikipedia.org/wiki/Twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuansan Wang</author>
<author>Christopher Thrasher</author>
<author>Evelyne Viegas</author>
<author>Xiaolong Li</author>
<author>Bo june Hsu</author>
</authors>
<title>An overview of microsoft web n-gram corpus and applications.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>45--48</pages>
<contexts>
<context position="33122" citStr="Wang et al., 2010" startWordPosition="5320" endWordPosition="5323">me perfect nonstandard token detection. The “Word-level w/o Context” results are generated by replacing each nonstandard token using the 1-best word-level candidate. Although the replacement process is static, it results in 70.97% fscore due to the high performance of the word-level system. We explore two language models (LM) for the Viterbi decoding process. First, a bigram LM is trained using the Edinburgh Twitter corpus (53,794,549 English tweets) with the SRILM toolkit (Stolcke, 2002) and Kneser-Ney smoothing; second, we retrieve the bigram probabilities from the Microsoft Web N-gram API (Wang et al., 2010) since this represents a more comprehensive web-based corpus. During decoding, we use the “VisualPrim” score as the emission probability, since this score best fits the log scale and applies to all candidates. For the Twitter LM, we apply a scaling factor of 0.5 to the “VisualPrim” score to make it comparable in scale to the LM probabilities. We use the 3- best word-level candidates for Viterbi decoding. In addition, we add the commonly used corrections for Twitter Dataset Message-level P/R/F (549 Tweets) Precision (%) Recall (%) F-score (%) Word-level w/o Context 75.69 66.81 70.97 w/ Context </context>
</contexts>
<marker>Wang, Thrasher, Viegas, Li, Hsu, 2010</marker>
<rawString>Kuansan Wang, Christopher Thrasher, Evelyne Viegas, Xiaolong Li, and Bo june (Paul) Hsu. 2010. An overview of microsoft web n-gram corpus and applications. In Proceedings of NAACL-HLT, pages 45–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenzhen Xue</author>
<author>Dawei Yin</author>
<author>Brian D Davison</author>
</authors>
<title>Normalizing microtext.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI Workshop on Analyzing Microtext,</booktitle>
<pages>74--79</pages>
<contexts>
<context position="9429" citStr="Xue et al., 2011" startWordPosition="1448" endWordPosition="1451">MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion factors in calculating the likelihood probabilities. (Gouws et al., 2011) revealed that different populations exhibit different shortening styles. Most of the above systems limit their processing scope to certain categories (e.g., deletion-based abbreviations, misspellings) or require large-scale human annotated corpus for training, which greatly hinders the scalability of the system. In this paper, we propose a novel cognitively-driven text normalization system that robustly tackle </context>
</contexts>
<marker>Xue, Yin, Davison, 2011</marker>
<rawString>Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011. Normalizing microtext. In Proceedings of the AAAI Workshop on Analyzing Microtext, pages 74–79.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>