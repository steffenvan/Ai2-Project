<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000266">
<title confidence="0.965398">
A Simple, Similarity-based Model for Selectional Preferences
</title>
<author confidence="0.997233">
Katrin Erk
</author>
<affiliation confidence="0.998732">
University of Texas at Austin
</affiliation>
<email confidence="0.996592">
katrin.erk@mail.utexas.edu
</email>
<sectionHeader confidence="0.980024" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898888888889">
We propose a new, simple model for the auto-
matic induction of selectional preferences, using
corpus-based semantic similarity metrics. Fo-
cusing on the task of semantic role labeling,
we compute selectional preferences for seman-
tic roles. In evaluations the similarity-based
model shows lower error rates than both Resnik&apos;s
WordNet-based model and the EM-based clus-
tering model, but has coverage problems.
</bodyText>
<sectionHeader confidence="0.995514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995038033898305">
Selectional preferences, which characterize typ-
ical arguments of predicates, are a very use-
ful and versatile knowledge source. They have
been used for example for syntactic disambigua-
tion (Hindle and Rooth, 1993), word sense dis-
ambiguation (WSD) (McCarthy and Carroll,
2003) and semantic role labeling (SRL) (Gildea
and Jurafsky, 2002).
The corpus-based induction of selectional
preferences was first proposed by Resnik (1996).
All later approaches have followed the same two-
step procedure, first collecting argument head-
words from a corpus, then generalizing to other,
similar words. Some approaches have used
WordNet for the generalization step (Resnik,
1996; Clark and Weir, 2001; Abe and Li, 1993),
others EM-based clustering (Rooth et al., 1999).
In this paper we propose a new, simple model
for selectional preference induction that uses
corpus-based semantic similarity metrics, such
as Cosine or Lin&apos;s (1998) mutual information-
based metric, for the generalization step. This
model does not require any manually created
lexical resources. In addition, the corpus for
computing the similarity metrics can be freely
chosen, allowing greater variation in the domain
of generalization than a fixed lexical resource.
We focus on one application of selectional
preferences: semantic role labeling. The ar-
gument positions for which we compute selec-
tional preferences will be semantic roles in the
FrameNet (Baker et al., 1998) paradigm, and
the predicates we consider will be semantic
classes of words rather than individual words
(which means that different preferences will be
learned for different senses of a predicate word).
In SRL, the two most pressing issues today are
(1) the development of strong semantic features
to complement the current mostly syntactically-
based systems, and (2) the problem of the do-
main dependence (Carreras and Marquez, 2005).
In the CoNLL-05 shared task, participating sys-
tems showed about 10 points F-score difference
between in-domain and out-of-domain test data.
Concerning (1), we focus on selectional prefer-
ences as the strongest candidate for informative
semantic features. Concerning (2), the corpus-
based similarity metrics that we use for selec-
tional preference induction open up interesting
possibilities of mixing domains.
We evaluate the similarity-based model
against Resnik&apos;s WordNet-based model as well
as the EM-based clustering approach. In the
evaluation, the similarity-model shows lower er-
ror rates than both Resnik&apos;s WordNet-based
model and the EM-based clustering model.
However, the EM-based clustering model has
higher coverage than both other paradigms.
Plan of the paper. After discussing previ-
</bodyText>
<page confidence="0.975146">
216
</page>
<note confidence="0.892271">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 216–223,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<page confidence="0.827053">
P(c)P(rp|c)P(w|c
</page>
<bodyText confidence="0.9995198">
ous approaches to selectional preference induc-
tion in Section 2, we introduce the similarity-
based model in Section 3. Section 4 describes
the data used for the experiments reported in
Section 5, and Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.997061" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999436785714286">
Selectional restrictions and selectional prefer-
ences that predicates impose on their arguments
have long been used in semantic theories, (see
e.g. (Katz and Fodor, 1963; Wilks, 1975)). The
induction of selectional preferences from corpus
data was pioneered by Resnik (1996). All sub-
sequent approaches have followed the same two-
step procedure, first collecting argument head-
words from a corpus, then generalizing over the
seen headwords to similar words. Resnik uses
the WordNet noun hierarchy for generalization.
His information-theoretic approach models the
selectional preference strength of an argument
positions rp of a predicate p as
</bodyText>
<equation confidence="0.9996905">
P(c|rp)
P(c|rp) log P (c)
</equation>
<bodyText confidence="0.999385">
where the c are WordNet synsets. The prefer-
ence that rp has for a given synset c0, the selec-
tional association between the two, is then de-
fined as the contribution of c0 to rp&apos;s selectional
preference strength:
</bodyText>
<equation confidence="0.996833666666667">
P(c0|rp) log P(c�|rp)
P (c�)
S(rp)
</equation>
<bodyText confidence="0.989742727272727">
Further WordNet-based approaches to selec-
tional preference induction include Clark and
Weir (2001), and Abe and Li (1993). Brock-
mann and Lapata (2003) perform a comparison
of WordNet-based models.
Rooth et al. (1999) generalize over seen head-
words using EM-based clustering rather than
WordNet. They model the probability of a word
w occurring as the argument rp of a predicate p
as being independently conditioned on a set of
classes C:
</bodyText>
<equation confidence="0.986997666666667">
E E
P(rp, w) = P(c, rp, w) =
cEC cEC
</equation>
<bodyText confidence="0.98497">
&apos;We write rp to indicate predicate-specific roles, like
“the direct object of catch”, rather than just “obj&amp;quot;.
The parameters P(c), P(rp|c) and P(w|c) are
estimated using the EM algorithm.
While there have been no isolated compar-
isons of the two generalization paradigms that
we are aware of, Gildea and Jurafsky&apos;s (2002)
task-based evaluation has found clustering-
based approaches to have better coverage than
WordNet generalization, that is, for a given role
there are more words for which they can state a
preference.
</bodyText>
<sectionHeader confidence="0.993119" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999940076923077">
The approach we are proposing makes use of
two corpora, a primary corpus and a gener-
alization corpus (which may, but need not, be
identical). The primary corpus is used to extract
tuples (p, rp, w) of a predicate, an argument
position and a seen headword. The general-
ization corpus is used to compute a corpus-based
semantic similarity metric.
Let Seen(rp) be the set of seen headwords for
an argument rp of a predicate p. Then we model
the selectional preference S of rp for a possible
headword w0 as a weighted sum of the similari-
ties between w0 and the seen headwords:
</bodyText>
<equation confidence="0.9981085">
ESrp(w0) = sim(w0, w) · wtrp(w)
wESeen(rp)
</equation>
<bodyText confidence="0.979729947368421">
sim(w0, w) is the similarity between the seen
and the potential headword, and wtrp(w) is the
weight of seen headword w.
Similarity sim(w0, w) will be computed on
the generalization corpus, again on the ba-
sis of extracted tuples (p, rp, w). We will
be using the similarity metrics shown in Ta-
ble 1: Cosine, the Dice and Jaccard coefficients,
and Hindle&apos;s (1990) and Lin&apos;s (1998) mutual
information-based metrics. We write f for fre-
quency, I for mutual information, and R(w) for
the set of arguments rp for which w occurs as a
headword.
In this paper we only study corpus-based met-
rics. The sim function can equally well be in-
)stantiated with a WordNet-based metric (for
an overview see Budanitsky and Hirst (2006)),
but we restrict our experiments to corpus-based
metrics (a) in the interest of greatest possible
</bodyText>
<equation confidence="0.954313466666667">
ES(rp) =
c
A(rp, c0) =
217
simcosine(w, w0) = Prp f(w,rp)·f(w0,rp) simDice(w, w0) = 2·|R(w)∩R(w0)|
qP qP
rp f(w,rp)2· rp f(w0,rp)2 |R(w)|+|R(w0)|
P
rp∈R(w)∩R(w0) I(w,r,p)I(w0,r,p)
simLin(w, w0) = P |R(w)∪R(w0)|
rp∈R(w) I(w,r,p) Prp∈R(w) I(w0,r,p) simJaccard(w, w0) = |R(w)∩R(w0)|
simHindle(w, w0) = Erp simHindle(w, w0, rp) where
simHindle(w, w0, rp) = { min(I(w,rp),I(w0,rp) if I(w, rp) &gt; 0 and I(w0, rp) &gt; 0
abs(max(I(w,rp),I(w0,rp))) if I(w, rp) &lt; 0 and I(w0, rp) &lt; 0
0 else
</equation>
<tableCaption confidence="0.976366">
Table 1: Similarity measures used
</tableCaption>
<bodyText confidence="0.9628603">
resource-independence and (b) in order to be
able to shape the similarity metric by the choice
of generalization corpus.
For the headword weights wtrp(w), the sim-
plest possibility is to assume a uniform weight
distribution, i.e. wtrp(w) = 1. In addition, we
test a frequency-based weight, i.e. wtrp(w) =
f(w, rp), and inverse document frequency, which
weighs a word according to its discriminativity:
num. words
</bodyText>
<equation confidence="0.836151">
wtrp (w) = log m. words to whose context w belongs.
</equation>
<bodyText confidence="0.998699947368421">
This similarity-based model of selectional
preferences is a straightforward implementa-
tion of the idea of generalization from seen
headwords to other, similar words. Like the
clustering-based model, it is not tied to the
availability of WordNet or any other manually
created resource. The model uses two corpora,
a primary corpus for the extraction of seen head-
words and a generalization corpus for the com-
putation of semantic similarity metrics. This
gives the model flexibility to influence the simi-
larity metric through the choice of text domain
of the generalization corpus.
Instantiation used in this paper. Our aim
is to compute selectional preferences for seman-
tic roles. So we choose a particular instantia-
tion of the similarity-based model that makes
use of the fact that the two-corpora approach
allows us to use different notions of “predicate”
and “argument” in the primary and general-
ization corpus. Our primary corpus will con-
sist of manually semantically annotated data,
and we will use semantic verb classes as pred-
icates and semantic roles as arguments. Ex-
amples of extracted (p, rp, w) tuples are (Moral-
ity evaluation, Evaluee, gamblers) and (Placing,
Goal, briefcase). Semantic similarity, on the
other hand, will be computed on automatically
syntactically parsed corpus, where the predi-
cates are words and the arguments are syntac-
tic dependents. Examples of extracted (p, rp, w)
tuples from the generalization corpus include
(catch, obj, frogs) and (intervene, in, deal).2
This instantiation of the similarity-based
model allows us to compute word sense specific
selectional preferences, generalizing over manu-
ally semantically annotated data using automat-
ically syntactically annotated data.
</bodyText>
<sectionHeader confidence="0.994239" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999951578947368">
We use FrameNet (Baker et al., 1998), a se-
mantic lexicon for English that groups words
in semantic classes called frames and lists se-
mantic roles for each frame. The FrameNet
1.3 annotated data comprises 139,439 sentences
from the British National Corpus (BNC). For
our experiments, we chose 100 frame-specific se-
mantic roles at random, 20 each from five fre-
quency bands: 50-100 annotated occurrences
of the role, 100-200 occurrences, 200-500, 500-
1000, and more than 1000 occurrences. The
annotated data for these 100 roles comprised
59,608 sentences, our primary corpus. To deter-
mine headwords of the semantic roles, the cor-
pus was parsed using the Collins (1997) parser.
Our generalization corpus is the BNC. It was
parsed using Minipar (Lin, 1993), which is con-
siderably faster than the Collins parser but
failed to parse about a third of all sentences.
</bodyText>
<footnote confidence="0.8359475">
2For details about the syntactic and semantic analyses
used, see Section 4.
</footnote>
<page confidence="0.994781">
218
</page>
<bodyText confidence="0.999973">
Accordingly, the arguments r extracted from
the generalization corpus are Minipar depen-
dencies, except that paths through preposition
nodes were collapsed, using the preposition as
the dependency relation. We obtained parses for
5,941,811 sentences of the generalization corpus.
The EM-based clustering model was com-
puted with all of the FrameNet 1.3 data (139,439
sentences) as input. Resnik&apos;s model was trained
on the primary corpus (59,608 sentences).
</bodyText>
<sectionHeader confidence="0.996884" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999983571428571">
In this section we describe experiments com-
paring the similarity-based model for selectional
preferences to Resnik&apos;s WordNet-based model
and to an EM-based clustering model3. For the
similarity-based model we test the five similar-
ity metrics and three weighting schemes listed
in section 3.
</bodyText>
<subsectionHeader confidence="0.51099">
Experimental design
</subsectionHeader>
<bodyText confidence="0.999774565217392">
Like Rooth et al. (1999) we evaluate selectional
preference induction approaches in a pseudo-
disambiguation task. In a test set of pairs
(rp, w), each headword w is paired with a con-
founder w&apos; chosen randomly from the BNC ac-
cording to its frequency4. Noun headwords are
paired with noun confounders in order not to
disadvantage Resnik&apos;s model, which only works
with nouns. The headword/confounder pairs
are only computed once and reused in all cross-
validation runs. The task is to choose the more
likely role headword from the pair (w, w&apos;).
In the main part of the experiment, we count
a pair as covered if both w and w&apos; are assigned
some level of preference by a model (“full cover-
age&amp;quot;). We contrast this with another condition,
where we count a pair as covered if at least one
of the two words w, w&apos; is assigned a level of pref-
erence by a model (“half coverage”). If only one
is assigned a preference, that word is counted as
chosen.
To test the performance difference between
models for significance, we use Dietterich&apos;s
</bodyText>
<footnote confidence="0.9414495">
3 W are grateful to Carsten Brockmann and Detlef
Prescher for the use of their software.
4 W exclude potential confounders that occur less
than 30 or more than 3,000 times.
</footnote>
<table confidence="0.9999">
Error Rate Coverage
Cosine 0.2667 0.3284
Dice 0.1951 0.3506
Hindle 0.2059 0.3530
Jaccard 0.1858 0.3506
Lin 0.1635 0.2214
EM 30/20 0.3115 0.5460
EM 40/20 0.3470 0.9846
Resnik 0.3953 0.3084
</table>
<tableCaption confidence="0.812227375">
Table 2: Error rate and coverage (micro-
average), similarity-based models with uniform
weights.
5x2cv (Dietterich, 1998). The test involves
five 2-fold cross-validation runs. Let di,j (i E
11, 2}, j E 11,. .. , 5}) be the difference in error
rates between the two models when using split
i of cross-validation run j as training data. Let
</tableCaption>
<equation confidence="0.938091571428571">
s2j = (d1,j − �dj)2 +(d2,j − �dj)2 be the variance for
cross-validation run j, with �dj = d1°&apos;�d2°&apos;
2 . Then
the 5x2cv t statistic is defined as
d1,1
/1 5 2
5 �j=1 sj
</equation>
<bodyText confidence="0.997252666666667">
Under the null hypothesis, the t statistic has
approximately a t distribution with 5 degrees of
freedom.5
</bodyText>
<sectionHeader confidence="0.967682" genericHeader="evaluation">
Results and discussion
</sectionHeader>
<bodyText confidence="0.999885">
Error rates. Table 2 shows error rates and
coverage for the different selectional prefer-
ence induction methods. The first five mod-
els are similarity-based, computed with uniform
weights. The name in the first column is the
name of the similarity metric used. Next come
EM-based clustering models, using 30 (40) clus-
ters and 20 re-estimation steps6, and the last
row lists the results for Resnik&apos;s WordNet-based
method. Results are micro-averaged.
The table shows very low error rates for the
similarity-based models, up to 15 points lower
than the EM-based models. The error rates
</bodyText>
<footnote confidence="0.997521714285714">
5Since the 5x2cv test fails when the error rates vary
wildly, we excluded cases where error rates differ by 0.8
or more across the 10 runs, using the threshold recom-
mended by Dietterich.
6The EM-based clustering software determines good
values for these two parameters through pseudo-
disambiguation tests on the training data.
</footnote>
<equation confidence="0.514114">
t=
</equation>
<page confidence="0.921199">
219
</page>
<table confidence="0.99944125">
Cos Dic Hin Jac Lin EM 40/20 Resnik
Cos -16 (73) -12 (73) -18 (74) -22 (57) 11 (67) 11 (74)
Dic 16 (73) 2 (74) -8 (85) -10 (64) 39 (47) 27 (62)
Hin 12 (73) -2 (74) -8 (75) -11 (63) 33 (57) 16 (67)
Jac 18 8 (85) 8 -7 (68) 42 (45) 30 (62)
Lin 22 (57) 10 (64) 11 (63) 7 ( 68) 29 (41) 28 (51)
EM 40/20 -11 ( 67 ) -39 ( 47 ) -33 ( 57 ) -42 ( 45 ) -29 ( 41 ) 3 ( 72 )
Resnik -11 (74) -27 (62) -16 (67) -30 (62) -28 (51) -3 (72)
</table>
<tableCaption confidence="0.837901333333333">
Table 3: Comparing similarity measures: number of wins minus losses (in brackets non-significant
cases) using Dietterich&apos;s 5x2cv; uniform weights; condition (1): both members of a pair must be
covered
</tableCaption>
<figure confidence="0.96358775">
Learning curve: num. headwords, sim_based-Jaccard-Plain, error_rate, all
0 100 200 300 400 500
numhw
Mon Apr 09 02:30:47 2007
</figure>
<figureCaption confidence="0.999801">
Figure 1: Learning curve: seen headwords ver-
</figureCaption>
<bodyText confidence="0.7154765">
sus error rate by frequency band, Jaccard, uni-
form weights
</bodyText>
<table confidence="0.968985666666667">
50-100 100-200 200-500 500-1000 1000-
Cos 0.3167 0.3203 0.2700 0.2534 0.2606
Jac 0.1802 0.2040 0.1761 0.1706 0.1927
</table>
<tableCaption confidence="0.944447333333333">
Table 4: Error rates for similarity-based mod-
els, by semantic role frequency band. Micro-
averages, uniform weights
</tableCaption>
<bodyText confidence="0.998721625">
of Resnik&apos;s model are considerably higher than
both the EM-based and the similarity-based
models, which is unexpected. While EM-based
models have been shown to work better in SRL
tasks (Gildea and Jurafsky, 2002), this has been
attributed to the difference in coverage.
In addition to the full coverage condition, we
also computed error rate and coverage for the
half coverage case. In this condition, the error
rates of the EM-based models are unchanged,
while the error rates for all similarity-based
models as well as Resnik&apos;s model rise to values
between 0.4 and 0.6. So the EM-based model
tends to have preferences only for the “right”
words. Why this is so is not clear. It may be a
genuine property, or an artifact of the FrameNet
data, which only contains chosen, illustrative
sentences for each frame. It is possible that
these sentences have fewer occurrences of highly
frequent but semantically less informative role
headwords like “it&amp;quot; or “that” exactly because of
their illustrative purpose.
Table 3 inspects differences between error
rates using Dietterich&apos;s 5x2cv, basically confirm-
ing Table 2. Each cell shows the wins minus
losses for the method listed in the row when
compared against the method in the column.
The number of cases that did not reach signifi-
cance is given in brackets.
Coverage. The coverage rates of the
similarity-based models, while comparable
to Resnik&apos;s model, are considerably lower than
for EM-based clustering, which achieves good
coverage with 30 and almost perfect coverage
with 40 clusters (Table 2). While peculiarities
of the FrameNet data may have influenced the
results in the EM-based model’s favor (see the
discussion of the half coverage condition above),
the low coverage of the similarity-based models
is still surprising. After all, the generalization
corpus of the similarity-based models is far
larger than the corpus used for clustering.
Given the learning curve in Figure 1 it is
unlikely that the reason for the lower cover-
age is data sparseness. However, EM-based
clustering is a soft clustering method, which
relates every predicate and every headword to
every cluster, if only with a very low probabil-
</bodyText>
<figure confidence="0.996305733333333">
error_rare
0.35
0.25
0.15
0.05
0.4
0.3
0.2
0.1
0
1000-
100-200
500-1000
200-500
50-100
</figure>
<page confidence="0.992921">
220
</page>
<bodyText confidence="0.998754106382979">
ity. In similarity-based models, on the other
hand, two words that have never been seen in
the same argument slot in the generalization
corpus will have zero similarity. That is, a
similarity-based model can assign a level of
preference for an argument rp and word wo only
if R(wo) n R(Seen(rp)) is nonempty. Since the
flexibility of similarity-based models extends to
the vector space for computing similarities, one
obvious remedy to the coverage problem would
be the use of a less sparse vector space. Given
the low error rates of similarity-based models,
it may even be advisable to use two vector
spaces, backing off to the denser one for words
not covered by the sparse but highly accurate
space used in this paper.
Parameters of similarity-based models.
Besides the similarity metric itself, which we dis-
cuss below, parameters of the similarity-based
models include the number of seen headwords,
the weighting scheme, and the number of similar
words for each headword.
Table 4 breaks down error rates by semantic
role frequency band for two of the similarity-
based models, micro-averaging over roles of the
same frequency band and over cross-validation
runs. As the table shows, there was some vari-
ation across frequency bands, but not as much
as between models.
The question of the number of seen headwords
necessary to compute selectional preferences is
further explored in Figure 1. The figure charts
the number of seen headwords against error rate
for a Jaccard similarity-based model (uniform
weights). As can be seen, error rates reach a
plateau at about 25 seen headwords for Jaccard.
For other similarity metrics the result is similar.
The weighting schemes wtrp had surprisingly
little influence on results. For Jaccard similar-
ity, the model had an error rate of 0.1858 for
uniform weights, 0.1874 for frequency weight-
ing, and 0.1806 for discriminativity. For other
similarity metrics the results were similar.
A cutoff was used in the similarity-based
model: For each seen headword, only the 500
most similar words (according to a given sim-
ilarity measure) were included in the computa-
</bodyText>
<table confidence="0.9665982">
Cos Dic Hin Jac Lin
(a) Freq. sim. 1889 3167 2959 3167 860
(b) Freq. wins 65% 73% 79% 72% 58%
(c) Num. sim. 81 60 67 60 66
(d) Intersec. 7.3 2.3 7.2 2.1 0.5
</table>
<tableCaption confidence="0.995197">
Table 5: Comparing sim. metrics: (a) avg. freq.
</tableCaption>
<bodyText confidence="0.961367975">
of similar words; (b) % of times the more fre-
quent word won; (c) number of distinct similar
words per seen headword; (d) avg. size of inter-
section between roles
tion; for all others, a similarity of 0 was assumed.
Experiments testing a range of values for this
parameter show that error rates stay stable for
parameter values &gt; 200.
So similarity-based models seem not overly
sensitive to the weighting scheme used, the num-
ber of seen headwords, or the number of similar
words per seen headword. The difference be-
tween similarity metrics, however, is striking.
Differences between similarity metrics.
As Table 2 shows, Lin and Jaccard worked best
(though Lin has very low coverage), Dice and
Hindle not as good, and Cosine showed the worst
performance. To determine possible reasons for
the difference, Table 5 explores properties of the
five similarity measures.
Given a set S = Seen(rp) of seen headwords
for some role rp, each similarity metric produces
a set like(S) of words that have nonzero simi-
larity to S, that is, to at least one word in S.
Line (a) shows the average frequency of words
in like(S). The results confirm that the Lin
and Cosine metrics tend to propose less frequent
words as similar.
Line (b) pursues the question of the frequency
bias further, showing the percentage of head-
word/confounder pairs for which the more fre-
quent of the two words “won&amp;quot; in the pseudo-
disambiguation task (using uniform weights).
This it is an indirect estimate of the frequency
bias of a similarity metric. Note that the head-
word actually was more frequent than the con-
founder in only 36% of all pairs.
These first two tests do not yield any expla-
nation for the low performance of Cosine, as the
results they show do not separate Cosine from
</bodyText>
<page confidence="0.99625">
221
</page>
<table confidence="0.989207705882353">
Jaccard Cosine
Ride vehicle:Vehicle truck 0.05 boat 0.05
coach 0.04 van 0.04 ship 0.04 lorry 0.04 crea-
ture 0.04 flight 0.04 guy 0.04 carriage 0.04 he-
licopter 0.04 lad 0.04
Ingest substance:Substance loaf 0.04 ice
cream 0.03 you 0.03 some 0.03 that 0.03 er
0.03 photo 0.03 kind 0.03 he 0.03 type 0.03
thing 0.03 milk 0.03
Ride vehicle:Vehicle it 1.18 there 0.88 they
0.43 that 0.34 i 0.23 ship 0.19 second one 0.19
machine 0.19 e 0.19 other one 0.19 response
0.19 second 0.19
Ingest substance:Substance there 1.23
that 0.50 object 0.27 argument 0.27 theme
0.27 version 0.27 machine 0.26 result 0.26
response 0.25 item 0.25 concept 0.25 s 0.24
</table>
<tableCaption confidence="0.955684">
Table 6: Highest-ranked induced headwords (seen headwords omitted) for two semantic classes of
the verb “take”: similarity-based models, Jaccard and Cosine, uniform weights.
</tableCaption>
<bodyText confidence="0.996203363636364">
all other metrics. Lines (c) and (d), however, do
just that. Line (c) looks at the size of like(S).
Since we are using a cutoff of 500 similar words
computed per word in S, the size of like(S) can
only vary if the same word is suggested as similar
for several seen headwords in S. This way, the
size of like(S) functions as an indicator of the
degree of uniformity or similarity that a sim-
ilarity metric “perceives” among the members
of S. To facilitate comparison across frequency
bands, line (c) normalizes by the size of S, show-
</bodyText>
<equation confidence="0.5290515">
ing |li���S)|
|S |micro-averaged over all roles. Here
</equation>
<bodyText confidence="0.999890555555556">
we see that Cosine seems to “perceive” consid-
erably less similarity among the seen headwords
than any of the other metrics.
Line (d) looks at the sets s25(r) of the 25 most
preferred potential headwords of roles r, show-
ing the average size of the intersection s25(r) fl
s25(r&apos;) between two roles (preferences computed
with uniform weights). It indicates another pos-
sible reason for Cosine’s problem: Cosine seems
to keep proposing the same words as similar for
different roles. We will see this tendency also in
the sample results we discuss next.
Sample results. Table 6 shows samples of
headwords induced by the similarity-based
model for two FrameNet senses of the verb
“take”: Ride vehicle (“take the bus&amp;quot;) and In-
gest substance (“take drugs”), a semantic class
that is exclusively about ingesting controlled
substances. The semantic role Vehicle of the
Ride vehicle frame and the role Substance of In-
gest substance are both typically realized as the
direct object of “take”. The table only shows
new induced headwords; seen headwords were
omitted from the list.
The particular implementation of the
similarity-based model we have chosen, using
frames and roles as predicates and arguments
in the primary corpus, should enable the model
to compute preferences specific to word senses.
The sample in Table 6 shows that this is indeed
the case: The preferences differ considerably
for the two senses (frames) of “take”, at least
for the Jaccard metric, which shows a clear
preference for vehicles for the Vehicle role. The
Substance role of Ingest substance is harder to
characterize, with very diverse seen headwords
such as “crack”, “lines”, “fluid&amp;quot;, “speed”.
While the highest-ranked induced words for
Jaccard do include three food items, there is
no word, with the possible exception of “ice
cream”, that could be construed as a controlled
substance. The induced headwords for the
Cosine metric are considerably less pertinent
for both roles and show the above-mentioned
tendency to repeat some high-frequency words.
The inspection of “take” anecdotally con-
firms that different selectional preferences are
learned for different senses. This point (which
comes down to the usability of selectional pref-
erences for WSD) should be verified in an em-
pirical evaluation, possibly in another pseudo-
disambiguation task, choosing as confounders
seen headwords for other senses of a predicate
word.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99995825">
We have introduced the similarity-based model
for inducing selectional preferences. Comput-
ing selectional preference as a weighted sum of
similarities to seen headwords, it is a straight-
</bodyText>
<page confidence="0.990202">
222
</page>
<bodyText confidence="0.999962585365854">
forward implementation of the idea of general-
ization from seen headwords to other, similar
words. The similarity-based model is particu-
larly simple and easy to compute, and seems not
very sensitive to parameters. Like the EM-based
clustering model, it is not dependent on lexical
resources. It is, however, more flexible in that it
induces similarities from a separate generaliza-
tion corpus, which allows us to control the simi-
larities we compute by the choice of text domain
for the generalization corpus. In this paper we
have used the model to compute sense-specific
selectional preferences for semantic roles.
In a pseudo-disambiguation task the simila-
rity-based model showed error rates down to
0.16, far lower than both EM-based clustering
and Resnik&apos;s WordNet model. However its cov-
erage is considerably lower than that of EM-
based clustering, comparable to Resnik&apos;s model.
The most probable reason for this is the spar-
sity of the underlying vector space. The choice
of similarity metric is critical in similarity-based
models, with Jaccard and Lin achieving the best
performance, and Cosine surprisingly bringing
up the rear.
Next steps will be to test the similarity-based
model “in vivo”, in an SRL task; to test the
model in a WSD task; to evaluate the model on
a primary corpus that is not semantically ana-
lyzed, for greater comparability to previous ap-
proaches; to explore other vector spaces to ad-
dress the coverage issue; and to experiment on
domain transfer, using an appropriate general-
ization corpus to induce selectional preferences
for a domain different from that of the primary
corpus. This is especially relevant in view of the
domain-dependence problem that SRL faces.
Acknowledgements Many thanks to Jason
Baldridge, Razvan Bunescu, Stefan Evert, Ray
Mooney, Ulrike and Sebastian Pad6, and Sabine
Schulte im Walde for helpful discussions.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999523018867925">
N. Abe and H. Li. 1993. Learning word association
norms using tree cut pair models. In Proceedings of
ICML 1993.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of COLING-ACL
1998, Montreal, Canada.
C. Brockmann and M. Lapata. 2003. Evaluating and
combining approaches to selectional preference acqui-
sition. In Proceedings of EACL 2003, Budapest.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32(1).
X. Carreras and L. Marquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-05, Ann Arbor, MI.
S. Clark and D. Weir. 2001. Class-based probability
estimation using a semantic hierarchy. In Proceedings
of NAACL 2001, Pittsburgh, PA.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL 1997,
Madrid, Spain.
T. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895–1923.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245–
288.
D. Hindle and M. Rooth. 1993. Structural ambiguity and
lexical relations. Computational Linguistics, 19(1).
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL 1990,
Pittsburg, Pennsylvania.
J. Katz and J. Fodor. 1963. The structure of a semantic
theory. Language, 39(2).
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proceedings of ACL 1993, Columbus, OH.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998,
Montreal, Canada.
D. McCarthy and J. Carroll. 2003. Disambiguating
nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computatinal Linguis-
tics, 29(4).
P. Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational re-
alization. Cognition, 61:127–159.
M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil.
1999. Inducing an semantically annotated lexicon via
EM-based clustering. In Proceedings of ACL 1999,
Maryland.
Y. Wilks. 1975. Preference semantics. In E. Keenan,
editor, Formal Semantics of Natural Language. Cam-
bridge University Press.
</reference>
<page confidence="0.999155">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.952705">
<title confidence="0.998757">A Simple, Similarity-based Model for Selectional Preferences</title>
<author confidence="0.993096">Katrin Erk</author>
<affiliation confidence="0.999853">University of Texas at Austin</affiliation>
<email confidence="0.999682">katrin.erk@mail.utexas.edu</email>
<abstract confidence="0.9955702">We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik&apos;s WordNet-based model and the EM-based clustering model, but has coverage problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>H Li</author>
</authors>
<title>Learning word association norms using tree cut pair models.</title>
<date>1993</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="1259" citStr="Abe and Li, 1993" startWordPosition="179" endWordPosition="182">ery useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin&apos;s (1998) mutual informationbased metric, for the generalization step. This model does not require any manually created lexical resources. In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource. We focus on one application of selectional preferences: semantic role labeling. Th</context>
<context position="4687" citStr="Abe and Li (1993)" startWordPosition="695" endWordPosition="698">r the seen headwords to similar words. Resnik uses the WordNet noun hierarchy for generalization. His information-theoretic approach models the selectional preference strength of an argument positions rp of a predicate p as P(c|rp) P(c|rp) log P (c) where the c are WordNet synsets. The preference that rp has for a given synset c0, the selectional association between the two, is then defined as the contribution of c0 to rp&apos;s selectional preference strength: P(c0|rp) log P(c�|rp) P (c�) S(rp) Further WordNet-based approaches to selectional preference induction include Clark and Weir (2001), and Abe and Li (1993). Brockmann and Lapata (2003) perform a comparison of WordNet-based models. Rooth et al. (1999) generalize over seen headwords using EM-based clustering rather than WordNet. They model the probability of a word w occurring as the argument rp of a predicate p as being independently conditioned on a set of classes C: E E P(rp, w) = P(c, rp, w) = cEC cEC &apos;We write rp to indicate predicate-specific roles, like “the direct object of catch”, rather than just “obj&amp;quot;. The parameters P(c), P(rp|c) and P(w|c) are estimated using the EM algorithm. While there have been no isolated comparisons of the two g</context>
</contexts>
<marker>Abe, Li, 1993</marker>
<rawString>N. Abe and H. Li. 1993. Learning word association norms using tree cut pair models. In Proceedings of ICML 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1984" citStr="Baker et al., 1998" startWordPosition="288" endWordPosition="291">ional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin&apos;s (1998) mutual informationbased metric, for the generalization step. This model does not require any manually created lexical resources. In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource. We focus on one application of selectional preferences: semantic role labeling. The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet (Baker et al., 1998) paradigm, and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word). In SRL, the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntacticallybased systems, and (2) the problem of the domain dependence (Carreras and Marquez, 2005). In the CoNLL-05 shared task, participating systems showed about 10 points F-score difference between in-domain and out-of-domain test data. Concerning (1), we focus </context>
<context position="9727" citStr="Baker et al., 1998" startWordPosition="1520" endWordPosition="1523">Evaluee, gamblers) and (Placing, Goal, briefcase). Semantic similarity, on the other hand, will be computed on automatically syntactically parsed corpus, where the predicates are words and the arguments are syntactic dependents. Examples of extracted (p, rp, w) tuples from the generalization corpus include (catch, obj, frogs) and (intervene, in, deal).2 This instantiation of the similarity-based model allows us to compute word sense specific selectional preferences, generalizing over manually semantically annotated data using automatically syntactically annotated data. 4 Data We use FrameNet (Baker et al., 1998), a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frame. The FrameNet 1.3 annotated data comprises 139,439 sentences from the British National Corpus (BNC). For our experiments, we chose 100 frame-specific semantic roles at random, 20 each from five frequency bands: 50-100 annotated occurrences of the role, 100-200 occurrences, 200-500, 500- 1000, and more than 1000 occurrences. The annotated data for these 100 roles comprised 59,608 sentences, our primary corpus. To determine headwords of the semantic roles, the corpus was p</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of COLING-ACL 1998, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockmann</author>
<author>M Lapata</author>
</authors>
<title>Evaluating and combining approaches to selectional preference acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL 2003,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="4716" citStr="Brockmann and Lapata (2003)" startWordPosition="699" endWordPosition="703">s to similar words. Resnik uses the WordNet noun hierarchy for generalization. His information-theoretic approach models the selectional preference strength of an argument positions rp of a predicate p as P(c|rp) P(c|rp) log P (c) where the c are WordNet synsets. The preference that rp has for a given synset c0, the selectional association between the two, is then defined as the contribution of c0 to rp&apos;s selectional preference strength: P(c0|rp) log P(c�|rp) P (c�) S(rp) Further WordNet-based approaches to selectional preference induction include Clark and Weir (2001), and Abe and Li (1993). Brockmann and Lapata (2003) perform a comparison of WordNet-based models. Rooth et al. (1999) generalize over seen headwords using EM-based clustering rather than WordNet. They model the probability of a word w occurring as the argument rp of a predicate p as being independently conditioned on a set of classes C: E E P(rp, w) = P(c, rp, w) = cEC cEC &apos;We write rp to indicate predicate-specific roles, like “the direct object of catch”, rather than just “obj&amp;quot;. The parameters P(c), P(rp|c) and P(w|c) are estimated using the EM algorithm. While there have been no isolated comparisons of the two generalization paradigms that </context>
</contexts>
<marker>Brockmann, Lapata, 2003</marker>
<rawString>C. Brockmann and M. Lapata. 2003. Evaluating and combining approaches to selectional preference acquisition. In Proceedings of EACL 2003, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased measures of semantic distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="6895" citStr="Budanitsky and Hirst (2006)" startWordPosition="1077" endWordPosition="1080"> is the weight of seen headword w. Similarity sim(w0, w) will be computed on the generalization corpus, again on the basis of extracted tuples (p, rp, w). We will be using the similarity metrics shown in Table 1: Cosine, the Dice and Jaccard coefficients, and Hindle&apos;s (1990) and Lin&apos;s (1998) mutual information-based metrics. We write f for frequency, I for mutual information, and R(w) for the set of arguments rp for which w occurs as a headword. In this paper we only study corpus-based metrics. The sim function can equally well be in)stantiated with a WordNet-based metric (for an overview see Budanitsky and Hirst (2006)), but we restrict our experiments to corpus-based metrics (a) in the interest of greatest possible ES(rp) = c A(rp, c0) = 217 simcosine(w, w0) = Prp f(w,rp)·f(w0,rp) simDice(w, w0) = 2·|R(w)∩R(w0)| qP qP rp f(w,rp)2· rp f(w0,rp)2 |R(w)|+|R(w0)| P rp∈R(w)∩R(w0) I(w,r,p)I(w0,r,p) simLin(w, w0) = P |R(w)∪R(w0)| rp∈R(w) I(w,r,p) Prp∈R(w) I(w0,r,p) simJaccard(w, w0) = |R(w)∩R(w0)| simHindle(w, w0) = Erp simHindle(w, w0, rp) where simHindle(w, w0, rp) = { min(I(w,rp),I(w0,rp) if I(w, rp) &gt; 0 and I(w0, rp) &gt; 0 abs(max(I(w,rp),I(w0,rp))) if I(w, rp) &lt; 0 and I(w0, rp) &lt; 0 0 else Table 1: Similarity me</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNetbased measures of semantic distance. Computational Linguistics, 32(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Marquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-05,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2417" citStr="Carreras and Marquez, 2005" startWordPosition="356" endWordPosition="359">ne application of selectional preferences: semantic role labeling. The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet (Baker et al., 1998) paradigm, and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word). In SRL, the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntacticallybased systems, and (2) the problem of the domain dependence (Carreras and Marquez, 2005). In the CoNLL-05 shared task, participating systems showed about 10 points F-score difference between in-domain and out-of-domain test data. Concerning (1), we focus on selectional preferences as the strongest candidate for informative semantic features. Concerning (2), the corpusbased similarity metrics that we use for selectional preference induction open up interesting possibilities of mixing domains. We evaluate the similarity-based model against Resnik&apos;s WordNet-based model as well as the EM-based clustering approach. In the evaluation, the similarity-model shows lower error rates than b</context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>X. Carreras and L. Marquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL-05, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>D Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1240" citStr="Clark and Weir, 2001" startWordPosition="175" endWordPosition="178">of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin&apos;s (1998) mutual informationbased metric, for the generalization step. This model does not require any manually created lexical resources. In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource. We focus on one application of selectional preferences: semanti</context>
<context position="4664" citStr="Clark and Weir (2001)" startWordPosition="690" endWordPosition="693">rpus, then generalizing over the seen headwords to similar words. Resnik uses the WordNet noun hierarchy for generalization. His information-theoretic approach models the selectional preference strength of an argument positions rp of a predicate p as P(c|rp) P(c|rp) log P (c) where the c are WordNet synsets. The preference that rp has for a given synset c0, the selectional association between the two, is then defined as the contribution of c0 to rp&apos;s selectional preference strength: P(c0|rp) log P(c�|rp) P (c�) S(rp) Further WordNet-based approaches to selectional preference induction include Clark and Weir (2001), and Abe and Li (1993). Brockmann and Lapata (2003) perform a comparison of WordNet-based models. Rooth et al. (1999) generalize over seen headwords using EM-based clustering rather than WordNet. They model the probability of a word w occurring as the argument rp of a predicate p as being independently conditioned on a set of classes C: E E P(rp, w) = P(c, rp, w) = cEC cEC &apos;We write rp to indicate predicate-specific roles, like “the direct object of catch”, rather than just “obj&amp;quot;. The parameters P(c), P(rp|c) and P(w|c) are estimated using the EM algorithm. While there have been no isolated c</context>
</contexts>
<marker>Clark, Weir, 2001</marker>
<rawString>S. Clark and D. Weir. 2001. Class-based probability estimation using a semantic hierarchy. In Proceedings of NAACL 2001, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL</booktitle>
<location>Madrid,</location>
<contexts>
<context position="10357" citStr="Collins (1997)" startWordPosition="1623" endWordPosition="1624">icon for English that groups words in semantic classes called frames and lists semantic roles for each frame. The FrameNet 1.3 annotated data comprises 139,439 sentences from the British National Corpus (BNC). For our experiments, we chose 100 frame-specific semantic roles at random, 20 each from five frequency bands: 50-100 annotated occurrences of the role, 100-200 occurrences, 200-500, 500- 1000, and more than 1000 occurrences. The annotated data for these 100 roles comprised 59,608 sentences, our primary corpus. To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser. Our generalization corpus is the BNC. It was parsed using Minipar (Lin, 1993), which is considerably faster than the Collins parser but failed to parse about a third of all sentences. 2For details about the syntactic and semantic analyses used, see Section 4. 218 Accordingly, the arguments r extracted from the generalization corpus are Minipar dependencies, except that paths through preposition nodes were collapsed, using the preposition as the dependency relation. We obtained parses for 5,941,811 sentences of the generalization corpus. The EM-based clustering model was computed with </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of ACL 1997, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<pages>10--1895</pages>
<contexts>
<context position="12913" citStr="Dietterich, 1998" startWordPosition="2040" endWordPosition="2041">ed a preference, that word is counted as chosen. To test the performance difference between models for significance, we use Dietterich&apos;s 3 W are grateful to Carsten Brockmann and Detlef Prescher for the use of their software. 4 W exclude potential confounders that occur less than 30 or more than 3,000 times. Error Rate Coverage Cosine 0.2667 0.3284 Dice 0.1951 0.3506 Hindle 0.2059 0.3530 Jaccard 0.1858 0.3506 Lin 0.1635 0.2214 EM 30/20 0.3115 0.5460 EM 40/20 0.3470 0.9846 Resnik 0.3953 0.3084 Table 2: Error rate and coverage (microaverage), similarity-based models with uniform weights. 5x2cv (Dietterich, 1998). The test involves five 2-fold cross-validation runs. Let di,j (i E 11, 2}, j E 11,. .. , 5}) be the difference in error rates between the two models when using split i of cross-validation run j as training data. Let s2j = (d1,j − �dj)2 +(d2,j − �dj)2 be the variance for cross-validation run j, with �dj = d1°&apos;�d2°&apos; 2 . Then the 5x2cv t statistic is defined as d1,1 /1 5 2 5 �j=1 sj Under the null hypothesis, the t statistic has approximately a t distribution with 5 degrees of freedom.5 Results and discussion Error rates. Table 2 shows error rates and coverage for the different selectional pref</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>T. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10:1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>288</pages>
<contexts>
<context position="893" citStr="Gildea and Jurafsky, 2002" startWordPosition="124" endWordPosition="127">rics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik&apos;s WordNet-based model and the EM-based clustering model, but has coverage problems. 1 Introduction Selectional preferences, which characterize typical arguments of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin&apos;s (1998) mutual information</context>
<context position="15635" citStr="Gildea and Jurafsky, 2002" startWordPosition="2525" endWordPosition="2528">card-Plain, error_rate, all 0 100 200 300 400 500 numhw Mon Apr 09 02:30:47 2007 Figure 1: Learning curve: seen headwords versus error rate by frequency band, Jaccard, uniform weights 50-100 100-200 200-500 500-1000 1000- Cos 0.3167 0.3203 0.2700 0.2534 0.2606 Jac 0.1802 0.2040 0.1761 0.1706 0.1927 Table 4: Error rates for similarity-based models, by semantic role frequency band. Microaverages, uniform weights of Resnik&apos;s model are considerably higher than both the EM-based and the similarity-based models, which is unexpected. While EM-based models have been shown to work better in SRL tasks (Gildea and Jurafsky, 2002), this has been attributed to the difference in coverage. In addition to the full coverage condition, we also computed error rate and coverage for the half coverage case. In this condition, the error rates of the EM-based models are unchanged, while the error rates for all similarity-based models as well as Resnik&apos;s model rise to values between 0.4 and 0.6. So the EM-based model tends to have preferences only for the “right” words. Why this is so is not clear. It may be a genuine property, or an artifact of the FrameNet data, which only contains chosen, illustrative sentences for each frame. I</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245– 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="770" citStr="Hindle and Rooth, 1993" startWordPosition="106" endWordPosition="109">e a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik&apos;s WordNet-based model and the EM-based clustering model, but has coverage problems. 1 Introduction Selectional preferences, which characterize typical arguments of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>D. Hindle and M. Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL</booktitle>
<location>Pittsburg, Pennsylvania.</location>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicateargument structures. In Proceedings of ACL 1990, Pittsburg, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Katz</author>
<author>J Fodor</author>
</authors>
<title>The structure of a semantic theory.</title>
<date>1963</date>
<journal>Language,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="3823" citStr="Katz and Fodor, 1963" startWordPosition="559" endWordPosition="562">ing previ216 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 216–223, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics P(c)P(rp|c)P(w|c ous approaches to selectional preference induction in Section 2, we introduce the similaritybased model in Section 3. Section 4 describes the data used for the experiments reported in Section 5, and Section 6 concludes. 2 Related Work Selectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories, (see e.g. (Katz and Fodor, 1963; Wilks, 1975)). The induction of selectional preferences from corpus data was pioneered by Resnik (1996). All subsequent approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing over the seen headwords to similar words. Resnik uses the WordNet noun hierarchy for generalization. His information-theoretic approach models the selectional preference strength of an argument positions rp of a predicate p as P(c|rp) P(c|rp) log P (c) where the c are WordNet synsets. The preference that rp has for a given synset c0, the selectional asso</context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>J. Katz and J. Fodor. 1963. The structure of a semantic theory. Language, 39(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL 1993,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="10443" citStr="Lin, 1993" startWordPosition="1637" endWordPosition="1638">les for each frame. The FrameNet 1.3 annotated data comprises 139,439 sentences from the British National Corpus (BNC). For our experiments, we chose 100 frame-specific semantic roles at random, 20 each from five frequency bands: 50-100 annotated occurrences of the role, 100-200 occurrences, 200-500, 500- 1000, and more than 1000 occurrences. The annotated data for these 100 roles comprised 59,608 sentences, our primary corpus. To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser. Our generalization corpus is the BNC. It was parsed using Minipar (Lin, 1993), which is considerably faster than the Collins parser but failed to parse about a third of all sentences. 2For details about the syntactic and semantic analyses used, see Section 4. 218 Accordingly, the arguments r extracted from the generalization corpus are Minipar dependencies, except that paths through preposition nodes were collapsed, using the preposition as the dependency relation. We obtained parses for 5,941,811 sentences of the generalization corpus. The EM-based clustering model was computed with all of the FrameNet 1.3 data (139,439 sentences) as input. Resnik&apos;s model was trained </context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>D. Lin. 1993. Principle-based parsing without overgeneration. In Proceedings of ACL 1993, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<location>Montreal, Canada.</location>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL 1998, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>J Carroll</author>
</authors>
<title>Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computatinal Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="832" citStr="McCarthy and Carroll, 2003" startWordPosition="115" endWordPosition="118">tional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik&apos;s WordNet-based model and the EM-based clustering model, but has coverage problems. 1 Introduction Selectional preferences, which characterize typical arguments of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similari</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>D. McCarthy and J. Carroll. 2003. Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences. Computatinal Linguistics, 29(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional constraints: An information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="984" citStr="Resnik (1996)" startWordPosition="138" endWordPosition="139">les. In evaluations the similarity-based model shows lower error rates than both Resnik&apos;s WordNet-based model and the EM-based clustering model, but has coverage problems. 1 Introduction Selectional preferences, which characterize typical arguments of predicates, are a very useful and versatile knowledge source. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin&apos;s (1998) mutual informationbased metric, for the generalization step. This model does not require any manually created</context>
<context position="3928" citStr="Resnik (1996)" startWordPosition="576" endWordPosition="577">23, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics P(c)P(rp|c)P(w|c ous approaches to selectional preference induction in Section 2, we introduce the similaritybased model in Section 3. Section 4 describes the data used for the experiments reported in Section 5, and Section 6 concludes. 2 Related Work Selectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories, (see e.g. (Katz and Fodor, 1963; Wilks, 1975)). The induction of selectional preferences from corpus data was pioneered by Resnik (1996). All subsequent approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing over the seen headwords to similar words. Resnik uses the WordNet noun hierarchy for generalization. His information-theoretic approach models the selectional preference strength of an argument positions rp of a predicate p as P(c|rp) P(c|rp) log P (c) where the c are WordNet synsets. The preference that rp has for a given synset c0, the selectional association between the two, is then defined as the contribution of c0 to rp&apos;s selectional preference strengt</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>P. Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61:127–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
<author>S Riezler</author>
<author>D Prescher</author>
<author>G Carroll</author>
<author>F Beil</author>
</authors>
<title>Inducing an semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL 1999,</booktitle>
<location>Maryland.</location>
<contexts>
<context position="1308" citStr="Rooth et al., 1999" startWordPosition="186" endWordPosition="189"> have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). The corpus-based induction of selectional preferences was first proposed by Resnik (1996). All later approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing to other, similar words. Some approaches have used WordNet for the generalization step (Resnik, 1996; Clark and Weir, 2001; Abe and Li, 1993), others EM-based clustering (Rooth et al., 1999). In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin&apos;s (1998) mutual informationbased metric, for the generalization step. This model does not require any manually created lexical resources. In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource. We focus on one application of selectional preferences: semantic role labeling. The argument positions for which we compute selecti</context>
<context position="4782" citStr="Rooth et al. (1999)" startWordPosition="710" endWordPosition="713">ion. His information-theoretic approach models the selectional preference strength of an argument positions rp of a predicate p as P(c|rp) P(c|rp) log P (c) where the c are WordNet synsets. The preference that rp has for a given synset c0, the selectional association between the two, is then defined as the contribution of c0 to rp&apos;s selectional preference strength: P(c0|rp) log P(c�|rp) P (c�) S(rp) Further WordNet-based approaches to selectional preference induction include Clark and Weir (2001), and Abe and Li (1993). Brockmann and Lapata (2003) perform a comparison of WordNet-based models. Rooth et al. (1999) generalize over seen headwords using EM-based clustering rather than WordNet. They model the probability of a word w occurring as the argument rp of a predicate p as being independently conditioned on a set of classes C: E E P(rp, w) = P(c, rp, w) = cEC cEC &apos;We write rp to indicate predicate-specific roles, like “the direct object of catch”, rather than just “obj&amp;quot;. The parameters P(c), P(rp|c) and P(w|c) are estimated using the EM algorithm. While there have been no isolated comparisons of the two generalization paradigms that we are aware of, Gildea and Jurafsky&apos;s (2002) task-based evaluatio</context>
<context position="11434" citStr="Rooth et al. (1999)" startWordPosition="1785" endWordPosition="1788">dependency relation. We obtained parses for 5,941,811 sentences of the generalization corpus. The EM-based clustering model was computed with all of the FrameNet 1.3 data (139,439 sentences) as input. Resnik&apos;s model was trained on the primary corpus (59,608 sentences). 5 Experiments In this section we describe experiments comparing the similarity-based model for selectional preferences to Resnik&apos;s WordNet-based model and to an EM-based clustering model3. For the similarity-based model we test the five similarity metrics and three weighting schemes listed in section 3. Experimental design Like Rooth et al. (1999) we evaluate selectional preference induction approaches in a pseudodisambiguation task. In a test set of pairs (rp, w), each headword w is paired with a confounder w&apos; chosen randomly from the BNC according to its frequency4. Noun headwords are paired with noun confounders in order not to disadvantage Resnik&apos;s model, which only works with nouns. The headword/confounder pairs are only computed once and reused in all crossvalidation runs. The task is to choose the more likely role headword from the pair (w, w&apos;). In the main part of the experiment, we count a pair as covered if both w and w&apos; are </context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil. 1999. Inducing an semantically annotated lexicon via EM-based clustering. In Proceedings of ACL 1999, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Preference semantics.</title>
<date>1975</date>
<booktitle>Formal Semantics of Natural Language.</booktitle>
<editor>In E. Keenan, editor,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3837" citStr="Wilks, 1975" startWordPosition="563" endWordPosition="564">gs of the 45th Annual Meeting of the Association of Computational Linguistics, pages 216–223, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics P(c)P(rp|c)P(w|c ous approaches to selectional preference induction in Section 2, we introduce the similaritybased model in Section 3. Section 4 describes the data used for the experiments reported in Section 5, and Section 6 concludes. 2 Related Work Selectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories, (see e.g. (Katz and Fodor, 1963; Wilks, 1975)). The induction of selectional preferences from corpus data was pioneered by Resnik (1996). All subsequent approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing over the seen headwords to similar words. Resnik uses the WordNet noun hierarchy for generalization. His information-theoretic approach models the selectional preference strength of an argument positions rp of a predicate p as P(c|rp) P(c|rp) log P (c) where the c are WordNet synsets. The preference that rp has for a given synset c0, the selectional association betwee</context>
</contexts>
<marker>Wilks, 1975</marker>
<rawString>Y. Wilks. 1975. Preference semantics. In E. Keenan, editor, Formal Semantics of Natural Language. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>