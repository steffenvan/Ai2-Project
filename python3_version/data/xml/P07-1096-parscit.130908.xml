<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.992984">
Guided Learning for Bidirectional Sequence Classification
</title>
<author confidence="0.877564">
Libin Shen
</author>
<affiliation confidence="0.614747">
BBN Technologies
</affiliation>
<address confidence="0.573271">
Cambridge, MA 02138, USA
</address>
<email confidence="0.990841">
lshen@bbn.com
</email>
<author confidence="0.994827">
Giorgio Satta
</author>
<affiliation confidence="0.910152333333333">
Dept. of Inf. Eng’g.
University of Padua
I-35131 Padova, Italy
</affiliation>
<email confidence="0.981711">
satta@dei.unipd.it
</email>
<author confidence="0.943532">
Aravind K. Joshi
</author>
<affiliation confidence="0.886407333333333">
Department of CIS
University of Pennsylvania
Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.997621">
joshi@seas.upenn.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958666666667">
In this paper, we propose guided learning,
a new learning framework for bidirectional
sequence classification. The tasks of learn-
ing the order of inference and training the
local classifier are dynamically incorporated
into a single Perceptron like learning algo-
rithm. We apply this novel learning algo-
rithm to POS tagging. It obtains an error rate
of 2.67% on the standard PTB test set, which
represents 3.3% relative error reduction over
the previous best result on the same data set,
while using fewer features.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958962962963">
Many NLP tasks can be modeled as a sequence clas-
sification problem, such as POS tagging, chunking,
and incremental parsing. A traditional method to
solve this problem is to decompose the whole task
into a set of individual tasks for each token in the in-
put sequence, and solve these small tasks in a fixed
order, usually from left to right. In this way, the out-
put of the previous small tasks can be used as the
input of the later tasks. HMM and MaxEnt Markov
Model are examples of this method.
Lafferty et al. (2001) showed that this approach
suffered from the so called label bias problem (Bot-
tou, 1991). They proposed Conditional Random
Fields (CRF) as a general solution for sequence clas-
sification. CRF models a sequence as an undirected
graph, which means that all the individual tasks are
solved simultaneously. Taskar et al. (2003) improved
the CRF method by employing the large margin
method to separate the gold standard sequence la-
beling from incorrect labellings. However, the com-
plexity of quadratic programming for the large mar-
gin approach prevented it from being used in large
scale NLP tasks.
Collins (2002) proposed a Perceptron like learn-
ing algorithm to solve sequence classification in the
traditional left-to-right order. This solution does not
suffer from the label bias problem. Compared to the
undirected methods, the Perceptron like algorithm
is faster in training. In this paper, we will improve
upon Collins’ algorithm by introducing a bidirec-
tional searching strategy, so as to effectively utilize
more context information at little extra cost.
When a bidirectional strategy is used, the main
problem is how to select the order of inference. Tsu-
ruoka and Tsujii (2005) proposed the easiest-first ap-
proach which greatly reduced the computation com-
plexity of inference while maintaining the accuracy
on labeling. However, the easiest-first approach only
serves as a heuristic rule. The order of inference is
not incorporated into the training of the MaxEnt clas-
sifier for individual labeling.
Here, we will propose a novel learning frame-
work, namely guided learning, to integrate classifi-
cation of individual tokens and inference order selec-
tion into a single learning task. We proposed a Per-
ceptron like learning algorithm (Collins and Roark,
2004; Daum´e III and Marcu, 2005) for guided learn-
ing. We apply this algorithm to POS tagging, a clas-
sic sequence learning problem. Our system reports
an error rate of 2.67% on the standard PTB test set,
a relative 3.3% error reduction of the previous best
system (Toutanova et al., 2003) by using fewer fea-
tures. By using deterministic search, it obtains an
error rate of 2.73%, a 5.9% relative error reduction
</bodyText>
<page confidence="0.940273">
760
</page>
<note confidence="0.925232">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999595272727273">
over the previous best deterministic algorithm (Tsu-
ruoka and Tsujii, 2005).
The new POS tagger is similar to (Toutanova et
al., 2003; Tsuruoka and Tsujii, 2005) in the way
that we employ context features. We use a bidi-
rectional search strategy (Woods, 1976; Satta and
Stock, 1994), and our algorithm is based on Percep-
tron learning (Collins, 2002). A unique contribution
of our work is on the integration of individual clas-
sification and inference order selection, which are
learned simultaneously.
</bodyText>
<sectionHeader confidence="0.924374" genericHeader="introduction">
2 Guided Learning for Bidirectional
Labeling
</sectionHeader>
<bodyText confidence="0.999956">
We first present an example of POS tagging to show
the idea of bidirectional labeling. Then we present
the inference algorithm and the learning algorithm.
</bodyText>
<subsectionHeader confidence="0.983007">
2.1 An Example of POS tagging
</subsectionHeader>
<bodyText confidence="0.967708583333334">
Suppose that we have an input sentence
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 0)
If we scan from left to right, we may find it
difficult to resolve the ambiguity of the label for
that, which could be either DT (determiner), or
IN (preposition or subordinating conjunction) in the
Penn Treebank. However, if we resolve the labels for
book and interesting, it would be relatively easy to
figure out the correct label for that.
Now, we show how bidirectional inference works
on this sample. Suppose we use beam search with
width of 2, and we use a window of (-2, 2) for con-
text features.
For the first step, we enumerate hypotheses for
each word. For example, found could have a label
VBN or VBD. Suppose that at this point the most
favorable action, out of the candidate hypotheses, is
the assignment of NN to book, according to the con-
text features defined on words. Then, we resolve the
label for book first. We maintain the top two hy-
potheses as shown below. Here, the second most fa-
vorable label for book is VB.
</bodyText>
<footnote confidence="0.8701192">
NN
VB
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 1)
</footnote>
<bodyText confidence="0.997476285714286">
At the second step, assume the most favorable ac-
tion is the assignment of label JJ to interesting in
the context of NN for book. Then we maintain the
top two hypotheses for span book interesting as
shown below. The second most favorable label for
interesting is still JJ, but in the context of VB for
book.
</bodyText>
<equation confidence="0.6192878">
NN JJ
VB JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 2)
</equation>
<bodyText confidence="0.974405">
Then, suppose we are most confident for assigning
labels VBD and VBN to found, in that order. We get
two separated tagged spans as shown below.
</bodyText>
<equation confidence="0.822956">
VBD NN JJ
VBN VB JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 3)
</equation>
<bodyText confidence="0.999853857142857">
In the next step, suppose we are most confident for
assigning label DT to that under the context of VBD
on the left and NN-JJ on the right side, as shown
below (second most favorable action, not discussed
here, is also displayed). After tagging w3, two sep-
arated spans merge into one, starting from found to
interesting.
</bodyText>
<sectionHeader confidence="0.572039" genericHeader="method">
VBD---DT---NN JJ
VBD---IN---NN JJ
</sectionHeader>
<bodyText confidence="0.714380833333333">
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 4)
For the last step, we assign label NNP to Agatha,
which could be an out-of-vocabulary word, under the
context of VBD-DT on the right.
</bodyText>
<footnote confidence="0.3490606">
NNP---VBD---DT---NN JJ
NNP---VBD---IN---NN JJ
Agatha found that book interesting
w1 w2 w3 w4 w5
(Step 5)
</footnote>
<bodyText confidence="0.999231">
This simple example has shown the advantage of
adopting a flexible search strategy. However, it is
still unclear how we maintain the hypotheses, how
we keep candidates and accepted labels and spans,
and how we employ dynamic programming. We will
answer these questions in the formal definition of the
inference algorithm in the next section.
</bodyText>
<page confidence="0.994393">
761
</page>
<subsectionHeader confidence="0.94249">
2.2 Inference Algorithm
</subsectionHeader>
<bodyText confidence="0.9828332">
Terminology: Let the input sequence be
w1w2 · · · wn. For each token wz, we are expected
to assign a label tz E T, with T the label set.
A subsequence wz · · · wj is called a span, and is
denoted [i, j]. Each span p considered by the al-
gorithm is associated with one or more hypotheses,
that is, sequences over T having the same length as
p. Part of the label sequence of each hypothesis is
used as a context for labeling tokens outside the span
p. For example, if a tri-gram model is adopted, we
use the two labels on the left boundary and the two
labels on the right boundary of the hypothesis for la-
beling outside tokens. The left two labels are called
the left interface, and the right two labels are called
the right interface. Left and right interfaces have
only one label in case of spans of length one.
A pair s = (Ilea, Ijght) with a left and a right
interface is called a state. We partition the hypothe-
ses associated with span p into sets compatible with
the same state. In practice, for span p, we use a ma-
trix Mp indexed by states, so that Mp(s), s = (Ilett,
I,jght), is the set of all hypotheses associated with p
that are compatible with Ilett and I ght.
For a span p and a state s, we denote the associated
top hypothesis as
</bodyText>
<equation confidence="0.9897015">
s.T = argmax V (h),
hcMp(s)
</equation>
<bodyText confidence="0.548866833333333">
where V is the score of a hypothesis (defined in (1)
below). Similarly, we denote the top state for p as
Algorithm 1 Inference Algorithm
Require: token sequence w1 · · · wn;
Require: beam width B;
Require: weight vector w;
</bodyText>
<listItem confidence="0.997824428571429">
1: Initialize P, the set of accepted spans;
2: Initialize Q, the queue of candidate spans;
3: repeat
4: span p&apos; +— argmaxpcQ U(p.S.T.A);
5: Update P with p&apos;;
6: Update Q with p&apos; and P;
7: until (Q = 0)
</listItem>
<bodyText confidence="0.999993833333333">
where U is the score of an action. In other words,
the score of an hypothesis is the sum of the score
of the most recent action h.A and the scores of the
top hypotheses of the context states. The score of
an action h.A is computed through a linear function
whose weight vector is w, as
</bodyText>
<equation confidence="0.999137">
U(h.A) = w · f(h.A), (2)
</equation>
<bodyText confidence="0.999963454545455">
where f(h.A) is the feature vector of action h.A,
which depends on h.SL and h.SR.
Algorithm: Algorithm 1 is the inference algorithm.
We are given the input sequence and two parame-
ters, beam width B to determine the number of states
maintained for each span, and weight vector w used
to compute the score of an action.
We first initialize the set P of accepted spans with
the empty set. Then we initialize the queue Q of
candidate spans with span [i, i] for each token wz,
and for each t E T assigned to wz we set
</bodyText>
<equation confidence="0.703275">
p.S = argmax V (s.T). M[z,z]((t, t)) = {i , t},
s: Mp(s)��O
</equation>
<bodyText confidence="0.999898272727273">
Therefore, for each span p, we have a top hypothe-
sis p.S.T, whose score is the highest among all the
hypotheses for span p.
Hypotheses are started and grown by means of
labeling actions. For each hypothesis h associated
with a span p we maintain its most recent labeling
action h.A, involving some token within p, as well
as the states h.SL and h.SR that have been used as
context by such an action, if any. Note that h.SL and
h.SR refer to spans that are subsequences of p. We
recursively compute the score of h as
</bodyText>
<equation confidence="0.995195">
V (h) = V (h.SL.T) + V (h.SR.T) + U(h.A), (1)
</equation>
<bodyText confidence="0.9999604">
where i , t represents the hypothesis consisting of
a single action which assigns label t to wz. This pro-
vides the set of starting hypotheses.
As for the example Agatha found that book
interesting in the previous subsection, we have
</bodyText>
<listItem confidence="0.998484166666667">
• P = 0
• Q = {[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]}
Suppose NN and VB are the two possible POS tags
for w4 book. We have
• M[4,4](NN, NN) = {h441 = 4 , NN}
• M[4,4](VB, VB) = {h442 = 4 , VB}
</listItem>
<bodyText confidence="0.8770315">
The most recent action of hypothesis h441 is to as-
sign NN to w4. According to Equation (2), the score
</bodyText>
<page confidence="0.981368">
762
</page>
<bodyText confidence="0.9949845">
of this action U(h441.A) depends on the features de-
fined on the local context of action. For example,
</bodyText>
<equation confidence="0.6308245">
if t = NN ∧ w−1 = that
f1001 (h441 .A) = 1 { 0 otherwise,
</equation>
<bodyText confidence="0.999768333333333">
where w−1 represents the left word. It should be
noted that, for all the features depending on the
neighboring tags, the value is always 0, since those
tags are still unknown in the step of initialization.
Since this operation does not depend on solved tags,
we have V (h441) = U(h411.A), according to Equa-
tion (1).
The core of the algorithm repeatedly selects a can-
didate span from Q, and uses it to update P and Q,
until a span covering the whole sequence is added to
P and Q becomes empty. This is explained in detail
below.
At each step, we remove from Q the span p&apos; such
that the action (not hypothesis) score of its top hy-
pothesis, p&apos;.S.T, is the highest. This represents the
labeling action for the next move that we are most
confident about. Now we need to update P and Q
with the selected span p&apos;. We add p&apos; to P, and re-
move from P the spans included in p&apos;, if any. Let
S be the set of removed spans. We remove from Q
each span which takes one of the spans in S as con-
text, and replace it with a new candidate span taking
p&apos; (and another accepted span) as context. We always
maintain B different states for each span.
Back to the previous example, after Step 3 is com-
pleted, w2 found, w4 book and w5 interesting
have been tagged and we have
</bodyText>
<listItem confidence="0.999527">
• P = {[2, 2], [4,5]}
• Q = {[1, 2], [2, 5]}
</listItem>
<bodyText confidence="0.998741428571429">
There are two candidate spans in Q, each with its as-
sociated hypotheses and most recent actions. More
specifically, we can either solve w1 based on the con-
text hypotheses for [2, 2], resulting in span [1, 2], or
else solve w3 based on the context hypotheses in
[2, 2] and [4, 5], resulting in span [2, 5].
The top two states for span [2, 2] are
</bodyText>
<listItem confidence="0.897844666666667">
• M[2,2](VBD, VBD) = {h221 = 2 → VBD}
• M[2,2](VBN, VBN) = {h222 = 2 → VBN}
and the top two states for span [4, 5] are
• M[4,5](NN-JJ, NN-JJ)
= {h451 = (NN,NN)5 → JJ}
• M[4,5](VB-JJ, VB-JJ)
</listItem>
<bodyText confidence="0.952434384615385">
= {h452 = (VB,VB)5 → JJ}
Here (NN,NN)5 → JJ represents the hypothesis
coming from the action of assigning JJ to w5 under
the left context state of (NN,NN). (VB,VB)5 → JJ
has a similar meaning.1
We first compute the hypotheses resulting from all
possible POS tag assignments to w3, under all possi-
ble state combinations of the neighboring spans [2, 2]
and [4, 5]. Suppose the highest score action consists
in the assignment of DT under the left context state
(VBD, VBD) and the right context state (NN-JJ, NN-
JJ). We obtain hypothesis h251 = (VBD,VBD)3 →
DT(NN-JJ, NN-JJ) with
</bodyText>
<equation confidence="0.999684">
V (h251) = V((VBD,VBD).T) +
V((NN-JJ,NN-JJ).T) + U(h251.A)
= V (h221) + V (h451) + w · f(h251.A)
</equation>
<bodyText confidence="0.9929107">
Here, features for action h251.A may depend on
the left tag VBD and right tags NN-JJ, which have
been solved before. More details of the feature func-
tions are given in Section 4.2. For example, we can
have features like
( 1 if t = DT ∧ t+2 = JJ
Sl 0 otherwise,
We maintain the top two states with the highest
hypothesis scores, if the beam width is set to two.
We have
</bodyText>
<listItem confidence="0.98797625">
• M[2,5](VBD-DT, NN-JJ) = {h251 =
(VBD,VBD)3 → DT(NN-JJ,NN-JJ)}
• M[2,5](VBD-IN, NN-JJ) = {h252 =
(VBD,VBD)3 → IN(NN-JJ,NN-JJ)}
</listItem>
<bodyText confidence="0.595629714285714">
Similarly, we compute the top hypotheses and
states for span [1, 2]. Suppose now the hypothesis
with the highest action score is h251. Then we up-
date P by adding [2, 5] and removing [2, 2] and [4, 5],
which are covered by [2, 5]. We also update Q by re-
moving [2,5] and [1, 2],2 and add new candidate span
[1, 5] resulting in
</bodyText>
<listItem confidence="0.9996495">
• P = {[2,5]}
• Q = {[1, 5]}
</listItem>
<footnote confidence="0.978611888888889">
1It should be noted that, in these cases, each state con-
tains only one hypothesis. However, if the span is longer than
4 words, there may exist multiple hypotheses for the same
state. For example, hypotheses DT-NN-VBD-DT-JJ and DT-
NN-VBN-DT-JJ have the same left interface DT-NN and right
interface DT-JJ.
2Span [1, 2] depends on [2, 2] and [2, 2] has been removed
from P. So it is no longer a valid candidate given the accepted
spans in P.
</footnote>
<equation confidence="0.962717">
f2002(h251.A) =
</equation>
<page confidence="0.962773">
763
</page>
<bodyText confidence="0.9604738">
The algorithm is especially designed in such a way Algorithm 2 Guided Learning Algorithm
that, at each step, some new span is added to P or
else some spans already present in P are extended
by some token(s). Furthermore, no pair of overlap-
ping spans is ever found in P, and the number of
pairs of overlapping spans that may be found in Q is
always bounded by a constant. This means that the
algorithm performs at most n iterations, and its run-
ning time is therefore O(B2n), that is, linear in the
length of the input sequence.
2.3 Learning Algorithm
In this section, we propose guided learning, a Per-
ceptron like algorithm, to learn the weight vector w,
as shown in Algorithm 2. We use p&apos;.G to represent
the gold standard hypothesis on span p&apos;.
For each input sequence Xr and the gold standard
sequence of labeling Yr, we first initialize P and Q
as in the inference algorithm. Then we select the
span for the next move as in Algorithm 1. If p&apos;.S.T,
the top hypothesis of the selected span p&apos;, is com-
patible with the gold standard, we update P and Q
as in Algorithm 1. Otherwise, we update the weight
vector in the Perceptron style, by promoting the fea-
tures of the gold standard action, and demoting the
features of the action of the top hypothesis. Then
we re-generate the queue Q with P and the updated
weight vector w. Specifically, we first remove all the
elements in Q, and then generate hypotheses for all
the possible spans based on the context spans in P.
Hypothesis scores and action scores are calculated
with the updated weight vector w.
A special aspect of Algorithm 2 is that we main-
tain two scores: the score of the action represents the
confidence for the next move, and the score of the
hypothesis represents the overall quality of a partial
result. The selection for the next action directly de-
pends on the score of the action, but not on the score
of the hypothesis. On the other hand, the score of the
hypothesis is used to maintain top partial results for
each span.
We briefly describe the soundness of the Guided
Learning Algorithm in terms of two aspects. First,
in Algorithm 2 weight update is activated whenever
there exists an incorrect state s, the action score of
whose top hypothesis s.T is higher than that of any
state in each span. We demote this action and pro-
mote the gold standard action on the same span.
764
Require: training sequence pairs {(Xr,Yr)11&lt;r&lt;R;
Require: beam width B and iterations I;
</bodyText>
<listItem confidence="0.955182157894737">
1: w &lt; --0;
2: for (i &lt;-- 1; i &lt; I; i++) do
3: for (r &lt;-- 1; r &lt; R; r++) do
4: Load sequence Xr and gold labeling Yr.
5: Initialize P, the set of accepted spans
6: Initialize Q, the queue of candidate spans;
7: repeat
8: p&apos; &lt;-- argmaxr,EQ U(p.S.T.A);
9: if (p&apos;.S.T = p&apos;.G) then
10: Update P with p&apos;;
11: Update Q with p&apos; and P;
12: else
13: promote(w, f(p&apos;.G.A));
14: demote(w,f(p&apos;.S.T.A));
15: Re-generate Q with w and P;
16: end if
17: until (Q = 0)
18: end for
19: end for
</listItem>
<bodyText confidence="0.999956693877551">
However, we do not automatically adopt the gold
standard action on this span. Instead, in the next
step, the top hypothesis of another span might be se-
lected based on the score of action, which means that
it becomes the most favorable action according to the
updated weights.
As a second aspect, if the action score of a gold
standard hypothesis is higher than that of any oth-
ers, this hypothesis and the corresponding span are
guaranteed to be selected at line 8 of Algorithm 2.
The reason for this is that the scores of the context
hypotheses of a gold standard hypothesis must be
no less than those of other hypotheses of the same
span. This could be shown recursively with respect
to Equation 1, because the context hypotheses of a
gold standard hypothesis are also compatible with
the gold standard.
Furthermore, if we take
(xi = f(p&apos;.G.A) − f(p&apos;.S.T.A), yi = +1)
as a positive sample, and
(xj = f(p&apos;.S.T.A) − f(p&apos;.G.A), yj = −1)
as a negative sample, the weight updates at lines 13
and 14 are a stochastic approximation of gradient de-
scent that minimizes the squared errors of the mis-
classified samples (Widrow and Hoff, 1960). What
is special with our learning algorithm is the strategy
used to select samples for training.
In general, this novel learning framework lies be-
tween supervised learning and reinforcement learn-
ing. Guided learning is more difficult than super-
vised learning, because we do not know the order of
inference. The order is learned automatically, and
partial output is in turn used to train the local clas-
sifier. Therefore, the order of inference and the lo-
cal classification are dynamically incorporated in the
learning phase.
Guided learning is not as hard as reinforcement
learning. At each local step in learning, we always
know the undesirable labeling actions according to
the gold standard, although we do not know which
is the most desirable. In this approach, we can eas-
ily collect the automatically generated negative sam-
ples, and use them in learning. These negative sam-
ples are exactly those we will face during inference
with the current weight vector.
In our experiments, we have used Averaged Per-
ceptron (Collins, 2002; Freund and Schapire, 1999)
and Perceptron with margin (Krauth and M´ezard,
1987) to improve performance.
</bodyText>
<sectionHeader confidence="0.999744" genericHeader="method">
3 Related Works
</sectionHeader>
<bodyText confidence="0.992501157894737">
Tsuruoka and Tsujii (2005) proposed a bidirectional
POS tagger, in which the order of inference is han-
dled with the easiest-first heuristic. Gim´enez and
M`arquez (2004) combined the results of a left-to-
right scan and a right-to-left scan. In our model, the
order of inference is dynamically incorporated into
the training of the local classifier.
Toutanova et al. (2003) reported a POS tagger
based on cyclic dependency network. In their work,
the order of inference is fixed as from left to right. In
this approach, large beam width is required to main-
tain the ambiguous hypotheses. In our approach, we
can handle tokens that we are most confident about
first, so that our system does not need a large beam.
As shown in Section 4.2, even deterministic infer-
ence shows rather good results.
Our guided learning can be modeled as a search
algorithm with Perceptron like learning (Daum´e III
and Marcu, 2005). However, as far as we know,
</bodyText>
<table confidence="0.99904825">
Data Set Sections Sentences Tokens
Training 0-18 38,219 912,344
Develop 19-21 5,527 131,768
Test 22-24 5,462 129,654
</table>
<tableCaption confidence="0.999674">
Table 1: Data set splits
</tableCaption>
<bodyText confidence="0.9999">
the mechanism of bidirectional search with an on-
line learning algorithm has not been investigated be-
fore. In (Daum´e III and Marcu, 2005), as well
as other similar works (Collins, 2002; Collins and
Roark, 2004; Shen and Joshi, 2005), only left-to-
right search was employed. Our guided learning al-
gorithm provides more flexibility in search with an
automatically learned order. In addition, our treat-
ment of the score of action and the score of hypoth-
esis is unique (see discussion in Section 2.3).
Furthermore, compared to the above works, our
guided learning algorithm is more aggressive on
learning. In (Collins and Roark, 2004; Shen and
Joshi, 2005), a search stops if there is no hypothe-
sis compatible with the gold standard in the queue
of candidates. In (Daum´e III and Marcu, 2005), the
search is resumed after some gold standard compat-
ible hypotheses are inserted into a queue for future
expansion, and the weights are updated correspond-
ingly. However, there is no guarantee that the up-
dated weights assign a higher score to those inserted
gold standard compatible hypotheses. In our algo-
rithm, the gold standard compatible hypotheses are
used for weight update only. As a result, after each
sentence is processed, the weight vector can usually
successfully predict the gold standard parse. There-
fore our learning algorithm is aggressive on weight
update.
As far as this aspect is concerned, our algorithm
is similar to the MIRA algorithm in (Crammer and
Singer, 2003). In MIRA, one always knows the cor-
rect hypothesis. In our case, we do not know the
correct order of operations. So we use our form of
weight update to implement aggressive learning.
</bodyText>
<sectionHeader confidence="0.983334" genericHeader="method">
4 Experiments on POS Tagging
</sectionHeader>
<subsectionHeader confidence="0.986886">
4.1 Settings
</subsectionHeader>
<bodyText confidence="0.996913">
We apply our guided learning algorithm to POS tag-
ging. We carry out experiments on the standard
data set of the Penn Treebank (PTB) (Marcus et al.,
1994). Following (Ratnaparkhi,1996; Collins, 2002;
Toutanova et al., 2003; Tsuruoka and Tsujii, 2005),
</bodyText>
<page confidence="0.993791">
765
</page>
<table confidence="0.9986">
Feature Sets Templates Error%
A Ratnaparkhi’s 3.05
B A + [t0, t1], [t0, t−1, t1], [t0, t1, t2] 2.92
C B + [t0, t−2], [t0, t2], [t0, t−2, w0], [t0, t−1, w0], [t0, t1, w0], 2.84
[t0, t2, w0], [t0, t−2, t−1, w0], [t0, t−1, t1, w0], [t0, t1, t2, w0]
D C + [t0, w−1, w0], [t0, w1, w0] 2.78
E D + [t0, X = prefix or suffix of w0], 4 &lt; |X |&lt; 9 2.72
</table>
<tableCaption confidence="0.999546">
Table 2: Experiments on the development data with beam width of 3
</tableCaption>
<bodyText confidence="0.9999214">
we cut the PTB into the training, development and
test sets as shown in Table 1. We use tools provided
by CoNLL-2005 3 to extract POS tags from the mrg
files of PTB. So the data set is the same as previous
work. We use the development set to select features
and estimate the number of iterations in training. In
our experiments, we enumerate all the POS tags for
each word instead of using a dictionary as in (Ratna-
parkhi, 1996), since the size of the tag set is tractable
and our learning algorithm is efficient enough.
</bodyText>
<sectionHeader confidence="0.900758" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.995113666666666">
Effect of Features: We first run the experiments to
evaluate the effect of features. We use templates to
define features. For this set of experiments, we set
the beam width B = 3 as a balance between speed
and accuracy. The guided learning algorithm usually
converges on the development data set in 4-8 itera-
tions over the training data.
Table 2 shows the error rate on the development
set with different features. We first use the same fea-
ture set used in (Ratnaparkhi, 1996), which includes
a set of prefix, suffix and lexical features, as well
as some bi-gram and tri-gram context features. Fol-
lowing (Collins, 2002), we do not distinguish rare
words. On set A, Ratnaparkhi’s feature set, our sys-
tem reports an error rate of 3.05% on the develop-
ment data set.
With set B, we include a few feature templates
which are symmetric to those in Ratnaparkhi’s set,
but are only available with bidirectional search. With
set C, we add more bi-gram and tri-gram features.
With set D, we include bi-lexical features. With set
E, we use prefixes and suffixes of length up to 9, as in
(Toutanova et al., 2003; Tsuruoka and Tsujii, 2005).
We obtain 2.72% of error rate. We will use this fea-
ture set on our final experiments on the test data.
Effect of Search and Learning Strategies: For the
second set of experiments, we evaluate the effect of
</bodyText>
<footnote confidence="0.9910765">
3http://www.lsi.upc.es/˜srlconll/soft.html, package srlconll-
1.1.tgz.
</footnote>
<table confidence="0.9998776">
Search Aggressive? Beam=1 Beam=3
L-to-R Yes 2.94 2.82
L-to-R No 3.24 2.75
Bi-Dir Yes 2.84 2.72
Bi-Dir No does not converge
</table>
<tableCaption confidence="0.999939">
Table 3: Experiments on the development data
</tableCaption>
<bodyText confidence="0.999865">
search methods, learning strategies, and beam width.
We use feature set E for this set of experiments. Ta-
ble 3 shows the error rates on the development data
set with both left-to-right (L-to-R) and bidirectional
(Bi-Dir) search methods. We also tested both aggres-
sive learning and non-aggressive learning strategies
with beam width of 1 and 3.
First, with non-aggressive learning on bidirec-
tional search, the error rate does not converge to a
comparable number. This is due to the fact that the
search space is too large in bidirectional search, if
we do not use aggressive learning to constrain the
samples for learning.
With aggressive learning, the bidirectional ap-
proach always shows advantages over left-to-right
search. However, the gap is not large. This is
due to the fact that the accuracy of POS tagging
is very high. As a result, we can always keep the
gold-standard tags in the beam even with left-to-right
search in training.
This can also explain why the performance of left-
to-right search with non-aggressive learning is close
to bidirectional search if the beam is large enough.
However, with beam width = 1, non-aggressive
learning over left-to-right search performs much
worse, because in this case it is more likely that the
gold-standard tag is not in the beam.
This set of experiments show that guided learn-
ing is more preferable for tasks with higher ambi-
guities. In our recent work (Shen and Joshi, 2007),
we have applied a variant of this algorithm to depen-
dency parsing, and showed significant improvement
over left-to-right non-aggressive learning strategy.
Comparison: Table 4 shows the comparison with
the previous works on the PTB test sections.
</bodyText>
<page confidence="0.992703">
766
</page>
<table confidence="0.9978738">
System Beam Error%
(Ratnaparkhi, 1996) 5 3.37
(Tsuruoka and Tsujii, 2005) 1 2.90
(Collins, 2002) - 2.89
Guided Learning, feature B 3 2.85
(Tsuruoka and Tsujii, 2005) all 2.85
(Gim´enez and M`arquez, 2004) - 2.84
(Toutanova et al., 2003) - 2.76
Guided Learning, feature E 1 2.73
Guided Learning, feature E 3 2.67
</table>
<tableCaption confidence="0.999978">
Table 4: Comparison with the previous works
</tableCaption>
<bodyText confidence="0.999971684210526">
According to the experiments shown above, we
build our best system by using feature set E with
beam width B = 3. The number of iterations on
the training data is estimated with respect to the de-
velopment data. We obtain an error rate of 2.67%
on the test data. With deterministic search, or beam
with B = 1, we obtain an error rate of 2.73%.
Compared to previous best result on the same data
set, 2.76% by (Toutanova et al., 2003), our best re-
sult shows a relative error reduction of 3.3%. This
result is very promising, since we have not used any
specially designed features in our experiments. It is
reported in (Toutanova et al., 2003) that a crude com-
pany name detector was used to generate features,
and it gave rise to significant improvement in per-
formance. However, it is difficult for us to duplicate
exactly the same feature for the purpose of compari-
son, although it is convenient to use features like that
in our framework.
</bodyText>
<sectionHeader confidence="0.999806" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999935764705882">
In this paper, we propose guided learning, a new
learning framework for bidirectional sequence clas-
sification. The tasks of learning the order of infer-
ence and training the local classifier are dynamically
incorporated into a single Perceptron like algorithm.
We apply this novel algorithm to POS tagging. It
obtains an error rate of 2.67% on the standard PTB
test set, which represents 3.3% relative error reduc-
tion over the previous best result (Toutanova et al.,
2003) on the same data set, while using fewer fea-
tures. By using deterministic search, it obtains an
error rate of 2.73%, a 5.9% relative error reduction
over the previous best deterministic algorithm (Tsu-
ruoka and Tsujii, 2005). It should be noted that the
error rate is close to the inter-annotator discrepancy
on PTB, the standard test set for POS tagging, there-
fore it is very difficult to achieve improvement.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999315">
L. Bottou. 1991. Une approche th´eorique de l’apprentissage
connexionniste: Applications a` la reconnaissance de la pa-
role. Ph.D. thesis, Universit´e de Paris XI.
M. Collins and B. Roark. 2004. Incremental parsing with the
perceptron algorithm. In ACL-2004.
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In EMNLP-2002.
K. Crammer and Y. Singer. 2003. Ultraconservative online
algorithms for multiclass problems. Journal of Machine
Learning Research, 3:951–991.
H. Daum´e III and D. Marcu. 2005. Learning as search opti-
mization: Approximate large margin methods for structured
prediction. In ICML-2005.
Y. Freund and R. E. Schapire. 1999. Large margin classifi-
cation using the perceptron algorithm. Machine Learning,
37(3):277–296.
J. Gim´enez and L. M`arquez. 2004. Svmtool: A general pos tag-
ger generator based on support vector machines. In LREC-
2004.
W. Krauth and M. M´ezard. 1987. Learning algorithms with
optimal stability in neural networks. Journal of Physics A,
20:745–752.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmentation and la-
beling sequence data. In ICML-2001.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994.
Building a large annotated corpus of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313–330.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In EMNLP-1996.
G. Satta and O. Stock. 1994. Bi-Directional Context-Free
Grammar Parsing for Natural Language Processing. Artifi-
cialIntelligence, 69(1-2).
L. Shen and A. K. Joshi. 2005. Incremental LTAG Parsing. In
EMNLP-2005.
L. Shen and A. K. Joshi. 2007. Bidirectional LTAG Depen-
dency Parsing. Technical Report 07-02, IRCS, UPenn.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In NIPS-2003.
K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic dependency
network. In NAACL-2003.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional inference
with the easiest-first strategy for tagging sequence data. In
EMNLP-2005.
B. Widrow and M. E. Hoff. 1960. Adaptive switching circuits.
IRE WESCON Convention Record, part 4.
W. Woods. 1976. Parsers in speech understanding systems.
Technical Report 3438, Vol. 4, 1–21, BBN Inc.
</reference>
<page confidence="0.996747">
767
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963263">
<title confidence="0.998756">Guided Learning for Bidirectional Sequence Classification</title>
<author confidence="0.999529">Libin Shen</author>
<affiliation confidence="0.99157">BBN Technologies</affiliation>
<address confidence="0.99998">Cambridge, MA 02138, USA</address>
<email confidence="0.999816">lshen@bbn.com</email>
<author confidence="0.999842">Giorgio Satta</author>
<affiliation confidence="0.999902">Dept. of Inf. Eng’g. University of Padua</affiliation>
<address confidence="0.999653">I-35131 Padova, Italy</address>
<email confidence="0.99776">satta@dei.unipd.it</email>
<author confidence="0.999892">Aravind K Joshi</author>
<affiliation confidence="0.999901">Department of CIS University of Pennsylvania</affiliation>
<address confidence="0.999443">Philadelphia, PA 19104, USA</address>
<email confidence="0.99986">joshi@seas.upenn.edu</email>
<abstract confidence="0.998224923076923">In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bottou</author>
</authors>
<title>Une approche th´eorique de l’apprentissage connexionniste: Applications a` la reconnaissance de la parole.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit´e de Paris XI.</institution>
<contexts>
<context position="1476" citStr="Bottou, 1991" startWordPosition="236" endWordPosition="238">duction Many NLP tasks can be modeled as a sequence classification problem, such as POS tagging, chunking, and incremental parsing. A traditional method to solve this problem is to decompose the whole task into a set of individual tasks for each token in the input sequence, and solve these small tasks in a fixed order, usually from left to right. In this way, the output of the previous small tasks can be used as the input of the later tasks. HMM and MaxEnt Markov Model are examples of this method. Lafferty et al. (2001) showed that this approach suffered from the so called label bias problem (Bottou, 1991). They proposed Conditional Random Fields (CRF) as a general solution for sequence classification. CRF models a sequence as an undirected graph, which means that all the individual tasks are solved simultaneously. Taskar et al. (2003) improved the CRF method by employing the large margin method to separate the gold standard sequence labeling from incorrect labellings. However, the complexity of quadratic programming for the large margin approach prevented it from being used in large scale NLP tasks. Collins (2002) proposed a Perceptron like learning algorithm to solve sequence classification i</context>
</contexts>
<marker>Bottou, 1991</marker>
<rawString>L. Bottou. 1991. Une approche th´eorique de l’apprentissage connexionniste: Applications a` la reconnaissance de la parole. Ph.D. thesis, Universit´e de Paris XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In ACL-2004.</booktitle>
<contexts>
<context position="3146" citStr="Collins and Roark, 2004" startWordPosition="495" endWordPosition="498">rder of inference. Tsuruoka and Tsujii (2005) proposed the easiest-first approach which greatly reduced the computation complexity of inference while maintaining the accuracy on labeling. However, the easiest-first approach only serves as a heuristic rule. The order of inference is not incorporated into the training of the MaxEnt classifier for individual labeling. Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task. We proposed a Perceptron like learning algorithm (Collins and Roark, 2004; Daum´e III and Marcu, 2005) for guided learning. We apply this algorithm to POS tagging, a classic sequence learning problem. Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features. By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics o</context>
<context position="21324" citStr="Collins and Roark, 2004" startWordPosition="3833" endWordPosition="3836">o that our system does not need a large beam. As shown in Section 4.2, even deterministic inference shows rather good results. Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daum´e III and Marcu, 2005). However, as far as we know, Data Set Sections Sentences Tokens Training 0-18 38,219 912,344 Develop 19-21 5,527 131,768 Test 22-24 5,462 129,654 Table 1: Data set splits the mechanism of bidirectional search with an online learning algorithm has not been investigated before. In (Daum´e III and Marcu, 2005), as well as other similar works (Collins, 2002; Collins and Roark, 2004; Shen and Joshi, 2005), only left-toright search was employed. Our guided learning algorithm provides more flexibility in search with an automatically learned order. In addition, our treatment of the score of action and the score of hypothesis is unique (see discussion in Section 2.3). Furthermore, compared to the above works, our guided learning algorithm is more aggressive on learning. In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates. In (Daum´e III and Marcu, 2005), the search is resume</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>M. Collins and B. Roark. 2004. Incremental parsing with the perceptron algorithm. In ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP-2002.</booktitle>
<contexts>
<context position="1995" citStr="Collins (2002)" startWordPosition="319" endWordPosition="320"> (2001) showed that this approach suffered from the so called label bias problem (Bottou, 1991). They proposed Conditional Random Fields (CRF) as a general solution for sequence classification. CRF models a sequence as an undirected graph, which means that all the individual tasks are solved simultaneously. Taskar et al. (2003) improved the CRF method by employing the large margin method to separate the gold standard sequence labeling from incorrect labellings. However, the complexity of quadratic programming for the large margin approach prevented it from being used in large scale NLP tasks. Collins (2002) proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order. This solution does not suffer from the label bias problem. Compared to the undirected methods, the Perceptron like algorithm is faster in training. In this paper, we will improve upon Collins’ algorithm by introducing a bidirectional searching strategy, so as to effectively utilize more context information at little extra cost. When a bidirectional strategy is used, the main problem is how to select the order of inference. Tsuruoka and Tsujii (2005) proposed the easiest-first</context>
<context position="4092" citStr="Collins, 2002" startWordPosition="651" endWordPosition="652"> search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005). The new POS tagger is similar to (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005) in the way that we employ context features. We use a bidirectional search strategy (Woods, 1976; Satta and Stock, 1994), and our algorithm is based on Perceptron learning (Collins, 2002). A unique contribution of our work is on the integration of individual classification and inference order selection, which are learned simultaneously. 2 Guided Learning for Bidirectional Labeling We first present an example of POS tagging to show the idea of bidirectional labeling. Then we present the inference algorithm and the learning algorithm. 2.1 An Example of POS tagging Suppose that we have an input sentence Agatha found that book interesting w1 w2 w3 w4 w5 (Step 0) If we scan from left to right, we may find it difficult to resolve the ambiguity of the label for that, which could be e</context>
<context position="19914" citStr="Collins, 2002" startWordPosition="3602" endWordPosition="3603">re, the order of inference and the local classification are dynamically incorporated in the learning phase. Guided learning is not as hard as reinforcement learning. At each local step in learning, we always know the undesirable labeling actions according to the gold standard, although we do not know which is the most desirable. In this approach, we can easily collect the automatically generated negative samples, and use them in learning. These negative samples are exactly those we will face during inference with the current weight vector. In our experiments, we have used Averaged Perceptron (Collins, 2002; Freund and Schapire, 1999) and Perceptron with margin (Krauth and M´ezard, 1987) to improve performance. 3 Related Works Tsuruoka and Tsujii (2005) proposed a bidirectional POS tagger, in which the order of inference is handled with the easiest-first heuristic. Gim´enez and M`arquez (2004) combined the results of a left-toright scan and a right-to-left scan. In our model, the order of inference is dynamically incorporated into the training of the local classifier. Toutanova et al. (2003) reported a POS tagger based on cyclic dependency network. In their work, the order of inference is fixed </context>
<context position="21299" citStr="Collins, 2002" startWordPosition="3831" endWordPosition="3832"> about first, so that our system does not need a large beam. As shown in Section 4.2, even deterministic inference shows rather good results. Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daum´e III and Marcu, 2005). However, as far as we know, Data Set Sections Sentences Tokens Training 0-18 38,219 912,344 Develop 19-21 5,527 131,768 Test 22-24 5,462 129,654 Table 1: Data set splits the mechanism of bidirectional search with an online learning algorithm has not been investigated before. In (Daum´e III and Marcu, 2005), as well as other similar works (Collins, 2002; Collins and Roark, 2004; Shen and Joshi, 2005), only left-toright search was employed. Our guided learning algorithm provides more flexibility in search with an automatically learned order. In addition, our treatment of the score of action and the score of hypothesis is unique (see discussion in Section 2.3). Furthermore, compared to the above works, our guided learning algorithm is more aggressive on learning. In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates. In (Daum´e III and Marcu, 20</context>
<context position="23012" citStr="Collins, 2002" startWordPosition="4115" endWordPosition="4116">andard parse. Therefore our learning algorithm is aggressive on weight update. As far as this aspect is concerned, our algorithm is similar to the MIRA algorithm in (Crammer and Singer, 2003). In MIRA, one always knows the correct hypothesis. In our case, we do not know the correct order of operations. So we use our form of weight update to implement aggressive learning. 4 Experiments on POS Tagging 4.1 Settings We apply our guided learning algorithm to POS tagging. We carry out experiments on the standard data set of the Penn Treebank (PTB) (Marcus et al., 1994). Following (Ratnaparkhi,1996; Collins, 2002; Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), 765 Feature Sets Templates Error% A Ratnaparkhi’s 3.05 B A + [t0, t1], [t0, t−1, t1], [t0, t1, t2] 2.92 C B + [t0, t−2], [t0, t2], [t0, t−2, w0], [t0, t−1, w0], [t0, t1, w0], 2.84 [t0, t2, w0], [t0, t−2, t−1, w0], [t0, t−1, t1, w0], [t0, t1, t2, w0] D C + [t0, w−1, w0], [t0, w1, w0] 2.78 E D + [t0, X = prefix or suffix of w0], 4 &lt; |X |&lt; 9 2.72 Table 2: Experiments on the development data with beam width of 3 we cut the PTB into the training, development and test sets as shown in Table 1. We use tools provided by CoNLL-2005 3 to extract POS </context>
<context position="24630" citStr="Collins, 2002" startWordPosition="4420" endWordPosition="4421">t of Features: We first run the experiments to evaluate the effect of features. We use templates to define features. For this set of experiments, we set the beam width B = 3 as a balance between speed and accuracy. The guided learning algorithm usually converges on the development data set in 4-8 iterations over the training data. Table 2 shows the error rate on the development set with different features. We first use the same feature set used in (Ratnaparkhi, 1996), which includes a set of prefix, suffix and lexical features, as well as some bi-gram and tri-gram context features. Following (Collins, 2002), we do not distinguish rare words. On set A, Ratnaparkhi’s feature set, our system reports an error rate of 3.05% on the development data set. With set B, we include a few feature templates which are symmetric to those in Ratnaparkhi’s set, but are only available with bidirectional search. With set C, we add more bi-gram and tri-gram features. With set D, we include bi-lexical features. With set E, we use prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005). We obtain 2.72% of error rate. We will use this feature set on our final experiments on th</context>
<context position="27358" citStr="Collins, 2002" startWordPosition="4871" endWordPosition="4872">-right search performs much worse, because in this case it is more likely that the gold-standard tag is not in the beam. This set of experiments show that guided learning is more preferable for tasks with higher ambiguities. In our recent work (Shen and Joshi, 2007), we have applied a variant of this algorithm to dependency parsing, and showed significant improvement over left-to-right non-aggressive learning strategy. Comparison: Table 4 shows the comparison with the previous works on the PTB test sections. 766 System Beam Error% (Ratnaparkhi, 1996) 5 3.37 (Tsuruoka and Tsujii, 2005) 1 2.90 (Collins, 2002) - 2.89 Guided Learning, feature B 3 2.85 (Tsuruoka and Tsujii, 2005) all 2.85 (Gim´enez and M`arquez, 2004) - 2.84 (Toutanova et al., 2003) - 2.76 Guided Learning, feature E 1 2.73 Guided Learning, feature E 3 2.67 Table 4: Comparison with the previous works According to the experiments shown above, we build our best system by using feature set E with beam width B = 3. The number of iterations on the training data is estimated with respect to the development data. We obtain an error rate of 2.67% on the test data. With deterministic search, or beam with B = 1, we obtain an error rate of 2.73%</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="22590" citStr="Crammer and Singer, 2003" startWordPosition="4041" endWordPosition="4044">otheses are inserted into a queue for future expansion, and the weights are updated correspondingly. However, there is no guarantee that the updated weights assign a higher score to those inserted gold standard compatible hypotheses. In our algorithm, the gold standard compatible hypotheses are used for weight update only. As a result, after each sentence is processed, the weight vector can usually successfully predict the gold standard parse. Therefore our learning algorithm is aggressive on weight update. As far as this aspect is concerned, our algorithm is similar to the MIRA algorithm in (Crammer and Singer, 2003). In MIRA, one always knows the correct hypothesis. In our case, we do not know the correct order of operations. So we use our form of weight update to implement aggressive learning. 4 Experiments on POS Tagging 4.1 Settings We apply our guided learning algorithm to POS tagging. We carry out experiments on the standard data set of the Penn Treebank (PTB) (Marcus et al., 1994). Following (Ratnaparkhi,1996; Collins, 2002; Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), 765 Feature Sets Templates Error% A Ratnaparkhi’s 3.05 B A + [t0, t1], [t0, t−1, t1], [t0, t1, t2] 2.92 C B + [t0, t−2], [t0</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>Learning as search optimization: Approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In ICML-2005.</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>H. Daum´e III and D. Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In ICML-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="19942" citStr="Freund and Schapire, 1999" startWordPosition="3604" endWordPosition="3607">f inference and the local classification are dynamically incorporated in the learning phase. Guided learning is not as hard as reinforcement learning. At each local step in learning, we always know the undesirable labeling actions according to the gold standard, although we do not know which is the most desirable. In this approach, we can easily collect the automatically generated negative samples, and use them in learning. These negative samples are exactly those we will face during inference with the current weight vector. In our experiments, we have used Averaged Perceptron (Collins, 2002; Freund and Schapire, 1999) and Perceptron with margin (Krauth and M´ezard, 1987) to improve performance. 3 Related Works Tsuruoka and Tsujii (2005) proposed a bidirectional POS tagger, in which the order of inference is handled with the easiest-first heuristic. Gim´enez and M`arquez (2004) combined the results of a left-toright scan and a right-to-left scan. In our model, the order of inference is dynamically incorporated into the training of the local classifier. Toutanova et al. (2003) reported a POS tagger based on cyclic dependency network. In their work, the order of inference is fixed as from left to right. In th</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gim´enez</author>
<author>L M`arquez</author>
</authors>
<title>Svmtool: A general pos tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In LREC2004.</booktitle>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>J. Gim´enez and L. M`arquez. 2004. Svmtool: A general pos tagger generator based on support vector machines. In LREC2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Krauth</author>
<author>M M´ezard</author>
</authors>
<title>Learning algorithms with optimal stability in neural networks.</title>
<date>1987</date>
<journal>Journal of Physics A,</journal>
<pages>20--745</pages>
<marker>Krauth, M´ezard, 1987</marker>
<rawString>W. Krauth and M. M´ezard. 1987. Learning algorithms with optimal stability in neural networks. Journal of Physics A, 20:745–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmentation and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML-2001.</booktitle>
<contexts>
<context position="1388" citStr="Lafferty et al. (2001)" startWordPosition="220" endWordPosition="223">reduction over the previous best result on the same data set, while using fewer features. 1 Introduction Many NLP tasks can be modeled as a sequence classification problem, such as POS tagging, chunking, and incremental parsing. A traditional method to solve this problem is to decompose the whole task into a set of individual tasks for each token in the input sequence, and solve these small tasks in a fixed order, usually from left to right. In this way, the output of the previous small tasks can be used as the input of the later tasks. HMM and MaxEnt Markov Model are examples of this method. Lafferty et al. (2001) showed that this approach suffered from the so called label bias problem (Bottou, 1991). They proposed Conditional Random Fields (CRF) as a general solution for sequence classification. CRF models a sequence as an undirected graph, which means that all the individual tasks are solved simultaneously. Taskar et al. (2003) improved the CRF method by employing the large margin method to separate the gold standard sequence labeling from incorrect labellings. However, the complexity of quadratic programming for the large margin approach prevented it from being used in large scale NLP tasks. Collins</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmentation and labeling sequence data. In ICML-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="22968" citStr="Marcus et al., 1994" startWordPosition="4109" endWordPosition="4112">vector can usually successfully predict the gold standard parse. Therefore our learning algorithm is aggressive on weight update. As far as this aspect is concerned, our algorithm is similar to the MIRA algorithm in (Crammer and Singer, 2003). In MIRA, one always knows the correct hypothesis. In our case, we do not know the correct order of operations. So we use our form of weight update to implement aggressive learning. 4 Experiments on POS Tagging 4.1 Settings We apply our guided learning algorithm to POS tagging. We carry out experiments on the standard data set of the Penn Treebank (PTB) (Marcus et al., 1994). Following (Ratnaparkhi,1996; Collins, 2002; Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), 765 Feature Sets Templates Error% A Ratnaparkhi’s 3.05 B A + [t0, t1], [t0, t−1, t1], [t0, t1, t2] 2.92 C B + [t0, t−2], [t0, t2], [t0, t−2, w0], [t0, t−1, w0], [t0, t1, w0], 2.84 [t0, t2, w0], [t0, t−2, t−1, w0], [t0, t−1, t1, w0], [t0, t1, t2, w0] D C + [t0, w−1, w0], [t0, w1, w0] 2.78 E D + [t0, X = prefix or suffix of w0], 4 &lt; |X |&lt; 9 2.72 Table 2: Experiments on the development data with beam width of 3 we cut the PTB into the training, development and test sets as shown in Table 1. We use to</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In EMNLP-1996.</booktitle>
<contexts>
<context position="23906" citStr="Ratnaparkhi, 1996" startWordPosition="4294" endWordPosition="4296">, t1, w0], [t0, t1, t2, w0] D C + [t0, w−1, w0], [t0, w1, w0] 2.78 E D + [t0, X = prefix or suffix of w0], 4 &lt; |X |&lt; 9 2.72 Table 2: Experiments on the development data with beam width of 3 we cut the PTB into the training, development and test sets as shown in Table 1. We use tools provided by CoNLL-2005 3 to extract POS tags from the mrg files of PTB. So the data set is the same as previous work. We use the development set to select features and estimate the number of iterations in training. In our experiments, we enumerate all the POS tags for each word instead of using a dictionary as in (Ratnaparkhi, 1996), since the size of the tag set is tractable and our learning algorithm is efficient enough. 4.2 Results Effect of Features: We first run the experiments to evaluate the effect of features. We use templates to define features. For this set of experiments, we set the beam width B = 3 as a balance between speed and accuracy. The guided learning algorithm usually converges on the development data set in 4-8 iterations over the training data. Table 2 shows the error rate on the development set with different features. We first use the same feature set used in (Ratnaparkhi, 1996), which includes a </context>
<context position="27300" citStr="Ratnaparkhi, 1996" startWordPosition="4861" endWordPosition="4862">ver, with beam width = 1, non-aggressive learning over left-to-right search performs much worse, because in this case it is more likely that the gold-standard tag is not in the beam. This set of experiments show that guided learning is more preferable for tasks with higher ambiguities. In our recent work (Shen and Joshi, 2007), we have applied a variant of this algorithm to dependency parsing, and showed significant improvement over left-to-right non-aggressive learning strategy. Comparison: Table 4 shows the comparison with the previous works on the PTB test sections. 766 System Beam Error% (Ratnaparkhi, 1996) 5 3.37 (Tsuruoka and Tsujii, 2005) 1 2.90 (Collins, 2002) - 2.89 Guided Learning, feature B 3 2.85 (Tsuruoka and Tsujii, 2005) all 2.85 (Gim´enez and M`arquez, 2004) - 2.84 (Toutanova et al., 2003) - 2.76 Guided Learning, feature E 1 2.73 Guided Learning, feature E 3 2.67 Table 4: Comparison with the previous works According to the experiments shown above, we build our best system by using feature set E with beam width B = 3. The number of iterations on the training data is estimated with respect to the development data. We obtain an error rate of 2.67% on the test data. With deterministic se</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tagger. In EMNLP-1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Satta</author>
<author>O Stock</author>
</authors>
<title>Bi-Directional Context-Free Grammar Parsing for Natural Language Processing.</title>
<date>1994</date>
<journal>ArtificialIntelligence,</journal>
<pages>69--1</pages>
<contexts>
<context position="4025" citStr="Satta and Stock, 1994" startWordPosition="638" endWordPosition="641">em (Toutanova et al., 2003) by using fewer features. By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005). The new POS tagger is similar to (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005) in the way that we employ context features. We use a bidirectional search strategy (Woods, 1976; Satta and Stock, 1994), and our algorithm is based on Perceptron learning (Collins, 2002). A unique contribution of our work is on the integration of individual classification and inference order selection, which are learned simultaneously. 2 Guided Learning for Bidirectional Labeling We first present an example of POS tagging to show the idea of bidirectional labeling. Then we present the inference algorithm and the learning algorithm. 2.1 An Example of POS tagging Suppose that we have an input sentence Agatha found that book interesting w1 w2 w3 w4 w5 (Step 0) If we scan from left to right, we may find it difficu</context>
</contexts>
<marker>Satta, Stock, 1994</marker>
<rawString>G. Satta and O. Stock. 1994. Bi-Directional Context-Free Grammar Parsing for Natural Language Processing. ArtificialIntelligence, 69(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Joshi</author>
</authors>
<title>Incremental LTAG Parsing.</title>
<date>2005</date>
<booktitle>In EMNLP-2005.</booktitle>
<contexts>
<context position="21347" citStr="Shen and Joshi, 2005" startWordPosition="3837" endWordPosition="3840">t need a large beam. As shown in Section 4.2, even deterministic inference shows rather good results. Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daum´e III and Marcu, 2005). However, as far as we know, Data Set Sections Sentences Tokens Training 0-18 38,219 912,344 Develop 19-21 5,527 131,768 Test 22-24 5,462 129,654 Table 1: Data set splits the mechanism of bidirectional search with an online learning algorithm has not been investigated before. In (Daum´e III and Marcu, 2005), as well as other similar works (Collins, 2002; Collins and Roark, 2004; Shen and Joshi, 2005), only left-toright search was employed. Our guided learning algorithm provides more flexibility in search with an automatically learned order. In addition, our treatment of the score of action and the score of hypothesis is unique (see discussion in Section 2.3). Furthermore, compared to the above works, our guided learning algorithm is more aggressive on learning. In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates. In (Daum´e III and Marcu, 2005), the search is resumed after some gold stand</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>L. Shen and A. K. Joshi. 2005. Incremental LTAG Parsing. In EMNLP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Joshi</author>
</authors>
<title>Bidirectional LTAG Dependency Parsing.</title>
<date>2007</date>
<tech>Technical Report 07-02, IRCS, UPenn.</tech>
<contexts>
<context position="27010" citStr="Shen and Joshi, 2007" startWordPosition="4817" endWordPosition="4820">tagging is very high. As a result, we can always keep the gold-standard tags in the beam even with left-to-right search in training. This can also explain why the performance of leftto-right search with non-aggressive learning is close to bidirectional search if the beam is large enough. However, with beam width = 1, non-aggressive learning over left-to-right search performs much worse, because in this case it is more likely that the gold-standard tag is not in the beam. This set of experiments show that guided learning is more preferable for tasks with higher ambiguities. In our recent work (Shen and Joshi, 2007), we have applied a variant of this algorithm to dependency parsing, and showed significant improvement over left-to-right non-aggressive learning strategy. Comparison: Table 4 shows the comparison with the previous works on the PTB test sections. 766 System Beam Error% (Ratnaparkhi, 1996) 5 3.37 (Tsuruoka and Tsujii, 2005) 1 2.90 (Collins, 2002) - 2.89 Guided Learning, feature B 3 2.85 (Tsuruoka and Tsujii, 2005) all 2.85 (Gim´enez and M`arquez, 2004) - 2.84 (Toutanova et al., 2003) - 2.76 Guided Learning, feature E 1 2.73 Guided Learning, feature E 3 2.67 Table 4: Comparison with the previou</context>
</contexts>
<marker>Shen, Joshi, 2007</marker>
<rawString>L. Shen and A. K. Joshi. 2007. Bidirectional LTAG Dependency Parsing. Technical Report 07-02, IRCS, UPenn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin markov networks.</title>
<date>2003</date>
<booktitle>In NIPS-2003.</booktitle>
<contexts>
<context position="1710" citStr="Taskar et al. (2003)" startWordPosition="271" endWordPosition="274">ual tasks for each token in the input sequence, and solve these small tasks in a fixed order, usually from left to right. In this way, the output of the previous small tasks can be used as the input of the later tasks. HMM and MaxEnt Markov Model are examples of this method. Lafferty et al. (2001) showed that this approach suffered from the so called label bias problem (Bottou, 1991). They proposed Conditional Random Fields (CRF) as a general solution for sequence classification. CRF models a sequence as an undirected graph, which means that all the individual tasks are solved simultaneously. Taskar et al. (2003) improved the CRF method by employing the large margin method to separate the gold standard sequence labeling from incorrect labellings. However, the complexity of quadratic programming for the large margin approach prevented it from being used in large scale NLP tasks. Collins (2002) proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order. This solution does not suffer from the label bias problem. Compared to the undirected methods, the Perceptron like algorithm is faster in training. In this paper, we will improve upon Collins’ al</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin markov networks. In NIPS-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In NAACL-2003.</booktitle>
<contexts>
<context position="3430" citStr="Toutanova et al., 2003" startWordPosition="545" endWordPosition="548">ot incorporated into the training of the MaxEnt classifier for individual labeling. Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task. We proposed a Perceptron like learning algorithm (Collins and Roark, 2004; Daum´e III and Marcu, 2005) for guided learning. We apply this algorithm to POS tagging, a classic sequence learning problem. Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features. By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005). The new POS tagger is similar to (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005) in the way that we employ context features. We use a bidirectional search strategy (Woods, 1976; Satta and Stock, 1994), and</context>
<context position="20408" citStr="Toutanova et al. (2003)" startWordPosition="3677" endWordPosition="3680">we will face during inference with the current weight vector. In our experiments, we have used Averaged Perceptron (Collins, 2002; Freund and Schapire, 1999) and Perceptron with margin (Krauth and M´ezard, 1987) to improve performance. 3 Related Works Tsuruoka and Tsujii (2005) proposed a bidirectional POS tagger, in which the order of inference is handled with the easiest-first heuristic. Gim´enez and M`arquez (2004) combined the results of a left-toright scan and a right-to-left scan. In our model, the order of inference is dynamically incorporated into the training of the local classifier. Toutanova et al. (2003) reported a POS tagger based on cyclic dependency network. In their work, the order of inference is fixed as from left to right. In this approach, large beam width is required to maintain the ambiguous hypotheses. In our approach, we can handle tokens that we are most confident about first, so that our system does not need a large beam. As shown in Section 4.2, even deterministic inference shows rather good results. Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daum´e III and Marcu, 2005). However, as far as we know, Data Set Sections Sentences Tokens </context>
<context position="23036" citStr="Toutanova et al., 2003" startWordPosition="4117" endWordPosition="4120">herefore our learning algorithm is aggressive on weight update. As far as this aspect is concerned, our algorithm is similar to the MIRA algorithm in (Crammer and Singer, 2003). In MIRA, one always knows the correct hypothesis. In our case, we do not know the correct order of operations. So we use our form of weight update to implement aggressive learning. 4 Experiments on POS Tagging 4.1 Settings We apply our guided learning algorithm to POS tagging. We carry out experiments on the standard data set of the Penn Treebank (PTB) (Marcus et al., 1994). Following (Ratnaparkhi,1996; Collins, 2002; Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), 765 Feature Sets Templates Error% A Ratnaparkhi’s 3.05 B A + [t0, t1], [t0, t−1, t1], [t0, t1, t2] 2.92 C B + [t0, t−2], [t0, t2], [t0, t−2, w0], [t0, t−1, w0], [t0, t1, w0], 2.84 [t0, t2, w0], [t0, t−2, t−1, w0], [t0, t−1, t1, w0], [t0, t1, t2, w0] D C + [t0, w−1, w0], [t0, w1, w0] 2.78 E D + [t0, X = prefix or suffix of w0], 4 &lt; |X |&lt; 9 2.72 Table 2: Experiments on the development data with beam width of 3 we cut the PTB into the training, development and test sets as shown in Table 1. We use tools provided by CoNLL-2005 3 to extract POS tags from the mrg files </context>
<context position="25110" citStr="Toutanova et al., 2003" startWordPosition="4505" endWordPosition="4508"> which includes a set of prefix, suffix and lexical features, as well as some bi-gram and tri-gram context features. Following (Collins, 2002), we do not distinguish rare words. On set A, Ratnaparkhi’s feature set, our system reports an error rate of 3.05% on the development data set. With set B, we include a few feature templates which are symmetric to those in Ratnaparkhi’s set, but are only available with bidirectional search. With set C, we add more bi-gram and tri-gram features. With set D, we include bi-lexical features. With set E, we use prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005). We obtain 2.72% of error rate. We will use this feature set on our final experiments on the test data. Effect of Search and Learning Strategies: For the second set of experiments, we evaluate the effect of 3http://www.lsi.upc.es/˜srlconll/soft.html, package srlconll1.1.tgz. Search Aggressive? Beam=1 Beam=3 L-to-R Yes 2.94 2.82 L-to-R No 3.24 2.75 Bi-Dir Yes 2.84 2.72 Bi-Dir No does not converge Table 3: Experiments on the development data search methods, learning strategies, and beam width. We use feature set E for this set of experiments. Table 3 shows the error </context>
<context position="27498" citStr="Toutanova et al., 2003" startWordPosition="4893" endWordPosition="4896">f experiments show that guided learning is more preferable for tasks with higher ambiguities. In our recent work (Shen and Joshi, 2007), we have applied a variant of this algorithm to dependency parsing, and showed significant improvement over left-to-right non-aggressive learning strategy. Comparison: Table 4 shows the comparison with the previous works on the PTB test sections. 766 System Beam Error% (Ratnaparkhi, 1996) 5 3.37 (Tsuruoka and Tsujii, 2005) 1 2.90 (Collins, 2002) - 2.89 Guided Learning, feature B 3 2.85 (Tsuruoka and Tsujii, 2005) all 2.85 (Gim´enez and M`arquez, 2004) - 2.84 (Toutanova et al., 2003) - 2.76 Guided Learning, feature E 1 2.73 Guided Learning, feature E 3 2.67 Table 4: Comparison with the previous works According to the experiments shown above, we build our best system by using feature set E with beam width B = 3. The number of iterations on the training data is estimated with respect to the development data. We obtain an error rate of 2.67% on the test data. With deterministic search, or beam with B = 1, we obtain an error rate of 2.73%. Compared to previous best result on the same data set, 2.76% by (Toutanova et al., 2003), our best result shows a relative error reduction</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In EMNLP-2005.</booktitle>
<contexts>
<context position="2568" citStr="Tsuruoka and Tsujii (2005)" startWordPosition="405" endWordPosition="409">m being used in large scale NLP tasks. Collins (2002) proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order. This solution does not suffer from the label bias problem. Compared to the undirected methods, the Perceptron like algorithm is faster in training. In this paper, we will improve upon Collins’ algorithm by introducing a bidirectional searching strategy, so as to effectively utilize more context information at little extra cost. When a bidirectional strategy is used, the main problem is how to select the order of inference. Tsuruoka and Tsujii (2005) proposed the easiest-first approach which greatly reduced the computation complexity of inference while maintaining the accuracy on labeling. However, the easiest-first approach only serves as a heuristic rule. The order of inference is not incorporated into the training of the MaxEnt classifier for individual labeling. Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task. We proposed a Perceptron like learning algorithm (Collins and Roark, 2004; Daum´e III and Marcu</context>
<context position="3819" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="601" endWordPosition="605">ng. We apply this algorithm to POS tagging, a classic sequence learning problem. Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features. By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005). The new POS tagger is similar to (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005) in the way that we employ context features. We use a bidirectional search strategy (Woods, 1976; Satta and Stock, 1994), and our algorithm is based on Perceptron learning (Collins, 2002). A unique contribution of our work is on the integration of individual classification and inference order selection, which are learned simultaneously. 2 Guided Learning for Bidirectional Labeling We first present an example of POS tagging to show the idea of bidirectional labeling. Then we present the inference algorithm and</context>
<context position="20063" citStr="Tsuruoka and Tsujii (2005)" startWordPosition="3622" endWordPosition="3625">rd as reinforcement learning. At each local step in learning, we always know the undesirable labeling actions according to the gold standard, although we do not know which is the most desirable. In this approach, we can easily collect the automatically generated negative samples, and use them in learning. These negative samples are exactly those we will face during inference with the current weight vector. In our experiments, we have used Averaged Perceptron (Collins, 2002; Freund and Schapire, 1999) and Perceptron with margin (Krauth and M´ezard, 1987) to improve performance. 3 Related Works Tsuruoka and Tsujii (2005) proposed a bidirectional POS tagger, in which the order of inference is handled with the easiest-first heuristic. Gim´enez and M`arquez (2004) combined the results of a left-toright scan and a right-to-left scan. In our model, the order of inference is dynamically incorporated into the training of the local classifier. Toutanova et al. (2003) reported a POS tagger based on cyclic dependency network. In their work, the order of inference is fixed as from left to right. In this approach, large beam width is required to maintain the ambiguous hypotheses. In our approach, we can handle tokens tha</context>
<context position="23064" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="4121" endWordPosition="4124">gorithm is aggressive on weight update. As far as this aspect is concerned, our algorithm is similar to the MIRA algorithm in (Crammer and Singer, 2003). In MIRA, one always knows the correct hypothesis. In our case, we do not know the correct order of operations. So we use our form of weight update to implement aggressive learning. 4 Experiments on POS Tagging 4.1 Settings We apply our guided learning algorithm to POS tagging. We carry out experiments on the standard data set of the Penn Treebank (PTB) (Marcus et al., 1994). Following (Ratnaparkhi,1996; Collins, 2002; Toutanova et al., 2003; Tsuruoka and Tsujii, 2005), 765 Feature Sets Templates Error% A Ratnaparkhi’s 3.05 B A + [t0, t1], [t0, t−1, t1], [t0, t1, t2] 2.92 C B + [t0, t−2], [t0, t2], [t0, t−2, w0], [t0, t−1, w0], [t0, t1, w0], 2.84 [t0, t2, w0], [t0, t−2, t−1, w0], [t0, t−1, t1, w0], [t0, t1, t2, w0] D C + [t0, w−1, w0], [t0, w1, w0] 2.78 E D + [t0, X = prefix or suffix of w0], 4 &lt; |X |&lt; 9 2.72 Table 2: Experiments on the development data with beam width of 3 we cut the PTB into the training, development and test sets as shown in Table 1. We use tools provided by CoNLL-2005 3 to extract POS tags from the mrg files of PTB. So the data set is t</context>
<context position="25138" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="4509" endWordPosition="4512"> prefix, suffix and lexical features, as well as some bi-gram and tri-gram context features. Following (Collins, 2002), we do not distinguish rare words. On set A, Ratnaparkhi’s feature set, our system reports an error rate of 3.05% on the development data set. With set B, we include a few feature templates which are symmetric to those in Ratnaparkhi’s set, but are only available with bidirectional search. With set C, we add more bi-gram and tri-gram features. With set D, we include bi-lexical features. With set E, we use prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005). We obtain 2.72% of error rate. We will use this feature set on our final experiments on the test data. Effect of Search and Learning Strategies: For the second set of experiments, we evaluate the effect of 3http://www.lsi.upc.es/˜srlconll/soft.html, package srlconll1.1.tgz. Search Aggressive? Beam=1 Beam=3 L-to-R Yes 2.94 2.82 L-to-R No 3.24 2.75 Bi-Dir Yes 2.84 2.72 Bi-Dir No does not converge Table 3: Experiments on the development data search methods, learning strategies, and beam width. We use feature set E for this set of experiments. Table 3 shows the error rates on the development dat</context>
<context position="27335" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="4865" endWordPosition="4868">on-aggressive learning over left-to-right search performs much worse, because in this case it is more likely that the gold-standard tag is not in the beam. This set of experiments show that guided learning is more preferable for tasks with higher ambiguities. In our recent work (Shen and Joshi, 2007), we have applied a variant of this algorithm to dependency parsing, and showed significant improvement over left-to-right non-aggressive learning strategy. Comparison: Table 4 shows the comparison with the previous works on the PTB test sections. 766 System Beam Error% (Ratnaparkhi, 1996) 5 3.37 (Tsuruoka and Tsujii, 2005) 1 2.90 (Collins, 2002) - 2.89 Guided Learning, feature B 3 2.85 (Tsuruoka and Tsujii, 2005) all 2.85 (Gim´enez and M`arquez, 2004) - 2.84 (Toutanova et al., 2003) - 2.76 Guided Learning, feature E 1 2.73 Guided Learning, feature E 3 2.67 Table 4: Comparison with the previous works According to the experiments shown above, we build our best system by using feature set E with beam width B = 3. The number of iterations on the training data is estimated with respect to the development data. We obtain an error rate of 2.67% on the test data. With deterministic search, or beam with B = 1, we obtain</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Y. Tsuruoka and J. Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In EMNLP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Widrow</author>
<author>M E Hoff</author>
</authors>
<title>Adaptive switching circuits.</title>
<date>1960</date>
<journal>IRE WESCON Convention Record, part</journal>
<volume>4</volume>
<contexts>
<context position="18883" citStr="Widrow and Hoff, 1960" startWordPosition="3432" endWordPosition="3435">e scores of the context hypotheses of a gold standard hypothesis must be no less than those of other hypotheses of the same span. This could be shown recursively with respect to Equation 1, because the context hypotheses of a gold standard hypothesis are also compatible with the gold standard. Furthermore, if we take (xi = f(p&apos;.G.A) − f(p&apos;.S.T.A), yi = +1) as a positive sample, and (xj = f(p&apos;.S.T.A) − f(p&apos;.G.A), yj = −1) as a negative sample, the weight updates at lines 13 and 14 are a stochastic approximation of gradient descent that minimizes the squared errors of the misclassified samples (Widrow and Hoff, 1960). What is special with our learning algorithm is the strategy used to select samples for training. In general, this novel learning framework lies between supervised learning and reinforcement learning. Guided learning is more difficult than supervised learning, because we do not know the order of inference. The order is learned automatically, and partial output is in turn used to train the local classifier. Therefore, the order of inference and the local classification are dynamically incorporated in the learning phase. Guided learning is not as hard as reinforcement learning. At each local st</context>
</contexts>
<marker>Widrow, Hoff, 1960</marker>
<rawString>B. Widrow and M. E. Hoff. 1960. Adaptive switching circuits. IRE WESCON Convention Record, part 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>Parsers in speech understanding systems.</title>
<date>1976</date>
<tech>Technical Report 3438,</tech>
<volume>4</volume>
<pages>1--21</pages>
<publisher>BBN Inc.</publisher>
<contexts>
<context position="4001" citStr="Woods, 1976" startWordPosition="636" endWordPosition="637">ous best system (Toutanova et al., 2003) by using fewer features. By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005). The new POS tagger is similar to (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005) in the way that we employ context features. We use a bidirectional search strategy (Woods, 1976; Satta and Stock, 1994), and our algorithm is based on Perceptron learning (Collins, 2002). A unique contribution of our work is on the integration of individual classification and inference order selection, which are learned simultaneously. 2 Guided Learning for Bidirectional Labeling We first present an example of POS tagging to show the idea of bidirectional labeling. Then we present the inference algorithm and the learning algorithm. 2.1 An Example of POS tagging Suppose that we have an input sentence Agatha found that book interesting w1 w2 w3 w4 w5 (Step 0) If we scan from left to right</context>
</contexts>
<marker>Woods, 1976</marker>
<rawString>W. Woods. 1976. Parsers in speech understanding systems. Technical Report 3438, Vol. 4, 1–21, BBN Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>