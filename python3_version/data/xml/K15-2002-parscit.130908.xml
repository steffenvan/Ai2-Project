<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000301">
<title confidence="0.997772">
A Refined End-to-End Discourse Parser
</title>
<author confidence="0.999718">
Jianxiang Wang, Man Lan*
</author>
<affiliation confidence="0.982817333333333">
Shanghai Key Laboratory of Multidimensional Information Processing
Department of Computer Science and Technology,
East China Normal University, Shanghai 200241, P. R. China
</affiliation>
<email confidence="0.996624">
51141201062@ecnu.cn, mlan@cs.ecnu.edu.cn*
</email>
<sectionHeader confidence="0.995586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9983363125">
The CoNLL-2015 shared task focuses on shal-
low discourse parsing, which takes a piece of
newswire text as input and returns the dis-
course relations in a PDTB style. In this paper,
we describe our discourse parser that partici-
pated in the shared task. We use 9 components
to construct the whole parser to identify dis-
course connectives, label arguments and clas-
sify the sense of Explicit or Non-Explicit re-
lations in free texts. Compared to previous
discourse parser, new components and fea-
tures are added in our system, which further
improves the overall performance of the dis-
course parser. Our parser ranks the first on
two test datasets, i.e., PDTB Section 23 and
a blind test dataset.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944">
An end-to-end discourse parser is given free texts
as input and returns discourse relations in a PDTB
style, where a connective acts as a predicate that
takes two text spans as its arguments. It can ben-
efit many downstream NLP applications, such as
information retrieval, question answering and auto-
matic summarization, etc. The extraction of exact
argument spans and Non-Explicit sense identifica-
tion have been shown to be the main challenges of
the discourse parsing (Lin et al., 2014).
Since the release of Penn Discourse Treebank
(PDTB) (Prasad et al., 2008), much research has
been carried out on PDTB to perform the subtasks
of a full end-to-end parser, such as identifying dis-
course connectives, labeling arguments and classi-
fying Explicit or Implicit relations. To identify dis-
course connectives from non-discourse ones and to
classify the Explicit relations, (Pitler and Nenkova,
2009) extracted syntactic features of connectives
from the constituent parses, and showed that syn-
tactic features improved performance in both sub-
tasks. For the argument labeling subtask, (Ghosh et
al., 2011) regarded it as a token-level sequence la-
beling task using conditional random fields (CRFs).
(Lin et al., 2014) proposed a tree subtraction algo-
rithm to extract the arguments. (Kong et al., 2014)
adopted a constituent-based approach to label argu-
ments. As for Implicit sense classification, (Pitler
et al., 2009), (Lin et al., 2009) and (Rutherford and
Xue, 2014) performed the classification using sev-
eral linguistically-informed features, such as verb
classes, production rules and Brown cluster pair.
(Lan et al., 2013) presented a multi-task learning
framework with the use of the prediction of explicit
discourse connective as auxiliary learning tasks to
improve the performance.
All of these research focus on the subtasks of the
PDTB, and can be viewed as isolated components
of a full parser. (Lin et al., 2014) constructed a full
parser on the top of these subtasks, which contained
multiple components joined in a sequential pipeline
architecture including a connective classifier, argu-
ment labeler, explicit classifier, non-explicit classi-
fier, and attribution span labeler. In this paper, we
followed the framework of (Lin et al., 2014) to con-
struct a discourse parser. However, our work differs
from that of Lin’s in that our system introduces new
components and features to improve the overall per-
formance. Specifically, (1) we build two different
</bodyText>
<page confidence="0.991917">
17
</page>
<note confidence="0.989686">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 17–24,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998462444444445">
extractors for Arg1 and Arg2 respectively for label-
ing Explicit arguments in the case of PS (i.e., Arg1
is located in some previous sentences of the connec-
tive); (2) we add new features to capture more infor-
mation for classification or recognition; (3) we build
two different argument extractors for Non-EntRel
relations in Non-Explicit; (4) we use the refined ar-
guments to improve the Non-Explicit sense classifi-
cation.
The organization of this work is as follows. Sec-
tion 2 gives a sketch description of our parser in a
flow chart and the function of every component in
this architecture. Section 3 describes the compo-
nents and features in detail. Section 4 reports the
preliminary experimental results on the training and
development dataset, and the final results on two test
datasets are shown in Section 5. Section 6 concludes
this work.
</bodyText>
<sectionHeader confidence="0.952386" genericHeader="method">
2 System Overview
</sectionHeader>
<figureCaption confidence="0.996628">
Figure 1: System pipeline for the discourse parser
</figureCaption>
<bodyText confidence="0.952694">
We design the discourse parser as a sequential
pipeline, shown in Figure 1, and the 9 components
of our parser are listed as follows.
First, for texts with Explicit connective words:
</bodyText>
<listItem confidence="0.9982385">
(1) Connective Classifier is to identify the dis-
course connectives from non-discourse ones.
(2) Arg1 Position Classifier is to decide the rel-
ative position of Arg1 – whether it is located within
the same sentence as the connective (SS) or in some
previous sentence of the connective (PS).
(3) SS Arguments Extractor is to extract the
spans of Arg1 and Arg2 in the SS case.
</listItem>
<bodyText confidence="0.939402">
In the PS case, we build two extractors to identify
the text spans for PS Arg1 and PS Arg2 respectively.
</bodyText>
<listItem confidence="0.7976524">
(4) PS Arg1 Extractor is to extract Arg1 for PS.
(5) PS Arg2 Extractor is to extract Arg2 for PS.
(6) Explicit Sense Classifier is to identify the
sense that this Explicit connective conveys.
Second, for all adjacent sentence pairs within
each paragraph, but not identified in any Explicit re-
lation:
(7) Non-Explicit Sense Classifier is to classify
the sense of each sentence pair into one of the Non-
Explicit relation senses.
</listItem>
<bodyText confidence="0.9486132">
Since attribution is not annotated for EntRel rela-
tions, if the output of the above Non-Explict sense
classifier is EntRel, we regard the previous sentence
as Arg1 and the next one as Arg2. Otherwise, we
build the following two argument extractors to label
</bodyText>
<listItem confidence="0.417912">
Arg1 and Arg2.
(8) Implicit Arg1 Extractor and (9) Implicit
</listItem>
<bodyText confidence="0.2980025">
Arg2 Extractor extract Arg1 and Arg2 for Non-
EntRel relations in Non-explicit, respectively.
</bodyText>
<sectionHeader confidence="0.942811" genericHeader="method">
3 Components and Features
</sectionHeader>
<bodyText confidence="0.983348">
Generally, our parser consists of 9 components,
which compose an Explicit parser and a Non-
Explicit parser. Most of features used in our parser
are borrowed from previous work (Kong et al., 2014;
Lin et al., 2014; Pitler et al., 2009; Pitler and
Nenkova, 2009; Rutherford and Xue, 2014).
</bodyText>
<subsectionHeader confidence="0.993906">
3.1 Explicit Parser
3.1.1 Connective Classifier
</subsectionHeader>
<bodyText confidence="0.999922066666667">
Since the input of the discourse parser is free text,
the first thing we need to do is to identify all con-
nective occurrences in text, and then to use the con-
nective classifier to decide whether they function as
discourse connectives or not.
For each connective occurrence C, we extract fea-
tures from its context, part-of-speech (POS) and the
parse tree of the connective’s sentence. Note that
previ and nexti indicate the first previous word and
the first next word of connective C respectively. For
a node in the parse tree, we use the POS combina-
tions of the node, its parent, its children to represent
the linked context.
The features we used for connective classsifi-
cation consist of the following: (1) Pitler’s: C
</bodyText>
<figure confidence="0.999464111111111">
0
1/2! 3#-(&apos;(#$
&amp;quot;+,--(.(%/
&gt;
?#$@89=+(&amp;(&apos;
5%$-%
&amp;quot;+,--(.(%/
!
&amp;quot;#$$%&amp;&apos;()%
&amp;quot;+,--(.(%/
55
35
C
B7=+(&amp;(&apos; 1/2!
89&apos;/,&amp;&apos;#/
A
B7=+(&amp;(&apos; 1/20
89&apos;/,&amp;&apos;#/
4
55 1/267%$&apos;-
89&apos;/,&amp;&apos;#/
:
35 1/2!
89&apos;/,&amp;&apos;#/
;
35 1/20
89&apos;/,&amp;&apos;#/
?#&apos; 8$&apos;D%+
8$&apos;D%+
&lt;
89=+(&amp;(&apos; 5%$-%
&amp;quot;+,--(.(%/
&gt;
?#$@89=+(&amp;(&apos;
5%$-%
&amp;quot;+,--(.(%/
</figure>
<page confidence="0.991674">
18
</page>
<bodyText confidence="0.999984142857143">
string (case-sensitive), self-category (the highest
node in the parse tree that covers only the connec-
tive words), parent-category (the parent of the self-
category), left-sibling-category (the left sibling of
the self-category), right-sibling-category (the right
sibling of the self-category), C-Syn interaction (the
pairwise interaction features between the connec-
tive C and each category feature (i.e., self-category,
parent-category, left-sibling-category, right-sibling-
category)) , Syn-Syn interaction (the interaction fea-
tures between pairs of category features); (2) Lin’s:
C POS, prev1 + C string, prev1 POS, prev1 POS
+ C POS, C string + next1, next1 POS, C POS +
next1 POS, path of C’s parent → root, compressed
path of C’s parent → root; (3) our new proposed fea-
tures: the POS tags of nodes from C’s parent →
root, parent-category linked context, right-sibling-
category linked context. Our three new features are
considered to capture more syntactic context infor-
mation of the connective C for connective classifica-
tion.
</bodyText>
<subsectionHeader confidence="0.800336">
3.1.2 Arg1 Position Classifier
</subsectionHeader>
<bodyText confidence="0.999885608695652">
After identifying the discourse connectives from
the texts, we come to locate the positions of Arg1
and Arg2 of the connective C. Since Arg2 is defined
as the argument with which the connective is syn-
tactically associated, its position is fixed once we lo-
cate the discourse connective C. So we only need to
identify the relative position of Arg1 as whether it is
located within the same sentence as the connective
(SS) or in some previous sentences of the connec-
tive (PS). We do not identify the case which Arg2
is located in some sentences following the sentence
containing the connective (FS), because the statisti-
cal distribution of (Prasad et al., 2008) shows that
less than 0.1% are FS for Explicit relations.
The features consist of the following: (1) Lin’s:
C string, C position (the position of connective C in
the sentence: start, middle, or end), C POS, prev1,
prev1 POS, prev1 + C, prev1 POS + C POS, prev2,
prev2 POS, prev2 + C, prev2 POS + C POS; (2)
our newly-proposed features: C POS + next1 POS,
next2, path of C → root. Note that prev2 and next2
indicate the second previous word and the second
next word of connective C, respectively.
</bodyText>
<subsectionHeader confidence="0.988078">
3.1.3 Argument Extractor
</subsectionHeader>
<bodyText confidence="0.999572739130435">
After the relative position of Arg1 is classified
as SS or PS in previous component, the argument
extractor is to extract the spans of Arg1 and Arg2
for the identified discourse connectives. Accord-
ing to (Kong et al., 2014), Kong’s constituent-based
approach outperforms Lin’s tree subtraction algo-
rithm for the Explicit arguments extraction. How-
ever, Lin only focused on the SS case, and Kong
treated the immediately preceding sentence as a spe-
cial constituent for PS, which means that they just
viewed the immediately preceding sentence as Arg1
and only extracted Arg2 for PS. So we only follow
Kong’s constituent-based approach to extract Arg1
and Arg2 for SS. However, for PS, we build two
different extractors for Arg1 and Arg2 separately.
Our intuition is that the two arguments have differ-
ent syntactic and discourse properties and a unified
model with the same feature set used for both may
not have enough discriminating power.
SS Arguments Extractor: In the case of SS,
we adopt (Kong et al., 2014)’s constituent-based ap-
proach without Joint Inference to extract Arg1 and
Arg2.
For PS, we build two argument extractors for
Arg1 and Arg2, respectively, as follows.
PS Arg1 Extractor: We consider the immedi-
ately previous sentence of connective C as the text
span where Arg1 occurs and then build a extractor to
label the Arg1 in it. Similar to Lin’s Attribution span
labeler, this extractor consists of two steps: split-
ting the sentence into clauses, and deciding, for each
clause, whether it belongs to Arg1 or not. First we
use nine punctuation symbols (...,.:;?!-∼) to split the
sentence into several parts and use the SBAR tag in
its parse tree to split each part into clauses. Second,
we build a classifier to decide each clause whether it
belongs to Arg1 or not.
On the one hand, the attribution relation is anno-
tated in PDTB, which expresses the “ownership” re-
lationship between abstract objects and individuals
or agents. However, the attribution annotation is ex-
cluded in CoNLL-2015 (Xue et al., 2015). There-
fore we borrow several attribution features from (Lin
et al., 2014) in order to distinguish the attribution-
related span from others. On the other hand, accord-
ing to the minimality principle of PDTB, the argu-
</bodyText>
<page confidence="0.996293">
19
</page>
<bodyText confidence="0.999920777777778">
ment annotation includes the minimal span of text
that is sufficient for the interpretation of the relation.
Since connectives have very close relationship with
discourse relation, we consider to adopt connective-
related features to capture text span for relation. We
choose the following features: (1) attribution-related
features from (Lin et al., 2014): lemmatized verbs
in curr, the first term of curr, the last term of curr,
the last term of prev + the first term of curr, and
</bodyText>
<listItem confidence="0.981458571428571">
(2) our proposed connective-related features: low-
ercased C string and C category (the syntactic cate-
gory of the connective: subordinating, coordinating,
or discourse adverbial), where curr and prev indicate
the current and previous clause respectively and the
corresponding category for the connective C is ob-
tained from the list provided in (Knott, 1996).
</listItem>
<bodyText confidence="0.987146647058824">
PS Arg2 Extractor: The PS Arg2 Extractor is
similar to the PS Arg1 Extractor. However, they dif-
fer as follows: (1) in the first step, we consider the
sentence containing connective C as the text span
where Arg2 occurs and besides the previous nine
punctuation symbols, we also use the connective C
to split the sentence; (2) we adopt different features
to build classifier: lowercased verbs in curr, lemma-
tized verbs in curr, the first term of curr, the last
term of curr, the last term of prev, the first term of
next, the last term of prev + the first term of curr, the
last term of curr + the first term of next, production
rules extracted from curr, curr position (i.e., the po-
sition of curr in the sentence: start, middle or end),
C string, lowercased C string, C position, C cate-
gory, path of C’s parent → root, compressed path of
C’s parent → root.
</bodyText>
<subsectionHeader confidence="0.745497">
3.1.4 Explicit Sense Classifier
</subsectionHeader>
<bodyText confidence="0.987168074074074">
From previous components, we have identified all
discourse connectives and their arguments from the
texts. Here, we move to decide what Explicit rela-
tion each of them conveys.
The features for this classifier consist of the
following: (1) Lin’s features: C string, C POS,
previ + C (2) Pitler’s features: self-category,
parent-category, left-sibling-category, right-sibling-
category, C-Syn interaction, Syn-Syn interaction.
(3) our five newly proposed features: parent-
category linked context, previous connective and its
POS of as and previous connective and its POS of
when. The first parent-category linked context fea-
ture is to provide more syntactic context informa-
tion for the classification. The last four features
are specially designed to disambiguate the relation
senses of the connective as or when, since the two
connectives often have ambiguity between Contin-
gency.Cause.Reason and Temporal.Synchrony. As
shown in Example 1, the previous connective of
the discourse connective as is But, therefore the
discourse connective as usually carries the Con-
tingency.Cause.Reason sense rather than Tempo-
ral.Synchrony.
(1) But the gains in Treasury bonds were pared as
stocks staged a partial recovery.
(Contingency.Cause.Reason – WSJ-1213)
</bodyText>
<subsectionHeader confidence="0.990145">
3.2 Non-Explicit Parser
</subsectionHeader>
<bodyText confidence="0.999915956521739">
In this section, we discuss the identification of the
Non-Explicit relations.
Since the Non-Explicit relations are only anno-
tated for adjacent sentence pairs within paragraphs,
we first collect all adjacent sentence pairs within
each paragraph, but not identified in any Explicit re-
lation. We assume the previous sentence as Arg1
and the next sentence as Arg2, and then identify the
sense by the features extracted from (Arg1, Arg2).
After that, we use Implicit Arg1 Extractor and Im-
plicit Arg2 Extractor to label Arg1 and Arg2 for
Non-EntRel relations in Non-Explicit, and for En-
tRel relations, we simply label the previous sentence
as Arg1 and the next as Arg2.
Moreover, as shown in Figure 1, we use the Non-
Explicit sense classifier again to identify the sense
on the refined arguments (extracted arguments from
Implicit Arg1&amp;Arg2 Extractor) rather than the adja-
cent sentence pairs (i.e., previous sentence as Arg1,
the next sentence as Arg2 ). Our expectation is that
the overall parser performance might be improved if
we extract features on refined argument spans rather
than original argument spans.
</bodyText>
<subsectionHeader confidence="0.723737">
3.2.1 Non-Explicit Sense Classifier
</subsectionHeader>
<bodyText confidence="0.999871833333333">
According to previous work, this component is
the most difficult one in the discourse parser. And
the features we adopted in this component are cho-
sen from (Lin et al., 2009; Pitler et al., 2009; Ruther-
ford and Xue, 2014), including: production rules,
dependency rules, first-last, first3, modality, verbs,
</bodyText>
<page confidence="0.987598">
20
</page>
<bodyText confidence="0.999648714285714">
Inquirer, polarity, immediately preceding discourse
connective of current sentence pair, Brown cluster
pairs. For the collection of production rules, de-
pendency rules, and Brown cluster pairs, we used a
frequency cutoff of 5 to remove infrequent features,
and for Brown cluster, we choose 3,200 classes, as
in (Rutherford and Xue, 2014).
</bodyText>
<subsectionHeader confidence="0.595848">
3.2.2 Implicit Arg1 Extractor
</subsectionHeader>
<bodyText confidence="0.999747">
The implicit Arg1 Extractor is performed to ex-
tract Arg1 for Non-EntRel relations in Non-Explicit,
which is done similarly to the PS Arg1 Extractor.
We first split the sentence into clauses and then de-
cide each clause whether it belongs to Arg1 or not.
The features extracted from the current and previous
clauses (curr and prev) are: the first term of curr,
the last term of prev, the cross product of the prev
and curr production rules, the path of the first term
of curr → the last term of prev, number of words of
curr.
</bodyText>
<subsectionHeader confidence="0.723284">
3.2.3 Implicit Arg2 Extractor
</subsectionHeader>
<bodyText confidence="0.999988818181818">
The implicit Arg2 Extractor is similar to that of
Arg1, but different features are extracted from the
current, previous, and next clauses (curr, prev, and
next), including: lowercased verbs in curr, the first
term of curr, the last term of prev, the last term of
prev + the first term of curr, the last term of curr +
the first term of next, curr position, the cross prod-
uct of the prev and curr production rules, the cross
product of the curr and next production rules, the
path of the first term of curr → the last term of prev,
number of words of curr.
</bodyText>
<sectionHeader confidence="0.996669" genericHeader="method">
4 Experiments on Training Data
</sectionHeader>
<bodyText confidence="0.97592972">
To implement the 9 components described above,
we compared two supervised machine learning al-
gorithms, i.e., MaxEnt and Naive Bayes, imple-
mented in MALLET toolkit1. For each compo-
nent, we chose the algorithm with better perfor-
mance. Specifically, we use Naive Bayes to build
Non-Explicit Sense Classifier, and MaxEnt for the
other 8 components.
We use PDTB Section 02-21 for training and Sec-
tion 22 for development, which are provided by
CoNLL-2015 with parse trees along with POS tags
1mallet.cs.umass.edu
produced by the Berkeley Parser. And we partici-
pate in the closed tracks, that is, only two resources
(i.e., Brown Clusters and MPQA Subjectivity Lexi-
con) are used in our discourse parser.
According to the requirement, a relation is con-
sidered to be correct if and only if: (1) the dis-
course connective is correctly detected (for explicit
discourse relations); (2) the sense of a discourse re-
lation is correctly predicted; (3) the text spans of
the two arguments as well as their labels (Arg1 and
Arg2) are correctly predicted. We use the official
measure F1 (harmonic mean of Precision and Re-
call) to evaluate performance.
</bodyText>
<subsectionHeader confidence="0.982019">
4.1 Results of Explicit Parser
</subsectionHeader>
<bodyText confidence="0.999682">
Table 1 reports the results of the explicit discourse
parser on development data set of three components
(i.e., Connective classifier, Arg1 position classifier
and Explicit sense classifier) without error propa-
gation (EP), where our new features are introduced.
We find that the F1 scores of all these classifiers are
increased by adding our new features (+new).
</bodyText>
<table confidence="0.994651">
Component P&amp;N and Lin + new
P R F1 (96) P R F1 (96)
Connective Classifier 94.80 93.97 94.38 95.28 95.00 95.14
Arg1 Position Classifier 97.82 98.88 98.35 99.77 99.57 99.69
Explicit Sense Classifier 89.11 89.11 89.11 90.14 90.14 90.14
</table>
<tableCaption confidence="0.981256">
Table 1: Results for three components which add in our
new features, no EP
</tableCaption>
<bodyText confidence="0.999947666666667">
To evaluate the performance of Explicit argu-
ments extraction, we build the PS baseline by label-
ing the previous sentence of the connective as Arg1,
and the text span between the connective and the be-
ginning of the next sentence as Arg2. Table 2 sum-
marizes the results of Explicit arguments extraction
with exact matching and without error propagation,
and the corresponding PS baseline is shown within
parentheses. Note that we removed the leading or
tailing punctuation from all text spans before eval-
uation. We see that the F1 of PS is improved by a
large margin for Arg1, Arg2 and Both by using two
separate PS argument extractors, and the overall F1
of Explicit arguments extraction is also increased by
2.51%.
</bodyText>
<subsectionHeader confidence="0.985183">
4.2 Results of Non-Explicit Parser
</subsectionHeader>
<bodyText confidence="0.99557">
Table 3 reports the results for Non-Explicit sense
classification without error propagation, where we
</bodyText>
<page confidence="0.996679">
21
</page>
<table confidence="0.99601125">
Arg1 F1 (%) Arg2 F1 (%) Both F1 (%)
SS 70.56 88.54 64.72
PS 50.64(44.20) 75.10(66.09) 39.91(32.61)
All 64.15(61.93) 84.25(81.15) 56.61(54.10)
</table>
<tableCaption confidence="0.894898">
Table 2: Results for Explicit arguments extraction, where
“All” indicates the arguments extraction for all the Ex-
plicit relations, and “Both” indicates Arg1 and Arg2 of a
relation are both exactly matched, no EP
</tableCaption>
<table confidence="0.999926">
P R F1 (%)
EntRel sense 58.54 66.98 62.47
All Non-Explicit Senses 43.12 42.72 42.92
</table>
<tableCaption confidence="0.968133">
Table 3: Results for Non-Explicit sense classification, no
EP
</tableCaption>
<bodyText confidence="0.999772823529412">
extract features on gold standard arguments of the
Non-Explicit relations. The first row gives the re-
sult of the EntRel identification. Since we only ex-
tract arguments for Non-EntRel relations in Non-
Explicit, the performance on EntRel identification is
important, since it affects the performance of argu-
ments extraction on Non-Explicit relations.
Table 4 reports the results for arguments extrac-
tion on Non-EntRel relations in Non-Explicit with-
out error propagation, where the first row shows the
result of the baseline system by labeling the pre-
vious sentence as Arg1 and the next sentence as
Arg2, and the second row shows the result when us-
ing two Implicit extractors. As we expected, using
two separate Implicit extractors achieves much bet-
ter performance than the baseline. Table 5 reports
the comparison results for the overall arguments ex-
traction of parser with error propagation, where the
first row indicates the performance when simply us-
ing the previous sentence as Arg1 and the next sen-
tence as Arg2 for all Non-Explicit relations, and the
second shows the results of using two Implicit ar-
gument extractors for Non-EntRel relations. We see
that the performance of the arguments extraction in-
creases, but not too much, due to the error propaga-
tion from the EntRel identification (P: 39.32%, R:
64.19%, F1: 48.76%; EP).
Table 6 shows the overall results, where the first
row is the overall performance of the parser when
identify Non-Explicit sense on original arguments
(i.e., adjacent sentence pairs), and the second row
is the results on refined arguments. We find that the
overall F1 of the parser is improved 0.41% by ex-
tracting features on the refined arguments.
</bodyText>
<table confidence="0.992000666666667">
Arg1 F1 Arg2 F1 Both F1
w/o Impl extractors 61.80 69.92 48.56
with Impl extractors 70.02 77.42 55.85
</table>
<tableCaption confidence="0.9905925">
Table 4: Results for using Implicit Arg1&amp;Arg2 extractors
on Non-EntRel relations in Non-Explicit, no EP
</tableCaption>
<table confidence="0.999724333333333">
Arg1 F1 Arg2 F1 Both F1
w/o Impl extractors 66.06 77.06 56.31
with Impl extractors 66.97 77.21 57.21
</table>
<tableCaption confidence="0.9446635">
Table 5: Results for overall argument extraction of the
parser, EP
</tableCaption>
<table confidence="0.998582333333333">
P R F1 (%)
on original arguments 37.18 37.67 37.43
on refined arguments 37.59 38.09 37.84
</table>
<tableCaption confidence="0.876929">
Table 6: Results of overall parser performance using
Non-Explicit sense classifier on original and refined ar-
guments
</tableCaption>
<sectionHeader confidence="0.994919" genericHeader="evaluation">
5 Results on Test Data Sets
</sectionHeader>
<bodyText confidence="0.999991818181818">
The above described discourse parser system is eval-
uated on two test datasets provided by the shared
task: (1) Section 23 in PDTB; (2) blind test set
drawn from a similar source and domain in terms of
F1. The officially released results are shown in Ta-
ble 7. Our parser ranks the first on both test datasets.
Although the two test datasets are both from news
wire domain and in PDTB style, there are difference
between the two datasets. For example, not all dis-
course connectives in blind test dataset are listed in
PDTB, e.g., “upon” is annotated as discourse con-
nective in blind test dataset while it is not in PDTB.
We compare our discourse parser with Lin’s on
PDTB Section 23. We find that new features pro-
posed in this work do help increase F1 of Explicit
connective classification by 0.54%. And for the
Explicit arguments extraction, our parser achieves
better performance as well. However, since the
sense labels of Explicit and Non-Explicit relations
in CoNLL-2015 differ from Lin’s, i.e., Lin used par-
tial sense labels of the second level (Type) by ex-
cluding several small categories while CoNLL-2015
</bodyText>
<page confidence="0.993337">
22
</page>
<table confidence="0.999089882352941">
on PDTB Section 23 on blind test data set
our parser Lin’s parser our parser 2nd rank parser
P R F1 (%) P R F1 (%) P R F1 (%) P R F1 (%)
Explicit connective 94.83 93.49 94.16 - - 93.62 93.48 90.29 91.86 92.57 87.41 89.92
Explicit Arg1 extraction 51.05 50.33 50.68 - - 47.68 49.16 47.48 48.31 50.48 47.66 49.03
Explicit Arg2 extraction 77.89 76.79 77.33 - - 70.27 75.61 73.02 74.29 72.76 68.71 70.68
Explicit Both extraction 45.54 44.90 45.22 - - 40.37 42.09 40.65 41.35 40.76 38.49 39.59
Explicit only sense 35.52 34.69 34.93 - - - 29.69 26.24 25.91 33.15 24.81 25.22
Non-Explicit Arg1 extraction 64.83 69.50 67.08 - - - 58.66 63.25 60.87 39.47 47.93 43.29
Non-Explicit Arg2 extraction 66.02 70.78 68.32 - - - 71.88 77.49 74.58 51.58 62.63 56.57
Non-Explicit Both extraction 51.20 54.89 52.98 - - - 48.58 52.37 50.41 34.93 42.42 38.31
Non-Explicit only sense 53.18 10.45 9.06 - - - 44.74 8.64 7.69 37.08 7.83 6.81
All Arg1 extraction 59.20 61.03 60.10 - - - 55.12 56.58 55.84 44.61 48.64 46.54
All Arg2 extraction 71.43 73.64 72.52 - - - 73.49 75.43 74.45 60.02 65.43 62.60
All Both extraction 48.62 50.13 49.36 - - - 45.77 46.98 46.37 37.25 40.61 38.86
Sense (Expicit+Non-Explicit) 31.44 30.42 29.83 - - - 25.07 22.13 21.82 25.00 19.60 18.87
Overall Parser 29.27 30.08 29.72 - - 20.64 23.69 24.32 24.00 20.94 22.83 21.84
</table>
<tableCaption confidence="0.978339">
Table 7: Results of our parser on PDTB Section 23 and the blind test dataset, Lin’s parser on PDTB Section 23 and the
2nd rank parser on blind test dataset, “All” indicates all relations (Explicit and Non-Explicit relations), “-” indicates
not available
</tableCaption>
<bodyText confidence="0.999171444444444">
used different sense labels (partial of the three sense
levels with excluding and/or merging several small
categories), the direct comparison on sense classifi-
cation as well as the parser performance is not pos-
sible.
Table 7 also shows the results of our parser and
the 2nd rank parser on blind test dataset, we see that
our parser achieves better performance, especially
on the arguments extraction.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999965333333333">
In this work, we have implemented a refined dis-
course parser by adding new components and fea-
tures based on Lin’s system. Specifically, we (1)
build two PS arguments extractors (i.e., PS Arg1 Ex-
tractor and PS Arg2 Extractor ) to improve perfor-
mance of Explicit arguments extraction, (2) propose
new features for building three classifiers (i.e, Con-
nective Classifier, Arg1 Position Classifier, Explicit
Sense Classifier), (3) construct two Implicit argu-
ments extractors (i.e., Implicit Arg1 Extractor and
Implicit Arg2 Extractor) for Non-EntRel relations,
and (4) perform Non-Explicit sense classification on
refined arguments. Our system ranks the first on
both test data sets, i.e. PDTB Section 23 and a blind
test dataset.
</bodyText>
<sectionHeader confidence="0.998752" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.899976833333333">
This research is supported by grants from Science
and Technology Commission of Shanghai Munici-
pality under research grant no. (14DZ2260800 and
15ZR1410700) and Shanghai Collaborative Innova-
tion Center of Trustworthy Software for Internet of
Things (ZF1213).
</bodyText>
<sectionHeader confidence="0.994581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999201117647059">
Sucheta Ghosh, Richard Johansson, and Sara Tonelli.
2011. Shallow discourse parsing with conditional ran-
dom fields. In In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP 2011. Citeseer.
Alistair Knott. 1996. A data-driven methodology for mo-
tivating a set of coherence relations.
Fang Kong, Hwee Tou Ng, and Guodong Zhou. 2014. A
constituent-based approach to argument labeling with
joint inference in discourse parsing. In Proceedings of
the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 68–77, Doha,
Qatar, October. Association for Computational Lin-
guistics.
Man Lan, Yu Xu, Zheng-Yu Niu, et al. 2013. Leveraging
synthetic discourse data via multi-task learning for im-
plicit discourse relation recognition. In ACL (1), pages
476–485. Citeseer.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343–351. As-
sociation for Computational Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, pages 1–34.
Emily Pitler and Ani Nenkova. 2009. Using syntax
to disambiguate explicit discourse connectives in text.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 13–16. Association for Computa-
tional Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Au-
tomatic sense prediction for implicit discourse rela-
</reference>
<page confidence="0.97058">
23
</page>
<reference confidence="0.999672894736842">
tions in text. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2, pages
683–691. Association for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L
Webber. 2008. The Penn Discourse TreeBank 2.0. In
LREC. Citeseer.
Attapol T Rutherford and Nianwen Xue. 2014. Discov-
ering implicit discourse relations through brown clus-
ter pair representation and coreference patterns. EACL
2014, page 645.
Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi
Prasad, Christopher Bryant, and Attapol Rutherford.
2015. The CoNLL-2015 Shared Task on Shallow Dis-
course Parsing. In Proceedings of the Nineteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, Beijing, China.
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388937">
<title confidence="0.998514">A Refined End-to-End Discourse Parser</title>
<author confidence="0.9570025">Man Shanghai Key Laboratory of Multidimensional Information Wang</author>
<affiliation confidence="0.989015">Department of Computer Science and</affiliation>
<note confidence="0.458847">East China Normal University, Shanghai 200241, P. R.</note>
<abstract confidence="0.996758235294117">The CoNLL-2015 shared task focuses on shallow discourse parsing, which takes a piece of newswire text as input and returns the discourse relations in a PDTB style. In this paper, we describe our discourse parser that particiin the shared task. We use to construct the whole parser to identify discourse connectives, label arguments and classify the sense of Explicit or Non-Explicit relations in free texts. Compared to previous discourse parser, new components and features are added in our system, which further improves the overall performance of the discourse parser. Our parser ranks the first on two test datasets, i.e., PDTB Section 23 and a blind test dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Richard Johansson</author>
<author>Sara Tonelli</author>
</authors>
<title>Shallow discourse parsing with conditional random fields. In</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011. Citeseer.</booktitle>
<contexts>
<context position="2087" citStr="Ghosh et al., 2011" startWordPosition="321" endWordPosition="324">l., 2014). Since the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), much research has been carried out on PDTB to perform the subtasks of a full end-to-end parser, such as identifying discourse connectives, labeling arguments and classifying Explicit or Implicit relations. To identify discourse connectives from non-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of e</context>
</contexts>
<marker>Ghosh, Johansson, Tonelli, 2011</marker>
<rawString>Sucheta Ghosh, Richard Johansson, and Sara Tonelli. 2011. Shallow discourse parsing with conditional random fields. In In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A data-driven methodology for motivating a set of coherence relations.</title>
<date>1996</date>
<contexts>
<context position="12733" citStr="Knott, 1996" startWordPosition="2064" endWordPosition="2065">ures to capture text span for relation. We choose the following features: (1) attribution-related features from (Lin et al., 2014): lemmatized verbs in curr, the first term of curr, the last term of curr, the last term of prev + the first term of curr, and (2) our proposed connective-related features: lowercased C string and C category (the syntactic category of the connective: subordinating, coordinating, or discourse adverbial), where curr and prev indicate the current and previous clause respectively and the corresponding category for the connective C is obtained from the list provided in (Knott, 1996). PS Arg2 Extractor: The PS Arg2 Extractor is similar to the PS Arg1 Extractor. However, they differ as follows: (1) in the first step, we consider the sentence containing connective C as the text span where Arg2 occurs and besides the previous nine punctuation symbols, we also use the connective C to split the sentence; (2) we adopt different features to build classifier: lowercased verbs in curr, lemmatized verbs in curr, the first term of curr, the last term of curr, the last term of prev, the first term of next, the last term of prev + the first term of curr, the last term of curr + the fi</context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Alistair Knott. 1996. A data-driven methodology for motivating a set of coherence relations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Hwee Tou Ng</author>
<author>Guodong Zhou</author>
</authors>
<title>A constituent-based approach to argument labeling with joint inference in discourse parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>68--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="2282" citStr="Kong et al., 2014" startWordPosition="353" endWordPosition="356">fying discourse connectives, labeling arguments and classifying Explicit or Implicit relations. To identify discourse connectives from non-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. All of these research focus on the subtasks of the PDTB, and can be viewed as isolated components of a full pa</context>
<context position="6274" citStr="Kong et al., 2014" startWordPosition="1000" endWordPosition="1003">bution is not annotated for EntRel relations, if the output of the above Non-Explict sense classifier is EntRel, we regard the previous sentence as Arg1 and the next one as Arg2. Otherwise, we build the following two argument extractors to label Arg1 and Arg2. (8) Implicit Arg1 Extractor and (9) Implicit Arg2 Extractor extract Arg1 and Arg2 for NonEntRel relations in Non-explicit, respectively. 3 Components and Features Generally, our parser consists of 9 components, which compose an Explicit parser and a NonExplicit parser. Most of features used in our parser are borrowed from previous work (Kong et al., 2014; Lin et al., 2014; Pitler et al., 2009; Pitler and Nenkova, 2009; Rutherford and Xue, 2014). 3.1 Explicit Parser 3.1.1 Connective Classifier Since the input of the discourse parser is free text, the first thing we need to do is to identify all connective occurrences in text, and then to use the connective classifier to decide whether they function as discourse connectives or not. For each connective occurrence C, we extract features from its context, part-of-speech (POS) and the parse tree of the connective’s sentence. Note that previ and nexti indicate the first previous word and the first n</context>
<context position="9884" citStr="Kong et al., 2014" startWordPosition="1597" endWordPosition="1600">on (the position of connective C in the sentence: start, middle, or end), C POS, prev1, prev1 POS, prev1 + C, prev1 POS + C POS, prev2, prev2 POS, prev2 + C, prev2 POS + C POS; (2) our newly-proposed features: C POS + next1 POS, next2, path of C → root. Note that prev2 and next2 indicate the second previous word and the second next word of connective C, respectively. 3.1.3 Argument Extractor After the relative position of Arg1 is classified as SS or PS in previous component, the argument extractor is to extract the spans of Arg1 and Arg2 for the identified discourse connectives. According to (Kong et al., 2014), Kong’s constituent-based approach outperforms Lin’s tree subtraction algorithm for the Explicit arguments extraction. However, Lin only focused on the SS case, and Kong treated the immediately preceding sentence as a special constituent for PS, which means that they just viewed the immediately preceding sentence as Arg1 and only extracted Arg2 for PS. So we only follow Kong’s constituent-based approach to extract Arg1 and Arg2 for SS. However, for PS, we build two different extractors for Arg1 and Arg2 separately. Our intuition is that the two arguments have different syntactic and discourse</context>
</contexts>
<marker>Kong, Ng, Zhou, 2014</marker>
<rawString>Fang Kong, Hwee Tou Ng, and Guodong Zhou. 2014. A constituent-based approach to argument labeling with joint inference in discourse parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68–77, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Man Lan</author>
<author>Yu Xu</author>
<author>Zheng-Yu Niu</author>
</authors>
<title>Leveraging synthetic discourse data via multi-task learning for implicit discourse relation recognition.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>476--485</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2609" citStr="Lan et al., 2013" startWordPosition="401" endWordPosition="404"> improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. All of these research focus on the subtasks of the PDTB, and can be viewed as isolated components of a full parser. (Lin et al., 2014) constructed a full parser on the top of these subtasks, which contained multiple components joined in a sequential pipeline architecture including a connective classifier, argument labeler, explicit classifier, non-explicit classifier, and attribution span labeler. In this paper, we followed the frame</context>
</contexts>
<marker>Lan, Xu, Niu, 2013</marker>
<rawString>Man Lan, Yu Xu, Zheng-Yu Niu, et al. 2013. Leveraging synthetic discourse data via multi-task learning for implicit discourse relation recognition. In ACL (1), pages 476–485. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>343--351</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2419" citStr="Lin et al., 2009" startWordPosition="374" endWordPosition="377">-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. All of these research focus on the subtasks of the PDTB, and can be viewed as isolated components of a full parser. (Lin et al., 2014) constructed a full parser on the top of these subtasks, which contained multiple components joined in a sequenti</context>
<context position="16205" citStr="Lin et al., 2009" startWordPosition="2620" endWordPosition="2623">se the NonExplicit sense classifier again to identify the sense on the refined arguments (extracted arguments from Implicit Arg1&amp;Arg2 Extractor) rather than the adjacent sentence pairs (i.e., previous sentence as Arg1, the next sentence as Arg2 ). Our expectation is that the overall parser performance might be improved if we extract features on refined argument spans rather than original argument spans. 3.2.1 Non-Explicit Sense Classifier According to previous work, this component is the most difficult one in the discourse parser. And the features we adopted in this component are chosen from (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2014), including: production rules, dependency rules, first-last, first3, modality, verbs, 20 Inquirer, polarity, immediately preceding discourse connective of current sentence pair, Brown cluster pairs. For the collection of production rules, dependency rules, and Brown cluster pairs, we used a frequency cutoff of 5 to remove infrequent features, and for Brown cluster, we choose 3,200 classes, as in (Rutherford and Xue, 2014). 3.2.2 Implicit Arg1 Extractor The implicit Arg1 Extractor is performed to extract Arg1 for Non-EntRel relations in Non-Explic</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 343–351. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-styled end-to-end discourse parser. Natural Language Engineering,</title>
<date>2014</date>
<pages>1--34</pages>
<contexts>
<context position="1477" citStr="Lin et al., 2014" startWordPosition="229" endWordPosition="232">discourse parser. Our parser ranks the first on two test datasets, i.e., PDTB Section 23 and a blind test dataset. 1 Introduction An end-to-end discourse parser is given free texts as input and returns discourse relations in a PDTB style, where a connective acts as a predicate that takes two text spans as its arguments. It can benefit many downstream NLP applications, such as information retrieval, question answering and automatic summarization, etc. The extraction of exact argument spans and Non-Explicit sense identification have been shown to be the main challenges of the discourse parsing (Lin et al., 2014). Since the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), much research has been carried out on PDTB to perform the subtasks of a full end-to-end parser, such as identifying discourse connectives, labeling arguments and classifying Explicit or Implicit relations. To identify discourse connectives from non-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et </context>
<context position="2906" citStr="Lin et al., 2014" startWordPosition="450" endWordPosition="453">d a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. All of these research focus on the subtasks of the PDTB, and can be viewed as isolated components of a full parser. (Lin et al., 2014) constructed a full parser on the top of these subtasks, which contained multiple components joined in a sequential pipeline architecture including a connective classifier, argument labeler, explicit classifier, non-explicit classifier, and attribution span labeler. In this paper, we followed the framework of (Lin et al., 2014) to construct a discourse parser. However, our work differs from that of Lin’s in that our system introduces new components and features to improve the overall performance. Specifically, (1) we build two different 17 Proceedings of the Nineteenth Conference on Computatio</context>
<context position="6292" citStr="Lin et al., 2014" startWordPosition="1004" endWordPosition="1007">ated for EntRel relations, if the output of the above Non-Explict sense classifier is EntRel, we regard the previous sentence as Arg1 and the next one as Arg2. Otherwise, we build the following two argument extractors to label Arg1 and Arg2. (8) Implicit Arg1 Extractor and (9) Implicit Arg2 Extractor extract Arg1 and Arg2 for NonEntRel relations in Non-explicit, respectively. 3 Components and Features Generally, our parser consists of 9 components, which compose an Explicit parser and a NonExplicit parser. Most of features used in our parser are borrowed from previous work (Kong et al., 2014; Lin et al., 2014; Pitler et al., 2009; Pitler and Nenkova, 2009; Rutherford and Xue, 2014). 3.1 Explicit Parser 3.1.1 Connective Classifier Since the input of the discourse parser is free text, the first thing we need to do is to identify all connective occurrences in text, and then to use the connective classifier to decide whether they function as discourse connectives or not. For each connective occurrence C, we extract features from its context, part-of-speech (POS) and the parse tree of the connective’s sentence. Note that previ and nexti indicate the first previous word and the first next word of connec</context>
<context position="11754" citStr="Lin et al., 2014" startWordPosition="1906" endWordPosition="1909">r each clause, whether it belongs to Arg1 or not. First we use nine punctuation symbols (...,.:;?!-∼) to split the sentence into several parts and use the SBAR tag in its parse tree to split each part into clauses. Second, we build a classifier to decide each clause whether it belongs to Arg1 or not. On the one hand, the attribution relation is annotated in PDTB, which expresses the “ownership” relationship between abstract objects and individuals or agents. However, the attribution annotation is excluded in CoNLL-2015 (Xue et al., 2015). Therefore we borrow several attribution features from (Lin et al., 2014) in order to distinguish the attributionrelated span from others. On the other hand, according to the minimality principle of PDTB, the argu19 ment annotation includes the minimal span of text that is sufficient for the interpretation of the relation. Since connectives have very close relationship with discourse relation, we consider to adopt connectiverelated features to capture text span for relation. We choose the following features: (1) attribution-related features from (Lin et al., 2014): lemmatized verbs in curr, the first term of curr, the last term of curr, the last term of prev + the </context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural Language Engineering, pages 1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>13--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1884" citStr="Pitler and Nenkova, 2009" startWordPosition="291" endWordPosition="294">l, question answering and automatic summarization, etc. The extraction of exact argument spans and Non-Explicit sense identification have been shown to be the main challenges of the discourse parsing (Lin et al., 2014). Since the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), much research has been carried out on PDTB to perform the subtasks of a full end-to-end parser, such as identifying discourse connectives, labeling arguments and classifying Explicit or Implicit relations. To identify discourse connectives from non-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification usin</context>
<context position="6339" citStr="Pitler and Nenkova, 2009" startWordPosition="1012" endWordPosition="1015">t of the above Non-Explict sense classifier is EntRel, we regard the previous sentence as Arg1 and the next one as Arg2. Otherwise, we build the following two argument extractors to label Arg1 and Arg2. (8) Implicit Arg1 Extractor and (9) Implicit Arg2 Extractor extract Arg1 and Arg2 for NonEntRel relations in Non-explicit, respectively. 3 Components and Features Generally, our parser consists of 9 components, which compose an Explicit parser and a NonExplicit parser. Most of features used in our parser are borrowed from previous work (Kong et al., 2014; Lin et al., 2014; Pitler et al., 2009; Pitler and Nenkova, 2009; Rutherford and Xue, 2014). 3.1 Explicit Parser 3.1.1 Connective Classifier Since the input of the discourse parser is free text, the first thing we need to do is to identify all connective occurrences in text, and then to use the connective classifier to decide whether they function as discourse connectives or not. For each connective occurrence C, we extract features from its context, part-of-speech (POS) and the parse tree of the connective’s sentence. Note that previ and nexti indicate the first previous word and the first next word of connective C respectively. For a node in the parse tr</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>683--691</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2399" citStr="Pitler et al., 2009" startWordPosition="370" endWordPosition="373">se connectives from non-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. All of these research focus on the subtasks of the PDTB, and can be viewed as isolated components of a full parser. (Lin et al., 2014) constructed a full parser on the top of these subtasks, which contained multiple components </context>
<context position="6313" citStr="Pitler et al., 2009" startWordPosition="1008" endWordPosition="1011">lations, if the output of the above Non-Explict sense classifier is EntRel, we regard the previous sentence as Arg1 and the next one as Arg2. Otherwise, we build the following two argument extractors to label Arg1 and Arg2. (8) Implicit Arg1 Extractor and (9) Implicit Arg2 Extractor extract Arg1 and Arg2 for NonEntRel relations in Non-explicit, respectively. 3 Components and Features Generally, our parser consists of 9 components, which compose an Explicit parser and a NonExplicit parser. Most of features used in our parser are borrowed from previous work (Kong et al., 2014; Lin et al., 2014; Pitler et al., 2009; Pitler and Nenkova, 2009; Rutherford and Xue, 2014). 3.1 Explicit Parser 3.1.1 Connective Classifier Since the input of the discourse parser is free text, the first thing we need to do is to identify all connective occurrences in text, and then to use the connective classifier to decide whether they function as discourse connectives or not. For each connective occurrence C, we extract features from its context, part-of-speech (POS) and the parse tree of the connective’s sentence. Note that previ and nexti indicate the first previous word and the first next word of connective C respectively. </context>
<context position="16226" citStr="Pitler et al., 2009" startWordPosition="2624" endWordPosition="2627"> sense classifier again to identify the sense on the refined arguments (extracted arguments from Implicit Arg1&amp;Arg2 Extractor) rather than the adjacent sentence pairs (i.e., previous sentence as Arg1, the next sentence as Arg2 ). Our expectation is that the overall parser performance might be improved if we extract features on refined argument spans rather than original argument spans. 3.2.1 Non-Explicit Sense Classifier According to previous work, this component is the most difficult one in the discourse parser. And the features we adopted in this component are chosen from (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2014), including: production rules, dependency rules, first-last, first3, modality, verbs, 20 Inquirer, polarity, immediately preceding discourse connective of current sentence pair, Brown cluster pairs. For the collection of production rules, dependency rules, and Brown cluster pairs, we used a frequency cutoff of 5 to remove infrequent features, and for Brown cluster, we choose 3,200 classes, as in (Rutherford and Xue, 2014). 3.2.2 Implicit Arg1 Extractor The implicit Arg1 Extractor is performed to extract Arg1 for Non-EntRel relations in Non-Explicit, which is done sim</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 683–691. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0. In LREC.</title>
<date>2008</date>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1552" citStr="Prasad et al., 2008" startWordPosition="241" endWordPosition="244"> PDTB Section 23 and a blind test dataset. 1 Introduction An end-to-end discourse parser is given free texts as input and returns discourse relations in a PDTB style, where a connective acts as a predicate that takes two text spans as its arguments. It can benefit many downstream NLP applications, such as information retrieval, question answering and automatic summarization, etc. The extraction of exact argument spans and Non-Explicit sense identification have been shown to be the main challenges of the discourse parsing (Lin et al., 2014). Since the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), much research has been carried out on PDTB to perform the subtasks of a full end-to-end parser, such as identifying discourse connectives, labeling arguments and classifying Explicit or Implicit relations. To identify discourse connectives from non-discourse ones and to classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using condit</context>
<context position="9140" citStr="Prasad et al., 2008" startWordPosition="1463" endWordPosition="1466">ctives from the texts, we come to locate the positions of Arg1 and Arg2 of the connective C. Since Arg2 is defined as the argument with which the connective is syntactically associated, its position is fixed once we locate the discourse connective C. So we only need to identify the relative position of Arg1 as whether it is located within the same sentence as the connective (SS) or in some previous sentences of the connective (PS). We do not identify the case which Arg2 is located in some sentences following the sentence containing the connective (FS), because the statistical distribution of (Prasad et al., 2008) shows that less than 0.1% are FS for Explicit relations. The features consist of the following: (1) Lin’s: C string, C position (the position of connective C in the sentence: start, middle, or end), C POS, prev1, prev1 POS, prev1 + C, prev1 POS + C POS, prev2, prev2 POS, prev2 + C, prev2 POS + C POS; (2) our newly-proposed features: C POS + next1 POS, next2, path of C → root. Note that prev2 and next2 indicate the second previous word and the second next word of connective C, respectively. 3.1.3 Argument Extractor After the relative position of Arg1 is classified as SS or PS in previous compo</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The Penn Discourse TreeBank 2.0. In LREC. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attapol T Rutherford</author>
<author>Nianwen Xue</author>
</authors>
<title>Discovering implicit discourse relations through brown cluster pair representation and coreference patterns. EACL</title>
<date>2014</date>
<pages>645</pages>
<contexts>
<context position="2450" citStr="Rutherford and Xue, 2014" startWordPosition="379" endWordPosition="382">classify the Explicit relations, (Pitler and Nenkova, 2009) extracted syntactic features of connectives from the constituent parses, and showed that syntactic features improved performance in both subtasks. For the argument labeling subtask, (Ghosh et al., 2011) regarded it as a token-level sequence labeling task using conditional random fields (CRFs). (Lin et al., 2014) proposed a tree subtraction algorithm to extract the arguments. (Kong et al., 2014) adopted a constituent-based approach to label arguments. As for Implicit sense classification, (Pitler et al., 2009), (Lin et al., 2009) and (Rutherford and Xue, 2014) performed the classification using several linguistically-informed features, such as verb classes, production rules and Brown cluster pair. (Lan et al., 2013) presented a multi-task learning framework with the use of the prediction of explicit discourse connective as auxiliary learning tasks to improve the performance. All of these research focus on the subtasks of the PDTB, and can be viewed as isolated components of a full parser. (Lin et al., 2014) constructed a full parser on the top of these subtasks, which contained multiple components joined in a sequential pipeline architecture includ</context>
<context position="6366" citStr="Rutherford and Xue, 2014" startWordPosition="1016" endWordPosition="1019"> sense classifier is EntRel, we regard the previous sentence as Arg1 and the next one as Arg2. Otherwise, we build the following two argument extractors to label Arg1 and Arg2. (8) Implicit Arg1 Extractor and (9) Implicit Arg2 Extractor extract Arg1 and Arg2 for NonEntRel relations in Non-explicit, respectively. 3 Components and Features Generally, our parser consists of 9 components, which compose an Explicit parser and a NonExplicit parser. Most of features used in our parser are borrowed from previous work (Kong et al., 2014; Lin et al., 2014; Pitler et al., 2009; Pitler and Nenkova, 2009; Rutherford and Xue, 2014). 3.1 Explicit Parser 3.1.1 Connective Classifier Since the input of the discourse parser is free text, the first thing we need to do is to identify all connective occurrences in text, and then to use the connective classifier to decide whether they function as discourse connectives or not. For each connective occurrence C, we extract features from its context, part-of-speech (POS) and the parse tree of the connective’s sentence. Note that previ and nexti indicate the first previous word and the first next word of connective C respectively. For a node in the parse tree, we use the POS combinat</context>
<context position="16253" citStr="Rutherford and Xue, 2014" startWordPosition="2628" endWordPosition="2632">in to identify the sense on the refined arguments (extracted arguments from Implicit Arg1&amp;Arg2 Extractor) rather than the adjacent sentence pairs (i.e., previous sentence as Arg1, the next sentence as Arg2 ). Our expectation is that the overall parser performance might be improved if we extract features on refined argument spans rather than original argument spans. 3.2.1 Non-Explicit Sense Classifier According to previous work, this component is the most difficult one in the discourse parser. And the features we adopted in this component are chosen from (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2014), including: production rules, dependency rules, first-last, first3, modality, verbs, 20 Inquirer, polarity, immediately preceding discourse connective of current sentence pair, Brown cluster pairs. For the collection of production rules, dependency rules, and Brown cluster pairs, we used a frequency cutoff of 5 to remove infrequent features, and for Brown cluster, we choose 3,200 classes, as in (Rutherford and Xue, 2014). 3.2.2 Implicit Arg1 Extractor The implicit Arg1 Extractor is performed to extract Arg1 for Non-EntRel relations in Non-Explicit, which is done similarly to the PS Arg1 Extra</context>
</contexts>
<marker>Rutherford, Xue, 2014</marker>
<rawString>Attapol T Rutherford and Nianwen Xue. 2014. Discovering implicit discourse relations through brown cluster pair representation and coreference patterns. EACL 2014, page 645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Hwee Tou Ng</author>
<author>Sameer Pradhan</author>
<author>Rashmi Prasad</author>
<author>Christopher Bryant</author>
<author>Attapol Rutherford</author>
</authors>
<date>2015</date>
<booktitle>The CoNLL-2015 Shared Task on Shallow Discourse Parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="11680" citStr="Xue et al., 2015" startWordPosition="1894" endWordPosition="1897">nsists of two steps: splitting the sentence into clauses, and deciding, for each clause, whether it belongs to Arg1 or not. First we use nine punctuation symbols (...,.:;?!-∼) to split the sentence into several parts and use the SBAR tag in its parse tree to split each part into clauses. Second, we build a classifier to decide each clause whether it belongs to Arg1 or not. On the one hand, the attribution relation is annotated in PDTB, which expresses the “ownership” relationship between abstract objects and individuals or agents. However, the attribution annotation is excluded in CoNLL-2015 (Xue et al., 2015). Therefore we borrow several attribution features from (Lin et al., 2014) in order to distinguish the attributionrelated span from others. On the other hand, according to the minimality principle of PDTB, the argu19 ment annotation includes the minimal span of text that is sufficient for the interpretation of the relation. Since connectives have very close relationship with discourse relation, we consider to adopt connectiverelated features to capture text span for relation. We choose the following features: (1) attribution-related features from (Lin et al., 2014): lemmatized verbs in curr, t</context>
</contexts>
<marker>Xue, Ng, Pradhan, Prasad, Bryant, Rutherford, 2015</marker>
<rawString>Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi Prasad, Christopher Bryant, and Attapol Rutherford. 2015. The CoNLL-2015 Shared Task on Shallow Discourse Parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>