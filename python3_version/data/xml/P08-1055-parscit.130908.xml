<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000538">
<title confidence="0.996417">
Intensional Summaries as Cooperative Responses in Dialogue:
Automation and Evaluation
</title>
<author confidence="0.997858">
Joseph Polifroni
</author>
<affiliation confidence="0.992456">
Nokia Research Center
</affiliation>
<address confidence="0.692889">
3 Cambridge Center
Cambridge, MA 02142, USA
</address>
<email confidence="0.998186">
joseph.polifroni@nokia.com
</email>
<author confidence="0.992843">
Marilyn Walker
</author>
<affiliation confidence="0.9974325">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.981254">
Sheffield, S1 4DP, United Kingdom
</address>
<email confidence="0.998319">
M.A.Walker@sheffield.ac.uk
</email>
<sectionHeader confidence="0.980047" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999354652173913">
Despite its long history, and a great deal
of research producing many useful algo-
rithms and observations, research in coop-
erative response generation has had little
impact on the recent commercialization of
dialogue technologies, particularly within
the spoken dialogue community. We hy-
pothesize that a particular type of cooper-
ative response, intensional summaries, are
effective for when users are unfamiliar with
the domain. We evaluate this hypothe-
sis with two experiments with CRUISER,
a DS for in-car or mobile users to access
restaurant information. First, we compare
CRUISER with a baseline system-initiative
DS, and show that users prefer CRUISER.
Then, we experiment with four algorithms
for constructing intensional summaries in
CRUISER, and show that two summary
types are equally effective: summaries that
maximize domain coverage and summaries
that maximize utility with respect to a
user model.
</bodyText>
<sectionHeader confidence="0.997309" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980996583333333">
Research in cooperative response generation has
had a long history and produced many useful
algorithms and observations (Mays 1980; Pol-
lack et al.1982; Joshi et al., 1986; Kalita et
al., 1986) inter alia. However, it has had little
impact on the recent commercialization of dia-
logue technologies, particularly within the spo-
ken dialogue community. We believe that this
lack of interest in cooperative response genera-
tion arises from two limitations of previous work:
(1) There has been relatively little empirical re-
search showing that cooperative responses lead
</bodyText>
<page confidence="0.991972">
479
</page>
<figureCaption confidence="0.589345875">
U1: Tell me about restaurants in London.
SI1: What kind of cuisine are you interested in?
C1: I know of 596 restaurants in London. I know
of 3 inexpensive vegetarian restaurants and 14
inexpensive Chinese restaurants. I also know
of 4 inexpensive Greek restaurants.
U2: Chinese.
SI2: Do you have a price range in mind?
C2: I know of 27 restaurants in London that
serve Chinese food. There are 8 inexpen-
sive Chinese restaurants in Chinatown and
2 inexpensive Chinese restaurants in Hamp-
stead/Kilburn. I also know of 1 inexpensive
Chinese restaurant in Soho.
U3: How about a cheap one?
SI3: What neighborhood would you like?
C3: I know of 1 inexpensive Chinese restaurant
in Hampstead/Kilburn with very good food
quality and 1 in Bayswater with good food
quality. I also know of 2 in Chinatown with
medium food quality.
Figure 1: Intensional summaries (C = CRUISER) as
compared with a system initiative (SI) strategy in
the London restaurant domain. U = User
</figureCaption>
<bodyText confidence="0.999345833333333">
to more natural, effective, or efficient dialogues
(Litman et al.1998; Demberg and Moore, 2006);
and (2) Previous work has hand-crafted such re-
sponses, or hand-annotated the database to sup-
port them (Kaplan, 1984; Kalita et al., 1986;
Cholvy, 1990; Polifroni et al., 2003; Benamara,
2004), which has made it difficult to port and
scale these algorithms.
Moreover, we believe that there is an even
greater need today for cooperative response gen-
eration. Larger and more complex datasets are
daily being created on the Web, as information
</bodyText>
<note confidence="0.818648">
Proceedings of ACL-08: HLT, pages 479–487,
</note>
<page confidence="0.547579">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999912553846154">
is integrated across multiple sites and vendors.
Many users will want to access this information
from a mobile device and will have little knowl-
edge of the domain. We hypothesize that these
users will need cooperative responses that select
and generalize the information provided.
In particular, we hypothesize that a partic-
ular type of cooperative response, intensional
summaries, when provided incrementally dur-
ing a dialogue, are effective for large or com-
plex domains, or when users are unfamiliar
with the domain. These intensional summaries
have the ability to describe the data that forms
the knowledge base of the system, as well as
relationships among the components of that
database. We have implemented intensional
summaries in CRUISER (Cooperative Responses
Using Intensional Summaries of Entities and Re-
lations), a DS for in-car or mobile users to access
restaurant information (Becker et al.2006; Weng
et al.2005; Weng et al.2006). Figure 1 contrasts
our proposed intensional summary strategy with
the system initiative strategy used in many di-
alogue systems (Walker et al., 2002; VXML,
2007).
Previous research on cooperative responses
has noted that summary strategies should
vary according to the context (Sparck Jones,
1993), and the interests and preferences of the
user (Gaasterland et al., 1992; Carenini and
Moore, 2000; Demberg and Moore, 2006).
A number of proposals have emphasized the
importance of making generalizations (Kaplan,
1984; Kalita et al., 1986; Joshi et al., 1986).
In this paper we explore different methods for
constructing intensional summaries and inves-
tigate their effectiveness. We present fully
automated algorithms for constructing inten-
sional summaries using knowledge discovery
techniques (Acar, 2005; Lesh and Mitzen-
macher, 2004; Han et al., 1996), and decision-
theoretic user models (Carenini and Moore,
2000).
We first explain in Sec. 2 our fully automated,
domain-independent algorithm for constructing
intensional summaries. Then we evaluate our
intensional summary strategy with two experi-
ments. First, in Sec. 3, we test the hypothesis
that users prefer summary responses in dialogue
systems. We also test a refinement of that hy-
pothesis, i.e., that users prefer summary type
responses when they are unfamiliar with a do-
main. We compare several versions of CRUISER
with the system-initiative strategy, exemplified
in Fig. 1, and show that users prefer CRUISER.
Then, in Sec. 4, we test four different algo-
rithms for constructing intensional summaries,
and show in Sec. 4.1 that two summary types
are equally effective: summaries that maximize
domain coverage and summaries that maximize
utility with respect to a user model. We also
show in Sec. 4.2 that we can predict with 68%
accuracy which summary type to use, a signifi-
cant improvement over the majority class base-
line of 47%. We sum up in Sec. 5.
</bodyText>
<sectionHeader confidence="0.990897" genericHeader="method">
2 Intensional Summaries
</sectionHeader>
<bodyText confidence="0.999361357142857">
This section describes algorithms which result in
the four types of intensional summaries shown in
Fig. 2. We first define intensional summaries as
follows. Let D be a domain comprised of a set R
of database records {ri, ...rn}. Each record con-
sists of a set of attributes {Aj, ..., An}, with as-
sociated values v: D(Ai)={vi,1, vi,2, ..., vi,n}. In
a dialogue system, a constraint is a value intro-
duced by a user with either an explicit or implied
associated attribute. A constraint c is a func-
tion over records in D such that cj(R) returns a
record r if r C_ D and r : Ai = c. The set of all
dialogue constraints {ci, ..., cn} is the context C
at any point in the dialogue. The set of records
R in D that satisfy C is the focal information:
R is the extension of C in D. For example, the
attribute cuisine in a restaurant domain has val-
ues such as “French” or “Italian”. A user utter-
ance instantiating a constraint on cuisine, e.g.,
“I&apos;m interested in Chinese food”, results in a set
of records for restaurants serving Chinese food.
Intensional summaries as shown in Fig. 2 are
descriptions of the focal information, that high-
light particular subsets of the focal information
and make generalizations over these subsets.
The algorithm for constructing intensional
summaries takes as input the focal information
R, and consists of the following steps:
</bodyText>
<listItem confidence="0.9804065">
• Rank attributes in context C, using one of two
ranking methods (Sec. 2.1);
</listItem>
<page confidence="0.994195">
480
</page>
<table confidence="0.987627894736842">
Type Ranking #atts Clusters Scoring Summary
Ref- Refiner 3 Single Size I know of 35 restaurants in London serving Indian
Sing value food. All price ranges are represented. Some of the
neighborhoods represented are Mayfair, Soho, and
Chelsea. Some of the nearby tube stations are Green
Park, South Kensington and Piccadilly Circus.
Ref- Refiner 2 Associative Size I know of 35 restaurants in London serving Indian
Assoc food. There are 3 medium-priced restaurants in May-
fair and 3 inexpensive ones in Soho. There are also
2 expensive ones in Chelsea.
UM- User 3 Single Utility I know of 35 restaurants in London serving Indian
Sing model value food. There are 6 with good food quality. There are
also 12 inexpensive restaurants and 4 with good ser-
vice quality.
UM- User 2 Associative Utility I know of 35 restaurants in London serving Indian
Assoc model food. There are 4 medium-priced restaurants with
good food quality and 10 with medium food quality.
There are also 4 that are inexpensive but have poor
food quality.
</table>
<figureCaption confidence="0.91234">
Figure 2: Four intensional summary types for a task specifying restaurants with Indian cuisine in London.
</figureCaption>
<listItem confidence="0.9938252">
• Select top-N attributes and construct clusters
using selected attributes (Sec. 2.2);
• Score and select top-N clusters (Sec. 2.3);
• Construct frames for generation, perform aggre-
gation and generate responses.
</listItem>
<subsectionHeader confidence="0.991446">
2.1 Attribute Ranking
</subsectionHeader>
<bodyText confidence="0.999979555555556">
We explore two candidates for attribute ranking:
User model and Refiner.
User model: The first algorithm utilizes
decision-theoretic user models to provide an at-
tribute ranking specific to each user (Carenini
and Moore, 2000). The database contains 596
restaurants in London, with up to 19 attributes
and their values. To utilize a user model, we
first elicit user ranked preferences for domain
attributes. Attributes that are unique across
all entities, or missing for many entities, are
automatically excluded, leaving six attributes:
cuisine, decor quality, food quality, price, ser-
vice, and neighborhood. These are ranked using
the SMARTER procedure (Edwards and Bar-
ron, 1994). Rankings are converted to weights
(w) for each attribute, with a formula which
guarantees that the weights sum to 1:
</bodyText>
<equation confidence="0.989758">
1 K 1
wk = K i=k i
</equation>
<bodyText confidence="0.999891678571428">
where K equals the number of attributes in the
ranking. The absolute rankings are used to se-
lect attributes. The weights are also used for
cluster scoring in Sec. 2.3. User model ranking
is used to produce UM-Sing and UM-Assoc
in Fig. 2.
Refiner method: The second attribute
ranking method is based on the Refiner algo-
rithm for summary construction (Polifroni et al.,
2003). The Refiner returns values for every at-
tribute in the focal information in frames or-
dered by frequency. If the counts for the top-N
(typically, 4) values for a particular attribute,
e.g., cuisine, exceeded M% (typically 80%) of
the total counts for all values, then that at-
tribute is selected. For example, 82% of In-
dian restaurants in the London database are in
the neighborhoods Mayfair, Soho, and Chelsea.
Neighborhood would, therefore, be chosen as an
attribute to speak about for Indian restaurants.
The thresholds M and N in the original Refiner
were set a priori, so it was possible that no at-
tribute met or exceeded the thresholds for a par-
ticular subset of the data. In addition, some en-
tities could have many unknown values for some
attributes.
Thus, to insure that all user queries result in
some summary response, we modify the Refiner
</bodyText>
<page confidence="0.993912">
481
</page>
<bodyText confidence="0.984007833333333">
method to include a ranking function for at-
tributes. This function favors attributes that
contain fewer unknown values but always re-
turns a ranked set of attributes. Refiner ranking
is used to produce Ref-Sing and Ref-Assoc in
Fig. 2.
</bodyText>
<subsectionHeader confidence="0.996772">
2.2 Subset Clustering
</subsectionHeader>
<bodyText confidence="0.999663942857143">
Because the focal information is typically too
large to be enumerated, a second parameter at-
tempts to find interesting clusters representing
subsets of the focal information to use for the
content of intensional summaries. We assume
that the coverage of the summary is important,
i.e., the larger the cluster, the more general the
summary.
The simplest algorithm for producing clusters
utilizes a specified number of the top-ranked at-
tributes to define a cluster. Single attributes,
as in the Ref-Sing and UM-Sing examples in
Fig. 2, typically produce large clusters. Thus
one algorithm uses the top three attributes to
produce clusters, defined by either a single value
(e.g., UM-Sing) or by the set of values that
comprise a significant portion of the total (e.g.,
Ref-Sing).
restaurants that serve Italian cuisine. We apply
c4.5 decision tree induction to compute associ-
ations among attributes (Kamber et al., 1997;
Quinlan, 1993). Each attribute in turn is desig-
nated as the dependent variable, with other at-
tributes used as predictors. Thus, each branch
in the tree represents a cluster described by the
attribute/value pairs that predict the leaf node.
Fig. 3 shows clusters of different sizes induced
from Indian restaurants in London. The cluster
size is determined by the number of attributes
used in tree induction. With two attributes, the
average cluster size at the leaf node is 60.4, but
drops to 4.2 with three attributes. Thus, we use
two attributes to produce associative clusters, as
shown in Fig. 2 (i.e., the Ref-Assoc and UM-
Assoc responses), to favor larger clusters.
</bodyText>
<subsectionHeader confidence="0.9876">
2.3 Cluster Scoring
</subsectionHeader>
<bodyText confidence="0.997957928571428">
The final parameter scores the clusters. One
scoring metric is based on cluster size. Single
attributes produce large clusters, while associa-
tion rules produce smaller clusters.
The second scoring method selects clusters
of high utility according to a user model. We
first assign scalar values to the six ranked at-
tributes (Sec. 2.1), using clustering methods as
described in (Polifroni et al., 2003) The weights
from the user model and the scalar values for
the attributes in the user model yield an overall
utility U for a cluster h, similar to utilities as
calculated for individual entities (Edwards and
Barron, 1994; Carenini and Moore, 2000):
</bodyText>
<figure confidence="0.985349333333333">
price_range
food_quality
food_quality
medium inexpensive
good medium poor Uh = K wk(xhk)
(4) (10) (4) k=1
</figure>
<figureCaption confidence="0.9944584">
Figure 3: A partial tree for Indian restaurants in
London, using price range as the predictor variable
and food quality as the dependent variable. The
numbers in parentheses are the size of the clusters
described by the path from the root.
</figureCaption>
<bodyText confidence="0.9999347">
However, we hypothesize that more informa-
tive and useful intensional summaries might be
constructed from clusters of discovered associ-
ations between attributes. For example, as-
sociations between price and cuisine produce
summaries such as There are 49 medium-priced
We use cluster size scoring with Refiner rank-
ing and utility scoring with user model ranking.
For conciseness, all intensional summaries are
based on the three highest scoring clusters.
</bodyText>
<subsectionHeader confidence="0.945831">
2.4 Summary
</subsectionHeader>
<bodyText confidence="0.999969333333333">
The algorithms for attribute selection and clus-
ter generation and scoring yield the four sum-
mary types in Table 2. Summary Ref-Sing is
constructed using (1) the Refiner attribute rank-
ing; and (2) no association rules. (The quanti-
fier (e.g., some, many) is based on the cover-
</bodyText>
<page confidence="0.993867">
482
</page>
<bodyText confidence="0.998571888888889">
age.) Summary Ref-Assoc is constructed us-
ing (1) the Refiner attribute ranking; and (2)
association rules for clustering. Summary UM-
Sing is constructed using (1) a user model with
ranking as above; and (2) no association rules.
Summary UM-Assoc is constructed using (1) a
user model with ranking of price, food, cuisine,
location, service, and decor; and (2) association
rules.
</bodyText>
<sectionHeader confidence="0.994246" genericHeader="method">
3 Experiment One
</sectionHeader>
<bodyText confidence="0.99997826">
This experiment asks whether subjects prefer
intensional summaries to a baseline system-
initiative strategy. We compare two types of in-
tensional summary responses from Fig. 2, Ref-
Assoc and UM-Assoc to system-initiative.
The 16 experimental subjects are asked to as-
sume three personas, in random order, chosen to
typify a range of user types, as in (Demberg and
Moore, 2006). Subjects were asked to read the
descriptions of each persona, which were avail-
able for reference, via a link, throughout the ex-
periment.
The first persona is the Londoner, represent-
ing someone who knows London and its restau-
rants quite well. The Londoner persona typi-
cally knows the specific information s/he is look-
ing for. We predict that the system-initiative
strategy in Fig. 1 will be preferred by this per-
sona, since our hypothesis is that users prefer
intensional summaries when they are unfamiliar
with the domain.
The second persona is the Generic tourist
(GT), who doesn’t know London well and does
not have strong preferences when it comes to
selecting a restaurant. The GT may want to
browse the domain, i.e. to learn about the struc-
ture of the domain and retrieve information by
recognition rather than specification (Belkin et
al., 1994). We hypothesize that the Ref-Assoc
strategy in Fig. 2 will best fit the GT, since the
corresponding clusters have good domain cover-
age.
The third persona is the UM tourist (UMT).
This persona may also want to browse the
database, since they are unfamiliar with Lon-
don. However, this user has expressed prefer-
ences about restaurants through a previous in-
teraction. The UMT in our experiment is con-
cerned with price and food quality (in that or-
der), and prefers restaurants in Central London.
After location, the UMT is most concerned with
cuisine type. The intensional summary labelled
Um-Assoc in Fig. 2 is based on this user model,
and is computed from discovered associations
among preferred attributes.
As each persona, subjects rate responses on
a Likert scale from 1-7, for each of four dia-
logues, each containing between three and four
query/response pairs. We do not allow tie votes
among the three choices.
</bodyText>
<subsectionHeader confidence="0.994483">
3.1 Experimental results
</subsectionHeader>
<bodyText confidence="0.999978885714286">
The primary hypothesis of this work is that
users prefer summary responses in dialogue sys-
tems, without reference to the context. To test
this hypothesis, we first compare Londoner re-
sponses (average rating 4.64) to the most highly
rated of the two intensional summaries (average
rating 5.29) for each query/response pair. This
difference is significant (df = 263,p &lt; .0001),
confirming that over users prefer an intensional
summary strategy to a system-initiative strat-
egy.
Table 1 shows ratings as a function of persona
and response type. Overall, subjects preferred
the responses tailored to their persona. The
Londoner persona signifcantly preferred Lon-
doner over UMT responses (df = 95, p &lt; .05),
but not more than GT responses. This con-
firms our hypothesis that users prefer incremen-
tal summaries in dialogue systems. Further,
it disconfirms our refinement of that hypothe-
sis, that users prefer summaries only when they
are unfamiliar with the domain. The fact that
no difference was found between Londoner and
GT responses indicates that GT responses con-
tain information that is perceived as useful even
when users are familiar with the domain.
The Generic Tourist persona also preferred
the GT responses, significantly more than the
Londoner responses (df = 95, p &lt; .05), but
not significantly more than the UMT responses.
We had hypothesized that the optimal summary
type for users completely new to a domain would
describe attributes that have high coverage of
the focal information. This hypothesis is discon-
firmed by these findings, that indicate that user
</bodyText>
<page confidence="0.998543">
483
</page>
<table confidence="0.9981326">
Response Type
Persona London GT UMT
London 5.02 4.55 4.32
GT 4.14 4.67 4.39
UM tourist 3.68 4.86 5.23
</table>
<tableCaption confidence="0.712621">
Table 1: Ratings by persona assumed. London =
</tableCaption>
<table confidence="0.9696745">
Londoner persona, GT = Generic tourist, UMT =
User Model tourist
User model Refiner
Association rules 3.4 2.9
Single attributes 3.0 3.4
User model Refiner
Small dataset 3.1 3.4
Large dataset 3.2 2.9
</table>
<tableCaption confidence="0.776512">
Table 2: User ratings showing the interaction be-
</tableCaption>
<bodyText confidence="0.9117525">
tween clustering method, attribute ranking, and
dataset size in summaries.
model information is helpful when constructing
summaries for any user interested in browsing.
Finally, the UM Tourist persona overwhelm-
ingly preferred UMT responses over Londoner
responses (df = 95,p &lt; .0001). However, UMT
responses were not significantly preferred to GT
responses. This confirms our hypothesis that
users prefer summary responses when they are
unfamiliar with the domain, but disconfirms
the hypothesis that users will prefer summaries
based on a user model. The results for both the
Generic Tourist and the UM Tourist show that
both types of intensional summaries contain use-
ful information.
</bodyText>
<sectionHeader confidence="0.989614" genericHeader="method">
4 Experiment Two
</sectionHeader>
<bodyText confidence="0.9998457">
The first experiment shows that users prefer in-
tensional summaries; the purpose of the sec-
ond experiment is to investigate what makes a
good intensional summary. We test the different
ways of constructing such summaries described
in Sec. 2, and illustrated in Fig. 2.
Experimental subjects were 18 students whose
user models were collected as described in
Sec. 2.3. For each user, the four summary types
were constructed for eight tasks in the London
restaurant domain, where a task is defined by a
query instantiating a particular attribute/value
combination in the domain (e.g., I&apos;m interested
in restaurants in Soho). The tasks were selected
to utilize a range of attributes. The focal in-
formation for four of the tasks (large set tasks)
were larger than 100 entities, while the focal in-
formation for the other four tasks were smaller
than 100 entities (small set tasks). Each task
was presented to the subject on its own web
page with the four intensional summaries pre-
sented as text on the web page. Each subject
was asked to carefully read and rate each al-
ternative summary response on a Likert scale
of 1... 5 in response to the statement, This re-
sponse contains information I would find useful
when choosing a restaurant. The subjects were
also asked to indicate which response they con-
sidered the best and the worst, and to provide
free-text comments about each response.
</bodyText>
<subsectionHeader confidence="0.994356">
4.1 Hypothesis Testing Results
</subsectionHeader>
<bodyText confidence="0.999200172413793">
We performed an analysis of variance with at-
tribute ranking (user model vs. refiner), clus-
tering method (association rules vs. single at-
tributes), and set size (large vs. small) as in-
dependent variables and user ratings as the de-
pendent variable. There was a main effect for set
size (df = 1, f = 6.7, p &lt; .01), with summaries
describing small datasets (3.3 average rating)
rated higher than those for large datasets (3.1
average rating).
There was also a significant interaction be-
tween attribute ranking and clustering method
(df = 1, f = 26.8,p &lt; .001). Table 2 shows
ratings for the four summary types. There are
no differences between the two highest rated
summaries: Ref-Sing (average 3.4) and UM-
Assoc (average 3.4). See Fig. 2. This suggests
that discovered associations provide useful con-
tent for intensional summaries, but only for at-
tributes ranked highly by the user model.
In addition, there was another significant in-
teraction between ranking method and setsize
(df = 1, f = 11.7, p &lt; .001). The ratings at the
bottom of Table 2 shows that overall, users rate
summaries of small datasets higher, but users
rate summaries higher for large datasets when a
user model is used. With small datasets, users
prefer summaries that don’t utilize user model
information.
</bodyText>
<page confidence="0.997862">
484
</page>
<bodyText confidence="0.999956">
We also calculate the average utility for each
response (Sec. 2.1) and find a strong correlation
between the rating and its utility (p &lt; .005).
When considering this correlation, it is impor-
tant to remember that utility can be calculated
for all responses, and there are cases where the
Refiner responses have high utility scores.
</bodyText>
<subsectionHeader confidence="0.998336">
4.2 Summary Type Prediction
</subsectionHeader>
<bodyText confidence="0.999360055555555">
Our experimental data suggest that characteris-
tics associated with the set of restaurants being
described are important, as well as utility in-
formation derived from application of a a user
model. The performance of a classifier in pre-
dicting summary type will indicate if trends we
discovered among user judgements carry over to
an automated means of selecting which response
type to use in a given context.
In a final experiment, for each task, we use the
highest rated summary as a class to be predicted
using C4.5 (Quinlan, 1993). Thus we have 4
classes: Ref-Sing, Ref-Assoc, UM-Sing, and
UM-Assoc. We derive two types of feature sets
from the responses: features derived from each
user model and features derived from attributes
of the query/response pair itself. The five fea-
ture sets for the user model are:
</bodyText>
<listItem confidence="0.910391380952381">
• umInfo: 6 features for the rankings for each at-
tribute for each user’s model, e.g. a summary
whose user had rated food quality most highly
would receive a &apos;5&apos; for the feature food quality;
• avgUtility: 4 features representing an average
utility score for each alternative summary re-
sponse, based on its clusters (Sec. 2.3).
• hiUtility: 4 features representing the highest
utility score among the three clusters selected
for each response;
• loUtility: 4 features representing the lowest util-
ity score among the three clusters selected for
each response;
• allUtility: 12 features consisting of the high,
low, and average utility scores from the previous
three feature sets.
Three feature sets are derived from the query
and response pair:
• numRests: 4 features for the coverage of each
response. For summary Ref-Assoc in Ta-
ble 2, numRests is 43; for summary UM-
</listItem>
<table confidence="0.603130727272727">
Assoc, numrests is 53.;
Sys Feature Sets Acc(%)
S1 allUtility 47.1
S2 task, numRests 51.5
S3 allUtility,umInfo 62.3∗
S4 allUtility,umInfo,numRests,task 63.2∗
S5 avgUtility,umInfo,numRests,task 62.5∗
S6 hiUtility,umInfo,numRests,task 66.9∗
S7 hiUtility,umInfo,numRests,task, dataset68.4∗
S8 loUtility,umInfo,numRests,task 60.3∗
S9 hiUtility,umInfo 64.0∗
</table>
<tableCaption confidence="0.904906666666667">
Table 3: Accuracy of feature sets for predicting pre-
ferred summary type. = p &lt; .05 as compared to
the Baseline (S1)).
</tableCaption>
<listItem confidence="0.992117833333333">
• task: A feature for the type of constraint used
to generate the focal information (e.g., cuisine,
price range).
• dataset: A feature for the size of the focal in-
formation subset (i.e., big, small), for values
greater and less than 100.
</listItem>
<bodyText confidence="0.999929296296296">
Table 3 shows the relative strengths of the two
types of features on classification accuracy. The
majority class baseline (System S1) is 47.1%.
The S2 system uses only features associated
with the query/response pair, and its accuracy
(51.5%) is not significantly higher than the base-
line. User model features perform better than
the baseline (S3 in Table 3), and combining
features from the query/response pair and the
user model significantly increases accuracy in all
cases. We experimented with using all the utility
scores (S4), as well as with using just the aver-
age (S5), the high (S6), and the low (S8). The
best performance (68.4%)is for the (S7) system
combination of features.
The classification rules in Table 4 for the best
system (S7) suggests some bases for users’ deci-
sions. The first rule is very simple, simply stat-
ing that, if the highest utility value of the Ref-
Sing response is lower than a particular thresh-
old, then use the UM-Assoc response. In other
words, if one of the two highest scoring response
types has a low utility, use the other.
The second rule in Table 4 shows the effect
that the number of restaurants in the response
has on summary choice. In this rule, the Ref-
Sing response is preferred when the highest util-
</bodyText>
<page confidence="0.997584">
485
</page>
<table confidence="0.9806295">
IF (HighestUtility: Ref-Sing) &lt; 0.18
THEN USE UM-Assoc
IF (HighestUtility: Ref-Assoc) &gt; 0.18) &amp;&amp;
(NumRestaurants: UM-Assoc &lt; 400) &amp;&amp;
(HighestUtility: UM-Assoc &lt; .47)
THEN USE Ref-Sing
IF (NumRestaurants: UM-Assoc &lt; 400) &amp;&amp;
(HighestUtility: UM-Assoc &lt; .57) &amp;&amp;
(HighestUtility: Ref-Assoc &gt; .2)
THEN USE Ref-Assoc
</table>
<tableCaption confidence="0.93759">
Table 4: Example classification rules from System 7
in Table 3.
</tableCaption>
<bodyText confidence="0.997441111111111">
ity value of that response is over a particular
threshold.
The final rule in Table 4 predicts Ref-Assoc,
the lowest overall scoring response type. When
the number of restaurants accounted for by
UM-Assoc, as well as the highest utility for
that response, are both below a certain thresh-
old, and the highest utility for the Ref-Assoc
response is above a certain threshold, then use
Ref-Assoc. The utility for any summary type
using the Refiner method is usually lower than
those using the user model, since overall utility is
not taken into account in summary construction.
However, even low utility summaries may men-
tion attributes the user finds important. That,
combined with higher coverage, could make that
summary type preferable over one constructed
to maximize user model utility.
</bodyText>
<sectionHeader confidence="0.999246" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99999348">
We first compared intensional summary coop-
erative responses against a system initiative di-
alogue strategy in CRUISER. Subjects assumed
three “personas”, a native Londoner, a tourist
who was interacting with the system for the first
time (GT), or a tourist for which the system
has a user model (UMT). The personas were
designed to reflect differing ends of the spectra
defined by Belkin to characterize information-
seeking strategies (Belkin et al., 1994). There
was a significant preference for intensional sum-
maries across all personas, but especially when
the personas were unfamiliar with the domain.
This preference indicates that the benefits of
intensional summaries outweigh the increase in
verbosity.
We then tested four algorithms for summary
construction. Results show that intensional
summaries based on a user model with associa-
tion rules, or on the Refiner method (Polifroni et
al., 2003), are equally effective. While (Dem-
berg and Moore, 2006) found that their user
model stepwise refinement (UMSR) method was
superior to the Refiner method, they also found
many situations (70 out of 190) in which the
Refiner method was preferred. Our experiment
was structured differently, but it suggests that,
in certain circumstances, or within certain do-
mains, users may wish to hear about choices
based on an analysis of focal information, irre-
spective of user preferences.
Our intensional summary algorithms auto-
matically construct summaries from a database,
along with user models collected via a domain-
independent method; thus we believe that
the methods described here are domain-
independent. Furthermore, in tests to deter-
mine whether a classifier can predict the best
summary type to use in a given context, we
achieved an accuracy of 68% as compared to a
majority class baseline of 47%, using dialogue
context features. Both of these results point
hopefully towards a different way of automating
dialogue design, one based on a combination of
user modelling and an analysis of contextual in-
formation. In future work we hope to test these
algorithms in other domains, and show that in-
tensional summaries can not only be automati-
cally derived but also lead to reduced task times
and increased task success.
</bodyText>
<sectionHeader confidence="0.999053" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9918696">
A.C. Acar and A. Motro. 2005. Intensional Encapsu-
lations of Database Subsets via Genetic Program-
ming. Proc, 16th Int. Conf. on Database and Ex-
pert Systems Applications. Copenhagen.
Tilman Becker, Nate Blaylock, Ciprian Gersten-
berger, Ivana Kruijff-Korbayova, Andreas Ko-
rthauer, Manfred Pinkal, Michael Pitz, Peter
Poller, and Jan Schehl. Natural and intuitive mul-
timodal dialogue for in-car applications: The sam-
mie system. In ECAI, pages 612–616, 2006.
</reference>
<page confidence="0.995165">
486
</page>
<reference confidence="0.999836533980582">
N. J. Belkin, C. Cool, A. Stein and U. Thiel. 1994.
Cases, Scripts, and Information Seeking Strate-
gies: On the Design of Interactive Information Re-
trieval Systems. Expert Systems and Applications,
9(3):379–395.
F. Benamara. 2004. Generating Intensional Answers
in Intelligent Question Answering Systems. Proc.
3rd Int. Conf. on Natural Language Generation
INLG.
G. Carenini and J. Moore. 2000. A Strategy for Gen-
erating Evaluative Arguments. Proc. First Int�l
Conf. on Natural Language Generation. 1307–
1314.
Brant Cheikes and Bonnie Webber. Elements of a
computational model of cooperative response gen-
eration. In Proc. Speech and Natural Language
Workshop, pages 216–220, Philadelphia, 1989.
X. Chen and Y-F. Wu. 2006. Personalized Knowl-
edge Discovery: Mining Novel Association Rules
from Text. Proc., SIAM Conference on Data Min-
ing.
L. Cholvy. 1990. Answering Queries Addressed
to a Rule Base. Revue d&apos;Intelligence Artificielle.
1(1):79–98.
V. Demberg and J. Moore. 2006 Information Pre-
sentation in Spoken Dialogue Systems. Proc. 11th
Conf. EACL..
W. Edwards and F. Hutton Barron. 1994. Smarts
and smarter: Improved simple methods for mul-
tiattribute utility measurement. Organizational
Behavior and Human Decision Processes. 60:306–
325.
T. Gaasterland and P. Godfrey and J. Minker. 1992.
An Overview of Cooperative Answering. Journal
of Intelligent Information Systems. 1(2):387–416.
J. Han, Y. Huang and N. Cercone. 1996. Intelligent
Query Answering by Knowledge Discovery Tech-
niques. IEEE Transactions on Knowledge and
Data Engineering. 8(3):373–390.
Aravind Joshi, Bonnie Webber, and Ralph M.
Weischedel. Living up to expectations: computing
expert responses. In HLT &apos;86: Proceedings of the
workshop on Strategic computing natural language,
pages 179–189, Morristown, NJ, USA, 1986. Asso-
ciation for Computational Linguistics.
J. Kalita and M.J. Colburn and G. McCalla. 1984.
A response to the need for summary responses.
COLING-84). 432–436.
M. Kamber, L. Winstone, W. Gong, S. Cheng and
J Han. 1997. Generalization and decision tree
induction: efficient classification in data mining.
Proc. 7th Int. Workshop on Research Issues in
Data Engineering (RIDE &apos;97). 111–121.
S.J.Kaplan. 1984. Designing a Portable Natural
Language Database Query System. ACM Trans-
actions on Database Systems, 9(1):1–19.
N. Lesh and M. Mitzenmacher. Interactive data
summarization: an example application. Proc.,
Working Conference on Advanced Visual Inter-
faces. Gallipoli, Italy. pages 183–187.
Diane J. Litman, Shimei Pan, and Marilyn A.
Walker. Evaluating response strategies in a web-
based spoken dialogue agent. In COLING-ACL,
pages 780–786, 1998.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the Automatic Generation of Mixed-Initiative Di-
alogue Systems from Web Content. Proc. Eu-
rospeech. 2721–2724.
E. Mays. Correcting misconceptions about database
structure. In Proceedings of the CSCSI &apos;80, 1980.
Martha E. Pollack, Julia Hirschberg, and Bonnie L.
Webber. User participation in the reasoning pro-
cesses of expert systems. In AAAI, pages 358–361,
1982.
J.R. Quinlan 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann. San Mateo, CA.
K. Sparck Jones. 1998. Automatic summarising:
factors and directions. I. Mani and M. Maybury,
eds. Advances in Automatic Text Summarization.
MIT Press.
M. Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J.
Garofolo, H. Hastie, A. Le, B. Pellom, A. Potami-
anos, R. Passonneau, R. Prasad, S. Roukos, G.
Sanders, S. Seneff and D. Stallard. 2002. DARPA
Communicator Evaluation: Progress from 2000 to
2001. Proc, ICSLP 2002.
F. Weng, L. Cavedon, B. Raghunathan, D. Mirkovic,
H. Cheng, H. Schmidt, H. Bratt, R. Mishra,
S. Peters, L. Zhao, S. Upson, E. Shriberg, and
C. Bergmann. Developing a conversational dia-
logue system for cognitively overloaded drivers. In
Proceedings, International Congress on Intelligent
Transportation Systems, 2005.
F. Weng, S. Varges, B. Raghunathan, F. Ratiu,
H. Pon-Barry, B. Lathrop, Q. Zhang, T. Schei-
deck, H. Bratt, K. Xu, M. Purver, R. Mishra,
M. Raya, S. Peters, Y. Meng, L. Cavedon, and
L. Shriberg. Chat: A conversational helper for
automotive tasks. In Proceedings, Interspeech: In-
ternational Conference on Spoken Language Pro-
cessing, 2006.
Voxeo. VoiceXML Development Guide.
http://voicexml.org.
</reference>
<page confidence="0.998389">
487
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839349">
<title confidence="0.998467">Intensional Summaries as Cooperative Responses in Dialogue: Automation and Evaluation</title>
<author confidence="0.999797">Joseph Polifroni</author>
<affiliation confidence="0.926462">Nokia Research Center 3 Cambridge Center</affiliation>
<address confidence="0.999992">Cambridge, MA 02142, USA</address>
<email confidence="0.999641">joseph.polifroni@nokia.com</email>
<author confidence="0.999928">Marilyn Walker</author>
<affiliation confidence="0.9998935">Department of Computer Science University of Sheffield</affiliation>
<address confidence="0.999148">Sheffield, S1 4DP, United Kingdom</address>
<email confidence="0.999012">M.A.Walker@sheffield.ac.uk</email>
<abstract confidence="0.999554708333333">Despite its long history, and a great deal of research producing many useful algorithms and observations, research in cooperative response generation has had little impact on the recent commercialization of dialogue technologies, particularly within the spoken dialogue community. We hypothesize that a particular type of cooperresponse, are effective for when users are unfamiliar with the domain. We evaluate this hypothesis with two experiments with CRUISER, a DS for in-car or mobile users to access restaurant information. First, we compare CRUISER with a baseline system-initiative DS, and show that users prefer CRUISER. Then, we experiment with four algorithms constructing summaries CRUISER, and show that two summary types are equally effective: summaries that maximize domain coverage and summaries that maximize utility with respect to a user model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A C Acar</author>
<author>A Motro</author>
</authors>
<title>Intensional Encapsulations of Database Subsets via Genetic Programming.</title>
<date>2005</date>
<booktitle>Proc, 16th Int. Conf. on Database and Expert Systems Applications.</booktitle>
<location>Copenhagen.</location>
<marker>Acar, Motro, 2005</marker>
<rawString>A.C. Acar and A. Motro. 2005. Intensional Encapsulations of Database Subsets via Genetic Programming. Proc, 16th Int. Conf. on Database and Expert Systems Applications. Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tilman Becker</author>
<author>Nate Blaylock</author>
<author>Ciprian Gerstenberger</author>
<author>Ivana Kruijff-Korbayova</author>
<author>Andreas Korthauer</author>
<author>Manfred Pinkal</author>
<author>Michael Pitz</author>
<author>Peter Poller</author>
<author>Jan Schehl</author>
</authors>
<title>Natural and intuitive multimodal dialogue for in-car applications: The sammie system.</title>
<date>2006</date>
<booktitle>In ECAI,</booktitle>
<pages>612--616</pages>
<marker>Becker, Blaylock, Gerstenberger, Kruijff-Korbayova, Korthauer, Pinkal, Pitz, Poller, Schehl, 2006</marker>
<rawString>Tilman Becker, Nate Blaylock, Ciprian Gerstenberger, Ivana Kruijff-Korbayova, Andreas Korthauer, Manfred Pinkal, Michael Pitz, Peter Poller, and Jan Schehl. Natural and intuitive multimodal dialogue for in-car applications: The sammie system. In ECAI, pages 612–616, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Belkin</author>
<author>C Cool</author>
<author>A Stein</author>
<author>U Thiel</author>
</authors>
<date>1994</date>
<booktitle>Cases, Scripts, and Information Seeking Strategies: On the Design of Interactive Information Retrieval Systems. Expert Systems and Applications,</booktitle>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="16387" citStr="Belkin et al., 1994" startWordPosition="2640" endWordPosition="2643">uite well. The Londoner persona typically knows the specific information s/he is looking for. We predict that the system-initiative strategy in Fig. 1 will be preferred by this persona, since our hypothesis is that users prefer intensional summaries when they are unfamiliar with the domain. The second persona is the Generic tourist (GT), who doesn’t know London well and does not have strong preferences when it comes to selecting a restaurant. The GT may want to browse the domain, i.e. to learn about the structure of the domain and retrieve information by recognition rather than specification (Belkin et al., 1994). We hypothesize that the Ref-Assoc strategy in Fig. 2 will best fit the GT, since the corresponding clusters have good domain coverage. The third persona is the UM tourist (UMT). This persona may also want to browse the database, since they are unfamiliar with London. However, this user has expressed preferences about restaurants through a previous interaction. The UMT in our experiment is concerned with price and food quality (in that order), and prefers restaurants in Central London. After location, the UMT is most concerned with cuisine type. The intensional summary labelled Um-Assoc in Fi</context>
<context position="28318" citStr="Belkin et al., 1994" startWordPosition="4587" endWordPosition="4590"> user finds important. That, combined with higher coverage, could make that summary type preferable over one constructed to maximize user model utility. 5 Conclusion We first compared intensional summary cooperative responses against a system initiative dialogue strategy in CRUISER. Subjects assumed three “personas”, a native Londoner, a tourist who was interacting with the system for the first time (GT), or a tourist for which the system has a user model (UMT). The personas were designed to reflect differing ends of the spectra defined by Belkin to characterize informationseeking strategies (Belkin et al., 1994). There was a significant preference for intensional summaries across all personas, but especially when the personas were unfamiliar with the domain. This preference indicates that the benefits of intensional summaries outweigh the increase in verbosity. We then tested four algorithms for summary construction. Results show that intensional summaries based on a user model with association rules, or on the Refiner method (Polifroni et al., 2003), are equally effective. While (Demberg and Moore, 2006) found that their user model stepwise refinement (UMSR) method was superior to the Refiner method</context>
</contexts>
<marker>Belkin, Cool, Stein, Thiel, 1994</marker>
<rawString>N. J. Belkin, C. Cool, A. Stein and U. Thiel. 1994. Cases, Scripts, and Information Seeking Strategies: On the Design of Interactive Information Retrieval Systems. Expert Systems and Applications, 9(3):379–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Benamara</author>
</authors>
<title>Generating Intensional Answers in Intelligent Question Answering Systems.</title>
<date>2004</date>
<booktitle>Proc. 3rd Int. Conf. on Natural Language Generation INLG.</booktitle>
<contexts>
<context position="3073" citStr="Benamara, 2004" startWordPosition="471" endWordPosition="472">now of 1 inexpensive Chinese restaurant in Hampstead/Kilburn with very good food quality and 1 in Bayswater with good food quality. I also know of 2 in Chinatown with medium food quality. Figure 1: Intensional summaries (C = CRUISER) as compared with a system initiative (SI) strategy in the London restaurant domain. U = User to more natural, effective, or efficient dialogues (Litman et al.1998; Demberg and Moore, 2006); and (2) Previous work has hand-crafted such responses, or hand-annotated the database to support them (Kaplan, 1984; Kalita et al., 1986; Cholvy, 1990; Polifroni et al., 2003; Benamara, 2004), which has made it difficult to port and scale these algorithms. Moreover, we believe that there is an even greater need today for cooperative response generation. Larger and more complex datasets are daily being created on the Web, as information Proceedings of ACL-08: HLT, pages 479–487, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics is integrated across multiple sites and vendors. Many users will want to access this information from a mobile device and will have little knowledge of the domain. We hypothesize that these users will need cooperative responses</context>
</contexts>
<marker>Benamara, 2004</marker>
<rawString>F. Benamara. 2004. Generating Intensional Answers in Intelligent Question Answering Systems. Proc. 3rd Int. Conf. on Natural Language Generation INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>J Moore</author>
</authors>
<title>A Strategy for Generating Evaluative Arguments.</title>
<date>2000</date>
<booktitle>Proc. First Int�l Conf. on Natural Language Generation.</booktitle>
<volume>1307</volume>
<pages>1314</pages>
<contexts>
<context position="4787" citStr="Carenini and Moore, 2000" startWordPosition="732" endWordPosition="735">ensional summaries in CRUISER (Cooperative Responses Using Intensional Summaries of Entities and Relations), a DS for in-car or mobile users to access restaurant information (Becker et al.2006; Weng et al.2005; Weng et al.2006). Figure 1 contrasts our proposed intensional summary strategy with the system initiative strategy used in many dialogue systems (Walker et al., 2002; VXML, 2007). Previous research on cooperative responses has noted that summary strategies should vary according to the context (Sparck Jones, 1993), and the interests and preferences of the user (Gaasterland et al., 1992; Carenini and Moore, 2000; Demberg and Moore, 2006). A number of proposals have emphasized the importance of making generalizations (Kaplan, 1984; Kalita et al., 1986; Joshi et al., 1986). In this paper we explore different methods for constructing intensional summaries and investigate their effectiveness. We present fully automated algorithms for constructing intensional summaries using knowledge discovery techniques (Acar, 2005; Lesh and Mitzenmacher, 2004; Han et al., 1996), and decisiontheoretic user models (Carenini and Moore, 2000). We first explain in Sec. 2 our fully automated, domain-independent algorithm for</context>
<context position="9344" citStr="Carenini and Moore, 2000" startWordPosition="1486" endWordPosition="1489">are also 4 that are inexpensive but have poor food quality. Figure 2: Four intensional summary types for a task specifying restaurants with Indian cuisine in London. • Select top-N attributes and construct clusters using selected attributes (Sec. 2.2); • Score and select top-N clusters (Sec. 2.3); • Construct frames for generation, perform aggregation and generate responses. 2.1 Attribute Ranking We explore two candidates for attribute ranking: User model and Refiner. User model: The first algorithm utilizes decision-theoretic user models to provide an attribute ranking specific to each user (Carenini and Moore, 2000). The database contains 596 restaurants in London, with up to 19 attributes and their values. To utilize a user model, we first elicit user ranked preferences for domain attributes. Attributes that are unique across all entities, or missing for many entities, are automatically excluded, leaving six attributes: cuisine, decor quality, food quality, price, service, and neighborhood. These are ranked using the SMARTER procedure (Edwards and Barron, 1994). Rankings are converted to weights (w) for each attribute, with a formula which guarantees that the weights sum to 1: 1 K 1 wk = K i=k i where K</context>
<context position="13677" citStr="Carenini and Moore, 2000" startWordPosition="2200" endWordPosition="2203">ores the clusters. One scoring metric is based on cluster size. Single attributes produce large clusters, while association rules produce smaller clusters. The second scoring method selects clusters of high utility according to a user model. We first assign scalar values to the six ranked attributes (Sec. 2.1), using clustering methods as described in (Polifroni et al., 2003) The weights from the user model and the scalar values for the attributes in the user model yield an overall utility U for a cluster h, similar to utilities as calculated for individual entities (Edwards and Barron, 1994; Carenini and Moore, 2000): price_range food_quality food_quality medium inexpensive good medium poor Uh = K wk(xhk) (4) (10) (4) k=1 Figure 3: A partial tree for Indian restaurants in London, using price range as the predictor variable and food quality as the dependent variable. The numbers in parentheses are the size of the clusters described by the path from the root. However, we hypothesize that more informative and useful intensional summaries might be constructed from clusters of discovered associations between attributes. For example, associations between price and cuisine produce summaries such as There are 49 </context>
</contexts>
<marker>Carenini, Moore, 2000</marker>
<rawString>G. Carenini and J. Moore. 2000. A Strategy for Generating Evaluative Arguments. Proc. First Int�l Conf. on Natural Language Generation. 1307– 1314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brant Cheikes</author>
<author>Bonnie Webber</author>
</authors>
<title>Elements of a computational model of cooperative response generation.</title>
<date>1989</date>
<booktitle>In Proc. Speech and Natural Language Workshop,</booktitle>
<pages>216--220</pages>
<location>Philadelphia,</location>
<marker>Cheikes, Webber, 1989</marker>
<rawString>Brant Cheikes and Bonnie Webber. Elements of a computational model of cooperative response generation. In Proc. Speech and Natural Language Workshop, pages 216–220, Philadelphia, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Chen</author>
<author>Y-F Wu</author>
</authors>
<title>Personalized Knowledge Discovery: Mining Novel Association Rules from Text.</title>
<date>2006</date>
<booktitle>Proc., SIAM Conference on Data Mining.</booktitle>
<marker>Chen, Wu, 2006</marker>
<rawString>X. Chen and Y-F. Wu. 2006. Personalized Knowledge Discovery: Mining Novel Association Rules from Text. Proc., SIAM Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cholvy</author>
</authors>
<title>Answering Queries Addressed to a Rule Base. Revue d&apos;Intelligence Artificielle.</title>
<date>1990</date>
<contexts>
<context position="3032" citStr="Cholvy, 1990" startWordPosition="465" endWordPosition="466">t neighborhood would you like? C3: I know of 1 inexpensive Chinese restaurant in Hampstead/Kilburn with very good food quality and 1 in Bayswater with good food quality. I also know of 2 in Chinatown with medium food quality. Figure 1: Intensional summaries (C = CRUISER) as compared with a system initiative (SI) strategy in the London restaurant domain. U = User to more natural, effective, or efficient dialogues (Litman et al.1998; Demberg and Moore, 2006); and (2) Previous work has hand-crafted such responses, or hand-annotated the database to support them (Kaplan, 1984; Kalita et al., 1986; Cholvy, 1990; Polifroni et al., 2003; Benamara, 2004), which has made it difficult to port and scale these algorithms. Moreover, we believe that there is an even greater need today for cooperative response generation. Larger and more complex datasets are daily being created on the Web, as information Proceedings of ACL-08: HLT, pages 479–487, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics is integrated across multiple sites and vendors. Many users will want to access this information from a mobile device and will have little knowledge of the domain. We hypothesize that th</context>
</contexts>
<marker>Cholvy, 1990</marker>
<rawString>L. Cholvy. 1990. Answering Queries Addressed to a Rule Base. Revue d&apos;Intelligence Artificielle. 1(1):79–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>J Moore</author>
</authors>
<title>Information Presentation in Spoken Dialogue Systems.</title>
<date>2006</date>
<booktitle>Proc. 11th Conf. EACL..</booktitle>
<contexts>
<context position="2880" citStr="Demberg and Moore, 2006" startWordPosition="438" endWordPosition="441">n Chinatown and 2 inexpensive Chinese restaurants in Hampstead/Kilburn. I also know of 1 inexpensive Chinese restaurant in Soho. U3: How about a cheap one? SI3: What neighborhood would you like? C3: I know of 1 inexpensive Chinese restaurant in Hampstead/Kilburn with very good food quality and 1 in Bayswater with good food quality. I also know of 2 in Chinatown with medium food quality. Figure 1: Intensional summaries (C = CRUISER) as compared with a system initiative (SI) strategy in the London restaurant domain. U = User to more natural, effective, or efficient dialogues (Litman et al.1998; Demberg and Moore, 2006); and (2) Previous work has hand-crafted such responses, or hand-annotated the database to support them (Kaplan, 1984; Kalita et al., 1986; Cholvy, 1990; Polifroni et al., 2003; Benamara, 2004), which has made it difficult to port and scale these algorithms. Moreover, we believe that there is an even greater need today for cooperative response generation. Larger and more complex datasets are daily being created on the Web, as information Proceedings of ACL-08: HLT, pages 479–487, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics is integrated across multiple site</context>
<context position="4813" citStr="Demberg and Moore, 2006" startWordPosition="736" endWordPosition="739">SER (Cooperative Responses Using Intensional Summaries of Entities and Relations), a DS for in-car or mobile users to access restaurant information (Becker et al.2006; Weng et al.2005; Weng et al.2006). Figure 1 contrasts our proposed intensional summary strategy with the system initiative strategy used in many dialogue systems (Walker et al., 2002; VXML, 2007). Previous research on cooperative responses has noted that summary strategies should vary according to the context (Sparck Jones, 1993), and the interests and preferences of the user (Gaasterland et al., 1992; Carenini and Moore, 2000; Demberg and Moore, 2006). A number of proposals have emphasized the importance of making generalizations (Kaplan, 1984; Kalita et al., 1986; Joshi et al., 1986). In this paper we explore different methods for constructing intensional summaries and investigate their effectiveness. We present fully automated algorithms for constructing intensional summaries using knowledge discovery techniques (Acar, 2005; Lesh and Mitzenmacher, 2004; Han et al., 1996), and decisiontheoretic user models (Carenini and Moore, 2000). We first explain in Sec. 2 our fully automated, domain-independent algorithm for constructing intensional </context>
<context position="15534" citStr="Demberg and Moore, 2006" startWordPosition="2497" endWordPosition="2500">structed using (1) a user model with ranking as above; and (2) no association rules. Summary UM-Assoc is constructed using (1) a user model with ranking of price, food, cuisine, location, service, and decor; and (2) association rules. 3 Experiment One This experiment asks whether subjects prefer intensional summaries to a baseline systeminitiative strategy. We compare two types of intensional summary responses from Fig. 2, RefAssoc and UM-Assoc to system-initiative. The 16 experimental subjects are asked to assume three personas, in random order, chosen to typify a range of user types, as in (Demberg and Moore, 2006). Subjects were asked to read the descriptions of each persona, which were available for reference, via a link, throughout the experiment. The first persona is the Londoner, representing someone who knows London and its restaurants quite well. The Londoner persona typically knows the specific information s/he is looking for. We predict that the system-initiative strategy in Fig. 1 will be preferred by this persona, since our hypothesis is that users prefer intensional summaries when they are unfamiliar with the domain. The second persona is the Generic tourist (GT), who doesn’t know London wel</context>
<context position="28821" citStr="Demberg and Moore, 2006" startWordPosition="4662" endWordPosition="4666">flect differing ends of the spectra defined by Belkin to characterize informationseeking strategies (Belkin et al., 1994). There was a significant preference for intensional summaries across all personas, but especially when the personas were unfamiliar with the domain. This preference indicates that the benefits of intensional summaries outweigh the increase in verbosity. We then tested four algorithms for summary construction. Results show that intensional summaries based on a user model with association rules, or on the Refiner method (Polifroni et al., 2003), are equally effective. While (Demberg and Moore, 2006) found that their user model stepwise refinement (UMSR) method was superior to the Refiner method, they also found many situations (70 out of 190) in which the Refiner method was preferred. Our experiment was structured differently, but it suggests that, in certain circumstances, or within certain domains, users may wish to hear about choices based on an analysis of focal information, irrespective of user preferences. Our intensional summary algorithms automatically construct summaries from a database, along with user models collected via a domainindependent method; thus we believe that the me</context>
</contexts>
<marker>Demberg, Moore, 2006</marker>
<rawString>V. Demberg and J. Moore. 2006 Information Presentation in Spoken Dialogue Systems. Proc. 11th Conf. EACL..</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Edwards</author>
<author>F Hutton Barron</author>
</authors>
<title>Smarts and smarter: Improved simple methods for multiattribute utility measurement. Organizational Behavior and Human Decision Processes.</title>
<date>1994</date>
<pages>60--306</pages>
<contexts>
<context position="9799" citStr="Edwards and Barron, 1994" startWordPosition="1554" endWordPosition="1558">odel and Refiner. User model: The first algorithm utilizes decision-theoretic user models to provide an attribute ranking specific to each user (Carenini and Moore, 2000). The database contains 596 restaurants in London, with up to 19 attributes and their values. To utilize a user model, we first elicit user ranked preferences for domain attributes. Attributes that are unique across all entities, or missing for many entities, are automatically excluded, leaving six attributes: cuisine, decor quality, food quality, price, service, and neighborhood. These are ranked using the SMARTER procedure (Edwards and Barron, 1994). Rankings are converted to weights (w) for each attribute, with a formula which guarantees that the weights sum to 1: 1 K 1 wk = K i=k i where K equals the number of attributes in the ranking. The absolute rankings are used to select attributes. The weights are also used for cluster scoring in Sec. 2.3. User model ranking is used to produce UM-Sing and UM-Assoc in Fig. 2. Refiner method: The second attribute ranking method is based on the Refiner algorithm for summary construction (Polifroni et al., 2003). The Refiner returns values for every attribute in the focal information in frames order</context>
<context position="13650" citStr="Edwards and Barron, 1994" startWordPosition="2196" endWordPosition="2199">ing The final parameter scores the clusters. One scoring metric is based on cluster size. Single attributes produce large clusters, while association rules produce smaller clusters. The second scoring method selects clusters of high utility according to a user model. We first assign scalar values to the six ranked attributes (Sec. 2.1), using clustering methods as described in (Polifroni et al., 2003) The weights from the user model and the scalar values for the attributes in the user model yield an overall utility U for a cluster h, similar to utilities as calculated for individual entities (Edwards and Barron, 1994; Carenini and Moore, 2000): price_range food_quality food_quality medium inexpensive good medium poor Uh = K wk(xhk) (4) (10) (4) k=1 Figure 3: A partial tree for Indian restaurants in London, using price range as the predictor variable and food quality as the dependent variable. The numbers in parentheses are the size of the clusters described by the path from the root. However, we hypothesize that more informative and useful intensional summaries might be constructed from clusters of discovered associations between attributes. For example, associations between price and cuisine produce summ</context>
</contexts>
<marker>Edwards, Barron, 1994</marker>
<rawString>W. Edwards and F. Hutton Barron. 1994. Smarts and smarter: Improved simple methods for multiattribute utility measurement. Organizational Behavior and Human Decision Processes. 60:306– 325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Gaasterland</author>
<author>P Godfrey</author>
<author>J Minker</author>
</authors>
<title>An Overview of Cooperative Answering.</title>
<date>1992</date>
<journal>Journal of Intelligent Information Systems.</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="4761" citStr="Gaasterland et al., 1992" startWordPosition="728" endWordPosition="731">e. We have implemented intensional summaries in CRUISER (Cooperative Responses Using Intensional Summaries of Entities and Relations), a DS for in-car or mobile users to access restaurant information (Becker et al.2006; Weng et al.2005; Weng et al.2006). Figure 1 contrasts our proposed intensional summary strategy with the system initiative strategy used in many dialogue systems (Walker et al., 2002; VXML, 2007). Previous research on cooperative responses has noted that summary strategies should vary according to the context (Sparck Jones, 1993), and the interests and preferences of the user (Gaasterland et al., 1992; Carenini and Moore, 2000; Demberg and Moore, 2006). A number of proposals have emphasized the importance of making generalizations (Kaplan, 1984; Kalita et al., 1986; Joshi et al., 1986). In this paper we explore different methods for constructing intensional summaries and investigate their effectiveness. We present fully automated algorithms for constructing intensional summaries using knowledge discovery techniques (Acar, 2005; Lesh and Mitzenmacher, 2004; Han et al., 1996), and decisiontheoretic user models (Carenini and Moore, 2000). We first explain in Sec. 2 our fully automated, domain</context>
</contexts>
<marker>Gaasterland, Godfrey, Minker, 1992</marker>
<rawString>T. Gaasterland and P. Godfrey and J. Minker. 1992. An Overview of Cooperative Answering. Journal of Intelligent Information Systems. 1(2):387–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Han</author>
<author>Y Huang</author>
<author>N Cercone</author>
</authors>
<title>Intelligent Query Answering by Knowledge Discovery Techniques.</title>
<date>1996</date>
<journal>IEEE Transactions on Knowledge and Data Engineering.</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="5243" citStr="Han et al., 1996" startWordPosition="798" endWordPosition="801">gies should vary according to the context (Sparck Jones, 1993), and the interests and preferences of the user (Gaasterland et al., 1992; Carenini and Moore, 2000; Demberg and Moore, 2006). A number of proposals have emphasized the importance of making generalizations (Kaplan, 1984; Kalita et al., 1986; Joshi et al., 1986). In this paper we explore different methods for constructing intensional summaries and investigate their effectiveness. We present fully automated algorithms for constructing intensional summaries using knowledge discovery techniques (Acar, 2005; Lesh and Mitzenmacher, 2004; Han et al., 1996), and decisiontheoretic user models (Carenini and Moore, 2000). We first explain in Sec. 2 our fully automated, domain-independent algorithm for constructing intensional summaries. Then we evaluate our intensional summary strategy with two experiments. First, in Sec. 3, we test the hypothesis that users prefer summary responses in dialogue systems. We also test a refinement of that hypothesis, i.e., that users prefer summary type responses when they are unfamiliar with a domain. We compare several versions of CRUISER with the system-initiative strategy, exemplified in Fig. 1, and show that use</context>
</contexts>
<marker>Han, Huang, Cercone, 1996</marker>
<rawString>J. Han, Y. Huang and N. Cercone. 1996. Intelligent Query Answering by Knowledge Discovery Techniques. IEEE Transactions on Knowledge and Data Engineering. 8(3):373–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
<author>Ralph M Weischedel</author>
</authors>
<title>Living up to expectations: computing expert responses.</title>
<date>1986</date>
<booktitle>In HLT &apos;86: Proceedings of the workshop on Strategic computing natural language,</booktitle>
<pages>179--189</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="1431" citStr="Joshi et al., 1986" startWordPosition="200" endWordPosition="203">with CRUISER, a DS for in-car or mobile users to access restaurant information. First, we compare CRUISER with a baseline system-initiative DS, and show that users prefer CRUISER. Then, we experiment with four algorithms for constructing intensional summaries in CRUISER, and show that two summary types are equally effective: summaries that maximize domain coverage and summaries that maximize utility with respect to a user model. 1 Introduction Research in cooperative response generation has had a long history and produced many useful algorithms and observations (Mays 1980; Pollack et al.1982; Joshi et al., 1986; Kalita et al., 1986) inter alia. However, it has had little impact on the recent commercialization of dialogue technologies, particularly within the spoken dialogue community. We believe that this lack of interest in cooperative response generation arises from two limitations of previous work: (1) There has been relatively little empirical research showing that cooperative responses lead 479 U1: Tell me about restaurants in London. SI1: What kind of cuisine are you interested in? C1: I know of 596 restaurants in London. I know of 3 inexpensive vegetarian restaurants and 14 inexpensive Chines</context>
<context position="4949" citStr="Joshi et al., 1986" startWordPosition="757" endWordPosition="760">mation (Becker et al.2006; Weng et al.2005; Weng et al.2006). Figure 1 contrasts our proposed intensional summary strategy with the system initiative strategy used in many dialogue systems (Walker et al., 2002; VXML, 2007). Previous research on cooperative responses has noted that summary strategies should vary according to the context (Sparck Jones, 1993), and the interests and preferences of the user (Gaasterland et al., 1992; Carenini and Moore, 2000; Demberg and Moore, 2006). A number of proposals have emphasized the importance of making generalizations (Kaplan, 1984; Kalita et al., 1986; Joshi et al., 1986). In this paper we explore different methods for constructing intensional summaries and investigate their effectiveness. We present fully automated algorithms for constructing intensional summaries using knowledge discovery techniques (Acar, 2005; Lesh and Mitzenmacher, 2004; Han et al., 1996), and decisiontheoretic user models (Carenini and Moore, 2000). We first explain in Sec. 2 our fully automated, domain-independent algorithm for constructing intensional summaries. Then we evaluate our intensional summary strategy with two experiments. First, in Sec. 3, we test the hypothesis that users p</context>
</contexts>
<marker>Joshi, Webber, Weischedel, 1986</marker>
<rawString>Aravind Joshi, Bonnie Webber, and Ralph M. Weischedel. Living up to expectations: computing expert responses. In HLT &apos;86: Proceedings of the workshop on Strategic computing natural language, pages 179–189, Morristown, NJ, USA, 1986. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kalita</author>
<author>M J Colburn</author>
<author>G McCalla</author>
</authors>
<title>A response to the need for summary responses.</title>
<date>1984</date>
<pages>84--432</pages>
<marker>Kalita, Colburn, McCalla, 1984</marker>
<rawString>J. Kalita and M.J. Colburn and G. McCalla. 1984. A response to the need for summary responses. COLING-84). 432–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kamber</author>
<author>L Winstone</author>
<author>W Gong</author>
<author>S Cheng</author>
<author>J Han</author>
</authors>
<title>Generalization and decision tree induction: efficient classification in data mining.</title>
<date>1997</date>
<booktitle>Proc. 7th Int. Workshop on Research Issues in Data Engineering (RIDE &apos;97).</booktitle>
<pages>111--121</pages>
<contexts>
<context position="12338" citStr="Kamber et al., 1997" startWordPosition="1981" endWordPosition="1984">he cluster, the more general the summary. The simplest algorithm for producing clusters utilizes a specified number of the top-ranked attributes to define a cluster. Single attributes, as in the Ref-Sing and UM-Sing examples in Fig. 2, typically produce large clusters. Thus one algorithm uses the top three attributes to produce clusters, defined by either a single value (e.g., UM-Sing) or by the set of values that comprise a significant portion of the total (e.g., Ref-Sing). restaurants that serve Italian cuisine. We apply c4.5 decision tree induction to compute associations among attributes (Kamber et al., 1997; Quinlan, 1993). Each attribute in turn is designated as the dependent variable, with other attributes used as predictors. Thus, each branch in the tree represents a cluster described by the attribute/value pairs that predict the leaf node. Fig. 3 shows clusters of different sizes induced from Indian restaurants in London. The cluster size is determined by the number of attributes used in tree induction. With two attributes, the average cluster size at the leaf node is 60.4, but drops to 4.2 with three attributes. Thus, we use two attributes to produce associative clusters, as shown in Fig. 2</context>
</contexts>
<marker>Kamber, Winstone, Gong, Cheng, Han, 1997</marker>
<rawString>M. Kamber, L. Winstone, W. Gong, S. Cheng and J Han. 1997. Generalization and decision tree induction: efficient classification in data mining. Proc. 7th Int. Workshop on Research Issues in Data Engineering (RIDE &apos;97). 111–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Kaplan</author>
</authors>
<title>Designing a Portable Natural Language Database Query System.</title>
<date>1984</date>
<journal>ACM Transactions on Database Systems,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="2997" citStr="Kaplan, 1984" startWordPosition="459" endWordPosition="460">U3: How about a cheap one? SI3: What neighborhood would you like? C3: I know of 1 inexpensive Chinese restaurant in Hampstead/Kilburn with very good food quality and 1 in Bayswater with good food quality. I also know of 2 in Chinatown with medium food quality. Figure 1: Intensional summaries (C = CRUISER) as compared with a system initiative (SI) strategy in the London restaurant domain. U = User to more natural, effective, or efficient dialogues (Litman et al.1998; Demberg and Moore, 2006); and (2) Previous work has hand-crafted such responses, or hand-annotated the database to support them (Kaplan, 1984; Kalita et al., 1986; Cholvy, 1990; Polifroni et al., 2003; Benamara, 2004), which has made it difficult to port and scale these algorithms. Moreover, we believe that there is an even greater need today for cooperative response generation. Larger and more complex datasets are daily being created on the Web, as information Proceedings of ACL-08: HLT, pages 479–487, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics is integrated across multiple sites and vendors. Many users will want to access this information from a mobile device and will have little knowledge of</context>
<context position="4907" citStr="Kaplan, 1984" startWordPosition="751" endWordPosition="752">le users to access restaurant information (Becker et al.2006; Weng et al.2005; Weng et al.2006). Figure 1 contrasts our proposed intensional summary strategy with the system initiative strategy used in many dialogue systems (Walker et al., 2002; VXML, 2007). Previous research on cooperative responses has noted that summary strategies should vary according to the context (Sparck Jones, 1993), and the interests and preferences of the user (Gaasterland et al., 1992; Carenini and Moore, 2000; Demberg and Moore, 2006). A number of proposals have emphasized the importance of making generalizations (Kaplan, 1984; Kalita et al., 1986; Joshi et al., 1986). In this paper we explore different methods for constructing intensional summaries and investigate their effectiveness. We present fully automated algorithms for constructing intensional summaries using knowledge discovery techniques (Acar, 2005; Lesh and Mitzenmacher, 2004; Han et al., 1996), and decisiontheoretic user models (Carenini and Moore, 2000). We first explain in Sec. 2 our fully automated, domain-independent algorithm for constructing intensional summaries. Then we evaluate our intensional summary strategy with two experiments. First, in S</context>
</contexts>
<marker>Kaplan, 1984</marker>
<rawString>S.J.Kaplan. 1984. Designing a Portable Natural Language Database Query System. ACM Transactions on Database Systems, 9(1):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Lesh</author>
<author>M Mitzenmacher</author>
</authors>
<title>Interactive data summarization: an example application.</title>
<date></date>
<booktitle>Proc., Working Conference on Advanced Visual Interfaces.</booktitle>
<pages>183--187</pages>
<location>Gallipoli,</location>
<marker>Lesh, Mitzenmacher, </marker>
<rawString>N. Lesh and M. Mitzenmacher. Interactive data summarization: an example application. Proc., Working Conference on Advanced Visual Interfaces. Gallipoli, Italy. pages 183–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Shimei Pan</author>
<author>Marilyn A Walker</author>
</authors>
<title>Evaluating response strategies in a webbased spoken dialogue agent.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>780--786</pages>
<marker>Litman, Pan, Walker, 1998</marker>
<rawString>Diane J. Litman, Shimei Pan, and Marilyn A. Walker. Evaluating response strategies in a webbased spoken dialogue agent. In COLING-ACL, pages 780–786, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Polifroni</author>
<author>G Chung</author>
<author>S Seneff</author>
</authors>
<title>Towards the Automatic Generation of Mixed-Initiative Dialogue Systems from Web Content.</title>
<date>2003</date>
<booktitle>Proc. Eurospeech.</booktitle>
<pages>2721--2724</pages>
<contexts>
<context position="3056" citStr="Polifroni et al., 2003" startWordPosition="467" endWordPosition="470"> would you like? C3: I know of 1 inexpensive Chinese restaurant in Hampstead/Kilburn with very good food quality and 1 in Bayswater with good food quality. I also know of 2 in Chinatown with medium food quality. Figure 1: Intensional summaries (C = CRUISER) as compared with a system initiative (SI) strategy in the London restaurant domain. U = User to more natural, effective, or efficient dialogues (Litman et al.1998; Demberg and Moore, 2006); and (2) Previous work has hand-crafted such responses, or hand-annotated the database to support them (Kaplan, 1984; Kalita et al., 1986; Cholvy, 1990; Polifroni et al., 2003; Benamara, 2004), which has made it difficult to port and scale these algorithms. Moreover, we believe that there is an even greater need today for cooperative response generation. Larger and more complex datasets are daily being created on the Web, as information Proceedings of ACL-08: HLT, pages 479–487, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics is integrated across multiple sites and vendors. Many users will want to access this information from a mobile device and will have little knowledge of the domain. We hypothesize that these users will need coop</context>
<context position="10310" citStr="Polifroni et al., 2003" startWordPosition="1647" endWordPosition="1650">ality, price, service, and neighborhood. These are ranked using the SMARTER procedure (Edwards and Barron, 1994). Rankings are converted to weights (w) for each attribute, with a formula which guarantees that the weights sum to 1: 1 K 1 wk = K i=k i where K equals the number of attributes in the ranking. The absolute rankings are used to select attributes. The weights are also used for cluster scoring in Sec. 2.3. User model ranking is used to produce UM-Sing and UM-Assoc in Fig. 2. Refiner method: The second attribute ranking method is based on the Refiner algorithm for summary construction (Polifroni et al., 2003). The Refiner returns values for every attribute in the focal information in frames ordered by frequency. If the counts for the top-N (typically, 4) values for a particular attribute, e.g., cuisine, exceeded M% (typically 80%) of the total counts for all values, then that attribute is selected. For example, 82% of Indian restaurants in the London database are in the neighborhoods Mayfair, Soho, and Chelsea. Neighborhood would, therefore, be chosen as an attribute to speak about for Indian restaurants. The thresholds M and N in the original Refiner were set a priori, so it was possible that no </context>
<context position="13430" citStr="Polifroni et al., 2003" startWordPosition="2158" endWordPosition="2161">de is 60.4, but drops to 4.2 with three attributes. Thus, we use two attributes to produce associative clusters, as shown in Fig. 2 (i.e., the Ref-Assoc and UMAssoc responses), to favor larger clusters. 2.3 Cluster Scoring The final parameter scores the clusters. One scoring metric is based on cluster size. Single attributes produce large clusters, while association rules produce smaller clusters. The second scoring method selects clusters of high utility according to a user model. We first assign scalar values to the six ranked attributes (Sec. 2.1), using clustering methods as described in (Polifroni et al., 2003) The weights from the user model and the scalar values for the attributes in the user model yield an overall utility U for a cluster h, similar to utilities as calculated for individual entities (Edwards and Barron, 1994; Carenini and Moore, 2000): price_range food_quality food_quality medium inexpensive good medium poor Uh = K wk(xhk) (4) (10) (4) k=1 Figure 3: A partial tree for Indian restaurants in London, using price range as the predictor variable and food quality as the dependent variable. The numbers in parentheses are the size of the clusters described by the path from the root. Howev</context>
<context position="28765" citStr="Polifroni et al., 2003" startWordPosition="4654" endWordPosition="4657">as a user model (UMT). The personas were designed to reflect differing ends of the spectra defined by Belkin to characterize informationseeking strategies (Belkin et al., 1994). There was a significant preference for intensional summaries across all personas, but especially when the personas were unfamiliar with the domain. This preference indicates that the benefits of intensional summaries outweigh the increase in verbosity. We then tested four algorithms for summary construction. Results show that intensional summaries based on a user model with association rules, or on the Refiner method (Polifroni et al., 2003), are equally effective. While (Demberg and Moore, 2006) found that their user model stepwise refinement (UMSR) method was superior to the Refiner method, they also found many situations (70 out of 190) in which the Refiner method was preferred. Our experiment was structured differently, but it suggests that, in certain circumstances, or within certain domains, users may wish to hear about choices based on an analysis of focal information, irrespective of user preferences. Our intensional summary algorithms automatically construct summaries from a database, along with user models collected via</context>
</contexts>
<marker>Polifroni, Chung, Seneff, 2003</marker>
<rawString>J. Polifroni, G. Chung, and S. Seneff. 2003. Towards the Automatic Generation of Mixed-Initiative Dialogue Systems from Web Content. Proc. Eurospeech. 2721–2724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mays</author>
</authors>
<title>Correcting misconceptions about database structure.</title>
<date>1980</date>
<booktitle>In Proceedings of the CSCSI &apos;80,</booktitle>
<contexts>
<context position="1391" citStr="Mays 1980" startWordPosition="194" endWordPosition="195">ypothesis with two experiments with CRUISER, a DS for in-car or mobile users to access restaurant information. First, we compare CRUISER with a baseline system-initiative DS, and show that users prefer CRUISER. Then, we experiment with four algorithms for constructing intensional summaries in CRUISER, and show that two summary types are equally effective: summaries that maximize domain coverage and summaries that maximize utility with respect to a user model. 1 Introduction Research in cooperative response generation has had a long history and produced many useful algorithms and observations (Mays 1980; Pollack et al.1982; Joshi et al., 1986; Kalita et al., 1986) inter alia. However, it has had little impact on the recent commercialization of dialogue technologies, particularly within the spoken dialogue community. We believe that this lack of interest in cooperative response generation arises from two limitations of previous work: (1) There has been relatively little empirical research showing that cooperative responses lead 479 U1: Tell me about restaurants in London. SI1: What kind of cuisine are you interested in? C1: I know of 596 restaurants in London. I know of 3 inexpensive vegetari</context>
</contexts>
<marker>Mays, 1980</marker>
<rawString>E. Mays. Correcting misconceptions about database structure. In Proceedings of the CSCSI &apos;80, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha E Pollack</author>
<author>Julia Hirschberg</author>
<author>Bonnie L Webber</author>
</authors>
<title>User participation in the reasoning processes of expert systems.</title>
<date>1982</date>
<booktitle>In AAAI,</booktitle>
<pages>358--361</pages>
<marker>Pollack, Hirschberg, Webber, 1982</marker>
<rawString>Martha E. Pollack, Julia Hirschberg, and Bonnie L. Webber. User participation in the reasoning processes of expert systems. In AAAI, pages 358–361, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="12354" citStr="Quinlan, 1993" startWordPosition="1985" endWordPosition="1986">general the summary. The simplest algorithm for producing clusters utilizes a specified number of the top-ranked attributes to define a cluster. Single attributes, as in the Ref-Sing and UM-Sing examples in Fig. 2, typically produce large clusters. Thus one algorithm uses the top three attributes to produce clusters, defined by either a single value (e.g., UM-Sing) or by the set of values that comprise a significant portion of the total (e.g., Ref-Sing). restaurants that serve Italian cuisine. We apply c4.5 decision tree induction to compute associations among attributes (Kamber et al., 1997; Quinlan, 1993). Each attribute in turn is designated as the dependent variable, with other attributes used as predictors. Thus, each branch in the tree represents a cluster described by the attribute/value pairs that predict the leaf node. Fig. 3 shows clusters of different sizes induced from Indian restaurants in London. The cluster size is determined by the number of attributes used in tree induction. With two attributes, the average cluster size at the leaf node is 60.4, but drops to 4.2 with three attributes. Thus, we use two attributes to produce associative clusters, as shown in Fig. 2 (i.e., the Ref-</context>
<context position="23580" citStr="Quinlan, 1993" startWordPosition="3827" endWordPosition="3828">where the Refiner responses have high utility scores. 4.2 Summary Type Prediction Our experimental data suggest that characteristics associated with the set of restaurants being described are important, as well as utility information derived from application of a a user model. The performance of a classifier in predicting summary type will indicate if trends we discovered among user judgements carry over to an automated means of selecting which response type to use in a given context. In a final experiment, for each task, we use the highest rated summary as a class to be predicted using C4.5 (Quinlan, 1993). Thus we have 4 classes: Ref-Sing, Ref-Assoc, UM-Sing, and UM-Assoc. We derive two types of feature sets from the responses: features derived from each user model and features derived from attributes of the query/response pair itself. The five feature sets for the user model are: • umInfo: 6 features for the rankings for each attribute for each user’s model, e.g. a summary whose user had rated food quality most highly would receive a &apos;5&apos; for the feature food quality; • avgUtility: 4 features representing an average utility score for each alternative summary response, based on its clusters (Se</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.R. Quinlan 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann. San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
</authors>
<title>Automatic summarising: factors and directions.</title>
<date>1998</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<editor>I. Mani and M. Maybury, eds.</editor>
<publisher>MIT Press.</publisher>
<marker>Jones, 1998</marker>
<rawString>K. Sparck Jones. 1998. Automatic summarising: factors and directions. I. Mani and M. Maybury, eds. Advances in Automatic Text Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>A Rudnicky</author>
<author>J Aberdeen</author>
<author>E Bratt</author>
<author>J Garofolo</author>
<author>H Hastie</author>
<author>A Le</author>
<author>B Pellom</author>
<author>A Potamianos</author>
<author>R Passonneau</author>
<author>R Prasad</author>
<author>S Roukos</author>
<author>G Sanders</author>
<author>S Seneff</author>
<author>D Stallard</author>
</authors>
<title>DARPA Communicator Evaluation: Progress from</title>
<date>2002</date>
<location>Proc, ICSLP</location>
<note>to</note>
<contexts>
<context position="4539" citStr="Walker et al., 2002" startWordPosition="695" endWordPosition="698">r when users are unfamiliar with the domain. These intensional summaries have the ability to describe the data that forms the knowledge base of the system, as well as relationships among the components of that database. We have implemented intensional summaries in CRUISER (Cooperative Responses Using Intensional Summaries of Entities and Relations), a DS for in-car or mobile users to access restaurant information (Becker et al.2006; Weng et al.2005; Weng et al.2006). Figure 1 contrasts our proposed intensional summary strategy with the system initiative strategy used in many dialogue systems (Walker et al., 2002; VXML, 2007). Previous research on cooperative responses has noted that summary strategies should vary according to the context (Sparck Jones, 1993), and the interests and preferences of the user (Gaasterland et al., 1992; Carenini and Moore, 2000; Demberg and Moore, 2006). A number of proposals have emphasized the importance of making generalizations (Kaplan, 1984; Kalita et al., 1986; Joshi et al., 1986). In this paper we explore different methods for constructing intensional summaries and investigate their effectiveness. We present fully automated algorithms for constructing intensional su</context>
</contexts>
<marker>Walker, Rudnicky, Aberdeen, Bratt, Garofolo, Hastie, Le, Pellom, Potamianos, Passonneau, Prasad, Roukos, Sanders, Seneff, Stallard, 2002</marker>
<rawString>M. Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom, A. Potamianos, R. Passonneau, R. Prasad, S. Roukos, G. Sanders, S. Seneff and D. Stallard. 2002. DARPA Communicator Evaluation: Progress from 2000 to 2001. Proc, ICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Weng</author>
<author>L Cavedon</author>
<author>B Raghunathan</author>
<author>D Mirkovic</author>
<author>H Cheng</author>
<author>H Schmidt</author>
<author>H Bratt</author>
<author>R Mishra</author>
<author>S Peters</author>
<author>L Zhao</author>
<author>S Upson</author>
<author>E Shriberg</author>
<author>C Bergmann</author>
</authors>
<title>Developing a conversational dialogue system for cognitively overloaded drivers.</title>
<date>2005</date>
<booktitle>In Proceedings, International Congress on Intelligent Transportation Systems,</booktitle>
<marker>Weng, Cavedon, Raghunathan, Mirkovic, Cheng, Schmidt, Bratt, Mishra, Peters, Zhao, Upson, Shriberg, Bergmann, 2005</marker>
<rawString>F. Weng, L. Cavedon, B. Raghunathan, D. Mirkovic, H. Cheng, H. Schmidt, H. Bratt, R. Mishra, S. Peters, L. Zhao, S. Upson, E. Shriberg, and C. Bergmann. Developing a conversational dialogue system for cognitively overloaded drivers. In Proceedings, International Congress on Intelligent Transportation Systems, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Weng</author>
<author>S Varges</author>
<author>B Raghunathan</author>
<author>F Ratiu</author>
<author>H Pon-Barry</author>
<author>B Lathrop</author>
<author>Q Zhang</author>
<author>T Scheideck</author>
<author>H Bratt</author>
<author>K Xu</author>
<author>M Purver</author>
<author>R Mishra</author>
<author>M Raya</author>
<author>S Peters</author>
<author>Y Meng</author>
<author>L Cavedon</author>
<author>L Shriberg</author>
</authors>
<title>Chat: A conversational helper for automotive tasks.</title>
<date>2006</date>
<booktitle>In Proceedings, Interspeech: International Conference on Spoken Language Processing,</booktitle>
<marker>Weng, Varges, Raghunathan, Ratiu, Pon-Barry, Lathrop, Zhang, Scheideck, Bratt, Xu, Purver, Mishra, Raya, Peters, Meng, Cavedon, Shriberg, 2006</marker>
<rawString>F. Weng, S. Varges, B. Raghunathan, F. Ratiu, H. Pon-Barry, B. Lathrop, Q. Zhang, T. Scheideck, H. Bratt, K. Xu, M. Purver, R. Mishra, M. Raya, S. Peters, Y. Meng, L. Cavedon, and L. Shriberg. Chat: A conversational helper for automotive tasks. In Proceedings, Interspeech: International Conference on Spoken Language Processing, 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Voxeo</author>
</authors>
<title>VoiceXML Development Guide.</title>
<note>http://voicexml.org.</note>
<marker>Voxeo, </marker>
<rawString>Voxeo. VoiceXML Development Guide. http://voicexml.org.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>