<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000175">
<title confidence="0.986125">
Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
</title>
<author confidence="0.975461">
Aria Haghighi and Dan Klein
</author>
<affiliation confidence="0.963923">
Computer Science Division
</affiliation>
<address confidence="0.764333">
UC Berkeley
</address>
<email confidence="0.997353">
{aria42, klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874428571428">
We present an unsupervised, nonparamet-
ric Bayesian approach to coreference reso-
lution which models both global entity iden-
tity across a corpus as well as the sequen-
tial anaphoric structure within each docu-
ment. While most existing coreference work
is driven by pairwise decisions, our model
is fully generative, producing each mention
from a combination of global entity proper-
ties and local attentional state. Despite be-
ing unsupervised, our system achieves a 70.3
MUC F1 measure on the MUC-6 test set,
broadly in the range of some recent super-
vised results.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999501192307692">
Referring to an entity in natural language can
broadly be decomposed into two processes. First,
speakers directly introduce new entities into dis-
course, entities which may be shared across dis-
courses. This initial reference is typically accom-
plished with proper or nominal expressions. Second,
speakers refer back to entities already introduced.
This anaphoric reference is canonically, though of
course not always, accomplished with pronouns, and
is governed by linguistic and cognitive constraints.
In this paper, we present a nonparametric generative
model of a document corpus which naturally con-
nects these two processes.
Most recent coreference resolution work has fo-
cused on the task of deciding which mentions (noun
phrases) in a document are coreferent. The domi-
nant approach is to decompose the task into a col-
lection of pairwise coreference decisions. One then
applies discriminative learning methods to pairs of
mentions, using features which encode properties
such as distance, syntactic environment, and so on
(Soon et al., 2001; Ng and Cardie, 2002). Although
such approaches have been successful, they have
several liabilities. First, rich features require plen-
tiful labeled data, which we do not have for corefer-
ence tasks in most domains and languages. Second,
coreference is inherently a clustering or partitioning
task. Naive pairwise methods can and do fail to pro-
duce coherent partitions. One classic solution is to
make greedy left-to-right linkage decisions. Recent
work has addressed this issue in more global ways.
McCallum and Wellner (2004) use graph partion-
ing in order to reconcile pairwise scores into a final
coherent clustering. Nonetheless, all these systems
crucially rely on pairwise models because cluster-
level models are much harder to work with, combi-
natorially, in discriminative approaches.
Another thread of coreference work has focused
on the problem of identifying matches between
documents (Milch et al., 2005; Bhattacharya and
Getoor, 2006; Daume and Marcu, 2005). These
methods ignore the sequential anaphoric structure
inside documents, but construct models of how and
when entities are shared between them.1 These
models, as ours, are generative ones, since the fo-
cus is on cluster discovery and the data is generally
unlabeled.
In this paper, we present a novel, fully genera-
tive, nonparametric Bayesian model of mentions in a
document corpus. Our model captures both within-
and cross-document coreference. At the top, a hi-
erarchical Dirichlet process (Teh et al., 2006) cap-
</bodyText>
<footnote confidence="0.6978935">
1Milch et al. (2005) works with citations rather than dis-
courses and does model the linear structure of the citations.
</footnote>
<page confidence="0.884026">
848
</page>
<note confidence="0.9274745">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848–855,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999361333333333">
tures cross-document entity (and parameter) shar-
ing, while, at the bottom, a sequential model of
salience captures within-document sequential struc-
ture. As a joint model of several kinds of discourse
variables, it can be used to make predictions about
either kind of coreference, though we focus experi-
mentally on within-document measures. To the best
of our ability to compare, our model achieves the
best unsupervised coreference performance.
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="method">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99986427027027">
We adopt the terminology of the Automatic Context
Extraction (ACE) task (NIST, 2004). For this paper,
we assume that each document in a corpus consists
of a set of mentions, typically noun phrases. Each
mention is a reference to some entity in the domain
of discourse. The coreference resolution task is to
partition the mentions according to referent. Men-
tions can be divided into three categories, proper
mentions (names), nominal mentions (descriptions),
and pronominal mentions (pronouns).
In section 3, we present a sequence of increas-
ingly enriched models, motivating each from short-
comings of the previous. As we go, we will indicate
the performance of each model on data from ACE
2004 (NIST, 2004). In particular, we used as our
development corpus the English translations of the
Arabic and Chinese treebanks, comprising 95 docu-
ments and about 3,905 mentions. This data was used
heavily for model design and hyperparameter selec-
tion. In section 5, we present final results for new
test data from MUC-6 on which no tuning or devel-
opment was performed. This test data will form our
basis for comparison to previous work.
In all experiments, as is common, we will assume
that we have been given as part of our input the true
mention boundaries, the head word of each mention
and the mention type (proper, nominal, or pronom-
inal). For the ACE data sets, the head and mention
type are given as part of the mention annotation. For
the MUC data, the head was crudely chosen to be
the rightmost mention token, and the mention type
was automatically detected. We will not assume
any other information to be present in the data be-
yond the text itself. In particular, unlike much re-
lated work, we do not assume gold named entity
recognition (NER) labels; indeed we do not assume
observed NER labels or POS tags at all. Our pri-
</bodyText>
<figureCaption confidence="0.907242">
Figure 1: Graphical model depiction of document level en-
tity models described in sections 3.1 and 3.2 respectively. The
shaded nodes indicate observed variables.
</figureCaption>
<bodyText confidence="0.9996532">
mary performance metric will be the MUC F1 mea-
sure (Vilain et al., 1995), commonly used to evalu-
ate coreference systems on a within-document basis.
Since our system relies on sampling, all results are
averaged over five random runs.
</bodyText>
<sectionHeader confidence="0.985777" genericHeader="method">
3 Coreference Resolution Models
</sectionHeader>
<bodyText confidence="0.999967066666667">
In this section, we present a sequence of gener-
ative coreference resolution models for document
corpora. All are essentially mixture models, where
the mixture components correspond to entities. As
far as notation, we assume a collection of I docu-
ments, each with Ji mentions. We use random vari-
ables Z to refer to (indices of) entities. We will use
0z to denote the parameters for an entity z, and 0
to refer to the concatenation of all such 0z. X will
refer somewhat loosely to the collection of variables
associated with a mention in our model (such as the
head or gender). We will be explicit about X and 0z
shortly.
Our goal will be to find the setting of the entity
indices which maximize the posterior probability:
</bodyText>
<equation confidence="0.9986776">
P(ZJX) = arg max P(Z,X)
Z
J
= arg max P(Z, X, 0) dP(0)
Z
</equation>
<bodyText confidence="0.996999333333333">
where Z, X, and 0 denote all the entity indices, ob-
served values, and parameters of the model. Note
that we take a Bayesian approach in which all pa-
rameters are integrated out (or sampled). The infer-
ence task is thus primarily a search problem over the
index labels Z.
</bodyText>
<figure confidence="0.943720842105263">
(a) (b)
H;
Zi
a
α
J
∞
�
∞
I
α
a
K
0 K
H;
J
I
Z* = arg max
Z
</figure>
<page confidence="0.905332">
849
</page>
<bodyText confidence="0.5551745">
The Weir Group1, whose2 headquarters3 is in the US4, is a large, specialized corporation5 investing in the area of electricity
generation. This power plant6, which7 will be situated in Rudong8, Jiangsu9, has an annual generation capacity of 2.4 million kilowatts.
The Weir Group1, whose1 headquarters2 is in the US3, is a large, specialized corporation4 investing in the area of electricity
generation. This power plant5, which1 will be situated in Rudong6, Jiangsu7, has an annual generation capacity of 2.4 million kilowatts.
The Weir Group1, whose1 headquarters2 is in the US3, is a large, specialized corporation4 investing in the area of electricity
generation. This power plant5, which5 will be situated in Rudong6, Jiangsu7, has an annual generation capacity of 2.4 million kilowatts.
</bodyText>
<figureCaption confidence="0.98384475">
Figure 2: Example output from various models. The output from (a) is from the infinite mixture model of section 3.2. It incorrectly
labels both boxed cases of anaphora. The output from (b) uses the pronoun head model of section 3.3. It correctly labels the first
case of anaphora but incorrectly labels the second pronominal as being coreferent with the dominant document entity The Weir
Group. This error is fixed by adding the salience feature component from section 3.4 as can be seen in (c).
</figureCaption>
<subsectionHeader confidence="0.999354">
3.1 A Finite Mixture Model
</subsectionHeader>
<bodyText confidence="0.99999">
Our first, overly simplistic, corpus model is the stan-
dard finite mixture of multinomials shown in fig-
ure 1(a). In this model, each document is indepen-
dent save for some global hyperparameters. Inside
each document, there is a finite mixture model with
a fixed number K of components. The distribution Q
over components (entities) is a draw from a symmet-
ric Dirichlet distribution with concentration α. For
each mention in the document, we choose a compo-
nent (an entity index) z from Q. Entity z is then asso-
ciated with a multinomial emission distribution over
head words with parameters Oh, which are drawn
from a symmetric Dirichlet over possible mention
heads with concentration AH.2 Note that here the X
for a mention consists only of the mention head H.
As we enrich our models, we simultaneously de-
velop an accompanying Gibbs sampling procedure
to obtain samples from P(Z|X).3 For now, all heads
H are observed and all parameters (Q and 0) can be
integrated out analytically: for details see Teh et al.
(2006). The only sampling is for the values of Zi,j,
the entity index of mention j in document i. The
relevant conditional distribution is:4
</bodyText>
<equation confidence="0.944099">
P(Zi,j|Z−i,j, H) ∝ P(Zi,j|Z−i,j)P(Hi,j|Z, H−i,j)
</equation>
<bodyText confidence="0.989496333333333">
where Hi,j is the head of mention j in document i.
Expanding each term, we have the contribution of
the prior:
</bodyText>
<equation confidence="0.980331">
P(Zi,j = z|Z−i,j) ∝ nz + α
</equation>
<bodyText confidence="0.5799995">
2In general, we will use a subscripted A to indicate concen-
tration for finite Dirichlet distributions. Unless otherwise spec-
ified, A concentration parameters will be set to a−4 and omitted
from diagrams.
3One could use the EM algorithm with this model, but EM
will not extend effectively to the subsequent models.
4Here, Z−i,j denotes Z − {Zi,j}
where nz is the number of elements of Z−i,j with
entity index z. Similarly we have for the contribu-
tion of the emissions:
</bodyText>
<equation confidence="0.982826">
P(Hi,j = h|Z, H−i,j) ∝ nh,z + AH
</equation>
<bodyText confidence="0.9996925">
where nh,z is the number of times we have seen head
h associated with entity index z in (Z, H−i,j).
</bodyText>
<subsectionHeader confidence="0.999692">
3.2 An Infinite Mixture Model
</subsectionHeader>
<bodyText confidence="0.99682995">
A clear drawback of the finite mixture model is the
requirement that we specify a priori a number of en-
tities K for a document. We would like our model
to select K in an effective, principled way. A mech-
anism for doing so is to replace the finite Dirichlet
prior on Q with the non-parametric Dirichlet process
(DP) prior (Ferguson, 1973).5 Doing so gives the
model in figure 1(b). Note that we now list an in-
finite number of mixture components in this model
since there can be an unbounded number of entities.
Rather than a finite Q with a symmetric Dirichlet
distribution, in which draws tend to have balanced
clusters, we now have an infinite Q. However, most
draws will have weights which decay exponentially
quickly in the prior (though not necessarily in the
posterior). Therefore, there is a natural penalty for
each cluster which is actually used.
With Z observed during sampling, we can inte-
grate out Q and calculate P(Zi,j|Z−i,j) analytically,
using the Chinese restaurant process representation:
</bodyText>
<equation confidence="0.919185">
�
α, if z = znew
P (Zi,j = z|Z−i,j) ∝ nz, otherwise (1)
</equation>
<bodyText confidence="0.996331">
where znew is a new entity index not used in Z−i,j
and nz is the number of mentions that have entity in-
dex z. Aside from this change, sampling is identical
</bodyText>
<footnote confidence="0.9498835">
5We do not give a detailed presentation of the Dirichlet pro-
cess here, but see Teh et al. (2006) for a presentation.
</footnote>
<page confidence="0.991264">
850
</page>
<figure confidence="0.998954">
(a) (b)
</figure>
<figureCaption confidence="0.978038">
Figure 3: (a) An entity and its parameters. (b)The head model
described in section 3.3. The shaded nodes indicate observed
variables. The mention type determines which set of parents are
used. The dependence of mention variable on entity parameters
0 and pronoun head model 9 is omitted.
</figureCaption>
<bodyText confidence="0.9998338">
to the finite mixture case, though with the number
of clusters actually occupied in each sample drifting
upwards or downwards.
This model yielded a 54.5 Fi on our develop-
ment data.6 This model is, however, hopelessly
crude, capturing nothing of the structure of coref-
erence. Its largest empirical problem is that, un-
surprisingly, pronoun mentions such as he are given
their own clusters, not labeled as coreferent with any
non-pronominal mention (see figure 2(a)).
</bodyText>
<subsectionHeader confidence="0.996802">
3.3 Pronoun Head Model
</subsectionHeader>
<bodyText confidence="0.999987294117647">
While an entity-specific multinomial distribution
over heads makes sense for proper, and some nom-
inal, mention heads, it does not make sense to gen-
erate pronominal mentions this same way. I.e., all
entities can be referred to by generic pronouns, the
choice of which depends on entity properties such as
gender, not the specific entity.
We therefore enrich an entity’s parameters 0 to
contain not only a distribution over lexical heads
0h, but also distributions (0t, 09, 0n) over proper-
ties, where 0t parametrizes a distribution over en-
tity types (PER, LOC, ORG, MISC), and 09 for gen-
der (MALE, FEMALE, NEUTER), and 0n for number
(SG, PL).7 We assume each of these property distri-
butions is drawn from a symmetric Dirichlet distri-
bution with small concentration parameter in order
to encourage a peaked posterior distribution.
</bodyText>
<footnote confidence="0.987218">
6See section 4 for inference details.
7It might seem that entities should simply have, for exam-
ple, a gender g rather than a distribution over genders 09. There
are two reasons to adopt the softer approach. First, one can
rationalize it in principle, for entities like cars or ships whose
grammatical gender is not deterministic. However, the real rea-
son is that inference is simplified. In any event, we found these
property distributions to be highly determinized in the posterior.
</footnote>
<figureCaption confidence="0.865182285714286">
Figure 4: Coreference model at the document level with entity
properties as well salience lists used for mention type distri-
butions. The diamond nodes indicate deterministic functions.
Shaded nodes indicate observed variables. Although it appears
that each mention head node has many parents, for a given men-
tion type, the mention head depends on only a small subset. De-
pendencies involving parameters 0 and 9 are omitted.
</figureCaption>
<bodyText confidence="0.999925157894737">
Previously, when an entity z generated a mention,
it drew a head word from 0hz . It now undergoes a
more complex and structured process. It first draws
an entity type T, a gender G, a number N from the
distributions 0t, 09, and 0n, respectively. Once the
properties are fetched, a mention type M is chosen
(proper, nominal, pronoun), according to a global
multinomial (again with a symmetric Dirichlet prior
and parameter AM). This corresponds to the (tem-
porary) assumption that the speaker makes a random
i.i.d. choice for the type of each mention.
Our head model will then generate a head, con-
ditioning on the entity, its properties, and the men-
tion type, as shown in figure 3(b). If M is not a
pronoun, the head is drawn directly from the en-
tity head multinomial with parameters 0hz . Other-
wise, it is drawn based on a global pronoun head dis-
tribution, conditioning on the entity properties and
parametrized by 9. Formally, it is given by:
</bodyText>
<equation confidence="0.94144675">
P(HIZ,T,G,N,M,0,9) =
�
P(HjT, G, N, 9), if M =PRO
P(Hj0h�), otherwise
</equation>
<bodyText confidence="0.996329571428571">
Although we can observe the number and gen-
der draws for some mentions, like personal pro-
nouns, there are some for which properties aren’t
observed (e.g., it). Because the entity prop-
erty draws are not (all) observed, we must now
sample the unobserved ones as well as the en-
tity indices Z. For instance, we could sample
</bodyText>
<figure confidence="0.974350825">
X=
Z Z
M T N G
H
B
0.
L1
M1 =
NAM
S1
T1 N1 G1
Zi r Z3
H1 =
&amp;quot;Bush&amp;quot;
L2
M2 =
NOM
S2
T2
H2 =
&amp;quot;president&amp;quot;
α
P g
∞ ∞
N2 G2
L3
M3 =
PRO
S3
T2
H3 =
&amp;quot;he&amp;quot;
B
N2 =
SG
G2 =
MALE
I
�1
Entity Type
</figure>
<table confidence="0.9671">
PERS : 0.97, LOC : 0.01, ORG: 0.01, MISC: 0.01
Gender �9
MALE: 0.98, FEM: 0.01, NEUTER: 0.01
Number �&amp;quot;
SING: 0.99, PLURAL: 0.01
Head �h
Bush : 0.90, President : 0.06, .....
851
Salience Feature Pronoun Proper Nominal
TOP 0.75 0.17 0.08
HIGH 0.55 0.28 0.17
MID 0.39 0.40 0.21
LOW 0.20 0.45 0.35
NONE 0.00 0.88 0.12
</table>
<tableCaption confidence="0.99391125">
Table 1: Posterior distribution of mention type given salience
by bucketing entity activation rank. Pronouns are preferred for
entities which have high salience and non-pronominal mentions
are preferred for inactive entities.
</tableCaption>
<bodyText confidence="0.999930884615385">
Ti,j, the entity type of pronominal mention j in
document i, using, P(Ti,j|Z, N, G, H, T−i,j) ∝
P(Ti,j|Z)P(Hi,j|T, N, G, H), where the posterior
distributions on the right hand side are straight-
forward because the parameter priors are all finite
Dirichlet. Sampling G and N are identical.
Of course we have prior knowledge about the re-
lationship between entity type and pronoun head
choice. For example, we expect that he is used for
mentions with T = PERSON. In general, we assume
that for each pronominal head we have a list of com-
patible entity types, which we encode via the prior
on 0. We assume 0 is drawn from a Dirichlet distri-
bution where each pronoun head is given a synthetic
count of (1 + AP) for each (t, g, n) where t is com-
patible with the pronoun and given AP otherwise.
So, while it will be possible in the posterior to use
he to refer to a non-person, it will be biased towards
being used with persons.
This model gives substantially improved predic-
tions: 64.1 Fi on our development data. As can be
seen in figure 2(b), this model does correct the sys-
tematic problem of pronouns being considered their
own entities. However, it still does not have a pref-
erence for associating pronominal references to en-
tities which are in any way local.
</bodyText>
<subsectionHeader confidence="0.997897">
3.4 Adding Salience
</subsectionHeader>
<bodyText confidence="0.999982275862069">
We would like our model to capture how mention
types are generated for a given entity in a robust and
somewhat language independent way. The choice of
entities may reasonably be considered to be indepen-
dent given the mixing weights Q, but how we realize
an entity is strongly dependent on context (Ge et al.,
1998).
In order to capture this in our model, we enrich
it as shown in figure 4. As we proceed through a
document, generating entities and their mentions,
we maintain a list of the active entities and their
saliences, or activity scores. Every time an entity is
mentioned, we increment its activity score by 1, and
every time we move to generate the next mention,
all activity scores decay by a constant factor of 0.5.
This gives rise to an ordered list of entity activations,
L, where the rank of an entity decays exponentially
as new mentions are generated. We call this list a
salience list. Given a salience list, L, each possible
entity z has some rank on this list. We discretize
these ranks into five buckets S: TOP (1), HIGH (2-
3), MID (4-6), LOW (7+), and NONE. Given the entity
choices Z, both the list L and buckets S are deter-
ministic (see figure 4). We assume that the mention
type M is conditioned on S as shown in figure 4.
We note that correctly sampling an entity now re-
quires that we incorporate terms for how a change
will affect all future salience values. This changes
our sampling equation for existing entities:
</bodyText>
<equation confidence="0.973968">
�P(Zi,j = z|Z−i,j) ∝ nz P(Mi,j,|Si,j,, Z) (2)
j,&gt;j
</equation>
<bodyText confidence="0.999988916666667">
where the product ranges over future mentions in the
document and Si,j, is the value of future salience
feature given the setting of all entities, including set-
ting the current entity Zi,j to z. A similar equation
holds for sampling a new entity. Note that, as dis-
cussed below, this full product can be truncated as
an approximation.
This model gives a 71.5 Fi on our development
data. Table 1 shows the posterior distribution of the
mention type given the salience feature. This model
fixes many anaphora errors and in particular fixes the
second anaphora error in figure 2(c).
</bodyText>
<subsectionHeader confidence="0.971449">
3.5 Cross Document Coreference
</subsectionHeader>
<bodyText confidence="0.9999514">
One advantage of a fully generative approach is that
we can allow entities to be shared between docu-
ments in a principled way, giving us the capacity to
do cross-document coreference. Moreover, sharing
across documents pools information about the prop-
erties of an entity across documents.
We can easily link entities across a corpus by as-
suming that the pool of entities is global, with global
mixing weights Qo drawn from a DP prior with
concentration parameter &apos;y. Each document uses
</bodyText>
<page confidence="0.997427">
852
</page>
<figureCaption confidence="0.99826625">
Figure 5: Graphical depiction of the HDP coreference model
described in section 3.5. The dependencies between the global
entity parameters 0 and pronoun head parameters 0 on the men-
tion observations are not depicted.
</figureCaption>
<bodyText confidence="0.999967882352941">
the same global entities, but each has a document-
specific distribution Qi drawn from a DP centered on
Q0 with concentration parameter α. Up to the point
where entities are chosen, this formulation follows
the basic hierarchical Dirichlet process prior of Teh
et al. (2006). Once the entities are chosen, our model
for the realization of the mentions is as before. This
model is depicted graphically in figure 5.
Although it is possible to integrate out Q0 as we
did the individual Qi, we instead choose for ef-
ficiency and simplicity to sample the global mix-
ture distribution Q0 from the posterior distribution
P(Q0|Z).8 The mention generation terms in the
model and sampler are unchanged.
In the full hierarchical model, our equation (1) for
sampling entities, ignoring the salience component
of section 3.4, becomes:
</bodyText>
<equation confidence="0.86446125">
�
αQu 0, if z = znew
P(Zi,j = z|Z−i,j, Q0)�
nz + αQz0, otherwise
</equation>
<bodyText confidence="0.998626222222222">
where Qz0 is the probability of the entity z under the
sampled global entity distribution and Qu0 is the un-
known component mass of this distribution.
The HDP layer of sharing improves the model’s
predictions to 72.5 F1 on our development data. We
should emphasize that our evaluation is of course
per-document and does not reflect cross-document
coreference decisions, only the gains through cross-
document sharing (see section 6.2).
</bodyText>
<footnote confidence="0.928865">
8We do not give the details here; see Teh et al. (2006) for de-
tails on how to implement this component of the sampler (called
“direct assignment” in that reference).
</footnote>
<sectionHeader confidence="0.995457" genericHeader="method">
4 Inference Details
</sectionHeader>
<bodyText confidence="0.99989525">
Up until now, we’ve discussed Gibbs sampling, but
we are not interested in sampling from the poste-
rior P(Z|X), but in finding its mode. Instead of
sampling directly from the posterior distribution, we
instead sample entities proportionally to exponen-
tiated entity posteriors. The exponent is given by
exp ci
k−1, where i is the current round number (start-
ing at i = 0), c = 1.5 and k = 20 is the total num-
ber of sampling epochs. This slowly raises the pos-
terior exponent from 1.0 to ec. In our experiments,
we found this procedure to outperform simulated an-
nealing. We also found sampling the T, G, and N
variables to be particularly inefficient, so instead we
maintain soft counts over each of these variables and
use these in place of a hard sampling scheme. We
also found that correctly accounting for the future
impact of salience changes to be particularly ineffi-
cient. However, ignoring those terms entirely made
negligible difference in final accuracy.9
</bodyText>
<sectionHeader confidence="0.9902" genericHeader="method">
5 Final Experiments
</sectionHeader>
<bodyText confidence="0.999997833333334">
We present our final experiments using the full
model developed in section 3. As in section 3, we
use true mention boundaries and evaluate using the
MUC F1 measure (Vilain et al., 1995). All hyper-
parameters were tuned on the development set only.
The document concentration parameter α was set by
taking a constant proportion of the average number
of mentions in a document across the corpus. This
number was chosen to minimize the squared error
between the number of proposed entities and true
entities in a document. It was not tuned to maximize
the F1 measure. A coefficient of 0.4 was chosen.
The global concentration coefficient -y was chosen
to be a constant proportion of αM, where M is the
number of documents in the corpus. We found 0.15
to be a good value using the same least-square pro-
cedure. The values for these coefficients were not
changed for the experiments on the test sets.
</bodyText>
<subsectionHeader confidence="0.998551">
5.1 MUC-6
</subsectionHeader>
<bodyText confidence="0.998658">
Our main evaluation is on the standard MUC-6 for-
mal test set.10 The standard experimental setup for
</bodyText>
<equation confidence="0.8154935">
L1
S,
M1 = T1 N1 G1
j
</equation>
<footnote confidence="0.998941666666667">
9This corresponds to truncating equation (2) at j&apos; = j.
10Since the MUC data is not annotated with mention types,
we automatically detect this information in the same way as Luo
</footnote>
<figure confidence="0.985690933333333">
H1 =&amp;quot;Bush&amp;quot;
Z, Z2 Z3
L,
M2 =
NOM
S2
T2
H2 =
&amp;quot;president&amp;quot;
�0
∞
A∞
N2 G2
L3
7
M3 =
PRO
S3
α
T2
H3 =
&amp;quot;he&amp;quot;
N2 =
SG
G2 =
MALE
m
∞
e
I
</figure>
<page confidence="0.996014">
853
</page>
<table confidence="0.9868171">
Dataset Num Docs. Prec. Recall F1
MUC-6 60 80.8 52.8 63.9
+DRYRUN-TRAIN 251 79.1 59.7 68.0
+ENGLISH-NWIRE 381 80.4 62.4 70.3
Dataset Prec. Recall F1
ENGLISH-NWIRE 66.7 62.3 64.2
ENGLISH-BNEWS 63.2 61.3 62.3
CHINESE-NWIRE 71.6 63.3 67.2
CHINESE-BNEWS 71.2 61.8 66.2
(a) (b)
</table>
<tableCaption confidence="0.999635">
Table 2: Formal Results: Our system evaluated using the MUC model theoretic measure Vilain et al. (1995). The table in (a) is
our performance on the thirty document MUC-6 formal test set with increasing amounts of training data. In all cases for the table,
we are evaluating on the same thirty document test set which is included in our training set, since our system in unsupervised. The
table in (b) is our performance on the ACE 2004 training sets.
</tableCaption>
<bodyText confidence="0.936301926829268">
this data is a 30/30 document train/test split. Train-
ing our system on all 60 documents of the training
and test set (as this is in an unsupervised system,
the unlabeled test documents are present at train-
ing time), but evaluating only on the test documents,
gave 63.9 F1 and is labeled MUC-6 in table 2(a).
One advantage of an unsupervised approach is
that we can easily utilize more data when learning a
model. We demonstrate the effectiveness of this fact
by evaluating on the MUC-6 test documents with in-
creasing amounts of unannotated training data. We
first added the 191 documents from the MUC-6
dryrun training set (which were not part of the train-
ing data for official MUC-6 evaluation). This model
gave 68.0 F1 and is labeled +DRYRUN-TRAIN in ta-
ble 2(a). We then added the ACE ENGLISH-NWIRE
training data, which is from a different corpora than
the MUC-6 test set and from a different time period.
This model gave 70.3 F1 and is labeled +ENGLISH-
NWIRE in table 2(a).
Our results on this test set are surprisingly com-
parable to, though slightly lower than, some recent
supervised systems. McCallum and Wellner (2004)
report 73.4 F1 on the formal MUC-6 test set, which
is reasonably close to our best MUC-6 number of
70.3 F1. McCallum and Wellner (2004) also report
a much lower 91.6 F1 on only proper nouns men-
tions. Our system achieves a 89.8 F1 when evalu-
ation is restricted to only proper mentions.11 The
et al. (2004). A mention is proper if it is annotated with NER
information. It is a pronoun if the head is on the list of En-
glish pronouns. Otherwise, it is a nominal mention. Note we do
not use the NER information for any purpose but determining
whether the mention is proper.
11The best results we know on the MUC-6 test set using the
standard setting are due to Luo et al. (2004) who report a 81.3
Fl (much higher than others). However, it is not clear this is a
comparable number, due to the apparent use of gold NER fea-
tures, which provide a strong clue to coreference. Regardless, it
is unsurprising that their system, which has many rich features,
would outperform ours.
</bodyText>
<table confidence="0.9982814">
HEAD ENT TYPE GENDER NUMBER
Bush: 1.0 PERS MALE SG
AP: 1.0 ORG NEUTER PL
viacom: 0.64, company: 0.36 ORG NEUTER SG
teamsters: 0.22, union: 0.78, MISC NEUTER PL
</table>
<tableCaption confidence="0.977315">
Table 3: Frequent entities occurring across documents along
with head distribution and mode of property distributions.
</tableCaption>
<bodyText confidence="0.9987924">
closest comparable unsupervised system is Cardie
and Wagstaff (1999) who use pairwise NP distances
to cluster document mentions. They report a 53.6 F1
on MUC6 when tuning distance metric weights to
maximize F1 on the development set.
</bodyText>
<subsectionHeader confidence="0.981828">
5.2 ACE 2004
</subsectionHeader>
<bodyText confidence="0.99983136">
We also performed experiments on ACE 2004 data.
Due to licensing restrictions, we did not have access
to the ACE 2004 formal development and test sets,
and so the results presented are on the training sets.
We report results on the newswire section (NWIRE
in table 2b) and the broadcast news section (BNEWS
in table 2b). These datasets include the prenomi-
nal mention type, which is not present in the MUC-
6 data. We treated prenominals analogously to the
treatment of proper and nominal mentions.
We also tested our system on the Chinese
newswire and broadcast news sections of the ACE
2004 training sets. Our relatively higher perfor-
mance on Chinese compared to English is perhaps
due to the lack of prenominal mentions in the Chi-
nese data, as well as the presence of fewer pronouns
compared to English.
Our ACE results are difficult to compare exactly
to previous work because we did not have access
to the restricted formal test set. However, we can
perform a rough comparison between our results on
the training data (without coreference annotation) to
supervised work which has used the same training
data (with coreference annotation) and evaluated on
the formal test set. Denis and Baldridge (2007) re-
</bodyText>
<page confidence="0.996313">
854
</page>
<bodyText confidence="0.9998435">
port 67.1 F1 and 69.2 F1 on the English NWIRE and
BNEWS respectively using true mention boundaries.
While our system underperforms the supervised sys-
tems, its accuracy is nonetheless promising.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.982499">
6.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999983384615385">
The largest source of error in our system is between
coreferent proper and nominal mentions. The most
common examples of this kind of error are appos-
itive usages e.g. George W. Bush, president of the
US, visited Idaho. Another error of this sort can be
seen in figure 2, where the corporation mention is
not labeled coreferent with the The Weir Group men-
tion. Examples such as these illustrate the regular (at
least in newswire) phenomenon that nominal men-
tions are used with informative intent, even when the
entity is salient and a pronoun could have been used
unambiguously. This aspect of nominal mentions is
entirely unmodeled in our system.
</bodyText>
<subsectionHeader confidence="0.992242">
6.2 Global Coreference
</subsectionHeader>
<bodyText confidence="0.999996833333333">
Since we do not have labeled cross-document coref-
erence data, we cannot evaluate our system’s cross-
document performance quantitatively. However, in
addition to observing the within-document gains
from sharing shown in section 3, we can manually
inspect the most frequently occurring entities in our
corpora. Table 3 shows some of the most frequently
occurring entities across the English ACE NWIRE
corpus. Note that Bush is the most frequent entity,
though his (and others’) nominal cluster president
is mistakenly its own entity. Merging of proper and
nominal clusters does occur as can be seen in table 3.
</bodyText>
<subsectionHeader confidence="0.974794">
6.3 Unsupervised NER
</subsectionHeader>
<bodyText confidence="0.973604764705883">
We can use our model to for unsupervised NER
tagging: for each proper mention, assign the mode
of the generating entity’s distribution over entity
types. Note that in our model the only way an en-
tity becomes associated with an entity type is by
the pronouns used to refer to it.12 If we evaluate
our system as an unsupervised NER tagger for the
proper mentions in the MUC-6 test set, it yields a
12Ge et al. (1998) exploit a similar idea to assign gender to
proper mentions.
per-label accuracy of 61.2% (on MUC labels). Al-
though nowhere near the performance of state-of-
the-art systems, this result beats a simple baseline of
always guessing PERSON (the most common entity
type), which yields 46.4%. This result is interest-
ing given that the model was not developed for the
purpose of inferring entity types whatsoever.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999996888888889">
We have presented a novel, unsupervised approach
to coreference resolution: global entities are shared
across documents, the number of entities is deter-
mined by the model, and mentions are generated by
a sequential salience model and a model of pronoun-
entity association. Although our system does not
perform quite as well as state-of-the-art supervised
systems, its performance is in the same general
range, despite the system being unsupervised.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999810777777778">
I. Bhattacharya and L. Getoor. 2006. A latent dirichlet model
for unsupervised entity resolution. SIAM conference on data
mining.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase corefer-
ence as clustering. EMNLP.
Hal Daume and Daniel Marcu. 2005. A Bayesian model for su-
pervised clustering with the Dirichlet process prior. JMLR.
Pascal Denis and Jason Baldridge. 2007. Global, joint determi-
nation of anaphoricity and coreference resolution using inte-
ger programming. HLT-NAACL.
Thomas Ferguson. 1973. A bayesian analysis of some non-
parametric problems. Annals of Statistics.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical
approach to anaphora resolution. Sixth Workshop on Very
Large Corpora.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kamb-
hatla, and Salim Roukos. 2004. A mention-synchronous
coreference resolution algorithm based on the bell tree. ACL.
Andrew McCallum and Ben Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun corefer-
ence. NIPS.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag,
Daniel L. Ong, and Andrey Kolobov. 2005. Blog: Proba-
bilistic models with unknown objects. IJCAI.
Vincent Ng and Claire Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. ACL.
NIST. 2004. The ACE evaluation plan.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics.
Yee Whye Teh, Michael Jordan, Matthew Beal, and David Blei.
2006. Hierarchical dirichlet processes. Journal of the Amer-
ican Statistical Association, 101.
Marc Vilain, John Burger, John Aberdeen, Dennis Connolly,
and Lynette Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. MUC-6.
</reference>
<page confidence="0.998994">
855
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950248">
<title confidence="0.996205">Unsupervised Coreference Resolution in a Nonparametric Bayesian Model</title>
<author confidence="0.999931">Aria Haghighi</author>
<author confidence="0.999931">Dan Klein</author>
<affiliation confidence="0.9982585">Computer Science Division UC Berkeley</affiliation>
<abstract confidence="0.997126533333333">We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 on the MUC-6 test set, broadly in the range of some recent supervised results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Bhattacharya</author>
<author>L Getoor</author>
</authors>
<title>A latent dirichlet model for unsupervised entity resolution. SIAM conference on data mining.</title>
<date>2006</date>
<contexts>
<context position="2737" citStr="Bhattacharya and Getoor, 2006" startWordPosition="411" endWordPosition="414">ethods can and do fail to produce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final coherent clustering. Nonetheless, all these systems crucially rely on pairwise models because clusterlevel models are much harder to work with, combinatorially, in discriminative approaches. Another thread of coreference work has focused on the problem of identifying matches between documents (Milch et al., 2005; Bhattacharya and Getoor, 2006; Daume and Marcu, 2005). These methods ignore the sequential anaphoric structure inside documents, but construct models of how and when entities are shared between them.1 These models, as ours, are generative ones, since the focus is on cluster discovery and the data is generally unlabeled. In this paper, we present a novel, fully generative, nonparametric Bayesian model of mentions in a document corpus. Our model captures both withinand cross-document coreference. At the top, a hierarchical Dirichlet process (Teh et al., 2006) cap1Milch et al. (2005) works with citations rather than discours</context>
</contexts>
<marker>Bhattacharya, Getoor, 2006</marker>
<rawString>I. Bhattacharya and L. Getoor. 2006. A latent dirichlet model for unsupervised entity resolution. SIAM conference on data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Kiri Wagstaff</author>
</authors>
<title>Noun phrase coreference as clustering.</title>
<date>1999</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="27681" citStr="Cardie and Wagstaff (1999)" startWordPosition="4741" endWordPosition="4744">3 Fl (much higher than others). However, it is not clear this is a comparable number, due to the apparent use of gold NER features, which provide a strong clue to coreference. Regardless, it is unsurprising that their system, which has many rich features, would outperform ours. HEAD ENT TYPE GENDER NUMBER Bush: 1.0 PERS MALE SG AP: 1.0 ORG NEUTER PL viacom: 0.64, company: 0.36 ORG NEUTER SG teamsters: 0.22, union: 0.78, MISC NEUTER PL Table 3: Frequent entities occurring across documents along with head distribution and mode of property distributions. closest comparable unsupervised system is Cardie and Wagstaff (1999) who use pairwise NP distances to cluster document mentions. They report a 53.6 F1 on MUC6 when tuning distance metric weights to maximize F1 on the development set. 5.2 ACE 2004 We also performed experiments on ACE 2004 data. Due to licensing restrictions, we did not have access to the ACE 2004 formal development and test sets, and so the results presented are on the training sets. We report results on the newswire section (NWIRE in table 2b) and the broadcast news section (BNEWS in table 2b). These datasets include the prenominal mention type, which is not present in the MUC6 data. We treate</context>
</contexts>
<marker>Cardie, Wagstaff, 1999</marker>
<rawString>Claire Cardie and Kiri Wagstaff. 1999. Noun phrase coreference as clustering. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Daniel Marcu</author>
</authors>
<title>A Bayesian model for supervised clustering with the Dirichlet process prior.</title>
<date>2005</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="2761" citStr="Daume and Marcu, 2005" startWordPosition="415" endWordPosition="418">ce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final coherent clustering. Nonetheless, all these systems crucially rely on pairwise models because clusterlevel models are much harder to work with, combinatorially, in discriminative approaches. Another thread of coreference work has focused on the problem of identifying matches between documents (Milch et al., 2005; Bhattacharya and Getoor, 2006; Daume and Marcu, 2005). These methods ignore the sequential anaphoric structure inside documents, but construct models of how and when entities are shared between them.1 These models, as ours, are generative ones, since the focus is on cluster discovery and the data is generally unlabeled. In this paper, we present a novel, fully generative, nonparametric Bayesian model of mentions in a document corpus. Our model captures both withinand cross-document coreference. At the top, a hierarchical Dirichlet process (Teh et al., 2006) cap1Milch et al. (2005) works with citations rather than discourses and does model the li</context>
</contexts>
<marker>Daume, Marcu, 2005</marker>
<rawString>Hal Daume and Daniel Marcu. 2005. A Bayesian model for supervised clustering with the Dirichlet process prior. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Global, joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<publisher>HLT-NAACL.</publisher>
<contexts>
<context position="29064" citStr="Denis and Baldridge (2007)" startWordPosition="4975" endWordPosition="4978">he ACE 2004 training sets. Our relatively higher performance on Chinese compared to English is perhaps due to the lack of prenominal mentions in the Chinese data, as well as the presence of fewer pronouns compared to English. Our ACE results are difficult to compare exactly to previous work because we did not have access to the restricted formal test set. However, we can perform a rough comparison between our results on the training data (without coreference annotation) to supervised work which has used the same training data (with coreference annotation) and evaluated on the formal test set. Denis and Baldridge (2007) re854 port 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. While our system underperforms the supervised systems, its accuracy is nonetheless promising. 6 Discussion 6.1 Error Analysis The largest source of error in our system is between coreferent proper and nominal mentions. The most common examples of this kind of error are appositive usages e.g. George W. Bush, president of the US, visited Idaho. Another error of this sort can be seen in figure 2, where the corporation mention is not labeled coreferent with the The Weir Group mention. Example</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. Global, joint determination of anaphoricity and coreference resolution using integer programming. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Ferguson</author>
</authors>
<title>A bayesian analysis of some nonparametric problems. Annals of Statistics.</title>
<date>1973</date>
<contexts>
<context position="11032" citStr="Ferguson, 1973" startWordPosition="1825" endWordPosition="1826">z is the number of elements of Z−i,j with entity index z. Similarly we have for the contribution of the emissions: P(Hi,j = h|Z, H−i,j) ∝ nh,z + AH where nh,z is the number of times we have seen head h associated with entity index z in (Z, H−i,j). 3.2 An Infinite Mixture Model A clear drawback of the finite mixture model is the requirement that we specify a priori a number of entities K for a document. We would like our model to select K in an effective, principled way. A mechanism for doing so is to replace the finite Dirichlet prior on Q with the non-parametric Dirichlet process (DP) prior (Ferguson, 1973).5 Doing so gives the model in figure 1(b). Note that we now list an infinite number of mixture components in this model since there can be an unbounded number of entities. Rather than a finite Q with a symmetric Dirichlet distribution, in which draws tend to have balanced clusters, we now have an infinite Q. However, most draws will have weights which decay exponentially quickly in the prior (though not necessarily in the posterior). Therefore, there is a natural penalty for each cluster which is actually used. With Z observed during sampling, we can integrate out Q and calculate P(Zi,j|Z−i,j</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas Ferguson. 1973. A bayesian analysis of some nonparametric problems. Annals of Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="18198" citStr="Ge et al., 1998" startWordPosition="3084" endWordPosition="3087">4.1 Fi on our development data. As can be seen in figure 2(b), this model does correct the systematic problem of pronouns being considered their own entities. However, it still does not have a preference for associating pronominal references to entities which are in any way local. 3.4 Adding Salience We would like our model to capture how mention types are generated for a given entity in a robust and somewhat language independent way. The choice of entities may reasonably be considered to be independent given the mixing weights Q, but how we realize an entity is strongly dependent on context (Ge et al., 1998). In order to capture this in our model, we enrich it as shown in figure 4. As we proceed through a document, generating entities and their mentions, we maintain a list of the active entities and their saliences, or activity scores. Every time an entity is mentioned, we increment its activity score by 1, and every time we move to generate the next mention, all activity scores decay by a constant factor of 0.5. This gives rise to an ordered list of entity activations, L, where the rank of an entity decays exponentially as new mentions are generated. We call this list a salience list. Given a sa</context>
<context position="31010" citStr="Ge et al. (1998)" startWordPosition="5301" endWordPosition="5304"> most frequent entity, though his (and others’) nominal cluster president is mistakenly its own entity. Merging of proper and nominal clusters does occur as can be seen in table 3. 6.3 Unsupervised NER We can use our model to for unsupervised NER tagging: for each proper mention, assign the mode of the generating entity’s distribution over entity types. Note that in our model the only way an entity becomes associated with an entity type is by the pronouns used to refer to it.12 If we evaluate our system as an unsupervised NER tagger for the proper mentions in the MUC-6 test set, it yields a 12Ge et al. (1998) exploit a similar idea to assign gender to proper mentions. per-label accuracy of 61.2% (on MUC labels). Although nowhere near the performance of state-ofthe-art systems, this result beats a simple baseline of always guessing PERSON (the most common entity type), which yields 46.4%. This result is interesting given that the model was not developed for the purpose of inferring entity types whatsoever. 7 Conclusion We have presented a novel, unsupervised approach to coreference resolution: global entities are shared across documents, the number of entities is determined by the model, and mentio</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<publisher>ACL.</publisher>
<contexts>
<context position="27038" citStr="Luo et al. (2004)" startWordPosition="4636" endWordPosition="4639">asonably close to our best MUC-6 number of 70.3 F1. McCallum and Wellner (2004) also report a much lower 91.6 F1 on only proper nouns mentions. Our system achieves a 89.8 F1 when evaluation is restricted to only proper mentions.11 The et al. (2004). A mention is proper if it is annotated with NER information. It is a pronoun if the head is on the list of English pronouns. Otherwise, it is a nominal mention. Note we do not use the NER information for any purpose but determining whether the mention is proper. 11The best results we know on the MUC-6 test set using the standard setting are due to Luo et al. (2004) who report a 81.3 Fl (much higher than others). However, it is not clear this is a comparable number, due to the apparent use of gold NER features, which provide a strong clue to coreference. Regardless, it is unsurprising that their system, which has many rich features, would outperform ours. HEAD ENT TYPE GENDER NUMBER Bush: 1.0 PERS MALE SG AP: 1.0 ORG NEUTER PL viacom: 0.64, company: 0.36 ORG NEUTER SG teamsters: 0.22, union: 0.78, MISC NEUTER PL Table 3: Frequent entities occurring across documents along with head distribution and mode of property distributions. closest comparable unsupe</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<publisher>NIPS.</publisher>
<contexts>
<context position="2320" citStr="McCallum and Wellner (2004)" startWordPosition="349" endWordPosition="352">eatures which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002). Although such approaches have been successful, they have several liabilities. First, rich features require plentiful labeled data, which we do not have for coreference tasks in most domains and languages. Second, coreference is inherently a clustering or partitioning task. Naive pairwise methods can and do fail to produce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final coherent clustering. Nonetheless, all these systems crucially rely on pairwise models because clusterlevel models are much harder to work with, combinatorially, in discriminative approaches. Another thread of coreference work has focused on the problem of identifying matches between documents (Milch et al., 2005; Bhattacharya and Getoor, 2006; Daume and Marcu, 2005). These methods ignore the sequential anaphoric structure inside documents, but construct models of how and when entities are shared between them.1 These model</context>
<context position="26364" citStr="McCallum and Wellner (2004)" startWordPosition="4508" endWordPosition="4511">nts with increasing amounts of unannotated training data. We first added the 191 documents from the MUC-6 dryrun training set (which were not part of the training data for official MUC-6 evaluation). This model gave 68.0 F1 and is labeled +DRYRUN-TRAIN in table 2(a). We then added the ACE ENGLISH-NWIRE training data, which is from a different corpora than the MUC-6 test set and from a different time period. This model gave 70.3 F1 and is labeled +ENGLISHNWIRE in table 2(a). Our results on this test set are surprisingly comparable to, though slightly lower than, some recent supervised systems. McCallum and Wellner (2004) report 73.4 F1 on the formal MUC-6 test set, which is reasonably close to our best MUC-6 number of 70.3 F1. McCallum and Wellner (2004) also report a much lower 91.6 F1 on only proper nouns mentions. Our system achieves a 89.8 F1 when evaluation is restricted to only proper mentions.11 The et al. (2004). A mention is proper if it is annotated with NER information. It is a pronoun if the head is on the list of English pronouns. Otherwise, it is a nominal mention. Note we do not use the NER information for any purpose but determining whether the mention is proper. 11The best results we know on </context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>Andrew McCallum and Ben Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Milch</author>
<author>Bhaskara Marthi</author>
<author>Stuart Russell</author>
<author>David Sontag</author>
<author>Daniel L Ong</author>
<author>Andrey Kolobov</author>
</authors>
<title>Blog: Probabilistic models with unknown objects.</title>
<date>2005</date>
<publisher>IJCAI.</publisher>
<contexts>
<context position="2706" citStr="Milch et al., 2005" startWordPosition="407" endWordPosition="410">sk. Naive pairwise methods can and do fail to produce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final coherent clustering. Nonetheless, all these systems crucially rely on pairwise models because clusterlevel models are much harder to work with, combinatorially, in discriminative approaches. Another thread of coreference work has focused on the problem of identifying matches between documents (Milch et al., 2005; Bhattacharya and Getoor, 2006; Daume and Marcu, 2005). These methods ignore the sequential anaphoric structure inside documents, but construct models of how and when entities are shared between them.1 These models, as ours, are generative ones, since the focus is on cluster discovery and the data is generally unlabeled. In this paper, we present a novel, fully generative, nonparametric Bayesian model of mentions in a document corpus. Our model captures both withinand cross-document coreference. At the top, a hierarchical Dirichlet process (Teh et al., 2006) cap1Milch et al. (2005) works with</context>
</contexts>
<marker>Milch, Marthi, Russell, Sontag, Ong, Kolobov, 2005</marker>
<rawString>Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag, Daniel L. Ong, and Andrey Kolobov. 2005. Blog: Probabilistic models with unknown objects. IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<journal>ACL. NIST.</journal>
<contexts>
<context position="1816" citStr="Ng and Cardie, 2002" startWordPosition="272" endWordPosition="275">and is governed by linguistic and cognitive constraints. In this paper, we present a nonparametric generative model of a document corpus which naturally connects these two processes. Most recent coreference resolution work has focused on the task of deciding which mentions (noun phrases) in a document are coreferent. The dominant approach is to decompose the task into a collection of pairwise coreference decisions. One then applies discriminative learning methods to pairs of mentions, using features which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002). Although such approaches have been successful, they have several liabilities. First, rich features require plentiful labeled data, which we do not have for coreference tasks in most domains and languages. Second, coreference is inherently a clustering or partitioning task. Naive pairwise methods can and do fail to produce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final coherent clustering. No</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. ACL. NIST. 2004. The ACE evaluation plan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Soon</author>
<author>H Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="1794" citStr="Soon et al., 2001" startWordPosition="268" endWordPosition="271">hed with pronouns, and is governed by linguistic and cognitive constraints. In this paper, we present a nonparametric generative model of a document corpus which naturally connects these two processes. Most recent coreference resolution work has focused on the task of deciding which mentions (noun phrases) in a document are coreferent. The dominant approach is to decompose the task into a collection of pairwise coreference decisions. One then applies discriminative learning methods to pairs of mentions, using features which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002). Although such approaches have been successful, they have several liabilities. First, rich features require plentiful labeled data, which we do not have for coreference tasks in most domains and languages. Second, coreference is inherently a clustering or partitioning task. Naive pairwise methods can and do fail to produce coherent partitions. One classic solution is to make greedy left-to-right linkage decisions. Recent work has addressed this issue in more global ways. McCallum and Wellner (2004) use graph partioning in order to reconcile pairwise scores into a final c</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. Soon, H. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael Jordan</author>
<author>Matthew Beal</author>
<author>David Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<contexts>
<context position="3271" citStr="Teh et al., 2006" startWordPosition="496" endWordPosition="499">ifying matches between documents (Milch et al., 2005; Bhattacharya and Getoor, 2006; Daume and Marcu, 2005). These methods ignore the sequential anaphoric structure inside documents, but construct models of how and when entities are shared between them.1 These models, as ours, are generative ones, since the focus is on cluster discovery and the data is generally unlabeled. In this paper, we present a novel, fully generative, nonparametric Bayesian model of mentions in a document corpus. Our model captures both withinand cross-document coreference. At the top, a hierarchical Dirichlet process (Teh et al., 2006) cap1Milch et al. (2005) works with citations rather than discourses and does model the linear structure of the citations. 848 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848–855, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics tures cross-document entity (and parameter) sharing, while, at the bottom, a sequential model of salience captures within-document sequential structure. As a joint model of several kinds of discourse variables, it can be used to make predictions about either kind of coreference, though</context>
<context position="9742" citStr="Teh et al. (2006)" startWordPosition="1592" endWordPosition="1595">n α. For each mention in the document, we choose a component (an entity index) z from Q. Entity z is then associated with a multinomial emission distribution over head words with parameters Oh, which are drawn from a symmetric Dirichlet over possible mention heads with concentration AH.2 Note that here the X for a mention consists only of the mention head H. As we enrich our models, we simultaneously develop an accompanying Gibbs sampling procedure to obtain samples from P(Z|X).3 For now, all heads H are observed and all parameters (Q and 0) can be integrated out analytically: for details see Teh et al. (2006). The only sampling is for the values of Zi,j, the entity index of mention j in document i. The relevant conditional distribution is:4 P(Zi,j|Z−i,j, H) ∝ P(Zi,j|Z−i,j)P(Hi,j|Z, H−i,j) where Hi,j is the head of mention j in document i. Expanding each term, we have the contribution of the prior: P(Zi,j = z|Z−i,j) ∝ nz + α 2In general, we will use a subscripted A to indicate concentration for finite Dirichlet distributions. Unless otherwise specified, A concentration parameters will be set to a−4 and omitted from diagrams. 3One could use the EM algorithm with this model, but EM will not extend ef</context>
<context position="12009" citStr="Teh et al. (2006)" startWordPosition="1996" endWordPosition="1999"> exponentially quickly in the prior (though not necessarily in the posterior). Therefore, there is a natural penalty for each cluster which is actually used. With Z observed during sampling, we can integrate out Q and calculate P(Zi,j|Z−i,j) analytically, using the Chinese restaurant process representation: � α, if z = znew P (Zi,j = z|Z−i,j) ∝ nz, otherwise (1) where znew is a new entity index not used in Z−i,j and nz is the number of mentions that have entity index z. Aside from this change, sampling is identical 5We do not give a detailed presentation of the Dirichlet process here, but see Teh et al. (2006) for a presentation. 850 (a) (b) Figure 3: (a) An entity and its parameters. (b)The head model described in section 3.3. The shaded nodes indicate observed variables. The mention type determines which set of parents are used. The dependence of mention variable on entity parameters 0 and pronoun head model 9 is omitted. to the finite mixture case, though with the number of clusters actually occupied in each sample drifting upwards or downwards. This model yielded a 54.5 Fi on our development data.6 This model is, however, hopelessly crude, capturing nothing of the structure of coreference. Its </context>
<context position="20969" citStr="Teh et al. (2006)" startWordPosition="3562" endWordPosition="3565">entities is global, with global mixing weights Qo drawn from a DP prior with concentration parameter &apos;y. Each document uses 852 Figure 5: Graphical depiction of the HDP coreference model described in section 3.5. The dependencies between the global entity parameters 0 and pronoun head parameters 0 on the mention observations are not depicted. the same global entities, but each has a documentspecific distribution Qi drawn from a DP centered on Q0 with concentration parameter α. Up to the point where entities are chosen, this formulation follows the basic hierarchical Dirichlet process prior of Teh et al. (2006). Once the entities are chosen, our model for the realization of the mentions is as before. This model is depicted graphically in figure 5. Although it is possible to integrate out Q0 as we did the individual Qi, we instead choose for efficiency and simplicity to sample the global mixture distribution Q0 from the posterior distribution P(Q0|Z).8 The mention generation terms in the model and sampler are unchanged. In the full hierarchical model, our equation (1) for sampling entities, ignoring the salience component of section 3.4, becomes: � αQu 0, if z = znew P(Zi,j = z|Z−i,j, Q0)� nz + αQz0,</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael Jordan, Matthew Beal, and David Blei. 2006. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<pages>6</pages>
<contexts>
<context position="6108" citStr="Vilain et al., 1995" startWordPosition="963" endWordPosition="966">MUC data, the head was crudely chosen to be the rightmost mention token, and the mention type was automatically detected. We will not assume any other information to be present in the data beyond the text itself. In particular, unlike much related work, we do not assume gold named entity recognition (NER) labels; indeed we do not assume observed NER labels or POS tags at all. Our priFigure 1: Graphical model depiction of document level entity models described in sections 3.1 and 3.2 respectively. The shaded nodes indicate observed variables. mary performance metric will be the MUC F1 measure (Vilain et al., 1995), commonly used to evaluate coreference systems on a within-document basis. Since our system relies on sampling, all results are averaged over five random runs. 3 Coreference Resolution Models In this section, we present a sequence of generative coreference resolution models for document corpora. All are essentially mixture models, where the mixture components correspond to entities. As far as notation, we assume a collection of I documents, each with Ji mentions. We use random variables Z to refer to (indices of) entities. We will use 0z to denote the parameters for an entity z, and 0 to refe</context>
<context position="23365" citStr="Vilain et al., 1995" startWordPosition="3969" endWordPosition="3972">imulated annealing. We also found sampling the T, G, and N variables to be particularly inefficient, so instead we maintain soft counts over each of these variables and use these in place of a hard sampling scheme. We also found that correctly accounting for the future impact of salience changes to be particularly inefficient. However, ignoring those terms entirely made negligible difference in final accuracy.9 5 Final Experiments We present our final experiments using the full model developed in section 3. As in section 3, we use true mention boundaries and evaluate using the MUC F1 measure (Vilain et al., 1995). All hyperparameters were tuned on the development set only. The document concentration parameter α was set by taking a constant proportion of the average number of mentions in a document across the corpus. This number was chosen to minimize the squared error between the number of proposed entities and true entities in a document. It was not tuned to maximize the F1 measure. A coefficient of 0.4 was chosen. The global concentration coefficient -y was chosen to be a constant proportion of αM, where M is the number of documents in the corpus. We found 0.15 to be a good value using the same leas</context>
<context position="24892" citStr="Vilain et al. (1995)" startWordPosition="4251" endWordPosition="4254">e MUC data is not annotated with mention types, we automatically detect this information in the same way as Luo H1 =&amp;quot;Bush&amp;quot; Z, Z2 Z3 L, M2 = NOM S2 T2 H2 = &amp;quot;president&amp;quot; �0 ∞ A∞ N2 G2 L3 7 M3 = PRO S3 α T2 H3 = &amp;quot;he&amp;quot; N2 = SG G2 = MALE m ∞ e I 853 Dataset Num Docs. Prec. Recall F1 MUC-6 60 80.8 52.8 63.9 +DRYRUN-TRAIN 251 79.1 59.7 68.0 +ENGLISH-NWIRE 381 80.4 62.4 70.3 Dataset Prec. Recall F1 ENGLISH-NWIRE 66.7 62.3 64.2 ENGLISH-BNEWS 63.2 61.3 62.3 CHINESE-NWIRE 71.6 63.3 67.2 CHINESE-BNEWS 71.2 61.8 66.2 (a) (b) Table 2: Formal Results: Our system evaluated using the MUC model theoretic measure Vilain et al. (1995). The table in (a) is our performance on the thirty document MUC-6 formal test set with increasing amounts of training data. In all cases for the table, we are evaluating on the same thirty document test set which is included in our training set, since our system in unsupervised. The table in (b) is our performance on the ACE 2004 training sets. this data is a 30/30 document train/test split. Training our system on all 60 documents of the training and test set (as this is in an unsupervised system, the unlabeled test documents are present at training time), but evaluating only on the test docu</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model-theoretic coreference scoring scheme. MUC-6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>