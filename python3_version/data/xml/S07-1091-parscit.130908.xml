<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038241">
<title confidence="0.9960835">
UNT: SubFinder: Combining Knowledge Sources for
Automatic Lexical Substitution
</title>
<author confidence="0.998762">
Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, Rada Mihalcea∗
</author>
<affiliation confidence="0.9996755">
Department of Computer Science and Engineering
University of North Texas
</affiliation>
<email confidence="0.992126">
samer@unt.edu, csomaia@unt.edu, carmenb@unt.edu, rss0089@unt.edu, rada@cs.unt.edu
</email>
<sectionHeader confidence="0.995569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998034">
This paper describes the University of North
Texas SUBFINDER system. The system is
able to provide the most likely set of sub-
stitutes for a word in a given context, by
combining several techniques and knowl-
edge sources. SUBFINDER has successfully
participated in the best and out of ten (oot)
tracks in the SEMEVAL lexical substitution
task, consistently ranking in the first or sec-
ond place.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895263157895">
Lexical substitution is defined as the task of identify-
ing the most likely alternatives (substitutes) for a tar-
get word, given its context (McCarthy, 2002). Many
natural language processing applications can bene-
fit from the availability of such alternative words,
including word sense disambiguation, lexical ac-
quisition, machine translation, information retrieval,
question answering, text simplification, and others.
The task is closely related to the problem of word
sense disambiguation, with the substitutes acting as
synonyms for the input word meaning. Unlike word
sense disambiguation however, lexical substitution
is not performed with respect to a given sense inven-
tory, but instead candidate synonyms are generated
“on the fly” for a given word occurrence. Thus, lexi-
cal substitution can be regarded in a way as a hybrid
task that combines word sense disambiguation and
distributional similarity, targeting the identification
of semantically similar words that fit the context.
</bodyText>
<sectionHeader confidence="0.611643" genericHeader="method">
2 A system for lexical substitution
</sectionHeader>
<bodyText confidence="0.9344324">
SUBFINDER is a system able to provide the most
likely set of substitutes for a word in a given context.
∗Contact author.
In SUBFINDER, the lexical substitution task is car-
ried out as a sequence of two steps. First, candidates
are extracted from a variety of knowledge sources;
so far, we experimented with WordNet (Fellbaum,
1998), Microsoft Encarta encyclopedia, Roget, as
well as synonym sets generated from bilingual dic-
tionaries, but additional knowledge sources can be
integrated as well. Second, provided a list of candi-
dates, a number of ranking methods are applied in
a weighted combination, resulting in a final list of
lexical substitutes ranked by their semantic fit with
both the input target word and the context.
</bodyText>
<sectionHeader confidence="0.980981" genericHeader="method">
3 Candidate Extraction
</sectionHeader>
<bodyText confidence="0.988615136363636">
Candidates are extracted using several lexical re-
sources, which are combined into a larger compre-
hensive resource.
WordNet: WordNet is a large lexical database of
English, with words grouped into synonym sets
called synsets. A problem we encountered with this
resource is that often times the only candidate in the
synset is the target word itself. Thus, to enlarge the
set of candidates, we use both the synonyms and the
hypernyms of the target word. We also remove the
target word from the synset, to ensure that only vi-
able candidates are considered.
Microsoft Encarta encyclopedia: The Microsoft
Encarta is an online encyclopedia and thesaurus re-
source, which provides for each word the part of
speech and a list of synonyms. Using the part of
speech as identified in the context, we are able to ex-
tract synsets for the target word. An important fea-
ture in the Encarta Thesaurus is that the first word
in the synset acts as a definition for the synset, and
therefore disambiguates the target word. This defi-
nition is maintained as a separate entry in the com-
</bodyText>
<page confidence="0.96083">
410
</page>
<bodyText confidence="0.98165075">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 410–413,
Prague, June 2007. c�2007 Association for Computational Linguistics
prehensive resource, and it is also added to its corre-
sponding synset.
Other Lexical Resources: We have also experi-
mented with two other lexical resources, namely the
Roget thesaurus and a thesaurus built using bilingual
dictionaries. In evaluations carried out on the devel-
opment data set, the best results were obtained using
only WordNet and Encarta, and thus these are the
resources used in the final SUBFINDER system.
All these resources entail different forms of synset
clustering. In order to merge them, we use the
largest overlap among them. It is important to note
that the choice of the first resource considered has
a bearing on the way the synsets are clustered. In
experiments ran on the development data set, the
best results were obtained using a lexical resource
constructed starting with the Microsoft Encarta The-
saurus and then mapping the WordNet synsets to it.
</bodyText>
<sectionHeader confidence="0.987029" genericHeader="method">
4 Candidate Ranking
</sectionHeader>
<bodyText confidence="0.999648111111111">
Several ranking methods are used to score the can-
didate substitutes, as described below.
Lexical Baseline (LB): In this approach we use
the pre-existing lexical resources to provide a rank-
ing over the candidate substitutes. We rank the can-
didates based on their occurrence in the two selected
lexical resources WordNet and Encarta, with those
occurring in both resources being assigned a higher
ranking. This technique emphasizes the resources
annotators’ agreement that the candidates belong in-
deed to the same synset.
Machine Translation (MT): We use machine
translation to translate the test sentences back-and-
forth between English and a second language. From
the resulting English translation, we extract the re-
placement that the machine translation engine pro-
vides for the target word. To locate the translated
word we scan the translation for any of the can-
didates (and their inflections) as obtained from the
comprehensive resource, and score the candidate
synset accordingly.
We experimented with a range of languages such
as French, Italian, Spanish, Simplified Chinese, and
German, but the best results obtained on the devel-
opment data were based on the French translations.
This could be explained because French is part of
the Romance languages family and synonyms to En-
glish words often find their roots in Latin. If we
consider again the word bright, it was translated
into French as intelligent and then translated back
into English as intelligent for obvious reasons. In
one instance, intelligent was the best replacement
for bright in the trial data. Despite the fact that we
also used Italian and Spanish (which are both Latin-
based) we can only assume that French worked bet-
ter because translation engines are better trained on
French. From the resulting English translation, we
extract the replacement that the machine translation
engine provides for the target word. To locate the
translated word we scan the translation for any of the
candidates (and their inflections) as obtained from
the comprehensive resource, and score the candidate
synset accordingly. The translation process was car-
ried out using Google and AltaVista translation en-
gines resulting in two systems MTG and MTA re-
spectively. The translation systems feature high pre-
cision when a candidate is found (about 20% of the
time), at the cost of low recall. The lexical baseline
method is therefore used when no candidates are re-
turned by the translation method.
Most Common Sense (MCS): Another method
we use for ranking candidates is to consider the
first word appearing in the first synset returned by
WordNet. When no words other than the target
word are available in this synset, the method recur-
sively searches the next synset available for the tar-
get word. In order to guarantee a sufficient number
of candidates, we use the lexical baseline method as
a baseline.
Language Model (LM): We model the semantic
fit of a candidate substitute within the given context
using a language model, expressed using the condi-
tional probability:
</bodyText>
<equation confidence="0.998147">
P(c|g) = P(c, g)/P(g) ≈ Count(c, g) (1)
</equation>
<bodyText confidence="0.999915">
where c represents a possible candidate and g rep-
resents the context. The probability P(g) of the
context is the same for all the candidates, hence we
can ignore it and estimate P(c|g) as the N-gram fre-
quency of the context where the target word is re-
placed by the proposed candidate. To avoid skewed
counts that can arise from the different morpholog-
ical inflections of the target word or the candidate
and the bias that the context might have toward any
specific inflection, we generalize P(c|g) to take into
account all the inflections of the selected candidate
as shown in equation 2.
</bodyText>
<equation confidence="0.952099333333333">
n
Pn(c|g) ≈ Count(cZ, g) (2)
Z=1
</equation>
<bodyText confidence="0.99880075">
where n is the number of possible inflections for the
candidate c.
We use the Google N-gram dataset to calculate the
term Count(cZ g). The Google N-gram corpus is a
</bodyText>
<page confidence="0.994541">
411
</page>
<bodyText confidence="0.99640325">
collection of English N-grams, ranging from one to
five N-grams, and their respective frequency counts
observed on the Web (Brants and Franz, 2006). In
order for the model to give high preference to the
longer N-grams, while maintaining the relative fre-
quencies of the shorter N-grams (typically more fre-
quent), we augment the counts of the higher order
N-grams with the maximum counts of the lower or-
der N-grams, hence guaranteeing that the score as-
signed to an N-gram of order N is higher than the
the score of an N-gram of order N − 1.
Semantic Relatedness using Latent Semantic
Analysis (LSA): We expect to find a strong se-
mantic relationship between a good candidate and
the target context. A relatively simple and efficient
way to measure such a relatedness is the Latent Se-
mantic Analysis (Landauer et al., 1998). Documents
and terms are mapped into a 300 dimensional latent
semantic space, providing the ability to measure the
semantic relatedness between two words or a word
and a context. We use the InfoMap package from
Stanford University’s Center for the Study of Lan-
guage and Information, trained on a collection of
approximately one million Wikipedia articles. The
rank of a candidate is given by its semantic related-
ness to the entire context sentence.
Information Retrieval (IR): Although the Lan-
guage Model approach is successful in ranking the
candidates, it suffers from the small N-gram size im-
posed by using the Google N-grams corpus. Such
a restriction is obvious in the following 5-gram ex-
ample who was a bright boy in which the context
is not sufficient to disambiguate between happy and
smart as possible candidates. As a result, we adapt
an information retrieval approach which uses all the
content words available in the given context. Similar
to the previous models, the target word in the con-
text is replaced by all the generated inflections of
the selected candidate and then queried using a web
search engine. The resulting rank represents the sum
of the total number of pages in which the candidate
or any of its inflections occur together with the con-
text. This also reflects the semantic relatedness or
the relevance of the candidate to the context.
Word Sense Disambiguation (WSD): Since pre-
vious work indicated the usefulness of word sense
disambiguation systems in lexical substitution (Da-
gan et al., 2006), we use the SenseLearner word
sense disambiguation tool (Mihalcea and Csomai,
2005) to disambiguate the target word and, accord-
ingly, to propose its synonyms as candidates.
Final System: Our candidate ranking methods are
aimed at different aspects of what constitutes a good
candidate. On one hand, we measure the semantic
relatedness of a candidate with the original context
(the LSA and WSD methods fall under this cate-
gory). On the other hand, we also want to ensure
that the candidate fits the context and leads to a well
formed English sentence (e.g., the language model
method). Given that the methods described earlier
aim at orthogonal aspects of the problem, it is ex-
pected that a combination of these will provide a
better overall ranking.
We use a voting mechanism, where we consider
the reciprocal of the rank of each candidates as given
by one of the described methods. The final score of
a candidate is given by the decreasing order of the
weighted sum of the reciprocal ranks:
</bodyText>
<equation confidence="0.7325875">
�score (ci) =
m∈rankings
</equation>
<bodyText confidence="0.999987">
To determine the weight A of each individual
ranking we run a genetic algorithm on the develop-
ment data, optimized for the mode precision and re-
call. Separate sets of weights are obtained for the
best and oot tasks. Table 1 shows the weights of
the individual ranking methods. As expected, for
the best task, the language model type of methods
obtain higher weights, whereas for the oot task, the
semantic methods seem to perform better.
</bodyText>
<sectionHeader confidence="0.998676" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999978727272727">
The SUBFINDER system participated in the best and
the oot tracks of the lexical substitution task. The
best track calls for any number of best guesses,
with the most promising one listed first. The credit
for each correct guess is divided by the number of
guesses. The oot track allows systems to make up to
10 guesses, without penalizing, and without being of
any benefit if less than 10 substitutes are provided.
The ordering of guesses in the oot metric is unim-
portant.
For both tracks, the evaluation is carried out using
precision and recall, calculated based on the number
of matching responses between the system and the
human annotators, respectively. A “mode” evalua-
tion is also conducted, which measures the ability of
the systems to capture the most frequent response
(the “mode”) from the gold standard annotations.
For details, please refer to the official task descrip-
tion document (McCarthy and Navigli, 2007).
Tables 2 and 3 show the results obtained by SUB-
FINDER in the best and oot tracks respectively. The
tables also show a breakdown of the results based
</bodyText>
<figure confidence="0.859326666666667">
1
Am m
rci
</figure>
<page confidence="0.996913">
412
</page>
<bodyText confidence="0.998840666666667">
on: only target words that were not identified as
multiwords (NMWT); only substitutes that were not
identified as multiwords (NMWS); only items with
sentences randomly selected from the Internet cor-
pus (RAND); only items with sentences manually se-
lected from the Internet corpus (MAN).
</bodyText>
<table confidence="0.992248333333333">
WSD LSA IR LB MCS MTA MTG LM
best 34 2 64 63 56 69 38 97
oot 6 82 7 28 46 14 32 68
</table>
<tableCaption confidence="0.996579">
Table 1: Weights of the individual ranking methods
</tableCaption>
<table confidence="0.978718333333333">
P R Mode P Mode R
OVERALL 12.77 12.77 20.73 20.73
Further Analysis
NMWT 13.46 13.46 21.63 21.63
NMWS 13.79 13.79 21.59 21.59
RAND 12.85 12.85 20.18 20.18
MAN 12.69 12.69 21.35 21.35
Baselines
WORDNET 9.95 9.95 15.28 15.28
LIN 8.84 8.53 14.69 14.23
Table 2: BEST results
P R Mode P Mode R
OVERALL 49.19 49.19 66.26 66.26
Further Analysis
NMWT 51.13 51.13 68.03 68.03
NMWS 54.01 54.01 70.15 70.15
RAND 51.71 51.71 68.04 68.04
MAN 46.26 46.26 64.24 64.24
Baselines
WORDNET 29.70 29.35 40.57 40.57
LIN 27.70 26.72 40.47 39.19
</table>
<tableCaption confidence="0.99934">
Table 3: OOT results
</tableCaption>
<bodyText confidence="0.99936465">
Compared to other systems participating in this
task, our system consistently ranks on the first or
second place. SUBFINDER clearly outperforms all
the other systems for the “mode” evaluation, show-
ing the ability of the system to find the substitute
most often preferred by the human annotators. In
addition, the system exceeds by a large margin all
the baselines calculated for the task, which select
substitutes based on existing lexical resources (e.g.,
WordNet or Lin distributional similarity).
Separate from the “official” submission, we ran
a second experiment where we optimized the com-
bination weights targeting high precision and recall
(rather than high mode). An evaluation of the system
using this new set of weights yields a precision and
recall of 13.34 with a mode of 21.71 for the best task,
surpassing the best system according to the anony-
mous results report. For the oot task, the precision
and recall increased to 50.30, still maintaining sec-
ond place.
</bodyText>
<sectionHeader confidence="0.999285" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99999405882353">
The lexical substitution task goes beyond simple
word sense disambiguation. To approach such a
task, we first need a good comprehensive and precise
lexical resource for candidate extraction. Secondly,
we need to semantically filter the highly diverse and
ambiguous set of candidates, while taking into ac-
count their fitness in the context in order to form
a proper linguistic expression. To accomplish this,
we built a system that incorporates lexical, semantic,
and probabilistic methods to capture both the seman-
tic similarity with the target word and the semantic
fit in the context. Compared to other systems partic-
ipating in this task, our system consistently ranks on
the first or second place. SUBFINDER clearly out-
performs all the other systems for the “mode” eval-
uation, proving its ability to find the substitute most
often preferred by the human annotators.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999866166666667">
This work was supported in part by the Texas Ad-
vanced Research Program under Grant #003594.
The authors are grateful to the Language and Infor-
mation Technologies research group at the Univer-
sity of North Texas for many useful discussions and
feedback on this work.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999682958333333">
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings of the In-
ternational Conference on Computational Linguistics
ACL/COLING 2006.
C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
D. McCarthy and R. Navigli. 2007. The semeval English
lexical substitution task. In Proceedings of the ACL
Semeval workshop.
D. McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
</reference>
<page confidence="0.998985">
413
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890353">
<title confidence="0.9963885">UNT: SubFinder: Combining Knowledge Sources for Automatic Lexical Substitution</title>
<author confidence="0.982405">Andras Csomai Hassan</author>
<author confidence="0.982405">Carmen Banea</author>
<author confidence="0.982405">Ravi Sinha</author>
<author confidence="0.982405">Rada</author>
<affiliation confidence="0.97004">Department of Computer Science and Engineering University of North Texas</affiliation>
<email confidence="0.998894">samer@unt.edu,csomaia@unt.edu,carmenb@unt.edu,rss0089@unt.edu,rada@cs.unt.edu</email>
<abstract confidence="0.997333636363636">This paper describes the University of North The system is able to provide the most likely set of substitutes for a word in a given context, by combining several techniques and knowlsources. successfully in the of ten (oot) in the substitution task, consistently ranking in the first or second place.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<title>Web 1t 5-gram version 1. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="8634" citStr="Brants and Franz, 2006" startWordPosition="1383" endWordPosition="1386">n arise from the different morphological inflections of the target word or the candidate and the bias that the context might have toward any specific inflection, we generalize P(c|g) to take into account all the inflections of the selected candidate as shown in equation 2. n Pn(c|g) ≈ Count(cZ, g) (2) Z=1 where n is the number of possible inflections for the candidate c. We use the Google N-gram dataset to calculate the term Count(cZ g). The Google N-gram corpus is a 411 collection of English N-grams, ranging from one to five N-grams, and their respective frequency counts observed on the Web (Brants and Franz, 2006). In order for the model to give high preference to the longer N-grams, while maintaining the relative frequencies of the shorter N-grams (typically more frequent), we augment the counts of the higher order N-grams with the maximum counts of the lower order N-grams, hence guaranteeing that the score assigned to an N-gram of order N is higher than the the score of an N-gram of order N − 1. Semantic Relatedness using Latent Semantic Analysis (LSA): We expect to find a strong semantic relationship between a good candidate and the target context. A relatively simple and efficient way to measure su</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1t 5-gram version 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>A Gliozzo</author>
<author>E Marmorshtein</author>
<author>C Strapparava</author>
</authors>
<title>Direct word sense matching for lexical substitution.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics ACL/COLING</booktitle>
<contexts>
<context position="10831" citStr="Dagan et al., 2006" startWordPosition="1752" endWordPosition="1756"> words available in the given context. Similar to the previous models, the target word in the context is replaced by all the generated inflections of the selected candidate and then queried using a web search engine. The resulting rank represents the sum of the total number of pages in which the candidate or any of its inflections occur together with the context. This also reflects the semantic relatedness or the relevance of the candidate to the context. Word Sense Disambiguation (WSD): Since previous work indicated the usefulness of word sense disambiguation systems in lexical substitution (Dagan et al., 2006), we use the SenseLearner word sense disambiguation tool (Mihalcea and Csomai, 2005) to disambiguate the target word and, accordingly, to propose its synonyms as candidates. Final System: Our candidate ranking methods are aimed at different aspects of what constitutes a good candidate. On one hand, we measure the semantic relatedness of a candidate with the original context (the LSA and WSD methods fall under this category). On the other hand, we also want to ensure that the candidate fits the context and leads to a well formed English sentence (e.g., the language model method). Given that the</context>
</contexts>
<marker>Dagan, Glickman, Gliozzo, Marmorshtein, Strapparava, 2006</marker>
<rawString>I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein, and C. Strapparava. 2006. Direct word sense matching for lexical substitution. In Proceedings of the International Conference on Computational Linguistics ACL/COLING 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet, An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2076" citStr="Fellbaum, 1998" startWordPosition="308" endWordPosition="309">or a given word occurrence. Thus, lexical substitution can be regarded in a way as a hybrid task that combines word sense disambiguation and distributional similarity, targeting the identification of semantically similar words that fit the context. 2 A system for lexical substitution SUBFINDER is a system able to provide the most likely set of substitutes for a word in a given context. ∗Contact author. In SUBFINDER, the lexical substitution task is carried out as a sequence of two steps. First, candidates are extracted from a variety of knowledge sources; so far, we experimented with WordNet (Fellbaum, 1998), Microsoft Encarta encyclopedia, Roget, as well as synonym sets generated from bilingual dictionaries, but additional knowledge sources can be integrated as well. Second, provided a list of candidates, a number of ranking methods are applied in a weighted combination, resulting in a final list of lexical substitutes ranked by their semantic fit with both the input target word and the context. 3 Candidate Extraction Candidates are extracted using several lexical resources, which are combined into a larger comprehensive resource. WordNet: WordNet is a large lexical database of English, with wor</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet, An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<contexts>
<context position="9306" citStr="Landauer et al., 1998" startWordPosition="1501" endWordPosition="1504">o the longer N-grams, while maintaining the relative frequencies of the shorter N-grams (typically more frequent), we augment the counts of the higher order N-grams with the maximum counts of the lower order N-grams, hence guaranteeing that the score assigned to an N-gram of order N is higher than the the score of an N-gram of order N − 1. Semantic Relatedness using Latent Semantic Analysis (LSA): We expect to find a strong semantic relationship between a good candidate and the target context. A relatively simple and efficient way to measure such a relatedness is the Latent Semantic Analysis (Landauer et al., 1998). Documents and terms are mapped into a 300 dimensional latent semantic space, providing the ability to measure the semantic relatedness between two words or a word and a context. We use the InfoMap package from Stanford University’s Center for the Study of Language and Information, trained on a collection of approximately one million Wikipedia articles. The rank of a candidate is given by its semantic relatedness to the entire context sentence. Information Retrieval (IR): Although the Language Model approach is successful in ranking the candidates, it suffers from the small N-gram size impose</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to latent semantic analysis. Discourse Processes, 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>The semeval English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Semeval workshop.</booktitle>
<contexts>
<context position="13236" citStr="McCarthy and Navigli, 2007" startWordPosition="2158" endWordPosition="2161">make up to 10 guesses, without penalizing, and without being of any benefit if less than 10 substitutes are provided. The ordering of guesses in the oot metric is unimportant. For both tracks, the evaluation is carried out using precision and recall, calculated based on the number of matching responses between the system and the human annotators, respectively. A “mode” evaluation is also conducted, which measures the ability of the systems to capture the most frequent response (the “mode”) from the gold standard annotations. For details, please refer to the official task description document (McCarthy and Navigli, 2007). Tables 2 and 3 show the results obtained by SUBFINDER in the best and oot tracks respectively. The tables also show a breakdown of the results based 1 Am m rci 412 on: only target words that were not identified as multiwords (NMWT); only substitutes that were not identified as multiwords (NMWS); only items with sentences randomly selected from the Internet corpus (RAND); only items with sentences manually selected from the Internet corpus (MAN). WSD LSA IR LB MCS MTA MTG LM best 34 2 64 63 56 69 38 97 oot 6 82 7 28 46 14 32 68 Table 1: Weights of the individual ranking methods P R Mode P Mod</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. The semeval English lexical substitution task. In Proceedings of the ACL Semeval workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
</authors>
<title>Lexical substitution as a task for wsd evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="876" citStr="McCarthy, 2002" startWordPosition="126" endWordPosition="127">@unt.edu, rss0089@unt.edu, rada@cs.unt.edu Abstract This paper describes the University of North Texas SUBFINDER system. The system is able to provide the most likely set of substitutes for a word in a given context, by combining several techniques and knowledge sources. SUBFINDER has successfully participated in the best and out of ten (oot) tracks in the SEMEVAL lexical substitution task, consistently ranking in the first or second place. 1 Introduction Lexical substitution is defined as the task of identifying the most likely alternatives (substitutes) for a target word, given its context (McCarthy, 2002). Many natural language processing applications can benefit from the availability of such alternative words, including word sense disambiguation, lexical acquisition, machine translation, information retrieval, question answering, text simplification, and others. The task is closely related to the problem of word sense disambiguation, with the substitutes acting as synonyms for the input word meaning. Unlike word sense disambiguation however, lexical substitution is not performed with respect to a given sense inventory, but instead candidate synonyms are generated “on the fly” for a given word</context>
</contexts>
<marker>McCarthy, 2002</marker>
<rawString>D. McCarthy. 2002. Lexical substitution as a task for wsd evaluation. In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>A Csomai</author>
</authors>
<title>Senselearner: Word sense disambiguation for all words in unrestricted text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="10915" citStr="Mihalcea and Csomai, 2005" startWordPosition="1765" endWordPosition="1768">rget word in the context is replaced by all the generated inflections of the selected candidate and then queried using a web search engine. The resulting rank represents the sum of the total number of pages in which the candidate or any of its inflections occur together with the context. This also reflects the semantic relatedness or the relevance of the candidate to the context. Word Sense Disambiguation (WSD): Since previous work indicated the usefulness of word sense disambiguation systems in lexical substitution (Dagan et al., 2006), we use the SenseLearner word sense disambiguation tool (Mihalcea and Csomai, 2005) to disambiguate the target word and, accordingly, to propose its synonyms as candidates. Final System: Our candidate ranking methods are aimed at different aspects of what constitutes a good candidate. On one hand, we measure the semantic relatedness of a candidate with the original context (the LSA and WSD methods fall under this category). On the other hand, we also want to ensure that the candidate fits the context and leads to a well formed English sentence (e.g., the language model method). Given that the methods described earlier aim at orthogonal aspects of the problem, it is expected </context>
</contexts>
<marker>Mihalcea, Csomai, 2005</marker>
<rawString>R. Mihalcea and A. Csomai. 2005. Senselearner: Word sense disambiguation for all words in unrestricted text. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, MI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>