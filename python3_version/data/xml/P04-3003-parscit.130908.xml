<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017122">
<title confidence="0.912498">
Constructing Transliteration Lexicons from Web Corpora
</title>
<author confidence="0.74731">
Jin-Shea Kuo1, 2 Ying-Kuei Yang2
</author>
<bodyText confidence="0.542020666666667">
1Chung-Hwa Telecommunication 2E. E. Dept., National Taiwan University of Science
Laboratories, Taiwan, R. O. C., 326 and Technology, Taiwan, R.O.C., 106
jskuo@cht.com.tw ykyang@mouse.ee.ntust.edu.tw
</bodyText>
<sectionHeader confidence="0.963338" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999842666666667">
This paper proposes a novel approach to automating
the construction of transliterated-term lexicons. A
simple syllable alignment algorithm is used to
construct confusion matrices for cross-language
syllable-phoneme conversion. Each row in the
confusion matrix consists of a set of syllables in the
source language that are (correctly or erroneously)
matched phonetically and statistically to a syllable in
the target language. Two conversions using
phoneme-to-phoneme and text-to-phoneme
syllabification algorithms are automatically deduced
from a training corpus of paired terms and are used
to calculate the degree of similarity between
phonemes for transliterated-term extraction. In a
large-scale experiment using this automated learning
process for conversions, more than 200,000
transliterated-term pairs were successfully extracted
by analyzing query results from Internet search
engines. Experimental results indicate the proposed
approach shows promise in transliterated-term
extraction.
</bodyText>
<sectionHeader confidence="0.992509" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940494949495">
Machine transliteration plays an important role in
machine translation. The importance of term
transliteration can be realized from our analysis of
the terms used in 200 qualifying sentences that were
randomly selected from English-Chinese mixed news
pages. Each qualifying sentence contained at least
one English word. Analysis showed that 17.43% of
the English terms were transliterated, and that most
of them were content words (words that carry
essential meaning, as opposed to grammatical
function words such as conjunctions, prepositions,
and auxiliary verbs).
In general, a transliteration process starts by first
examining a pre-compiled lexicon which contains
many transliterated-term pairs collected manually or
automatically. If a term is not found in the lexicon,
the transliteration system then deals with this out-of-
vocabulary (OOV) term to try to generate a
transliterated-term via a sequence of pipelined
conversions (Knight, 1998). Before this issue can be
dealt with, a large quantity of transliterated-term
pairs are required to train conversion models.
Preparing a lexicon composed of transliterated term
pairs is time- and labor-intensive. Constructing such
a lexicon automatically is the most important goal of
this paper. The problem is how to collect
transliterated-term pairs from text resources.
Query logs recorded by Internet search engines
reveal users&apos; intentions and contain much information
about users&apos; behaviors. (Brill, 2001) proposed an
interactive process that used query logs for extracting
English-Japanese transliterated-terms. Under this
method, a large initial number of term pairs were
compiled manually. It is time-consuming to prepare
such an initial training set, and the resource used is
not publicly accessible.
The Internet is one of the largest distributed
databases in the world. It comprises various kinds of
data and at the same time is growing rapidly. Though
the World Wide Web is not systematically organized,
much invaluable information can be obtained from
this large text corpus. Many researchers dealing with
natural language processing, machine translation,
and information retrieval have focused on exploiting
such non-parallel Web data (Al-Onaizan, 2002; Fung,
1998;). Also, online texts contain the latest terms that
may not be found in existing dictionaries. Regularly
exploring Web corpora is a good way to update
dictionaries.
Transliterated-term extraction using non-parallel
corpora has also been conducted (Kuo, 2003).
Automated speech recognition-generated confusion
matrices (AGCM) have been used successfully to
bootstrap term extraction from Web pages collected
by a software spider.
AGCM were used successfully not only to alleviate
pronunciation variation, especially the sociolinguistic
causes, but also to construct a method for cross-
language syllable-phoneme conversion (CLSPC).
This is a mapping from a source-language syllable
into its target-language counterpart. The problem is
how to produce such conversions if AGCM are not
available for the targeted language pair. To generate
confusion matrices from automated speech
recognition requires the effort of collecting many
speech corpora for model training, costing time and
labor. Automatically constructing a CLSPC without
AGCM is the other main focus of this paper.
Web pages, which are dynamically updated and
publicly accessible, are important to many
researchers. However, if many personally guided
spiders were simultaneously collecting Web pages,
they might cause a network traffic jam. Internet
search engines, which update their data periodically,
provide search services that are also publicly
accessible. A user can select only the pages of
interest from Internet search engines; this mitigates
the possibility that a network traffic jam will be
caused by many personally guided spiders.
Possibly aligned candidate strings in two languages,
which may belong to two completely different
language families, are selected using local context
analysis from non-parallel corpora (Kuo, 2003). In
order to determine the degree of similarity between
possible candidate strings, a method for converting
such aligned terms cross-linguistically into the same
representation in syllables is needed. A syllable is the
basic pronunciation unit used in this paper. The tasks
discussed in this paper are first to align syllables
cross-linguistically, then to construct a cross-
linguistic relation, and third to use the trained
relation to extract transliterated-term pairs.
The remainder of the paper is organized as follows:
Section 2 describes how English-Chinese
transliterated-term pairs can be extracted
automatically. Experimental results are presented in
Section 3. Section 4 analyzes on the performance
achieved by the extraction. Conclusions are drawn in
Section 5.
</bodyText>
<sectionHeader confidence="0.976765" genericHeader="method">
2. The Proposed Approach
</sectionHeader>
<bodyText confidence="0.999956605263158">
An algorithm based on minimizing the edit distance
between words with the same representation has
been proposed (Brill, 2001). However, the mapping
between cross-linguistic phonemes is obtained only
after the cross-linguistic relation is constructed. Such
a relation is not available at the very beginning.
A simple and fast approach is proposed here to
overcome this problem. Initially, 200 verified correct
English-Chinese transliterated-term pairs are
collected manually. One of the most important
attributes of these term pairs is that the numbers of
syllables in the source-language term and the target-
language term are equal. The syllables of both
languages can also be decomposed further into
phonemes. The algorithm that adopts equal syllable
numbers to align syllables and phonemes cross-
linguistically is called the simple syllable alignment
algorithm (SSAA). This algorithm generates syllable
and phoneme mapping tables between the source and
target languages. These two mapping tables can be
used to calculate similarity between candidate strings
in transliterated-term extraction. With the mapping,
the transliterated-term pairs can be extracted. The
obtained term pairs can be selected according to the
criterion of equal syllable segments. These qualified
term pairs can then be merged with the previous set
to form a larger set of qualified term pairs. The new
set of qualified term pairs can be used again to
construct a new cross-linguistic mapping for the next
term extraction. This process iterates until no more
new term pairs are produced or until other criteria are
met. The conversions used in the last round of the
training phase are then used to extract large-scale
transliterated-term pairs from query results.
Two types of cross-linguistic relations, phoneme-
to-phoneme (PP) and text-to-phoneme (TP), can be
used depending on whether a source-language letter-
to-sound system is available or not.
</bodyText>
<subsectionHeader confidence="0.999244">
2.1 Construction of a Relation Using Phoneme-to-
Phoneme Mapping
</subsectionHeader>
<bodyText confidence="0.999746782608696">
If a letter-to-phoneme system is available, a
phoneme-based syllabification algorithm (PSA) is
used for constructing a cross-linguistic relation, then
a phoneme-to-phoneme (PP) mapping is selected.
Each word in the located English string is converted
into phonemes using MBRDICO (Pagel, 1998). In
order to compare English terms with Chinese terms
in syllables, the generated English phonemes are
syllabified into consonant-vowel pairs. Each
consonant-vowel pair is then converted into a
Chinese syllable. The PSA used here is basically the
same as the classical one (Jurafsky, 2000), but has
some minor modifications. Traditionally, an English
syllable is composed of an initial consonant cluster
followed by a vowel and then a final consonant
cluster. However, in order to convert English
syllables to Chinese ones, the final consonant cluster
is appended only when it is a nasal. The other
consonants in the final consonant cluster are then
segmented into isolated consonants. Such a syllable
may be viewed as the basic pronunciation unit in
transliterated-term extraction.
After English phonemes are grouped into syllables,
the English syllables can be converted into Chinese
ones according to the results produced by using
SSAA. The accuracy of the conversion can improve
progressively if the cross-linguistic relation is
deduced from a large quantity of transliterated-term
pairs.
Take the word “polder” as an example. First, it is
converted into /polda/ using the letter-to-phoneme
system, and then according to the phoneme-based
syllabification algorithm (PSA), it is divided into /po/,
/l/, and /da/, where /l/ is an isolated consonant.
Second, these English syllables are then converted
into Chinese syllables using the trained cross-
linguistic relation; for example, /po/, /l/, and /dO/ are
converted into /po/, /er/, and /de/ (in Pin-yin),
respectively. /l/ is a syllable with only an isolated
consonant. A final is appended to its converted
Chinese syllable in order to make it complete
because not all Chinese initials are legal syllables.
The other point worth noting is that /l/, a consonant
in English, is converted into its Chinese equivalent,
/er/, but, /er/ is a final (a kind of complex vowel) in
Chinese.
</bodyText>
<subsectionHeader confidence="0.9998185">
2.2 Construction of a Relation Using Text-to-
Phoneme Mapping
</subsectionHeader>
<bodyText confidence="0.9999778">
If a source language letter-to-phoneme system is
not available, a simple text-based syllabification
algorithm (TSA) is used and a text-to-phoneme (TP)
mapping is selected. An English word is frequently
composed of multiple syllables; whereas, every
Chinese character is a monosyllable. First, each
English character in an English term is identified as a
consonant, a vowel or a nasal. For example, the
characters “a”, “b” and “n” are viewed as a vowel, a
consonant and a nasal, respectively. Second,
consecutive characters of the same attribute form a
cluster. However, some characters, such as “ch”,
“ng” and “ph”, always combine together to form
complex consonants. Such complex consonants are
also taken into account in the syllabification process.
A Chinese syllable is composed of an initial and a
final. An initial is similar to a consonant in English,
and a final is analogous to a vowel or a combination
of a vowel and a nasal. Using the proposed simple
syllable alignment algorithm, a conversion using TP
mapping can be produced. The conversion can also
be used in transliterated-term extraction from non-
parallel web corpora.
The automated construction of a cross-linguistic
mapping eliminates the dependency on AGCM
reported in (Kuo, 2003) and makes transliterated-
term extraction for other language pairs possible. The
cross-linguistic relation constructed using TSA and
TP is called CTP; on the other hand, the cross-
linguistic relation using PSA and PP is called CPP.
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="method">
3 The Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.9818855">
3.1 Training Cross-language Syllable-phoneme
Conversions
</subsectionHeader>
<bodyText confidence="0.989113">
An English-Chinese text corpus of 500MB in
15,822,984 pages, which was collected from the
Internet using a web spider and was converted to
plain text, was used as a training set. This corpus is
called SET1. From SET1, 80,094 qualifying
sentences that occupied 5MB were extracted. A
qualifying sentence was a sentence composed of at
least one English string.
Two experiments were conducted using either CPP
or CTP on SET1. Figure 1 shows the progress of
extracting transliterated-term pairs achieved using
CPP mapping. A noteworthy phenomenon was that
phoneme conversion produced more term pairs than
syllable conversion did at the very beginning of
training. This is because, initially, the quality of the
syllable combinations is not good enough. The
phonemes exerted finer-grained control than
syllables did. However, when the generated syllable
combinations improved in quality, the situation
changed. Finally, extraction performed using syllable
conversion outperformed that achieved using
phoneme conversion. Note also that the results
produced by using phonemes quickly approached the
saturation state. This is because the English phoneme
set is small. When phonemes were used
independently to perform term extraction, fewer
extracted term pairs were produced than were
produced using syllables or a combination of
syllables and phonemes.
</bodyText>
<figureCaption confidence="0.990911428571429">
Figure 1. The progress of extracting transliterated-
term pairs using CPP conversion
Figure 2 shows the progress of extracting
transliterated-term pairs using CTP. The same
situation also occurred at the very beginning of
training. Comparing the results generated using CPP
and CTP, CPP outperformed CTP in terms of the
quantity of extracted term pairs because the
combinations obtained using TSA are larger than
those obtained using PSA. This is also revealed by
the results generated at iteration 1 and shown in
Figures 1 and 2.
Figure 2. The progress of extracting transliterated-
term pairs using CTP conversion.
</figureCaption>
<figure confidence="0.870752194444444">
7000
6500
6000
5500
5000
4500
4000
Syllable (S)
Phoneme (P)
S+P
500
0
Iter #1 Iter #2 Iter #3 Iter #4 Iter #5 Iter #6
3500
3000
2500
2000
1500
1000
6000
Iter #1 Iter #2 Iter #3 Iter #4 Iter #5 Iter #6
4500
4000
2500
2000
5500
5000
3500
3000
1500
1000
500
0
Syllable (S)
Phoneme (P)
S+P
</figure>
<subsectionHeader confidence="0.974943">
3.2 Transliterated-term Extraction
</subsectionHeader>
<bodyText confidence="0.999863756756757">
The Web is growing rapidly. It is a rich information
source for many researchers. Internet search engines
have collected a huge number of Web pages for
public searching (Brin, 1998). Submitting queries to
these search engines and analyzing the results can
help researchers to understand the usages of
transliterated-term pairs.
Query results are text snippets shown in a page
returned from an Internet search engine in response
to a query. These text snippets may be composed of
texts that are extracted from the beginning of pages
or from the texts around the keywords matched in the
pages. Though a snippet presents only a portion of
the full text, it provides an alternative way to
summarize the pages matched.
Initially, 200 personal names were randomly
selected from the names in the 1990 census
conducted by the US Census Bureau1 as queries to
be submitted to Internet search engines. CPP and
CTP were obtained in the last round of the training
phase. The estimated numbers of distinct qualifying
term pairs (EDQTP) obtained by analyzing query
results and by using CPP and CTP mappings for 7
days are shown in Table 1. A qualifying term pair
means a term pair that is verified manually to be
correct. EDQTP are term pairs that are not verified
manually but are estimated according to the precision
achieved during the training phase.
Finally, a text corpus called SET2 was obtained by
iteratively submitting queries to search engines.
SET2 occupies 3.17GB and is composed of 67,944
pages in total. The term pairs extracted using CTP
were much fewer in number than those extracted
using CPP. This is because the TSA used in this
study, though effective, is very simple and
rudimentary. A finer-grained syllabification
algorithm would improve performance.
</bodyText>
<table confidence="0.792177">
CPP CTP
EDQTP 201,732 110,295
</table>
<tableCaption confidence="0.991923">
Table 1. The term pairs extracted from Internet
search engines using PP and TP mappings.
</tableCaption>
<sectionHeader confidence="0.999467" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999873333333333">
Comparing the performances achieved by CPP and
CTP, the results obtained by using CPP were better
than those with CTP. The reason is that TSA is very
simple. A better TSA would produce better results.
Though TSA is simple, it is still effective in
automatically extracting a large quantity of term
</bodyText>
<footnote confidence="0.942656">
1http://www.census.gov/genealogy/names/
</footnote>
<bodyText confidence="0.9997312">
pairs. Also, TSA has an advantage over PSA is that
no letter-to-phoneme system is required. It could be
helpful when applying the proposed approach to
other language pairs, where such a mapping may not
be available.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999926944444445">
An approach to constructing transliterated-term
lexicons has been presented in this paper. A simple
alignment algorithm has been used to automatically
construct confusion matrices for cross-language
syllable-phoneme conversion using phoneme-to-
phoneme (PP) and text-to-phoneme (TP)
syllabification algorithms. The proposed approach
not only reduces the need for using automated
speech recognition-generated confusion matrices, but
also eliminates the need for a letter-to-phoneme
system for source-language terms if TP is used to
construct a cross-language syllable-phoneme
conversion and to successfully extract transliterated-
term pairs from query results returned by Internet
search engines. The performance achieved using PP
and TP has been compared and discussed. The
overall experimental results show that this approach
is very promising for transliterated-term extraction.
</bodyText>
<sectionHeader confidence="0.999038" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999802896551724">
Al-Onaizan Y. and Knight K. 2002. Machine
Transliteration of Names in Arabic Text, In Proceedings
of ACL Workshop on Computational Approaches to
Semitic Languages, pp. 34-46.
Brill E., Kacmarcik G., Brockett C. 2001. Automatically
Harvesting Katakana-English Term Pairs from Search
Engine Query Logs, In Proceedings of Natural
Language Processing Pacific Rim Symposium, pp. 393-
399.
Brin S. and Page L. 1998. The Anatomy of a Large-scale
Hypertextual Web Search Engine, In Proceedings of 7th
International World Wide Web Conference, pp. 107-117.
Fung P. and Yee L.-Y. 1998. An IR Approach for
Translating New Words from Nonparallel, Comparable
Texts. In Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and 7th
International Conference on Computational Linguistics,
pp. 414-420.
Jurafsky D. and Martin J. H. 2000. Speech and Language
Processing, pp. 102-120, Prentice-Hall, New Jersey.
Knight K. and Graehl J. 1998. Machine Transliteration,
Computational Linguistics, Vol. 24, No. 4, pp.599-612.
Kuo J. S. and Yang Y. K. 2003. Automatic Transliterated-
term Extraction Using Confusion Matrix from Non-
parallel Corpora, In Proceedings of ROCLING XV
Computational Linguistics Conference, pp.17-32.
Pagel V., Lenzo K., and Black A. 1998. Letter to Sound
Rules for Accented Lexicon Compression, In
Proceedings of ICSLP, pp. 2015-2020.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000141">
<title confidence="0.988013">Constructing Transliteration Lexicons from Web Corpora</title>
<email confidence="0.57245">2Ying-Kuei</email>
<affiliation confidence="0.9649">Telecommunication E. Dept., National Taiwan University of Science</affiliation>
<address confidence="0.980278">Laboratories, Taiwan, R. O. C., 326 and Technology, Taiwan, R.O.C., 106</address>
<email confidence="0.758136">jskuo@cht.com.twykyang@mouse.ee.ntust.edu.tw</email>
<abstract confidence="0.997211254166666">This paper proposes a novel approach to automating the construction of transliterated-term lexicons. A simple syllable alignment algorithm is used to construct confusion matrices for cross-language syllable-phoneme conversion. Each row in the confusion matrix consists of a set of syllables in the source language that are (correctly or erroneously) matched phonetically and statistically to a syllable in the target language. Two conversions using phoneme-to-phoneme and text-to-phoneme syllabification algorithms are automatically deduced from a training corpus of paired terms and are used to calculate the degree of similarity between phonemes for transliterated-term extraction. In a large-scale experiment using this automated learning process for conversions, more than 200,000 transliterated-term pairs were successfully extracted by analyzing query results from Internet search engines. Experimental results indicate the proposed approach shows promise in transliterated-term extraction. Machine transliteration plays an important role in machine translation. The importance of term transliteration can be realized from our analysis of the terms used in 200 qualifying sentences that were randomly selected from English-Chinese mixed news pages. Each qualifying sentence contained at least one English word. Analysis showed that 17.43% of the English terms were transliterated, and that most of them were content words (words that carry essential meaning, as opposed to grammatical function words such as conjunctions, prepositions, and auxiliary verbs). In general, a transliteration process starts by first examining a pre-compiled lexicon which contains many transliterated-term pairs collected manually or automatically. If a term is not found in the lexicon, the transliteration system then deals with this out-ofvocabulary (OOV) term to try to generate a transliterated-term via a sequence of pipelined conversions (Knight, 1998). Before this issue can be dealt with, a large quantity of transliterated-term pairs are required to train conversion models. Preparing a lexicon composed of transliterated term pairs is timeand labor-intensive. Constructing such a lexicon automatically is the most important goal of this paper. The problem is how to collect transliterated-term pairs from text resources. Query logs recorded by Internet search engines reveal users&apos; intentions and contain much information about users&apos; behaviors. (Brill, 2001) proposed an interactive process that used query logs for extracting English-Japanese transliterated-terms. Under this method, a large initial number of term pairs were compiled manually. It is time-consuming to prepare such an initial training set, and the resource used is not publicly accessible. The Internet is one of the largest distributed databases in the world. It comprises various kinds of data and at the same time is growing rapidly. Though the World Wide Web is not systematically organized, much invaluable information can be obtained from this large text corpus. Many researchers dealing with natural language processing, machine translation, and information retrieval have focused on exploiting such non-parallel Web data (Al-Onaizan, 2002; Fung, 1998;). Also, online texts contain the latest terms that may not be found in existing dictionaries. Regularly exploring Web corpora is a good way to update dictionaries. Transliterated-term extraction using non-parallel corpora has also been conducted (Kuo, 2003). Automated speech recognition-generated confusion matrices (AGCM) have been used successfully to bootstrap term extraction from Web pages collected by a software spider. AGCM were used successfully not only to alleviate pronunciation variation, especially the sociolinguistic causes, but also to construct a method for crosslanguage syllable-phoneme conversion (CLSPC). This is a mapping from a source-language syllable into its target-language counterpart. The problem is how to produce such conversions if AGCM are not available for the targeted language pair. To generate confusion matrices from automated speech recognition requires the effort of collecting many speech corpora for model training, costing time and labor. Automatically constructing a CLSPC without AGCM is the other main focus of this paper. Web pages, which are dynamically updated and publicly accessible, are important to many researchers. However, if many personally guided spiders were simultaneously collecting Web pages, they might cause a network traffic jam. Internet search engines, which update their data periodically, provide search services that are also publicly accessible. A user can select only the pages of interest from Internet search engines; this mitigates the possibility that a network traffic jam will be caused by many personally guided spiders. Possibly aligned candidate strings in two languages, which may belong to two completely different language families, are selected using local context analysis from non-parallel corpora (Kuo, 2003). In order to determine the degree of similarity between possible candidate strings, a method for converting such aligned terms cross-linguistically into the same representation in syllables is needed. A syllable is the basic pronunciation unit used in this paper. The tasks discussed in this paper are first to align syllables cross-linguistically, then to construct a crosslinguistic relation, and third to use the trained relation to extract transliterated-term pairs. The remainder of the paper is organized as follows: Section 2 describes how English-Chinese transliterated-term pairs can be extracted automatically. Experimental results are presented in Section 3. Section 4 analyzes on the performance achieved by the extraction. Conclusions are drawn in Section 5. 2. The Proposed Approach An algorithm based on minimizing the edit distance between words with the same representation has been proposed (Brill, 2001). However, the mapping between cross-linguistic phonemes is obtained only after the cross-linguistic relation is constructed. Such a relation is not available at the very beginning. A simple and fast approach is proposed here to overcome this problem. Initially, 200 verified correct English-Chinese transliterated-term pairs are collected manually. One of the most important attributes of these term pairs is that the numbers of syllables in the source-language term and the targetlanguage term are equal. The syllables of both languages can also be decomposed further into phonemes. The algorithm that adopts equal syllable numbers to align syllables and phonemes crosslinguistically is called the simple syllable alignment algorithm (SSAA). This algorithm generates syllable and phoneme mapping tables between the source and target languages. These two mapping tables can be used to calculate similarity between candidate strings in transliterated-term extraction. With the mapping, the transliterated-term pairs can be extracted. The obtained term pairs can be selected according to the criterion of equal syllable segments. These qualified term pairs can then be merged with the previous set to form a larger set of qualified term pairs. The new set of qualified term pairs can be used again to construct a new cross-linguistic mapping for the next term extraction. This process iterates until no more new term pairs are produced or until other criteria are met. The conversions used in the last round of the training phase are then used to extract large-scale transliterated-term pairs from query results. Two types of cross-linguistic relations, phonemeto-phoneme (PP) and text-to-phoneme (TP), can be used depending on whether a source-language letterto-sound system is available or not. 2.1 Construction of a Relation Using Phoneme-to- Phoneme Mapping If a letter-to-phoneme system is available, a phoneme-based syllabification algorithm (PSA) is used for constructing a cross-linguistic relation, then a phoneme-to-phoneme (PP) mapping is selected. Each word in the located English string is converted into phonemes using MBRDICO (Pagel, 1998). In order to compare English terms with Chinese terms in syllables, the generated English phonemes are syllabified into consonant-vowel pairs. Each consonant-vowel pair is then converted into a Chinese syllable. The PSA used here is basically the same as the classical one (Jurafsky, 2000), but has some minor modifications. Traditionally, an English syllable is composed of an initial consonant cluster followed by a vowel and then a final consonant cluster. However, in order to convert English syllables to Chinese ones, the final consonant cluster is appended only when it is a nasal. The other consonants in the final consonant cluster are then segmented into isolated consonants. Such a syllable may be viewed as the basic pronunciation unit in transliterated-term extraction. After English phonemes are grouped into syllables, the English syllables can be converted into Chinese ones according to the results produced by using SSAA. The accuracy of the conversion can improve progressively if the cross-linguistic relation is deduced from a large quantity of transliterated-term pairs. the word an example. First, it is into using the letter-to-phoneme system, and then according to the phoneme-based syllabification algorithm (PSA), it is divided into /po/, and where /l/ is an isolated consonant. Second, these English syllables are then converted Chinese syllables using the trained crossrelation; for example, /po/, /l/, and are converted into /po/, /er/, and /de/ (in Pin-yin), respectively. /l/ is a syllable with only an isolated consonant. A final is appended to its converted Chinese syllable in order to make it complete because not all Chinese initials are legal syllables. The other point worth noting is that /l/, a consonant in English, is converted into its Chinese equivalent, /er/, but, /er/ is a final (a kind of complex vowel) in Chinese. 2.2 Construction of a Relation Using Text-to- Phoneme Mapping If a source language letter-to-phoneme system is not available, a simple text-based syllabification algorithm (TSA) is used and a text-to-phoneme (TP) mapping is selected. An English word is frequently composed of multiple syllables; whereas, every Chinese character is a monosyllable. First, each English character in an English term is identified as a consonant, a vowel or a nasal. For example, the viewed as a vowel, a consonant and a nasal, respectively. Second, consecutive characters of the same attribute form a However, some characters, such as always combine together to form complex consonants. Such complex consonants are also taken into account in the syllabification process. A Chinese syllable is composed of an initial and a final. An initial is similar to a consonant in English, and a final is analogous to a vowel or a combination of a vowel and a nasal. Using the proposed simple syllable alignment algorithm, a conversion using TP mapping can be produced. The conversion can also be used in transliterated-term extraction from nonparallel web corpora. The automated construction of a cross-linguistic mapping eliminates the dependency on AGCM reported in (Kuo, 2003) and makes transliteratedterm extraction for other language pairs possible. The cross-linguistic relation constructed using TSA and TP is called CTP; on the other hand, the crosslinguistic relation using PSA and PP is called CPP.</abstract>
<note confidence="0.913948">3 The Experimental Results 3.1 Training Cross-language Syllable-phoneme Conversions An English-Chinese text corpus of 500MB in 15,822,984 pages, which was collected from the</note>
<abstract confidence="0.997575268292683">Internet using a web spider and was converted to plain text, was used as a training set. This corpus is called SET1. From SET1, 80,094 qualifying sentences that occupied 5MB were extracted. A qualifying sentence was a sentence composed of at least one English string. Two experiments were conducted using either CPP or CTP on SET1. Figure 1 shows the progress of extracting transliterated-term pairs achieved using CPP mapping. A noteworthy phenomenon was that phoneme conversion produced more term pairs than syllable conversion did at the very beginning of training. This is because, initially, the quality of the syllable combinations is not good enough. The phonemes exerted finer-grained control than syllables did. However, when the generated syllable combinations improved in quality, the situation changed. Finally, extraction performed using syllable conversion outperformed that achieved using phoneme conversion. Note also that the results produced by using phonemes quickly approached the saturation state. This is because the English phoneme set is small. When phonemes were used independently to perform term extraction, fewer extracted term pairs were produced than were produced using syllables or a combination of syllables and phonemes. Figure 1. The progress of extracting transliteratedterm pairs using CPP conversion Figure 2 shows the progress of extracting transliterated-term pairs using CTP. The same situation also occurred at the very beginning of training. Comparing the results generated using CPP and CTP, CPP outperformed CTP in terms of the quantity of extracted term pairs because the combinations obtained using TSA are larger than those obtained using PSA. This is also revealed by the results generated at iteration 1 and shown in Figures 1 and 2. Figure 2. The progress of extracting transliteratedterm pairs using CTP conversion.</abstract>
<address confidence="0.936506857142857">7000 6500 6000 5500 5000 4500 4000</address>
<note confidence="0.7030315">Syllable (S) Phoneme (P) S+P 500 0 Iter #1 Iter #2 Iter #3 Iter #4 Iter #5 Iter #6</note>
<address confidence="0.939800833333333">3500 3000 2500 2000 1500 1000 6000 Iter #1 Iter #2 Iter #3 Iter #4 Iter #5 Iter #6 4500 4000 2500 2000 5500 5000 3500 3000 1500 1000</address>
<note confidence="0.6287315">500 0</note>
<title confidence="0.57853125">Syllable (S) Phoneme (P) S+P 3.2 Transliterated-term Extraction</title>
<abstract confidence="0.994275805555556">The Web is growing rapidly. It is a rich information source for many researchers. Internet search engines have collected a huge number of Web pages for public searching (Brin, 1998). Submitting queries to these search engines and analyzing the results can help researchers to understand the usages of transliterated-term pairs. Query results are text snippets shown in a page returned from an Internet search engine in response to a query. These text snippets may be composed of texts that are extracted from the beginning of pages or from the texts around the keywords matched in the pages. Though a snippet presents only a portion of the full text, it provides an alternative way to summarize the pages matched. Initially, 200 personal names were randomly selected from the names in the 1990 census by the US Census as queries to be submitted to Internet search engines. CPP and CTP were obtained in the last round of the training phase. The estimated numbers of distinct qualifying term pairs (EDQTP) obtained by analyzing query results and by using CPP and CTP mappings for 7 days are shown in Table 1. A qualifying term pair means a term pair that is verified manually to be correct. EDQTP are term pairs that are not verified manually but are estimated according to the precision achieved during the training phase. Finally, a text corpus called SET2 was obtained by iteratively submitting queries to search engines. SET2 occupies 3.17GB and is composed of 67,944 pages in total. The term pairs extracted using CTP were much fewer in number than those extracted using CPP. This is because the TSA used in this study, though effective, is very simple and rudimentary. A finer-grained syllabification algorithm would improve performance. CPP CTP EDQTP 201,732 110,295 Table 1. The term pairs extracted from Internet engines using PP and TP 4 Discussion Comparing the performances achieved by CPP and CTP, the results obtained by using CPP were better than those with CTP. The reason is that TSA is very simple. A better TSA would produce better results. Though TSA is simple, it is still effective in automatically extracting a large quantity of term pairs. Also, TSA has an advantage over PSA is that no letter-to-phoneme system is required. It could be helpful when applying the proposed approach to other language pairs, where such a mapping may not be available. 5 Conclusions An approach to constructing transliterated-term lexicons has been presented in this paper. A simple alignment algorithm has been used to automatically construct confusion matrices for cross-language syllable-phoneme conversion using phoneme-tophoneme (PP) and text-to-phoneme (TP) syllabification algorithms. The proposed approach not only reduces the need for using automated speech recognition-generated confusion matrices, but also eliminates the need for a letter-to-phoneme system for source-language terms if TP is used to construct a cross-language syllable-phoneme conversion and to successfully extract transliteratedterm pairs from query results returned by Internet search engines. The performance achieved using PP and TP has been compared and discussed. The overall experimental results show that this approach is very promising for transliterated-term extraction.</abstract>
<note confidence="0.891829533333333">References Al-Onaizan Y. and Knight K. 2002. Machine of Names in Arabic Text, In of ACL Workshop on Computational Approaches to Semitic Languages, pp. 34-46. Brill E., Kacmarcik G., Brockett C. 2001. Automatically Harvesting Katakana-English Term Pairs from Search Query Logs, In of Natural Processing Pacific Rim pp. 393- 399. Brin S. and Page L. 1998. The Anatomy of a Large-scale Web Search Engine, In of World Wide Web Conference, 107-117. Fung P. and Yee L.-Y. 1998. An IR Approach for Translating New Words from Nonparallel, Comparable In Proceedings of the Annual Meeting of the for Computational Linguistics and International Conference on Computational Linguistics, pp. 414-420. Jurafsky D. and Martin J. H. 2000. Speech and Language Processing, pp. 102-120, Prentice-Hall, New Jersey. Knight K. and Graehl J. 1998. Machine Transliteration, Computational Linguistics, Vol. 24, No. 4, pp.599-612. Kuo J. S. and Yang Y. K. 2003. Automatic Transliteratedterm Extraction Using Confusion Matrix from Non- Corpora, In of ROCLING XV Linguistics pp.17-32. Pagel V., Lenzo K., and Black A. 1998. Letter to Sound Rules for Accented Lexicon Compression, In of pp. 2015-2020.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Machine Transliteration of Names in Arabic Text,</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>34--46</pages>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan Y. and Knight K. 2002. Machine Transliteration of Names in Arabic Text, In Proceedings of ACL Workshop on Computational Approaches to Semitic Languages, pp. 34-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>G Kacmarcik</author>
<author>C Brockett</author>
</authors>
<title>Automatically Harvesting Katakana-English Term Pairs from Search Engine Query Logs,</title>
<date>2001</date>
<booktitle>In Proceedings of Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>393--399</pages>
<marker>Brill, Kacmarcik, Brockett, 2001</marker>
<rawString>Brill E., Kacmarcik G., Brockett C. 2001. Automatically Harvesting Katakana-English Term Pairs from Search Engine Query Logs, In Proceedings of Natural Language Processing Pacific Rim Symposium, pp. 393-399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The Anatomy of a Large-scale Hypertextual Web Search Engine,</title>
<date>1998</date>
<booktitle>In Proceedings of 7th International World Wide Web Conference,</booktitle>
<pages>107--117</pages>
<marker>Brin, Page, 1998</marker>
<rawString>Brin S. and Page L. 1998. The Anatomy of a Large-scale Hypertextual Web Search Engine, In Proceedings of 7th International World Wide Web Conference, pp. 107-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>L-Y Yee</author>
</authors>
<title>An IR Approach for Translating New Words from Nonparallel, Comparable Texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 7th International Conference on Computational Linguistics,</booktitle>
<pages>414--420</pages>
<marker>Fung, Yee, 1998</marker>
<rawString>Fung P. and Yee L.-Y. 1998. An IR Approach for Translating New Words from Nonparallel, Comparable Texts. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 7th International Conference on Computational Linguistics, pp. 414-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<date>2000</date>
<booktitle>Speech and Language Processing,</booktitle>
<pages>102--120</pages>
<location>Prentice-Hall, New Jersey.</location>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Jurafsky D. and Martin J. H. 2000. Speech and Language Processing, pp. 102-120, Prentice-Hall, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<journal>Machine Transliteration, Computational Linguistics,</journal>
<volume>24</volume>
<pages>599--612</pages>
<marker>Knight, Graehl, 1998</marker>
<rawString>Knight K. and Graehl J. 1998. Machine Transliteration, Computational Linguistics, Vol. 24, No. 4, pp.599-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Kuo</author>
<author>Y K Yang</author>
</authors>
<title>Automatic Transliteratedterm Extraction Using Confusion Matrix from Nonparallel Corpora,</title>
<date>2003</date>
<booktitle>In Proceedings of ROCLING XV Computational Linguistics Conference,</booktitle>
<pages>17--32</pages>
<marker>Kuo, Yang, 2003</marker>
<rawString>Kuo J. S. and Yang Y. K. 2003. Automatic Transliteratedterm Extraction Using Confusion Matrix from Nonparallel Corpora, In Proceedings of ROCLING XV Computational Linguistics Conference, pp.17-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Pagel</author>
<author>K Lenzo</author>
<author>A Black</author>
</authors>
<title>Letter to Sound Rules for Accented Lexicon Compression,</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>2015--2020</pages>
<marker>Pagel, Lenzo, Black, 1998</marker>
<rawString>Pagel V., Lenzo K., and Black A. 1998. Letter to Sound Rules for Accented Lexicon Compression, In Proceedings of ICSLP, pp. 2015-2020.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>