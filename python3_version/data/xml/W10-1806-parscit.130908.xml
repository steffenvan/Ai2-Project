<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.977638">
Anveshan: A Framework for Analysis of Multiple Annotators’ Labeling
Behavior
</title>
<author confidence="0.8726065">
Vikas Bhardwaj,
Rebecca J. Passonneau and Ansaf Salleb-Aouissi
</author>
<affiliation confidence="0.95014">
Columbia University
</affiliation>
<address confidence="0.884964">
New York, NY, USA
</address>
<email confidence="0.971821">
vsb2108@columbia.edu
</email>
<note confidence="0.617368">
(becky@cs|ansaf@ccls).columbia.edu
Nancy Ide
Vassar College
Poughkeepsie, NY, USA
</note>
<email confidence="0.901059">
ide@cs.vassar.edu
</email>
<sectionHeader confidence="0.978115" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947592592593">
Manual annotation of natural language to
capture linguistic information is essen-
tial for NLP tasks involving supervised
machine learning of semantic knowledge.
Judgements of meaning can be more or
less subjective, in which case instead of
a single correct label, the labels assigned
might vary among annotators based on the
annotators’ knowledge, age, gender, intu-
itions, background, and so on. We intro-
duce a framework ”Anveshan,” where we
investigate annotator behavior to find out-
liers, cluster annotators by behavior, and
identify confusable labels. We also in-
vestigate the effectiveness of using trained
annotators versus a larger number of un-
trained annotators on a word sense annota-
tion task. The annotation data comes from
a word sense disambiguation task for pol-
ysemous words, annotated by both trained
annotators and untrained annotators from
Amazon’s Mechanical turk. Our results
show that Anveshan is effective in uncov-
ering patterns in annotator behavior, and
we also show that trained annotators are
superior to a larger number of untrained
annotators for this task.
</bodyText>
<sectionHeader confidence="0.972697" genericHeader="keywords">
1 Credits
</sectionHeader>
<bodyText confidence="0.994411">
This work was supported by a research supple-
ment to the National Science Foundation CRI
award 0708952.
</bodyText>
<sectionHeader confidence="0.996119" genericHeader="introduction">
2 Introduction
</sectionHeader>
<bodyText confidence="0.999924704545455">
Manual annotation of language data in order to
capture linguistic knowledge has become increas-
ingly important for semantic and pragmatic an-
notation tasks. A very short list of a few such
tasks illustrates the range of types of annotation,
in varying stages of development: predicate ar-
gument structure (Palmer et al., 2005b), dialogue
acts (Hu et al., 2009), discourse structure (Carbone
et al., 2004), opinion (Wiebe and Cardie, 2005),
emotion (Alm et al., 2005). The number of ef-
forts to create corpus resources that include man-
ual annotations has also been growing. A common
approach in assessing the resulting manual anno-
tations is to report a single quantitative measure
reflecting the quality of the annotations, either a
summary statistic such as percent agreement, or
an agreement coefficient from the family of met-
rics that include Krippendorff’s alpha (Krippen-
dorff, 1980) and Cohen’s kappa (Cohen, 1960).
We present some new assessment methods to use
in combination with an agreement coefficient for
understanding annotator behavior when there are
multiple annotators and many annotation values.
Anveshan (Annotation Variance Estimation)1 is
a suite of procedures for analyzing patterns of
agreement and disagreement among annotators,
as well as the distributions of annotation values
across annotators. Anveshan thus makes it pos-
sible to explore annotator behavior in more detail.
Currently, it includes three types of analysis: inter-
annotator agreement (IA) among all subsets of an-
notators, leverage of annotation values for outlier
detection, and metrics for comparing annotators’
distributions of annotation values (e.g., Kullbach-
Liebler divergence).
As an illustration of the utility of Anveshan, we
compare two groups of annotators on the same an-
notation word sense annotation tasks: a half dozen
trained annotators and fourteen Mechanical Turk-
ers. Previous work has argued that it can be cost
effective to collect multiple labels from untrained
labelers at a low cost per label, and to combine
the multiple labels through a voting method, rather
than to collect single labels from highly trained la-
</bodyText>
<note confidence="0.643457">
1Anveshan is a Sanskrit word which literally means
search or exploration.
</note>
<page confidence="0.637266">
47
</page>
<note confidence="0.9935685">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 47–55,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.98770425">
belers (Snow et al., 2008; Sheng et al., 2008; Lam
and Stork, 2003). The tasks included in (Snow et
al., 2008), for example, include word sense an-
notation; in contrast to our case, where the av-
erage number of senses per word is 9.5, the one
word sense annotation task had three senses. We
find that the same half dozen trained annotators
can agree well or not on sense labels for poly-
semous words. When they agree less well, we
find that it is possible to distinguish between prob-
lems in the labels (e.g., confusable senses) and
systematic differences of interpretation among an-
notators. When we use twice the number of Me-
chanical Turkers as trained annotators for three of
our ten polysemous words, we find inconsistent re-
sults.
The next section of the paper presents the moti-
vation for Anveshan and its relevance to the word
sense annotation task, followed by a section on
related work. The word sense annotation data is
given in section 5. Anveshan is described in the
subsequent section, followed by the results of its
application to the two data sets. We discuss the
comparison of trained annotators and Mechanical
Turkers, as well as differences among words, in
section 7. Section 7 concludes with a short recap
of Anveshan in general, and its application to word
sense annotations in particular.
</bodyText>
<sectionHeader confidence="0.99063" genericHeader="method">
3 Beyond Interannotator Agreement (IA)
</sectionHeader>
<bodyText confidence="0.999989333333334">
Assessing the reliability of an annotation typically
addresses the question of whether different anno-
tators (effectively) assign the same annotation la-
bels. Various measures can be used to compare
different annotators, including agreement coeffi-
cients such as Krippendorff’s alpha (Krippendorff,
1980). Extensive reviews of the properties of such
coefficients have been presented elsewhere, e.g.,
(Artstein and Poesio, 2008). Briefly, an agree-
ment produce values in the interval [-1,1] indicat-
ing how much of the observed agreement is above
(or below) agreement that would be predicted by
chance (value of 0). To measure reliability in this
way is to assume that for most of the instances in
the data, there is a single correct response. Here
we present the use of reliability metrics and other
measures for word sense annotation, and we as-
sume that in some cases there may not be a single
correct response. When annotators have less than
excellent agreement, we aim to examine possible
causes.
We take word sense to be a problematic anno-
tation to perform, thus requiring a deeper under-
standing of the conditions under which annotators
might disagree. The many reasons can only be
touched on here. For example, word senses are
not discrete, atomic units that can be delimited and
enumerated. While dictionaries and other lexical
resoures, such as WordNet (Miller et al., 1993) or
the Hector lexicon (cf. SENSEVAL-1 (Kilgarriff
and Palmer, 2000)), do provide enumerations of
the senses for a given word, and their interrela-
tions (e.g., a list of senses, a tree of senses), it is
widely agreed that this is a convenient abstraction,
if for no other reason than the fact that words shift
meanings along with the communicative needs of
the groups of individuals who use them. The con-
text in which a word is used plays a significant role
in restricting the current sense. As a result, it is
often argued that the best representation for word
meaning would consist in clustering the contexts
in which words are used (Kilgarriff, 1997). Yet
even this would be insufficient because new com-
munities arise, new behaviors and artifacts emerge
along with them, hence new contexts of use and
new clusters. At the same time, contexts of use
and the senses that go along with them can fade
away (cf. the use of handbag discussed in (Kilgar-
riff, 1997) pertaining to disco dancing). Because
an enumeration of word senses is somewhat arti-
ficial, annotators might disagree on word senses
because they disagree on the boundaries between
one sense and another, just as professional lexi-
cographers do.
Apart from the artificiality of creating flat or
hierarchical sense inventories, the meanings of
words can vary in their subjectivity, due to differ-
ences in the perception or experience of individu-
als. This can be true for word senses that are inher-
ently relative, such as cold (as in, turn up the ther-
mostat, it’s too cold in here); or that derive their
meaning from cultural norms that may differ from
community to community, such as justice; or that
change as one grows older, e.g., whether a long
time to wait pertains to hours versus days.
Despite the arguments against using word sense
inventories, until they are replaced with an equally
convenient and more representative abstraction,
they are an extremely convenient computational
representation. We rely on WordNet senses, which
are presented to annotators with a gloss (defini-
tion) and with example uses. In order to better un-
</bodyText>
<page confidence="0.907458">
48
</page>
<bodyText confidence="0.998616625">
derstand reasons for disagreement on senses, we
collect labels from multiple annotators. When an-
notators agree, having multiple annotators is re-
dundant. But when annotators disagree, having
multiple annotators is necessary in order to de-
termine whether the disagreement is due to noise
based on insufficiently clear sense definitions ver-
sus a systematic difference between individuals,
e.g., those who see a glass as half empty where
others see it as half full. To insure the opportu-
nity to observe how varied the labeling of a single
word can be, we collect word sense annotations
from multiple annotators. One potential benefit of
such investigation might be a better understanding
of how to model word meaning.
In sum, we hypothesize the following cases:
</bodyText>
<listItem confidence="0.966676333333333">
• Outliers: A small proportion of annotators
may assign senses in a manner that differs
markedly from the remaining annotators.
• Confusability of senses: If multiple annota-
tors assign multiple senses in an apparently
random fashion, it may be that the senses are
not sufficiently distinct.
• Systematic differences among subsets of an-
notators: If the same 50% of annotators al-
</listItem>
<bodyText confidence="0.744864">
ways pick sense X where the remaining an-
notators always pick sense Y, it may be that
properties of the annotators, such as their age
cohort, account for the disagreement.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999982076923077">
There has been a decade-long community-wide ef-
fort to evaluate word sense disambiguation (WSD)
systems across languages in the four Senseval ef-
forts (1998, 2001, 2004, and 2007, cf. (Kilgarriff,
1998; Pedersen, 2002a; Pedersen, 2002b; Palmer
et al., 2005a)), with a corollary effort to investi-
gate the issues pertaining to preparation of man-
ually annotated gold standard corpora tagged for
word senses (Palmer et al., 2005a).
Differences in IA and system performance
across part-of-speech have been examined, as
in (Ng et al., 1999; Palmer et al., 2005a). Fac-
tors that have been proposed as affecting agree-
ment include whether annotators are allowed to as-
sign multilabels (V´eronis, 1998; Ide et al., 2002;
Passonneau et al., 2006), the number or granu-
larity of senses (Ng et al., 1999), merging of re-
lated senses (Snow et al., 2007), sense similar-
ity (Chugur et al., 2002), entropy (Diab, 2004;
Palmer et al., 2005a), and reactions times required
to distinguish senses (Klein and Murphy, 2002;
Ide and Wilks, 2006).
We anticipate that one of the ways in which the
data will be used will be to train machine learning
approaches to WSD. Noise in labeling and the im-
pact on machine learning has been discussed from
various perspectives. In (Reidsma and Carletta,
2008), it is argued that machine learning perfor-
mance does not vary consistently with interannota-
tor agreement. Through a simulation study, the au-
thors find that machine learning performance can
degrade or not with lower agreement, depending
on whether the disagreement is due to noise or sys-
tematic behavior. Noise has relatively little impact
compared with systematic disagreements. In (Pas-
sonneau et al., 2008), a similar lack of correla-
tion between interannotator agreement and ma-
chine learning performance is found in an empiri-
cal investigation.
</bodyText>
<sectionHeader confidence="0.874919" genericHeader="method">
5 Word Sense Annotation Data
</sectionHeader>
<subsectionHeader confidence="0.994017">
5.1 Trained Annotator data
</subsectionHeader>
<bodyText confidence="0.99997828">
The Manually Annotated Sub-Corpus (MASC)
project (Ide et al., 2010) is creating a small,
representative corpus of American English written
and spoken texts drawn from the Open American
National Corpus (OANC).2 The MASC corpus
includes hand-validated or manual annotations
for a variety of linguistic phenomena. The first
MASC release, available as of May 2010, consists
of 82K words.3 One of the goals of MASC is
to support efforts to harmonize WordNet (Miller
et al., 1993) and FrameNet (Ruppenhofer et al.,
2006), in order to bring the sense distinctions each
makes into better alignment.
We chose ten fairly frequent, moderately poly-
semous words for sense tagging. One hundred oc-
currences of each word were sense annotated by
five or six trained annotators. The ten words are
shown in Table 1, the words are grouped by part of
speech, with the number of WordNet senses, the
number of senses used by the trained annotators
(TAs), the number of annotators, and Alpha. We
call this the Trained annotator (TA) data.
We find that interannotator agreement (IA)
among half a dozen annotators varies depending
on the word. For ten words nearly balanced with
</bodyText>
<footnote confidence="0.9762665">
2http://www.anc.org
3http://www.anc.org/MASC/Home.html
</footnote>
<page confidence="0.830701">
49
</page>
<table confidence="0.997401083333333">
Word-pos Senses Ann Alpha
Avail. Used
long-j 9 4 6 0.67
fair-j 10 6 5 0.54
quiet-j 6 5 6 0.49
time-n 10 8 5 0.68
work-n 7 7 5 0.62
land-n 11 9 6 0.49
show-v 12 10 5 0.46
tell-v 8 8 6 0.46
know-v 11 10 5 0.37
say-v 11 10 6 0.37
</table>
<tableCaption confidence="0.997918">
Table 1: Interannotator agreement on ten poly-
</tableCaption>
<bodyText confidence="0.9775384375">
semous words: three adjectives, three nouns and
four verbs among trained annotators
respect to part of speech, we find a range of about
0.50 to 0.70 for nouns and adjectives, and about
0.37 to 0.46 for verbs. Table 1 shows the ten words
and the alpha scores for the same five or six an-
notators. The layout of the table illustrates both
that verbs have lower agreement than adjectives
or nouns, and that within each part of speech, an-
notators achieve varying levels of agreement, de-
pending on the word. The annotators, their level
of training, the number of sense choices, the anno-
tation tool, and other factors remain constant from
word to word. Thus we hypothesize that the differ-
ences in IA reflect differences in the degree of sub-
jectivity of the sense choices, the sense similarity,
or both. Anveshan is a data exploration framework
to help understand the differences in the ability of
the same annotators to agree well on sense anno-
tation for some words and not others.
As shown, annotators achieve respectable
agreement on long, time and work, and lower
agreement on the remaining words. Verbs have
lower agreement overall.
Figure 1 shows WordNet senses for long in the
form displayed to annotators, who used an annota-
tion GUI developed in Java. The sense number ap-
pears in the first column, followed by the glosses,
then sample phrases; only three senses are shown,
to conserve space. Note that annotators did not see
the WordNet synsets (sets of synonymous words)
for a given sense.
</bodyText>
<subsectionHeader confidence="0.962136">
5.2 Mechanical Turk data
</subsectionHeader>
<bodyText confidence="0.579513">
Amazon’s Mechanical Turk is a crowd-sourcing
marketplace where Human Intelligence Tasks
</bodyText>
<table confidence="0.9982046">
Word-pos Senses Ann Alpha
Avail. Used
long-j 9 9 14 0.15
fair-j 10 10 14 0.25
quiet-j 6 6 15 0.08
</table>
<tableCaption confidence="0.8450345">
Table 2: Interannotator agreement on adjectives
among Mechanical Turk annotators
</tableCaption>
<bodyText confidence="0.999158263157895">
(HITs) such as sense annotation for words in a
sentence, can be set up and results from a large
number of annotators (or turkers) can be obtained
quickly. We used Mechanical Turk to obtain anno-
tations from 14 annotators on the set of adjectives
to analyze IA for a larger set of untrained annota-
tors.
The task was set up to get 150 occurrences an-
notated for each of the three adjectives: fair, long
and quiet, by 14 mechanical turk annotators each.
100 of these occurrences were the same as those
done by the trained annotators. For each word,
the 150 instances were divided into 15 HITs of 10
instances each. The average submit time of a HIT
was 200 seconds. We report the IA among the Me-
chanical Turk annotators using Krippendorff’s Al-
pha in Table 2. As shown, the turkers have poor
agreement, particularly on long and quiet, which
is at the chance level.
</bodyText>
<sectionHeader confidence="0.996488" genericHeader="method">
6 Anveshan
</sectionHeader>
<bodyText confidence="0.998742142857143">
Anveshan: Annotation Variance Estimation, is
our approach to perform a more subtle analysis
of inter-annotator agreement. Anveshan uses sim-
ple statistical methods to achieve the three goals
identified in section 3: outlier detection, confus-
able senses, and distinct subsets of annotators that
agree with each other.
</bodyText>
<subsectionHeader confidence="0.997285">
6.1 Method
</subsectionHeader>
<bodyText confidence="0.9978556">
This section uses the following notation to explain
Anveshan’s methodology:
We assume that we have n annotators annotat-
ing m senses. The probability of annotator a using
sense si is given by
</bodyText>
<equation confidence="0.984246">
Pa(S = si) = E&apos; � unt ( )
count(s, a)
</equation>
<bodyText confidence="0.646534">
where, count(si, a) is number of times si was
used by a.
</bodyText>
<figure confidence="0.908949857142857">
50
1 primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of time
or a duration as specified: “a long life”; “a long boring speech”; “a long time”; “a long friendship”;
“a long game”; “long ago”; “an hour long”
2 primarily spatial sense; of relatively great or greater than average spatial extension or extension as specified:
“a long road”; “a long distance”; “contained many long words”; “ten miles long”
3 of relatively great height: “a race of long gaunt men” (Sherwood Anderson); “looked out the long French windows”
</figure>
<figureCaption confidence="0.99997">
Figure 1: Three of the WordNet senses for ”Long”
</figureCaption>
<bodyText confidence="0.997605">
Anveshan uses the Kullbach-Liebler divergence
(KLD), Jensen-Shannon divergence (JSD) and
Leverage to compare probability distributions.
The KLD of two probability distributions P and
Q is given by:
</bodyText>
<equation confidence="0.9994235">
P (i)
P(i) log Q(i)
</equation>
<bodyText confidence="0.994362">
JSD is a modified version of KLD, it is also
known as total divergence to the average, and is
given by:
</bodyText>
<equation confidence="0.963267">
1 1
JSD(P, Q) = 2KLD(P, M) + 2KLD(Q, M)
</equation>
<bodyText confidence="0.712206">
where
</bodyText>
<equation confidence="0.997649">
M = (P + Q)/2
</equation>
<bodyText confidence="0.9969965">
We define Leverage Lev of probability distribu-
tion P over Q as:
</bodyText>
<figureCaption confidence="0.914596">
Figure 2: Distance measure (KLD) for Annotators
of long in TA Data
</figureCaption>
<figure confidence="0.942124956521739">
0.6
0.4
0.2
0.8
0
1
A107 A101 A103 A102 A105 A108
0.6
0.4
0.2
0.8
0
A101 A102 A103 A105 A107 A108
101
102
999
103
108
KLD(P, Q) = �
i
� |P(h) − Q(h) |Figure 3: Sense Usage distribution for long by an-
Lev(P, Q) = notators in TA Data
k
</figure>
<bodyText confidence="0.95357">
We now compute the following statistics:
</bodyText>
<listItem confidence="0.993305166666667">
• For each annotator ai, we compute Pai.
• We compute Pavg, which is (Ei Pai)/n.
• We compute Lev(Pai, Pavg), ∀i
• Then we compute JSD(Pai, Paj) ∀(i, j),
where i, j ≤ n and i =6 j
• Lastly, we compute a distance measure for
</listItem>
<bodyText confidence="0.937989842105263">
each annotator, by computing the KLD be-
tween each annotator and the average of
the remaining annotators, i.e. we get
∀i, Dai = KLD(Pai, Q), where Q =
(Ej7�i Paj)/(n − 1)
These statistics give us a deeper understanding
of annotator behavior. Looking at the sense us-
age probabilities, we can identify how frequently
senses are used by an annotator. We can see how
much an annotator deviates from the average sense
usage distribution by looking at Leverage. JSD be-
tween two annotators gives us a measure of how
close they are to each other. KLD of an annota-
tor with the remaining annotators shows us how
different the annotator is from the rest. In the fol-
lowing section we show results, which illustrate
the effectiveness of Anveshan in identifying use-
ful patterns in the data from the trained annotators
(TAs) and Mechanical Turkers (MTs).
</bodyText>
<subsectionHeader confidence="0.89135">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999990666666667">
We used Anveshan on all data from TAs and MTs.
We were successful in correctly identifying out-
liers on many words. Also, analyzing the sense
usage patterns and observing the JSD and KLD
scores gave us useful insights on annotator differ-
ences. In the figures for this section, the six TAs
are represented by their unique identifiers (A101,
A102, A103, A105, A107, A108). Word senses
are identified by adding 100 to the WordNet sense
</bodyText>
<page confidence="0.83813">
51
</page>
<table confidence="0.997471">
Word Old Alpha Ann Dropped New Alpha
long 0.67 1 0.80
land 0.49 1 0.54
know 0.377 1 0.48
tell 0.45 2 0.52
say 0.37 2 0.44
fair 0.54 2 0.63
</table>
<tableCaption confidence="0.881596">
Table 3: Increase in IA score by dropping annota-
tors (TA Data)
</tableCaption>
<figureCaption confidence="0.958507">
Figure 5: Sense usage patterns of annotators ‘107’
and ‘108’ for show in TA Data
</figureCaption>
<figure confidence="0.999897966666667">
0.25
0.15
0.05
0.3
0.2
0.1
0
105 102 104 103 101 999 108 106 107 110 109
A107
A108
0.25
0.15
0.05
0.2
0.1
0
105 102 104 103 101 999 108 106 107 110 109
A102
A105
0.35
0.25
0.15
0.05
0.3
0.2
0.1
0
105 102 104 103 101 999 108 106 107 110 109
A101
Overall
</figure>
<figureCaption confidence="0.996406">
Figure 4: Sense usage patterns of annotators ‘102’
</figureCaption>
<bodyText confidence="0.987834137931035">
and ‘105’ for show in TA Data
number. An additional “None of the Above” label
is represented as 999; annotators select this when
no sense applies, when the word occurs as part of
a large lexical unit (collocation) with a clearly dis-
tinct meaning, or when the sentence is not a cor-
rect example for other reasons (e.g., wrong part of
speech).
Figure 2 shows the distance measure (KLD) for
each annotator from the rest of the annotators for
the word long with respect to the probability for
each of the four senses used (cf. Table 1). It can
be clearly seen that annotator A108 is an outlier.
A108 differs in her excessive use of label 999, as
shown in Figure 3. Indeed, by dropping A108,
we see that the IA score (Alpha) jumps from 0.67
to 0.8 for long. Similar results were obtained
for annotations for other words as well. Table 3
shows the jump in IA score after outlier(s) were
dropped.
Anveshan helps us differentiate between noisy
disagreement versus systematic disagreement.
The word show with 5 annotators has a low
agreement score of 0.45. By looking at the
sense distributions for the various annotators,
and observing annotation preferences for each
annotator, we can see that annotators A102 and
A105 have similar behavior (Figure 4, with a
pairwise alpha of 0.52 versus 0.46 for all five
Figure 6: Sense usage distribution of annotator
‘101’ vs. the average of all annotators for show
in TA Data
annotators), and annotators A107 and A108 have
similar behavior (Figure 5, with a pairwise alpha
of 0.53). In contrast, Annotator A101 has very
distinct preferences (Figure 6). This behavior
is captured by computing JSD scores among all
pairs of annotators. As can be seen in Figure 7,
the pairs A102-A105 and A107-A108 have very
low JSD values, indicating similarity in annotator
behavior. At the same time we also see the pairs
having A101 in them have a much higher JSD
score, which is attributed to the fact that A101
is different from everyone else. If we look at
corresponding Alpha scores, we see that pairs
having low JSD values have higher agreement
scores and vice versa.
Observing the sense usage distributions also
helps us identify confusable senses. For example,
Figure 8 shows us the differences in sense usage
patterns of A101, A103 and the average of all
annotators for the word say. We can see that
A101 and A103 deviate in distinct ways from the
average. A101 prefers sense 101 whereas A103
prefers sense 102. This indicates that sense 101
and 102 might be confusable. Sense 1 is given
as “expressing words”; sense 2 as “report or
maintain”.
</bodyText>
<page confidence="0.730001">
52
</page>
<figureCaption confidence="0.9087195">
Figure 7: JSD and Alpha scores for pairs of anno-
tators for show in TA Data
Figure 8: Sense usage distribution for say in TA
Data for annotators ‘101’ and ‘103’
</figureCaption>
<figure confidence="0.992941333333333">
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
A102 A105 A108 A101 A107
</figure>
<figureCaption confidence="0.971395">
Figure 9: Distance measure (KLD) for annotators
of work in TA Data
</figureCaption>
<bodyText confidence="0.4973485">
Figure 10: Sense usage distribution among MTs
for long
</bodyText>
<note confidence="0.3918275">
105A9
99d 102f101aFi
</note>
<bodyText confidence="0.975758586206896">
gure11:S ens eusa gedis tr ibutionamo ngTAsa
ndMTs forfairA nv eshannoto nlyhelps usu nderst
andun der- ly ingpatt ern sin a nnota torbehav
ioran dremo venoi se fr omIA s cor es , butalsohe
lpsiden ti fycaseswhe reth ereis nono isea ndnosy
st ematics ubs et s ofa nn ota tor stha tagre ew
itheacho ther .An examplecan be see ninf ortheno
unwork.We observedt hat th e a nnotator sd onotha ve
la rgel yd iff erentbehav ior ,which isrefle ctedinF
igu re9. Asn oneofth ean not ators a res ign ific
ant lydiffe r-en tfro mth eoth ers , theK LD scor es
arel ow andthe pl
ottedline doe snothav ean ys teep r ises, ass
een in Figure2. Si milartother esultsfo rT A data,A
nves hanw assu cc essfu li niden ti fyingou tli
ersinMech an-ic al T urk data as well.I no rd
ertoco mp are theagre em entamongTA san dMT s,wel ook
e datIAsc or eso fallsubset sofa nnota to rs forthe
thre ead -jec tive sint heMech anic al T urk dat
a.Weo bse rved TAs for all thatMTsusedmuchmore senses than
eus- wordsandthattherewas alotof noiseins ens
sense age us- distribution. Figure10 illustratesthe
ly statistics age forlongamongMTs,for frequ ent
used senses.
resamongall We also lookedat agreements co
any subsets of subsets of MTstosee if ther are
annotators who agree as much we as T ob- As
nd
served that for both long and quiet, there were no
</bodyText>
<figure confidence="0.990571947368421">
0.6
0.4
0.3
0.2
0.5
0.1
0
A105
A102
A108
A107
A105
A101
A102
A101
A107
A101
A108
A101
JSD
Alpha
0.6
0.4
0.3
0.2
0.5
0.1
0
Overall A101 A103
102
101
108
103
0.6
0.4
0.2
1.2
0.8
0
1
Overall A101 A102 A104 A107 A108 A111 A112 A115 A116 A117 A118 A119 A120 A121
106
109
999
108
103
102
101
0.8
0.6
0.4
0.2
1.2
0
1
, a
53
</figure>
<bodyText confidence="0.9996415">
subsets of MT annotators whose agreement was
comparable or greater than the same number of the
TAs, however for fair, we found one set of 5 an-
notators whose IA score (0.61) was greater than
the IA score (0.54) of trained annotators. We also
observed that among both these pairs of annota-
tors, the frequently used senses were the same, as
illustrated in Figure 11. Still, the two groups of an-
notators have sufficiently distinct sense usage that
the overall IA for the combined set drops to 0.43.
</bodyText>
<sectionHeader confidence="0.96614" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999983703703704">
For annotations on a subjective task, there are
cases where there is no single correct label. In
this paper, we presented Anveshan, an approach to
study annotator behavior and to explore datasets
with multiple annotators, and with a large set of
annotation values. Here we looked at data from
half a dozen trained annotators and fourteen un-
trained Mechanical Turkers on word sense anno-
tation for polysemous words. The analysis using
Anveshan provided many insights into sources of
disagreement among the annotators.
We learn that IA Scores do not give us a com-
plete picture and it is necessary to delve deeper
and study annotator behavior in order to identify
noise possibly due to sense confusability, to elim-
inate noise due to outliers, and to identify system-
atic differences where subsets of annotators have
much higher IA than the full set.
The results from Anveshan are encouraging and
the methodology can be readily extended to study
patterns in human behavior. We plan to extend
our work by looking at JSD scores of all subsets
of annotators instead of pairs, to identify larger
subsets of annotators who have similar behavior.
We also plan to investigate other statistical meth-
ods of outlier detection such as the orthogonalized
Gnanadesikan-Kettenring estimator.
</bodyText>
<sectionHeader confidence="0.987251" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991985625">
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine learn-
ing for text-based emotion prediction. In HLT ’05:
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 579–586, Morristown, NJ,
USA. Association for Computational Linguistics.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Marco Carbone, Yaakov Gal, Stuart Shieber, and Bar-
bara Grosz. 2004. Unifying annotated discourse hi-
erarchies to create a gold standard. In Proceedings
of the 5th Sigdial Workshop on Discourse and Dia-
logue.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 32–39,
Philadelphia.
Jacob Cohen. 1960. A coeffiecient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37–46.
Mona Diab. 2004. Relieving the data acquisition bot-
tleneck in word sense disambiguation. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 303–311.
Jun Hu, Rebecca J. Passonneau, and Owen Rambow.
2009. Contrasting the interaction structure of an
email and a telephone corpus: A machine learning
approach to annotation of dialogue function units.
In Proceedings of the 10th SIGDIAL on Dialogue
and Discourse.
Nancy Ide and Yorick Wilks. 2006. Making sense
about sense. In E. Agirre and P. Edmonds, editors,
Word Sense Disambiguation: Algorithms and Appli-
cations, pages 47–74, Dordrecht, The Netherlands.
Springer.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002.
Sense discrimination with parallel corpora. In Pro-
ceedings of ACL’02 Workshop on Word Sense Dis-
ambiguation: Recent Successes and Future Direc-
tions, pages 54–60, Philadelphia.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the
people. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden.
Adam Kilgarriff and Martha Palmer. 2000. Introduc-
tion to the special issue on senseval. Computers and
the Humanities, 34:1–2.
Adam Kilgarriff. 1997. I don’t believe in word senses.
Computers and the Humanities, 31:91–113.
Adam Kilgarriff. 1998. SENSEVAL: An exercise in
evaluating word sense disambiguation programs. In
Proceedings of the First International Conference
on Language Resources and Evaluation (LREC),
pages 581–588, Granada.
Devra Klein and Gregory Murphy. 2002. Paper has
been my ruin: Conceptual relations of polysemous
words. Journal of Memory and Language, 47:548–
70.
</reference>
<page confidence="0.520321">
54
</page>
<reference confidence="0.999668926829268">
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA.
Chuck P. Lam and David G. Stork. 2003. Evaluating
classifiers by means of test data with noisy labels.
In Proceedings of the 18th International Joint Con-
ference on Artificial Intelligence (IJCAI-03), pages
513–518, Acapulco.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to WordNet: An on-line lexical database
(revised). Technical Report Cognitive Science
Laboratory (CSL) Report 43, Princeton University,
Princeton. Revised March 1993.
Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.
1999. A case study on inter-annotator agreement for
word sense disambiguation. In SIGLEX Workshop
On Standardizing Lexical Resources.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005a. Making fine-grained and coarse-
grained sense distinctions. Journal of Natural Lan-
guage Engineering, 13.2:137–163.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005b. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1):71–106.
Rebecca J. Passonneau, Nizar Habash, and Owen Ram-
bow. 2006. Inter-annotator agreement on a mul-
tilingual semantic annotation task. In Proceedings
of the International Conference on Language Re-
sources and Evaluation (LREC), pages 1951–1956,
Genoa, Italy.
Rebecca Passonneau, Tom Lippincott, Tae Yano, and
Judith Klavans. 2008. Relation between agreement
measures on human labeling and machine learning
performance: results from an art history domain. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC),
pages 2841–2848.
Ted Pedersen. 2002a. Assessing system agreement
and instance difficulty in the lexical sample tasks of
Senseval-2. In Proceedings of the ACL-02 Workshop
on Word Sense Disambiguation: Recent Successes
and Future Directions, pages 40–46.
Ted Pedersen. 2002b. Evaluating the effectiveness of
ensembles of decision trees in disambiguating SEN-
SEVAL lexical samples. In Proceedings of the ACL-
02 Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 81–87.
Dennis Reidsma and Jean Carletta. 2008. Reliabil-
ity measurement without limits. Comput. Linguist.,
34(3):319–326.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. Available from
http://framenet.icsi.berkeley.edu/index.php.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceeding of the 14th ACM SIG KDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 614–622, Las Vegas.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2007.
Learning to merge word senses. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1005–
1014, Prague.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but
is it good? evaluating non-expert annotations for
natural language tasks. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 254–263, Honolulu.
Jean V´eronis. 1998. A study of polysemy judgements
and inter-annotator agreement. In SENSEVAL Work-
shop, pages Sussex, England.
Janyce Wiebe and Claire Cardie. 2005. Annotating
expressions of opinions and emotions in language.
language resources and evaluation. In Language
Resources and Evaluation (formerly Computers and
the Humanities, page 2005.
</reference>
<page confidence="0.940401">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.108463">
<title confidence="0.996180333333333">Anveshan: A Framework for Analysis of Multiple Annotators’ Labeling Behavior Vikas</title>
<author confidence="0.999221">J Passonneau</author>
<address confidence="0.586272">Columbia</address>
<author confidence="0.473178">New York</author>
<author confidence="0.473178">NY</author>
<email confidence="0.99744">(becky@cs|ansaf@ccls).columbia.edu</email>
<author confidence="0.663179">Nancy Vassar Poughkeepsie</author>
<author confidence="0.663179">NY</author>
<email confidence="0.999364">ide@cs.vassar.edu</email>
<abstract confidence="0.999640464285714">Manual annotation of natural language to capture linguistic information is essential for NLP tasks involving supervised machine learning of semantic knowledge. Judgements of meaning can be more or less subjective, in which case instead of a single correct label, the labels assigned might vary among annotators based on the annotators’ knowledge, age, gender, intuitions, background, and so on. We introduce a framework ”Anveshan,” where we investigate annotator behavior to find outliers, cluster annotators by behavior, and identify confusable labels. We also investigate the effectiveness of using trained annotators versus a larger number of untrained annotators on a word sense annotation task. The annotation data comes from a word sense disambiguation task for polysemous words, annotated by both trained annotators and untrained annotators from Amazon’s Mechanical turk. Our results show that Anveshan is effective in uncovering patterns in annotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task.</abstract>
<note confidence="0.94420675">1 Credits This work was supported by a research supplement to the National Science Foundation CRI award 0708952.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
<author>Dan Roth</author>
<author>Richard Sproat</author>
</authors>
<title>Emotions from text: machine learning for text-based emotion prediction.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>579--586</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1980" citStr="Alm et al., 2005" startWordPosition="293" endWordPosition="296">annotators for this task. 1 Credits This work was supported by a research supplement to the National Science Foundation CRI award 0708952. 2 Introduction Manual annotation of language data in order to capture linguistic knowledge has become increasingly important for semantic and pragmatic annotation tasks. A very short list of a few such tasks illustrates the range of types of annotation, in varying stages of development: predicate argument structure (Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff’s alpha (Krippendorff, 1980) and Cohen’s kappa (Cohen, 1960). We present some new assessment methods to use in combination with an agreement coefficient for understanding annotator behavior when there are mult</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat. 2005. Emotions from text: machine learning for text-based emotion prediction. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 579–586, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="5640" citStr="Artstein and Poesio, 2008" startWordPosition="866" endWordPosition="869">differences among words, in section 7. Section 7 concludes with a short recap of Anveshan in general, and its application to word sense annotations in particular. 3 Beyond Interannotator Agreement (IA) Assessing the reliability of an annotation typically addresses the question of whether different annotators (effectively) assign the same annotation labels. Various measures can be used to compare different annotators, including agreement coefficients such as Krippendorff’s alpha (Krippendorff, 1980). Extensive reviews of the properties of such coefficients have been presented elsewhere, e.g., (Artstein and Poesio, 2008). Briefly, an agreement produce values in the interval [-1,1] indicating how much of the observed agreement is above (or below) agreement that would be predicted by chance (value of 0). To measure reliability in this way is to assume that for most of the instances in the data, there is a single correct response. Here we present the use of reliability metrics and other measures for word sense annotation, and we assume that in some cases there may not be a single correct response. When annotators have less than excellent agreement, we aim to examine possible causes. We take word sense to be a pr</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Carbone</author>
<author>Yaakov Gal</author>
<author>Stuart Shieber</author>
<author>Barbara Grosz</author>
</authors>
<title>Unifying annotated discourse hierarchies to create a gold standard.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th Sigdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="1918" citStr="Carbone et al., 2004" startWordPosition="283" endWordPosition="286">t trained annotators are superior to a larger number of untrained annotators for this task. 1 Credits This work was supported by a research supplement to the National Science Foundation CRI award 0708952. 2 Introduction Manual annotation of language data in order to capture linguistic knowledge has become increasingly important for semantic and pragmatic annotation tasks. A very short list of a few such tasks illustrates the range of types of annotation, in varying stages of development: predicate argument structure (Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff’s alpha (Krippendorff, 1980) and Cohen’s kappa (Cohen, 1960). We present some new assessment methods to use in combination with an agreement coeffi</context>
</contexts>
<marker>Carbone, Gal, Shieber, Grosz, 2004</marker>
<rawString>Marco Carbone, Yaakov Gal, Stuart Shieber, and Barbara Grosz. 2004. Unifying annotated discourse hierarchies to create a gold standard. In Proceedings of the 5th Sigdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Chugur</author>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
</authors>
<title>Polysemy and sense proximity in the senseval-2 test suite.</title>
<date>2002</date>
<booktitle>In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>32--39</pages>
<location>Philadelphia.</location>
<contexts>
<context position="10880" citStr="Chugur et al., 2002" startWordPosition="1740" endWordPosition="1743">5a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not wi</context>
</contexts>
<marker>Chugur, Gonzalo, Verdejo, 2002</marker>
<rawString>Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002. Polysemy and sense proximity in the senseval-2 test suite. In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 32–39, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coeffiecient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="2431" citStr="Cohen, 1960" startWordPosition="366" endWordPosition="367">(Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff’s alpha (Krippendorff, 1980) and Cohen’s kappa (Cohen, 1960). We present some new assessment methods to use in combination with an agreement coefficient for understanding annotator behavior when there are multiple annotators and many annotation values. Anveshan (Annotation Variance Estimation)1 is a suite of procedures for analyzing patterns of agreement and disagreement among annotators, as well as the distributions of annotation values across annotators. Anveshan thus makes it possible to explore annotator behavior in more detail. Currently, it includes three types of analysis: interannotator agreement (IA) among all subsets of annotators, leverage o</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coeffiecient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>Relieving the data acquisition bottleneck in word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>303--311</pages>
<contexts>
<context position="10901" citStr="Diab, 2004" startWordPosition="1745" endWordPosition="1746">o investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, d</context>
</contexts>
<marker>Diab, 2004</marker>
<rawString>Mona Diab. 2004. Relieving the data acquisition bottleneck in word sense disambiguation. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 303–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hu</author>
<author>Rebecca J Passonneau</author>
<author>Owen Rambow</author>
</authors>
<title>Contrasting the interaction structure of an email and a telephone corpus: A machine learning approach to annotation of dialogue function units.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th SIGDIAL on Dialogue and Discourse.</booktitle>
<contexts>
<context position="1874" citStr="Hu et al., 2009" startWordPosition="277" endWordPosition="280">nnotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task. 1 Credits This work was supported by a research supplement to the National Science Foundation CRI award 0708952. 2 Introduction Manual annotation of language data in order to capture linguistic knowledge has become increasingly important for semantic and pragmatic annotation tasks. A very short list of a few such tasks illustrates the range of types of annotation, in varying stages of development: predicate argument structure (Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff’s alpha (Krippendorff, 1980) and Cohen’s kappa (Cohen, 1960). We present some new assessment methods to</context>
</contexts>
<marker>Hu, Passonneau, Rambow, 2009</marker>
<rawString>Jun Hu, Rebecca J. Passonneau, and Owen Rambow. 2009. Contrasting the interaction structure of an email and a telephone corpus: A machine learning approach to annotation of dialogue function units. In Proceedings of the 10th SIGDIAL on Dialogue and Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Yorick Wilks</author>
</authors>
<title>Making sense about sense.</title>
<date>2006</date>
<booktitle>Word Sense Disambiguation: Algorithms and Applications,</booktitle>
<pages>47--74</pages>
<editor>In E. Agirre and P. Edmonds, editors,</editor>
<publisher>Springer.</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="11022" citStr="Ide and Wilks, 2006" startWordPosition="1762" endWordPosition="1765">senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared </context>
</contexts>
<marker>Ide, Wilks, 2006</marker>
<rawString>Nancy Ide and Yorick Wilks. 2006. Making sense about sense. In E. Agirre and P. Edmonds, editors, Word Sense Disambiguation: Algorithms and Applications, pages 47–74, Dordrecht, The Netherlands. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Tomaz Erjavec</author>
<author>Dan Tufis</author>
</authors>
<title>Sense discrimination with parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>54--60</pages>
<location>Philadelphia.</location>
<contexts>
<context position="10712" citStr="Ide et al., 2002" startWordPosition="1709" endWordPosition="1712">(WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning pe</context>
</contexts>
<marker>Ide, Erjavec, Tufis, 2002</marker>
<rawString>Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense discrimination with parallel corpora. In Proceedings of ACL’02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 54–60, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Collin Baker</author>
<author>Christiane Fellbaum</author>
<author>Rebecca Passonneau</author>
</authors>
<title>The manually annotated sub-corpus: A community resource for and by the people.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="11942" citStr="Ide et al., 2010" startWordPosition="1909" endWordPosition="1912">s not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared with systematic disagreements. In (Passonneau et al., 2008), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation. 5 Word Sense Annotation Data 5.1 Trained Annotator data The Manually Annotated Sub-Corpus (MASC) project (Ide et al., 2010) is creating a small, representative corpus of American English written and spoken texts drawn from the Open American National Corpus (OANC).2 The MASC corpus includes hand-validated or manual annotations for a variety of linguistic phenomena. The first MASC release, available as of May 2010, consists of 82K words.3 One of the goals of MASC is to support efforts to harmonize WordNet (Miller et al., 1993) and FrameNet (Ruppenhofer et al., 2006), in order to bring the sense distinctions each makes into better alignment. We chose ten fairly frequent, moderately polysemous words for sense tagging.</context>
</contexts>
<marker>Ide, Baker, Fellbaum, Passonneau, 2010</marker>
<rawString>Nancy Ide, Collin Baker, Christiane Fellbaum, and Rebecca Passonneau. 2010. The manually annotated sub-corpus: A community resource for and by the people. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<title>Introduction to the special issue on senseval.</title>
<date>2000</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="6660" citStr="Kilgarriff and Palmer, 2000" startWordPosition="1039" endWordPosition="1042">se annotation, and we assume that in some cases there may not be a single correct response. When annotators have less than excellent agreement, we aim to examine possible causes. We take word sense to be a problematic annotation to perform, thus requiring a deeper understanding of the conditions under which annotators might disagree. The many reasons can only be touched on here. For example, word senses are not discrete, atomic units that can be delimited and enumerated. While dictionaries and other lexical resoures, such as WordNet (Miller et al., 1993) or the Hector lexicon (cf. SENSEVAL-1 (Kilgarriff and Palmer, 2000)), do provide enumerations of the senses for a given word, and their interrelations (e.g., a list of senses, a tree of senses), it is widely agreed that this is a convenient abstraction, if for no other reason than the fact that words shift meanings along with the communicative needs of the groups of individuals who use them. The context in which a word is used plays a significant role in restricting the current sense. As a result, it is often argued that the best representation for word meaning would consist in clustering the contexts in which words are used (Kilgarriff, 1997). Yet even this </context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>Adam Kilgarriff and Martha Palmer. 2000. Introduction to the special issue on senseval. Computers and the Humanities, 34:1–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>I don’t believe in word senses.</title>
<date>1997</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>31--91</pages>
<contexts>
<context position="7244" citStr="Kilgarriff, 1997" startWordPosition="1144" endWordPosition="1145">L-1 (Kilgarriff and Palmer, 2000)), do provide enumerations of the senses for a given word, and their interrelations (e.g., a list of senses, a tree of senses), it is widely agreed that this is a convenient abstraction, if for no other reason than the fact that words shift meanings along with the communicative needs of the groups of individuals who use them. The context in which a word is used plays a significant role in restricting the current sense. As a result, it is often argued that the best representation for word meaning would consist in clustering the contexts in which words are used (Kilgarriff, 1997). Yet even this would be insufficient because new communities arise, new behaviors and artifacts emerge along with them, hence new contexts of use and new clusters. At the same time, contexts of use and the senses that go along with them can fade away (cf. the use of handbag discussed in (Kilgarriff, 1997) pertaining to disco dancing). Because an enumeration of word senses is somewhat artificial, annotators might disagree on word senses because they disagree on the boundaries between one sense and another, just as professional lexicographers do. Apart from the artificiality of creating flat or</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Adam Kilgarriff. 1997. I don’t believe in word senses. Computers and the Humanities, 31:91–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>SENSEVAL: An exercise in evaluating word sense disambiguation programs.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>581--588</pages>
<location>Granada.</location>
<contexts>
<context position="10206" citStr="Kilgarriff, 1998" startWordPosition="1630" endWordPosition="1631"> senses: If multiple annotators assign multiple senses in an apparently random fashion, it may be that the senses are not sufficiently distinct. • Systematic differences among subsets of annotators: If the same 50% of annotators always pick sense X where the remaining annotators always pick sense Y, it may be that properties of the annotators, such as their age cohort, account for the disagreement. 4 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of </context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Adam Kilgarriff. 1998. SENSEVAL: An exercise in evaluating word sense disambiguation programs. In Proceedings of the First International Conference on Language Resources and Evaluation (LREC), pages 581–588, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Devra Klein</author>
<author>Gregory Murphy</author>
</authors>
<title>Paper has been my ruin: Conceptual relations of polysemous words.</title>
<date>2002</date>
<journal>Journal of Memory and Language,</journal>
<volume>47</volume>
<pages>70</pages>
<contexts>
<context position="11000" citStr="Klein and Murphy, 2002" startWordPosition="1758" endWordPosition="1761">corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively l</context>
</contexts>
<marker>Klein, Murphy, 2002</marker>
<rawString>Devra Klein and Gregory Murphy. 2002. Paper has been my ruin: Conceptual relations of polysemous words. Journal of Memory and Language, 47:548– 70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="2399" citStr="Krippendorff, 1980" startWordPosition="360" endWordPosition="362">elopment: predicate argument structure (Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff’s alpha (Krippendorff, 1980) and Cohen’s kappa (Cohen, 1960). We present some new assessment methods to use in combination with an agreement coefficient for understanding annotator behavior when there are multiple annotators and many annotation values. Anveshan (Annotation Variance Estimation)1 is a suite of procedures for analyzing patterns of agreement and disagreement among annotators, as well as the distributions of annotation values across annotators. Anveshan thus makes it possible to explore annotator behavior in more detail. Currently, it includes three types of analysis: interannotator agreement (IA) among all s</context>
<context position="5517" citStr="Krippendorff, 1980" startWordPosition="851" endWordPosition="852">pplication to the two data sets. We discuss the comparison of trained annotators and Mechanical Turkers, as well as differences among words, in section 7. Section 7 concludes with a short recap of Anveshan in general, and its application to word sense annotations in particular. 3 Beyond Interannotator Agreement (IA) Assessing the reliability of an annotation typically addresses the question of whether different annotators (effectively) assign the same annotation labels. Various measures can be used to compare different annotators, including agreement coefficients such as Krippendorff’s alpha (Krippendorff, 1980). Extensive reviews of the properties of such coefficients have been presented elsewhere, e.g., (Artstein and Poesio, 2008). Briefly, an agreement produce values in the interval [-1,1] indicating how much of the observed agreement is above (or below) agreement that would be predicted by chance (value of 0). To measure reliability in this way is to assume that for most of the instances in the data, there is a single correct response. Here we present the use of reliability metrics and other measures for word sense annotation, and we assume that in some cases there may not be a single correct res</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuck P Lam</author>
<author>David G Stork</author>
</authors>
<title>Evaluating classifiers by means of test data with noisy labels.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-03),</booktitle>
<pages>513--518</pages>
<location>Acapulco.</location>
<contexts>
<context position="3941" citStr="Lam and Stork, 2003" startWordPosition="591" endWordPosition="594">alf dozen trained annotators and fourteen Mechanical Turkers. Previous work has argued that it can be cost effective to collect multiple labels from untrained labelers at a low cost per label, and to combine the multiple labels through a voting method, rather than to collect single labels from highly trained la1Anveshan is a Sanskrit word which literally means search or exploration. 47 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 47–55, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics belers (Snow et al., 2008; Sheng et al., 2008; Lam and Stork, 2003). The tasks included in (Snow et al., 2008), for example, include word sense annotation; in contrast to our case, where the average number of senses per word is 9.5, the one word sense annotation task had three senses. We find that the same half dozen trained annotators can agree well or not on sense labels for polysemous words. When they agree less well, we find that it is possible to distinguish between problems in the labels (e.g., confusable senses) and systematic differences of interpretation among annotators. When we use twice the number of Mechanical Turkers as trained annotators for th</context>
</contexts>
<marker>Lam, Stork, 2003</marker>
<rawString>Chuck P. Lam and David G. Stork. 2003. Evaluating classifiers by means of test data with noisy labels. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-03), pages 513–518, Acapulco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database (revised).</title>
<date>1993</date>
<tech>Technical Report Cognitive Science Laboratory (CSL) Report 43,</tech>
<institution>Princeton University, Princeton. Revised</institution>
<contexts>
<context position="6592" citStr="Miller et al., 1993" startWordPosition="1029" endWordPosition="1032">e use of reliability metrics and other measures for word sense annotation, and we assume that in some cases there may not be a single correct response. When annotators have less than excellent agreement, we aim to examine possible causes. We take word sense to be a problematic annotation to perform, thus requiring a deeper understanding of the conditions under which annotators might disagree. The many reasons can only be touched on here. For example, word senses are not discrete, atomic units that can be delimited and enumerated. While dictionaries and other lexical resoures, such as WordNet (Miller et al., 1993) or the Hector lexicon (cf. SENSEVAL-1 (Kilgarriff and Palmer, 2000)), do provide enumerations of the senses for a given word, and their interrelations (e.g., a list of senses, a tree of senses), it is widely agreed that this is a convenient abstraction, if for no other reason than the fact that words shift meanings along with the communicative needs of the groups of individuals who use them. The context in which a word is used plays a significant role in restricting the current sense. As a result, it is often argued that the best representation for word meaning would consist in clustering the</context>
<context position="12349" citStr="Miller et al., 1993" startWordPosition="1974" endWordPosition="1977">otator agreement and machine learning performance is found in an empirical investigation. 5 Word Sense Annotation Data 5.1 Trained Annotator data The Manually Annotated Sub-Corpus (MASC) project (Ide et al., 2010) is creating a small, representative corpus of American English written and spoken texts drawn from the Open American National Corpus (OANC).2 The MASC corpus includes hand-validated or manual annotations for a variety of linguistic phenomena. The first MASC release, available as of May 2010, consists of 82K words.3 One of the goals of MASC is to support efforts to harmonize WordNet (Miller et al., 1993) and FrameNet (Ruppenhofer et al., 2006), in order to bring the sense distinctions each makes into better alignment. We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find that interannotator agreement (IA) among half a doz</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1993. Introduction to WordNet: An on-line lexical database (revised). Technical Report Cognitive Science Laboratory (CSL) Report 43, Princeton University, Princeton. Revised March 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Chung Yong Lim</author>
<author>Shou King Foo</author>
</authors>
<title>A case study on inter-annotator agreement for word sense disambiguation.</title>
<date>1999</date>
<booktitle>In SIGLEX Workshop On Standardizing Lexical Resources.</booktitle>
<contexts>
<context position="10538" citStr="Ng et al., 1999" startWordPosition="1680" endWordPosition="1683"> annotators, such as their age cohort, account for the disagreement. 4 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to </context>
</contexts>
<marker>Ng, Lim, Foo, 1999</marker>
<rawString>Hwee Tou Ng, Chung Yong Lim, and Shou King Foo. 1999. A case study on inter-annotator agreement for word sense disambiguation. In SIGLEX Workshop On Standardizing Lexical Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fine-grained and coarsegrained sense distinctions.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>13--2</pages>
<contexts>
<context position="1839" citStr="Palmer et al., 2005" startWordPosition="271" endWordPosition="274"> effective in uncovering patterns in annotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task. 1 Credits This work was supported by a research supplement to the National Science Foundation CRI award 0708952. 2 Introduction Manual annotation of language data in order to capture linguistic knowledge has become increasingly important for semantic and pragmatic annotation tasks. A very short list of a few such tasks illustrates the range of types of annotation, in varying stages of development: predicate argument structure (Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff’s alpha (Krippendorff, 1980) and Cohen’s kappa (Cohen, 1960). We pre</context>
<context position="10261" citStr="Palmer et al., 2005" startWordPosition="1636" endWordPosition="1639">es in an apparently random fashion, it may be that the senses are not sufficiently distinct. • Systematic differences among subsets of annotators: If the same 50% of annotators always pick sense X where the remaining annotators always pick sense Y, it may be that properties of the annotators, such as their age cohort, account for the disagreement. 4 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (C</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2005</marker>
<rawString>Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum. 2005a. Making fine-grained and coarsegrained sense distinctions. Journal of Natural Language Engineering, 13.2:137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1839" citStr="Palmer et al., 2005" startWordPosition="271" endWordPosition="274"> effective in uncovering patterns in annotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task. 1 Credits This work was supported by a research supplement to the National Science Foundation CRI award 0708952. 2 Introduction Manual annotation of language data in order to capture linguistic knowledge has become increasingly important for semantic and pragmatic annotation tasks. A very short list of a few such tasks illustrates the range of types of annotation, in varying stages of development: predicate argument structure (Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff’s alpha (Krippendorff, 1980) and Cohen’s kappa (Cohen, 1960). We pre</context>
<context position="10261" citStr="Palmer et al., 2005" startWordPosition="1636" endWordPosition="1639">es in an apparently random fashion, it may be that the senses are not sufficiently distinct. • Systematic differences among subsets of annotators: If the same 50% of annotators always pick sense X where the remaining annotators always pick sense Y, it may be that properties of the annotators, such as their age cohort, account for the disagreement. 4 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (C</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005b. The proposition bank: An annotated corpus of semantic roles. Comput. Linguist., 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Inter-annotator agreement on a multilingual semantic annotation task.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1951--1956</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="10738" citStr="Passonneau et al., 2006" startWordPosition="1713" endWordPosition="1716">ss languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary co</context>
</contexts>
<marker>Passonneau, Habash, Rambow, 2006</marker>
<rawString>Rebecca J. Passonneau, Nizar Habash, and Owen Rambow. 2006. Inter-annotator agreement on a multilingual semantic annotation task. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), pages 1951–1956, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
<author>Tom Lippincott</author>
<author>Tae Yano</author>
<author>Judith Klavans</author>
</authors>
<title>Relation between agreement measures on human labeling and machine learning performance: results from an art history domain.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>2841--2848</pages>
<contexts>
<context position="11681" citStr="Passonneau et al., 2008" startWordPosition="1868" endWordPosition="1872">s in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared with systematic disagreements. In (Passonneau et al., 2008), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation. 5 Word Sense Annotation Data 5.1 Trained Annotator data The Manually Annotated Sub-Corpus (MASC) project (Ide et al., 2010) is creating a small, representative corpus of American English written and spoken texts drawn from the Open American National Corpus (OANC).2 The MASC corpus includes hand-validated or manual annotations for a variety of linguistic phenomena. The first MASC release, available as of May 2010, consists of 82K words.3 One of the goals of M</context>
</contexts>
<marker>Passonneau, Lippincott, Yano, Klavans, 2008</marker>
<rawString>Rebecca Passonneau, Tom Lippincott, Tae Yano, and Judith Klavans. 2008. Relation between agreement measures on human labeling and machine learning performance: results from an art history domain. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC), pages 2841–2848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Assessing system agreement and instance difficulty in the lexical sample tasks of Senseval-2.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>40--46</pages>
<contexts>
<context position="10222" citStr="Pedersen, 2002" startWordPosition="1632" endWordPosition="1633">le annotators assign multiple senses in an apparently random fashion, it may be that the senses are not sufficiently distinct. • Systematic differences among subsets of annotators: If the same 50% of annotators always pick sense X where the remaining annotators always pick sense Y, it may be that properties of the annotators, such as their age cohort, account for the disagreement. 4 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (</context>
</contexts>
<marker>Pedersen, 2002</marker>
<rawString>Ted Pedersen. 2002a. Assessing system agreement and instance difficulty in the lexical sample tasks of Senseval-2. In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 40–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Evaluating the effectiveness of ensembles of decision trees in disambiguating SENSEVAL lexical samples.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>81--87</pages>
<contexts>
<context position="10222" citStr="Pedersen, 2002" startWordPosition="1632" endWordPosition="1633">le annotators assign multiple senses in an apparently random fashion, it may be that the senses are not sufficiently distinct. • Systematic differences among subsets of annotators: If the same 50% of annotators always pick sense X where the remaining annotators always pick sense Y, it may be that properties of the annotators, such as their age cohort, account for the disagreement. 4 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (</context>
</contexts>
<marker>Pedersen, 2002</marker>
<rawString>Ted Pedersen. 2002b. Evaluating the effectiveness of ensembles of decision trees in disambiguating SENSEVAL lexical samples. In Proceedings of the ACL02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 81–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability measurement without limits.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="11273" citStr="Reidsma and Carletta, 2008" startWordPosition="1806" endWordPosition="1809">re allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared with systematic disagreements. In (Passonneau et al., 2008), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation. 5 Word Sense Annotation Data 5.1 Trained Annotator dat</context>
</contexts>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma and Jean Carletta. 2008. Reliability measurement without limits. Comput. Linguist., 34(3):319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>Framenet ii: Extended theory and practice. Available from http://framenet.icsi.berkeley.edu/index.php.</title>
<date>2006</date>
<contexts>
<context position="12389" citStr="Ruppenhofer et al., 2006" startWordPosition="1980" endWordPosition="1983">ng performance is found in an empirical investigation. 5 Word Sense Annotation Data 5.1 Trained Annotator data The Manually Annotated Sub-Corpus (MASC) project (Ide et al., 2010) is creating a small, representative corpus of American English written and spoken texts drawn from the Open American National Corpus (OANC).2 The MASC corpus includes hand-validated or manual annotations for a variety of linguistic phenomena. The first MASC release, available as of May 2010, consists of 82K words.3 One of the goals of MASC is to support efforts to harmonize WordNet (Miller et al., 1993) and FrameNet (Ruppenhofer et al., 2006), in order to bring the sense distinctions each makes into better alignment. We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find that interannotator agreement (IA) among half a dozen annotators varies depending on the wo</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2006. Framenet ii: Extended theory and practice. Available from http://framenet.icsi.berkeley.edu/index.php.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S Sheng</author>
<author>Foster Provost</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Get another label? improving data quality and data mining using multiple, noisy labelers.</title>
<date>2008</date>
<booktitle>In Proceeding of the 14th ACM SIG KDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>614--622</pages>
<location>Las Vegas.</location>
<contexts>
<context position="3919" citStr="Sheng et al., 2008" startWordPosition="587" endWordPosition="590">nnotation tasks: a half dozen trained annotators and fourteen Mechanical Turkers. Previous work has argued that it can be cost effective to collect multiple labels from untrained labelers at a low cost per label, and to combine the multiple labels through a voting method, rather than to collect single labels from highly trained la1Anveshan is a Sanskrit word which literally means search or exploration. 47 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 47–55, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics belers (Snow et al., 2008; Sheng et al., 2008; Lam and Stork, 2003). The tasks included in (Snow et al., 2008), for example, include word sense annotation; in contrast to our case, where the average number of senses per word is 9.5, the one word sense annotation task had three senses. We find that the same half dozen trained annotators can agree well or not on sense labels for polysemous words. When they agree less well, we find that it is possible to distinguish between problems in the labels (e.g., confusable senses) and systematic differences of interpretation among annotators. When we use twice the number of Mechanical Turkers as tra</context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeirotis. 2008. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceeding of the 14th ACM SIG KDD International Conference on Knowledge Discovery and Data Mining, pages 614–622, Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning to merge word senses.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1005--1014</pages>
<location>Prague.</location>
<contexts>
<context position="10840" citStr="Snow et al., 2007" startWordPosition="1733" endWordPosition="1736">a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine le</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2007</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2007. Learning to merge word senses. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1005– 1014, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>254--263</pages>
<location>Honolulu.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 254–263, Honolulu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>A study of polysemy judgements and inter-annotator agreement.</title>
<date>1998</date>
<booktitle>In SENSEVAL Workshop,</booktitle>
<pages>pages</pages>
<location>Sussex, England.</location>
<marker>V´eronis, 1998</marker>
<rawString>Jean V´eronis. 1998. A study of polysemy judgements and inter-annotator agreement. In SENSEVAL Workshop, pages Sussex, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. language resources and evaluation.</title>
<date>2005</date>
<booktitle>In Language Resources and Evaluation (formerly Computers and the Humanities,</booktitle>
<pages>page</pages>
<contexts>
<context position="1952" citStr="Wiebe and Cardie, 2005" startWordPosition="288" endWordPosition="291">r to a larger number of untrained annotators for this task. 1 Credits This work was supported by a research supplement to the National Science Foundation CRI award 0708952. 2 Introduction Manual annotation of language data in order to capture linguistic knowledge has become increasingly important for semantic and pragmatic annotation tasks. A very short list of a few such tasks illustrates the range of types of annotation, in varying stages of development: predicate argument structure (Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorff’s alpha (Krippendorff, 1980) and Cohen’s kappa (Cohen, 1960). We present some new assessment methods to use in combination with an agreement coefficient for understanding annotator </context>
</contexts>
<marker>Wiebe, Cardie, 2005</marker>
<rawString>Janyce Wiebe and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. language resources and evaluation. In Language Resources and Evaluation (formerly Computers and the Humanities, page 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>