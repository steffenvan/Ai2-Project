<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000681">
<title confidence="0.9573085">
Modeling Annotators:
A Generative Approach to Learning from Annotator Rationales*
</title>
<author confidence="0.926514">
Omar F. Zaidan and Jason Eisner
</author>
<affiliation confidence="0.985605">
Dept. of Computer Science, Johns Hopkins University
</affiliation>
<address confidence="0.769855">
Baltimore, MD 21218, USA
</address>
<email confidence="0.997449">
{ozaidan,jason}@cs.jhu.edu
</email>
<sectionHeader confidence="0.997364" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999805266666667">
A human annotator can provide hints to a machine learner
by highlighting contextual “rationales” for each of his
or her annotations (Zaidan et al., 2007). How can one
exploit this side information to better learn the desired
parameters 0? We present a generative model of how
a given annotator, knowing the true 0, stochastically
chooses rationales. Thus, observing the rationales helps
us infer the true 0. We collect substring rationales for
a sentiment classification task (Pang and Lee, 2004) and
use them to obtain significant accuracy improvements for
each annotator. Our new generative approach exploits the
rationales more effectively than our previous “masking
SVM” approach. It is also more principled, and could be
adapted to help learn other kinds of probabilistic classi-
fiers for quite different tasks.
</bodyText>
<sectionHeader confidence="0.99345" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.9999613125">
Many recent papers aim to reduce the amount of an-
notated data needed to train the parameters of a sta-
tistical model. Well-known paradigms include ac-
tive learning, semi-supervised learning, and either
domain adaptation or cross-lingual transfer from ex-
isting annotated data.
A rather different paradigm is to change the ac-
tual task that is given to annotators, giving them a
greater hand in shaping the learned classifier. Af-
ter all, human annotators themselves are more than
just black-box classifiers to be run on training data.
They possess some introspective knowledge about
their own classification procedure. The hope is to
mine this knowledge rapidly via appropriate ques-
tions and use it to help train a machine classifier.
How to do this, however, is still being explored.
</bodyText>
<subsectionHeader confidence="0.976115">
1.1 Hand-crafted rules
</subsectionHeader>
<bodyText confidence="0.938554">
An obvious option is to have the annotators directly
express their knowledge by hand-crafting rules. This
&apos;This work was supported by National Science Foundation
grant No. 0347822 and the JHU WSE/APL Partnership Fund.
Special thanks to Christine Piatko for many useful discussions.
</bodyText>
<page confidence="0.998219">
31
</page>
<bodyText confidence="0.9976125">
approach remains “data-driven” if the annotators re-
peatedly refine their system against a corpus of la-
beled or unlabeled examples. This achieves high
performance in some domains, such as NP chunk-
ing (Brill and Ngai, 1999), but requires more analyt-
ical skill from the annotators. One empirical study
(Ngai and Yarowsky, 2000) found that it also re-
quired more annotation time than active learning.
</bodyText>
<subsectionHeader confidence="0.929894">
1.2 Feature selection by humans
</subsectionHeader>
<bodyText confidence="0.999990217391304">
More recent work has focused on statistical classi-
fiers. Training such classifiers faces the “credit as-
signment problem.” Given a training example x with
many features, which features are responsible for its
annotated class y? It may take many training exam-
ples to distinguish useful vs. irrelevant features.1
To reduce the number of training examples
needed, one can ask annotators to examine or pro-
pose some candidate features. This is possible even
for the very large feature sets that are typically used
in NLP. In document classification, Raghavan et al.
(2006) show that feature selection by an oracle could
be helpful, and that humans are both rapid and rea-
sonably good at distinguishing highly useful n-gram
features from randomly chosen ones, even when
viewing these n-grams out of context.
Druck et al. (2008) show annotators some features
f from a fixed feature set, and ask them to choose a
class label y such that p(y  |f) is as high as possible.
Haghighi and Klein (2006) do the reverse: for each
class label y, they ask the annotators to propose a
few “prototypical” features f such that p(y  |f) is as
high as possible.
</bodyText>
<subsectionHeader confidence="0.995429">
1.3 Feature selection in context
</subsectionHeader>
<bodyText confidence="0.9990365">
The above methods consider features out of context.
An annotator might have an easier time examining
</bodyText>
<footnote confidence="0.957468666666667">
1Most NLP systems use thousands or millions of features,
because it is helpful to include lexical features over a large vo-
cabulary, often conjoined with lexical or non-lexical context.
</footnote>
<note confidence="0.9052955">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 31–40,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998624">
features in context to recognize whether they appear
relevant. This is particularly true for features that
are only modestly or only sometimes helpful, which
may be abundant in NLP tasks.
Thus, Raghavan et al. (2006) propose an active
learning method in which, while classifying a train-
ing document, the annotator also identifies some fea-
tures of that document as particularly relevant. E.g.,
the annotator might highlight particular unigrams as
he or she reads the document. In their proposal, a
feature that is highlighted in any document is as-
sumed to be globally more relevant. Its dimension
in feature space is scaled by a factor of 10 so that
this feature has more influence on distances or inner
products, and hence on the learned classifier.
</bodyText>
<subsectionHeader confidence="0.969368">
1.4 Concerns about marking features
</subsectionHeader>
<bodyText confidence="0.999368692307692">
Despite the success of the above work, we have
several concerns about asking annotators to identify
globally relevant features.
First, a feature in isolation really does not have a
well-defined worth. A feature may be useful only in
conjunction with other features,2 or be useful only
to the extent that other correlated features are not
selected to do the same work.
Second, it is not clear how an annotator would
easily view and highlight features in context, ex-
cept for the simplest feature sets. In the phrase
Apple shares up 3%, there may be several fea-
tures that fire on the substring Apple—responding
to the string Apple, its case-invariant form apple,
its lemma apple- (which would also respond to ap-
ples), its context-dependent sense Apple2, its part
of speech noun, etc. How does the annotator indi-
cate which of these features are relevant?
Third, annotating features is only appropriate
when the feature set can be easily understood by a
human. This is not always the case. It would be hard
for annotators to read, write, or evaluate a descrip-
tion of a complex syntactic configuration in NLP or
a convolution filter in machine vision.
Fourth, traditional annotation efforts usually try to
remain agnostic about the machine learning methods
</bodyText>
<footnote confidence="0.9840712">
2For example, a linear classifier can learn that most training
examples satisfy A --+ B by setting BA = −5 and BAns = +5,
but this solution requires selecting both A and A∧B as features.
More simply, a polynomial kernel can consider the conjunction
A ∧ B only if both A and B are selected as features.
</footnote>
<bodyText confidence="0.999780375">
and features to be used. The project’s cost is justi-
fied by saying that the annotations will be reused by
many researchers (perhaps in a “shared task”), who
are free to compete on how they tackle the learning
problem. Unfortunately, feature annotation commits
to a particular feature set at annotation time. Subse-
quent research cannot easily adjust the definition of
the features, or obtain annotation of new features.
</bodyText>
<sectionHeader confidence="0.817501" genericHeader="introduction">
2 Annotating Rationales
</sectionHeader>
<bodyText confidence="0.999988545454546">
To solve these problems, we propose that annotators
should not select features but rather mark relevant
portions of the example. In earlier work (Zaidan et
al., 2007), we called these markings “rationales.”
For example, when classifying a movie review as
positive or negative, the annotator would also high-
light phrases that supported that judgment. Figure 1
shows two such rationales.
A multi-annotator timing study (Zaidan et al.,
2007) found that highlighting rationale phrases
while reading movie reviews only doubled annota-
tion time, although annotators marked 5–11 ratio-
nale substrings in addition to the simple binary class.
The benefit justified the extra time. Furthermore,
much of the benefit could have been obtained by giv-
ing rationales for only a fraction of the reviews.
In the visual domain, when classifying an im-
age as containing a zoo, the annotator might circle
some animals or cages and the sign reading “Zoo.”
The Peekaboom game (von Ahn et al., 2006) was in
fact built to elicit such approximate yet relevant re-
gions of images. Further scenarios were discussed in
(Zaidan et al., 2007): rationale annotation for named
entities, linguistic relations, or handwritten digits.
Annotating rationales does not require the anno-
tator to think about the feature space, nor even to
know anything about it. Arguably this makes an-
notation easier and more flexible. It also preserves
the reusability of the annotated data. Anyone is free
to reuse our collected rationales (section 4) to aid
in learning a classifier with richer features, or a dif-
ferent kind of classifier altogether, using either our
procedures or novel procedures.
</bodyText>
<sectionHeader confidence="0.96109" genericHeader="method">
3 Modeling Rationale Annotations
</sectionHeader>
<bodyText confidence="0.9988505">
As rationales are more indirect than explicit features,
they present a trickier machine learning problem.
</bodyText>
<page confidence="0.998609">
32
</page>
<bodyText confidence="0.999950214285714">
We wish to learn the parameters θ of some classi-
fier. How can the annotator’s rationales help us to
do this without many training examples? We will
have to exploit a presumed relationship between the
rationales and the optimal value of θ (i.e., the value
that we would learn on an infinite training set).
This paper exploits an explicit, parametric model
of that relationship. The model’s parameters φ are
intended to capture what that annotator is doing
when he or she marks rationales. Most importantly,
they capture how he or she is influenced by the true
θ. Given this, our learning method will prefer values
of θ that would adequately explain the rationales (as
well as the training classifications).
</bodyText>
<subsectionHeader confidence="0.998915">
3.1 A generative approach
</subsectionHeader>
<bodyText confidence="0.998697555555556">
For concreteness, we will assume that the task is
document classification. Our training data consists
of n triples {(x1, y1, r1), ..., (xn, yn, rn)1), where xi
is a document, yi is its annotated class, and ri is its
rationale markup. At test time we will have to pre-
dict yn+1 from xn+1, without any rn+1.
We propose to jointly choose parameter vectors θ
and φ to maximize the following regularized condi-
tional likelihood:3
</bodyText>
<equation confidence="0.992344166666667">
n
p(yi, ri  |xi, θ, φ) - pprior(θ, φ) (1)
i=1
n
def = pθ(yi  |xi) - pφ(ri  |xi, yi, θ) - pprior(θ, φ)
i=1
</equation>
<bodyText confidence="0.999788666666666">
Here we are trying to model all the annotations, both
yi and ri. The first factor predicts yi using an ordi-
nary probabilistic classifier pθ, while the novel sec-
ond factor predicts ri using a model pφ of how an-
notators generate the rationale annotations.
The crucial point is that the second factor depends
on θ (since ri is supposed to reflect the relation be-
tween xi and yi that is modeled by θ). As a result,
the learner has an incentive to modify θ in a way
that increases the second factor, even if this some-
what decreases the first factor on training data.4
3It would be preferable to integrate out 0 (and even B), but
more difficult.
4Interestingly, even examples where the annotation yi is
wrong or unhelpful can provide useful information about B via
the pair (yi, ri). Two annotators marking the same movie re-
view might disagree on whether it is overall a positive or nega-
After training, one should simply use the first fac-
tor pθ(y  |x) to classify test documents x. The sec-
ond factor is irrelevant for test documents, since they
have not been annotated with rationales r.
The second factor may likewise be omitted for any
training documents i that have not been annotated
with rationales, as there is no ri to predict in those
cases. In the extreme case where no documents are
annotated with rationales, equation (1) reduces to
the standard training procedure.
</bodyText>
<subsectionHeader confidence="0.998657">
3.2 Noisy channel design of rationale models
</subsectionHeader>
<bodyText confidence="0.997411393939394">
Like ordinary class annotations, rationale annota-
tions present us with a “credit assignment problem,”
albeit a smaller one that is limited to features that fire
“in the vicinity” of the rationale r. Some of these
θ-features were likely responsible for the classifica-
tion y and hence triggered the rationale. Other such
θ-features were just innocent bystanders.
Thus, the interesting part of our model is pφ(r |
x, y, θ), which models the rationale annotation pro-
cess. The rationales r reflect θ, but in noisy ways.
Taking this noisy channel idea seriously, pφ(r |
x, y, θ) should consider two questions when assess-
ing whether r is a plausible set of rationales given
θ. First, it needs a “language model” of rationales:
does r consist of rationales that are well-formed a
priori, i.e., before θ is considered? Second, it needs
a “channel model”: does r faithfully signal the fea-
tures of θ that strongly support classifying x as y?
If a feature contributes heavily to the classification
of document x as class y, then the channel model
should tell us which parts of document x tend to be
highlighted as a result.
The channel model must know about the partic-
ular kinds of features that are extracted by f and
scored by θ. Suppose the feature not ... gripping,5
with weight θh, is predictive of the annotated class y.
This raises the probabilities of the annotator’s high-
lighting each of various words, or combinations of
words, in a phrase like not the most gripping ban-
quet on film. The channel model parameters in φ
tive review—but the second factor still allows learning positive
features from the first annotator’s positive rationales, and nega-
tive features from the second annotator’s negative rationales.
</bodyText>
<footnote confidence="0.732733">
5Our current experiments use only unigram features, to
match past work, but we use this example to outline how our
approach generalizes to complex linguistic (or visual) features.
</footnote>
<page confidence="0.998121">
33
</page>
<bodyText confidence="0.999794968253969">
should specify how much each of these probabilities
is raised, based on the magnitude of Oh E R, the
class y, and the fact that the feature is an instance
of the template &lt;Neg&gt; ... &lt;Adjective&gt;. (Thus, 0
has no parameters specific to the word gripping; it
is a low-dimensional vector that only describes the
annotator’s general style in translating 0 into r.)
The language model, however, is independent of
the feature set 0. It models what rationales tend to
look like in the input domain—e.g., documents or
images. In the document case, 0 should describe:
How frequent and how long are typical rationales?
Do their edges tend to align with punctuation or ma-
jor syntactic boundaries in x? Are they rarer in the
middle of a document, or in certain documents?6
Thanks to the language model, we do not need to
posit high 0 features to explain every word in a ratio-
nale. The language model can “explain away” some
words as having been highlighted only because this
annotator prefers not to end a rationale in mid-
phrase, or prefers to sweep up close-together fea-
tures with a single long rationale rather than many
short ones. Similarly, the language model can help
explain why some words, though important, might
not have been included in any rationale of r.
If there are multiple annotators, one can learn dif-
ferent 0 parameters for each annotator, reflecting
their different annotation styles.7 We found this to
be useful (section 8.2).
We remark that our generative modeling approach
(equation (1)) would also apply if r were not ratio-
nale markup, but some other kind of so-called “side
information,” such as the feature annotations dis-
cussed in section 1. For example, Raghavan et al.
(2006) assume that if feature h is relevant—a bi-
6Our current experiments do not model this last point. How-
ever, we imagine that if the document only has a few 0-features
that support the classification, the annotator will probably mark
most of them, whereas if such features are abundant, the anno-
tator may lazily mark only a few of the strongest ones. A simple
approach would equip 0 with a different “bias” or “threshold”
parameter 0. for each rationale training document x, to mod-
ulate the a priori probability of marking a rationale in x. By
fitting this bias parameter, we deduce how lazy the annotator
was (for whatever reason) on document x. If desired, a prior
on 0. could consider whether x has many strong 0-features,
whether the annotator has recently had a coffee break, etc.
7Given insufficient rationale data to recover some annota-
tor’s 0 well, one could smooth using data from other annotators.
But in our situation, 0 had relatively few parameters to learn.
nary distinction—iff it was selected in at least one
document. But it might be more informative to ob-
serve that h was selected in 3 of the 10 documents
where it appeared, and to predict this via a model
po(3 of 10  |Oh), where 0 describes (e.g.) how to de-
rive a binomial parameter nonlinearly from Oh. This
approach would not how often h was marked and in-
fer how relevant is feature h (i.e., infer Bh). In this
case, po is a simple channel that transforms relevant
features into direct indicators of the feature. Our
side information merely requires a more complex
transformation—from relevant features into well-
formed rationales, modulated by documents.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="method">
4 Experimental Data: Movie Reviews
</sectionHeader>
<bodyText confidence="0.999628655172414">
In Zaidan et al. (2007), we introduced the “Movie
Review Polarity Dataset Enriched with Annotator
Rationales.”8 It is based on the dataset of Pang and
Lee (2004),9 which consists of 1000 positive and
1000 negative movie reviews, tokenized and divided
into 10 folds (F0–F9). All our experiments use F9
as their final blind test set.
The enriched dataset adds rationale annotations
produced by an annotator A0, who annotated folds
F0–F8 of the movie review set with rationales (in the
form of textual substrings) that supported the gold-
standard classifications. We will use A0’s data to
determine the improvement of our method over a
(log-linear) baseline model without rationales. We
also use A0 to compare against the “masking SVM”
method and SVM baseline of Zaidan et al. (2007).
Since 0 can be tuned to a particular annotator, we
would also like to know how well this works with
data from annotators other than A0. We randomly
selected 100 reviews (50 positive and 50 negative)
and collected both class and rationale annotation
data from each of six new annotators A3–A8,10 fol-
lowing the same procedures as (Zaidan et al., 2007).
We report results using only data from A3–A5, since
we used the data from A6–A8 as development data
in the early stages of our work.
We use this new rationale-enriched dataset8 to de-
termine if our method works well across annotators.
We will only be able to carry out that comparison
</bodyText>
<footnote confidence="0.98987775">
8Available at http://cs.jhu.edu/∼ozaidan/rationales.
9Polarity dataset version 2.0.
10We avoid annotator names A1–A2, which were already
used in (Zaidan et al., 2007).
</footnote>
<page confidence="0.993652">
34
</page>
<figureCaption confidence="0.978686272727273">
Figure 1: Rationales as sequence an-
notation: the annotator highlighted
two textual segments as rationales for
a positive class. Highlighted words in
x~ are tagged I in ~r, and other words
are tagged O. The figure also shows
some φ-features. For instance, gO(,)-,
is a count of O-I transitions that occur
with a comma as the left word. Notice
also that grel is the sum of the under-
lined values.
</figureCaption>
<bodyText confidence="0.923065">
at small training set sizes, due to limited data from
A3–A8. The larger A0 dataset will still allow us to
evaluate our method on a range of training set sizes.
</bodyText>
<sectionHeader confidence="0.999843" genericHeader="method">
5 Detailed Models
</sectionHeader>
<subsectionHeader confidence="0.999526">
5.1 Modeling class annotations with pθ
</subsectionHeader>
<bodyText confidence="0.999606272727273">
We define the basic classifier pθ in equation (1) to be
a standard conditional log-linear model:
We encode its rationales as a corresponding tag se-
quence r~ = r1, ..., rM, as illustrated in Figure 1.
Here rm E {I, O1 according to whether the token
xm is in a rationale (i.e., xm was at least partly high-
lighted) or outside all rationales. x1 and xM are
special boundary symbols, tagged with O.
We predict the full tag sequence r~at once using
a conditional random field (Lafferty et al., 2001). A
CRF is just another conditional log-linear model:
</bodyText>
<equation confidence="0.996447333333333">
u(r, x, y,
~θ)
pφ(r|x,y,
def exp
~θ) =
def �
~θ)
~θ)
Zφ(x, y,
def
pθ(y  |x) def = exp(~θ · ~f(x, y)) = u(x, y)
Zθ(x) (2)
Zθ(x)
~φ · ~g(r,x,y, ~θ))
Zφ(x, y,
</equation>
<bodyText confidence="0.999667833333333">
where ~f(·) extracts a feature vector from a classified
~
document, θ are the corresponding weights of those
features, and Zθ(x) def � Ey u(x, y) is a normalizer.
We use the same set of binary features as in pre-
vious work on this dataset (Pang et al., 2002; Pang
and Lee, 2004; Zaidan et al., 2007). Specifically, let
V = {v1, ..., v177441 be the set of word types with
count &gt; 4 in the full 2000-document corpus. Define
fh(x, y) to be y if vh appears at least once in x, and
0 otherwise. Thus θ E 817744, and positive weights
in θ favor class label y = +1 and equally discourage
y = -1, while negative weights do the opposite.
This standard unigram feature set is linguistically
impoverished, but serves as a good starting point for
studying rationales. Future work should consider
more complex features and how they are signaled by
rationales, as discussed in section 3.2.
</bodyText>
<subsectionHeader confidence="0.999601">
5.2 Modeling rationale annotations with pφ
</subsectionHeader>
<bodyText confidence="0.9999846">
The rationales collected in this task are textual seg-
ments of a document to be classified. The docu-
ment itself is a word token sequence x~ = x1, ..., xM.
where ~g(·) extracts a feature vector, φ~ are the
corresponding weights of those features, and
</bodyText>
<equation confidence="0.854157">
def
Zφ(x, y, ~θ) = Er u(r, x, y, ~θ) is a normalizer.
</equation>
<bodyText confidence="0.9999125">
As usual for linear-chain CRFs, ~g(·) extracts two
kinds of features: first-order “emission” features that
relate rm to (xm, y, θ), and second-order “transi-
tion” features that relate rm to rm_1 (although some
of these also look at x).
These two kinds of features respectively capture
the “channel model” and “language model” of sec-
tion 3.2. The former says rm is I because xm is
associated with a relevant θ-feature. The latter says
rm is I simply because it is next to another I.
</bodyText>
<subsectionHeader confidence="0.981733">
5.3 Emission φ-features (“channel model”)
</subsectionHeader>
<bodyText confidence="0.999939857142857">
Recall that our θ-features (at present) correspond to
unigrams. Given (~x, y, ~θ), let us say that a unigram
w E x~ is relevant, irrelevant, or anti-relevant if
y · θw is respectively » 0, Pz� 0, or « 0. That is, w
is relevant if its presence in x strongly supports the
annotated class y, and anti-relevant if its presence
strongly supports the opposite class -y.
</bodyText>
<page confidence="0.998794">
35
</page>
<figureCaption confidence="0.7032264">
Figure 2: The
function family B3
in equation (3),
shown for s ∈
{10, 2, −2,−10}.
</figureCaption>
<bodyText confidence="0.986245">
We would like to learn the extent Orel to which
annotators try to include relevant unigrams in their
rationales, and the (usually lesser) extent 0antirel to
which they try to exclude anti-relevant unigrams.
�
This will help us infer 0 from the rationales.
The details are as follows. Orel and 0antirel are the
weights of two emission features extracted by g:
</bodyText>
<equation confidence="0.9997275">
I(rm = I) · B10(y · 0,;J
I(rm = I) · B−10(y · 0,;J
</equation>
<bodyText confidence="0.9999038">
Here I(·) denotes the indicator function, returning
1 or 0 according to whether its argument is true or
false. Relevance and negated anti-relevance are re-
spectively measured by the differentiable nonlinear
functions B10 and B−10, which are defined by
</bodyText>
<equation confidence="0.999593">
B3(a) = (log(1 + exp(a · s)) − log(2))/s (3)
</equation>
<bodyText confidence="0.965492939393939">
and graphed in Figure 2. Sample values of B10 and
grel are shown in Figure 1.
How does this work? The grel feature is a sum
over all unigrams in the document x. It does not fire
strongly on the irrelevant or anti-relevant unigrams,
since B10 is close to zero there.11 But it fires posi-
tively on relevant unigrams w if they are tagged with
I, and the strength of such firing increases approxi-
mately linearly with 8,,,. Since the weight Orel &gt; 0 in
practice, this means that raising a relevant unigram’s
8,,, (if y = +1) will proportionately raise its log-
odds of being tagged with I. Symmetrically, since
Oantirel &gt; 0 in practice, lowering an anti-relevant un-
igram’s 8,,, (if y = +1) will proportionately lower
11B10 sets the threshold for relevance to be about 0. One
could also include versions of the grel feature that set a higher
threshold, using B10(y · Oma_ − threshold).
its log-odds of being tagged with I, though not nec-
essarily at the same rate as for relevant unigrams.12
Should 0 also include traditional CRF emis-
sion features, which would recognize that particular
words like great tend to be tagged as I? No! Such
features would undoubtedly do a better job predict-
ing the rationales and hence increasing equation (1).
However, crucially, our true goal is not to predict
the rationales but to recover the classifier parame-
ters 0. Thus, if great tends to be highlighted, then
the model should not be permitted to explain this
directly by increasing some feature 0great, but only
indirectly by increasing Bgreat. We therefore permit
our rationale prediction model to consider only the
two emission features grel and gantirel, which see the
words in x only through their 0-values.
</bodyText>
<subsectionHeader confidence="0.810808">
5.4 Transition 0-features (“language model”)
</subsectionHeader>
<bodyText confidence="0.9685748">
Annotators highlight more than just the relevant un-
igrams. (After all, they aren’t told that our current
0-features are unigrams.) They tend to mark full
phrases, though perhaps taking care to exclude anti-
relevant portions. 0 models these phrases’ shape, via
weights for several “language model” features.
Most important are the 4 traditional CRF tag tran-
sition features gO-O, gO-I, gI-I, gI-O. For example,
gO-I counts the number of O-to-I transitions in r�
(see Figure 1). Other things equal, an annotator with
high OO-I is predicted to have many rationales per
1000 words. And if 0I-I is high, rationales are pre-
dicted to be long phrases (including more irrelevant
unigrams around or between the relevant ones).
We also learn more refined versions of these fea-
tures, which consider how the transition probabil-
ities are influenced by the punctuation and syntax
�
of the document x (independent of 0). These re-
fined features are more specific and hence more
sparsely trained. Their weights reflect deviations
from the simpler, “backed-off” transition features
such as gO-I. (Again, see Figure 1 for examples.)
Conditioning on left word. A feature of the form
gt1(v)-t2 is specified by a pair of tag types t1, t2 ∈
{I, O} and a vocabulary word type v. It counts the
12If the two rates are equal (0rel = we get a simpler
model in which the log-odds change exactly linearly with 0. for
each w, regardless of w’s relevance/irrelevance/anti-relevance.
This follows from the fact that B3(a) +B_3(a) simplifies to a.
</bodyText>
<equation confidence="0.9085796">
grel(x, y, r, 0) def M
= m=1
gantirel(x, y, r,
M
m=1
0) def
=
36
n
i=1
</equation>
<bodyText confidence="0.99990946875">
number of times an t1–t2 transition occurs in r~ con-
ditioned on v appearing as the first of the two word
tokens where the transition occurs. Our experiments
include gt1(v)-t2 features that tie I-O and O-I tran-
sitions to the 4 most frequent punctuation marks v
(comma, period, ?, !).
Conditioning on right word. A feature gt1-t2(v)
is similar, but v must appear as the second of the
two word tokens where the transition occurs. Again
here, we use gt1-t2(v) features that tie I-O and O-I
transitions to the four punctuation marks mentioned
above. We also include five features that tie O-I
transitions to the words no, not, so, very, and quite,
since in our development data, those words were
more likely than others to start rationales.13
Conditioning on syntactic boundary. We parsed
each rationale-annotated training document (no
parsing is needed at test time).14 We then marked
each word bigram x1-x2 with three nonterminals:
NEnd is the nonterminal of the largest constituent
that contains x1 and not x2, NStart is the nontermi-
nal of the largest constituent that contains x2 and
not x1, and NCross is the nonterminal of the smallest
constituent that contains both x1 and x2.
For a nonterminal N and pair of tag types (t1, t2),
we define three features, gt1-t2/E=N, gt1-t2/S=N,
and gt1-t2/C=N, which count the number of times
a t1-t2 transition occurs in r~ with N matching the
NEnd, NStart, or NCross nonterminal, respectively.
Our experiments include these features for 11 com-
mon nonterminal types N (DOC, TOP, S, SBAR,
FRAG, PRN, NP, VP, PP, ADJP, QP).
</bodyText>
<sectionHeader confidence="0.970933" genericHeader="method">
6 Training: Joint Optimization of 0 and 0
</sectionHeader>
<bodyText confidence="0.99236125">
To train our model, we use L-BFGS to locally max-
imize the log of the objective function (1):15
13These are the function words with count &gt; 40 in a random
sample of 100 documents, and which were associated with the
O-I tag transition at more than twice the average rate. We do
not use any other lexical 0-features that reference x, for fear that
they would enable the learner to explain the rationales without
changing 0 as desired (see the end of section 5.3).
</bodyText>
<footnote confidence="0.8906335">
14We parse each sentence with the Collins parser (Collins,
1999). Then the document has one big parse tree, whose root is
DOC, with each sentence being a child of DOC.
15One might expect this function to be convex because pe and
po are both log-linear models with no hidden variables. How-
ever, log po(ri  |xi, yi, 0) is not necessarily convex in 0.
</footnote>
<equation confidence="0.98497625">
log pθ(yi  |xi) 212kθk2
θ
log pφ(ri  |xi, yi, θ)) − 212kφk2 (4)
φ
</equation>
<bodyText confidence="0.99655884">
This defines ppoor from (1) to be a standard diago-
nal Gaussian prior, with variances σ2θ and σ2φ for the
two sets of parameters. We optimize σ2 θ in our ex-
periments. As for σ2φ, different values did not affect
the results, since we have a large number of {I,O}
rationale tags to train relatively few φ weights; so
we simply use σ2φ = 1 in all of our experiments.
Note the new C factor in equation (4). Our ini-
tial experiments showed that optimizing equation (4)
without C led to an increase in the likelihood of the
rationale data at the expense of classification accu-
racy, which degraded noticeably. This is because
the second sum in (4) has a much larger magnitude
than the first: in a set of 100 documents, it predicts
around 74,000 binary {I,O} tags, versus the one
hundred binary class labels. While we are willing
to reduce the log-likelihood of the training classifi-
cations (the first sum) to a certain extent, focusing
too much on modeling rationales (the second sum)
is clearly not our ultimate goal, and so we optimize
C on development data to achieve some balance be-
tween the two terms of equation (4). Typical values
of C range from 1
300 to 150.16
We perform alternating optimization on θ and φ:
</bodyText>
<listItem confidence="0.999022">
1. Initialize θ to maximize equation (4) but with
C = 0 (i.e. based only on class data).
2. Fix θ, and find φ that maximizes equation (4).
3. Fix φ, and find θ that maximizes equation (4).
4. Repeat 2 and 3 until convergence.
</listItem>
<bodyText confidence="0.9998982">
The L-BFGS method requires calculating the gra-
dient of the objective function (4). The partial
derivatives with respect to components of θ and φ
involve calculating expectations of the feature func-
tions, which can be computed in linear time (with
respect to the size of the training set) using the
forward-backward algorithm for CRFs. The par-
tial derivatives also involve the derivative of (3),
to determine how changing θ will affect the firing
strength of the emission features grel and gantirel.
</bodyText>
<footnote confidence="0.8136635">
16C also balances our confidence in the classifications y
against our confidence in the rationales r; either may be noisy.
</footnote>
<equation confidence="0.870102333333333">
n
+C(
i=1
</equation>
<page confidence="0.998334">
37
</page>
<sectionHeader confidence="0.998477" genericHeader="method">
7 Experimental Procedures
</sectionHeader>
<bodyText confidence="0.999796444444444">
We report on two sets of experiments. In the first
set, we use the annotation data that A3–A5 provided
for the small set of 100 documents (as well as the
data from A0 on those same 100 documents). In
the second set, we used A0’s abundant annotation
data to evaluate our method with training set sizes up
to 1600 documents, and compare it with three other
methods: log-linear baseline, SVM baseline, and the
SVM masking method of (Zaidan et al., 2007).
</bodyText>
<subsectionHeader confidence="0.998046">
7.1 Learning curves
</subsectionHeader>
<bodyText confidence="0.999412857142857">
The learning curves reported in section 8.1 are gen-
erated exactly as in (Zaidan et al., 2007). Each curve
shows classification accuracy at training set sizes
T = 1, 2,..., 9 folds (i.e. 200, 400,..., 1600 training
documents). For a given size T, the reported accu-
racy is an average of 9 experiments with different
subsets of the entire training set, each of size T:
</bodyText>
<equation confidence="0.937974">
acc(F9  |Fi+1 U ... U Fi+T) (5)
</equation>
<bodyText confidence="0.999985333333333">
where Fj denotes the fold numbered j mod 9, and
acc(F9  |Y ) means classification accuracy on the
held-out test set F9 after training on set Y .
We use an appropriate paired permutation test, de-
tailed in (Zaidan et al., 2007), to test differences in
(5). We call a difference significant at p &lt; 0.05.
</bodyText>
<subsectionHeader confidence="0.992003">
7.2 Comparison to “masking SVM” method
</subsectionHeader>
<bodyText confidence="0.999976647058824">
We compare our method to the “masking SVM”
method of (Zaidan et al., 2007). Briefly, that method
used rationales to construct several so-called con-
trast examples from every training example. A con-
trast example is obtained by “masking out” one of
the rationales highlighted to support the training ex-
ample’s class. A good classifier should have more
trouble on this modified example. Hence, Zaidan et
al. (2007) required the learned SVM to classify each
contrast example with a smaller margin than the cor-
responding original example (and did not require it
to be classified correctly).
The masking SVM learner relies on a simple geo-
metric principle; is trivial to implement on top of an
existing SVM learner; and works well. However, we
believe that the generative method we present here is
more interesting and should apply more broadly.
</bodyText>
<figureCaption confidence="0.96283175">
Figure 3: Classification accuracy curves for the 4 meth-
ods: the two baseline learners that only utilize class data,
and the two learners that also utilize rationale annota-
tions. The SVM curves are from (Zaidan et al., 2007).
</figureCaption>
<bodyText confidence="0.999937166666667">
First, the masking method is specific to improving
an SVM learner, whereas our method can be used to
improve any classifier by adding a rationale-based
regularizer (the second half of equation (4)) to its
objective function during training.
More important, there are tasks where it is unclear
how to generate contrast examples. For the movie
review task, it was natural to mask out a rationale
by pretending its words never occurred in the doc-
ument. After all, most word types do not appear in
most documents, so it is natural to consider the non-
presence of a word as a “default” state to which we
can revert. But in an image classification task, how
should one modify the image’s features to ignore
some spatial region marked as a rationale? There is
usually no natural “default” value to which we could
set the pixels. Our method, on the other hand, elim-
inates contrast examples altogether.
</bodyText>
<sectionHeader confidence="0.941432" genericHeader="evaluation">
8 Experimental Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.991911">
8.1 The added benefit of rationales
</subsectionHeader>
<bodyText confidence="0.999891375">
Fig. 3 shows learning curves for four methods. A
log-linear model shows large and significant im-
provements, at all training sizes, when we incor-
porate rationales into its training via equation (4).
Moreover, the resulting classifier consistently out-
performs17 prior work, the masking SVM, which
starts with a slightly better baseline classifier (an
SVM) but incorporates the rationales more crudely.
</bodyText>
<figure confidence="0.888897666666666">
17Differences are not significant at sizes 200, 1000, and 1600.
8
1
9
�
i=0
</figure>
<page confidence="0.992624">
38
</page>
<table confidence="0.999686111111111">
size A0 A3 A4 A5
SVM baseline 100 72.0 72.0 72.0 70.0
SVM+contrasts 100 75.0 73.0 74.0 72.0
Log-linear baseline 100 71.0 73.0 71.0 70.0
Log-linear+rats 100 76.0 76.0 77.0 74.0
SVM baseline 20 63.4 62.2 60.4 62.6
SVM+contrasts 20 65.4 63.4 62.4 64.8
Log-linear baseline 20 63.0 62.2 60.2 62.4
Log-linear+rats 20 65.8 63.6 63.4 64.8
</table>
<tableCaption confidence="0.90767">
Table 1: Accuracy rates using each annotator’s data. In a
given column, a value in italics is not significantly differ-
ent from the highest value in that column, which is bold-
faced. The size=20 results average over 5 experiments.
</tableCaption>
<bodyText confidence="0.999719571428571">
To confirm that we could successfully model an-
notators other than A0, we performed the same
comparison for annotators A3–A5; each had pro-
vided class and rationale annotations on a small 100-
document training set. We trained a separate 0 for
each annotator. Table 1 shows improvements over
baseline, usually significant, at 2 training set sizes.
</bodyText>
<subsectionHeader confidence="0.995795">
8.2 Analysis
</subsectionHeader>
<bodyText confidence="0.999981695652174">
Examining the learned weights 0� gives insight into
annotator behavior. High weights include I-O and
O-I transitions conditioned on punctuation, e.g.,
0I(.)-O = 3.55,18 as well as rationales ending at the
end of a major phrase, e.g., 0I-O/E=VP = 1.88.
The large emission feature weights, e.g., 0rel =
14.68 and 0antirel = 15.30, tie rationales closely to
0 values, as hoped. For example, in Figure 1, the
word w = succeeds, with 0,,, = 0.13, drives up
p(I)/p(O) by a factor of 7 (in a positive document)
relative to a word with 0,,, = 0.
In fact, feature ablation experiments showed that
almost all the classification benefit from rationales
can be obtained by using only these 2 emission
0-features and the 4 unconditioned transition 0-
features. Our full 0 (115 features) merely improves
our ability to predict the rationales (whose likeli-
hood does increase significantly with more features).
We also checked that annotators’ styles differ
enough that it helps to tune 0 to the “target” annota-
tor A who gave the rationales. Table 3 shows that a 0
model trained on A’s own rationales does best at pre-
dicting new rationales from A. Table 2 shows that as
</bodyText>
<footnote confidence="0.75572">
18When trained on folds F4–F8 with A0’s rationales.
</footnote>
<table confidence="0.9982204">
0A0 0A3 0A4 0A5 Baseline
OA0 76.0 73.0 74.0 73.0 71.0
OA3 73.0 76.0 74.0 73.0 73.0
OA4 75.0 73.0 77.0 74.0 71.0
OA5 74.0 71.0 72.0 74.0 70.0
</table>
<tableCaption confidence="0.99347625">
Table 2: Accuracy rate for an annotator’s 0 (rows) ob-
tained when using some other annotator’s 0 (columns).
Notice that the diagonal entries and the baseline column
are taken from rows of Table 1 (size=100).
</tableCaption>
<table confidence="0.998924833333333">
0A0 0A3 0A4 0A5 Trivial
model
−L(rA0) 0.073 0.086 0.077 0.088 0.135
−L(rA3) 0.084 0.068 0.071 0.068 0.130
−L(rA4) 0.088 0.084 0.075 0.085 0.153
−L(rA5) 0.058 0.044 0.047 0.044 0.111
</table>
<tableCaption confidence="0.9911255">
Table 3: Cross-entropy per tag of rationale annotations
r� for each annotator (rows), when predicted from that
</tableCaption>
<bodyText confidence="0.9350132">
�
annotator’s x and 0 via a possibly different annotator’s
0 (columns). For comparison, the trivial model is a bi-
gram model of F, which is trained on the target annotator
but ignores x and B. 5-fold cross-validation on the 100-
document set was used to prevent testing on training data.
a result, classification performance on the test set is
usually best if it was A’s own 0 that was used to help
learn 0 from A’s rationales. In both cases, however,
a different annotator’s 0 is better than nothing.
</bodyText>
<sectionHeader confidence="0.998433" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999994777777778">
We have demonstrated a effective method for elic-
iting extra knowledge from naive annotators, in
the form of lightweight “rationales” for their an-
notations. By explicitly modeling the annotator’s
rationale-marking process, we are able to infer a bet-
ter model of the original annotations.
We showed that our method performs signifi-
cantly better than two strong baseline classifiers,
and also outperforms our previous discriminative
method for exploiting rationales (Zaidan et al.,
2007). We also saw that it worked across four anno-
tators who have different rationale-marking styles.
In future, we are interested in new domains that
can adaptively solicit rationales for some or all
training examples. Our new method, being essen-
tially Bayesian inference, is potentially extensible to
many other situations—other tasks, classifier archi-
tectures, and more complex features.
</bodyText>
<page confidence="0.999168">
39
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998698">
Eric Brill and Grace Ngai. 1999. Man [and woman] vs.
machine: A case study in base noun phrase learning.
In Proceedings of the 37th ACL Conference.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of ACM Special Interest
Group on Information Retrieval, (SIGIR).
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 320–327, New York City,
USA, June. Association for Computational Linguis-
tics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 117–125, Hong Kong.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL, pages 271–
278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP, pages 79–86.
Hema Raghavan and James Allan. 2007. An interactive
algorithm for asking and incorporating feature feed-
back into support vector machines. In Proceedings of
SIGIR.
Hema Raghavan, Omid Madani, and Rosie Jones. 2006.
Active learning on both features and instances. Jour-
nal of Machine Learning Research, 7:1655–1686,
Aug.
Luis von Ahn, Ruoran Liu, and Manuel Blum. 2006.
Peekaboom: A game for locating objects. In CHI
’06: Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems, pages 55–64.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using “annotator rationales” to improve machine
learning for text categorization. In NAACL HLT 2007;
Proceedings of the Main Conference, pages 260–267,
April.
</reference>
<page confidence="0.998636">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.008135">
<title confidence="0.998919">Modeling Annotators: Generative Approach to Learning from Annotator</title>
<author confidence="0.999359">F Zaidan</author>
<affiliation confidence="0.999232">Dept. of Computer Science, Johns Hopkins</affiliation>
<address confidence="0.998312">Baltimore, MD 21218,</address>
<abstract confidence="0.979904919216647">A human annotator can provide hints to a machine learner by highlighting contextual “rationales” for each of his or her annotations (Zaidan et al., 2007). How can one exploit this side information to better learn the desired We present a generative model of how given annotator, knowing the true stochastically chooses rationales. Thus, observing the rationales helps infer the true We collect substring rationales for a sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator. Our new generative approach exploits the rationales more effectively than our previous “masking SVM” approach. It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks. 1 Background Many recent papers aim to reduce the amount of annotated data needed to train the parameters of a statistical model. Well-known paradigms include active learning, semi-supervised learning, and either domain adaptation or cross-lingual transfer from existing annotated data. A rather different paradigm is to change the acis given to annotators, giving them a greater hand in shaping the learned classifier. After all, human annotators themselves are more than just black-box classifiers to be run on training data. They possess some introspective knowledge about their own classification procedure. The hope is to mine this knowledge rapidly via appropriate questions and use it to help train a machine classifier. do this, however, is still being explored. 1.1 Hand-crafted rules An obvious option is to have the annotators directly express their knowledge by hand-crafting rules. This work was supported by National Science Foundation grant No. 0347822 and the JHU WSE/APL Partnership Fund. Special thanks to Christine Piatko for many useful discussions. 31 approach remains “data-driven” if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples. This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators. One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. 1.2 Feature selection by humans More recent work has focused on statistical classifiers. Training such classifiers faces the “credit asproblem.” Given a training example many features, which features are responsible for its class It may take many training examto distinguish useful vs. irrelevant To reduce the number of training examples needed, one can ask annotators to examine or propose some candidate features. This is possible even for the very large feature sets that are typically used in NLP. In document classification, Raghavan et al. (2006) show that feature selection by an oracle could be helpful, and that humans are both rapid and reagood at distinguishing highly useful features from randomly chosen ones, even when these out of context. Druck et al. (2008) show annotators some features a fixed feature set, and ask them to choose a label that as high as possible. Haghighi and Klein (2006) do the reverse: for each label they ask the annotators to propose a “prototypical” features that as high as possible. 1.3 Feature selection in context The above methods consider features out of context. An annotator might have an easier time examining NLP systems use thousands or millions of features, because it is helpful to include lexical features over a large vocabulary, often conjoined with lexical or non-lexical context. of the 2008 Conference on Empirical Methods in Natural Language pages 31–40, October 2008. Association for Computational Linguistics context recognize whether they appear relevant. This is particularly true for features that are only modestly or only sometimes helpful, which may be abundant in NLP tasks. Thus, Raghavan et al. (2006) propose an active learning method in which, while classifying a training document, the annotator also identifies some feaof as particularly relevant. E.g., the annotator might highlight particular unigrams as he or she reads the document. In their proposal, a feature that is highlighted in any document is assumed to be globally more relevant. Its dimension in feature space is scaled by a factor of 10 so that this feature has more influence on distances or inner products, and hence on the learned classifier. 1.4 Concerns about marking features Despite the success of the above work, we have several concerns about asking annotators to identify globally relevant features. First, a feature in isolation really does not have a well-defined worth. A feature may be useful only in with other or be useful only the extent that other correlated features are selected to do the same work. Second, it is not clear how an annotator would easily view and highlight features in context, except for the simplest feature sets. In the phrase shares up 3%, may be several feathat fire on the substring the string its case-invariant form lemma would also respond to apits context-dependent sense its part speech etc. How does the annotator indicate which of these features are relevant? Third, annotating features is only appropriate when the feature set can be easily understood by a human. This is not always the case. It would be hard for annotators to read, write, or evaluate a description of a complex syntactic configuration in NLP or a convolution filter in machine vision. Fourth, traditional annotation efforts usually try to remain agnostic about the machine learning methods example, a linear classifier can learn that most training satisfy setting this solution requires selecting both features. More simply, a polynomial kernel can consider the conjunction if both selected as features. and features to be used. The project’s cost is justified by saying that the annotations will be reused by many researchers (perhaps in a “shared task”), who are free to compete on how they tackle the learning problem. Unfortunately, feature annotation commits to a particular feature set at annotation time. Subsequent research cannot easily adjust the definition of the features, or obtain annotation of new features. 2 Annotating Rationales To solve these problems, we propose that annotators not features rather relevant of the In earlier work (Zaidan et al., 2007), we called these markings “rationales.” For example, when classifying a movie review as positive or negative, the annotator would also highlight phrases that supported that judgment. Figure 1 shows two such rationales. A multi-annotator timing study (Zaidan et al., 2007) found that highlighting rationale phrases while reading movie reviews only doubled annotation time, although annotators marked 5–11 rationale substrings in addition to the simple binary class. The benefit justified the extra time. Furthermore, much of the benefit could have been obtained by giving rationales for only a fraction of the reviews. In the visual domain, when classifying an image as containing a zoo, the annotator might circle some animals or cages and the sign reading “Zoo.” Peekaboom Ahn et al., 2006) was in fact built to elicit such approximate yet relevant regions of images. Further scenarios were discussed in (Zaidan et al., 2007): rationale annotation for named entities, linguistic relations, or handwritten digits. Annotating rationales does not require the annotator to think about the feature space, nor even to know anything about it. Arguably this makes annotation easier and more flexible. It also preserves the reusability of the annotated data. Anyone is free to reuse our collected rationales (section 4) to aid in learning a classifier with richer features, or a different kind of classifier altogether, using either our procedures or novel procedures. 3 Modeling Rationale Annotations As rationales are more indirect than explicit features, they present a trickier machine learning problem. 32 wish to learn the parameters some classifier. How can the annotator’s rationales help us to do this without many training examples? We will have to exploit a presumed relationship between the and the optimal value of the value that we would learn on an infinite training set). This paper exploits an explicit, parametric model that relationship. The model’s parameters intended to capture what that annotator is doing when he or she marks rationales. Most importantly, they capture how he or she is influenced by the true Given this, our learning method will prefer values would adequately explain the rationales (as well as the training classifications). 3.1 A generative approach For concreteness, we will assume that the task is document classification. Our training data consists ..., where a document, its annotated class, and its rationale markup. At test time we will have to prefromwithout any propose to jointly choose parameter vectors maximize the following regularized condin θ, n Here we are trying to model all the annotations, both The first factor predicts an ordiprobabilistic classifier while the novel secfactor predicts a model how annotators generate the rationale annotations. The crucial point is that the second factor depends supposed to reflect the relation beis modeled by As a result, learner has an incentive to modify a way that increases the second factor, even if this somedecreases the first factor on training would be preferable to integrate out even but more difficult. even examples where the annotation or unhelpful can provide useful information about pair Two annotators marking the same movie remight disagree on whether it is overall a positive or nega- After training, one should simply use the first facclassify test documents The second factor is irrelevant for test documents, since they not been annotated with rationales The second factor may likewise be omitted for any documents have not been annotated rationales, as there is no predict in those cases. In the extreme case where no documents are annotated with rationales, equation (1) reduces to the standard training procedure. 3.2 Noisy channel design of rationale models Like ordinary class annotations, rationale annotations present us with a “credit assignment problem,” albeit a smaller one that is limited to features that fire the vicinity” of the rationale Some of these were likely responsible for the classificahence triggered the rationale. Other such were just innocent bystanders. the interesting part of our model is y, which models the rationale annotation prorationales but in noisy ways. this noisy channel idea seriously, y, consider two questions when assesswhether a plausible set of rationales given First, it needs a “language model” of rationales: of rationales that are well-formed i.e., before considered? Second, it needs “channel model”: does signal the feaof strongly support classifying If a feature contributes heavily to the classification document class then the model tell us which document to be highlighted as a result. The channel model must know about the partickinds of features that are extracted by by Suppose the feature ... weight is predictive of the annotated class This raises the probabilities of the annotator’s highlighting each of various words, or combinations of in a phrase like the most gripping banon The channel model parameters in tive review—but the second factor still allows learning positive features from the first annotator’s positive rationales, and negative features from the second annotator’s negative rationales. current experiments use only unigram features, to match past work, but we use this example to outline how our approach generalizes to complex linguistic (or visual) features. 33 specify how of these probabilities raised, based on the magnitude of the and the fact that the feature is an instance the template ... (Thus, no parameters specific to the word it is a low-dimensional vector that only describes the general style in translating however, is independent of feature set It models what rationales tend to look like in the input domain—e.g., documents or In the document case, describe: How frequent and how long are typical rationales? Do their edges tend to align with punctuation or masyntactic boundaries in Are they rarer in the of a document, or in certain Thanks to the language model, we do not need to high to explain every word in a rationale. The language model can “explain away” some words as having been highlighted only because this annotator prefers not to end a rationale in midphrase, or prefers to sweep up close-together features with a single long rationale rather than many short ones. Similarly, the language model can help explain why some words, though important, might been included in any rationale of If there are multiple annotators, one can learn diffor each annotator, reflecting different annotation found this to be useful (section 8.2). We remark that our generative modeling approach (1)) would also apply if not rationale markup, but some other kind of so-called “side such as the discussed in section 1. For example, Raghavan et al. assume that if feature relevant—a bicurrent experiments do not model this last point. Howwe imagine that if the document only has a few that support the classification, the annotator will probably mark most of them, whereas if such features are abundant, the annotator may lazily mark only a few of the strongest ones. A simple would equip a different “bias” or “threshold” each rationale training document to modthe priori of marking a rationale in By fitting this bias parameter, we deduce how lazy the annotator (for whatever reason) on document If desired, a prior consider whether many strong whether the annotator has recently had a coffee break, etc. insufficient rationale data to recover some annotaone could smooth using data from other annotators. in our situation, relatively few parameters to learn. nary distinction—iff it was selected in at least one document. But it might be more informative to obthat selected in 3 of the 10 documents where it appeared, and to predict this via a model of 10 where (e.g.) how to dea binomial parameter nonlinearly from This would not often marked and inrelevant feature infer In this a simple channel that transforms relevant features into direct indicators of the feature. Our side information merely requires a more complex transformation—from relevant features into wellformed rationales, modulated by documents. 4 Experimental Data: Movie Reviews In Zaidan et al. (2007), we introduced the “Movie Review Polarity Dataset Enriched with Annotator is based on the dataset of Pang and consists of 1000 positive and 1000 negative movie reviews, tokenized and divided 10 folds All our experiments use as their final blind test set. The enriched dataset adds rationale annotations produced by an annotator A0, who annotated folds the movie review set with rationales (in the form of textual substrings) that supported the goldstandard classifications. We will use A0’s data to determine the improvement of our method over a (log-linear) baseline model without rationales. We also use A0 to compare against the “masking SVM” method and SVM baseline of Zaidan et al. (2007). be tuned to a particular annotator, we would also like to know how well this works with data from annotators other than A0. We randomly selected 100 reviews (50 positive and 50 negative) and collected both class and rationale annotation from each of six new annotators following the same procedures as (Zaidan et al., 2007). We report results using only data from A3–A5, since we used the data from A6–A8 as development data in the early stages of our work. use this new rationale-enriched determine if our method works well across annotators. We will only be able to carry out that comparison at dataset version 2.0. avoid annotator names A1–A2, which were already used in (Zaidan et al., 2007). 34 Figure 1: Rationales as sequence an-notation: the annotator highlighted two textual segments as rationales for a positive class. Highlighted words in tagged and other words tagged The figure also shows For instance, a count of that occur with a comma as the left word. Notice that the sum of the under-lined values. at small training set sizes, due to limited data from A3–A8. The larger A0 dataset will still allow us to evaluate our method on a range of training set sizes. 5 Detailed Models Modeling class annotations with define the basic classifier equation (1) to be a standard conditional log-linear model: We encode its rationales as a corresponding tag se- ..., as illustrated in Figure 1. to whether the token rationale (i.e., at least partly highor rationales. boundary symbols, tagged with predict the full tag sequence once using a conditional random field (Lafferty et al., 2001). A CRF is just another conditional log-linear model: x, y, = y, def = y, where a feature vector from a classified ~ the corresponding weights of those and a normalizer. We use the same set of binary features as in previous work on this dataset (Pang et al., 2002; Pang and Lee, 2004; Zaidan et al., 2007). Specifically, let ..., the set of word types with the full 2000-document corpus. Define be at least once in and Thus positive weights class label +1 equally discourage while negative weights do the opposite. This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales. Future work should consider complex features and how signaled by rationales, as discussed in section 3.2. Modeling rationale annotations with The rationales collected in this task are textual segments of a document to be classified. The docuitself is a word token sequence ..., a feature vector, the corresponding weights of those features, and def y, = x, y, a normalizer. usual for linear-chain CRFs, two kinds of features: first-order “emission” features that y, and second-order “transifeatures that relate some these also look at These two kinds of features respectively capture the “channel model” and “language model” of sec- 3.2. The former says with a relevant The latter says because it is next to another Emission (“channel model”) that our (at present) correspond to Given y, let us say that a unigram or respectively or That is, relevant if its presence in supports the class and anti-relevant if its presence supports the opposite class 35 Figure 2: The family in equation (3), for 2, would like to learn the extent which try to unigrams in their and the (usually lesser) extent they try to unigrams. � will help us infer the rationales. details are as follows. the of two emission features extracted by the indicator function, returning to whether its argument is true or false. Relevance and negated anti-relevance are respectively measured by the differentiable nonlinear which are defined by = (log(1 + graphed in Figure 2. Sample values of shown in Figure 1. does this work? The is a sum all unigrams in the document It does not fire strongly on the irrelevant or anti-relevant unigrams, close to zero it fires posion relevant unigrams they are tagged with and the strength of such firing increases approxilinearly with Since the weight practice, this means that raising a relevant unigram’s will proportionately raise its logof being tagged with Symmetrically, since practice, lowering an anti-relevant unwill proportionately lower the threshold for relevance to be about 0. One also include versions of the that set a higher using log-odds of being tagged with though not necat the same rate as for relevant include traditional CRF emission features, which would recognize that particular like to be tagged as No! Such features would undoubtedly do a better job predicting the rationales and hence increasing equation (1). However, crucially, our true goal is not to predict the rationales but to recover the classifier parame- Thus, if to be highlighted, then the model should not be permitted to explain this by increasing some feature but only by increasing We therefore permit our rationale prediction model to consider only the emission features which see the in through their Transition (“language model”) Annotators highlight more than just the relevant unigrams. (After all, they aren’t told that our current are unigrams.) They tend to mark full phrases, though perhaps taking care to exclude antiportions. these phrases’ shape, via weights for several “language model” features. Most important are the 4 traditional CRF tag tranfeatures For example, number of in (see Figure 1). Other things equal, an annotator with predicted to have many rationales per words. And if high, rationales are predicted to be long phrases (including more irrelevant unigrams around or between the relevant ones). We also learn more refined versions of these features, which consider how the transition probabilities are influenced by the punctuation and syntax � the document of These refined features are more specific and hence more sparsely trained. Their weights reflect deviations from the simpler, “backed-off” transition features as (Again, see Figure 1 for examples.) on left word. feature of the form specified by a pair of tag types a vocabulary word type It counts the the two rates get a simpler in which the log-odds change exactly linearly with regardless of relevance/irrelevance/anti-relevance. follows from the fact that to y, r, M = y, r, M = 36 of times an occurs in conon as the first of the two word tokens where the transition occurs. Our experiments that tie tranto the 4 most frequent punctuation marks period, on right word. feature similar, but appear as the second of the two word tokens where the transition occurs. Again we use that tie transitions to the four punctuation marks mentioned We also include five features that tie to the words and since in our development data, those words were likely than others to start on syntactic boundary. parsed each rationale-annotated training document (no is needed at test We then marked word bigram three nonterminals: the nonterminal of the largest constituent contains not the nontermiof the largest constituent that contains and the nonterminal of the that contains both a nonterminal pair of tag types define three features, which count the number of times occurs in the or respectively. Our experiments include these features for 11 comnonterminal types Training: Joint Optimization of To train our model, we use L-BFGS to locally maxthe log of the objective function are the function words with count a random sample of 100 documents, and which were associated with the transition at more than twice the average rate. We do use any other lexical that reference for fear that they would enable the learner to explain the rationales without desired (see the end of section 5.3). parse each sentence with the Collins parser (Collins, 1999). Then the document has one big parse tree, whose root is with each sentence being a child of might expect this function to be convex because both log-linear models with no hidden variables. Hownot necessarily convex in θ log φ defines (1) to be a standard diago- Gaussian prior, with variances and for the sets of parameters. We optimize our ex- As for different values did not affect results, since we have a large number of tags to train relatively few so simply use = 1 all of our experiments. the new in equation (4). Our initial experiments showed that optimizing equation (4) to an increase in the likelihood of the rationale data at the expense of classification accuracy, which degraded noticeably. This is because the second sum in (4) has a much larger magnitude than the first: in a set of 100 documents, it predicts 74,000 binary versus the one hundred binary class labels. While we are willing to reduce the log-likelihood of the training classifications (the first sum) to a certain extent, focusing too much on modeling rationales (the second sum) is clearly not our ultimate goal, and so we optimize development data to achieve some balance between the two terms of equation (4). Typical values from perform alternating optimization on Initialize maximize equation (4) but with 0 based only on class data). Fix and find maximizes equation (4). Fix and find maximizes equation (4). 4. Repeat 2 and 3 until convergence. The L-BFGS method requires calculating the gradient of the objective function (4). The partial with respect to components of involve calculating expectations of the feature functions, which can be computed in linear time (with respect to the size of the training set) using the forward-backward algorithm for CRFs. The partial derivatives also involve the derivative of (3), determine how changing affect the firing of the emission features and balances our confidence in the classifications our confidence in the rationales either may be noisy. n 37 7 Experimental Procedures We report on two sets of experiments. In the first set, we use the annotation data that A3–A5 provided for the small set of 100 documents (as well as the data from A0 on those same 100 documents). In the second set, we used A0’s abundant annotation data to evaluate our method with training set sizes up to 1600 documents, and compare it with three other methods: log-linear baseline, SVM baseline, and the SVM masking method of (Zaidan et al., 2007). 7.1 Learning curves The learning curves reported in section 8.1 are generated exactly as in (Zaidan et al., 2007). Each curve shows classification accuracy at training set sizes 1, 2,..., 9 (i.e. 400,..., 1600 For a given size the reported accuracy is an average of 9 experiments with different of the entire training set, each of size the fold numbered 9, classification accuracy on the test set training on set We use an appropriate paired permutation test, detailed in (Zaidan et al., 2007), to test differences in We call a difference significant at &lt; 7.2 Comparison to “masking SVM” method We compare our method to the “masking SVM” method of (Zaidan et al., 2007). Briefly, that method rationales to construct several so-called conexamples every training example. A contrast example is obtained by “masking out” one of the rationales highlighted to support the training exclass. A should have more trouble on this modified example. Hence, Zaidan et al. (2007) required the learned SVM to classify each contrast example with a smaller margin than the corresponding original example (and did not require it to be classified correctly). The masking SVM learner relies on a simple geometric principle; is trivial to implement on top of an existing SVM learner; and works well. However, we believe that the generative method we present here is more interesting and should apply more broadly. Figure 3: Classification accuracy curves for the 4 methods: the two baseline learners that only utilize class data, and the two learners that also utilize rationale annotations. The SVM curves are from (Zaidan et al., 2007). First, the masking method is specific to improving an SVM learner, whereas our method can be used to improve any classifier by adding a rationale-based regularizer (the second half of equation (4)) to its objective function during training. More important, there are tasks where it is unclear how to generate contrast examples. For the movie review task, it was natural to mask out a rationale by pretending its words never occurred in the document. After all, most word types do not appear in most documents, so it is natural to consider the nonpresence of a word as a “default” state to which we can revert. But in an image classification task, how should one modify the image’s features to ignore some spatial region marked as a rationale? There is usually no natural “default” value to which we could set the pixels. Our method, on the other hand, eliminates contrast examples altogether. 8 Experimental Results and Analysis 8.1 The added benefit of rationales Fig. 3 shows learning curves for four methods. A log-linear model shows large and significant improvements, at all training sizes, when we incorporate rationales into its training via equation (4). Moreover, the resulting classifier consistently outprior work, the masking SVM, which starts with a slightly better baseline classifier (an SVM) but incorporates the rationales more crudely. are not significant at sizes 200, 1000, and 1600. 8 1 9 � 38 size A0 A3 A4 A5 SVM baseline 100 72.0 72.0 72.0 70.0 SVM+contrasts 100 75.0 73.0 74.0 72.0 Log-linear baseline 100 71.0 73.0 71.0 70.0 Log-linear+rats 100 76.0 76.0 77.0 74.0 SVM baseline 20 63.4 62.2 60.4 62.6 SVM+contrasts 20 65.4 63.4 62.4 64.8 Log-linear baseline 20 63.0 62.2 60.2 62.4 Log-linear+rats 20 65.8 63.6 63.4 64.8 Table 1: Accuracy rates using each annotator’s data. In a column, a value in not significantly differfrom the highest value in that column, which is bold- The size=20 results average over 5 experiments. To confirm that we could successfully model annotators other than A0, we performed the same comparison for annotators A3–A5; each had provided class and rationale annotations on a small 100training set. We trained a separate each annotator. Table 1 shows improvements over baseline, usually significant, at 2 training set sizes. 8.2 Analysis the learned weights insight into behavior. High weights include conditioned on punctuation, e.g., as well as rationales ending at the of a major phrase, e.g., 1.88. large emission feature weights, e.g., 15.30, rationales closely to as hoped. For example, in Figure 1, the with 0.13, up a factor of 7 (in a positive document) to a word with 0. In fact, feature ablation experiments showed that almost all the classification benefit from rationales can be obtained by using only these 2 emission and the 4 unconditioned transition Our full features) merely improves our ability to predict the rationales (whose likelihood does increase significantly with more features). We also checked that annotators’ styles differ that it helps to tune the “target” annotagave the rationales. Table 3 shows that a trained on does best at prenew rationales from Table 2 shows that as trained on folds A0’s rationales. Baseline 76.0 73.0 74.0 73.0 71.0 73.0 76.0 74.0 73.0 73.0 75.0 73.0 77.0 74.0 71.0 74.0 71.0 72.0 74.0 70.0 2: Accuracy rate for an annotator’s obwhen using some other annotator’s Notice that the diagonal entries and the baseline column are taken from rows of Table 1 (size=100). Trivial model 0.073 0.086 0.077 0.088 0.135 0.084 0.068 0.071 0.068 0.130 0.088 0.084 0.075 0.085 0.153 0.058 0.044 0.047 0.044 0.111 Table 3: Cross-entropy per tag of rationale annotations each annotator (rows), when predicted from that � a possibly different annotator’s For comparison, the trivial model is a bimodel of which is trained on the target annotator ignores cross-validation on the 100document set was used to prevent testing on training data. a result, classification performance on the test set is best if it was was used to help rationales. In both cases, however, different annotator’s better than nothing. 9 Conclusions We have demonstrated a effective method for elicextra knowledge from in the form of lightweight “rationales” for their annotations. By explicitly modeling the annotator’s rationale-marking process, we are able to infer a better model of the original annotations. We showed that our method performs significantly better than two strong baseline classifiers, and also outperforms our previous discriminative method for exploiting rationales (Zaidan et al., 2007). We also saw that it worked across four annotators who have different rationale-marking styles. In future, we are interested in new domains that can adaptively solicit rationales for some or all training examples. Our new method, being essentially Bayesian inference, is potentially extensible to many other situations—other tasks, classifier architectures, and more complex features. 39 References Eric Brill and Grace Ngai. 1999. Man [and woman] vs. machine: A case study in base noun phrase learning. of the 37th ACL Collins. 1999. Statistical Models Natural Language Ph.D. thesis, University of Pennsylvania. G. Druck, G. Mann, and A. McCallum. 2008. Learning from labeled features using generalized expectacriteria. In of ACM Special Interest on Information Retrieval, A. Haghighi and D. Klein. 2006. Prototype-driven learnfor sequence models. In of the Human Language Technology Conference of the NAACL, pages 320–327, New York City, USA, June. Association for Computational Linguistics. John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic modfor segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base phrase chunking. In of the 38th Annual Meeting of the Association for Computational pages 117–125, Hong Kong. B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization on minimum cuts. In of pages 271– 278. B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning In of pages 79–86. Hema Raghavan and James Allan. 2007. An interactive algorithm for asking and incorporating feature feedinto support vector machines. In of Hema Raghavan, Omid Madani, and Rosie Jones. 2006. learning on both features and instances. Jourof Machine Learning 7:1655–1686, Aug.</abstract>
<note confidence="0.8769477">Luis von Ahn, Ruoran Liu, and Manuel Blum. 2006. A game for locating objects. In ’06: Proceedings of the SIGCHI Conference on Hu- Factors in Computing pages 55–64. Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine for text categorization. In HLT 2007; of the Main pages 260–267, April. 40</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Grace Ngai</author>
</authors>
<title>Man [and woman] vs. machine: A case study in base noun phrase learning.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th ACL Conference.</booktitle>
<contexts>
<context position="2365" citStr="Brill and Ngai, 1999" startWordPosition="362" endWordPosition="365">se it to help train a machine classifier. How to do this, however, is still being explored. 1.1 Hand-crafted rules An obvious option is to have the annotators directly express their knowledge by hand-crafting rules. This &apos;This work was supported by National Science Foundation grant No. 0347822 and the JHU WSE/APL Partnership Fund. Special thanks to Christine Piatko for many useful discussions. 31 approach remains “data-driven” if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples. This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators. One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. 1.2 Feature selection by humans More recent work has focused on statistical classifiers. Training such classifiers faces the “credit assignment problem.” Given a training example x with many features, which features are responsible for its annotated class y? It may take many training examples to distinguish useful vs. irrelevant features.1 To reduce the number of training examples needed, one can ask annotators to examine</context>
</contexts>
<marker>Brill, Ngai, 1999</marker>
<rawString>Eric Brill and Grace Ngai. 1999. Man [and woman] vs. machine: A case study in base noun phrase learning. In Proceedings of the 37th ACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="27918" citStr="Collins, 1999" startWordPosition="4731" endWordPosition="4732">OP, S, SBAR, FRAG, PRN, NP, VP, PP, ADJP, QP). 6 Training: Joint Optimization of 0 and 0 To train our model, we use L-BFGS to locally maximize the log of the objective function (1):15 13These are the function words with count &gt; 40 in a random sample of 100 documents, and which were associated with the O-I tag transition at more than twice the average rate. We do not use any other lexical 0-features that reference x, for fear that they would enable the learner to explain the rationales without changing 0 as desired (see the end of section 5.3). 14We parse each sentence with the Collins parser (Collins, 1999). Then the document has one big parse tree, whose root is DOC, with each sentence being a child of DOC. 15One might expect this function to be convex because pe and po are both log-linear models with no hidden variables. However, log po(ri |xi, yi, 0) is not necessarily convex in 0. log pθ(yi |xi) 212kθk2 θ log pφ(ri |xi, yi, θ)) − 212kφk2 (4) φ This defines ppoor from (1) to be a standard diagonal Gaussian prior, with variances σ2θ and σ2φ for the two sets of parameters. We optimize σ2 θ in our experiments. As for σ2φ, different values did not affect the results, since we have a large number </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM Special Interest Group on Information Retrieval,</booktitle>
<location>(SIGIR).</location>
<contexts>
<context position="3391" citStr="Druck et al. (2008)" startWordPosition="528" endWordPosition="531">for its annotated class y? It may take many training examples to distinguish useful vs. irrelevant features.1 To reduce the number of training examples needed, one can ask annotators to examine or propose some candidate features. This is possible even for the very large feature sets that are typically used in NLP. In document classification, Raghavan et al. (2006) show that feature selection by an oracle could be helpful, and that humans are both rapid and reasonably good at distinguishing highly useful n-gram features from randomly chosen ones, even when viewing these n-grams out of context. Druck et al. (2008) show annotators some features f from a fixed feature set, and ask them to choose a class label y such that p(y |f) is as high as possible. Haghighi and Klein (2006) do the reverse: for each class label y, they ask the annotators to propose a few “prototypical” features f such that p(y |f) is as high as possible. 1.3 Feature selection in context The above methods consider features out of context. An annotator might have an easier time examining 1Most NLP systems use thousands or millions of features, because it is helpful to include lexical features over a large vocabulary, often conjoined wit</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>G. Druck, G. Mann, and A. McCallum. 2008. Learning from labeled features using generalized expectation criteria. In Proceedings of ACM Special Interest Group on Information Retrieval, (SIGIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="3556" citStr="Haghighi and Klein (2006)" startWordPosition="560" endWordPosition="563"> one can ask annotators to examine or propose some candidate features. This is possible even for the very large feature sets that are typically used in NLP. In document classification, Raghavan et al. (2006) show that feature selection by an oracle could be helpful, and that humans are both rapid and reasonably good at distinguishing highly useful n-gram features from randomly chosen ones, even when viewing these n-grams out of context. Druck et al. (2008) show annotators some features f from a fixed feature set, and ask them to choose a class label y such that p(y |f) is as high as possible. Haghighi and Klein (2006) do the reverse: for each class label y, they ask the annotators to propose a few “prototypical” features f such that p(y |f) is as high as possible. 1.3 Feature selection in context The above methods consider features out of context. An annotator might have an easier time examining 1Most NLP systems use thousands or millions of features, because it is helpful to include lexical features over a large vocabulary, often conjoined with lexical or non-lexical context. Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 31–40, Honolulu, October 2008. c�2008</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 320–327, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="19368" citStr="Lafferty et al., 2001" startWordPosition="3231" endWordPosition="3234">still allow us to evaluate our method on a range of training set sizes. 5 Detailed Models 5.1 Modeling class annotations with pθ We define the basic classifier pθ in equation (1) to be a standard conditional log-linear model: We encode its rationales as a corresponding tag sequence r~ = r1, ..., rM, as illustrated in Figure 1. Here rm E {I, O1 according to whether the token xm is in a rationale (i.e., xm was at least partly highlighted) or outside all rationales. x1 and xM are special boundary symbols, tagged with O. We predict the full tag sequence r~at once using a conditional random field (Lafferty et al., 2001). A CRF is just another conditional log-linear model: u(r, x, y, ~θ) pφ(r|x,y, def exp ~θ) = def � ~θ) ~θ) Zφ(x, y, def pθ(y |x) def = exp(~θ · ~f(x, y)) = u(x, y) Zθ(x) (2) Zθ(x) ~φ · ~g(r,x,y, ~θ)) Zφ(x, y, where ~f(·) extracts a feature vector from a classified ~ document, θ are the corresponding weights of those features, and Zθ(x) def � Ey u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset (Pang et al., 2002; Pang and Lee, 2004; Zaidan et al., 2007). Specifically, let V = {v1, ..., v177441 be the set of word types with count &gt; 4 in the full</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>117--125</pages>
<location>Hong Kong.</location>
<contexts>
<context position="2468" citStr="Ngai and Yarowsky, 2000" startWordPosition="378" endWordPosition="381">-crafted rules An obvious option is to have the annotators directly express their knowledge by hand-crafting rules. This &apos;This work was supported by National Science Foundation grant No. 0347822 and the JHU WSE/APL Partnership Fund. Special thanks to Christine Piatko for many useful discussions. 31 approach remains “data-driven” if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples. This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators. One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. 1.2 Feature selection by humans More recent work has focused on statistical classifiers. Training such classifiers faces the “credit assignment problem.” Given a training example x with many features, which features are responsible for its annotated class y? It may take many training examples to distinguish useful vs. irrelevant features.1 To reduce the number of training examples needed, one can ask annotators to examine or propose some candidate features. This is possible even for the very large feature sets that are typ</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 117–125, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="723" citStr="Pang and Lee, 2004" startWordPosition="104" endWordPosition="107">on Eisner Dept. of Computer Science, Johns Hopkins University Baltimore, MD 21218, USA {ozaidan,jason}@cs.jhu.edu Abstract A human annotator can provide hints to a machine learner by highlighting contextual “rationales” for each of his or her annotations (Zaidan et al., 2007). How can one exploit this side information to better learn the desired parameters 0? We present a generative model of how a given annotator, knowing the true 0, stochastically chooses rationales. Thus, observing the rationales helps us infer the true 0. We collect substring rationales for a sentiment classification task (Pang and Lee, 2004) and use them to obtain significant accuracy improvements for each annotator. Our new generative approach exploits the rationales more effectively than our previous “masking SVM” approach. It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks. 1 Background Many recent papers aim to reduce the amount of annotated data needed to train the parameters of a statistical model. Well-known paradigms include active learning, semi-supervised learning, and either domain adaptation or cross-lingual transfer from existing annotated</context>
<context position="16838" citStr="Pang and Lee (2004)" startWordPosition="2798" endWordPosition="2801">cribes (e.g.) how to derive a binomial parameter nonlinearly from Oh. This approach would not how often h was marked and infer how relevant is feature h (i.e., infer Bh). In this case, po is a simple channel that transforms relevant features into direct indicators of the feature. Our side information merely requires a more complex transformation—from relevant features into wellformed rationales, modulated by documents. 4 Experimental Data: Movie Reviews In Zaidan et al. (2007), we introduced the “Movie Review Polarity Dataset Enriched with Annotator Rationales.”8 It is based on the dataset of Pang and Lee (2004),9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F0–F9). All our experiments use F9 as their final blind test set. The enriched dataset adds rationale annotations produced by an annotator A0, who annotated folds F0–F8 of the movie review set with rationales (in the form of textual substrings) that supported the goldstandard classifications. We will use A0’s data to determine the improvement of our method over a (log-linear) baseline model without rationales. We also use A0 to compare against the “masking SVM” method and SVM baseline of Za</context>
<context position="19853" citStr="Pang and Lee, 2004" startWordPosition="3328" endWordPosition="3331"> boundary symbols, tagged with O. We predict the full tag sequence r~at once using a conditional random field (Lafferty et al., 2001). A CRF is just another conditional log-linear model: u(r, x, y, ~θ) pφ(r|x,y, def exp ~θ) = def � ~θ) ~θ) Zφ(x, y, def pθ(y |x) def = exp(~θ · ~f(x, y)) = u(x, y) Zθ(x) (2) Zθ(x) ~φ · ~g(r,x,y, ~θ)) Zφ(x, y, where ~f(·) extracts a feature vector from a classified ~ document, θ are the corresponding weights of those features, and Zθ(x) def � Ey u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset (Pang et al., 2002; Pang and Lee, 2004; Zaidan et al., 2007). Specifically, let V = {v1, ..., v177441 be the set of word types with count &gt; 4 in the full 2000-document corpus. Define fh(x, y) to be y if vh appears at least once in x, and 0 otherwise. Thus θ E 817744, and positive weights in θ favor class label y = +1 and equally discourage y = -1, while negative weights do the opposite. This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales. Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2. 5</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proc. of ACL, pages 271– 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="19833" citStr="Pang et al., 2002" startWordPosition="3324" endWordPosition="3327"> and xM are special boundary symbols, tagged with O. We predict the full tag sequence r~at once using a conditional random field (Lafferty et al., 2001). A CRF is just another conditional log-linear model: u(r, x, y, ~θ) pφ(r|x,y, def exp ~θ) = def � ~θ) ~θ) Zφ(x, y, def pθ(y |x) def = exp(~θ · ~f(x, y)) = u(x, y) Zθ(x) (2) Zθ(x) ~φ · ~g(r,x,y, ~θ)) Zφ(x, y, where ~f(·) extracts a feature vector from a classified ~ document, θ are the corresponding weights of those features, and Zθ(x) def � Ey u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset (Pang et al., 2002; Pang and Lee, 2004; Zaidan et al., 2007). Specifically, let V = {v1, ..., v177441 be the set of word types with count &gt; 4 in the full 2000-document corpus. Define fh(x, y) to be y if vh appears at least once in x, and 0 otherwise. Thus θ E 817744, and positive weights in θ favor class label y = +1 and equally discourage y = -1, while negative weights do the opposite. This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales. Future work should consider more complex features and how they are signaled by rationales, as discuss</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proc. of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hema Raghavan</author>
<author>James Allan</author>
</authors>
<title>An interactive algorithm for asking and incorporating feature feedback into support vector machines.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<marker>Raghavan, Allan, 2007</marker>
<rawString>Hema Raghavan and James Allan. 2007. An interactive algorithm for asking and incorporating feature feedback into support vector machines. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hema Raghavan</author>
<author>Omid Madani</author>
<author>Rosie Jones</author>
</authors>
<title>Active learning on both features and instances.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--1655</pages>
<contexts>
<context position="3138" citStr="Raghavan et al. (2006)" startWordPosition="486" endWordPosition="489">e than active learning. 1.2 Feature selection by humans More recent work has focused on statistical classifiers. Training such classifiers faces the “credit assignment problem.” Given a training example x with many features, which features are responsible for its annotated class y? It may take many training examples to distinguish useful vs. irrelevant features.1 To reduce the number of training examples needed, one can ask annotators to examine or propose some candidate features. This is possible even for the very large feature sets that are typically used in NLP. In document classification, Raghavan et al. (2006) show that feature selection by an oracle could be helpful, and that humans are both rapid and reasonably good at distinguishing highly useful n-gram features from randomly chosen ones, even when viewing these n-grams out of context. Druck et al. (2008) show annotators some features f from a fixed feature set, and ask them to choose a class label y such that p(y |f) is as high as possible. Haghighi and Klein (2006) do the reverse: for each class label y, they ask the annotators to propose a few “prototypical” features f such that p(y |f) is as high as possible. 1.3 Feature selection in context</context>
<context position="4415" citStr="Raghavan et al. (2006)" startWordPosition="695" endWordPosition="698"> annotator might have an easier time examining 1Most NLP systems use thousands or millions of features, because it is helpful to include lexical features over a large vocabulary, often conjoined with lexical or non-lexical context. Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 31–40, Honolulu, October 2008. c�2008 Association for Computational Linguistics features in context to recognize whether they appear relevant. This is particularly true for features that are only modestly or only sometimes helpful, which may be abundant in NLP tasks. Thus, Raghavan et al. (2006) propose an active learning method in which, while classifying a training document, the annotator also identifies some features of that document as particularly relevant. E.g., the annotator might highlight particular unigrams as he or she reads the document. In their proposal, a feature that is highlighted in any document is assumed to be globally more relevant. Its dimension in feature space is scaled by a factor of 10 so that this feature has more influence on distances or inner products, and hence on the learned classifier. 1.4 Concerns about marking features Despite the success of the abo</context>
<context position="15020" citStr="Raghavan et al. (2006)" startWordPosition="2487" endWordPosition="2490">a single long rationale rather than many short ones. Similarly, the language model can help explain why some words, though important, might not have been included in any rationale of r. If there are multiple annotators, one can learn different 0 parameters for each annotator, reflecting their different annotation styles.7 We found this to be useful (section 8.2). We remark that our generative modeling approach (equation (1)) would also apply if r were not rationale markup, but some other kind of so-called “side information,” such as the feature annotations discussed in section 1. For example, Raghavan et al. (2006) assume that if feature h is relevant—a bi6Our current experiments do not model this last point. However, we imagine that if the document only has a few 0-features that support the classification, the annotator will probably mark most of them, whereas if such features are abundant, the annotator may lazily mark only a few of the strongest ones. A simple approach would equip 0 with a different “bias” or “threshold” parameter 0. for each rationale training document x, to modulate the a priori probability of marking a rationale in x. By fitting this bias parameter, we deduce how lazy the annotato</context>
</contexts>
<marker>Raghavan, Madani, Jones, 2006</marker>
<rawString>Hema Raghavan, Omid Madani, and Rosie Jones. 2006. Active learning on both features and instances. Journal of Machine Learning Research, 7:1655–1686, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Ruoran Liu</author>
<author>Manuel Blum</author>
</authors>
<title>Peekaboom: A game for locating objects.</title>
<date>2006</date>
<booktitle>In CHI ’06: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>55--64</pages>
<marker>von Ahn, Liu, Blum, 2006</marker>
<rawString>Luis von Ahn, Ruoran Liu, and Manuel Blum. 2006. Peekaboom: A game for locating objects. In CHI ’06: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 55–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Using “annotator rationales” to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In NAACL HLT 2007; Proceedings of the Main Conference,</booktitle>
<pages>260--267</pages>
<contexts>
<context position="7147" citStr="Zaidan et al., 2007" startWordPosition="1152" endWordPosition="1155"> features. and features to be used. The project’s cost is justified by saying that the annotations will be reused by many researchers (perhaps in a “shared task”), who are free to compete on how they tackle the learning problem. Unfortunately, feature annotation commits to a particular feature set at annotation time. Subsequent research cannot easily adjust the definition of the features, or obtain annotation of new features. 2 Annotating Rationales To solve these problems, we propose that annotators should not select features but rather mark relevant portions of the example. In earlier work (Zaidan et al., 2007), we called these markings “rationales.” For example, when classifying a movie review as positive or negative, the annotator would also highlight phrases that supported that judgment. Figure 1 shows two such rationales. A multi-annotator timing study (Zaidan et al., 2007) found that highlighting rationale phrases while reading movie reviews only doubled annotation time, although annotators marked 5–11 rationale substrings in addition to the simple binary class. The benefit justified the extra time. Furthermore, much of the benefit could have been obtained by giving rationales for only a fracti</context>
<context position="16700" citStr="Zaidan et al. (2007)" startWordPosition="2776" endWordPosition="2779">ive to observe that h was selected in 3 of the 10 documents where it appeared, and to predict this via a model po(3 of 10 |Oh), where 0 describes (e.g.) how to derive a binomial parameter nonlinearly from Oh. This approach would not how often h was marked and infer how relevant is feature h (i.e., infer Bh). In this case, po is a simple channel that transforms relevant features into direct indicators of the feature. Our side information merely requires a more complex transformation—from relevant features into wellformed rationales, modulated by documents. 4 Experimental Data: Movie Reviews In Zaidan et al. (2007), we introduced the “Movie Review Polarity Dataset Enriched with Annotator Rationales.”8 It is based on the dataset of Pang and Lee (2004),9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F0–F9). All our experiments use F9 as their final blind test set. The enriched dataset adds rationale annotations produced by an annotator A0, who annotated folds F0–F8 of the movie review set with rationales (in the form of textual substrings) that supported the goldstandard classifications. We will use A0’s data to determine the improvement of our metho</context>
<context position="18259" citStr="Zaidan et al., 2007" startWordPosition="3031" endWordPosition="3034"> and 50 negative) and collected both class and rationale annotation data from each of six new annotators A3–A8,10 following the same procedures as (Zaidan et al., 2007). We report results using only data from A3–A5, since we used the data from A6–A8 as development data in the early stages of our work. We use this new rationale-enriched dataset8 to determine if our method works well across annotators. We will only be able to carry out that comparison 8Available at http://cs.jhu.edu/∼ozaidan/rationales. 9Polarity dataset version 2.0. 10We avoid annotator names A1–A2, which were already used in (Zaidan et al., 2007). 34 Figure 1: Rationales as sequence annotation: the annotator highlighted two textual segments as rationales for a positive class. Highlighted words in x~ are tagged I in ~r, and other words are tagged O. The figure also shows some φ-features. For instance, gO(,)-, is a count of O-I transitions that occur with a comma as the left word. Notice also that grel is the sum of the underlined values. at small training set sizes, due to limited data from A3–A8. The larger A0 dataset will still allow us to evaluate our method on a range of training set sizes. 5 Detailed Models 5.1 Modeling class anno</context>
<context position="19875" citStr="Zaidan et al., 2007" startWordPosition="3332" endWordPosition="3335">agged with O. We predict the full tag sequence r~at once using a conditional random field (Lafferty et al., 2001). A CRF is just another conditional log-linear model: u(r, x, y, ~θ) pφ(r|x,y, def exp ~θ) = def � ~θ) ~θ) Zφ(x, y, def pθ(y |x) def = exp(~θ · ~f(x, y)) = u(x, y) Zθ(x) (2) Zθ(x) ~φ · ~g(r,x,y, ~θ)) Zφ(x, y, where ~f(·) extracts a feature vector from a classified ~ document, θ are the corresponding weights of those features, and Zθ(x) def � Ey u(x, y) is a normalizer. We use the same set of binary features as in previous work on this dataset (Pang et al., 2002; Pang and Lee, 2004; Zaidan et al., 2007). Specifically, let V = {v1, ..., v177441 be the set of word types with count &gt; 4 in the full 2000-document corpus. Define fh(x, y) to be y if vh appears at least once in x, and 0 otherwise. Thus θ E 817744, and positive weights in θ favor class label y = +1 and equally discourage y = -1, while negative weights do the opposite. This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales. Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2. 5.2 Modeling rationale </context>
<context position="30812" citStr="Zaidan et al., 2007" startWordPosition="5249" endWordPosition="5252">ntirel. 16C also balances our confidence in the classifications y against our confidence in the rationales r; either may be noisy. n +C( i=1 37 7 Experimental Procedures We report on two sets of experiments. In the first set, we use the annotation data that A3–A5 provided for the small set of 100 documents (as well as the data from A0 on those same 100 documents). In the second set, we used A0’s abundant annotation data to evaluate our method with training set sizes up to 1600 documents, and compare it with three other methods: log-linear baseline, SVM baseline, and the SVM masking method of (Zaidan et al., 2007). 7.1 Learning curves The learning curves reported in section 8.1 are generated exactly as in (Zaidan et al., 2007). Each curve shows classification accuracy at training set sizes T = 1, 2,..., 9 folds (i.e. 200, 400,..., 1600 training documents). For a given size T, the reported accuracy is an average of 9 experiments with different subsets of the entire training set, each of size T: acc(F9 |Fi+1 U ... U Fi+T) (5) where Fj denotes the fold numbered j mod 9, and acc(F9 |Y ) means classification accuracy on the held-out test set F9 after training on set Y . We use an appropriate paired permutat</context>
<context position="32631" citStr="Zaidan et al., 2007" startWordPosition="5562" endWordPosition="5565">assify each contrast example with a smaller margin than the corresponding original example (and did not require it to be classified correctly). The masking SVM learner relies on a simple geometric principle; is trivial to implement on top of an existing SVM learner; and works well. However, we believe that the generative method we present here is more interesting and should apply more broadly. Figure 3: Classification accuracy curves for the 4 methods: the two baseline learners that only utilize class data, and the two learners that also utilize rationale annotations. The SVM curves are from (Zaidan et al., 2007). First, the masking method is specific to improving an SVM learner, whereas our method can be used to improve any classifier by adding a rationale-based regularizer (the second half of equation (4)) to its objective function during training. More important, there are tasks where it is unclear how to generate contrast examples. For the movie review task, it was natural to mask out a rationale by pretending its words never occurred in the document. After all, most word types do not appear in most documents, so it is natural to consider the nonpresence of a word as a “default” state to which we </context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In NAACL HLT 2007; Proceedings of the Main Conference, pages 260–267, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>