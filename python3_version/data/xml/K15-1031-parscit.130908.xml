<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.074427">
<title confidence="0.976785">
Deep Neural Language Models for Machine Translation
</title>
<author confidence="0.998382">
Minh-Thang Luong Michael Kayser Christopher D. Manning
</author>
<affiliation confidence="0.995892">
Computer Science Department, Stanford University, Stanford, CA, 94305
</affiliation>
<email confidence="0.988011">
{lmthang, mkayser, manning}@stanford.edu
</email>
<sectionHeader confidence="0.993835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951291666667">
Neural language models (NLMs) have
been able to improve machine translation
(MT) thanks to their ability to generalize
well to long contexts. Despite recent suc-
cesses of deep neural networks in speech
and vision, the general practice in MT
is to incorporate NLMs with only one or
two hidden layers and there have not been
clear results on whether having more lay-
ers helps. In this paper, we demonstrate
that deep NLMs with three or four lay-
ers outperform those with fewer layers in
terms of both the perplexity and the trans-
lation quality. We combine various tech-
niques to successfully train deep NLMs
that jointly condition on both the source
and target contexts. When reranking n-
best lists of a strong web-forum baseline,
our deep models yield an average boost
of 0.5 TER / 0.5 BLEU points compared
to using a shallow NLM. Additionally, we
adapt our models to a new sms-chat do-
main and obtain a similar gain of 1.0 TER
/ 0.5 BLEU points.1
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990325">
Deep neural networks (DNNs) have been success-
ful in learning more complex functions than shal-
low ones (Bengio, 2009) and exceled in many
challenging tasks such as in speech (Hinton et al.,
2012) and vision (Krizhevsky et al., 2012). These
results have sparked interest in applying DNNs
to natural language processing problems as well.
Specifically, in machine translation (MT), there
</bodyText>
<footnote confidence="0.9061295">
1Our code and related materials are publicly available at
http://stanford.edu/˜lmthang/nlm.
</footnote>
<bodyText confidence="0.999968103448276">
has been an active body of work recently in uti-
lizing neural language models (NLMs) to improve
translation quality. However, to the best of our
knowledge, work in this direction only makes use
of NLMs with either one or two hidden layers. For
example, Schwenk (2010, 2012) and Son et al.
(2012) used shallow NLMs with a single hidden
layer for reranking. Vaswani et al. (2013) consid-
ered two-layer NLMs for decoding but provided
no comparison among models of various depths.
Devlin et al. (2014) reported only a small gain
when decoding with a two-layer NLM over a sin-
gle layer one. There have not been clear results on
whether adding more layers to NLMs helps.
In this paper, we demonstrate that deep NLMs
with three or four layers are better than those
with fewer layers in terms of the perplexity and
the translation quality. We detail how we com-
bine various techniques from past work to suc-
cessfully train deep NLMs that condition on both
the source and target contexts. When reranking n-
best lists of a strong web-forum MT baseline, our
deep models achieve an additional improvement
of 0.5 TER / 0.5 BLEU compared to using a shal-
low NLM. Furthermore, by fine-tuning general in-
domain NLMs with out-of-domain data, we obtain
a similar boost of 1.0 TER / 0.5 BLEU points over
a strong domain-adapted sms-chat baseline com-
pared to utilizing a shallow NLM.
</bodyText>
<sectionHeader confidence="0.983229" genericHeader="method">
2 Neural Language Models
</sectionHeader>
<bodyText confidence="0.996671428571428">
We briefly describe the NLM architecture and
training objective used in this work as well as com-
pare our approach to other related work.
Architecture. Neural language models are fun-
damentally feed-forward networks as described in
(Bengio et al., 2003), but not necessarily lim-
ited to only a single hidden layer. Like any
</bodyText>
<page confidence="0.990179">
305
</page>
<note confidence="0.612018">
Proceedings of the 19th Conference on Computational Language Learning, pages 305–309,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.995244142857143">
other language model, NLMs specify a distribu-
tion, p(w|c), to predict the next word w given a
context c. The first step is to lookup embeddings
for words in the context and concatenate them to
form an input, h(0), to the first hidden layer. We
then repeatedly build up hidden representations as
follows, for l = 1, ... , n:
</bodyText>
<equation confidence="0.999946">
(W(l)h(l−1) + b(l))
h(l) = f (1)
</equation>
<bodyText confidence="0.995733333333333">
where f is a non-linear fuction such as tanh. The
predictive distribution, p(w|c), is then derived us-
ing the standard softmax:
</bodyText>
<equation confidence="0.9996745">
s = W(sm)h(n) + b(sm)
exp(sw) (2)
p(w|c) =
,wEV exp(sw)
</equation>
<bodyText confidence="0.999096095238095">
Objective. The typical way of training NLMs is
to maximize the training data likelihood, or equiv-
alently, to minimize the cross-entropy objective of
the following form: E(c,w)ET − log p(w|c).
Training NLMs can be prohibitively slow due
to the computationally expensive softmax layer.
As a result, past works have tried to use a more
efficient version of the softmax such as the hi-
erarchical softmax (Morin, 2005; Mnih and Hin-
ton, 2007; Mnih and Hinton, 2009) or the class-
based one (Mikolov et al., 2010; Mikolov et al.,
2011). Recently, the noise-contrastive estimation
(NCE) technique (Gutmann and Hyv¨arinen, 2012)
has been applied to train NLMs in (Mnih and Teh,
2012; Vaswani et al., 2013) to avoid explicitly
computing the normalization factors.
Devlin et al. (2014) used a modified version
of the cross-entropy objective, the self-normalized
one. The idea is to not only improve the predic-
tion, p(w|c), but also to push the normalization
factor per context, Zc, close to 1:
</bodyText>
<equation confidence="0.999348">
J = � − log p(w|c) + α log2(Zc) (3)
(c,w)ET
</equation>
<bodyText confidence="0.99993196875">
While self-normalization does not lead to speed up
in training, it allows trained models to be applied
efficiently at test time without computing the nor-
malization factors. This is similar in flavor to NCE
but allows for flexibility (through α) in how hard
we want to “squeeze” the normalization factors.
Training deep NLMs. We follow (Devlin et al.,
2014) to train self-normalized NLMs, condition-
ing on both the source and target contexts. Unlike
(Devlin et al., 2014), we found that using the recti-
fied linear function, max{0, x}, proposed in (Nair
and Hinton, 2010), works better than tanh. The
rectified linear function was used in (Vaswani et
al., 2013) as well. Furthermore, while these works
use a fixed learning rate throughout, we found that
having a simple learning rate schedule is useful
in training well-performing deep NLMs. This has
also been demonstrated in (Sutskever et al., 2014;
Luong et al., 2015) and is detailed in Section 3.
We do not perform any gradient clipping and no-
tice that learning is more stable when short sen-
tences of length less than or equal to 2 are re-
moved. Bias terms are used for all hidden layers
as well as the softmax layer as described earlier,
which is slightly different from other work such as
(Vaswani et al., 2013). All these details contribute
to our success in training deep NLMs.
For simplicity, the same vocabulary is used for
both the embedding and the softmax matrices.2 In
addition, we adopt the standard softmax to take ad-
vantage of GPUs in performing large matrix mul-
tiplications. All hyperparameters are given later.
</bodyText>
<sectionHeader confidence="0.99888" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.991703">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999961666666667">
We use the Chinese-English bitext in the DARPA
BOLT (Broad Operational Language Translation)
program, with 11.1M parallel sentences (281M
Chinese words and 307M English words). We re-
serve 585 sentences for validation, i.e., choosing
hyperparameters, and 1124 sentences for testing.3
</bodyText>
<subsectionHeader confidence="0.998786">
3.2 NLM Training
</subsectionHeader>
<bodyText confidence="0.999902916666667">
We train our NLMs described in Section 2 with
SGD, using: (a) a source window of size 5, i.e.,
11-gram source context4, (b) a 4-word target his-
tory, i.e., 5-gram target LM, (c) a self-normalized
weight α = 0.1, (d) a mini-batch of size 128, and
(e) a learning rate of 0.1 (training costs are nor-
malized by the mini-batch size). All weights are
uniformly initialized in [−0.01,0.01]. We train
our models for 4 epochs (after 2 epochs, the learn-
ing rate is halved every 0.5 epoch). The vocab-
ularies are limited to the top 40K frequent words
for both Chinese and English. All words not in
</bodyText>
<footnote confidence="0.9909682">
2Some work (Schwenk, 2010; Schwenk et al., 2012) uti-
lize a smaller softmax vocabulary, called short-list.
3The test set is from BOLT and labeled as p1r6 dev.
4We used an alignment heuristic similar to Devlin et al.
(2014) but applicable to our phrase-based MT system.
</footnote>
<page confidence="0.993981">
306
</page>
<table confidence="0.9994886">
Models Valid Test  |log Z|
1 layer 9.39 8.99 0.51
2 layers 9.20 8.96 0.50
3 layers 8.64 8.13 0.43
4 layers 8.10 7.71 0.35
</table>
<tableCaption confidence="0.99914">
Table 1: Training NLMs – validation and test
</tableCaption>
<bodyText confidence="0.994332857142857">
perplexities achieved by self-normalized NLMs of
various depths. We report the  |log Z |value (base
e), similar to Devlin et al. (2014), to indicate how
good each model is in pushing the log normaliza-
tion factors towards 0. All perplexities are derived
by explicitly computing the normalization factors.
these vocabularies are replaced by a universal un-
known symbol. Embeddings are of size 256 and
all hidden layers have 512 units each. Our train-
ing speed on a single Tesla K40 GPU device is
about 1000 target words per second and it gener-
ally takes about 10-14 days to fully train a model.
We present the NLM training results in Table 1.
With more layers, the model succeeds in learning
more complex functions; the prediction, hence,
becomes more accurate as evidenced by smaller
perplexities for both the validation and test sets.
Interestingly, we observe that deeper nets can learn
self-normalized NLMs better: the mean log nor-
malization factor,  |log Z |in Eq. (3), is driven to-
wards 0 as the depth increases.5
</bodyText>
<subsectionHeader confidence="0.987574">
3.3 MT Reranking with NLMs
</subsectionHeader>
<bodyText confidence="0.999940153846154">
Our MT models are built using the Phrasal MT
toolkit (Cer et al., 2010). In addition to the stan-
dard dense feature set6, we include a variety of
sparse features for rules, word pairs, and word
classes, as described in (Green et al., 2014). Our
decoder uses three language models.7 We use a
tuning set of 396K words in the newswire and web
domains and tune our systems using online ex-
pected error rate training as in (Green et al., 2014).
Our tuning metric is (BLEU-TER)/2.
We run a discriminative reranker on the 1000-
best output of a decoder with MERT. The features
used in reranking include all the dense features,
</bodyText>
<footnote confidence="0.9904188">
5As a reference point, though not directly comparable,
Devlin et al. (2014) achieved 0.68 for I log ZI on a different
test set with the same self-normalized constant α=0.1.
6Consisting of forward and backward translation mod-
els, lexical weighting, linear distortion, word penalty, phrase
penalty and language model.
7One is trained on the English side of the bitext, one
is trained on a 16.3-billion-word monolingual corpus taken
from various domains, and one is a class-based language
model trained on the same large monolingual corpus.
</footnote>
<table confidence="0.9997817">
System dev test1 test2
Tj BT Tj BT Tj BT
baseline 53.7 33.1 55.1 31.3 63.5 16.5
Reranking
1 layer 52.9 34.3 54.5 32.0 63.1 16.7
2layers 52.7 34.1 54.3 31.9 63.0 16.8
3layers 52.5 34.5 54.3 32.3 62.5 17.3
4layers 52.6 34.7 54.1 32.4 62.7 17.2
vs. baseline +1.2† +1.6† +1.0† +1.1† +1.0† +0.8†
vs. 1 layer +0.4† +0.4† +0.4† +0.4† +0.6† +0.6†
</table>
<tableCaption confidence="0.977367">
Table 2: Web-forum Results – TER (T)
</tableCaption>
<bodyText confidence="0.9831985">
and BLEU (B) scores on both the dev set
(dev10wb dev), used to tune reranking weights,
and the test sets (dev10wb syscomtune and
p1r6 dev accordingly). Relative improvements
between the best system and the baseline as well
as the 1-layer model are bolded. † marks improve-
ments that are statistically significant (p&lt;0.05).
an aggregate decoder score, and an NLM score.
We learn the reranker weights on a second tuning
set, different from the decoder tuning set, to make
the reranker less biased towards the dense features.
This second tuning set consists of 33K words of
web-forum text and is important to obtain good
improvements with reranking.
</bodyText>
<sectionHeader confidence="0.803962" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.999975631578948">
As shown in Table 2, it is not obvious if the depth-
2 model is better than the single layer one, both
of which are what past work used. In contrast,
reranking with deep NLMs of three or four lay-
ers are clearly better, yielding average improve-
ments of 1.0 TER / 1.0 BLEU points over the base-
line and 0.5 TER / 0.5 BLEU points over the sys-
tem reranked with the 1-layer model, all of which
are statisfically significant according to the test de-
scribed in (Riezler and Maxwell, 2005).
The fact that the improvements in terms of the
intrinsic metrics listed in Table 1 do translate into
gains in translation quality is interesting. It rein-
forces the trend reported in (Luong et al., 2015)
that better source-conditioned perplexities lead to
better translation scores. This phenomon is a use-
ful result as in the past, many intrinsic metrics,
e.g., alignment error rate, do not necessarily cor-
relate with MT quality metrics.
</bodyText>
<subsectionHeader confidence="0.899588">
3.5 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.94318">
For the sms-chat domain, we use a tune set of
260K words in the newswire, web, and sms-chat
domains to tune the decoder weights and a sepa-
</bodyText>
<page confidence="0.995901">
307
</page>
<table confidence="0.999352222222222">
System dev test
Tj BT Tj BT
baseline 62.2 18.7 57.3 23.3
Reranking
1 layer (5.42, 0.51) 60.1 22.0 56.2 25.9
2layers (5.50, 0.51) 60.7 21.5 56.3 26.0
3 layers (5.34 0.43) 59.9 21.4 55.2 26.4
vs. baseline +2.3t +3.3t +2.1t +3.1t
vs. 1 layer +0.2 -0.6 +1.0t +0.5
</table>
<tableCaption confidence="0.825123285714286">
Table 3: Domain-adaptation Results – transla-
tion scores for the sms-chat domain similar to
Table 2. We use p2r2smscht dev for dev and
p2r2smscht syscomtune for test. The test perplex-
ities and the  |log Z |values of our domain-adapted
NLMs are shown in italics. $ marks improvements
that are statistically significant (p&lt;0.01).
</tableCaption>
<bodyText confidence="0.9904895">
rate small, 8K words set to tune reranking weights.
To train adapted NLMs, we use models previously
trained on general in-domain data and further fine-
tune with out-domain data for about 4 hours.8
Similar to the web-forum domain, for sms-chat,
Table 3 shows that on the test set, our deep NLM
with three layers yields a significant gain of 2.1
TER / 3.1 BLEU points over the baseline and 1.0
TER / 0.5 BLEU points over the 1-layer reranked
system. It is worth pointing out that for such a
small amount of out-domain training data, depth
becomes less effective as exhibited through the in-
significant BLEU gain in test and a drop in dev
when comparing between the 1- and 3-layer mod-
els. We exclude the 4-layer NLM as it seems to
have overfitted the training data. Nevertheless, we
still achieve decent gains in using NLMs for MT
domain adaptation.
</bodyText>
<sectionHeader confidence="0.996833" genericHeader="method">
4 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999562">
4.1 NLM Training
</subsectionHeader>
<bodyText confidence="0.999952888888889">
We show in Figure 1 the learning curves for vari-
ous NLMs, demonstrating that deep nets are better
than the shallow NLM with a single hidden layer.
Starting from minibatch 20K, the ranking is gen-
erally maintained that deeper NLMs have better
cross-entropies. The gaps become less discernible
from minibatch 30K onwards, but numerically, as
the model becomes deeper, the average gaps, in
perplexities, are consistently 40.1, 1.1, and 2.0.
</bodyText>
<footnote confidence="0.8935165">
8Our sms-chat corpus consists of 146K sentences (1.6M
Chinese and 1.9M English words). We randomly select 3000
sentences for validation and 3000 sentences for test. Models
are trained for 8 iterations with the same hyperparameters.
</footnote>
<figureCaption confidence="0.9917205">
Figure 1: NLM Learning Curve – test cross-
entropies (loge perplexities) for various NLMs.
</figureCaption>
<subsectionHeader confidence="0.999454">
4.2 Reranking Settings
</subsectionHeader>
<bodyText confidence="0.999301285714286">
In Table 4, we compare reranking using all dense
features (All) to conditions which use only dense
LM features (LM) and optionally, include a word
penalty (WP) feature. All these settings include
an NLM score and an aggregate decoder score. As
shown, it is best to include all dense features at
reranking time.
</bodyText>
<table confidence="0.9992908">
All LMs + WP LMs
1 layer 11.3 11.3 11.4
2 layers 11.2 11.4 11.5
3 layers 11.0 11.1 11.4
4 layers 10.9 11.2 11.3
</table>
<tableCaption confidence="0.9629515">
Table 4: Reranking Conditions – (TER-BLEU)/2
scores when reranking the web-forum baseline.
</tableCaption>
<sectionHeader confidence="0.999497" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999974909090909">
It is worth mentioning another active line of re-
search in building end-to-end neural MT systems
(Kalchbrenner and Blunsom, 2013; Sutskever et
al., 2014; Bahdanau et al., 2015; Luong et al.,
2015; Jean et al., 2015). These methods have
not yet demonstrated success on challenging lan-
guage pairs such as English-Chinese. Arsoy et al.
(2012) have preliminarily examined deep NLMs
for speech recognition, however, we believe, this
is the first work that puts deep NLMs into the con-
text of MT.
</bodyText>
<sectionHeader confidence="0.996711" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.997977">
In this paper, we have bridged the gap that past
work did not show, that is, neural language mod-
els with more than two layers can help improve
translation quality. Our results confirm the trend
reported in (Luong et al., 2015) that source-
conditioned perplexity strongly correlates with
MT performance. We have also demonstrated the
</bodyText>
<figure confidence="0.997890866666667">
Mini−batches
x 104
6.5
6
1 layer
2 layers
3 layers
4 layers
5.5
5
4.5
4
Cross−Entropy
3.5
31 2 3 4 5 6 7 8 9 10
</figure>
<page confidence="0.992385">
308
</page>
<bodyText confidence="0.993575">
use of deep NLMs to obtain decent gains in out-
of-domain conditions.
</bodyText>
<sectionHeader confidence="0.925894" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99992875">
We gratefully acknowledge support from a gift
from Bloomberg L.P. and from the Defense
Advanced Research Projects Agency (DARPA)
Broad Operational Language Translation (BOLT)
program under contract HR0011-12-C-0015
through IBM. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the author(s) and do not
necessarily reflect the view of DARPA, or the US
government. We thank members of the Stanford
NLP Group as well as the annonymous reviewers
for their valuable comments and feedbacks.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999300352941176">
Ebru Arsoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In NAACL WLM Workshop.
D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural
machine translation by jointly learning to align and
translate. In ICLR.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jan-
vin. 2003. A neural probabilistic language model.
JMLR, 3:1137–1155.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and Trends in Machine Learning,
2(1):1–127, January.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In ACL,
Demonstration Session.
J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz,
and J. Makhoul. 2014. Fast and robust neural net-
work joint models for statistical machine translation.
In ACL.
S. Green, D. Cer, and C. D. Manning. 2014. An em-
pirical comparison of features and tuning for phrase-
based machine translation. In WMT.
Michael Gutmann and Aapo Hyv¨arinen. 2012. Noise-
contrastive estimation of unnormalized statistical
models, with applications to natural image statistics.
JMLR, 13:307–361.
G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed,
N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath, and B. Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recogni-
tion. IEEE Signal Processing Magazine.
S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
ACL.
N. Kalchbrenner and P. Blunsom. 2013. Recurrent
continuous translation models. In EMNLP.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
ImageNet classification with deep convolutional
neural networks. In NIPS.
M.-T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and
W. Zaremba. 2015. Addressing the rare word prob-
lem in neural machine translation. In ACL.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In ICASSP.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In ICML.
Andriy Mnih and Geoffrey Hinton. 2009. A scalable
hierarchical distributed language model. In NIPS.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In ICML.
Frederic Morin. 2005. Hierarchical probabilistic neu-
ral network language model. In AISTATS.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In ICML.
Stefan Riezler and T. John Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In ACL Workshop, Intrinsic/Extrinsic
Evaluation Measures for MT and Summarization.
H. Schwenk, A. Rousseau, and M. Attik. 2012. Large,
pruned or continuous space language models on a
gpu for statistical machine translation. In NAACL
WLM workshop.
H. Schwenk. 2010. Continuous space language mod-
els for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, (93):137–146.
Le Hai Son, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL-HLT.
I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence
to sequence learning with neural networks. In NIPS.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
EMNLP.
</reference>
<page confidence="0.999282">
309
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.717739">
<title confidence="0.99943">Deep Neural Language Models for Machine Translation</title>
<author confidence="0.997788">Minh-Thang Luong Michael Kayser Christopher D</author>
<affiliation confidence="0.986062">Computer Science Department, Stanford University, Stanford, CA,</affiliation>
<email confidence="0.947655">mkayser,</email>
<abstract confidence="0.998360125">Neural language models (NLMs) have been able to improve machine translation (MT) thanks to their ability to generalize well to long contexts. Despite recent successes of deep neural networks in speech and vision, the general practice in MT is to incorporate NLMs with only one or two hidden layers and there have not been clear results on whether having more layers helps. In this paper, we demonstrate that deep NLMs with three or four layers outperform those with fewer layers in terms of both the perplexity and the translation quality. We combine various techniques to successfully train deep NLMs that jointly condition on both the source target contexts. When reranking best lists of a strong web-forum baseline, our deep models yield an average boost 0.5 0.5 compared to using a shallow NLM. Additionally, we adapt our models to a new sms-chat doand obtain a similar gain of 1.0</abstract>
<intro confidence="0.788119">0.5</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ebru Arsoy</author>
<author>Tara N Sainath</author>
<author>Brian Kingsbury</author>
<author>Bhuvana Ramabhadran</author>
</authors>
<title>Deep neural network language models.</title>
<date>2012</date>
<booktitle>In NAACL WLM Workshop.</booktitle>
<contexts>
<context position="15440" citStr="Arsoy et al. (2012)" startWordPosition="2617" endWordPosition="2620">n, it is best to include all dense features at reranking time. All LMs + WP LMs 1 layer 11.3 11.3 11.4 2 layers 11.2 11.4 11.5 3 layers 11.0 11.1 11.4 4 layers 10.9 11.2 11.3 Table 4: Reranking Conditions – (TER-BLEU)/2 scores when reranking the web-forum baseline. 5 Related Work It is worth mentioning another active line of research in building end-to-end neural MT systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015). These methods have not yet demonstrated success on challenging language pairs such as English-Chinese. Arsoy et al. (2012) have preliminarily examined deep NLMs for speech recognition, however, we believe, this is the first work that puts deep NLMs into the context of MT. 6 Conclusion In this paper, we have bridged the gap that past work did not show, that is, neural language models with more than two layers can help improve translation quality. Our results confirm the trend reported in (Luong et al., 2015) that sourceconditioned perplexity strongly correlates with MT performance. We have also demonstrated the Mini−batches x 104 6.5 6 1 layer 2 layers 3 layers 4 layers 5.5 5 4.5 4 Cross−Entropy 3.5 31 2 3 4 5 6 7</context>
</contexts>
<marker>Arsoy, Sainath, Kingsbury, Ramabhadran, 2012</marker>
<rawString>Ebru Arsoy, Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. 2012. Deep neural network language models. In NAACL WLM Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bahdanau</author>
<author>K Cho</author>
<author>Y Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2015</date>
<booktitle>In ICLR.</booktitle>
<contexts>
<context position="15276" citStr="Bahdanau et al., 2015" startWordPosition="2590" endWordPosition="2593">h use only dense LM features (LM) and optionally, include a word penalty (WP) feature. All these settings include an NLM score and an aggregate decoder score. As shown, it is best to include all dense features at reranking time. All LMs + WP LMs 1 layer 11.3 11.3 11.4 2 layers 11.2 11.4 11.5 3 layers 11.0 11.1 11.4 4 layers 10.9 11.2 11.3 Table 4: Reranking Conditions – (TER-BLEU)/2 scores when reranking the web-forum baseline. 5 Related Work It is worth mentioning another active line of research in building end-to-end neural MT systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015). These methods have not yet demonstrated success on challenging language pairs such as English-Chinese. Arsoy et al. (2012) have preliminarily examined deep NLMs for speech recognition, however, we believe, this is the first work that puts deep NLMs into the context of MT. 6 Conclusion In this paper, we have bridged the gap that past work did not show, that is, neural language models with more than two layers can help improve translation quality. Our results confirm the trend reported in (Luong et al., 2015) that sourceconditioned perplexity strongly co</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2015</marker>
<rawString>D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--1137</pages>
<contexts>
<context position="3291" citStr="Bengio et al., 2003" startWordPosition="542" endWordPosition="545">g web-forum MT baseline, our deep models achieve an additional improvement of 0.5 TER / 0.5 BLEU compared to using a shallow NLM. Furthermore, by fine-tuning general indomain NLMs with out-of-domain data, we obtain a similar boost of 1.0 TER / 0.5 BLEU points over a strong domain-adapted sms-chat baseline compared to utilizing a shallow NLM. 2 Neural Language Models We briefly describe the NLM architecture and training objective used in this work as well as compare our approach to other related work. Architecture. Neural language models are fundamentally feed-forward networks as described in (Bengio et al., 2003), but not necessarily limited to only a single hidden layer. Like any 305 Proceedings of the 19th Conference on Computational Language Learning, pages 305–309, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics other language model, NLMs specify a distribution, p(w|c), to predict the next word w given a context c. The first step is to lookup embeddings for words in the context and concatenate them to form an input, h(0), to the first hidden layer. We then repeatedly build up hidden representations as follows, for l = 1, ... , n: (W(l)h(l−1) + b(l)) h(l) = f (1) </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A neural probabilistic language model. JMLR, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for ai. Foundations and Trends</title>
<date>2009</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1299" citStr="Bengio, 2009" startWordPosition="212" endWordPosition="213">e with fewer layers in terms of both the perplexity and the translation quality. We combine various techniques to successfully train deep NLMs that jointly condition on both the source and target contexts. When reranking nbest lists of a strong web-forum baseline, our deep models yield an average boost of 0.5 TER / 0.5 BLEU points compared to using a shallow NLM. Additionally, we adapt our models to a new sms-chat domain and obtain a similar gain of 1.0 TER / 0.5 BLEU points.1 1 Introduction Deep neural networks (DNNs) have been successful in learning more complex functions than shallow ones (Bengio, 2009) and exceled in many challenging tasks such as in speech (Hinton et al., 2012) and vision (Krizhevsky et al., 2012). These results have sparked interest in applying DNNs to natural language processing problems as well. Specifically, in machine translation (MT), there 1Our code and related materials are publicly available at http://stanford.edu/˜lmthang/nlm. has been an active body of work recently in utilizing neural language models (NLMs) to improve translation quality. However, to the best of our knowledge, work in this direction only makes use of NLMs with either one or two hidden layers. F</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for ai. Foundations and Trends in Machine Learning, 2(1):1–127, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>M Galley</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Phrasal: A statistical machine translation toolkit for exploring new model features. In ACL, Demonstration Session.</title>
<date>2010</date>
<contexts>
<context position="9133" citStr="Cer et al., 2010" startWordPosition="1537" endWordPosition="1540">t 1000 target words per second and it generally takes about 10-14 days to fully train a model. We present the NLM training results in Table 1. With more layers, the model succeeds in learning more complex functions; the prediction, hence, becomes more accurate as evidenced by smaller perplexities for both the validation and test sets. Interestingly, we observe that deeper nets can learn self-normalized NLMs better: the mean log normalization factor, |log Z |in Eq. (3), is driven towards 0 as the depth increases.5 3.3 MT Reranking with NLMs Our MT models are built using the Phrasal MT toolkit (Cer et al., 2010). In addition to the standard dense feature set6, we include a variety of sparse features for rules, word pairs, and word classes, as described in (Green et al., 2014). Our decoder uses three language models.7 We use a tuning set of 396K words in the newswire and web domains and tune our systems using online expected error rate training as in (Green et al., 2014). Our tuning metric is (BLEU-TER)/2. We run a discriminative reranker on the 1000- best output of a decoder with MERT. The features used in reranking include all the dense features, 5As a reference point, though not directly comparable</context>
</contexts>
<marker>Cer, Galley, Jurafsky, Manning, 2010</marker>
<rawString>D. Cer, M. Galley, D. Jurafsky, and C. D. Manning. 2010. Phrasal: A statistical machine translation toolkit for exploring new model features. In ACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Devlin</author>
<author>R Zbib</author>
<author>Z Huang</author>
<author>T Lamar</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2154" citStr="Devlin et al. (2014)" startWordPosition="346" endWordPosition="349"> machine translation (MT), there 1Our code and related materials are publicly available at http://stanford.edu/˜lmthang/nlm. has been an active body of work recently in utilizing neural language models (NLMs) to improve translation quality. However, to the best of our knowledge, work in this direction only makes use of NLMs with either one or two hidden layers. For example, Schwenk (2010, 2012) and Son et al. (2012) used shallow NLMs with a single hidden layer for reranking. Vaswani et al. (2013) considered two-layer NLMs for decoding but provided no comparison among models of various depths. Devlin et al. (2014) reported only a small gain when decoding with a two-layer NLM over a single layer one. There have not been clear results on whether adding more layers to NLMs helps. In this paper, we demonstrate that deep NLMs with three or four layers are better than those with fewer layers in terms of the perplexity and the translation quality. We detail how we combine various techniques from past work to successfully train deep NLMs that condition on both the source and target contexts. When reranking nbest lists of a strong web-forum MT baseline, our deep models achieve an additional improvement of 0.5 T</context>
<context position="4845" citStr="Devlin et al. (2014)" startWordPosition="801" endWordPosition="804">e following form: E(c,w)ET − log p(w|c). Training NLMs can be prohibitively slow due to the computationally expensive softmax layer. As a result, past works have tried to use a more efficient version of the softmax such as the hierarchical softmax (Morin, 2005; Mnih and Hinton, 2007; Mnih and Hinton, 2009) or the classbased one (Mikolov et al., 2010; Mikolov et al., 2011). Recently, the noise-contrastive estimation (NCE) technique (Gutmann and Hyv¨arinen, 2012) has been applied to train NLMs in (Mnih and Teh, 2012; Vaswani et al., 2013) to avoid explicitly computing the normalization factors. Devlin et al. (2014) used a modified version of the cross-entropy objective, the self-normalized one. The idea is to not only improve the prediction, p(w|c), but also to push the normalization factor per context, Zc, close to 1: J = � − log p(w|c) + α log2(Zc) (3) (c,w)ET While self-normalization does not lead to speed up in training, it allows trained models to be applied efficiently at test time without computing the normalization factors. This is similar in flavor to NCE but allows for flexibility (through α) in how hard we want to “squeeze” the normalization factors. Training deep NLMs. We follow (Devlin et a</context>
<context position="7804" citStr="Devlin et al. (2014)" startWordPosition="1306" endWordPosition="1309">rmalized weight α = 0.1, (d) a mini-batch of size 128, and (e) a learning rate of 0.1 (training costs are normalized by the mini-batch size). All weights are uniformly initialized in [−0.01,0.01]. We train our models for 4 epochs (after 2 epochs, the learning rate is halved every 0.5 epoch). The vocabularies are limited to the top 40K frequent words for both Chinese and English. All words not in 2Some work (Schwenk, 2010; Schwenk et al., 2012) utilize a smaller softmax vocabulary, called short-list. 3The test set is from BOLT and labeled as p1r6 dev. 4We used an alignment heuristic similar to Devlin et al. (2014) but applicable to our phrase-based MT system. 306 Models Valid Test |log Z| 1 layer 9.39 8.99 0.51 2 layers 9.20 8.96 0.50 3 layers 8.64 8.13 0.43 4 layers 8.10 7.71 0.35 Table 1: Training NLMs – validation and test perplexities achieved by self-normalized NLMs of various depths. We report the |log Z |value (base e), similar to Devlin et al. (2014), to indicate how good each model is in pushing the log normalization factors towards 0. All perplexities are derived by explicitly computing the normalization factors. these vocabularies are replaced by a universal unknown symbol. Embeddings are of</context>
<context position="9755" citStr="Devlin et al. (2014)" startWordPosition="1646" endWordPosition="1649">In addition to the standard dense feature set6, we include a variety of sparse features for rules, word pairs, and word classes, as described in (Green et al., 2014). Our decoder uses three language models.7 We use a tuning set of 396K words in the newswire and web domains and tune our systems using online expected error rate training as in (Green et al., 2014). Our tuning metric is (BLEU-TER)/2. We run a discriminative reranker on the 1000- best output of a decoder with MERT. The features used in reranking include all the dense features, 5As a reference point, though not directly comparable, Devlin et al. (2014) achieved 0.68 for I log ZI on a different test set with the same self-normalized constant α=0.1. 6Consisting of forward and backward translation models, lexical weighting, linear distortion, word penalty, phrase penalty and language model. 7One is trained on the English side of the bitext, one is trained on a 16.3-billion-word monolingual corpus taken from various domains, and one is a class-based language model trained on the same large monolingual corpus. System dev test1 test2 Tj BT Tj BT Tj BT baseline 53.7 33.1 55.1 31.3 63.5 16.5 Reranking 1 layer 52.9 34.3 54.5 32.0 63.1 16.7 2layers 5</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>An empirical comparison of features and tuning for phrasebased machine translation.</title>
<date>2014</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="9300" citStr="Green et al., 2014" startWordPosition="1567" endWordPosition="1570">model succeeds in learning more complex functions; the prediction, hence, becomes more accurate as evidenced by smaller perplexities for both the validation and test sets. Interestingly, we observe that deeper nets can learn self-normalized NLMs better: the mean log normalization factor, |log Z |in Eq. (3), is driven towards 0 as the depth increases.5 3.3 MT Reranking with NLMs Our MT models are built using the Phrasal MT toolkit (Cer et al., 2010). In addition to the standard dense feature set6, we include a variety of sparse features for rules, word pairs, and word classes, as described in (Green et al., 2014). Our decoder uses three language models.7 We use a tuning set of 396K words in the newswire and web domains and tune our systems using online expected error rate training as in (Green et al., 2014). Our tuning metric is (BLEU-TER)/2. We run a discriminative reranker on the 1000- best output of a decoder with MERT. The features used in reranking include all the dense features, 5As a reference point, though not directly comparable, Devlin et al. (2014) achieved 0.68 for I log ZI on a different test set with the same self-normalized constant α=0.1. 6Consisting of forward and backward translation</context>
</contexts>
<marker>Green, Cer, Manning, 2014</marker>
<rawString>S. Green, D. Cer, and C. D. Manning. 2014. An empirical comparison of features and tuning for phrasebased machine translation. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gutmann</author>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics.</title>
<date>2012</date>
<journal>JMLR,</journal>
<pages>13--307</pages>
<marker>Gutmann, Hyv¨arinen, 2012</marker>
<rawString>Michael Gutmann and Aapo Hyv¨arinen. 2012. Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics. JMLR, 13:307–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>L Deng</author>
<author>D Yu</author>
<author>G Dahl</author>
<author>A Mohamed</author>
<author>N Jaitly</author>
<author>A Senior</author>
<author>V Vanhoucke</author>
<author>P Nguyen</author>
<author>T Sainath</author>
<author>B Kingsbury</author>
</authors>
<title>Deep neural networks for acoustic modeling in speech recognition.</title>
<date>2012</date>
<journal>IEEE Signal Processing Magazine.</journal>
<contexts>
<context position="1377" citStr="Hinton et al., 2012" startWordPosition="224" endWordPosition="227"> quality. We combine various techniques to successfully train deep NLMs that jointly condition on both the source and target contexts. When reranking nbest lists of a strong web-forum baseline, our deep models yield an average boost of 0.5 TER / 0.5 BLEU points compared to using a shallow NLM. Additionally, we adapt our models to a new sms-chat domain and obtain a similar gain of 1.0 TER / 0.5 BLEU points.1 1 Introduction Deep neural networks (DNNs) have been successful in learning more complex functions than shallow ones (Bengio, 2009) and exceled in many challenging tasks such as in speech (Hinton et al., 2012) and vision (Krizhevsky et al., 2012). These results have sparked interest in applying DNNs to natural language processing problems as well. Specifically, in machine translation (MT), there 1Our code and related materials are publicly available at http://stanford.edu/˜lmthang/nlm. has been an active body of work recently in utilizing neural language models (NLMs) to improve translation quality. However, to the best of our knowledge, work in this direction only makes use of NLMs with either one or two hidden layers. For example, Schwenk (2010, 2012) and Son et al. (2012) used shallow NLMs with </context>
</contexts>
<marker>Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior, Vanhoucke, Nguyen, Sainath, Kingsbury, 2012</marker>
<rawString>G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. 2012. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ebastien Jean</author>
<author>Kyunghyun Cho</author>
<author>Roland Memisevic</author>
<author>Yoshua Bengio</author>
</authors>
<title>On using very large target vocabulary for neural machine translation.</title>
<date>2015</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="15316" citStr="Jean et al., 2015" startWordPosition="2598" endWordPosition="2601">nally, include a word penalty (WP) feature. All these settings include an NLM score and an aggregate decoder score. As shown, it is best to include all dense features at reranking time. All LMs + WP LMs 1 layer 11.3 11.3 11.4 2 layers 11.2 11.4 11.5 3 layers 11.0 11.1 11.4 4 layers 10.9 11.2 11.3 Table 4: Reranking Conditions – (TER-BLEU)/2 scores when reranking the web-forum baseline. 5 Related Work It is worth mentioning another active line of research in building end-to-end neural MT systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015). These methods have not yet demonstrated success on challenging language pairs such as English-Chinese. Arsoy et al. (2012) have preliminarily examined deep NLMs for speech recognition, however, we believe, this is the first work that puts deep NLMs into the context of MT. 6 Conclusion In this paper, we have bridged the gap that past work did not show, that is, neural language models with more than two layers can help improve translation quality. Our results confirm the trend reported in (Luong et al., 2015) that sourceconditioned perplexity strongly correlates with MT performance. We have al</context>
</contexts>
<marker>Jean, Cho, Memisevic, Bengio, 2015</marker>
<rawString>S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target vocabulary for neural machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kalchbrenner</author>
<author>P Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="15229" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="2582" endWordPosition="2585">anking using all dense features (All) to conditions which use only dense LM features (LM) and optionally, include a word penalty (WP) feature. All these settings include an NLM score and an aggregate decoder score. As shown, it is best to include all dense features at reranking time. All LMs + WP LMs 1 layer 11.3 11.3 11.4 2 layers 11.2 11.4 11.5 3 layers 11.0 11.1 11.4 4 layers 10.9 11.2 11.3 Table 4: Reranking Conditions – (TER-BLEU)/2 scores when reranking the web-forum baseline. 5 Related Work It is worth mentioning another active line of research in building end-to-end neural MT systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015). These methods have not yet demonstrated success on challenging language pairs such as English-Chinese. Arsoy et al. (2012) have preliminarily examined deep NLMs for speech recognition, however, we believe, this is the first work that puts deep NLMs into the context of MT. 6 Conclusion In this paper, we have bridged the gap that past work did not show, that is, neural language models with more than two layers can help improve translation quality. Our results confirm the trend reported in (Luong et al., 2015</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>N. Kalchbrenner and P. Blunsom. 2013. Recurrent continuous translation models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krizhevsky</author>
<author>I Sutskever</author>
<author>G E Hinton</author>
</authors>
<title>ImageNet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1414" citStr="Krizhevsky et al., 2012" startWordPosition="230" endWordPosition="233">niques to successfully train deep NLMs that jointly condition on both the source and target contexts. When reranking nbest lists of a strong web-forum baseline, our deep models yield an average boost of 0.5 TER / 0.5 BLEU points compared to using a shallow NLM. Additionally, we adapt our models to a new sms-chat domain and obtain a similar gain of 1.0 TER / 0.5 BLEU points.1 1 Introduction Deep neural networks (DNNs) have been successful in learning more complex functions than shallow ones (Bengio, 2009) and exceled in many challenging tasks such as in speech (Hinton et al., 2012) and vision (Krizhevsky et al., 2012). These results have sparked interest in applying DNNs to natural language processing problems as well. Specifically, in machine translation (MT), there 1Our code and related materials are publicly available at http://stanford.edu/˜lmthang/nlm. has been an active body of work recently in utilizing neural language models (NLMs) to improve translation quality. However, to the best of our knowledge, work in this direction only makes use of NLMs with either one or two hidden layers. For example, Schwenk (2010, 2012) and Son et al. (2012) used shallow NLMs with a single hidden layer for reranking. </context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012. ImageNet classification with deep convolutional neural networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-T Luong</author>
<author>I Sutskever</author>
<author>Q V Le</author>
<author>O Vinyals</author>
<author>W Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation.</title>
<date>2015</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6016" citStr="Luong et al., 2015" startWordPosition="998" endWordPosition="1001">tors. Training deep NLMs. We follow (Devlin et al., 2014) to train self-normalized NLMs, conditioning on both the source and target contexts. Unlike (Devlin et al., 2014), we found that using the rectified linear function, max{0, x}, proposed in (Nair and Hinton, 2010), works better than tanh. The rectified linear function was used in (Vaswani et al., 2013) as well. Furthermore, while these works use a fixed learning rate throughout, we found that having a simple learning rate schedule is useful in training well-performing deep NLMs. This has also been demonstrated in (Sutskever et al., 2014; Luong et al., 2015) and is detailed in Section 3. We do not perform any gradient clipping and notice that learning is more stable when short sentences of length less than or equal to 2 are removed. Bias terms are used for all hidden layers as well as the softmax layer as described earlier, which is slightly different from other work such as (Vaswani et al., 2013). All these details contribute to our success in training deep NLMs. For simplicity, the same vocabulary is used for both the embedding and the softmax matrices.2 In addition, we adopt the standard softmax to take advantage of GPUs in performing large ma</context>
<context position="11934" citStr="Luong et al., 2015" startWordPosition="2024" endWordPosition="2027">etter than the single layer one, both of which are what past work used. In contrast, reranking with deep NLMs of three or four layers are clearly better, yielding average improvements of 1.0 TER / 1.0 BLEU points over the baseline and 0.5 TER / 0.5 BLEU points over the system reranked with the 1-layer model, all of which are statisfically significant according to the test described in (Riezler and Maxwell, 2005). The fact that the improvements in terms of the intrinsic metrics listed in Table 1 do translate into gains in translation quality is interesting. It reinforces the trend reported in (Luong et al., 2015) that better source-conditioned perplexities lead to better translation scores. This phenomon is a useful result as in the past, many intrinsic metrics, e.g., alignment error rate, do not necessarily correlate with MT quality metrics. 3.5 Domain Adaptation For the sms-chat domain, we use a tune set of 260K words in the newswire, web, and sms-chat domains to tune the decoder weights and a sepa307 System dev test Tj BT Tj BT baseline 62.2 18.7 57.3 23.3 Reranking 1 layer (5.42, 0.51) 60.1 22.0 56.2 25.9 2layers (5.50, 0.51) 60.7 21.5 56.3 26.0 3 layers (5.34 0.43) 59.9 21.4 55.2 26.4 vs. baselin</context>
<context position="15296" citStr="Luong et al., 2015" startWordPosition="2594" endWordPosition="2597">tures (LM) and optionally, include a word penalty (WP) feature. All these settings include an NLM score and an aggregate decoder score. As shown, it is best to include all dense features at reranking time. All LMs + WP LMs 1 layer 11.3 11.3 11.4 2 layers 11.2 11.4 11.5 3 layers 11.0 11.1 11.4 4 layers 10.9 11.2 11.3 Table 4: Reranking Conditions – (TER-BLEU)/2 scores when reranking the web-forum baseline. 5 Related Work It is worth mentioning another active line of research in building end-to-end neural MT systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015). These methods have not yet demonstrated success on challenging language pairs such as English-Chinese. Arsoy et al. (2012) have preliminarily examined deep NLMs for speech recognition, however, we believe, this is the first work that puts deep NLMs into the context of MT. 6 Conclusion In this paper, we have bridged the gap that past work did not show, that is, neural language models with more than two layers can help improve translation quality. Our results confirm the trend reported in (Luong et al., 2015) that sourceconditioned perplexity strongly correlates with MT per</context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2015</marker>
<rawString>M.-T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and W. Zaremba. 2015. Addressing the rare word problem in neural machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Interspeech.</booktitle>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In ICASSP.</booktitle>
<marker>Mikolov, Kombrink, Burget, Cernock´y, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="4508" citStr="Mnih and Hinton, 2007" startWordPosition="747" endWordPosition="751">(1) where f is a non-linear fuction such as tanh. The predictive distribution, p(w|c), is then derived using the standard softmax: s = W(sm)h(n) + b(sm) exp(sw) (2) p(w|c) = ,wEV exp(sw) Objective. The typical way of training NLMs is to maximize the training data likelihood, or equivalently, to minimize the cross-entropy objective of the following form: E(c,w)ET − log p(w|c). Training NLMs can be prohibitively slow due to the computationally expensive softmax layer. As a result, past works have tried to use a more efficient version of the softmax such as the hierarchical softmax (Morin, 2005; Mnih and Hinton, 2007; Mnih and Hinton, 2009) or the classbased one (Mikolov et al., 2010; Mikolov et al., 2011). Recently, the noise-contrastive estimation (NCE) technique (Gutmann and Hyv¨arinen, 2012) has been applied to train NLMs in (Mnih and Teh, 2012; Vaswani et al., 2013) to avoid explicitly computing the normalization factors. Devlin et al. (2014) used a modified version of the cross-entropy objective, the self-normalized one. The idea is to not only improve the prediction, p(w|c), but also to push the normalization factor per context, Zc, close to 1: J = � − log p(w|c) + α log2(Zc) (3) (c,w)ET While self</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="4532" citStr="Mnih and Hinton, 2009" startWordPosition="752" endWordPosition="755">near fuction such as tanh. The predictive distribution, p(w|c), is then derived using the standard softmax: s = W(sm)h(n) + b(sm) exp(sw) (2) p(w|c) = ,wEV exp(sw) Objective. The typical way of training NLMs is to maximize the training data likelihood, or equivalently, to minimize the cross-entropy objective of the following form: E(c,w)ET − log p(w|c). Training NLMs can be prohibitively slow due to the computationally expensive softmax layer. As a result, past works have tried to use a more efficient version of the softmax such as the hierarchical softmax (Morin, 2005; Mnih and Hinton, 2007; Mnih and Hinton, 2009) or the classbased one (Mikolov et al., 2010; Mikolov et al., 2011). Recently, the noise-contrastive estimation (NCE) technique (Gutmann and Hyv¨arinen, 2012) has been applied to train NLMs in (Mnih and Teh, 2012; Vaswani et al., 2013) to avoid explicitly computing the normalization factors. Devlin et al. (2014) used a modified version of the cross-entropy objective, the self-normalized one. The idea is to not only improve the prediction, p(w|c), but also to push the normalization factor per context, Zc, close to 1: J = � − log p(w|c) + α log2(Zc) (3) (c,w)ET While self-normalization does not </context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2009. A scalable hierarchical distributed language model. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="4744" citStr="Mnih and Teh, 2012" startWordPosition="786" endWordPosition="789">ximize the training data likelihood, or equivalently, to minimize the cross-entropy objective of the following form: E(c,w)ET − log p(w|c). Training NLMs can be prohibitively slow due to the computationally expensive softmax layer. As a result, past works have tried to use a more efficient version of the softmax such as the hierarchical softmax (Morin, 2005; Mnih and Hinton, 2007; Mnih and Hinton, 2009) or the classbased one (Mikolov et al., 2010; Mikolov et al., 2011). Recently, the noise-contrastive estimation (NCE) technique (Gutmann and Hyv¨arinen, 2012) has been applied to train NLMs in (Mnih and Teh, 2012; Vaswani et al., 2013) to avoid explicitly computing the normalization factors. Devlin et al. (2014) used a modified version of the cross-entropy objective, the self-normalized one. The idea is to not only improve the prediction, p(w|c), but also to push the normalization factor per context, Zc, close to 1: J = � − log p(w|c) + α log2(Zc) (3) (c,w)ET While self-normalization does not lead to speed up in training, it allows trained models to be applied efficiently at test time without computing the normalization factors. This is similar in flavor to NCE but allows for flexibility (through α) i</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In AISTATS.</booktitle>
<contexts>
<context position="4485" citStr="Morin, 2005" startWordPosition="745" endWordPosition="746">l)) h(l) = f (1) where f is a non-linear fuction such as tanh. The predictive distribution, p(w|c), is then derived using the standard softmax: s = W(sm)h(n) + b(sm) exp(sw) (2) p(w|c) = ,wEV exp(sw) Objective. The typical way of training NLMs is to maximize the training data likelihood, or equivalently, to minimize the cross-entropy objective of the following form: E(c,w)ET − log p(w|c). Training NLMs can be prohibitively slow due to the computationally expensive softmax layer. As a result, past works have tried to use a more efficient version of the softmax such as the hierarchical softmax (Morin, 2005; Mnih and Hinton, 2007; Mnih and Hinton, 2009) or the classbased one (Mikolov et al., 2010; Mikolov et al., 2011). Recently, the noise-contrastive estimation (NCE) technique (Gutmann and Hyv¨arinen, 2012) has been applied to train NLMs in (Mnih and Teh, 2012; Vaswani et al., 2013) to avoid explicitly computing the normalization factors. Devlin et al. (2014) used a modified version of the cross-entropy objective, the self-normalized one. The idea is to not only improve the prediction, p(w|c), but also to push the normalization factor per context, Zc, close to 1: J = � − log p(w|c) + α log2(Zc)</context>
</contexts>
<marker>Morin, 2005</marker>
<rawString>Frederic Morin. 2005. Hierarchical probabilistic neural network language model. In AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve restricted boltzmann machines.</title>
<date>2010</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="5666" citStr="Nair and Hinton, 2010" startWordPosition="941" endWordPosition="944">ose to 1: J = � − log p(w|c) + α log2(Zc) (3) (c,w)ET While self-normalization does not lead to speed up in training, it allows trained models to be applied efficiently at test time without computing the normalization factors. This is similar in flavor to NCE but allows for flexibility (through α) in how hard we want to “squeeze” the normalization factors. Training deep NLMs. We follow (Devlin et al., 2014) to train self-normalized NLMs, conditioning on both the source and target contexts. Unlike (Devlin et al., 2014), we found that using the rectified linear function, max{0, x}, proposed in (Nair and Hinton, 2010), works better than tanh. The rectified linear function was used in (Vaswani et al., 2013) as well. Furthermore, while these works use a fixed learning rate throughout, we found that having a simple learning rate schedule is useful in training well-performing deep NLMs. This has also been demonstrated in (Sutskever et al., 2014; Luong et al., 2015) and is detailed in Section 3. We do not perform any gradient clipping and notice that learning is more stable when short sentences of length less than or equal to 2 are removed. Bias terms are used for all hidden layers as well as the softmax layer </context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>T John Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In ACL Workshop, Intrinsic/Extrinsic Evaluation Measures for MT and Summarization.</booktitle>
<contexts>
<context position="11730" citStr="Riezler and Maxwell, 2005" startWordPosition="1989" endWordPosition="1992">e features. This second tuning set consists of 33K words of web-forum text and is important to obtain good improvements with reranking. 3.4 Results As shown in Table 2, it is not obvious if the depth2 model is better than the single layer one, both of which are what past work used. In contrast, reranking with deep NLMs of three or four layers are clearly better, yielding average improvements of 1.0 TER / 1.0 BLEU points over the baseline and 0.5 TER / 0.5 BLEU points over the system reranked with the 1-layer model, all of which are statisfically significant according to the test described in (Riezler and Maxwell, 2005). The fact that the improvements in terms of the intrinsic metrics listed in Table 1 do translate into gains in translation quality is interesting. It reinforces the trend reported in (Luong et al., 2015) that better source-conditioned perplexities lead to better translation scores. This phenomon is a useful result as in the past, many intrinsic metrics, e.g., alignment error rate, do not necessarily correlate with MT quality metrics. 3.5 Domain Adaptation For the sms-chat domain, we use a tune set of 260K words in the newswire, web, and sms-chat domains to tune the decoder weights and a sepa3</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and T. John Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In ACL Workshop, Intrinsic/Extrinsic Evaluation Measures for MT and Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>A Rousseau</author>
<author>M Attik</author>
</authors>
<title>Large, pruned or continuous space language models on a gpu for statistical machine translation.</title>
<date>2012</date>
<booktitle>In NAACL WLM workshop.</booktitle>
<contexts>
<context position="7631" citStr="Schwenk et al., 2012" startWordPosition="1276" endWordPosition="1279"> NLMs described in Section 2 with SGD, using: (a) a source window of size 5, i.e., 11-gram source context4, (b) a 4-word target history, i.e., 5-gram target LM, (c) a self-normalized weight α = 0.1, (d) a mini-batch of size 128, and (e) a learning rate of 0.1 (training costs are normalized by the mini-batch size). All weights are uniformly initialized in [−0.01,0.01]. We train our models for 4 epochs (after 2 epochs, the learning rate is halved every 0.5 epoch). The vocabularies are limited to the top 40K frequent words for both Chinese and English. All words not in 2Some work (Schwenk, 2010; Schwenk et al., 2012) utilize a smaller softmax vocabulary, called short-list. 3The test set is from BOLT and labeled as p1r6 dev. 4We used an alignment heuristic similar to Devlin et al. (2014) but applicable to our phrase-based MT system. 306 Models Valid Test |log Z| 1 layer 9.39 8.99 0.51 2 layers 9.20 8.96 0.50 3 layers 8.64 8.13 0.43 4 layers 8.10 7.71 0.35 Table 1: Training NLMs – validation and test perplexities achieved by self-normalized NLMs of various depths. We report the |log Z |value (base e), similar to Devlin et al. (2014), to indicate how good each model is in pushing the log normalization factor</context>
</contexts>
<marker>Schwenk, Rousseau, Attik, 2012</marker>
<rawString>H. Schwenk, A. Rousseau, and M. Attik. 2012. Large, pruned or continuous space language models on a gpu for statistical machine translation. In NAACL WLM workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
</authors>
<title>Continuous space language models for statistical machine translation.</title>
<date>2010</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>93--137</pages>
<contexts>
<context position="1924" citStr="Schwenk (2010" startWordPosition="309" endWordPosition="310"> in many challenging tasks such as in speech (Hinton et al., 2012) and vision (Krizhevsky et al., 2012). These results have sparked interest in applying DNNs to natural language processing problems as well. Specifically, in machine translation (MT), there 1Our code and related materials are publicly available at http://stanford.edu/˜lmthang/nlm. has been an active body of work recently in utilizing neural language models (NLMs) to improve translation quality. However, to the best of our knowledge, work in this direction only makes use of NLMs with either one or two hidden layers. For example, Schwenk (2010, 2012) and Son et al. (2012) used shallow NLMs with a single hidden layer for reranking. Vaswani et al. (2013) considered two-layer NLMs for decoding but provided no comparison among models of various depths. Devlin et al. (2014) reported only a small gain when decoding with a two-layer NLM over a single layer one. There have not been clear results on whether adding more layers to NLMs helps. In this paper, we demonstrate that deep NLMs with three or four layers are better than those with fewer layers in terms of the perplexity and the translation quality. We detail how we combine various tec</context>
<context position="7608" citStr="Schwenk, 2010" startWordPosition="1274" endWordPosition="1275">ng We train our NLMs described in Section 2 with SGD, using: (a) a source window of size 5, i.e., 11-gram source context4, (b) a 4-word target history, i.e., 5-gram target LM, (c) a self-normalized weight α = 0.1, (d) a mini-batch of size 128, and (e) a learning rate of 0.1 (training costs are normalized by the mini-batch size). All weights are uniformly initialized in [−0.01,0.01]. We train our models for 4 epochs (after 2 epochs, the learning rate is halved every 0.5 epoch). The vocabularies are limited to the top 40K frequent words for both Chinese and English. All words not in 2Some work (Schwenk, 2010; Schwenk et al., 2012) utilize a smaller softmax vocabulary, called short-list. 3The test set is from BOLT and labeled as p1r6 dev. 4We used an alignment heuristic similar to Devlin et al. (2014) but applicable to our phrase-based MT system. 306 Models Valid Test |log Z| 1 layer 9.39 8.99 0.51 2 layers 9.20 8.96 0.50 3 layers 8.64 8.13 0.43 4 layers 8.10 7.71 0.35 Table 1: Training NLMs – validation and test perplexities achieved by self-normalized NLMs of various depths. We report the |log Z |value (base e), similar to Devlin et al. (2014), to indicate how good each model is in pushing the l</context>
</contexts>
<marker>Schwenk, 2010</marker>
<rawString>H. Schwenk. 2010. Continuous space language models for statistical machine translation. The Prague Bulletin of Mathematical Linguistics, (93):137–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Hai Son</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="1953" citStr="Son et al. (2012)" startWordPosition="313" endWordPosition="316">s such as in speech (Hinton et al., 2012) and vision (Krizhevsky et al., 2012). These results have sparked interest in applying DNNs to natural language processing problems as well. Specifically, in machine translation (MT), there 1Our code and related materials are publicly available at http://stanford.edu/˜lmthang/nlm. has been an active body of work recently in utilizing neural language models (NLMs) to improve translation quality. However, to the best of our knowledge, work in this direction only makes use of NLMs with either one or two hidden layers. For example, Schwenk (2010, 2012) and Son et al. (2012) used shallow NLMs with a single hidden layer for reranking. Vaswani et al. (2013) considered two-layer NLMs for decoding but provided no comparison among models of various depths. Devlin et al. (2014) reported only a small gain when decoding with a two-layer NLM over a single layer one. There have not been clear results on whether adding more layers to NLMs helps. In this paper, we demonstrate that deep NLMs with three or four layers are better than those with fewer layers in terms of the perplexity and the translation quality. We detail how we combine various techniques from past work to suc</context>
</contexts>
<marker>Son, Allauzen, Yvon, 2012</marker>
<rawString>Le Hai Son, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sutskever</author>
<author>O Vinyals</author>
<author>Q V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="5995" citStr="Sutskever et al., 2014" startWordPosition="994" endWordPosition="997">e” the normalization factors. Training deep NLMs. We follow (Devlin et al., 2014) to train self-normalized NLMs, conditioning on both the source and target contexts. Unlike (Devlin et al., 2014), we found that using the rectified linear function, max{0, x}, proposed in (Nair and Hinton, 2010), works better than tanh. The rectified linear function was used in (Vaswani et al., 2013) as well. Furthermore, while these works use a fixed learning rate throughout, we found that having a simple learning rate schedule is useful in training well-performing deep NLMs. This has also been demonstrated in (Sutskever et al., 2014; Luong et al., 2015) and is detailed in Section 3. We do not perform any gradient clipping and notice that learning is more stable when short sentences of length less than or equal to 2 are removed. Bias terms are used for all hidden layers as well as the softmax layer as described earlier, which is slightly different from other work such as (Vaswani et al., 2013). All these details contribute to our success in training deep NLMs. For simplicity, the same vocabulary is used for both the embedding and the softmax matrices.2 In addition, we adopt the standard softmax to take advantage of GPUs i</context>
<context position="15253" citStr="Sutskever et al., 2014" startWordPosition="2586" endWordPosition="2589">(All) to conditions which use only dense LM features (LM) and optionally, include a word penalty (WP) feature. All these settings include an NLM score and an aggregate decoder score. As shown, it is best to include all dense features at reranking time. All LMs + WP LMs 1 layer 11.3 11.3 11.4 2 layers 11.2 11.4 11.5 3 layers 11.0 11.1 11.4 4 layers 10.9 11.2 11.3 Table 4: Reranking Conditions – (TER-BLEU)/2 scores when reranking the web-forum baseline. 5 Related Work It is worth mentioning another active line of research in building end-to-end neural MT systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015). These methods have not yet demonstrated success on challenging language pairs such as English-Chinese. Arsoy et al. (2012) have preliminarily examined deep NLMs for speech recognition, however, we believe, this is the first work that puts deep NLMs into the context of MT. 6 Conclusion In this paper, we have bridged the gap that past work did not show, that is, neural language models with more than two layers can help improve translation quality. Our results confirm the trend reported in (Luong et al., 2015) that sourceconditioned</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with large-scale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2035" citStr="Vaswani et al. (2013)" startWordPosition="327" endWordPosition="330">. These results have sparked interest in applying DNNs to natural language processing problems as well. Specifically, in machine translation (MT), there 1Our code and related materials are publicly available at http://stanford.edu/˜lmthang/nlm. has been an active body of work recently in utilizing neural language models (NLMs) to improve translation quality. However, to the best of our knowledge, work in this direction only makes use of NLMs with either one or two hidden layers. For example, Schwenk (2010, 2012) and Son et al. (2012) used shallow NLMs with a single hidden layer for reranking. Vaswani et al. (2013) considered two-layer NLMs for decoding but provided no comparison among models of various depths. Devlin et al. (2014) reported only a small gain when decoding with a two-layer NLM over a single layer one. There have not been clear results on whether adding more layers to NLMs helps. In this paper, we demonstrate that deep NLMs with three or four layers are better than those with fewer layers in terms of the perplexity and the translation quality. We detail how we combine various techniques from past work to successfully train deep NLMs that condition on both the source and target contexts. W</context>
<context position="4767" citStr="Vaswani et al., 2013" startWordPosition="790" endWordPosition="793">data likelihood, or equivalently, to minimize the cross-entropy objective of the following form: E(c,w)ET − log p(w|c). Training NLMs can be prohibitively slow due to the computationally expensive softmax layer. As a result, past works have tried to use a more efficient version of the softmax such as the hierarchical softmax (Morin, 2005; Mnih and Hinton, 2007; Mnih and Hinton, 2009) or the classbased one (Mikolov et al., 2010; Mikolov et al., 2011). Recently, the noise-contrastive estimation (NCE) technique (Gutmann and Hyv¨arinen, 2012) has been applied to train NLMs in (Mnih and Teh, 2012; Vaswani et al., 2013) to avoid explicitly computing the normalization factors. Devlin et al. (2014) used a modified version of the cross-entropy objective, the self-normalized one. The idea is to not only improve the prediction, p(w|c), but also to push the normalization factor per context, Zc, close to 1: J = � − log p(w|c) + α log2(Zc) (3) (c,w)ET While self-normalization does not lead to speed up in training, it allows trained models to be applied efficiently at test time without computing the normalization factors. This is similar in flavor to NCE but allows for flexibility (through α) in how hard we want to “</context>
<context position="6362" citStr="Vaswani et al., 2013" startWordPosition="1064" endWordPosition="1067">i et al., 2013) as well. Furthermore, while these works use a fixed learning rate throughout, we found that having a simple learning rate schedule is useful in training well-performing deep NLMs. This has also been demonstrated in (Sutskever et al., 2014; Luong et al., 2015) and is detailed in Section 3. We do not perform any gradient clipping and notice that learning is more stable when short sentences of length less than or equal to 2 are removed. Bias terms are used for all hidden layers as well as the softmax layer as described earlier, which is slightly different from other work such as (Vaswani et al., 2013). All these details contribute to our success in training deep NLMs. For simplicity, the same vocabulary is used for both the embedding and the softmax matrices.2 In addition, we adopt the standard softmax to take advantage of GPUs in performing large matrix multiplications. All hyperparameters are given later. 3 Experiments 3.1 Data We use the Chinese-English bitext in the DARPA BOLT (Broad Operational Language Translation) program, with 11.1M parallel sentences (281M Chinese words and 307M English words). We reserve 585 sentences for validation, i.e., choosing hyperparameters, and 1124 sente</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>