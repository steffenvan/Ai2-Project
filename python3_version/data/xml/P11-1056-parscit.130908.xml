<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000152">
<title confidence="0.99691">
Exploiting Syntactico-Semantic Structures for Relation Extraction
</title>
<author confidence="0.985865">
Yee Seng Chan and Dan Roth
</author>
<affiliation confidence="0.995559">
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.998512">
{chanys,danr}@illinois.edu
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993532">
In this paper, we observe that there exists a
second dimension to the relation extraction
(RE) problem that is orthogonal to the relation
type dimension. We show that most of these
second dimensional structures are relatively
constrained and not difficult to identify. We
propose a novel algorithmic approach to RE
that starts by first identifying these structures
and then, within these, identifying the seman-
tic type of the relation. In the real RE problem
where relation arguments need to be identi-
fied, exploiting these structures also allows re-
ducing pipelined propagated errors. We show
that this RE framework provides significant
improvement in RE performance.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958705882353">
Relation extraction (RE) has been defined as the task
of identifying a given set of semantic binary rela-
tions in text. For instance, given the span of text
“... the Seattle zoo ... ”, one would like to extract the
relation that “the Seattle zoo” is located-at “Seattle”.
RE has been frequently studied over the last few
years as a supervised learning task, learning from
spans of text that are annotated with a set of seman-
tic relations of interest. However, most approaches
to RE have assumed that the relations’ arguments
are given as input (Chan and Roth, 2010; Jiang and
Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and
therefore offer only a partial solution to the problem.
Conceptually, this is a rather simple approach as
all spans of texts are treated uniformly and are be-
ing mapped to one of several relation types of in-
terest. However, these approaches to RE require a
large amount of manually annotated training data to
achieve good performance, making it difficult to ex-
pand the set of target relations. Moreover, as we
show, these approaches become brittle when the re-
lations’ arguments are not given but rather need to
be identified in the data too.
In this paper we build on the observation that there
exists a second dimension to the relation extraction
problem that is orthogonal to the relation type di-
mension: all relation types are expressed in one of
several constrained syntactico-semantic structures.
As we show, identifying where the text span is on the
syntactico-semantic structure dimension first, can be
leveraged in the RE process to yield improved per-
formance. Moreover, working in the second dimen-
sion provides robustness to the real RE problem, that
of identifying arguments along with the relations be-
tween them.
For example, in “the Seattle zoo”, the entity men-
tion “Seattle” modifies the noun “zoo”. Thus, the
two mentions “Seattle” and “the Seattle zoo”, are
involved in what we later call a premodifier rela-
tion, one of several syntactico-semantic structures
we identify in Section 3.
We highlight that all relation types can be ex-
pressed in one of several syntactico-semantic struc-
tures – Premodifiers, Possessive, Preposition, For-
mulaic and Verbal. As it turns out, most of these
structures are relatively constrained and are not dif-
ficult to identify. This suggests a novel algorith-
mic approach to RE that starts by first identifying
these structures and then, within these, identifying
the semantic type of the relation. Not only does this
approach provide significantly improved RE perfor-
</bodyText>
<page confidence="0.970854">
551
</page>
<note confidence="0.9795315">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 551–560,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.981145244444444">
mance, it carries with it two additional advantages.
First, leveraging the syntactico-semantic struc-
ture is especially beneficial in the presence of small
amounts of data. Second, and more important, is the
fact that exploiting the syntactico-semantic dimen-
sion provides several new options for dealing with
the full RE problem – incorporating the argument
identification into the problem. We explore one of
these possibilities, making use of the constrained
structures as a way to aid in the identification of the
relations’ arguments. We show that this already pro-
vides significant gain, and discuss other possibilities
that can be explored. The contributions of this paper
are summarized below:
• We highlight that all relation types are ex-
pressed as one of several syntactico-semantic
structures and show that most of these are rela-
tively constrained and not difficult to identify.
Consequently, working first in this structural
dimension can be leveraged in the RE process
to improve performance.
• We show that when one does not have a large
number of training examples, exploiting the
syntactico-semantic structures is crucial for RE
performance.
• We show how to leverage these constrained
structures to improve RE when the relations’
arguments are not given. The constrained struc-
tures allow us to jointly entertain argument can-
didates and relations built with them as argu-
ments. Specifically, we show that considering
argument candidates which otherwise would
have been discarded (provided they exist in
syntactico-semantic structures), we reduce er-
ror propagation along a standard pipeline RE
architecture, and that this joint inference pro-
cess leads to improved RE performance.
In the next section, we describe our relation ex-
traction framework that leverages the syntactico-
semantic structures. We then present these struc-
tures in Section 3. We describe our mention entity
typing system in Section 4 and features for the RE
system in Section 5. We present our RE experiments
in Section 6 and perform analysis in Section 7, be-
fore concluding in Section 8.
</bodyText>
<equation confidence="0.995188769230769">
S = {premodifier, possessive, preposition, formulaic}
gold mentions in training data ✓utrain
Dg = {(mi, mj) E ✓utrain X ✓utrain �
mi in same sentence as mj n i =� j n i &lt; j}
REbase = RE classifier trained on Dg
Ds = 0
for each (mi, mj) E Dg
do
p = structure inference on (mi, mj) using patterns
if p E S V (mi, mj) was annotated with a S structure
Ds = Ds U (mi, mj)
done
REs = RE classifier trained on Ds
</equation>
<bodyText confidence="0.240224">
Output: REbase and REs
</bodyText>
<figureCaption confidence="0.803483">
Figure 1: Training a regular baseline RE classi-
fier REbase and a RE classifier leveraging syntactico-
semantic structures REs.
</figureCaption>
<sectionHeader confidence="0.987088" genericHeader="method">
2 Relation Extraction Framework
</sectionHeader>
<bodyText confidence="0.987923363636364">
In Figure 1, we show the algorithm for training
a typical baseline RE classifier (REbase), and for
training a RE classifier that leverages the syntactico-
semantic structures (REs).
During evaluation and when the gold mentions are
already annotated, we apply REs as follows. When
given a test example mention pair (xi,xj), we per-
form structure inference on it using the patterns de-
scribed in Section 3. If (xi,xj) is identified as hav-
ing any of the four syntactico-semantic structures S,
apply REs to predict the relation label, else apply
REbase.
Next, we show in Figure 2 our joint inference al-
gorithmic framework that leverages the syntactico-
semantic structures for RE, when mentions need to
be predicted. Since the structures are fairly con-
strained, we can use them to consider mention can-
didates that are originally predicted as non men-
tions. As shown in Figure 2, we conservatively in-
clude such mentions when forming mention pairs,
provided their null labels are predicted with a low
probability t1.
</bodyText>
<footnote confidence="0.813337">
1In this work, we arbitrary set t=0.2. After the experiments,
and in our own analysis, we observe that t=0.25 achieves better
performance. Besides using the probability of the 1-best predic-
tion, one could also for instance, use the probability difference
between the first and second best predictions. However, select-
ing an optimal t value is not the main focus of this work.
</footnote>
<page confidence="0.966526">
552
</page>
<equation confidence="0.981375222222222">
S = {premodifier, possessive, preposition, formulaic}
candidate mentions Mcand
Let Lm = argmaxPMET(y|m, B), m ∈ Mcand
y
selected mentions Msel = {m ∈ Mcand |
Lm =6 null ∨ PMET (null|m, B) ≤ t}
QhasNull = {(mi,mj) ∈ Msel × Msel |
mi in same sentence as mj ∧ i =6 j ∧ i &lt; j ∧
(Lmi =6 null ∨ Lmj =6 null)}
</equation>
<bodyText confidence="0.890835">
Let pool of relation predictions R = ∅
</bodyText>
<equation confidence="0.973803090909091">
for each (mi, mj) ∈ QhasNull
do
p = structure inference on (mi, mj) using patterns
if p ∈ S
r = relation prediction for (mi, mj) using REs
R = R ∪ r
else if Lmi =6 null ∧ Lmj =6 null
r = relation prediction for (mi, mj) using REbase
R = R ∪ r
done
Output: R
</equation>
<figureCaption confidence="0.9849552">
Figure 2: RE using predicted mentions and patterns. Ab-
breviations: Lm: predicted entity label for mention m us-
ing the mention entity typing (MET) classifier described
in Section 4; PMET: prediction probability according to
the MET classifier; t: used for thresholding.
</figureCaption>
<bodyText confidence="0.999956086956522">
There is a large body of work in using patterns
to extract relations (Fundel et al., 2007; Greenwood
and Stevenson, 2006; Zhu et al., 2009). However,
these works operate along the first dimension, that
of using patterns to mine for relation type examples.
In contrast, in our RE framework, we apply patterns
to identify the syntactico-semantic structure dimen-
sion first, and leverage this in the RE process. In
(Roth and Yih, 2007), the authors used entity types
to constrain the (first dimensional) relation types al-
lowed among them. In our work, although a few of
our patterns involve semantic type comparison, most
of the patterns are syntactic in nature.
In this work, we performed RE evaluation on the
NIST Automatic Content Extraction (ACE) corpus.
Most prior RE evaluation on ACE data assumed that
mentions are already pre-annotated and given as in-
put (Chan and Roth, 2010; Jiang and Zhai, 2007;
Zhou et al., 2005). An exception is the work of
(Kambhatla, 2004), where the author evaluated on
the ACE-2003 corpus. In that work, the author did
not address the pipelined errors propagated from the
mention identification process.
</bodyText>
<sectionHeader confidence="0.994547" genericHeader="method">
3 Syntactico-Semantic Structures
</sectionHeader>
<bodyText confidence="0.99181455">
In this paper, we performed RE on the ACE-2004
corpus. In ACE-2004 when the annotators tagged a
pair of mentions with a relation, they also specified
the type of syntactico-semantic structure2. ACE-
2004 identified five types of structures: premodi-
fier, possessive, preposition, formulaic, and verbal.
We are unaware of any previous computational ap-
proaches that recognize these structures automati-
cally in text, as we do, and use it in the context of
RE (or any other problem). In (Qian et al., 2008), the
authors reported the recall scores of their RE system
on the various syntactico-semantic structures. But
they do not attempt to recognize nor leverage these
structures.
In this work, we focus on detecting the first four
structures. These four structures cover 80% of the
mention pairs having valid semantic relations (we
give the detailed breakdown in Section 7) and we
show that they are relatively easy to identify using
simple rules or patterns. In this section, we indicate
mentions using square bracket pairs, and use mi and
mj to represent a mention pair. We now describe the
four structures.
Premodifier relations specify the proper adjective
or proper noun premodifier and the following noun
it modifies, e.g.: [the [Seattle] zoo]
Possessive indicates that the first mention is in a
possessive case, e.g.: [[California] ’s Governor]
Preposition indicates that the two mentions are
semantically related via the existence of a preposi-
tion, e.g.: [officials] in [California]
Formulaic The ACE04 annotation guideline3 in-
dicates the annotation of several formulaic relations,
including for example address: [Medford] , [Mas-
sachusetts]
2ACE-2004 termed it as lexical condition. We use the term
syntactico-semantic structure in this paper as the mention pair
exists in specific syntactic structures, and we use rules or pat-
terns that are syntactically and semantically motivated to detect
these structures.
</bodyText>
<footnote confidence="0.9100815">
3http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-
2.PDF
</footnote>
<page confidence="0.989476">
553
</page>
<bodyText confidence="0.863250444444444">
Structure type Pattern
Premodifier Basic pattern: [u* [v+] w+] , where u, v, w represent words
Each w is a noun or adjective
If u* is not empty, then u*: JJ+ V JJ “and” JJ? V CD JJ* V RB DT JJ? V RB CD JJ V
DT (RB|JJ|VBG|VBD|VBN|CD)?
Let w1 = first word in w+. w1 # “’s” and POS tag of w1 # POS
Let vl = last word in v+. POS tag of vl # PRP$ nor WP$
Possessive Basic pattern: [u? [v+] w+] , where u, v, w represent words
Let w1 = first word in w+. If w1 = “’s” V POS tag of w1 = POS, accept mention pair
Let vl = last word in v+. If POS tag of vl = PRP$ or WP$, accept mention pair
Preposition Basic pattern: [mi] v* [mj], where v represent words
and number of prepositions in the text span v* between them = 0, 1, or 2
If satisfy pattern: IN [mi][mj], accept mention pair
If satisfy pattern: [mi] (IN|TO) [mj], accept mention pair
If all labels in Gd start with “prep”, accept mention pair
Formulaic If satisfy pattern: [mi] / [mj] n E,(mi) = PER n E,(mj) = ORG, accept mention pair
If satisfy pattern: [mi][mj]
If E,(mi) = PER n E,(mj) = ORG V GPE, accept mention pair
</bodyText>
<tableCaption confidence="0.750757">
Table 1: Rules and patterns for the four syntactico-semantic structures. Regular expression notations: ‘*’ matches
</tableCaption>
<bodyText confidence="0.938844636363636">
the preceding element zero or more times; ‘+’ matches the preceding element one or more times; ‘?’ indicates that
the preceding element is optional; ‘|’ indicates or. Abbreviations: E,(m): coarse-grained entity type of mention m;
Gd: labels in dependency path between the headword of two mentions. We use square brackets ‘[’ and ‘]’ to denote
mention boundaries. The ‘/’ in the Formulaic row denotes the occurrence of a lexical ‘/’ in text.
In this rest of this section, we present the
rules/patterns for detecting the above four
syntactico-semantic structure, giving an overview
of them in Table 1. We plan to release all of the
rules/patterns along with associated code4. Notice
that the patterns are intuitive and mostly syntactic in
nature.
</bodyText>
<subsectionHeader confidence="0.993501">
3.1 Premodifier Structures
</subsectionHeader>
<listItem confidence="0.93245925">
• We require that one of the mentions completely
include the other mention. Thus, the basic pat-
tern is [u* [v+] w+].
• If u* is not empty, we require that it satisfies
any of the following POS tag sequences: JJ+ V
JJ and JJ? V CD JJ*, etc. These are (optional)
POS tag sequences that normally start a valid
noun phrase.
• We use two patterns to differentiate between
premodifier relations and possessive relations,
by checking for the existence of POS tags
PRP$, WP$, POS, and the word “’s”.
</listItem>
<footnote confidence="0.881404">
4http://cogcomp.cs.illinois.edu/page/publications
</footnote>
<subsectionHeader confidence="0.991055">
3.2 Possessive Structures
</subsectionHeader>
<listItem confidence="0.961594333333333">
• The basic pattern for possessive is similar to
that for premodifier: [u? [v+] w+]
• If the word immediately following v+ is “’s” or
its POS tag is “POS”, we accept the mention
pair. If the POS tag of the last word in v+ is ei-
ther PRP$ or WP$, we accept the mention pair.
</listItem>
<subsectionHeader confidence="0.881144">
3.3 Preposition Structures
</subsectionHeader>
<listItem confidence="0.994856875">
• We first require the two mentions to be non-
overlapping, and check for the existence of
patterns such as “IN [mi] [mj]” and “[mi]
(IN|TO) [mj]”.
• If the only dependency labels in the depen-
dency path between the head words of mi and
mj are “prep” (prepositional modifier), accept
the mention pair.
</listItem>
<subsectionHeader confidence="0.658536">
3.4 Formulaic Structures
</subsectionHeader>
<listItem confidence="0.9938186">
• The ACE-2004 annotator guidelines specify
that several relations such as reporter signing
off, addresses, etc. are often specified in stan-
dard structures. We check for the existence of
patterns such as “[mi] / [mj]”, “[mi] [mj]”,
</listItem>
<page confidence="0.990481">
554
</page>
<table confidence="0.983443666666667">
Category Feature
For every POS of wk and offset from lw
word wk wk and offset from lw
in POS of wk, wk, and offset from lw
mention mi POS of wk, offset from lw, and lw
Bc(wk) and offset from lw
POS of wk, Bc(wk), and offset from lw
POS of wk, offset from lw, and Bc(lw)
Contextual C−1,−1 of mi
C+1,+1 of mi
P−1,−1 of mi
P+1,+1 of mi
</table>
<tableCaption confidence="0.87527">
NE tags tag of NE, if lw of NE coincides
with lw of mi in the sentence
Syntactic parse-label of parse tree constituent
parse that exactly covers mi
parse-labels of parse tree constituents
covering mi
Table 2: Features used in our mention entity typing
(MET) system. The abbreviations are as follows. lw:
last word in the mention; Bc(w): the brown cluster bit
string representing w; NE: named entity
</tableCaption>
<bodyText confidence="0.9403345">
and whether they satisfy certain semantic entity
type constraints.
</bodyText>
<sectionHeader confidence="0.962098" genericHeader="method">
4 Mention Extraction System
</sectionHeader>
<bodyText confidence="0.9999844">
As part of our experiments, we perform RE using
predicted mentions. We first describe the features
(an overview is given in Table 2) and then describe
how we extract candidate mentions from sentences
during evaluation.
</bodyText>
<subsectionHeader confidence="0.994256">
4.1 Mention Extraction Features
</subsectionHeader>
<bodyText confidence="0.992931133333333">
Features for every word in the mention For ev-
ery word wk in a mention mi, we extract seven fea-
tures. These are a combination of wk itself, its POS
tag, and its integer offset from the last word (lw) in
the mention. For instance, given the mention “the
operation room”, the offsets for the three words in
the mention are -2, -1, and 0 respectively. These
features are meant to capture the word and POS tag
sequences in mentions.
We also use word clusters which are automat-
ically generated from unlabeled texts, using the
Brown clustering (Bc) algorithm of (Brown et al.,
1992). This algorithm outputs a binary tree where
words are leaves in the tree. Each word (leaf) in the
tree can be represented by its unique path from the
</bodyText>
<table confidence="0.2308555">
Category Feature
POS POS of single word between m1, m2
hw of mi, mj and P−1,−1 of mi, mj
hw of mi, mj and P−1,−1 of mi, mj
hw of mi, mj and P+1,+1 of mi, mj
hw of mi, mj and P−2,−1 of mi, mj
hw of mi, mj and P−1,+1 of mi, mj
hw of mi, mj and P+1,+2 of mi, mj
</table>
<tableCaption confidence="0.986548">
Base chunk any base phrase chunk between mi, mj
Table 3: Additional RE features.
</tableCaption>
<bodyText confidence="0.999833894736842">
root and this path can be represented as a simple bit
string. As part of our features, we use the cluster bit
string representation of wk and lw.
Contextual We extract the word C_1,_1 immedi-
ately before mi, the word C+1,+1 immediately after
mi, and their associated POS tags P.
NE tags We automatically annotate the sentences
with named entity (NE) tags using the named en-
tity tagger of (Ratinov and Roth, 2009). This tagger
annotates proper nouns with the tags PER (person),
ORG (organization), LOC (location), or MISC (mis-
cellaneous). If the lw of mi coincides (actual token
offset) with the lw of any NE annotated by the NE
tagger, we extract the NE tag as a feature.
Syntactic parse We parse the sentences using the
syntactic parser of (Klein and Manning, 2003). We
extract the label of the parse tree constituent (if it ex-
ists) that exactly covers the mention, and also labels
of all constituents that covers the mention.
</bodyText>
<subsectionHeader confidence="0.995366">
4.2 Extracting Candidate Mentions
</subsectionHeader>
<bodyText confidence="0.999893916666667">
From a sentence, we gather the following as candi-
date mentions: all nouns and possessive pronouns,
all named entities annotated by the the NE tagger
(Ratinov and Roth, 2009), all base noun phrase (NP)
chunks, all chunks satisfying the pattern: NP (PP
NP)+, all NP constituents in the syntactic parse tree,
and from each of these constituents, all substrings
consisting of two or more words, provided the sub-
strings do not start nor end on punctuation marks.
These mention candidates are then fed to our men-
tion entity typing (MET) classifier for type predic-
tion (more details in Section 6.3).
</bodyText>
<page confidence="0.998621">
555
</page>
<sectionHeader confidence="0.993218" genericHeader="method">
5 Relation Extraction System
</sectionHeader>
<bodyText confidence="0.9999548125">
We build a supervised RE system using sentences
annotated with entity mentions and predefined target
relations. During evaluation, when given a pair of
mentions mi, mj, the system predicts whether any
of the predefined target relation holds between the
mention pair.
Most of our features are based on the work of
(Zhou et al., 2005; Chan and Roth, 2010). Due to
space limitations, we refer the reader to our prior
work (Chan and Roth, 2010) for the lexical, struc-
tural, mention-level, entity type, and dependency
features. Here, we only describe the features that
were not used in that work.
As part of our RE system, we need to extract the
head word (hw) of a mention (m), which we heuris-
tically determine as follows: if m contains a prepo-
sition and a noun preceding the preposition, we use
the noun as the hw. If there is no preposition in m,
we use the last noun in m as the hw.
POS features If there is a single word between the
two mentions, we extract its POS tag. Given the hw
of m, Pi,j refers to the sequence of POS tags in the
immediate context of hw (we exclude the POS tag
of hw). The offsets i and j denote the position (rela-
tive to hw) of the first and last POS tag respectively.
For instance, P_2,_1 denotes the sequence of two
POS tags on the immediate left of hw, and P_1,+1
denotes the POS tag on the immediate left of hw and
the POS tag on the immediate right of hw.
Base phrase chunk We add a boolean feature to
detect whether there is any base phrase chunk in the
text span between the two mentions.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999938">
We use the ACE-2004 dataset (catalog
LDC2005T09 from the Linguistic Data Con-
sortium) to conduct our experiments. Following
prior work, we use the news wire (nwire) and
broadcast news (bnews) corpora of ACE-2004 for
our experiments, which consists of 345 documents.
To build our RE system, we use the LIBLINEAR
(Fan et al., 2008) package, with its default settings
of L2-loss SVM (dual) as the solver, and we use an
epsilon of 0.1. To ensure that this baseline RE sys-
tem based on the features in Section 5 is competi-
tive, we compare against the state-of-the-art feature-
based RE systems of (Jiang and Zhai, 2007) and
(Chan and Roth, 2010). In these works, the au-
thors reported performance on undirected coarse-
grained RE. Performing 5-fold cross validation on
the nwire and bnews corpora, (Jiang and Zhai, 2007)
and (Chan and Roth, 2010) reported F-measures of
71.5 and 71.2, respectively. Using the same evalua-
tion setting, our baseline RE system achieves a com-
petitive 71.4 F-measure.
We build three RE classifiers: binary, coarse, fine.
Lumping all the predefined target relations into a
single label, we build a binary classifier to predict
whether any of the predefined relations exists be-
tween a given mention pair.
In this work, we model the argument order of the
mentions when performing RE, since relations are
usually asymmetric in nature. For instance, we con-
sider mi:EMP-ORG:mj and mj:EMP-ORG:mi to
be distinct relation types. In our experiments, we ex-
tracted a total of 55,520 examples or mention pairs.
Out of these, 4,011 are positive relation examples
annotated with 6 coarse-grained relation types and
22 fine-grained relation types5.
We build a coarse-grained classifier to disam-
biguate between 13 relation labels (two asymmetric
labels for each of the 6 coarse-grained relation types
and a null label). We similarly build a fine-grained
classifier to disambiguate between 45 relation labels.
</bodyText>
<subsectionHeader confidence="0.990859">
6.1 Evaluation Method
</subsectionHeader>
<bodyText confidence="0.999967909090909">
For our experiments, we adopt the experimental set-
ting in our prior work (Chan and Roth, 2010) of en-
suring that all examples from a single document are
either all used for training, or all used for evaluation.
In that work, we also highlight that ACE anno-
tators rarely duplicate a relation link for coreferent
mentions. For instance, assume mentions mi, mj,
and mk are in the same sentence, mentions mi and
mj are coreferent, and the annotators tag the men-
tion pair mj, mk with a particular relation r. The
annotators will rarely duplicate the same (implicit)
</bodyText>
<footnote confidence="0.777264166666667">
5We omit a single relation: Discourse (DISC). The ACE-
2004 annotation guidelines states that the DISC relation is es-
tablished only for the purposes of the discourse and does not
reference an official entity relevant to world knowledge. In this
work, we focus on semantically meaningful relations. Further-
more, the DISC relation is dropped in ACE-2005.
</footnote>
<page confidence="0.991019">
556
</page>
<table confidence="0.998887875">
RE model Rec% 10 documents 5% of data F1% 80% of data F1%
Pre% F1% Rec% Pre% Rec% Pre%
Binary 58.0 80.3 67.4 64.4 80.6 71.6 73.2 84.0 78.2
Binary+Patterns 73.1 78.5 75.7 (+8.3) 75.3 80.6 77.9 80.1 84.2 82.1
Coarse 33.5 62.5 43.6 42.4 66.2 51.7 62.1 75.5 68.1
Coarse+Patterns 44.2 59.6 50.8 (+7.2) 51.2 64.2 56.9 68.0 75.4 71.5
Fine 18.1 47.0 26.1 26.3 51.6 34.9 51.6 68.4 58.8
Fine+Patterns 24.8 43.5 31.6 (+5.5) 32.2 48.9 38.9 56.4 67.5 61.5
</table>
<tableCaption confidence="0.991375">
Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.
</tableCaption>
<table confidence="0.999913625">
RE model Rec% 10 documents 5% of data F1% 80% of data F1%
Pre% F1% Rec% Pre% Rec% Pre%
Binary 32.2 46.6 38.1 35.5 48.9 41.1 40.1 52.7 45.5
Binary+Patterns 46.7 45.9 46.3 (+8.2) 47.6 47.8 47.2 50.2 50.4 50.3
Coarse 18.6 41.1 25.6 22.4 40.9 28.9 32.3 47.5 38.5
Coarse+Patterns 26.8 34.7 30.2 (+4.6) 30.3 37.0 33.3 38.9 42.9 40.8
Fine 10.7 32.2 16.1 14.6 33.4 20.3 26.9 44.3 33.5
Fine+Patterns 15.7 26.3 19.7 (+3.6) 19.4 29.2 23.3 31.7 38.3 34.7
</table>
<tableCaption confidence="0.999779">
Table 5: Micro-averaged (across the 5 folds) RE results using predicted mentions.
</tableCaption>
<bodyText confidence="0.999976111111111">
relation r between mi and mk, thus leaving the gold
relation label as null. Whether this is correct or not is
debatable. However, to avoid being penalized when
our RE system actually correctly predicts the label
of an implicit relation, we take the following ap-
proach.
During evaluation, if our system correctly pre-
dicts an implicit label, we simply switch its predic-
tion to the null label. Since the RE recall scores
only take into account non-null relation labels, this
scoring method does not change the recall, but could
marginally increase the precision scores by decreas-
ing the count of RE predictions. In our experi-
ments, we observe that both the usual and our scor-
ing method give very similar RE results and the ex-
perimental trends remain the same. Of course, us-
ing this scoring method requires coreference infor-
mation, which is available in the ACE data.
</bodyText>
<subsectionHeader confidence="0.999529">
6.2 RE Evaluation Using Gold Mentions
</subsectionHeader>
<bodyText confidence="0.999935806451613">
To perform our experiments, we split the 345 docu-
ments into 5 equal sets. In each of the 5 folds, 4 sets
(276 documents) are reserved for drawing training
examples, while the remaining set (69 documents)
is used as evaluation data. In the experiments de-
scribed in this section, we use the gold mentions
available in the data.
When one only has a small amount of train-
ing data, it is crucial to take advantage of external
knowledge such as the syntactico-semantic struc-
tures. To simulate this setting, in each fold, we ran-
domly selected 10 documents from the fold’s avail-
able training documents (about 3% of the total 345
documents) as training data. We built one binary,
one coarse-grained, and one fine-grained classifier
for each fold.
In Section 2, we described how we trained a base-
line RE classifier (REbase) and a RE classifier using
the syntactico-semantic patterns (REs).
We first apply REbase on each test example men-
tion pair (mi,mj) to obtain the RE baseline results,
showing these in Table 4 under the column “10 doc-
uments”, and in the rows “Binary”, “Coarse”, and
“Fine”. We then applied REs on the test exam-
ples as described in Section 2, showing the results
in the rows “Binary+Patterns”, “Coarse+Patterns”,
and “Fine+Patterns”. The results show that by us-
ing syntactico-semantic structures, we obtain signif-
icant F-measure improvements of 8.3, 7.2, and 5.5
for binary, coarse-grained, and fine-grained relation
predictions respectively.
</bodyText>
<subsectionHeader confidence="0.998641">
6.3 RE Evaluation Using Predicted Mentions
</subsectionHeader>
<bodyText confidence="0.999534333333333">
Next, we perform our experiments using predicted
mentions. ACE-2004 defines 7 coarse-grained entity
types, each of which are then refined into 43 fine-
</bodyText>
<page confidence="0.984212">
557
</page>
<figure confidence="0.995798193548387">
Improvement in (gold mentions) RE by using patterns Improvement in (predicted mentions) RE by using patterns
()
4
2
8
7
6
5
3
0
1
Binary+Pattern
Coarse+Pattern
Fine+Pattern
()
4
2
8
7
6
5
3
0
1
Binary+Pattern
Coarse+Pattern
Fine+Pattern
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Proportion (%) of data used for training
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Proportion (%) of data used for training
</figure>
<figureCaption confidence="0.999928">
Figure 3: Improvement in (gold mention) RE. Figure 4: Improvement in (predicted mention) RE.
</figureCaption>
<bodyText confidence="0.999944576923077">
grained entity types. Using the ACE data annotated
with mentions and predefined entity types, we build
a fine-grained mention entity typing (MET) clas-
sifier to disambiguate between 44 labels (43 fine-
grained and a null label to indicate not a mention).
To obtain the coarse-grained entity type predictions
from the classifier, we simply check which coarse-
grained type the fine-grained prediction belongs to.
We use the LIBLINEAR package with the same set-
tings as earlier specified for the RE system. In each
fold, we build a MET classifier using all the (276)
training documents in that fold.
We apply REbase on all mention pairs (mi,mj)
where both mi and mj have non null entity type pre-
dictions. We show these baseline results in the Rows
“Binary”, “Coarse”, and “Fine” of Table 5.
In Section 2, we described our algorithmic ap-
proach (Figure 2) that takes advantage of the struc-
tures with predicted mentions. We show the results
of this approach in the Rows “Binary+Patterns”,
“Coarse+Patterns”, and “Fine+Patterns” of Table
5. The results show that by leveraging syntactico-
semantic structures, we obtain significant F-measure
improvements of 8.2, 4.6, and 3.6 for binary, coarse-
grained, and fine-grained relation predictions re-
spectively.
</bodyText>
<sectionHeader confidence="0.996258" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.999359">
We first show statistics regarding the syntactico-
semantic structures. In Section 3, we mentioned
that ACE-2004 identified five types of structures:
premodifier, possessive, preposition, formulaic, and
</bodyText>
<tableCaption confidence="0.871356">
Table 6: Recall and precision of the patterns.
</tableCaption>
<bodyText confidence="0.999979">
verbal. On the 4,011 examples that we experimented
on, premodifiers are the most frequent, account-
ing for 30.5% of the examples (or about 1,224 ex-
amples). The occurrence distributions of the other
structures are 18.9% (possessive), 23.9% (preposi-
tion), 7.2% (formulaic), and 19.5% (verbal). Hence,
the four syntactico-semantic structures that we fo-
cused on in this paper account for a large majority
(80%) of the relations.
In Section 6, we note that out of 55,520 men-
tion pairs, only 4,011 exhibit valid relations. Thus,
the proportion of positive relation examples is very
sparse at 7.2%. If we can effectively identify and
discard most of the negative relation examples, it
should improve RE performance, including yielding
training data with a more balanced label distribution.
We now analyze the utility of the patterns. As
shown in Table 6, the patterns are effective in infer-
ring the structure of mention pairs. For instance, ap-
plying the premodifier patterns on the 55,520 men-
tion pairs, we correctly identified 86.8% of the 1,224
premodifier occurrences as premodifiers, while in-
curring a false-positive rate of only about 20%6. We
</bodyText>
<footnote confidence="0.7967785">
6Random selection will give a precision of about 2.2%
(1,224 out of 55,520) and thus a false-positive rate of 97.8%
</footnote>
<figure confidence="0.9952057">
Pattern type
PreMod
Poss
Prep
Formula
Rec% Pre%
86.8 79.7
94.3 88.3
94.6 20.0
85.5 62.2
</figure>
<page confidence="0.989718">
558
</page>
<bodyText confidence="0.999977321428571">
note that preposition structures are relatively harder
to identify. Some of the reasons are due to possi-
bly multiple prepositions in between a mention pair,
preposition sense ambiguity, pp-attachment ambigu-
ity, etc. However, in general, we observe that infer-
ring the structures allows us to discard a large por-
tion of the mention pairs which have no valid re-
lation between them. The intuition behind this is
the following: if we infer that there is a syntactico-
semantic structure between a mention pair, then it
is likely that the mention pair exhibits a valid rela-
tion. Conversely, if there is a valid relation between
a mention pair, then it is likely that there exists a
syntactico-semantic structure between the mentions.
Next, we repeat the experiments in Section 6.2
and Section 6.3, while gradually increasing the
amount of training data used for training the RE
classifiers. The detailed results of using 5% and 80%
of all available data are shown in Table 4 and Table
5. Note that these settings are with respect to all 345
documents and thus the 80% setting represents us-
ing all 276 training documents in each fold. We plot
the intermediate results in Figure 3 and Figure 4. We
note that leveraging the structures provides improve-
ments on all experimental settings. Also, intuitively,
the binary predictions benefit the most from lever-
aging the structures. How to further exploit this is a
possible future work.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998817862068966">
In this paper, we propose a novel algorithmic ap-
proach to RE by exploiting syntactico-semantic
structures. We show that this approach provides
several advantages and improves RE performance.
There are several interesting directions for future
work. There are probably many near misses when
we apply our structure patterns on predicted men-
tions. For instance, for both premodifier and posses-
sive structures, we require that one mention com-
pletely includes the other. Relaxing this might
potentially recover additional valid mention pairs
and improve performance. We could also try to
learn classifiers to automatically identify and disam-
biguate between the different syntactico-semantic
structures. It will also be interesting to feedback the
predictions of the structure patterns to the mention
entity typing classifier and possibly retrain to obtain
a better classifier.
Acknowledgements This research is supported by
the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
We thank Ming-Wei Chang and Quang Do for
building the mention extraction system.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998512361111111">
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 152–160.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Katrin Fundel, Robert K¨uffner, and Ralf Zimmer. 2007.
Relex – Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365–371.
Mark A. Greenwood and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation extrac-
tion patterns. In Proceedings of the COLING-ACL
Workshop on Information Extraction Beyond The Doc-
ument, pages 29–35.
Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extraction.
In Proceedings of Human Language Technologies -
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL), pages 113–120.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics and International Joint Confer-
ence on Natural Language Processing (ACL-IJCNLP),
pages 1012–1020.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 178–181.
</reference>
<page confidence="0.984583">
559
</page>
<reference confidence="0.999276481481481">
Dan Klein and Christoper D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In The Conference on Advances in Neural
Information Processing Systems (NIPS), pages 3–10.
Longhua Qian, Guodong Zhou, Qiaomin Zhu, and Peide
Qian. 2008. Relation extraction using convolution
tree kernel expanded with entity features. In Pacific
Asia Conference on Language, Information and Com-
putation, pages 415–421.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Annual Conference on Computational
Natural Language Learning (CoNLL), pages 147–155.
Dan Roth and Wen Tau Yih. 2007. Global inference for
entity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
427–434.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-
Rong Wen. 2009. Statsnowball: a statistical approach
to extracting entity relationships. In The International
World Wide Web Conference, pages 101–110.
</reference>
<page confidence="0.996996">
560
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912791">
<title confidence="0.999942">Exploiting Syntactico-Semantic Structures for Relation Extraction</title>
<author confidence="0.999786">Seng Chan Roth</author>
<affiliation confidence="0.969168">University of Illinois at</affiliation>
<abstract confidence="0.99634625">In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="16671" citStr="Brown et al., 1992" startWordPosition="2803" endWordPosition="2806">evaluation. 4.1 Mention Extraction Features Features for every word in the mention For every word wk in a mention mi, we extract seven features. These are a combination of wk itself, its POS tag, and its integer offset from the last word (lw) in the mention. For instance, given the mention “the operation room”, the offsets for the three words in the mention are -2, -1, and 0 respectively. These features are meant to capture the word and POS tag sequences in mentions. We also use word clusters which are automatically generated from unlabeled texts, using the Brown clustering (Bc) algorithm of (Brown et al., 1992). This algorithm outputs a binary tree where words are leaves in the tree. Each word (leaf) in the tree can be represented by its unique path from the Category Feature POS POS of single word between m1, m2 hw of mi, mj and P−1,−1 of mi, mj hw of mi, mj and P−1,−1 of mi, mj hw of mi, mj and P+1,+1 of mi, mj hw of mi, mj and P−2,−1 of mi, mj hw of mi, mj and P−1,+1 of mi, mj hw of mi, mj and P+1,+2 of mi, mj Base chunk any base phrase chunk between mi, mj Table 3: Additional RE features. root and this path can be represented as a simple bit string. As part of our features, we use the cluster bit</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Exploiting background knowledge for relation extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>152--160</pages>
<contexts>
<context position="1418" citStr="Chan and Roth, 2010" startWordPosition="220" endWordPosition="223">gnificant improvement in RE performance. 1 Introduction Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text. For instance, given the span of text “... the Seattle zoo ... ”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”. RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest. However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem. Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations. Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too</context>
<context position="9364" citStr="Chan and Roth, 2010" startWordPosition="1554" endWordPosition="1557">ontrast, in our RE framework, we apply patterns to identify the syntactico-semantic structure dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus. Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005). An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus. In that work, the author did not address the pipelined errors propagated from the mention identification process. 3 Syntactico-Semantic Structures In this paper, we performed RE on the ACE-2004 corpus. In ACE-2004 when the annotators tagged a pair of mentions with a relation, they also specified the type of syntactico-semantic structure2. ACE2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and verbal. We are unaw</context>
<context position="19102" citStr="Chan and Roth, 2010" startWordPosition="3239" endWordPosition="3242">onsisting of two or more words, provided the substrings do not start nor end on punctuation marks. These mention candidates are then fed to our mention entity typing (MET) classifier for type prediction (more details in Section 6.3). 555 5 Relation Extraction System We build a supervised RE system using sentences annotated with entity mentions and predefined target relations. During evaluation, when given a pair of mentions mi, mj, the system predicts whether any of the predefined target relation holds between the mention pair. Most of our features are based on the work of (Zhou et al., 2005; Chan and Roth, 2010). Due to space limitations, we refer the reader to our prior work (Chan and Roth, 2010) for the lexical, structural, mention-level, entity type, and dependency features. Here, we only describe the features that were not used in that work. As part of our RE system, we need to extract the head word (hw) of a mention (m), which we heuristically determine as follows: if m contains a preposition and a noun preceding the preposition, we use the noun as the hw. If there is no preposition in m, we use the last noun in m as the hw. POS features If there is a single word between the two mentions, we ext</context>
<context position="20920" citStr="Chan and Roth, 2010" startWordPosition="3578" endWordPosition="3581">004 dataset (catalog LDC2005T09 from the Linguistic Data Consortium) to conduct our experiments. Following prior work, we use the news wire (nwire) and broadcast news (bnews) corpora of ACE-2004 for our experiments, which consists of 345 documents. To build our RE system, we use the LIBLINEAR (Fan et al., 2008) package, with its default settings of L2-loss SVM (dual) as the solver, and we use an epsilon of 0.1. To ensure that this baseline RE system based on the features in Section 5 is competitive, we compare against the state-of-the-art featurebased RE systems of (Jiang and Zhai, 2007) and (Chan and Roth, 2010). In these works, the authors reported performance on undirected coarsegrained RE. Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported F-measures of 71.5 and 71.2, respectively. Using the same evaluation setting, our baseline RE system achieves a competitive 71.4 F-measure. We build three RE classifiers: binary, coarse, fine. Lumping all the predefined target relations into a single label, we build a binary classifier to predict whether any of the predefined relations exists between a given mention pair. In this work, we </context>
<context position="22308" citStr="Chan and Roth, 2010" startWordPosition="3797" endWordPosition="3800"> to be distinct relation types. In our experiments, we extracted a total of 55,520 examples or mention pairs. Out of these, 4,011 are positive relation examples annotated with 6 coarse-grained relation types and 22 fine-grained relation types5. We build a coarse-grained classifier to disambiguate between 13 relation labels (two asymmetric labels for each of the 6 coarse-grained relation types and a null label). We similarly build a fine-grained classifier to disambiguate between 45 relation labels. 6.1 Evaluation Method For our experiments, we adopt the experimental setting in our prior work (Chan and Roth, 2010) of ensuring that all examples from a single document are either all used for training, or all used for evaluation. In that work, we also highlight that ACE annotators rarely duplicate a relation link for coreferent mentions. For instance, assume mentions mi, mj, and mk are in the same sentence, mentions mi and mj are coreferent, and the annotators tag the mention pair mj, mk with a particular relation r. The annotators will rarely duplicate the same (implicit) 5We omit a single relation: Discourse (DISC). The ACE2004 annotation guidelines states that the DISC relation is established only for </context>
</contexts>
<marker>Chan, Roth, 2010</marker>
<rawString>Yee Seng Chan and Dan Roth. 2010. Exploiting background knowledge for relation extraction. In Proceedings of the International Conference on Computational Linguistics (COLING), pages 152–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="20612" citStr="Fan et al., 2008" startWordPosition="3522" endWordPosition="3525"> tags on the immediate left of hw, and P_1,+1 denotes the POS tag on the immediate left of hw and the POS tag on the immediate right of hw. Base phrase chunk We add a boolean feature to detect whether there is any base phrase chunk in the text span between the two mentions. 6 Experiments We use the ACE-2004 dataset (catalog LDC2005T09 from the Linguistic Data Consortium) to conduct our experiments. Following prior work, we use the news wire (nwire) and broadcast news (bnews) corpora of ACE-2004 for our experiments, which consists of 345 documents. To build our RE system, we use the LIBLINEAR (Fan et al., 2008) package, with its default settings of L2-loss SVM (dual) as the solver, and we use an epsilon of 0.1. To ensure that this baseline RE system based on the features in Section 5 is competitive, we compare against the state-of-the-art featurebased RE systems of (Jiang and Zhai, 2007) and (Chan and Roth, 2010). In these works, the authors reported performance on undirected coarsegrained RE. Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported F-measures of 71.5 and 71.2, respectively. Using the same evaluation setting, our ba</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Fundel</author>
<author>Robert K¨uffner</author>
<author>Ralf Zimmer</author>
</authors>
<title>Relex – Relation extraction using dependency parse trees.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Fundel, K¨uffner, Zimmer, 2007</marker>
<rawString>Katrin Fundel, Robert K¨uffner, and Ralf Zimmer. 2007. Relex – Relation extraction using dependency parse trees. Bioinformatics, 23(3):365–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Greenwood</author>
<author>Mark Stevenson</author>
</authors>
<title>Improving semi-supervised acquisition of relation extraction patterns.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL Workshop on Information Extraction Beyond The Document,</booktitle>
<pages>29--35</pages>
<contexts>
<context position="8605" citStr="Greenwood and Stevenson, 2006" startWordPosition="1429" endWordPosition="1432">∈ QhasNull do p = structure inference on (mi, mj) using patterns if p ∈ S r = relation prediction for (mi, mj) using REs R = R ∪ r else if Lmi =6 null ∧ Lmj =6 null r = relation prediction for (mi, mj) using REbase R = R ∪ r done Output: R Figure 2: RE using predicted mentions and patterns. Abbreviations: Lm: predicted entity label for mention m using the mention entity typing (MET) classifier described in Section 4; PMET: prediction probability according to the MET classifier; t: used for thresholding. There is a large body of work in using patterns to extract relations (Fundel et al., 2007; Greenwood and Stevenson, 2006; Zhu et al., 2009). However, these works operate along the first dimension, that of using patterns to mine for relation type examples. In contrast, in our RE framework, we apply patterns to identify the syntactico-semantic structure dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automati</context>
</contexts>
<marker>Greenwood, Stevenson, 2006</marker>
<rawString>Mark A. Greenwood and Mark Stevenson. 2006. Improving semi-supervised acquisition of relation extraction patterns. In Proceedings of the COLING-ACL Workshop on Information Extraction Beyond The Document, pages 29–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies -North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1440" citStr="Jiang and Zhai, 2007" startWordPosition="224" endWordPosition="227"> in RE performance. 1 Introduction Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text. For instance, given the span of text “... the Seattle zoo ... ”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”. RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest. However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem. Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations. Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too. In this paper we bui</context>
<context position="9386" citStr="Jiang and Zhai, 2007" startWordPosition="1558" endWordPosition="1561">amework, we apply patterns to identify the syntactico-semantic structure dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus. Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005). An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus. In that work, the author did not address the pipelined errors propagated from the mention identification process. 3 Syntactico-Semantic Structures In this paper, we performed RE on the ACE-2004 corpus. In ACE-2004 when the annotators tagged a pair of mentions with a relation, they also specified the type of syntactico-semantic structure2. ACE2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and verbal. We are unaware of any previous co</context>
<context position="20894" citStr="Jiang and Zhai, 2007" startWordPosition="3573" endWordPosition="3576">xperiments We use the ACE-2004 dataset (catalog LDC2005T09 from the Linguistic Data Consortium) to conduct our experiments. Following prior work, we use the news wire (nwire) and broadcast news (bnews) corpora of ACE-2004 for our experiments, which consists of 345 documents. To build our RE system, we use the LIBLINEAR (Fan et al., 2008) package, with its default settings of L2-loss SVM (dual) as the solver, and we use an epsilon of 0.1. To ensure that this baseline RE system based on the features in Section 5 is competitive, we compare against the state-of-the-art featurebased RE systems of (Jiang and Zhai, 2007) and (Chan and Roth, 2010). In these works, the authors reported performance on undirected coarsegrained RE. Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported F-measures of 71.5 and 71.2, respectively. Using the same evaluation setting, our baseline RE system achieves a competitive 71.4 F-measure. We build three RE classifiers: binary, coarse, fine. Lumping all the predefined target relations into a single label, we build a binary classifier to predict whether any of the predefined relations exists between a given menti</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. A systematic exploration of the feature space for relation extraction. In Proceedings of Human Language Technologies -North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
</authors>
<title>Multi-task transfer learning for weakly-supervised relation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP),</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="1453" citStr="Jiang, 2009" startWordPosition="228" endWordPosition="229">Introduction Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text. For instance, given the span of text “... the Seattle zoo ... ”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”. RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest. However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem. Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations. Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too. In this paper we build on the obs</context>
</contexts>
<marker>Jiang, 2009</marker>
<rawString>Jing Jiang. 2009. Multi-task transfer learning for weakly-supervised relation extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 1012–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>178--181</pages>
<contexts>
<context position="9453" citStr="Kambhatla, 2004" startWordPosition="1572" endWordPosition="1573">e dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus. Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005). An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus. In that work, the author did not address the pipelined errors propagated from the mention identification process. 3 Syntactico-Semantic Structures In this paper, we performed RE on the ACE-2004 corpus. In ACE-2004 when the annotators tagged a pair of mentions with a relation, they also specified the type of syntactico-semantic structure2. ACE2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and verbal. We are unaware of any previous computational approaches that recognize these structures automaticall</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 178–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christoper D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In The Conference on Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>3--10</pages>
<contexts>
<context position="17927" citStr="Klein and Manning, 2003" startWordPosition="3043" endWordPosition="3046"> lw. Contextual We extract the word C_1,_1 immediately before mi, the word C+1,+1 immediately after mi, and their associated POS tags P. NE tags We automatically annotate the sentences with named entity (NE) tags using the named entity tagger of (Ratinov and Roth, 2009). This tagger annotates proper nouns with the tags PER (person), ORG (organization), LOC (location), or MISC (miscellaneous). If the lw of mi coincides (actual token offset) with the lw of any NE annotated by the NE tagger, we extract the NE tag as a feature. Syntactic parse We parse the sentences using the syntactic parser of (Klein and Manning, 2003). We extract the label of the parse tree constituent (if it exists) that exactly covers the mention, and also labels of all constituents that covers the mention. 4.2 Extracting Candidate Mentions From a sentence, we gather the following as candidate mentions: all nouns and possessive pronouns, all named entities annotated by the the NE tagger (Ratinov and Roth, 2009), all base noun phrase (NP) chunks, all chunks satisfying the pattern: NP (PP NP)+, all NP constituents in the syntactic parse tree, and from each of these constituents, all substrings consisting of two or more words, provided the </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christoper D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In The Conference on Advances in Neural Information Processing Systems (NIPS), pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Qiaomin Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Relation extraction using convolution tree kernel expanded with entity features.</title>
<date>2008</date>
<booktitle>In Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>415--421</pages>
<contexts>
<context position="10152" citStr="Qian et al., 2008" startWordPosition="1680" endWordPosition="1683"> not address the pipelined errors propagated from the mention identification process. 3 Syntactico-Semantic Structures In this paper, we performed RE on the ACE-2004 corpus. In ACE-2004 when the annotators tagged a pair of mentions with a relation, they also specified the type of syntactico-semantic structure2. ACE2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and verbal. We are unaware of any previous computational approaches that recognize these structures automatically in text, as we do, and use it in the context of RE (or any other problem). In (Qian et al., 2008), the authors reported the recall scores of their RE system on the various syntactico-semantic structures. But they do not attempt to recognize nor leverage these structures. In this work, we focus on detecting the first four structures. These four structures cover 80% of the mention pairs having valid semantic relations (we give the detailed breakdown in Section 7) and we show that they are relatively easy to identify using simple rules or patterns. In this section, we indicate mentions using square bracket pairs, and use mi and mj to represent a mention pair. We now describe the four structu</context>
</contexts>
<marker>Qian, Zhou, Zhu, Qian, 2008</marker>
<rawString>Longhua Qian, Guodong Zhou, Qiaomin Zhu, and Peide Qian. 2008. Relation extraction using convolution tree kernel expanded with entity features. In Pacific Asia Conference on Language, Information and Computation, pages 415–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>147--155</pages>
<contexts>
<context position="17573" citStr="Ratinov and Roth, 2009" startWordPosition="2982" endWordPosition="2985"> mi, mj and P+1,+1 of mi, mj hw of mi, mj and P−2,−1 of mi, mj hw of mi, mj and P−1,+1 of mi, mj hw of mi, mj and P+1,+2 of mi, mj Base chunk any base phrase chunk between mi, mj Table 3: Additional RE features. root and this path can be represented as a simple bit string. As part of our features, we use the cluster bit string representation of wk and lw. Contextual We extract the word C_1,_1 immediately before mi, the word C+1,+1 immediately after mi, and their associated POS tags P. NE tags We automatically annotate the sentences with named entity (NE) tags using the named entity tagger of (Ratinov and Roth, 2009). This tagger annotates proper nouns with the tags PER (person), ORG (organization), LOC (location), or MISC (miscellaneous). If the lw of mi coincides (actual token offset) with the lw of any NE annotated by the NE tagger, we extract the NE tag as a feature. Syntactic parse We parse the sentences using the syntactic parser of (Klein and Manning, 2003). We extract the label of the parse tree constituent (if it exists) that exactly covers the mention, and also labels of all constituents that covers the mention. 4.2 Extracting Candidate Mentions From a sentence, we gather the following as candid</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL), pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen Tau Yih</author>
</authors>
<title>Global inference for entity and relation identification via a linear programming formulation.</title>
<date>2007</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8916" citStr="Roth and Yih, 2007" startWordPosition="1480" endWordPosition="1483">entity label for mention m using the mention entity typing (MET) classifier described in Section 4; PMET: prediction probability according to the MET classifier; t: used for thresholding. There is a large body of work in using patterns to extract relations (Fundel et al., 2007; Greenwood and Stevenson, 2006; Zhu et al., 2009). However, these works operate along the first dimension, that of using patterns to mine for relation type examples. In contrast, in our RE framework, we apply patterns to identify the syntactico-semantic structure dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus. Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005). An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus. In that wo</context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>Dan Roth and Wen Tau Yih. 2007. Global inference for entity and relation identification via a linear programming formulation. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>427--434</pages>
<contexts>
<context position="1473" citStr="Zhou et al., 2005" startWordPosition="230" endWordPosition="233">Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text. For instance, given the span of text “... the Seattle zoo ... ”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”. RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest. However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem. Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest. However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations. Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too. In this paper we build on the observation that there </context>
<context position="9406" citStr="Zhou et al., 2005" startWordPosition="1562" endWordPosition="1565">erns to identify the syntactico-semantic structure dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus. Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005). An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus. In that work, the author did not address the pipelined errors propagated from the mention identification process. 3 Syntactico-Semantic Structures In this paper, we performed RE on the ACE-2004 corpus. In ACE-2004 when the annotators tagged a pair of mentions with a relation, they also specified the type of syntactico-semantic structure2. ACE2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and verbal. We are unaware of any previous computational approach</context>
<context position="19080" citStr="Zhou et al., 2005" startWordPosition="3235" endWordPosition="3238">s, all substrings consisting of two or more words, provided the substrings do not start nor end on punctuation marks. These mention candidates are then fed to our mention entity typing (MET) classifier for type prediction (more details in Section 6.3). 555 5 Relation Extraction System We build a supervised RE system using sentences annotated with entity mentions and predefined target relations. During evaluation, when given a pair of mentions mi, mj, the system predicts whether any of the predefined target relation holds between the mention pair. Most of our features are based on the work of (Zhou et al., 2005; Chan and Roth, 2010). Due to space limitations, we refer the reader to our prior work (Chan and Roth, 2010) for the lexical, structural, mention-level, entity type, and dependency features. Here, we only describe the features that were not used in that work. As part of our RE system, we need to extract the head word (hw) of a mention (m), which we heuristically determine as follows: if m contains a preposition and a noun preceding the preposition, we use the noun as the hw. If there is no preposition in m, we use the last noun in m as the hw. POS features If there is a single word between th</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 427–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Zaiqing Nie</author>
<author>Xiaojiang Liu</author>
<author>Bo Zhang</author>
<author>JiRong Wen</author>
</authors>
<title>Statsnowball: a statistical approach to extracting entity relationships.</title>
<date>2009</date>
<booktitle>In The International World Wide Web Conference,</booktitle>
<pages>101--110</pages>
<contexts>
<context position="8624" citStr="Zhu et al., 2009" startWordPosition="1433" endWordPosition="1436">erence on (mi, mj) using patterns if p ∈ S r = relation prediction for (mi, mj) using REs R = R ∪ r else if Lmi =6 null ∧ Lmj =6 null r = relation prediction for (mi, mj) using REbase R = R ∪ r done Output: R Figure 2: RE using predicted mentions and patterns. Abbreviations: Lm: predicted entity label for mention m using the mention entity typing (MET) classifier described in Section 4; PMET: prediction probability according to the MET classifier; t: used for thresholding. There is a large body of work in using patterns to extract relations (Fundel et al., 2007; Greenwood and Stevenson, 2006; Zhu et al., 2009). However, these works operate along the first dimension, that of using patterns to mine for relation type examples. In contrast, in our RE framework, we apply patterns to identify the syntactico-semantic structure dimension first, and leverage this in the RE process. In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them. In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature. In this work, we performed RE evaluation on the NIST Automatic Content Extractio</context>
</contexts>
<marker>Zhu, Nie, Liu, Zhang, Wen, 2009</marker>
<rawString>Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and JiRong Wen. 2009. Statsnowball: a statistical approach to extracting entity relationships. In The International World Wide Web Conference, pages 101–110.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>