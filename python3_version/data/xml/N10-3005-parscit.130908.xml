<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003828">
<title confidence="0.98477">
Towards a Matrix-based Distributional Model of Meaning
</title>
<author confidence="0.94432">
Eugenie Giesbrecht
</author>
<affiliation confidence="0.9083755">
FZI Forschungszentrum Informatik
at the University of Karlsruhe
</affiliation>
<address confidence="0.711239">
Haid-und-Neu-Str. 10-14, Karlsruhe, Germany
</address>
<email confidence="0.994591">
giesbrecht@fzi.de
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997845">
Vector-based distributional models
of semantics have proven useful
and adequate in a variety of natural
language processing tasks. How-
ever, most of them lack at least
one key requirement in order to
serve as an adequate representa-
tion of natural language, namely
sensitivity to structural information
such as word order. We propose a
novel approach that offers a poten-
tial of integrating order-dependent
word contexts in a completely un-
supervised manner by assigning to
words characteristic distributional
matrices. The proposed model is
applied to the task of free associa-
tions. In the end, the first results as
well as directions for future work
are discussed.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981347826087">
In natural language processing as well as in informa-
tion retrieval, Vector Space Model (VSM) (Salton et
al., 1975) and Word Space Model (WSM) (Sch¨utze,
1993; Lund and Burgess, 1996) have become the
mainstream for text representation. VSMs embody
the distributional hypothesis of meaning, the main
assumption of which is that a word is known “by
the company it keeps” (Firth, 1957). VSMs proved
to perform well in a number of cognitive tasks such
as synonymy identification (Landauer and Dumais,
1997), automatic thesaurus construction (Grefen-
stette, 1994) and many others. However, it has been
long recognized that these models are too weak to
represent natural language to a satisfactory extent.
With VSMs, the assumption is made that word co-
occurrence is essentially independent of word order.
All the co-occurrence information is thus fed into
one vector per word.
Suppose our “background knowledge” corpus
consists of one sentence: Peter kicked the ball. It
follows that the distributional meanings of both PE-
TER and BALL would be in a similar way defined by
the co-occurring KICK which is insufficient, as BALL
can be only kicked by somebody but not kick itself;
in case of PETER, both ways of interpretation should
be possible. To overcome the aforementioned prob-
lems with vector-based models, we suggest a novel
distributional paradigm for representing text in that
we introduce a further dimension into a “standard”
two-dimensional word space model. That allows us
to count correlations for three words at a time. In
short, given a vocabulary V , context width w = m
and tokens t1, t2, t3, ..., ti E V , for token ti a matrix
of size V x V is generated that has nonzero values
in cells where ti appears between ti_m and ti+m.
Note that this 3-dimensional representation al-
lows us to integrate word order information into the
model in a completely unsupervised manner as well
as to achieve a richer word representation as a matrix
instead of a vector.
The remainder of the paper is organized as fol-
lows. After a recap of basic mathematical no-
tions and operations used in the model in Section 2,
we introduce the proposed three-dimensional tensor-
based model of text representation in Section 3. First
evaluation experiments are reported in Section 4.
</bodyText>
<page confidence="0.979728">
23
</page>
<note confidence="0.5718915">
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 23–28,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.466840833333333">
After a brief overview of related work in Section 5,
we provide some concluding remarks and sugges-
tions for future work in Section 6.
IIMII = � � � � n1 n2 n3 (M(r, s, t))2.
E E E
r=1 s=1 t=1
</table>
<sectionHeader confidence="0.934222" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.998869">
In this section, we provide a brief introduction to
tensors and the basics of mathematical operations
that are employed in the suggested model.
</bodyText>
<equation confidence="0.872971666666667">
First, given d natural numbers n1, ... , nd, a (real)
n1 x ... x nd tensor can be defined as a function
T : {1,...,n1} x ... x {1,...,nd} —* R, map-
</equation>
<bodyText confidence="0.99947585">
ping d-tuples of natural numbers to real numbers.
Intuitively, a tensor can best be thought of as a d-
dimensional table (or array) carrying real numbers
as entries. Thereby n1, ... , nd determine the exten-
sion of the array in the different directions. Obvi-
ously, matrices can be conceived as n1xn2-tensors
and vectors as n1-tensors.
In our setting, we will work with tensors where
d = 3 and for the sake of better understandability
we will introduce the necessary notions for this case
only.
Our work employs higher-order singular value
decomposition (HOSVD), which generalizes the
method of singular value decomposition (SVD)
from matrices to arbitrary tensors.
Given an n1xn2xn3 tensor T, its Tucker decom-
position (Tucker, 1966) for given natural numbers
m1, m2, m3 consists of an m1xm2xm3 tensor G
and three matrices A, B, and C of formats n1xm1,
n2xm2, and n3xm3, respectively, such that
</bodyText>
<equation confidence="0.996847">
T(i,j,k) =
G(r, s, t)·A(i, r)·B(j, s)·C(k, t).
</equation>
<bodyText confidence="0.9999652">
The idea here is to represent the large-size ten-
sor T by the smaller “core” tensor G. The matrices
A, B, and C can be seen as linear transformations
“compressing” input vectors from dimension ni into
dimension mi. Note that a precise representation of
T is not always possible. Rather one may attempt
to approximate T as well as possible, i.e. find the
tensor T&apos; for which a Tucker decomposition exists
and which has the least distance to T. Thereby, the
notion of distance is captured by IIT − T&apos;II, where
T − T&apos; is the tensor obtained by entry-wise subtrac-
tion and II · II is the Frobenius norm defined by
In fact, the described way of approximating a ten-
sor is called dimensionality reduction and is often
used for reducing noise in multi-dimensional data.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="method">
3 Proposed Model
</sectionHeader>
<bodyText confidence="0.996841258064516">
Our motivation is to integrate structure into the ge-
ometrical representation of text meaning while ad-
hering to the ideas of distributional semantics. For
this, we introduce a third dimension that allows us
to separate the left and right contexts of the words.
As we process text, we accumulate the left and right
word co-occurrences to represent the meaning of the
current word. Formally, given a corpus K, a list L
of tokens, and a context width w, we define its ten-
sor representation TIC by letting TIC(i, j, k) be the
number of occurrences of L(j) s L(i) s&apos; L(k) in
sentences in K where s, s&apos; are (possibly empty) se-
quences of at most w − 1 tokens. For example, sup-
pose our corpus consists of three sentences: “Paul
kicked the ball slowly. Peter kicked the ball slowly.
Paul kicked Peter.” We let w = 1, presuming prior
stop words removal. We obtain a 5 x 5 x 5 tensor.
Table 1 displays two i-slices of the resulting tensor
T showing left vs. right context dependencies.
KICK PETER PAUL KICK BALL SLOWLY
PETER 0 0 0 1 0
PAUL 1 0 0 1 0
KICK 0 0 0 0 0
BALL 0 0 0 0 0
SLOWLY 0 0 0 0 0
BALL PETER PAUL KICK BALL SLOWLY
PETER 0 0 0 0 0
PAUL 0 0 0 0 0
KICK 0 0 0 0 2
BALL 0 0 0 0 0
SLOWLY 0 0 0 0 0
</bodyText>
<tableCaption confidence="0.937516">
Table 1: Slices of T for the terms KICK (i = 3) and BALL
</tableCaption>
<bodyText confidence="0.9554102">
(i = 4).
Similarly to traditional vector-based distributional
models, dimensionality reduction needs to be per-
formed in three dimensions either, as the resulting
tensor is very sparse (see the examples of KICK and
</bodyText>
<equation confidence="0.995817777777778">
M1
E
r=1
M2
E
s=1
M3
E
t=1
</equation>
<page confidence="0.98997">
24
</page>
<bodyText confidence="0.870167866666667">
BALL). To this end, we employ Tucker decompo-
sition for 3 dimensions as introduced in Section 2.
For this, Matlab Tensor Toolbox1 (Bader and Kolda,
2006) is used.
A detailed overview of computational complexity
of Tucker decomposition algorithms in Tensor Tool-
box is provided in Turney (2007). The drawback of
those is that their complexity is cubic in the number
of factorization dimensions and unfeasible for large
datasets. However, new memory efficient tensor de-
composition algorithms have been proposed in the
meantime. Thus, Memory Efficient Tucker (MET)
is available in Matlab Tensor Toolbox since Version
2.3. Rendle and Schmidt-Thieme (2010) present a
new factorization method with linear complexity.
</bodyText>
<sectionHeader confidence="0.995182" genericHeader="method">
4 Evaluation Issues
</sectionHeader>
<subsectionHeader confidence="0.944849">
4.1 Task
</subsectionHeader>
<bodyText confidence="0.999968541666667">
Vector-based distributional similarity methods have
proven to be a valuable tool for a number of tasks
on automatic discovery of semantic relatedness be-
tween words, like synonymy tests (Rapp, 2003) or
detection of analogical similarity (Turney, 2006).
A somewhat related task is the task of finding out
to what extent (statistical) similarity measures cor-
relate with free word associations2. Furthermore,
this task was suggested as a shared task for the eval-
uation of word space models at Lexical Semantics
Workshop at ESSLLI 2008. Free associations are
the words that come to the mind of a native speaker
when he or she is presented with a so-called stimu-
lus word. The percent of test subjects that produce
certain response to a given stimulus determines the
degree of a free association between a stimulus and
a response.
Despite the widespread usage of vector-based
models to retrieve semantically similar words, it is
still rather unclear what type of linguistic phenom-
ena they model (cf. Heylen et al. (2008), Wand-
macher et al. (2008)). The same is true for free as-
sociations. There are a number of relations accord-
ing to which a word may be associated with another
</bodyText>
<footnote confidence="0.983627">
1Version 2.3
2One of the reasons to choose this evaluation setting was that
the dataset for free word associations task is freely available at
http://wordspace.collocations.de/doku.php/data:esslli2008:start
(in contrast to, e.g., the synonymy test set).
</footnote>
<bodyText confidence="0.998777666666667">
word. For example, Aitchison (2003) distinguishes
four types of associations: co-ordination, colloca-
tion, superordination and synonymy. This affords
an opportunity to use the task of free associations as
a “baseline” for distributional similarity.
For this task, workshop organizers have proposed
three subtasks, one of which - discrimination - we
adapt in this paper. Test sets have been provided
by the workshop organizers. The former are based
on the Edinburgh Associative Thesaurus3 (EAT),
a freely available database of English association
norms.
Discrimination task includes a test set of over-
all 300 word pairs that were classified according to
three classes of association strengths:
</bodyText>
<listItem confidence="0.997860375">
• FIRST strongly associated word pairs as indi-
cated by more than 50% of test subjects as first
responses;
• HAPAX word associations that were produced
by a single test subject;
• RANDOM random combinations of words from
EAT that were never produced as a stimulus -
response pair.
</listItem>
<subsectionHeader confidence="0.952895">
4.2 Procedure
</subsectionHeader>
<bodyText confidence="0.996594">
To collect the three-way co-occurrence information,
we experiment with the UKWAC corpus (A. Fer-
raresi and Bernardini, 2008), as suggested by the
workshop organizers, in order to get comparable re-
sults. As UKWAC is a huge Web-derived corpus
consisting of about 2 billion tokens, it was impos-
sible at the current stage to process the whole cor-
pus. As the subsections of UKWAC contain ran-
domly chosen documents, one can train the model
on any of the subsections.
We limited out test set to the word pairs for which
the constituent words occur more than 50 times in
the test corpus. Thereby, we ended up with a test set
consisting of 222 word pairs.
We proceed in the following way. For each pair
of words:
</bodyText>
<footnote confidence="0.71076075">
1. Gather N sentences, i.e. contexts, for each of
the two words4, here N = 50;
3http://www.eat.rl.ac.uk/
4This corpus “preprocessing” step was mainly due to lim-
</footnote>
<page confidence="0.997094">
25
</page>
<bodyText confidence="0.7373886">
2. Build a 3-dimensional tensor from the subcor-
pus obtained in (1), given a context width w=5,
i.e. 5 words to the left and 5 words to the right
of the target word), taking sentence boundaries
into consideration;
</bodyText>
<listItem confidence="0.980363166666667">
3. Reduce 5 times the dimensionality of the tensor
obtained in (2) by means of Tucker decomposi-
tion;
4. Extract two matrices of both constituents of the
word pair and compare those by means of co-
sine similarity5.
</listItem>
<bodyText confidence="0.9999252">
Here, we follow the tradition of vector-based
models where cosine is usually used to measure se-
mantic relatedness. One of the future direction in
matrix-based meaning representation is to investi-
gate further matrix comparison metrics.
</bodyText>
<subsectionHeader confidence="0.9009">
4.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99992456">
Tables 2 and 3 show the resulting accuracies6 for
training and test sets. th denotes cosine threshold
values that were used for grouping the results. Here,
th is taken to be the function of the size s of the data
set. Thus, given a training set of size s = 60 and
3 classes, we define an “equally distributed” thresh-
old th, = 60/3 = 20 (s. Table 2) and a “linearly
growing” threshold the = ,�, ,�, rest (s. Table 3).
It is not quite apparent, how the threshold for
differentiating between the groups should be deter-
mined under given conditions. Usually, such mea-
sures are defined on the basis of training data (e.g.
Wandmacher et al. (2008)). It was not applicable
in our case as, due to the current implementation of
the model as well as insufficient computational re-
sources for the time being, we could not build one
big model for all experiment iterations.
Also, the intuition we have gained with this kind
of thresholds is that as soon as you change the un-
derlying corpus or the model parameters, you may
need to define new thresholds (cf. Tables 2 and 3).
ited processing power we had at our disposal at the moment the
experiments were conducted. With this step, we considerably
reduced the size of the corpus and guaranteed a certain number
of contexts per relevant word.
</bodyText>
<footnote confidence="0.961988">
5Cosine similarity is determined as a normalized inner prod-
uct
6Accuracy is defined in the following way: Accuracy =
right/(right + wrong)
</footnote>
<bodyText confidence="0.999947827586207">
Thresholds in geometric models of meaning can not
be just fixed, just as the measure of similarity cannot
be easily quantified by humans.
It would be straightforward to compare the perfor-
mance of the proposed model with its 2-dimensional
analogue. Wandmacher et al. (2008) obtain in aver-
age better results with their LSA-based model for
this task. Specifically, they observe very good re-
sults for RANDOM associations (78.2% accuracy)
but the lowest results for the FIRST, i.e. strongest,
associations (50%). In constrast, the outcome for
RANDOM in our model is the worst. However, the
bigger the threshold, the more accurate is getting
the model for the FIRST associations. For exam-
ple, with a threshold of th = 0.2 for the test set
- 4 out of 5 highest ranked pairs were highly asso-
ciated (FIRST) and the fifth pair was from the HA-
PAX group. For HAPAX word associations, no simi-
lar regularities could be observed.
The resulting accuracies may seem to be poor at
this stage. However, it is worth mentioning that
this is a highly difficult and corpus-dependent task
for automatic processing. The reported results have
been obtained based on very small corpora, contain-
ing ca. 100 sentences per iteration (cf. Wandmacher
et al. (2008) use a corpus of 108 million words to
train their LSA-Model). Consequently, it is not pos-
sible to compare both results directly, as they have
been produced under very different conditions.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999302">
5.1 Matrix Approaches
</subsectionHeader>
<bodyText confidence="0.999753333333333">
There have been a number of efforts to integrate syn-
tax into vector-based models with alternating suc-
cess. Some used (dependency) parsing to feed the
models (Grefenstette, 1994; Lin, 1998; Pad´o and La-
pata, 2007); the others utilized only part of speech
information, e.g., Widdows (2003).
In many cases, these syntactically enhanced mod-
els improved the performance (Grefenstette, 1994;
Lin, 1998; Pad´o and Lapata, 2007). Sometimes,
however, rather controversial results were observed.
Thus, Widdows (2003) reported both positive and
negative effects for the task of developing tax-
onomies. On the one side, POS information in-
creased the performance for common nouns; on the
other side, it degraded the outcome for proper nouns
</bodyText>
<page confidence="0.990935">
26
</page>
<table confidence="0.9943445">
TRAIN TEST
FIRST 12/20 (60%) (th = 0.022) 25/74 (33%) (th = 0.078))
HAPAX 7/20 (35%) (th = 0.008) 35/74 (47%) th = 0.042)
RANDOM 8/20 (40%) 23/74 (31%)
TOTAL (F/H/R) 27/60 (45%) 83/222 (37.4%)
FIRST/HORR7 44/60 (73.33%) 125/222 (56.3%)
</table>
<tableCaption confidence="0.710158">
Table 2: Accuracies for the “equally distributed” threshold for training and test sets
</tableCaption>
<table confidence="0.999634166666667">
TRAIN TEST
FIRST 9/15 (60%) (th = 0.0309) 20/55 (36.4%) (th = 0.09)
HAPAX 8/20 (40%) (th = 0.0101) 39/74 (52.7%) (th = 0.047)
RANDOM 10/25 (40%) 24/93 (25.8%)
TOTAL (F/H/R) 27/60 (45%) 108/222 (48.6%)
FIRST/HORR8 43/60 (71.60%) 113/222 (50.9%)
</table>
<tableCaption confidence="0.999315">
Table 3: Accuracies for a “linearly growing” threshold for training and test sets
</tableCaption>
<bodyText confidence="0.995733">
and verbs.
Sahlgren et al. (2008) incorporate word order in-
formation into context vectors in an unsupervised
manner by means of permutation.
Recently, Erk and Pad´o (2008) proposed a struc-
tured vector space model where a word is repre-
sented by several vectors reflecting the words lexical
meaning as well as its selectional preferences. The
motivation behind their work is very close to ours,
namely, that single vectors are too weak to represent
word meaning. However, we argue that a matrix-
based representation allows us to integrate contex-
tual information in a more general manner.
</bodyText>
<subsectionHeader confidence="0.997835">
5.2 Tensor Approaches
</subsectionHeader>
<bodyText confidence="0.999855857142857">
Among the early attempts to apply higher-order ten-
sors instead of vectors to text data is the work of Liu
et al. (2005) who show that Tensor Space Model is
consistently better than VSM for text classification.
Cai et al. (2006) suggest a 3-dimensional represen-
tation for documents and evaluate the model on the
task of document clustering.
The above as well as a couple of other projects in
this area in information retrieval community leave
open the question of how to convey text into a three-
dimensional tensor. They still use vector-based rep-
resentation as the basis and then just mathematically
convert vectors into tensors, without linguistic justi-
fication of such transformations.
Further, there are few works that extend the term-
document matrix with metadata as a third dimension
(Chew et al., 2007; Sun et al., 2006).
Turney (2007) is one of the few to study the ap-
plication of tensors to word space models. However,
the emphasis in that paper is more on the evaluation
of different tensor decomposition models for such
spaces than on the formal model of text representa-
tion in three dimensions. Van de Cruys (2009) sug-
gests a three-way model of co-occurrence similar to
ours. In contrast to Van de Cruys (2009), we are
not using any explicit syntactic preprocessing. Fur-
thermore, our focus is more on the model itself as a
general model of meaning.
</bodyText>
<sectionHeader confidence="0.997804" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999908882352941">
In this paper, we propose a novel approach to text
representation inspired by the ideas of distributional
semantics. In particular, our model suggests a solu-
tion to the problem of integrating word order infor-
mation in vector spaces in an unsupervised manner.
First experiments on the task of free associations are
reported. However, we are not in the position yet to
commit ourselves to any representative statements.
A thorough evaluation of the model still needs to be
done. Next steps include, amongst others, evaluat-
ing the suggested model with a bigger data corpus as
well as using stemming and more sophisticated fill-
ing of word matrices, e.g., by introducing advanced
weighting schemes into the matrices instead of sim-
ple counts.
Furthermore, we started with evaluation on the
task which has been proposed for the evaluation of
</bodyText>
<page confidence="0.992678">
27
</page>
<bodyText confidence="0.999687428571429">
word space models at the level of word meaning. We
need, however, to evaluate the model for the tasks
where word order information matters more, e.g. on
selectional preferences or paraphrasing.
Last but not least, we plan to address the issue of
modeling compositional meaning with matrix-based
distributional model of meaning.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99982925">
This work is supported by German “Federal Min-
istry of Economics” (BMWi) under the project The-
seus (number 01MQ07019). Many thanks to the
anonymous reviewers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999779406976744">
M. Baroni A. Ferraresi, E. Zanchetta and S. Bernardini.
2008. Introducing and evaluating ukWaC, a very large
Web-derived corpus of English. In Proceedings of the
WAC4 Workshop at LREC’08.
Jean Aitchison. 2003. Words in the Mind: An Introduc-
tion to the Mental Lexicon. Wiley-Blackwell.
Brett W. Bader and Tamara G. Kolda. 2006. Algorithm
862: Matlab tensor classes for fast algorithm prototyp-
ing. ACM Trans. Math. Softw., 32(4):635–653.
Deng Cai, Xiaofei He, and Jiawei Han. 2006. Tensor
space model for document analysis. In SIGIR, pages
625–626. ACM.
Peter Chew, Brett Bader, Tamara Kolda, and Ahmed Ab-
delali. 2007. Cross-language information retrieval us-
ing PARAFAC2. In Proc. KDD’07, pages 143–152.
ACM.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
EMNLP, pages 897–906. ACL.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
55. Studies in linguistic analysis, pages 1–32.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Springer.
Kris Heylen, Yves Peirsman, Dirk Geeraerts, and Dirk
Speelman. 2008. Modelling word similarity: an eval-
uation of automatic synonymy extraction algorithms.
In Proceedings of LREC’08, pages 3243–3249.
T. K. Landauer and S. T Dumais. 1997. Solution to
Plato’s Problem: The Latent Semantic Analysis The-
ory of Acquisition, Induction and Representation of
Knowledge. Psychological Review, 104(2):211–240.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of ACL’98, pages 768–
774. ACL.
Ning Liu, Benyu Zhang, Jun Yan, Zheng Chen, Wenyin
Liu, Fengshan Bai, and Leefeng Chien. 2005. Text
representation: from vector to tensor. In Proc.
ICDM05.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instrumen-
tation, and Computers, pages 203–20.
Sebastian Pad´o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
Ninth Machine Translation Summit, pages 315–322.
Steffen Rendle and Lars Schmidt-Thieme. 2010. Pair-
wise interaction tensor factorization for personalized
tag recommendation. In WSDM ’10: Proceedings of
the third ACM international conference on Web search
and data mining, pages 81–90, New York, NY, USA.
ACM.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proc. CogSci08, pages 1300–1305.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18(11):613–620.
Hinrich Sch¨utze. 1993. Word space. In Advances in
NIPS 5, pages 895–902.
J. Sun, D. Tao, and C. Faloutsos. 2006. Beyond
streams and graphs: Dynamic tensor analysis. In Proc.
KDD’06, pages 374–383.
L.R. Tucker. 1966. Some mathematical notes on three-
mode factor analysis. Psychometrika, 31(3).
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
P. Turney. 2007. Empirical evaluation of four tensor de-
composition algorithms. Technical report. Technical
Report ERB-1152.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction. In
GEMS ’09: Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics, pages
83–90, Morristown, NJ, USA. ACL.
Tonio Wandmacher, Ekaterina Ovchinnikova, and
Theodore Alexandrov. 2008. Does Latent Semantic
Analysis reflect human associations. In Proceedings
of the Lexical Semantics workshop at ESSLLI, Ham-
burg, Germany.
Dominic Widdows. 2003. Unsupervised methods for de-
veloping taxonomies by combining syntactic and sta-
tistical information. In Proceedings of NAACL’03,
pages 197–204. ACL.
</reference>
<page confidence="0.999071">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.165462">
<title confidence="0.66168725">Towards a Matrix-based Distributional Model of Meaning Eugenie FZI Forschungszentrum at the University of</title>
<address confidence="0.58826">Haid-und-Neu-Str. 10-14, Karlsruhe,</address>
<email confidence="0.996677">giesbrecht@fzi.de</email>
<abstract confidence="0.999608666666666">Vector-based distributional models of semantics have proven useful and adequate in a variety of natural language processing tasks. However, most of them lack at least one key requirement in order to serve as an adequate representation of natural language, namely sensitivity to structural information such as word order. We propose a novel approach that offers a potential of integrating order-dependent word contexts in a completely unsupervised manner by assigning to words characteristic distributional matrices. The proposed model is applied to the task of free associations. In the end, the first results as well as directions for future work are discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni A Ferraresi</author>
<author>E Zanchetta</author>
<author>S Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large Web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the WAC4 Workshop at LREC’08.</booktitle>
<marker>Ferraresi, Zanchetta, Bernardini, 2008</marker>
<rawString>M. Baroni A. Ferraresi, E. Zanchetta and S. Bernardini. 2008. Introducing and evaluating ukWaC, a very large Web-derived corpus of English. In Proceedings of the WAC4 Workshop at LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Aitchison</author>
</authors>
<title>Words in the Mind: An Introduction to the Mental Lexicon.</title>
<date>2003</date>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="9216" citStr="Aitchison (2003)" startWordPosition="1574" endWordPosition="1575">ad usage of vector-based models to retrieve semantically similar words, it is still rather unclear what type of linguistic phenomena they model (cf. Heylen et al. (2008), Wandmacher et al. (2008)). The same is true for free associations. There are a number of relations according to which a word may be associated with another 1Version 2.3 2One of the reasons to choose this evaluation setting was that the dataset for free word associations task is freely available at http://wordspace.collocations.de/doku.php/data:esslli2008:start (in contrast to, e.g., the synonymy test set). word. For example, Aitchison (2003) distinguishes four types of associations: co-ordination, collocation, superordination and synonymy. This affords an opportunity to use the task of free associations as a “baseline” for distributional similarity. For this task, workshop organizers have proposed three subtasks, one of which - discrimination - we adapt in this paper. Test sets have been provided by the workshop organizers. The former are based on the Edinburgh Associative Thesaurus3 (EAT), a freely available database of English association norms. Discrimination task includes a test set of overall 300 word pairs that were classif</context>
</contexts>
<marker>Aitchison, 2003</marker>
<rawString>Jean Aitchison. 2003. Words in the Mind: An Introduction to the Mental Lexicon. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
</authors>
<title>Algorithm 862: Matlab tensor classes for fast algorithm prototyping.</title>
<date>2006</date>
<journal>ACM Trans. Math. Softw.,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="7170" citStr="Bader and Kolda, 2006" startWordPosition="1252" endWordPosition="1255"> 1 0 0 1 0 KICK 0 0 0 0 0 BALL 0 0 0 0 0 SLOWLY 0 0 0 0 0 BALL PETER PAUL KICK BALL SLOWLY PETER 0 0 0 0 0 PAUL 0 0 0 0 0 KICK 0 0 0 0 2 BALL 0 0 0 0 0 SLOWLY 0 0 0 0 0 Table 1: Slices of T for the terms KICK (i = 3) and BALL (i = 4). Similarly to traditional vector-based distributional models, dimensionality reduction needs to be performed in three dimensions either, as the resulting tensor is very sparse (see the examples of KICK and M1 E r=1 M2 E s=1 M3 E t=1 24 BALL). To this end, we employ Tucker decomposition for 3 dimensions as introduced in Section 2. For this, Matlab Tensor Toolbox1 (Bader and Kolda, 2006) is used. A detailed overview of computational complexity of Tucker decomposition algorithms in Tensor Toolbox is provided in Turney (2007). The drawback of those is that their complexity is cubic in the number of factorization dimensions and unfeasible for large datasets. However, new memory efficient tensor decomposition algorithms have been proposed in the meantime. Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version 2.3. Rendle and Schmidt-Thieme (2010) present a new factorization method with linear complexity. 4 Evaluation Issues 4.1 Task Vector-based d</context>
</contexts>
<marker>Bader, Kolda, 2006</marker>
<rawString>Brett W. Bader and Tamara G. Kolda. 2006. Algorithm 862: Matlab tensor classes for fast algorithm prototyping. ACM Trans. Math. Softw., 32(4):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deng Cai</author>
<author>Xiaofei He</author>
<author>Jiawei Han</author>
</authors>
<title>Tensor space model for document analysis.</title>
<date>2006</date>
<booktitle>In SIGIR,</booktitle>
<pages>625--626</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="16809" citStr="Cai et al. (2006)" startWordPosition="2842" endWordPosition="2845">e a word is represented by several vectors reflecting the words lexical meaning as well as its selectional preferences. The motivation behind their work is very close to ours, namely, that single vectors are too weak to represent word meaning. However, we argue that a matrixbased representation allows us to integrate contextual information in a more general manner. 5.2 Tensor Approaches Among the early attempts to apply higher-order tensors instead of vectors to text data is the work of Liu et al. (2005) who show that Tensor Space Model is consistently better than VSM for text classification. Cai et al. (2006) suggest a 3-dimensional representation for documents and evaluate the model on the task of document clustering. The above as well as a couple of other projects in this area in information retrieval community leave open the question of how to convey text into a threedimensional tensor. They still use vector-based representation as the basis and then just mathematically convert vectors into tensors, without linguistic justification of such transformations. Further, there are few works that extend the termdocument matrix with metadata as a third dimension (Chew et al., 2007; Sun et al., 2006). T</context>
</contexts>
<marker>Cai, He, Han, 2006</marker>
<rawString>Deng Cai, Xiaofei He, and Jiawei Han. 2006. Tensor space model for document analysis. In SIGIR, pages 625–626. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Chew</author>
<author>Brett Bader</author>
<author>Tamara Kolda</author>
<author>Ahmed Abdelali</author>
</authors>
<title>Cross-language information retrieval using PARAFAC2. In</title>
<date>2007</date>
<booktitle>Proc. KDD’07,</booktitle>
<pages>143--152</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17387" citStr="Chew et al., 2007" startWordPosition="2935" endWordPosition="2938"> text classification. Cai et al. (2006) suggest a 3-dimensional representation for documents and evaluate the model on the task of document clustering. The above as well as a couple of other projects in this area in information retrieval community leave open the question of how to convey text into a threedimensional tensor. They still use vector-based representation as the basis and then just mathematically convert vectors into tensors, without linguistic justification of such transformations. Further, there are few works that extend the termdocument matrix with metadata as a third dimension (Chew et al., 2007; Sun et al., 2006). Turney (2007) is one of the few to study the application of tensors to word space models. However, the emphasis in that paper is more on the evaluation of different tensor decomposition models for such spaces than on the formal model of text representation in three dimensions. Van de Cruys (2009) suggests a three-way model of co-occurrence similar to ours. In contrast to Van de Cruys (2009), we are not using any explicit syntactic preprocessing. Furthermore, our focus is more on the model itself as a general model of meaning. 6 Summary and Future Work In this paper, we pro</context>
</contexts>
<marker>Chew, Bader, Kolda, Abdelali, 2007</marker>
<rawString>Peter Chew, Brett Bader, Tamara Kolda, and Ahmed Abdelali. 2007. Cross-language information retrieval using PARAFAC2. In Proc. KDD’07, pages 143–152. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>897--906</pages>
<publisher>ACL.</publisher>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In EMNLP, pages 897–906. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-55. Studies in linguistic analysis,</title>
<date>1957</date>
<pages>1--32</pages>
<contexts>
<context position="1268" citStr="Firth, 1957" startWordPosition="190" endWordPosition="191">manner by assigning to words characteristic distributional matrices. The proposed model is applied to the task of free associations. In the end, the first results as well as directions for future work are discussed. 1 Introduction In natural language processing as well as in information retrieval, Vector Space Model (VSM) (Salton et al., 1975) and Word Space Model (WSM) (Sch¨utze, 1993; Lund and Burgess, 1996) have become the mainstream for text representation. VSMs embody the distributional hypothesis of meaning, the main assumption of which is that a word is known “by the company it keeps” (Firth, 1957). VSMs proved to perform well in a number of cognitive tasks such as synonymy identification (Landauer and Dumais, 1997), automatic thesaurus construction (Grefenstette, 1994) and many others. However, it has been long recognized that these models are too weak to represent natural language to a satisfactory extent. With VSMs, the assumption is made that word cooccurrence is essentially independent of word order. All the co-occurrence information is thus fed into one vector per word. Suppose our “background knowledge” corpus consists of one sentence: Peter kicked the ball. It follows that the d</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J.R. Firth. 1957. A synopsis of linguistic theory 1930-55. Studies in linguistic analysis, pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Springer.</publisher>
<contexts>
<context position="1443" citStr="Grefenstette, 1994" startWordPosition="214" endWordPosition="216">ll as directions for future work are discussed. 1 Introduction In natural language processing as well as in information retrieval, Vector Space Model (VSM) (Salton et al., 1975) and Word Space Model (WSM) (Sch¨utze, 1993; Lund and Burgess, 1996) have become the mainstream for text representation. VSMs embody the distributional hypothesis of meaning, the main assumption of which is that a word is known “by the company it keeps” (Firth, 1957). VSMs proved to perform well in a number of cognitive tasks such as synonymy identification (Landauer and Dumais, 1997), automatic thesaurus construction (Grefenstette, 1994) and many others. However, it has been long recognized that these models are too weak to represent natural language to a satisfactory extent. With VSMs, the assumption is made that word cooccurrence is essentially independent of word order. All the co-occurrence information is thus fed into one vector per word. Suppose our “background knowledge” corpus consists of one sentence: Peter kicked the ball. It follows that the distributional meanings of both PETER and BALL would be in a similar way defined by the co-occurring KICK which is insufficient, as BALL can be only kicked by somebody but not </context>
<context position="14772" citStr="Grefenstette, 1994" startWordPosition="2514" endWordPosition="2515"> is a highly difficult and corpus-dependent task for automatic processing. The reported results have been obtained based on very small corpora, containing ca. 100 sentences per iteration (cf. Wandmacher et al. (2008) use a corpus of 108 million words to train their LSA-Model). Consequently, it is not possible to compare both results directly, as they have been produced under very different conditions. 5 Related Work 5.1 Matrix Approaches There have been a number of efforts to integrate syntax into vector-based models with alternating success. Some used (dependency) parsing to feed the models (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007); the others utilized only part of speech information, e.g., Widdows (2003). In many cases, these syntactically enhanced models improved the performance (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). Sometimes, however, rather controversial results were observed. Thus, Widdows (2003) reported both positive and negative effects for the task of developing taxonomies. On the one side, POS information increased the performance for common nouns; on the other side, it degraded the outcome for proper nouns 26 TRAIN TEST FIRST 12/20 (60%) (th = 0.022) 25/7</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kris Heylen</author>
<author>Yves Peirsman</author>
<author>Dirk Geeraerts</author>
<author>Dirk Speelman</author>
</authors>
<title>Modelling word similarity: an evaluation of automatic synonymy extraction algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC’08,</booktitle>
<pages>3243--3249</pages>
<contexts>
<context position="8769" citStr="Heylen et al. (2008)" startWordPosition="1504" endWordPosition="1507">urthermore, this task was suggested as a shared task for the evaluation of word space models at Lexical Semantics Workshop at ESSLLI 2008. Free associations are the words that come to the mind of a native speaker when he or she is presented with a so-called stimulus word. The percent of test subjects that produce certain response to a given stimulus determines the degree of a free association between a stimulus and a response. Despite the widespread usage of vector-based models to retrieve semantically similar words, it is still rather unclear what type of linguistic phenomena they model (cf. Heylen et al. (2008), Wandmacher et al. (2008)). The same is true for free associations. There are a number of relations according to which a word may be associated with another 1Version 2.3 2One of the reasons to choose this evaluation setting was that the dataset for free word associations task is freely available at http://wordspace.collocations.de/doku.php/data:esslli2008:start (in contrast to, e.g., the synonymy test set). word. For example, Aitchison (2003) distinguishes four types of associations: co-ordination, collocation, superordination and synonymy. This affords an opportunity to use the task of free </context>
</contexts>
<marker>Heylen, Peirsman, Geeraerts, Speelman, 2008</marker>
<rawString>Kris Heylen, Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2008. Modelling word similarity: an evaluation of automatic synonymy extraction algorithms. In Proceedings of LREC’08, pages 3243–3249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction and Representation of Knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1388" citStr="Landauer and Dumais, 1997" startWordPosition="207" endWordPosition="210">task of free associations. In the end, the first results as well as directions for future work are discussed. 1 Introduction In natural language processing as well as in information retrieval, Vector Space Model (VSM) (Salton et al., 1975) and Word Space Model (WSM) (Sch¨utze, 1993; Lund and Burgess, 1996) have become the mainstream for text representation. VSMs embody the distributional hypothesis of meaning, the main assumption of which is that a word is known “by the company it keeps” (Firth, 1957). VSMs proved to perform well in a number of cognitive tasks such as synonymy identification (Landauer and Dumais, 1997), automatic thesaurus construction (Grefenstette, 1994) and many others. However, it has been long recognized that these models are too weak to represent natural language to a satisfactory extent. With VSMs, the assumption is made that word cooccurrence is essentially independent of word order. All the co-occurrence information is thus fed into one vector per word. Suppose our “background knowledge” corpus consists of one sentence: Peter kicked the ball. It follows that the distributional meanings of both PETER and BALL would be in a similar way defined by the co-occurring KICK which is insuff</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T Dumais. 1997. Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction and Representation of Knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL’98,</booktitle>
<pages>768--774</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="14783" citStr="Lin, 1998" startWordPosition="2516" endWordPosition="2517">lt and corpus-dependent task for automatic processing. The reported results have been obtained based on very small corpora, containing ca. 100 sentences per iteration (cf. Wandmacher et al. (2008) use a corpus of 108 million words to train their LSA-Model). Consequently, it is not possible to compare both results directly, as they have been produced under very different conditions. 5 Related Work 5.1 Matrix Approaches There have been a number of efforts to integrate syntax into vector-based models with alternating success. Some used (dependency) parsing to feed the models (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007); the others utilized only part of speech information, e.g., Widdows (2003). In many cases, these syntactically enhanced models improved the performance (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). Sometimes, however, rather controversial results were observed. Thus, Widdows (2003) reported both positive and negative effects for the task of developing taxonomies. On the one side, POS information increased the performance for common nouns; on the other side, it degraded the outcome for proper nouns 26 TRAIN TEST FIRST 12/20 (60%) (th = 0.022) 25/74 (33%) (th</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of ACL’98, pages 768– 774. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ning Liu</author>
<author>Benyu Zhang</author>
<author>Jun Yan</author>
<author>Zheng Chen</author>
<author>Wenyin Liu</author>
<author>Fengshan Bai</author>
<author>Leefeng Chien</author>
</authors>
<title>Text representation: from vector to tensor. In</title>
<date>2005</date>
<booktitle>Proc. ICDM05.</booktitle>
<contexts>
<context position="16701" citStr="Liu et al. (2005)" startWordPosition="2824" endWordPosition="2827">manner by means of permutation. Recently, Erk and Pad´o (2008) proposed a structured vector space model where a word is represented by several vectors reflecting the words lexical meaning as well as its selectional preferences. The motivation behind their work is very close to ours, namely, that single vectors are too weak to represent word meaning. However, we argue that a matrixbased representation allows us to integrate contextual information in a more general manner. 5.2 Tensor Approaches Among the early attempts to apply higher-order tensors instead of vectors to text data is the work of Liu et al. (2005) who show that Tensor Space Model is consistently better than VSM for text classification. Cai et al. (2006) suggest a 3-dimensional representation for documents and evaluate the model on the task of document clustering. The above as well as a couple of other projects in this area in information retrieval community leave open the question of how to convey text into a threedimensional tensor. They still use vector-based representation as the basis and then just mathematically convert vectors into tensors, without linguistic justification of such transformations. Further, there are few works tha</context>
</contexts>
<marker>Liu, Zhang, Yan, Chen, Liu, Bai, Chien, 2005</marker>
<rawString>Ning Liu, Benyu Zhang, Jun Yan, Zheng Chen, Wenyin Liu, Fengshan Bai, and Leefeng Chien. 2005. Text representation: from vector to tensor. In Proc. ICDM05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instrumentation, and Computers,</journal>
<pages>203--20</pages>
<contexts>
<context position="1069" citStr="Lund and Burgess, 1996" startWordPosition="156" endWordPosition="159"> natural language, namely sensitivity to structural information such as word order. We propose a novel approach that offers a potential of integrating order-dependent word contexts in a completely unsupervised manner by assigning to words characteristic distributional matrices. The proposed model is applied to the task of free associations. In the end, the first results as well as directions for future work are discussed. 1 Introduction In natural language processing as well as in information retrieval, Vector Space Model (VSM) (Salton et al., 1975) and Word Space Model (WSM) (Sch¨utze, 1993; Lund and Burgess, 1996) have become the mainstream for text representation. VSMs embody the distributional hypothesis of meaning, the main assumption of which is that a word is known “by the company it keeps” (Firth, 1957). VSMs proved to perform well in a number of cognitive tasks such as synonymy identification (Landauer and Dumais, 1997), automatic thesaurus construction (Grefenstette, 1994) and many others. However, it has been long recognized that these models are too weak to represent natural language to a satisfactory extent. With VSMs, the assumption is made that word cooccurrence is essentially independent </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instrumentation, and Computers, pages 203–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Word sense discovery based on sense descriptor dissimilarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth Machine Translation Summit,</booktitle>
<pages>315--322</pages>
<contexts>
<context position="7953" citStr="Rapp, 2003" startWordPosition="1371" endWordPosition="1372">heir complexity is cubic in the number of factorization dimensions and unfeasible for large datasets. However, new memory efficient tensor decomposition algorithms have been proposed in the meantime. Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version 2.3. Rendle and Schmidt-Thieme (2010) present a new factorization method with linear complexity. 4 Evaluation Issues 4.1 Task Vector-based distributional similarity methods have proven to be a valuable tool for a number of tasks on automatic discovery of semantic relatedness between words, like synonymy tests (Rapp, 2003) or detection of analogical similarity (Turney, 2006). A somewhat related task is the task of finding out to what extent (statistical) similarity measures correlate with free word associations2. Furthermore, this task was suggested as a shared task for the evaluation of word space models at Lexical Semantics Workshop at ESSLLI 2008. Free associations are the words that come to the mind of a native speaker when he or she is presented with a so-called stimulus word. The percent of test subjects that produce certain response to a given stimulus determines the degree of a free association between </context>
</contexts>
<marker>Rapp, 2003</marker>
<rawString>Reinhard Rapp. 2003. Word sense discovery based on sense descriptor dissimilarity. In Proceedings of the Ninth Machine Translation Summit, pages 315–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Rendle</author>
<author>Lars Schmidt-Thieme</author>
</authors>
<title>Pairwise interaction tensor factorization for personalized tag recommendation.</title>
<date>2010</date>
<booktitle>In WSDM ’10: Proceedings of the third ACM international conference on Web search and data mining,</booktitle>
<pages>81--90</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7667" citStr="Rendle and Schmidt-Thieme (2010)" startWordPosition="1326" endWordPosition="1329">d, we employ Tucker decomposition for 3 dimensions as introduced in Section 2. For this, Matlab Tensor Toolbox1 (Bader and Kolda, 2006) is used. A detailed overview of computational complexity of Tucker decomposition algorithms in Tensor Toolbox is provided in Turney (2007). The drawback of those is that their complexity is cubic in the number of factorization dimensions and unfeasible for large datasets. However, new memory efficient tensor decomposition algorithms have been proposed in the meantime. Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version 2.3. Rendle and Schmidt-Thieme (2010) present a new factorization method with linear complexity. 4 Evaluation Issues 4.1 Task Vector-based distributional similarity methods have proven to be a valuable tool for a number of tasks on automatic discovery of semantic relatedness between words, like synonymy tests (Rapp, 2003) or detection of analogical similarity (Turney, 2006). A somewhat related task is the task of finding out to what extent (statistical) similarity measures correlate with free word associations2. Furthermore, this task was suggested as a shared task for the evaluation of word space models at Lexical Semantics Work</context>
</contexts>
<marker>Rendle, Schmidt-Thieme, 2010</marker>
<rawString>Steffen Rendle and Lars Schmidt-Thieme. 2010. Pairwise interaction tensor factorization for personalized tag recommendation. In WSDM ’10: Proceedings of the third ACM international conference on Web search and data mining, pages 81–90, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
<author>A Holst</author>
<author>P Kanerva</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>In Proc. CogSci08,</booktitle>
<pages>1300--1305</pages>
<contexts>
<context position="16008" citStr="Sahlgren et al. (2008)" startWordPosition="2709" endWordPosition="2712">0.078)) HAPAX 7/20 (35%) (th = 0.008) 35/74 (47%) th = 0.042) RANDOM 8/20 (40%) 23/74 (31%) TOTAL (F/H/R) 27/60 (45%) 83/222 (37.4%) FIRST/HORR7 44/60 (73.33%) 125/222 (56.3%) Table 2: Accuracies for the “equally distributed” threshold for training and test sets TRAIN TEST FIRST 9/15 (60%) (th = 0.0309) 20/55 (36.4%) (th = 0.09) HAPAX 8/20 (40%) (th = 0.0101) 39/74 (52.7%) (th = 0.047) RANDOM 10/25 (40%) 24/93 (25.8%) TOTAL (F/H/R) 27/60 (45%) 108/222 (48.6%) FIRST/HORR8 43/60 (71.60%) 113/222 (50.9%) Table 3: Accuracies for a “linearly growing” threshold for training and test sets and verbs. Sahlgren et al. (2008) incorporate word order information into context vectors in an unsupervised manner by means of permutation. Recently, Erk and Pad´o (2008) proposed a structured vector space model where a word is represented by several vectors reflecting the words lexical meaning as well as its selectional preferences. The motivation behind their work is very close to ours, namely, that single vectors are too weak to represent word meaning. However, we argue that a matrixbased representation allows us to integrate contextual information in a more general manner. 5.2 Tensor Approaches Among the early attempts t</context>
</contexts>
<marker>Sahlgren, Holst, Kanerva, 2008</marker>
<rawString>M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permutations as a means to encode order in word space. In Proc. CogSci08, pages 1300–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="1001" citStr="Salton et al., 1975" startWordPosition="145" endWordPosition="148">ey requirement in order to serve as an adequate representation of natural language, namely sensitivity to structural information such as word order. We propose a novel approach that offers a potential of integrating order-dependent word contexts in a completely unsupervised manner by assigning to words characteristic distributional matrices. The proposed model is applied to the task of free associations. In the end, the first results as well as directions for future work are discussed. 1 Introduction In natural language processing as well as in information retrieval, Vector Space Model (VSM) (Salton et al., 1975) and Word Space Model (WSM) (Sch¨utze, 1993; Lund and Burgess, 1996) have become the mainstream for text representation. VSMs embody the distributional hypothesis of meaning, the main assumption of which is that a word is known “by the company it keeps” (Firth, 1957). VSMs proved to perform well in a number of cognitive tasks such as synonymy identification (Landauer and Dumais, 1997), automatic thesaurus construction (Grefenstette, 1994) and many others. However, it has been long recognized that these models are too weak to represent natural language to a satisfactory extent. With VSMs, the a</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Commun. ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Word space.</title>
<date>1993</date>
<booktitle>In Advances in NIPS 5,</booktitle>
<pages>895--902</pages>
<marker>Sch¨utze, 1993</marker>
<rawString>Hinrich Sch¨utze. 1993. Word space. In Advances in NIPS 5, pages 895–902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sun</author>
<author>D Tao</author>
<author>C Faloutsos</author>
</authors>
<title>Beyond streams and graphs: Dynamic tensor analysis.</title>
<date>2006</date>
<booktitle>In Proc. KDD’06,</booktitle>
<pages>374--383</pages>
<contexts>
<context position="17406" citStr="Sun et al., 2006" startWordPosition="2939" endWordPosition="2942">n. Cai et al. (2006) suggest a 3-dimensional representation for documents and evaluate the model on the task of document clustering. The above as well as a couple of other projects in this area in information retrieval community leave open the question of how to convey text into a threedimensional tensor. They still use vector-based representation as the basis and then just mathematically convert vectors into tensors, without linguistic justification of such transformations. Further, there are few works that extend the termdocument matrix with metadata as a third dimension (Chew et al., 2007; Sun et al., 2006). Turney (2007) is one of the few to study the application of tensors to word space models. However, the emphasis in that paper is more on the evaluation of different tensor decomposition models for such spaces than on the formal model of text representation in three dimensions. Van de Cruys (2009) suggests a three-way model of co-occurrence similar to ours. In contrast to Van de Cruys (2009), we are not using any explicit syntactic preprocessing. Furthermore, our focus is more on the model itself as a general model of meaning. 6 Summary and Future Work In this paper, we propose a novel approa</context>
</contexts>
<marker>Sun, Tao, Faloutsos, 2006</marker>
<rawString>J. Sun, D. Tao, and C. Faloutsos. 2006. Beyond streams and graphs: Dynamic tensor analysis. In Proc. KDD’06, pages 374–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tucker</author>
</authors>
<title>Some mathematical notes on threemode factor analysis.</title>
<date>1966</date>
<journal>Psychometrika,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="4535" citStr="Tucker, 1966" startWordPosition="745" endWordPosition="746">le (or array) carrying real numbers as entries. Thereby n1, ... , nd determine the extension of the array in the different directions. Obviously, matrices can be conceived as n1xn2-tensors and vectors as n1-tensors. In our setting, we will work with tensors where d = 3 and for the sake of better understandability we will introduce the necessary notions for this case only. Our work employs higher-order singular value decomposition (HOSVD), which generalizes the method of singular value decomposition (SVD) from matrices to arbitrary tensors. Given an n1xn2xn3 tensor T, its Tucker decomposition (Tucker, 1966) for given natural numbers m1, m2, m3 consists of an m1xm2xm3 tensor G and three matrices A, B, and C of formats n1xm1, n2xm2, and n3xm3, respectively, such that T(i,j,k) = G(r, s, t)·A(i, r)·B(j, s)·C(k, t). The idea here is to represent the large-size tensor T by the smaller “core” tensor G. The matrices A, B, and C can be seen as linear transformations “compressing” input vectors from dimension ni into dimension mi. Note that a precise representation of T is not always possible. Rather one may attempt to approximate T as well as possible, i.e. find the tensor T&apos; for which a Tucker decomposi</context>
</contexts>
<marker>Tucker, 1966</marker>
<rawString>L.R. Tucker. 1966. Some mathematical notes on threemode factor analysis. Psychometrika, 31(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="8006" citStr="Turney, 2006" startWordPosition="1378" endWordPosition="1379">tion dimensions and unfeasible for large datasets. However, new memory efficient tensor decomposition algorithms have been proposed in the meantime. Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version 2.3. Rendle and Schmidt-Thieme (2010) present a new factorization method with linear complexity. 4 Evaluation Issues 4.1 Task Vector-based distributional similarity methods have proven to be a valuable tool for a number of tasks on automatic discovery of semantic relatedness between words, like synonymy tests (Rapp, 2003) or detection of analogical similarity (Turney, 2006). A somewhat related task is the task of finding out to what extent (statistical) similarity measures correlate with free word associations2. Furthermore, this task was suggested as a shared task for the evaluation of word space models at Lexical Semantics Workshop at ESSLLI 2008. Free associations are the words that come to the mind of a native speaker when he or she is presented with a so-called stimulus word. The percent of test subjects that produce certain response to a given stimulus determines the degree of a free association between a stimulus and a response. Despite the widespread usa</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Empirical evaluation of four tensor decomposition algorithms.</title>
<date>2007</date>
<tech>Technical report. Technical Report ERB-1152.</tech>
<contexts>
<context position="7309" citStr="Turney (2007)" startWordPosition="1275" endWordPosition="1276">0 0 SLOWLY 0 0 0 0 0 Table 1: Slices of T for the terms KICK (i = 3) and BALL (i = 4). Similarly to traditional vector-based distributional models, dimensionality reduction needs to be performed in three dimensions either, as the resulting tensor is very sparse (see the examples of KICK and M1 E r=1 M2 E s=1 M3 E t=1 24 BALL). To this end, we employ Tucker decomposition for 3 dimensions as introduced in Section 2. For this, Matlab Tensor Toolbox1 (Bader and Kolda, 2006) is used. A detailed overview of computational complexity of Tucker decomposition algorithms in Tensor Toolbox is provided in Turney (2007). The drawback of those is that their complexity is cubic in the number of factorization dimensions and unfeasible for large datasets. However, new memory efficient tensor decomposition algorithms have been proposed in the meantime. Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version 2.3. Rendle and Schmidt-Thieme (2010) present a new factorization method with linear complexity. 4 Evaluation Issues 4.1 Task Vector-based distributional similarity methods have proven to be a valuable tool for a number of tasks on automatic discovery of semantic relatedness bet</context>
<context position="17421" citStr="Turney (2007)" startWordPosition="2943" endWordPosition="2944">) suggest a 3-dimensional representation for documents and evaluate the model on the task of document clustering. The above as well as a couple of other projects in this area in information retrieval community leave open the question of how to convey text into a threedimensional tensor. They still use vector-based representation as the basis and then just mathematically convert vectors into tensors, without linguistic justification of such transformations. Further, there are few works that extend the termdocument matrix with metadata as a third dimension (Chew et al., 2007; Sun et al., 2006). Turney (2007) is one of the few to study the application of tensors to word space models. However, the emphasis in that paper is more on the evaluation of different tensor decomposition models for such spaces than on the formal model of text representation in three dimensions. Van de Cruys (2009) suggests a three-way model of co-occurrence similar to ours. In contrast to Van de Cruys (2009), we are not using any explicit syntactic preprocessing. Furthermore, our focus is more on the model itself as a general model of meaning. 6 Summary and Future Work In this paper, we propose a novel approach to text repr</context>
</contexts>
<marker>Turney, 2007</marker>
<rawString>P. Turney. 2007. Empirical evaluation of four tensor decomposition algorithms. Technical report. Technical Report ERB-1152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2009</date>
<booktitle>In GEMS ’09: Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>83--90</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<marker>Van de Cruys, 2009</marker>
<rawString>Tim Van de Cruys. 2009. A non-negative tensor factorization model for selectional preference induction. In GEMS ’09: Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 83–90, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tonio Wandmacher</author>
<author>Ekaterina Ovchinnikova</author>
<author>Theodore Alexandrov</author>
</authors>
<title>Does Latent Semantic Analysis reflect human associations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Lexical Semantics workshop at ESSLLI,</booktitle>
<location>Hamburg, Germany.</location>
<contexts>
<context position="8795" citStr="Wandmacher et al. (2008)" startWordPosition="1508" endWordPosition="1512">was suggested as a shared task for the evaluation of word space models at Lexical Semantics Workshop at ESSLLI 2008. Free associations are the words that come to the mind of a native speaker when he or she is presented with a so-called stimulus word. The percent of test subjects that produce certain response to a given stimulus determines the degree of a free association between a stimulus and a response. Despite the widespread usage of vector-based models to retrieve semantically similar words, it is still rather unclear what type of linguistic phenomena they model (cf. Heylen et al. (2008), Wandmacher et al. (2008)). The same is true for free associations. There are a number of relations according to which a word may be associated with another 1Version 2.3 2One of the reasons to choose this evaluation setting was that the dataset for free word associations task is freely available at http://wordspace.collocations.de/doku.php/data:esslli2008:start (in contrast to, e.g., the synonymy test set). word. For example, Aitchison (2003) distinguishes four types of associations: co-ordination, collocation, superordination and synonymy. This affords an opportunity to use the task of free associations as a “baselin</context>
<context position="12361" citStr="Wandmacher et al. (2008)" startWordPosition="2106" endWordPosition="2109">how the resulting accuracies6 for training and test sets. th denotes cosine threshold values that were used for grouping the results. Here, th is taken to be the function of the size s of the data set. Thus, given a training set of size s = 60 and 3 classes, we define an “equally distributed” threshold th, = 60/3 = 20 (s. Table 2) and a “linearly growing” threshold the = ,�, ,�, rest (s. Table 3). It is not quite apparent, how the threshold for differentiating between the groups should be determined under given conditions. Usually, such measures are defined on the basis of training data (e.g. Wandmacher et al. (2008)). It was not applicable in our case as, due to the current implementation of the model as well as insufficient computational resources for the time being, we could not build one big model for all experiment iterations. Also, the intuition we have gained with this kind of thresholds is that as soon as you change the underlying corpus or the model parameters, you may need to define new thresholds (cf. Tables 2 and 3). ited processing power we had at our disposal at the moment the experiments were conducted. With this step, we considerably reduced the size of the corpus and guaranteed a certain </context>
<context position="14370" citStr="Wandmacher et al. (2008)" startWordPosition="2447" endWordPosition="2450">he more accurate is getting the model for the FIRST associations. For example, with a threshold of th = 0.2 for the test set - 4 out of 5 highest ranked pairs were highly associated (FIRST) and the fifth pair was from the HAPAX group. For HAPAX word associations, no similar regularities could be observed. The resulting accuracies may seem to be poor at this stage. However, it is worth mentioning that this is a highly difficult and corpus-dependent task for automatic processing. The reported results have been obtained based on very small corpora, containing ca. 100 sentences per iteration (cf. Wandmacher et al. (2008) use a corpus of 108 million words to train their LSA-Model). Consequently, it is not possible to compare both results directly, as they have been produced under very different conditions. 5 Related Work 5.1 Matrix Approaches There have been a number of efforts to integrate syntax into vector-based models with alternating success. Some used (dependency) parsing to feed the models (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007); the others utilized only part of speech information, e.g., Widdows (2003). In many cases, these syntactically enhanced models improved the performance (Grefenst</context>
</contexts>
<marker>Wandmacher, Ovchinnikova, Alexandrov, 2008</marker>
<rawString>Tonio Wandmacher, Ekaterina Ovchinnikova, and Theodore Alexandrov. 2008. Does Latent Semantic Analysis reflect human associations. In Proceedings of the Lexical Semantics workshop at ESSLLI, Hamburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL’03,</booktitle>
<pages>197--204</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="14883" citStr="Widdows (2003)" startWordPosition="2532" endWordPosition="2533">based on very small corpora, containing ca. 100 sentences per iteration (cf. Wandmacher et al. (2008) use a corpus of 108 million words to train their LSA-Model). Consequently, it is not possible to compare both results directly, as they have been produced under very different conditions. 5 Related Work 5.1 Matrix Approaches There have been a number of efforts to integrate syntax into vector-based models with alternating success. Some used (dependency) parsing to feed the models (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007); the others utilized only part of speech information, e.g., Widdows (2003). In many cases, these syntactically enhanced models improved the performance (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). Sometimes, however, rather controversial results were observed. Thus, Widdows (2003) reported both positive and negative effects for the task of developing taxonomies. On the one side, POS information increased the performance for common nouns; on the other side, it degraded the outcome for proper nouns 26 TRAIN TEST FIRST 12/20 (60%) (th = 0.022) 25/74 (33%) (th = 0.078)) HAPAX 7/20 (35%) (th = 0.008) 35/74 (47%) th = 0.042) RANDOM 8/20 (40%) 23/74 (31%) TOTAL</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. 2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In Proceedings of NAACL’03, pages 197–204. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>