<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009229">
<title confidence="0.926466">
Hidden Markov Tree Model in Dependency-based Machine Translation∗
</title>
<author confidence="0.928562">
Zdenˇek ˇZabokrtsk´y
</author>
<affiliation confidence="0.9423825">
Charles University in Prague
Institute of Formal and Applied Linguistics
</affiliation>
<email confidence="0.965518">
zabokrtsky@ufal.mff.cuni.cz
</email>
<author confidence="0.950484">
Martin Popel
</author>
<affiliation confidence="0.789688">
Charles University in Prague
Institute of Formal and Applied Linguistics
</affiliation>
<email confidence="0.971174">
popel@matfyz.cz
</email>
<sectionHeader confidence="0.993237" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974052631579">
We would like to draw attention to Hid-
den Markov Tree Models (HMTM), which
are to our knowledge still unexploited in
the field of Computational Linguistics, in
spite of highly successful Hidden Markov
(Chain) Models. In dependency trees,
the independence assumptions made by
HMTM correspond to the intuition of lin-
guistic dependency. Therefore we suggest
to use HMTM and tree-modified Viterbi
algorithm for tasks interpretable as label-
ing nodes of dependency trees. In par-
ticular, we show that the transfer phase
in a Machine Translation system based
on tectogrammatical dependency trees can
be seen as a task suitable for HMTM.
When using the HMTM approach for
the English-Czech translation, we reach a
moderate improvement over the baseline.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.971706175438597">
Hidden Markov Tree Models (HMTM) were intro-
duced in (Crouse et al., 1998) and used in appli-
cations such as image segmentation, signal classi-
fication, denoising, and image document catego-
rization, see (Durand et al., 2004) for references.
Although Hidden Markov Models belong to the
most successful techniques in Computational Lin-
guistics (CL), the HMTM modification remains to
the best of our knowledge unknown in the field.
The first novel claim made in this paper is that
the independence assumptions made by Markov
Tree Models can be useful for modeling syntactic
trees. Especially, they fit dependency trees well,
because these models assume conditional depen-
dence (in the probabilistic sense) only along tree
∗ The work on this project was supported by the grants
MSM 0021620838, GAAV ˇCR 1ET101120503, and MˇSMT
ˇCR LC536. We thank Jan Hajiˇc and three anonymous review-
ers for many useful comments.
edges, which corresponds to intuition behind de-
pendency relations (in the linguistic sense) in de-
pendency trees. Moreover, analogously to applica-
tions of HMM on sequence labeling, HMTM can
be used for labeling nodes of a dependency tree,
interpreted as revealing the hidden states1 in the
tree nodes, given another (observable) labeling of
the nodes of the same tree.
The second novel claim is that HMTMs are
suitable for modeling the transfer phase in Ma-
chine Translation systems based on deep-syntactic
dependency trees. Emission probabilities rep-
resent the translation model, whereas transition
(edge) probabilities represent the target-language
tree model. This decomposition can be seen as
a tree-shaped analogy to the popular n-gram ap-
proaches to Statistical Machine Translation (e.g.
(Koehn et al., 2003)), in which translation and lan-
guage models are trainable separately too. More-
over, given the input dependency tree and HMTM
parameters, there is a computationally efficient
HMTM-modified Viterbi algorithm for finding the
globally optimal target dependency tree.
It should be noted that when using HMTM, the
source-language and target-language trees are re-
quired to be isomorphic. Obviously, this is an un-
realistic assumption in real translation. However,
we argue that tectogrammatical deep-syntactic de-
pendency trees (as introduced in the Functional
Generative Description framework, (Sgall, 1967))
are relatively close to this requirement, which
makes the HMTM approach practically testable.
As for the related work, one can found a num-
ber of experiments with dependency-based MT
in the literature, e.g., (Boguslavsky et al., 2004),
(Menezes and Richardson, 2001), (Bojar, 2008).
However, to our knowledge none of the published
systems searches for the optimal target representa-
</bodyText>
<footnote confidence="0.947516">
1HMTM looses the HMM’s time and finite automaton in-
terpretability, as the observations are not organized linearly.
However, the terms “state” and “transition” are still used.
</footnote>
<page confidence="0.92559">
145
</page>
<note confidence="0.868369666666667">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 145–148,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
SOURCE-LANGUAGE T-TREE TARGET-LANGUAGE T-TREE
</note>
<figureCaption confidence="0.999551">
Figure 1: Tectogrammatical transfer as a task for HMTM.
</figureCaption>
<figure confidence="0.988584">
X(v)
transition probs.
P(X(U) IX(v))
from tree model
hidden states
(node translation
hypotheses)
optimal state tree
revealed by tree Viterbi
TRANSFER
X(U)
Y(U)
emission probs.
P(Y(U) IX(U))
from backward
translation model
</figure>
<bodyText confidence="0.719809">
tion in a way similar to HMTM.
</bodyText>
<sectionHeader confidence="0.924929" genericHeader="method">
2 Hidden Markov Tree Models
</sectionHeader>
<bodyText confidence="0.928737285714286">
HMTM are described very briefly in this section.
More detailed information can be found in (Du-
rand et al., 2004) and in (Diligenti et al., 2003).
Suppose that V = {v1,... , v|V |} is the set of
tree nodes, r is the root node and p is a function
from V \r to V storing the parent node of each
non-root node. Suppose two sequences of ran-
dom variables, X = (X(v1), ... , X(v|V |)) and
Y = (Y (v1), ... ,Y (v|V |)), which label all nodes
from V . Let X(v) be understood as a hidden state
of the node v, taking a value from a finite state
space S = {s1, ... , sK}. Let Y (v) be understood
as a symbol observable on the node v, taking
a value from an alphabet K = {k1, ... , k2}.
Analogously to (first-order) HMMs, (first-order)
HMTMs make two independence assumptions:
(1) given X(p(v)), X(v) is conditionally inde-
pendent of any other nodes, and (2) given X(v),
Y (v) is conditionally independent of any other
nodes. Given these independence assumptions,
the following factorization formula holds:2
</bodyText>
<equation confidence="0.969841666666667">
P(Y ,X) = P(Y (r)|X(r))P(X(r)) ·
H P(Y (v)|X(v))P(X(v)|X(p(v))) (1)
vEV\r
</equation>
<bodyText confidence="0.9950855">
We see that HMTM (analogously to HMM,
again) is defined by the following parameters:
</bodyText>
<footnote confidence="0.904858">
2In this work we limit ourselves to fully stationary
HMTMs. This means that the transition and emission prob-
abilities are independent of v. This “node invariance” is an
analogy to HMM’s time invariance.
</footnote>
<listItem confidence="0.9999255">
• P(X(v)|X(p(v))) – transition probabilities
between the hidden states of two tree-
adjacent nodes,3
• P(Y (v)|X(v)) – emission probabilities.
</listItem>
<bodyText confidence="0.996368166666667">
Naturally the question appears how to restore
the most probable hidden tree labeling given the
observed tree labeling (and given the tree topol-
ogy, of course). As shown in (Durand et al., 2004),
a modification of the HMM Viterbi algorithm can
be found for HMTM. Briefly, the algorithm starts
at leaf nodes and continues upwards, storing in
each node for each state and each its child the op-
timal downward pointer to the child’s hidden state.
When the root is reached, the optimal state tree is
retrieved by downward recursion along the point-
ers from the optimal root state.
</bodyText>
<sectionHeader confidence="0.904555" genericHeader="method">
3 Tree Transfer as a Task for HMTM
</sectionHeader>
<bodyText confidence="0.995366166666667">
HMTM Assumptions from the MT Viewpoint.
We suggest to use HMTM in the conventional
tree-based analysis-transfer-synthesis translation
scheme: (1) First we analyze an input sentence to
a certain level of abstraction on which the sentence
representation is tree-shaped. (2) Then we use
HMTM-modified Viterbi algorithm for creating
the target-language tree from the source-language
tree. Labels on the source-language nodes are
treated as emitted (observable) symbols, while la-
bels on the target-language nodes are understood
as hidden states which are being searched for
</bodyText>
<footnote confidence="0.960099666666667">
3The need for parametrizing also P(X(r)) (prior proba-
bilites of hidden states in the root node) can be avoided by
adding an artificial root whose state is fixed.
</footnote>
<page confidence="0.996723">
146
</page>
<bodyText confidence="0.935656858490566">
(Figure 1). (3) Finally, we synthesize the target-
language sentence from the target-language tree.
In the HMTM transfer step, the HMTM emis-
sion probabilities can be interpreted as probabil-
ities from the “backward” (source given target)
node-to-node translation model. HMTM transi-
tion probabilities can be interpreted as probabil-
ities from the target-language tree model. This is
an important feature from the MT viewpoint, since
the decomposition into translation model and lan-
guage model proved to be extremely useful in sta-
tistical MT since (Brown et al., 1993). It allows
to compensate the lack of parallel resources by the
relative abundance of monolingual resources.
Another advantage of the HMTM approach is
that it allows us to disregard the ordering of de-
cisions made with the individual nodes (which
would be otherwise nontrivial, as for a given node
there might be constraints and preferences coming
both from its parent and from its children). Like in
HMM, it is the notion of hidden states that facil-
itates “summarizing” distributed information and
finding the global optimum.
On the other hand, there are several limitations
implied by HMTMs which we have to consider be-
fore applying it to MT: (1) There can be only one
labeling function on the source-language nodes,
and one labeling function on the target-language
nodes. (2) The set of hidden states and the al-
phabet of emitted symbols must be finite. (3) The
source-language tree and the target-language tree
are required to be isomorphic. In other words, only
node labeling can be changed in the transfer step.
The first two assumption are easy to fulfill, but
the third assumption concerning the tree isomor-
phism is problematic. There is no known linguistic
theory guaranteeing identically shaped tree repre-
sentations of a sentence and its translation. How-
ever, we would like to show in the following that
the tectogrammatical layer of language description
is close enough to this ideal to make the HMTM
approach practically applicable.
Why Tectogrammatical Trees? Tectogram-
matical layer of language description was
introduced within the Functional Generative
Description framework, (Sgall, 1967) and has
been further elaborated in the Prague Dependency
Treebank project, (Hajiˇc and others, 2006).
On the tectogrammatical layer, each sentence is
represented as a tectogrammatical tree (t-tree for
short; abbreviations t-node and t-layer are used in
the further text too). The main features of t-trees
(from the viewpoint of our experiments) are fol-
lowing. Each sentence is represented as a depen-
dency tree, whose nodes correspond to autoseman-
tic (meaningful) words and whose edges corre-
spond to syntactic-semantic relations (dependen-
cies). The nodes are labeled with the lemmas of
the autosemantic words. Functional words (such
as prepositions, auxiliary verbs, and subordinat-
ing conjunctions) do not have nodes of their own.
Information conveyed by word inflection or func-
tional words in the surface sentence shape is repre-
sented by specialized semantic attributes attached
to t-nodes (such as number or tense).
T-trees are still language specific (e.g. be-
cause of lemmas), but they largely abstract from
language-specific means of expressing non-lexical
meanings (such as inflection, agglutination, func-
tional words). Next reason for using t-trees as the
transfer medium is that they allow for a natural
transfer factorization. One can separate the trans-
fer into three relatively independent channels:4 (1)
transfer of lexicalization (stored in t-node’s lemma
attribute), (2) transfer of syntactizations (stored
in t-node’s formeme attribute),5 and (3) transfer
of semantically indispensable grammatical cate-
gories6 such as number with nouns and tense with
verbs (stored in specialized t-node’s attributes).
Another motivation for using t-trees is that
we believe that local tree contexts in t-trees
carry more information relevant for correct lexical
choice, compared to linear contexts in the surface
sentence shapes, mainly because of long-distance
dependencies and coordination structures.
Observed Symbols, Hidden States, and HMTM
Parameters. The most difficult part of the
tectogrammatical transfer step lies in transfer-
4Full independence assumption about the three channels
would be inadequate, but it can be at least used for smoothing
the translation probabilities.
5Under the term syntactization (the second channel) we
understand morphosyntactic form – how the given lemma is
“shaped” on the surface. We use the t-node attribute formeme
(which is not a genuine element of the semantically ori-
ented t-layer, but rather only a technical means that facili-
tates modeling the transition between t-trees and surface sen-
tence shapes) to capture syntactization of the given t-node,
with values such as n:subj – semantic noun (s.n.) in sub-
ject position, n:for+X – s.n. with preposition for, n:poss –
possessive form of s.n., v:because+fin – semantic verb as a
subordinating finite clause introduced by because), adj:attr –
semantic adjective in attributive position.
6Categories only imposed by grammatical constraints
(e.g. grammatical number with verbs imposed by subject-
verb agreement in Czech) are disregarded on the t-layer.
</bodyText>
<page confidence="0.996494">
147
</page>
<bodyText confidence="0.999931375">
ring lexicalization and syntactization (attributes
lemma and formeme), while the other attributes
(node ordering, grammatical number, gender,
tense, person, negation, degree of comparison
etc.) can be transferred by much less complex
methods. As there can be only one input labeling
function, we treat the following ordered pair as
the observed symbol: Y (v) _ (Lsrc(v), Fsrc(v))
where Lsrc(v) is the source-language lemma of
the node v and Fsrc(v) is its source-language
formeme. Analogously, hidden state of node v is
the ordered couple X(v) _ (Ltr9(v),Ftr9(v)),
where Ltr9(v) is the target-language lemma of
the node v and Ftr9(v) is its target-language
formeme. Parameters of such HMTM are then
following:
</bodyText>
<equation confidence="0.982617">
P(X(v)|X(ρ(v))) = P(Ltrg(v), Ftrg(v)|Ltrg(ρ(v)), Ftrg(ρ(v)))
</equation>
<bodyText confidence="0.999146333333333">
– probability of a node labeling given its parent
labeling; it can be estimated from a parsed
target-language monolingual corpus, and
</bodyText>
<equation confidence="0.901296">
P(Y (v)|X(v)) = P(Lsrc(v), F src(v)|Ltrg(v), Ftrg(v))
</equation>
<bodyText confidence="0.999645545454545">
– backward translation probability; it can be esti-
mated from a parsed and aligned parallel corpus.
To summarize: the task of tectogrammatical
transfer can be formulated as revealing the values
of node labeling functions Ltr9 and Ftr9 given the
tree topology and given the values of node label-
ing functions Lsrc and Fsrc. Given the HMTM
parameters specified above, the task can be solved
using HMTM-modified Viterbi algorithm by inter-
preting the first pair as the hidden state and the
second pair as the observation.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.9998229">
To check the real applicability of HMTM transfer,
we performed the following preliminary MT ex-
periment. First, we used the tectogrammar-based
MT system described in (ˇZabokrtsk´y et al., 2008)
as a baseline.7 Then we substituted its transfer
phase by the HMTM variant, with parameters esti-
mated from 800 million word Czech corpus and 60
million word parallel corpus. As shown in Table 1,
the HMTM approach outperforms the baseline so-
lution both in terms of BLEU and NIST metrics.
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.8149135">
HMTM is a new approach in the field of CL. In our
opinion, it has a big potential for modeling syntac-
</bodyText>
<footnote confidence="0.995168666666667">
7For evaluation purposes we used 2700 sentences from
the evaluation section of WMT 2009 Shared Translation
Task. http://www.statmt.org/wmt09/
</footnote>
<table confidence="0.972822">
System BLEU NIST
baseline system 0.0898 4.5672
HMTM modification 0.1043 4.8445
</table>
<tableCaption confidence="0.999815">
Table 1: Evaluation of English-Czech translation.
</tableCaption>
<bodyText confidence="0.999893">
tic trees. To show how it can be used, we applied
HMTM in an experiment on English-Czech tree-
based Machine Translation and reached an im-
provement over the solution without HMTM.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999083585365854">
Igor Boguslavsky, Leonid Iomdin, and Victor Sizov.
2004. Multilinguality in ETAP-3: Reuse of Lexi-
cal Resources. In Proceedings of Workshop Multi-
lingual Linguistic Resources, COLING, pages 7–14.
Ondˇrej Bojar. 2008. Exploiting Linguistic Data in Ma-
chine Translation. Ph.D. thesis, ´UFAL, MFF UK,
Prague, Czech Republic.
Peter E. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics.
Matthew Crouse, Robert Nowak, and Richard Bara-
niuk. 1998. Wavelet-based statistical signal pro-
cessing using hidden markov models. IEEE Trans-
actions on Signal Processing, 46(4):886–902.
Michelangelo Diligenti, Paolo Frasconi, and Marco
Gori. 2003. Hidden tree Markov models for doc-
ument image classification. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 25:2003.
Jean-Baptiste Durand, Paulo Goncalv`es, and Yann
Gu´edon. 2004. Computational methods for hid-
den Markov tree models - An application to wavelet
trees. IEEE Transactions on Signal Processing,
52(9):2551–2560.
Jan Hajiˇc et al. 2006. Prague Dependency Treebank
2.0. Linguistic Data Consortium, LDC Catalog No.:
LDC2006T01, Philadelphia.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase based translation. In Pro-
ceedings of the HLT/NAACL.
Arul Menezes and Stephen D. Richardson. 2001. A
best-first alignment algorithm for automatic extrac-
tion of transfer mappings from bilingual corpora. In
Proceedings of the workshop on Data-driven meth-
ods in machine translation, volume 14, pages 1–8.
Petr Sgall. 1967. Generativnipopis jazyka a ˇcesk´a
deklinace. Academia, Prague, Czech Republic.
Zdenˇek ˇZabokrtsk´y, Jan Pt´aˇcek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on SMT, ACL.
</reference>
<page confidence="0.996721">
148
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.320298">
<title confidence="0.996071">Markov Tree Model in Dependency-based Machine</title>
<affiliation confidence="0.6830075">Charles University in Prague Institute of Formal and Applied Linguistics</affiliation>
<email confidence="0.707277">zabokrtsky@ufal.mff.cuni.cz</email>
<author confidence="0.991686">Martin Popel</author>
<affiliation confidence="0.9831325">Charles University in Prague Institute of Formal and Applied Linguistics</affiliation>
<email confidence="0.773448">popel@matfyz.cz</email>
<abstract confidence="0.99958265">We would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees. In particular, we show that the transfer phase in a Machine Translation system based on tectogrammatical dependency trees can be seen as a task suitable for HMTM. When using the HMTM approach for the English-Czech translation, we reach a moderate improvement over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Igor Boguslavsky</author>
<author>Leonid Iomdin</author>
<author>Victor Sizov</author>
</authors>
<title>Multilinguality in ETAP-3: Reuse of Lexical Resources.</title>
<date>2004</date>
<booktitle>In Proceedings of Workshop Multilingual Linguistic Resources, COLING,</booktitle>
<pages>7--14</pages>
<contexts>
<context position="3608" citStr="Boguslavsky et al., 2004" startWordPosition="540" endWordPosition="543"> finding the globally optimal target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable. As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representa1HMTM looses the HMM’s time and finite automaton interpretability, as the observations are not organized linearly. However, the terms “state” and “transition” are still used. 145 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 145–148, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP SOURCE-LANGUAGE T-TREE TARGET-LANGUAGE T-TREE Figure 1: Tectogrammatical transfer as a task for HMTM. X(v) transition probs. P(X(U) IX(v)) from </context>
</contexts>
<marker>Boguslavsky, Iomdin, Sizov, 2004</marker>
<rawString>Igor Boguslavsky, Leonid Iomdin, and Victor Sizov. 2004. Multilinguality in ETAP-3: Reuse of Lexical Resources. In Proceedings of Workshop Multilingual Linguistic Resources, COLING, pages 7–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
</authors>
<title>Exploiting Linguistic Data</title>
<date>2008</date>
<booktitle>in Machine Translation. Ph.D. thesis, ´UFAL, MFF UK,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3655" citStr="Bojar, 2008" startWordPosition="548" endWordPosition="549">uld be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable. As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representa1HMTM looses the HMM’s time and finite automaton interpretability, as the observations are not organized linearly. However, the terms “state” and “transition” are still used. 145 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 145–148, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP SOURCE-LANGUAGE T-TREE TARGET-LANGUAGE T-TREE Figure 1: Tectogrammatical transfer as a task for HMTM. X(v) transition probs. P(X(U) IX(v)) from tree model hidden states (node translation hypo</context>
</contexts>
<marker>Bojar, 2008</marker>
<rawString>Ondˇrej Bojar. 2008. Exploiting Linguistic Data in Machine Translation. Ph.D. thesis, ´UFAL, MFF UK, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="7850" citStr="Brown et al., 1993" startWordPosition="1234" endWordPosition="1237">d by adding an artificial root whose state is fixed. 146 (Figure 1). (3) Finally, we synthesize the targetlanguage sentence from the target-language tree. In the HMTM transfer step, the HMTM emission probabilities can be interpreted as probabilities from the “backward” (source given target) node-to-node translation model. HMTM transition probabilities can be interpreted as probabilities from the target-language tree model. This is an important feature from the MT viewpoint, since the decomposition into translation model and language model proved to be extremely useful in statistical MT since (Brown et al., 1993). It allows to compensate the lack of parallel resources by the relative abundance of monolingual resources. Another advantage of the HMTM approach is that it allows us to disregard the ordering of decisions made with the individual nodes (which would be otherwise nontrivial, as for a given node there might be constraints and preferences coming both from its parent and from its children). Like in HMM, it is the notion of hidden states that facilitates “summarizing” distributed information and finding the global optimum. On the other hand, there are several limitations implied by HMTMs which we</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter E. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Crouse</author>
<author>Robert Nowak</author>
<author>Richard Baraniuk</author>
</authors>
<title>Wavelet-based statistical signal processing using hidden markov models.</title>
<date>1998</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>46</volume>
<issue>4</issue>
<contexts>
<context position="1131" citStr="Crouse et al., 1998" startWordPosition="163" endWordPosition="166">els. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees. In particular, we show that the transfer phase in a Machine Translation system based on tectogrammatical dependency trees can be seen as a task suitable for HMTM. When using the HMTM approach for the English-Czech translation, we reach a moderate improvement over the baseline. 1 Introduction Hidden Markov Tree Models (HMTM) were introduced in (Crouse et al., 1998) and used in applications such as image segmentation, signal classification, denoising, and image document categorization, see (Durand et al., 2004) for references. Although Hidden Markov Models belong to the most successful techniques in Computational Linguistics (CL), the HMTM modification remains to the best of our knowledge unknown in the field. The first novel claim made in this paper is that the independence assumptions made by Markov Tree Models can be useful for modeling syntactic trees. Especially, they fit dependency trees well, because these models assume conditional dependence (in </context>
</contexts>
<marker>Crouse, Nowak, Baraniuk, 1998</marker>
<rawString>Matthew Crouse, Robert Nowak, and Richard Baraniuk. 1998. Wavelet-based statistical signal processing using hidden markov models. IEEE Transactions on Signal Processing, 46(4):886–902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelangelo Diligenti</author>
<author>Paolo Frasconi</author>
<author>Marco Gori</author>
</authors>
<title>Hidden tree Markov models for document image classification.</title>
<date>2003</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>25--2003</pages>
<contexts>
<context position="4591" citStr="Diligenti et al., 2003" startWordPosition="689" endWordPosition="692"> Conference Short Papers, pages 145–148, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP SOURCE-LANGUAGE T-TREE TARGET-LANGUAGE T-TREE Figure 1: Tectogrammatical transfer as a task for HMTM. X(v) transition probs. P(X(U) IX(v)) from tree model hidden states (node translation hypotheses) optimal state tree revealed by tree Viterbi TRANSFER X(U) Y(U) emission probs. P(Y(U) IX(U)) from backward translation model tion in a way similar to HMTM. 2 Hidden Markov Tree Models HMTM are described very briefly in this section. More detailed information can be found in (Durand et al., 2004) and in (Diligenti et al., 2003). Suppose that V = {v1,... , v|V |} is the set of tree nodes, r is the root node and p is a function from V \r to V storing the parent node of each non-root node. Suppose two sequences of random variables, X = (X(v1), ... , X(v|V |)) and Y = (Y (v1), ... ,Y (v|V |)), which label all nodes from V . Let X(v) be understood as a hidden state of the node v, taking a value from a finite state space S = {s1, ... , sK}. Let Y (v) be understood as a symbol observable on the node v, taking a value from an alphabet K = {k1, ... , k2}. Analogously to (first-order) HMMs, (first-order) HMTMs make two indepe</context>
</contexts>
<marker>Diligenti, Frasconi, Gori, 2003</marker>
<rawString>Michelangelo Diligenti, Paolo Frasconi, and Marco Gori. 2003. Hidden tree Markov models for document image classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Durand</author>
<author>Paulo Goncalv`es</author>
<author>Yann Gu´edon</author>
</authors>
<title>Computational methods for hidden Markov tree models - An application to wavelet trees.</title>
<date>2004</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>52</volume>
<issue>9</issue>
<marker>Durand, Goncalv`es, Gu´edon, 2004</marker>
<rawString>Jean-Baptiste Durand, Paulo Goncalv`es, and Yann Gu´edon. 2004. Computational methods for hidden Markov tree models - An application to wavelet trees. IEEE Transactions on Signal Processing, 52(9):2551–2560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank 2.0. Linguistic Data Consortium, LDC Catalog No.: LDC2006T01,</booktitle>
<location>Philadelphia.</location>
<marker>Hajiˇc, 2006</marker>
<rawString>Jan Hajiˇc et al. 2006. Prague Dependency Treebank 2.0. Linguistic Data Consortium, LDC Catalog No.: LDC2006T01, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL.</booktitle>
<contexts>
<context position="2774" citStr="Koehn et al., 2003" startWordPosition="418" endWordPosition="421">used for labeling nodes of a dependency tree, interpreted as revealing the hidden states1 in the tree nodes, given another (observable) labeling of the nodes of the same tree. The second novel claim is that HMTMs are suitable for modeling the transfer phase in Machine Translation systems based on deep-syntactic dependency trees. Emission probabilities represent the translation model, whereas transition (edge) probabilities represent the target-language tree model. This decomposition can be seen as a tree-shaped analogy to the popular n-gram approaches to Statistical Machine Translation (e.g. (Koehn et al., 2003)), in which translation and language models are trainable separately too. Moreover, given the input dependency tree and HMTM parameters, there is a computationally efficient HMTM-modified Viterbi algorithm for finding the globally optimal target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967))</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase based translation. In Proceedings of the HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Stephen D Richardson</author>
</authors>
<title>A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the workshop on Data-driven methods in machine translation,</booktitle>
<volume>14</volume>
<pages>1--8</pages>
<contexts>
<context position="3640" citStr="Menezes and Richardson, 2001" startWordPosition="544" endWordPosition="547">l target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable. As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representa1HMTM looses the HMM’s time and finite automaton interpretability, as the observations are not organized linearly. However, the terms “state” and “transition” are still used. 145 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 145–148, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP SOURCE-LANGUAGE T-TREE TARGET-LANGUAGE T-TREE Figure 1: Tectogrammatical transfer as a task for HMTM. X(v) transition probs. P(X(U) IX(v)) from tree model hidden states (node t</context>
</contexts>
<marker>Menezes, Richardson, 2001</marker>
<rawString>Arul Menezes and Stephen D. Richardson. 2001. A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora. In Proceedings of the workshop on Data-driven methods in machine translation, volume 14, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>Generativnipopis jazyka a ˇcesk´a deklinace.</title>
<date>1967</date>
<location>Academia, Prague, Czech Republic.</location>
<contexts>
<context position="3373" citStr="Sgall, 1967" startWordPosition="505" endWordPosition="506">et al., 2003)), in which translation and language models are trainable separately too. Moreover, given the input dependency tree and HMTM parameters, there is a computationally efficient HMTM-modified Viterbi algorithm for finding the globally optimal target dependency tree. It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable. As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representa1HMTM looses the HMM’s time and finite automaton interpretability, as the observations are not organized linearly. However, the terms “state” and “transition” are still used. 145 Proceedings of the ACL-IJCNLP 2009 Conf</context>
<context position="9452" citStr="Sgall, 1967" startWordPosition="1491" endWordPosition="1492">ged in the transfer step. The first two assumption are easy to fulfill, but the third assumption concerning the tree isomorphism is problematic. There is no known linguistic theory guaranteeing identically shaped tree representations of a sentence and its translation. However, we would like to show in the following that the tectogrammatical layer of language description is close enough to this ideal to make the HMTM approach practically applicable. Why Tectogrammatical Trees? Tectogrammatical layer of language description was introduced within the Functional Generative Description framework, (Sgall, 1967) and has been further elaborated in the Prague Dependency Treebank project, (Hajiˇc and others, 2006). On the tectogrammatical layer, each sentence is represented as a tectogrammatical tree (t-tree for short; abbreviations t-node and t-layer are used in the further text too). The main features of t-trees (from the viewpoint of our experiments) are following. Each sentence is represented as a dependency tree, whose nodes correspond to autosemantic (meaningful) words and whose edges correspond to syntactic-semantic relations (dependencies). The nodes are labeled with the lemmas of the autosemant</context>
</contexts>
<marker>Sgall, 1967</marker>
<rawString>Petr Sgall. 1967. Generativnipopis jazyka a ˇcesk´a deklinace. Academia, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Jan Pt´aˇcek</author>
<author>Petr Pajas</author>
</authors>
<title>TectoMT: Highly Modular MT System with Tectogrammatics Used as Transfer Layer.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd Workshop on SMT, ACL.</booktitle>
<marker>ˇZabokrtsk´y, Pt´aˇcek, Pajas, 2008</marker>
<rawString>Zdenˇek ˇZabokrtsk´y, Jan Pt´aˇcek, and Petr Pajas. 2008. TectoMT: Highly Modular MT System with Tectogrammatics Used as Transfer Layer. In Proceedings of the 3rd Workshop on SMT, ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>