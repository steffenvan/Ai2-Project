<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000774">
<title confidence="0.990518666666667">
Top-Down Cohesion
Segmentation in
Summarization
</title>
<author confidence="0.898647333333333">
Doina Tatar
Andreea Diana Mihis
Gabriela Serban
</author>
<affiliation confidence="0.938939">
University &amp;quot;Babe¸s-Bolyai&amp;quot; Cluj-Napoca (Romania)
</affiliation>
<email confidence="0.995593">
email: dtatar@cs.ubbcluj.ro
</email>
<sectionHeader confidence="0.989358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952461538461">
The paper proposes a new method of linear text segmentation based on
lexical cohesion of a text. Namely, first a single chain of disambiguated
words in a text is established, then the rips of this single chain are consid-
ered as boundaries for the segments of the cohesion text structure (Cohe-
sion TextTiling or CTT). The summaries of arbitrarily length are obtained
by extraction using three different methods applied to the obtained seg-
ments. The informativeness of the obtained summaries is compared with
the informativeness of the pair summaries of the same length obtained us-
ing an earlier method of logical segmentation by text entailment (Logical
TextTiling or LTT). Some experiments about CTT and LTT methods are
carried out for four “classical&amp;quot; texts in summarization literature showing
that the quality of the summarization using cohesion segmentation (CTT)
is better than the quality using logical segmentation (LTT).
</bodyText>
<page confidence="0.996849">
389
</page>
<note confidence="0.859917">
390 Tatar, Mihis, and Serban
</note>
<sectionHeader confidence="0.994416" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910142857143">
Text summarization has become the subject of an intense research in the last years
and it is still an emerging field (Orasan, 2006; Radev et al., 2002; Hovy, 2003; Mani,
2001). The research is done in the extracts (which we are treating in this paper) and
abstracts areas. The most important task of summarization is to identify the most
informative (salient) parts of a text comparatively with the rest. A good segmentation
of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and
Elhadad, 1999; Reynar, 1998).
This paper proposes a new method of linear text segmentation based on lexical
cohesion of a text. Namely, first a single chain of disambiguated words in a text is
established, then the rips of this chain are considered. These rips are boundaries of the
segments in the cohesion structure of the text. Due to some similarities with TextTiling
algorithm for topic shifts detection of Hearst (1997), the method is called Cohesion
TextTiling (CTT).
The paper is structured as follows: in Section 2 we present the problem of Word
Sense Disambiguation by a chain algorithm and the derived CTT method. In Sec-
tion 3, some notions about textual entailment and logical segmentation of a text by
LTT method are discussed. Summarization by different methods after segmentation
is the topic of Section 4. The parallel application of CTT and LTT methods to four
&amp;quot;classical&amp;quot; texts in summarization literature, two narrative and two newspapers, and
some statistics of the results are presented in Section 5. We finish the article with
conclusions and possible further work directions.
</bodyText>
<sectionHeader confidence="0.881922" genericHeader="method">
2 A top-down cohesion segmentation method
</sectionHeader>
<subsectionHeader confidence="0.98889">
2.1 Lexical chains
</subsectionHeader>
<bodyText confidence="0.999990307692308">
A lexical chain is a sequence of words such that the meaning of each word from the se-
quence can be obtained unambiguously from the meaning of the rest of words (Morris
and Hirst, 1991; Barzilay and Elhadad, 1999; Harabagiu and Moldovan, 1997; Silber
and McCoy, 2002; Stokes, 2004). The map of all lexical chains of a text provides a
representation of the lexical cohesive structure of the text. Usually a lexical chain is
obtained in a bottom-up fashion, by taking each candidate word of a text, and finding
an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or
WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the
appropriate sense in the current chain, and the senses of the other words in the chain
are updated. If no relation is found, then a new chain is initiated.
Our method approaches the construction of lexical chains in a reverse order: we
first disambiguate the whole text and then construct the lexical chains which cover as
much as possible the text.
</bodyText>
<subsectionHeader confidence="0.999238">
2.2 CHAD algorithm
</subsectionHeader>
<bodyText confidence="0.9998538">
It is known that in the last years many researchers studied the possibility to globally
disambiguate a text. In Tatar et al. (2007) is presented CHAD algorithm, a Lesk’s
type algorithm based on WordNet, that doesn’t require syntactic analysis and syntac-
tic parsing. As usually for a Lesk’s type algorithm, it starts from the idea that a word’s
dictionary definition is a good indicator for the senses of this word and uses the defi-
</bodyText>
<subsectionHeader confidence="0.684312">
Top-down Cohesion Segmentation in Summarization 391
</subsectionHeader>
<bodyText confidence="0.999979578947369">
nition in the dictionary directly. The base of the algorithm is the disambiguation of a
triplet of words, using Dice’s overlap or Jaccard’s measures. Shortly, CHAD begins
with the disambiguation of a triplet w1w2w3 and then adds to the right the following
words to be disambiguated. Hence it disambiguates at a time a new triplet, where first
two words are already associated with the best senses and the disambiguation of the
third word depends on the disambiguation of these first two words.
Due to the brevity of definitions in WordNet (WN), the first sense in WN for a word
wi (WN 1st sense) must be associated in some cases in a &amp;quot;forced&amp;quot; way. The forced
condition represents the situation that any sense of wi is related with the senses of the
words wi_2 and wi_1. Thus the forced condition signals that a lexical chain stops,
and, perhaps, a new one begins.
Comparing the precision obtained with CHAD and the precision obtained by the
WN 1st sense algorithm for 10 files of Brown corpus (Tatar et al., 2007) we obtained
the result: for 7 files the difference was greater or equal to 0.04 (favorable to WN 1st),
and for 3 files was lower. For example, in the worst case (Brown 01 file), the precisions
obtained by CHAD are: 0.625 for Dice’s measure, 0.627 for Overlap measure, 0.638
for Jaccard’s measure while the precision obtained by WN 1st sense is 0.688. Let us
remark that CHAD is used to mark the discontinuity in cohesion, while WN 1st sense
algorithm is unable to do this.
</bodyText>
<subsectionHeader confidence="0.997536">
2.3 CHAD and lexical chains
</subsectionHeader>
<bodyText confidence="0.999915666666667">
The CHAD algorithm shows what words in a sentence are unrelated as senses with
the previously words: these are the words which receive a &amp;quot;forced&amp;quot; first WN sense.
Of course, these are regarded differently from the words which receive a &amp;quot;justified&amp;quot;
first WN sense. Scoring each sentence of a text by the number of &amp;quot;forced&amp;quot; to first WN
sense words in this sentence, we will provide a representation of the lexical cohesive
structure of the text. If F is this number, then the valleys (the local minima) in the
graph representing the function 1/F will represent the boundaries between lexical
chains (see Figure 2).
Lexical chains could serve further as a basis for an algorithm of segmentation. As
our method of determination of lexical chains is linear, the corresponding segmenta-
tion is also linear. The obtained segments could be used effectively in summarization.
In this respect, our method of summarization falls in the discourse-based category.
In contrast with other theories about discourse segmentation, as Rhetorical Struc-
ture Theory (RST) of Mann and Thompson (1988), attentional/intentional structure
of Grosz and Sidner (1986) or parsed RST tree of Marcu (1997), our CTT method
(and also, as presented below, our LTT method) supposes a linear segmentation (ver-
sus hierarchical segmentation) which results in an advantage from a computational
viewpoint.
</bodyText>
<sectionHeader confidence="0.886102" genericHeader="method">
3 Segmentation by Logical TextTiling
</sectionHeader>
<subsectionHeader confidence="0.997576">
3.1 Text entailment
</subsectionHeader>
<bodyText confidence="0.99993025">
Text entailment is an autonomous field of Natural Language Processing and it rep-
resents the subject of some recent Pascal Challenges. As is established in an earlier
paper (Tatar et al., 2007), a text T entails an hypothesis H, denoted by T → H, iff H
is less informative than T. A method to prove T → H which relies on this definition
</bodyText>
<note confidence="0.725855">
392 Tatar, Mihis, and Serban
</note>
<bodyText confidence="0.9943334">
consists in the verification of the relation: sim(T,H)T &lt; sim(T,H)H. Here sim(T,H)T
and sim(T,H)H are text-to-text similarities introduced in Corley and Mihalcea (2005).
The method used by our tool for Text entailment verification calculates the similarity
between T and H by cosine, thus the above relation becomes cos(T,H)T &lt; cos(T,H)H
(Tatar et al., 2007).
</bodyText>
<subsectionHeader confidence="0.999712">
3.2 Logical segmentation
</subsectionHeader>
<bodyText confidence="0.999988538461539">
Tatar et al. (2008) present a method named logical segmentation because the score of
a sentence is the number of sentences of the text which are entailed by it. Representing
the scores of sentences as a graph, we obtain a structure which indicates how the most
important sentences alternate with ones less important and which organizes the text
according to its logical content. Simply, a valley (a local minimum) in the obtained
logical structure of the text is a boundary between two logical segments (see Figure 1).
The method is called Logical TextTiling (LTT), due to some similarities with the
TextTiling algorithm for topic shifts detection (Hearst, 1997). The drawback of LTT,
that the number of the segments is fixed for a given text (as it results from its logical
structure), is eliminated by a method to dynamically correlate the number of the logi-
cal segments obtained by LTT with the required length of the summary. Let us remark
that LTT does not require a predicate-argument analysis. The only semantic structure
processing required is the Text Entailment verification.
</bodyText>
<sectionHeader confidence="0.950157" genericHeader="method">
4 Summarization by segmentation
</sectionHeader>
<subsectionHeader confidence="0.994732">
4.1 Scoring the segments
</subsectionHeader>
<bodyText confidence="0.944579258064516">
An algorithm of segmentation has usually the following function:
INPUT: a list of sentences S1,...,Sn and a list of scores score(S1),...,score(Sn);
OUTPUT: a list of segments Seg1,...,SegN.
Given a set of N segments (obtained by CTT or LTT) we need a criterion to select
those sentences from a segment which will be introduced in the summary. Thus,
after the score of a sentence is calculated, we calculate a score of a segment. The final
score, Scorefinal, of a sentence is weighted by the score of the segment which contains
it. The summary is generated by selecting from each segment a number of sentences
proportional with the score of the segment. The method has some advantages when a
desired level of granularity of summarization is imposed.
The summarization algorithm with Arbitrarily Length of the summary (AL) is the
following:
INPUT: The segments Seg1,...SegN, the length of summary X (as parameter),
Scorefinal(Si) for each sentence Si;
OUTPUT: A summary SUM of length X, where from each segment Segj are selected
NSenSegj sentences. The method of selecting the sentences is given by defini-
tions Sum1,Sum2,Sum3 (Section 4.2).
Remark: A number of segments Segj may have NSenSegj &gt; 1. If X &lt; N then a
number of segments Segj must have NSenSegj = 0
Top-down Cohesion Segmentation in Summarization 393
In Section 5 (Experiments) the variant of summarization algorithm as above is de-
noted as Var1. In the variant Var2 a second choice of computing the score of a
segment is considered. Namely, the score is not normalized, and it is equal with the
sum of its sentences scores, without been divided to the segment length. The draw-
back of Var1 is that in some cases a very long segment can contain some sentences
with a high score and many sentences with a very low score, the final score of this
segment will be a small one and those important sentences will not be included in the
final summary. The drawback of Var2 is that of increased importance of the length
of the segment in some cases. Thus, the score of a short segment with high sentences
scores will be less then one of a long segment with small sentences scores, and again
some important sentences will be lost.
</bodyText>
<sectionHeader confidence="0.529339" genericHeader="method">
4.2 Strategies for summary calculus
</sectionHeader>
<bodyText confidence="0.9998326">
The method of extracting sentences from the segments is decisive for the quality of
the summary. The deletion of an arbitrary amount of source material between two
sentences which are adjacent in the summary has the potential of losing essential in-
formation. We propose and compare some simple strategies for including sentences
in the summary:
</bodyText>
<listItem confidence="0.918539888888889">
• Our first strategy is to include in the summary the first sentence from each seg-
ment, as this is of special importance for a segment. The corresponding sum-
mary will be denoted by Sum1.
• The second way is that for each segment the sentence(s) with a maximal score
are considered the most important for this segment, and hence they are included
in the summary. The corresponding summary is denoted by Sum2.
• The third way of reasoning is that from each segment the most informative
sentence(s) (the least similar) relative to the previously selected sentences are
picked up. The corresponding summary is denoted by Sum3.
</listItem>
<sectionHeader confidence="0.998922" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.8758645">
In our experiments for CTT method each sentence is scored as following: Score(Si) =
1 where nuwi is the number of words &amp;quot;forced&amp;quot; to get the first WN sense in the
</bodyText>
<subsubsectionHeader confidence="0.309859">
nuwi
</subsubsectionHeader>
<bodyText confidence="0.999441363636364">
sentence Si. If nuwi = 0 then Score(Si) = 2. The graph of the logical structure for the
text Hirst is presented in Figure 1 while the graph for the cohesion structure for the
same text is presented in Figure 2.
We have applied CTT and LTT methods of segmentation and summarization to
four texts denoted in the following by: Hirst (Morris and Hirst, 1991), Koan (Richie,
1991), Tucker1 (Tucker, 1999) and Tucker2 (Tucker, 1999).1 The denotations are as
following: LSi for LTT with Sumi method, CSi for CTT with Sumi method. Also an
ideal summary (IdS) has been constructed by taking the majority occurrences of the
sentences in all LSi and CSi summaries. IdS is the last raw of the table. For the text
Tucker1 the summaries with five sentences obtained by us and the summary obtained
by the author with CLASP (Tucker, 1999) are presented in Table 1.
</bodyText>
<footnote confidence="0.972542">
1All these texts are shown on-line at http://www.cs.ubbcluj.ro/~dtatar/nlp/ (first entries).
</footnote>
<note confidence="0.881241">
394 Tatar, Mihis, and Serban
</note>
<figureCaption confidence="0.999993">
Figure 1: The logical structure of the text Hirst
Figure 2: The cohesion structure of the text Hirst
</figureCaption>
<tableCaption confidence="0.992172">
Table 1: The summaries with the length 5 for the text Tacker1 compared with the
author’s summary
</tableCaption>
<figure confidence="0.973959736842105">
Method 5 sent Tacker1
LS1 1,16,17,34,35 6,8,16,23,34
LS2 1,16,17,34,35
LS3 1,32,34,43,44
CS1 1,23,31,34,40
CS2 8,23,31,35,43
CS3 1,9,27,34,39
IdS 1,16,17,34,35
Logical structure of Hirst text
11
0
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41
Positions of sentences
2.25
0
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41
Positions of sentences
Cohesion structure of the Hirst text
Top-down Cohesion Segmentation in Summarization 395
</figure>
<tableCaption confidence="0.956908">
Table 2: The average informativeness of CTT and LTT summaries for all texts
</tableCaption>
<table confidence="0.6485239">
Method Var1 Var2
LS1 0.606538745 0.603946109
LS2 0.580429322 0.577647764
LS3 0.594914426 0.600104854
CS1 0.607369111 0.603053171
CS2 0.592993154 0.589675201
Var1 + Var2
0.605242427
0.579038543
0.59750964
0.605211141
0.591334178
CTTaverage 0.6106624 0.5958102
LTTaverage 0.593960831 0.593899576
average 0.602311633 0.594854934
CS3 0.631625044 0.594702506
0.613163775
0.598583284
0.593930203
0.6032363
</table>
<subsectionHeader confidence="0.986764">
5.1 Evaluation of the summarization
</subsectionHeader>
<bodyText confidence="0.999841">
There is no an unique formal method to evaluate the quality of a summary. In this
paper we use as a measure of the quality of a summary, the similarity (calculated
as cosine) between the summarized (initial) text and the summaries obtained with
different methods. We call this similarity &amp;quot;the informativeness&amp;quot;.
The informativeness of the different types of summaries Sum1,Sum2,Sum3 (see
Section 4.2) and of different lengths (5, 6 and 10) is calculated for each text. Then, the
average informativeness for all four texts is calculated. A view with the these average
results of informativeness, calculated with different methods, in variants Var1 and
Var2, is given in the Table 2.
Let us remark that for obtaining summaries with different lengths, after a first seg-
mentation with CTT and LTT methods the algorithm AL from Section 4.1 is applied.
Table 2 displays the results announced in the abstract: the quality of CTT sum-
maries is better than the quality of the LTT summaries from the point of view of
informativeness.
</bodyText>
<subsectionHeader confidence="0.997709">
5.2 Implementation details
</subsectionHeader>
<bodyText confidence="0.9997902">
The methods presented in this paper are fully implemented: we used our own systems
of Text Entailment verification, Word Sense Disambiguation, top-down lexical chains
determination, LTT and CTT segmentation, summarization with Sumi and AL meth-
ods. The programs are realized in Java and C++. WordNet (Miller, 1995) is used by
our system of Word Sense Disambiguation.
</bodyText>
<sectionHeader confidence="0.995068" genericHeader="conclusions">
6 Conclusion and further work
</sectionHeader>
<bodyText confidence="0.999907625">
This paper shows that the text segmentation by lexical chains and by text entailment
relation between sentences are good bases for obtaining highly accurate summaries.
Moreover, our method replaces the usually bottom-up lexical chain construction with
a top-down one, where first a single chain of disambiguated words is established and
then it is divided in a sequence of many shorter lexical chains. The segmentation of
text follows the sequence of lexical chains. Our methods of summarization control the
length of the summaries by a process of scoring the segments. Thus, more material is
extracted from the strongest segments.
</bodyText>
<note confidence="0.757452">
396 Tatar, Mihis, and Serban
</note>
<bodyText confidence="0.896147">
The evaluation indicates acceptable performance when informativeness of sum-
maries is considered. However, our methods have the potential to be improved: in
CTT method we correspond a segment to a lexical chain. We intend to improve our
scoring method of a segment by considering some recent method of scoring lexical
chains (Ercan and Cicekli, 2008). Also, we intend to study how anaphora resolution
could improve the lexical chains and the segmentation. We further intend to apply
the presented methods to the corpus of texts DUC2002 and to evaluate them with the
standard ROUGE method (for our experiments we didn’t have the necessary human
made summaries).
</bodyText>
<sectionHeader confidence="0.99787" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.743568">
This work has been supported by PN2 Grant TD 400/2007.
</bodyText>
<sectionHeader confidence="0.998815" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998573795918368">
Barzilay, R. and M. Elhadad (1999). Using lexical chains for Text summarization. In
J. Mani and M. Maybury (Eds.), Advances in Automated Text Summarization. MIT
Press.
Boguraev, B. and M. Neff (2000). Lexical Cohesion, Discourse Segmentation and
Document Summarization. In Proceedings of the 33rd Hawaii International Con-
ference on System Sciences.
Corley, C. and R. Mihalcea (2005). Measuring the semantic similarity of texts. In
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, Ann Arbor.
Ercan, G. and I. Cicekli (2008). Lexical cohesion based topic modeling for summa-
rization. In Proceedings of the Cicling 2008, pp. 582–592.
Harabagiu, S. and D. Moldovan (1997). TextNet – a textbased intelligent system.
Natural Language Engineering 3(2), 171–190.
Hearst, M. (1997). Texttiling: Segmenting text into multi-paragraph subtopic pas-
sages. Computational Linguistics 23(1), 33–64.
Hovy, E. (2003). Text summarization. In R. Mitkov (Ed.), The Oxford Handbook of
Computational Linguistics. Oxford University Press.
Mani, I. (2001). Automatic summarization. John Benjamins.
Marcu, D. (1997). From discourse structure to text summaries. In Proceedings of the
ACL/EACL ’97 Workshop on Intelligent Scalable Text Summarization, pp. 82–88.
Miller, G. (1995). WordNet: a lexical database for english. Comm. of the
ACM 38(11), 39–41.
Morris, J. and G. Hirst (1991). Lexical cohesion computed by thesaural relations as
an indicator of the structure of text. Computational Linguistics 17(1), 21–48.
Top-down Cohesion Segmentation in Summarization 397
Orasan, C. (2006). Comparative evaluation of modular automatic summarization sys-
tems using CAST. Ph. D. thesis, University of Wolverhampton.
Radev, D., E. Hovy, and K. McKeown (2002). Introduction to the special issues on
summarization. Computational Linguistics 28, 399–408.
Reynar, J. (1998). Topic Segmentation: algorithms and applications. Ph. D. thesis,
Univ. of Penn.
Richie, D. (1991). The koan. In Z. Inklings (Ed.), Some Stories, Fables, Parables and
Sermons, pp. 25–27.
Silber, H. and K. McCoy (2002). Efficiently computed lexical chains, as an inter-
mediate representation for automatic text summarization. Computational Linguis-
tics 28(4), 487–496.
Stokes, N. (2004). Applications of Lexical Cohesion Analysis in the Topic Detection
and Tracking Domain. Ph. D. thesis, National University of Ireland, Dublin.
Tatar, D., A. M. G. Serban, and R. Mihalcea (2007). Text entailment as directional
relation. In Proceedings of CALP07 Workshop at RANLP2007, Borovets, Bulgaria,
pp. 53–58.
Tatar, D., A. Mihis, and D. Lupsa (2008). Text entailment for logical segmentation
and summarization. In 13th International Conference on Applications of Natural
Language to Information Systems, pp. 233–244.
Tatar, D., G. Serban, A. Mihis, M. Lupea, D. Lupsa, and M. Frentiu (2007). A chain
dictionary method for wsd and applications. In Proceedings of the International
Conference on Knowledge Engineering,Principles and Techniques, pp. 41–49.
Tucker, R. (1999). Automatic summarising and the CLASP system. Ph. D. thesis,
University of Cambridge.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.245314">
<title confidence="0.990722333333333">Top-Down Cohesion Segmentation in Summarization</title>
<author confidence="0.859292">Doina Tatar Andreea Diana Mihis Gabriela Serban</author>
<affiliation confidence="0.777904">Cluj-Napoca (Romania)</affiliation>
<abstract confidence="0.998476">The paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of this single chain are considered as boundaries for the segments of the cohesion text structure (Cohesion TextTiling or CTT). The summaries of arbitrarily length are obtained by extraction using three different methods applied to the obtained segments. The informativeness of the obtained summaries is compared with the informativeness of the pair summaries of the same length obtained using an earlier method of logical segmentation by text entailment (Logical TextTiling or LTT). Some experiments about CTT and LTT methods are carried out for four “classical&amp;quot; texts in summarization literature showing that the quality of the summarization using cohesion segmentation (CTT) is better than the quality using logical segmentation (LTT).</abstract>
<address confidence="0.5601105">389 390 Tatar, Mihis, and Serban</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for Text summarization. In</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1679" citStr="Barzilay and Elhadad, 1999" startWordPosition="257" endWordPosition="260">etter than the quality using logical segmentation (LTT). 389 390 Tatar, Mihis, and Serban 1 Introduction Text summarization has become the subject of an intense research in the last years and it is still an emerging field (Orasan, 2006; Radev et al., 2002; Hovy, 2003; Mani, 2001). The research is done in the extracts (which we are treating in this paper) and abstracts areas. The most important task of summarization is to identify the most informative (salient) parts of a text comparatively with the rest. A good segmentation of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and Elhadad, 1999; Reynar, 1998). This paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of this chain are considered. These rips are boundaries of the segments in the cohesion structure of the text. Due to some similarities with TextTiling algorithm for topic shifts detection of Hearst (1997), the method is called Cohesion TextTiling (CTT). The paper is structured as follows: in Section 2 we present the problem of Word Sense Disambiguation by a chain algorithm and the derived </context>
<context position="3037" citStr="Barzilay and Elhadad, 1999" startWordPosition="480" endWordPosition="483">arization by different methods after segmentation is the topic of Section 4. The parallel application of CTT and LTT methods to four &amp;quot;classical&amp;quot; texts in summarization literature, two narrative and two newspapers, and some statistics of the results are presented in Section 5. We finish the article with conclusions and possible further work directions. 2 A top-down cohesion segmentation method 2.1 Lexical chains A lexical chain is a sequence of words such that the meaning of each word from the sequence can be obtained unambiguously from the meaning of the rest of words (Morris and Hirst, 1991; Barzilay and Elhadad, 1999; Harabagiu and Moldovan, 1997; Silber and McCoy, 2002; Stokes, 2004). The map of all lexical chains of a text provides a representation of the lexical cohesive structure of the text. Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the appropriate sense in the current chain, and the senses of the other words in the chain are updated. If no relation is found, then a ne</context>
</contexts>
<marker>Barzilay, Elhadad, 1999</marker>
<rawString>Barzilay, R. and M. Elhadad (1999). Using lexical chains for Text summarization. In J. Mani and M. Maybury (Eds.), Advances in Automated Text Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>M Neff</author>
</authors>
<title>Lexical Cohesion, Discourse Segmentation and Document Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 33rd Hawaii International Conference on System Sciences.</booktitle>
<contexts>
<context position="1651" citStr="Boguraev and Neff, 2000" startWordPosition="253" endWordPosition="256">n segmentation (CTT) is better than the quality using logical segmentation (LTT). 389 390 Tatar, Mihis, and Serban 1 Introduction Text summarization has become the subject of an intense research in the last years and it is still an emerging field (Orasan, 2006; Radev et al., 2002; Hovy, 2003; Mani, 2001). The research is done in the extracts (which we are treating in this paper) and abstracts areas. The most important task of summarization is to identify the most informative (salient) parts of a text comparatively with the rest. A good segmentation of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and Elhadad, 1999; Reynar, 1998). This paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of this chain are considered. These rips are boundaries of the segments in the cohesion structure of the text. Due to some similarities with TextTiling algorithm for topic shifts detection of Hearst (1997), the method is called Cohesion TextTiling (CTT). The paper is structured as follows: in Section 2 we present the problem of Word Sense Disambiguation by a chai</context>
</contexts>
<marker>Boguraev, Neff, 2000</marker>
<rawString>Boguraev, B. and M. Neff (2000). Lexical Cohesion, Discourse Segmentation and Document Summarization. In Proceedings of the 33rd Hawaii International Conference on System Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Corley</author>
<author>R Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<location>Ann Arbor.</location>
<contexts>
<context position="7824" citStr="Corley and Mihalcea (2005)" startWordPosition="1285" endWordPosition="1288">age from a computational viewpoint. 3 Segmentation by Logical TextTiling 3.1 Text entailment Text entailment is an autonomous field of Natural Language Processing and it represents the subject of some recent Pascal Challenges. As is established in an earlier paper (Tatar et al., 2007), a text T entails an hypothesis H, denoted by T → H, iff H is less informative than T. A method to prove T → H which relies on this definition 392 Tatar, Mihis, and Serban consists in the verification of the relation: sim(T,H)T &lt; sim(T,H)H. Here sim(T,H)T and sim(T,H)H are text-to-text similarities introduced in Corley and Mihalcea (2005). The method used by our tool for Text entailment verification calculates the similarity between T and H by cosine, thus the above relation becomes cos(T,H)T &lt; cos(T,H)H (Tatar et al., 2007). 3.2 Logical segmentation Tatar et al. (2008) present a method named logical segmentation because the score of a sentence is the number of sentences of the text which are entailed by it. Representing the scores of sentences as a graph, we obtain a structure which indicates how the most important sentences alternate with ones less important and which organizes the text according to its logical content. Simp</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Corley, C. and R. Mihalcea (2005). Measuring the semantic similarity of texts. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ercan</author>
<author>I Cicekli</author>
</authors>
<title>Lexical cohesion based topic modeling for summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Cicling</booktitle>
<pages>582--592</pages>
<marker>Ercan, Cicekli, 2008</marker>
<rawString>Ercan, G. and I. Cicekli (2008). Lexical cohesion based topic modeling for summarization. In Proceedings of the Cicling 2008, pp. 582–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
</authors>
<title>TextNet – a textbased intelligent system.</title>
<date>1997</date>
<journal>Natural Language Engineering</journal>
<volume>3</volume>
<issue>2</issue>
<pages>171--190</pages>
<contexts>
<context position="3067" citStr="Harabagiu and Moldovan, 1997" startWordPosition="484" endWordPosition="487">ds after segmentation is the topic of Section 4. The parallel application of CTT and LTT methods to four &amp;quot;classical&amp;quot; texts in summarization literature, two narrative and two newspapers, and some statistics of the results are presented in Section 5. We finish the article with conclusions and possible further work directions. 2 A top-down cohesion segmentation method 2.1 Lexical chains A lexical chain is a sequence of words such that the meaning of each word from the sequence can be obtained unambiguously from the meaning of the rest of words (Morris and Hirst, 1991; Barzilay and Elhadad, 1999; Harabagiu and Moldovan, 1997; Silber and McCoy, 2002; Stokes, 2004). The map of all lexical chains of a text provides a representation of the lexical cohesive structure of the text. Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the appropriate sense in the current chain, and the senses of the other words in the chain are updated. If no relation is found, then a new chain is initiated. Our meth</context>
</contexts>
<marker>Harabagiu, Moldovan, 1997</marker>
<rawString>Harabagiu, S. and D. Moldovan (1997). TextNet – a textbased intelligent system. Natural Language Engineering 3(2), 171–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<issue>1</issue>
<pages>33--64</pages>
<contexts>
<context position="2091" citStr="Hearst (1997)" startWordPosition="327" endWordPosition="328">identify the most informative (salient) parts of a text comparatively with the rest. A good segmentation of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and Elhadad, 1999; Reynar, 1998). This paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of this chain are considered. These rips are boundaries of the segments in the cohesion structure of the text. Due to some similarities with TextTiling algorithm for topic shifts detection of Hearst (1997), the method is called Cohesion TextTiling (CTT). The paper is structured as follows: in Section 2 we present the problem of Word Sense Disambiguation by a chain algorithm and the derived CTT method. In Section 3, some notions about textual entailment and logical segmentation of a text by LTT method are discussed. Summarization by different methods after segmentation is the topic of Section 4. The parallel application of CTT and LTT methods to four &amp;quot;classical&amp;quot; texts in summarization literature, two narrative and two newspapers, and some statistics of the results are presented in Section 5. We </context>
<context position="8703" citStr="Hearst, 1997" startWordPosition="1429" endWordPosition="1430">l segmentation because the score of a sentence is the number of sentences of the text which are entailed by it. Representing the scores of sentences as a graph, we obtain a structure which indicates how the most important sentences alternate with ones less important and which organizes the text according to its logical content. Simply, a valley (a local minimum) in the obtained logical structure of the text is a boundary between two logical segments (see Figure 1). The method is called Logical TextTiling (LTT), due to some similarities with the TextTiling algorithm for topic shifts detection (Hearst, 1997). The drawback of LTT, that the number of the segments is fixed for a given text (as it results from its logical structure), is eliminated by a method to dynamically correlate the number of the logical segments obtained by LTT with the required length of the summary. Let us remark that LTT does not require a predicate-argument analysis. The only semantic structure processing required is the Text Entailment verification. 4 Summarization by segmentation 4.1 Scoring the segments An algorithm of segmentation has usually the following function: INPUT: a list of sentences S1,...,Sn and a list of sco</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, M. (1997). Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics 23(1), 33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Text summarization. In</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics.</booktitle>
<editor>R. Mitkov (Ed.),</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1320" citStr="Hovy, 2003" startWordPosition="200" endWordPosition="201">f the pair summaries of the same length obtained using an earlier method of logical segmentation by text entailment (Logical TextTiling or LTT). Some experiments about CTT and LTT methods are carried out for four “classical&amp;quot; texts in summarization literature showing that the quality of the summarization using cohesion segmentation (CTT) is better than the quality using logical segmentation (LTT). 389 390 Tatar, Mihis, and Serban 1 Introduction Text summarization has become the subject of an intense research in the last years and it is still an emerging field (Orasan, 2006; Radev et al., 2002; Hovy, 2003; Mani, 2001). The research is done in the extracts (which we are treating in this paper) and abstracts areas. The most important task of summarization is to identify the most informative (salient) parts of a text comparatively with the rest. A good segmentation of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and Elhadad, 1999; Reynar, 1998). This paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of this chain are considered. Thes</context>
</contexts>
<marker>Hovy, 2003</marker>
<rawString>Hovy, E. (2003). Text summarization. In R. Mitkov (Ed.), The Oxford Handbook of Computational Linguistics. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic summarization.</title>
<date>2001</date>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="1333" citStr="Mani, 2001" startWordPosition="202" endWordPosition="203">ummaries of the same length obtained using an earlier method of logical segmentation by text entailment (Logical TextTiling or LTT). Some experiments about CTT and LTT methods are carried out for four “classical&amp;quot; texts in summarization literature showing that the quality of the summarization using cohesion segmentation (CTT) is better than the quality using logical segmentation (LTT). 389 390 Tatar, Mihis, and Serban 1 Introduction Text summarization has become the subject of an intense research in the last years and it is still an emerging field (Orasan, 2006; Radev et al., 2002; Hovy, 2003; Mani, 2001). The research is done in the extracts (which we are treating in this paper) and abstracts areas. The most important task of summarization is to identify the most informative (salient) parts of a text comparatively with the rest. A good segmentation of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and Elhadad, 1999; Reynar, 1998). This paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of this chain are considered. These rips are bo</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Mani, I. (2001). Automatic summarization. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>From discourse structure to text summaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL ’97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--88</pages>
<contexts>
<context position="7042" citStr="Marcu (1997)" startWordPosition="1160" endWordPosition="1161">e boundaries between lexical chains (see Figure 2). Lexical chains could serve further as a basis for an algorithm of segmentation. As our method of determination of lexical chains is linear, the corresponding segmentation is also linear. The obtained segments could be used effectively in summarization. In this respect, our method of summarization falls in the discourse-based category. In contrast with other theories about discourse segmentation, as Rhetorical Structure Theory (RST) of Mann and Thompson (1988), attentional/intentional structure of Grosz and Sidner (1986) or parsed RST tree of Marcu (1997), our CTT method (and also, as presented below, our LTT method) supposes a linear segmentation (versus hierarchical segmentation) which results in an advantage from a computational viewpoint. 3 Segmentation by Logical TextTiling 3.1 Text entailment Text entailment is an autonomous field of Natural Language Processing and it represents the subject of some recent Pascal Challenges. As is established in an earlier paper (Tatar et al., 2007), a text T entails an hypothesis H, denoted by T → H, iff H is less informative than T. A method to prove T → H which relies on this definition 392 Tatar, Mihi</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Marcu, D. (1997). From discourse structure to text summaries. In Proceedings of the ACL/EACL ’97 Workshop on Intelligent Scalable Text Summarization, pp. 82–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<journal>Comm. of the ACM</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="16047" citStr="Miller, 1995" startWordPosition="2650" endWordPosition="2651">fter a first segmentation with CTT and LTT methods the algorithm AL from Section 4.1 is applied. Table 2 displays the results announced in the abstract: the quality of CTT summaries is better than the quality of the LTT summaries from the point of view of informativeness. 5.2 Implementation details The methods presented in this paper are fully implemented: we used our own systems of Text Entailment verification, Word Sense Disambiguation, top-down lexical chains determination, LTT and CTT segmentation, summarization with Sumi and AL methods. The programs are realized in Java and C++. WordNet (Miller, 1995) is used by our system of Word Sense Disambiguation. 6 Conclusion and further work This paper shows that the text segmentation by lexical chains and by text entailment relation between sentences are good bases for obtaining highly accurate summaries. Moreover, our method replaces the usually bottom-up lexical chain construction with a top-down one, where first a single chain of disambiguated words is established and then it is divided in a sequence of many shorter lexical chains. The segmentation of text follows the sequence of lexical chains. Our methods of summarization control the length of</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, G. (1995). WordNet: a lexical database for english. Comm. of the ACM 38(11), 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics</journal>
<volume>17</volume>
<issue>1</issue>
<pages>21--48</pages>
<contexts>
<context position="3009" citStr="Morris and Hirst, 1991" startWordPosition="476" endWordPosition="479">thod are discussed. Summarization by different methods after segmentation is the topic of Section 4. The parallel application of CTT and LTT methods to four &amp;quot;classical&amp;quot; texts in summarization literature, two narrative and two newspapers, and some statistics of the results are presented in Section 5. We finish the article with conclusions and possible further work directions. 2 A top-down cohesion segmentation method 2.1 Lexical chains A lexical chain is a sequence of words such that the meaning of each word from the sequence can be obtained unambiguously from the meaning of the rest of words (Morris and Hirst, 1991; Barzilay and Elhadad, 1999; Harabagiu and Moldovan, 1997; Silber and McCoy, 2002; Stokes, 2004). The map of all lexical chains of a text provides a representation of the lexical cohesive structure of the text. Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the appropriate sense in the current chain, and the senses of the other words in the chain are updated. If no </context>
<context position="12903" citStr="Morris and Hirst, 1991" startWordPosition="2146" endWordPosition="2149">viously selected sentences are picked up. The corresponding summary is denoted by Sum3. 5 Experiments In our experiments for CTT method each sentence is scored as following: Score(Si) = 1 where nuwi is the number of words &amp;quot;forced&amp;quot; to get the first WN sense in the nuwi sentence Si. If nuwi = 0 then Score(Si) = 2. The graph of the logical structure for the text Hirst is presented in Figure 1 while the graph for the cohesion structure for the same text is presented in Figure 2. We have applied CTT and LTT methods of segmentation and summarization to four texts denoted in the following by: Hirst (Morris and Hirst, 1991), Koan (Richie, 1991), Tucker1 (Tucker, 1999) and Tucker2 (Tucker, 1999).1 The denotations are as following: LSi for LTT with Sumi method, CSi for CTT with Sumi method. Also an ideal summary (IdS) has been constructed by taking the majority occurrences of the sentences in all LSi and CSi summaries. IdS is the last raw of the table. For the text Tucker1 the summaries with five sentences obtained by us and the summary obtained by the author with CLASP (Tucker, 1999) are presented in Table 1. 1All these texts are shown on-line at http://www.cs.ubbcluj.ro/~dtatar/nlp/ (first entries). 394 Tatar, M</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, J. and G. Hirst (1991). Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics 17(1), 21–48.</rawString>
</citation>
<citation valid="false">
<title>Top-down Cohesion Segmentation in</title>
<journal>Summarization</journal>
<volume>397</volume>
<marker></marker>
<rawString>Top-down Cohesion Segmentation in Summarization 397</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Orasan</author>
</authors>
<title>Comparative evaluation of modular automatic summarization systems using CAST.</title>
<date>2006</date>
<tech>Ph. D. thesis,</tech>
<institution>University of Wolverhampton.</institution>
<contexts>
<context position="1288" citStr="Orasan, 2006" startWordPosition="194" endWordPosition="195">ompared with the informativeness of the pair summaries of the same length obtained using an earlier method of logical segmentation by text entailment (Logical TextTiling or LTT). Some experiments about CTT and LTT methods are carried out for four “classical&amp;quot; texts in summarization literature showing that the quality of the summarization using cohesion segmentation (CTT) is better than the quality using logical segmentation (LTT). 389 390 Tatar, Mihis, and Serban 1 Introduction Text summarization has become the subject of an intense research in the last years and it is still an emerging field (Orasan, 2006; Radev et al., 2002; Hovy, 2003; Mani, 2001). The research is done in the extracts (which we are treating in this paper) and abstracts areas. The most important task of summarization is to identify the most informative (salient) parts of a text comparatively with the rest. A good segmentation of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and Elhadad, 1999; Reynar, 1998). This paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of</context>
</contexts>
<marker>Orasan, 2006</marker>
<rawString>Orasan, C. (2006). Comparative evaluation of modular automatic summarization systems using CAST. Ph. D. thesis, University of Wolverhampton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>E Hovy</author>
<author>K McKeown</author>
</authors>
<title>Introduction to the special issues on summarization.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<pages>399--408</pages>
<contexts>
<context position="1308" citStr="Radev et al., 2002" startWordPosition="196" endWordPosition="199">he informativeness of the pair summaries of the same length obtained using an earlier method of logical segmentation by text entailment (Logical TextTiling or LTT). Some experiments about CTT and LTT methods are carried out for four “classical&amp;quot; texts in summarization literature showing that the quality of the summarization using cohesion segmentation (CTT) is better than the quality using logical segmentation (LTT). 389 390 Tatar, Mihis, and Serban 1 Introduction Text summarization has become the subject of an intense research in the last years and it is still an emerging field (Orasan, 2006; Radev et al., 2002; Hovy, 2003; Mani, 2001). The research is done in the extracts (which we are treating in this paper) and abstracts areas. The most important task of summarization is to identify the most informative (salient) parts of a text comparatively with the rest. A good segmentation of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and Elhadad, 1999; Reynar, 1998). This paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of this chain are cons</context>
</contexts>
<marker>Radev, Hovy, McKeown, 2002</marker>
<rawString>Radev, D., E. Hovy, and K. McKeown (2002). Introduction to the special issues on summarization. Computational Linguistics 28, 399–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
</authors>
<title>Topic Segmentation: algorithms and applications.</title>
<date>1998</date>
<tech>Ph. D. thesis,</tech>
<institution>Univ. of Penn.</institution>
<contexts>
<context position="1694" citStr="Reynar, 1998" startWordPosition="261" endWordPosition="262"> logical segmentation (LTT). 389 390 Tatar, Mihis, and Serban 1 Introduction Text summarization has become the subject of an intense research in the last years and it is still an emerging field (Orasan, 2006; Radev et al., 2002; Hovy, 2003; Mani, 2001). The research is done in the extracts (which we are treating in this paper) and abstracts areas. The most important task of summarization is to identify the most informative (salient) parts of a text comparatively with the rest. A good segmentation of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and Elhadad, 1999; Reynar, 1998). This paper proposes a new method of linear text segmentation based on lexical cohesion of a text. Namely, first a single chain of disambiguated words in a text is established, then the rips of this chain are considered. These rips are boundaries of the segments in the cohesion structure of the text. Due to some similarities with TextTiling algorithm for topic shifts detection of Hearst (1997), the method is called Cohesion TextTiling (CTT). The paper is structured as follows: in Section 2 we present the problem of Word Sense Disambiguation by a chain algorithm and the derived CTT method. In </context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Reynar, J. (1998). Topic Segmentation: algorithms and applications. Ph. D. thesis, Univ. of Penn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Richie</author>
</authors>
<title>The koan.</title>
<date>1991</date>
<booktitle>In Z. Inklings (Ed.), Some Stories, Fables, Parables and Sermons,</booktitle>
<pages>25--27</pages>
<contexts>
<context position="12924" citStr="Richie, 1991" startWordPosition="2151" endWordPosition="2152">picked up. The corresponding summary is denoted by Sum3. 5 Experiments In our experiments for CTT method each sentence is scored as following: Score(Si) = 1 where nuwi is the number of words &amp;quot;forced&amp;quot; to get the first WN sense in the nuwi sentence Si. If nuwi = 0 then Score(Si) = 2. The graph of the logical structure for the text Hirst is presented in Figure 1 while the graph for the cohesion structure for the same text is presented in Figure 2. We have applied CTT and LTT methods of segmentation and summarization to four texts denoted in the following by: Hirst (Morris and Hirst, 1991), Koan (Richie, 1991), Tucker1 (Tucker, 1999) and Tucker2 (Tucker, 1999).1 The denotations are as following: LSi for LTT with Sumi method, CSi for CTT with Sumi method. Also an ideal summary (IdS) has been constructed by taking the majority occurrences of the sentences in all LSi and CSi summaries. IdS is the last raw of the table. For the text Tucker1 the summaries with five sentences obtained by us and the summary obtained by the author with CLASP (Tucker, 1999) are presented in Table 1. 1All these texts are shown on-line at http://www.cs.ubbcluj.ro/~dtatar/nlp/ (first entries). 394 Tatar, Mihis, and Serban Figu</context>
</contexts>
<marker>Richie, 1991</marker>
<rawString>Richie, D. (1991). The koan. In Z. Inklings (Ed.), Some Stories, Fables, Parables and Sermons, pp. 25–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Silber</author>
<author>K McCoy</author>
</authors>
<title>Efficiently computed lexical chains, as an intermediate representation for automatic text summarization.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>4</issue>
<pages>487--496</pages>
<contexts>
<context position="3091" citStr="Silber and McCoy, 2002" startWordPosition="488" endWordPosition="491">opic of Section 4. The parallel application of CTT and LTT methods to four &amp;quot;classical&amp;quot; texts in summarization literature, two narrative and two newspapers, and some statistics of the results are presented in Section 5. We finish the article with conclusions and possible further work directions. 2 A top-down cohesion segmentation method 2.1 Lexical chains A lexical chain is a sequence of words such that the meaning of each word from the sequence can be obtained unambiguously from the meaning of the rest of words (Morris and Hirst, 1991; Barzilay and Elhadad, 1999; Harabagiu and Moldovan, 1997; Silber and McCoy, 2002; Stokes, 2004). The map of all lexical chains of a text provides a representation of the lexical cohesive structure of the text. Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the appropriate sense in the current chain, and the senses of the other words in the chain are updated. If no relation is found, then a new chain is initiated. Our method approaches the constr</context>
</contexts>
<marker>Silber, McCoy, 2002</marker>
<rawString>Silber, H. and K. McCoy (2002). Efficiently computed lexical chains, as an intermediate representation for automatic text summarization. Computational Linguistics 28(4), 487–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stokes</author>
</authors>
<title>Applications of Lexical Cohesion Analysis in the Topic Detection and Tracking Domain.</title>
<date>2004</date>
<tech>Ph. D. thesis,</tech>
<institution>National University of Ireland,</institution>
<location>Dublin.</location>
<contexts>
<context position="3106" citStr="Stokes, 2004" startWordPosition="492" endWordPosition="493">arallel application of CTT and LTT methods to four &amp;quot;classical&amp;quot; texts in summarization literature, two narrative and two newspapers, and some statistics of the results are presented in Section 5. We finish the article with conclusions and possible further work directions. 2 A top-down cohesion segmentation method 2.1 Lexical chains A lexical chain is a sequence of words such that the meaning of each word from the sequence can be obtained unambiguously from the meaning of the rest of words (Morris and Hirst, 1991; Barzilay and Elhadad, 1999; Harabagiu and Moldovan, 1997; Silber and McCoy, 2002; Stokes, 2004). The map of all lexical chains of a text provides a representation of the lexical cohesive structure of the text. Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the appropriate sense in the current chain, and the senses of the other words in the chain are updated. If no relation is found, then a new chain is initiated. Our method approaches the construction of lexic</context>
</contexts>
<marker>Stokes, 2004</marker>
<rawString>Stokes, N. (2004). Applications of Lexical Cohesion Analysis in the Topic Detection and Tracking Domain. Ph. D. thesis, National University of Ireland, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tatar</author>
<author>A M G Serban</author>
<author>R Mihalcea</author>
</authors>
<title>Text entailment as directional relation.</title>
<date>2007</date>
<booktitle>In Proceedings of CALP07 Workshop at RANLP2007,</booktitle>
<pages>53--58</pages>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="4003" citStr="Tatar et al. (2007)" startWordPosition="646" endWordPosition="649">Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the appropriate sense in the current chain, and the senses of the other words in the chain are updated. If no relation is found, then a new chain is initiated. Our method approaches the construction of lexical chains in a reverse order: we first disambiguate the whole text and then construct the lexical chains which cover as much as possible the text. 2.2 CHAD algorithm It is known that in the last years many researchers studied the possibility to globally disambiguate a text. In Tatar et al. (2007) is presented CHAD algorithm, a Lesk’s type algorithm based on WordNet, that doesn’t require syntactic analysis and syntactic parsing. As usually for a Lesk’s type algorithm, it starts from the idea that a word’s dictionary definition is a good indicator for the senses of this word and uses the defiTop-down Cohesion Segmentation in Summarization 391 nition in the dictionary directly. The base of the algorithm is the disambiguation of a triplet of words, using Dice’s overlap or Jaccard’s measures. Shortly, CHAD begins with the disambiguation of a triplet w1w2w3 and then adds to the right the fo</context>
<context position="5368" citStr="Tatar et al., 2007" startWordPosition="880" endWordPosition="883">es and the disambiguation of the third word depends on the disambiguation of these first two words. Due to the brevity of definitions in WordNet (WN), the first sense in WN for a word wi (WN 1st sense) must be associated in some cases in a &amp;quot;forced&amp;quot; way. The forced condition represents the situation that any sense of wi is related with the senses of the words wi_2 and wi_1. Thus the forced condition signals that a lexical chain stops, and, perhaps, a new one begins. Comparing the precision obtained with CHAD and the precision obtained by the WN 1st sense algorithm for 10 files of Brown corpus (Tatar et al., 2007) we obtained the result: for 7 files the difference was greater or equal to 0.04 (favorable to WN 1st), and for 3 files was lower. For example, in the worst case (Brown 01 file), the precisions obtained by CHAD are: 0.625 for Dice’s measure, 0.627 for Overlap measure, 0.638 for Jaccard’s measure while the precision obtained by WN 1st sense is 0.688. Let us remark that CHAD is used to mark the discontinuity in cohesion, while WN 1st sense algorithm is unable to do this. 2.3 CHAD and lexical chains The CHAD algorithm shows what words in a sentence are unrelated as senses with the previously word</context>
<context position="7483" citStr="Tatar et al., 2007" startWordPosition="1226" endWordPosition="1229">se segmentation, as Rhetorical Structure Theory (RST) of Mann and Thompson (1988), attentional/intentional structure of Grosz and Sidner (1986) or parsed RST tree of Marcu (1997), our CTT method (and also, as presented below, our LTT method) supposes a linear segmentation (versus hierarchical segmentation) which results in an advantage from a computational viewpoint. 3 Segmentation by Logical TextTiling 3.1 Text entailment Text entailment is an autonomous field of Natural Language Processing and it represents the subject of some recent Pascal Challenges. As is established in an earlier paper (Tatar et al., 2007), a text T entails an hypothesis H, denoted by T → H, iff H is less informative than T. A method to prove T → H which relies on this definition 392 Tatar, Mihis, and Serban consists in the verification of the relation: sim(T,H)T &lt; sim(T,H)H. Here sim(T,H)T and sim(T,H)H are text-to-text similarities introduced in Corley and Mihalcea (2005). The method used by our tool for Text entailment verification calculates the similarity between T and H by cosine, thus the above relation becomes cos(T,H)T &lt; cos(T,H)H (Tatar et al., 2007). 3.2 Logical segmentation Tatar et al. (2008) present a method named</context>
</contexts>
<marker>Tatar, Serban, Mihalcea, 2007</marker>
<rawString>Tatar, D., A. M. G. Serban, and R. Mihalcea (2007). Text entailment as directional relation. In Proceedings of CALP07 Workshop at RANLP2007, Borovets, Bulgaria, pp. 53–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tatar</author>
<author>A Mihis</author>
<author>D Lupsa</author>
</authors>
<title>Text entailment for logical segmentation and summarization.</title>
<date>2008</date>
<booktitle>In 13th International Conference on Applications of Natural Language to Information Systems,</booktitle>
<pages>233--244</pages>
<contexts>
<context position="8060" citStr="Tatar et al. (2008)" startWordPosition="1323" endWordPosition="1326">d in an earlier paper (Tatar et al., 2007), a text T entails an hypothesis H, denoted by T → H, iff H is less informative than T. A method to prove T → H which relies on this definition 392 Tatar, Mihis, and Serban consists in the verification of the relation: sim(T,H)T &lt; sim(T,H)H. Here sim(T,H)T and sim(T,H)H are text-to-text similarities introduced in Corley and Mihalcea (2005). The method used by our tool for Text entailment verification calculates the similarity between T and H by cosine, thus the above relation becomes cos(T,H)T &lt; cos(T,H)H (Tatar et al., 2007). 3.2 Logical segmentation Tatar et al. (2008) present a method named logical segmentation because the score of a sentence is the number of sentences of the text which are entailed by it. Representing the scores of sentences as a graph, we obtain a structure which indicates how the most important sentences alternate with ones less important and which organizes the text according to its logical content. Simply, a valley (a local minimum) in the obtained logical structure of the text is a boundary between two logical segments (see Figure 1). The method is called Logical TextTiling (LTT), due to some similarities with the TextTiling algorith</context>
</contexts>
<marker>Tatar, Mihis, Lupsa, 2008</marker>
<rawString>Tatar, D., A. Mihis, and D. Lupsa (2008). Text entailment for logical segmentation and summarization. In 13th International Conference on Applications of Natural Language to Information Systems, pp. 233–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tatar</author>
<author>G Serban</author>
<author>A Mihis</author>
<author>M Lupea</author>
<author>D Lupsa</author>
<author>M Frentiu</author>
</authors>
<title>A chain dictionary method for wsd and applications.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Knowledge Engineering,Principles and Techniques,</booktitle>
<pages>41--49</pages>
<contexts>
<context position="4003" citStr="Tatar et al. (2007)" startWordPosition="646" endWordPosition="649">Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the appropriate sense in the current chain, and the senses of the other words in the chain are updated. If no relation is found, then a new chain is initiated. Our method approaches the construction of lexical chains in a reverse order: we first disambiguate the whole text and then construct the lexical chains which cover as much as possible the text. 2.2 CHAD algorithm It is known that in the last years many researchers studied the possibility to globally disambiguate a text. In Tatar et al. (2007) is presented CHAD algorithm, a Lesk’s type algorithm based on WordNet, that doesn’t require syntactic analysis and syntactic parsing. As usually for a Lesk’s type algorithm, it starts from the idea that a word’s dictionary definition is a good indicator for the senses of this word and uses the defiTop-down Cohesion Segmentation in Summarization 391 nition in the dictionary directly. The base of the algorithm is the disambiguation of a triplet of words, using Dice’s overlap or Jaccard’s measures. Shortly, CHAD begins with the disambiguation of a triplet w1w2w3 and then adds to the right the fo</context>
<context position="5368" citStr="Tatar et al., 2007" startWordPosition="880" endWordPosition="883">es and the disambiguation of the third word depends on the disambiguation of these first two words. Due to the brevity of definitions in WordNet (WN), the first sense in WN for a word wi (WN 1st sense) must be associated in some cases in a &amp;quot;forced&amp;quot; way. The forced condition represents the situation that any sense of wi is related with the senses of the words wi_2 and wi_1. Thus the forced condition signals that a lexical chain stops, and, perhaps, a new one begins. Comparing the precision obtained with CHAD and the precision obtained by the WN 1st sense algorithm for 10 files of Brown corpus (Tatar et al., 2007) we obtained the result: for 7 files the difference was greater or equal to 0.04 (favorable to WN 1st), and for 3 files was lower. For example, in the worst case (Brown 01 file), the precisions obtained by CHAD are: 0.625 for Dice’s measure, 0.627 for Overlap measure, 0.638 for Jaccard’s measure while the precision obtained by WN 1st sense is 0.688. Let us remark that CHAD is used to mark the discontinuity in cohesion, while WN 1st sense algorithm is unable to do this. 2.3 CHAD and lexical chains The CHAD algorithm shows what words in a sentence are unrelated as senses with the previously word</context>
<context position="7483" citStr="Tatar et al., 2007" startWordPosition="1226" endWordPosition="1229">se segmentation, as Rhetorical Structure Theory (RST) of Mann and Thompson (1988), attentional/intentional structure of Grosz and Sidner (1986) or parsed RST tree of Marcu (1997), our CTT method (and also, as presented below, our LTT method) supposes a linear segmentation (versus hierarchical segmentation) which results in an advantage from a computational viewpoint. 3 Segmentation by Logical TextTiling 3.1 Text entailment Text entailment is an autonomous field of Natural Language Processing and it represents the subject of some recent Pascal Challenges. As is established in an earlier paper (Tatar et al., 2007), a text T entails an hypothesis H, denoted by T → H, iff H is less informative than T. A method to prove T → H which relies on this definition 392 Tatar, Mihis, and Serban consists in the verification of the relation: sim(T,H)T &lt; sim(T,H)H. Here sim(T,H)T and sim(T,H)H are text-to-text similarities introduced in Corley and Mihalcea (2005). The method used by our tool for Text entailment verification calculates the similarity between T and H by cosine, thus the above relation becomes cos(T,H)T &lt; cos(T,H)H (Tatar et al., 2007). 3.2 Logical segmentation Tatar et al. (2008) present a method named</context>
</contexts>
<marker>Tatar, Serban, Mihis, Lupea, Lupsa, Frentiu, 2007</marker>
<rawString>Tatar, D., G. Serban, A. Mihis, M. Lupea, D. Lupsa, and M. Frentiu (2007). A chain dictionary method for wsd and applications. In Proceedings of the International Conference on Knowledge Engineering,Principles and Techniques, pp. 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tucker</author>
</authors>
<title>Automatic summarising and the CLASP system.</title>
<date>1999</date>
<tech>Ph. D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="12948" citStr="Tucker, 1999" startWordPosition="2154" endWordPosition="2155">ding summary is denoted by Sum3. 5 Experiments In our experiments for CTT method each sentence is scored as following: Score(Si) = 1 where nuwi is the number of words &amp;quot;forced&amp;quot; to get the first WN sense in the nuwi sentence Si. If nuwi = 0 then Score(Si) = 2. The graph of the logical structure for the text Hirst is presented in Figure 1 while the graph for the cohesion structure for the same text is presented in Figure 2. We have applied CTT and LTT methods of segmentation and summarization to four texts denoted in the following by: Hirst (Morris and Hirst, 1991), Koan (Richie, 1991), Tucker1 (Tucker, 1999) and Tucker2 (Tucker, 1999).1 The denotations are as following: LSi for LTT with Sumi method, CSi for CTT with Sumi method. Also an ideal summary (IdS) has been constructed by taking the majority occurrences of the sentences in all LSi and CSi summaries. IdS is the last raw of the table. For the text Tucker1 the summaries with five sentences obtained by us and the summary obtained by the author with CLASP (Tucker, 1999) are presented in Table 1. 1All these texts are shown on-line at http://www.cs.ubbcluj.ro/~dtatar/nlp/ (first entries). 394 Tatar, Mihis, and Serban Figure 1: The logical struct</context>
</contexts>
<marker>Tucker, 1999</marker>
<rawString>Tucker, R. (1999). Automatic summarising and the CLASP system. Ph. D. thesis, University of Cambridge.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>