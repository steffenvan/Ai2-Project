<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009416">
<title confidence="0.8766195">
Book Reviews
WordNet: An Electronic Lexical Database
</title>
<author confidence="0.859708">
Christiane Fellbaum (editor)
</author>
<affiliation confidence="0.879088">
(Rider University and Princeton University)
</affiliation>
<figureCaption confidence="0.62194725">
Cambridge, MA: The MIT Press
(Language, speech, and communication
series), 1998, xxii+423 pp; hardbound,
ISBN 0-262-06197-X, $50.00
</figureCaption>
<figure confidence="0.915067">
Reviewed by
Dekang Lin
</figure>
<affiliation confidence="0.472294">
University of Manitoba
</affiliation>
<bodyText confidence="0.999736375">
WordNet is perhaps the most important and widely used lexical resource for natural
language processing systems up to now. WordNet: An Electronic Lexical Database, edited
by Christiane Fellbaum, discusses the design of WordNet from both theoretical and
historical perspectives, provides an up-to-date description of the lexical database, and
presents a set of applications of WordNet. The book contains a foreword by George
Miller, an introduction by Christiane Fellbaum, seven chapters from the Cognitive
Sciences Laboratory of Princeton University, where WordNet was produced, and nine
chapters contributed by scientists from elsewhere.
Miller&apos;s foreword offers a fascinating account of the history of WordNet. He dis-
cusses the presuppositions of such a lexical database, how the top-level noun categories
were determined, and the sources of the words in WordNet. He also writes about the
evolution of WordNet from its original incarnation as a dictionary browser to a broad-
coverage lexicon, and the involvement of different people during its various stages of
development over a decade. It makes very interesting reading for casual and serious
users of WordNet and anyone who is grateful for the existence of WordNet.
The book is organized in three parts. Part I is about WordNet itself and consists
of four chapters: &amp;quot;Nouns in WordNet&amp;quot; by George Miller, &amp;quot;Modifiers in WordNet&amp;quot; by
Katherine Miller, &amp;quot;A semantic network of English verbs&amp;quot; by Christiane Fellbaum, and
&amp;quot;Design and implementation of the WordNet lexical database and search software&amp;quot;
by Randee Tengi. These chapters are essentially updated versions of four papers from
Miller (1990). Compared with the earlier papers, the chapters in this book focus more
on the underlying assumptions and rationales behind the design decisions. The de-
scription of the information contained in WordNet, however, is not as detailed as in
Miller (1990).
The main new additions in these chapters include an explanation of sense grouping
in George Miller&apos;s chapter, a section about adverbs in Katherine Miller&apos;s chapter,
observations about autohyponymy (one sense of a word being a hyponym of another
sense of the same word) and autoantonymy (one sense of a word being an antonym of
another sense of the same word) in Fellbaum&apos;s chapter, and Tengi&apos;s description of the
Grinder, a program that converts the files the lexicographers work with to searchable
lexical databases.
The three papers in Part II are characterized as &amp;quot;extensions, enhancements and
</bodyText>
<subsectionHeader confidence="0.974119">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.997226653061224">
new perspectives on WordNet&amp;quot;. Marti Hearst&apos;s Chapter 5, &amp;quot;Automated discovery of
WordNet relations,&amp;quot; investigates automatic detection of WordNet-style lexicosemantic
relationships in large corpora, using rules such as this:
&amp;quot;such NP0 as NPI, . . . , {and / or} NPk&amp;quot; z NP0 is a hypernym of NPi,
, and NPk.
Similar techniques have been adopted in many previous approaches, as she notes.
She also sketches a procedure for discovering new patterns, although it was not im-
plemented. She manually inspected 200 instances that matched one of her patterns.
About 20% of the hypothesized hypernym relations were already in WordNet. About
30% were not in WordNet but were classified as &amp;quot;good&amp;quot; or &amp;quot;pretty good.&amp;quot; The rest
were errors of various kinds.
Chapter 6, &amp;quot;Representing verb alternations in WordNet&amp;quot; by Kohl, Jones, Berwick,
and Nomura, augments WordNet with Beth Levin&apos;s (1993) classification of English
verbs. Since the underlying hypothesis of Levin&apos;s work is that semantic properties of
words determine their syntactic properties, it would be extremely interesting to see the
result of superimposing an independently constructed semantic structure, WordNet,
onto Levin&apos;s verb classifications. It is a pity, therefore, that this enhancement did not
make it into WordNet 1.6, as predicted in the book.
Chapter 7, &amp;quot;The formalization of WordNet by methods of relational concept analy-
sis&amp;quot; by Uta Priss, attempts to formalize WordNet using set-theoretic concepts. Accord-
ing to Priss, &amp;quot;[the theoretical analysis] does not provide a complete system of axioms
for semantic relations, but it can facilitate the investigation of the logical properties of
those relations&amp;quot; (p. 179). She shows three fragments of WordNet where the relation-
ships could be better structured. It is not clear, unfortunately, how the formalization
could identify these fragments, leaving one to wonder whether it is simply a fancy
way to state something obvious.
The chapters in Part III are about applications that use WordNet in a variety of
ways: as a list of word senses (Chapters 8 and 9), as a taxonomy hierarchy (Chapter 10),
and as a semantic network (Chapters 11-16).
Chapters 8 and 9, &amp;quot;Building semantic concordances&amp;quot; by Landes, Leacock, and
Tengi and &amp;quot;Performance and confidence in a semantic annotation task&amp;quot; by Fellbaum,
Grabowski, and Landes, are concerned with SemCor, a 250,000-word corpus in which
all the open-class words are tagged with word senses from WordNet. This corpus can
serve several purposes, such as giving feedback to lexicographers about the appropri-
ateness and completeness of word senses and providing frequency information and
example sentences for word senses. While SemCor is probably too small for statistical
learning, it is certainly large enough to act as a test bed for word sense disambiguation
systems.
Chapter 8 describes the construction process of SemCor, the software tool used
(called ConText), the user interface of ConText, and analysis of errors and inconsistency.
It also covers the training of taggers and some quality control issues.
Chapter 9 is a description and analysis of an experiment to measure the accuracy
and confidence in the semantic tagging task. The subjects in the experiment are 17
taggers (undergraduate and graduate students). They each tagged 254 polysemous
words in a 660-word passage. For each sense tag, they also indicated the degree of
confidence for the tag by assigning a number from 1 (highly certain) to 5 (highly un-
certain). The taggers were divided into two groups. In the lexicon given to the first
group (8 taggers), the senses of words were listed in descending order of their fre-
quencies. In the lexicon given to the second group (9 taggers), they were in random
</bodyText>
<page confidence="0.981393">
293
</page>
<note confidence="0.422816">
Computational Linguistics Volume 25, Number 2
</note>
<bodyText confidence="0.999917647058824">
order. The sense tags assigned by the taggers were compared with answer keys cre-
ated by expert lexicographers. The percentage of agreement between the taggers and
the experts, as well as between the taggers themselves, were measured. It was found
that when word senses were ordered by their frequencies, the tagger-expert agreement
was 75.2% and intertagger agreement was 79.7%; when word senses were randomly
ordered, the tagger-expert agreement was 72.8% and intertagger agreement was 79.9%.
The explanation for the higher intertagger agreement was that &amp;quot;naive speakers&amp;quot; (tag-
gers) have a mental lexicon different from that of the lexicographers. The even bigger
difference for the group using randomly ordered senses was explained by the hypoth-
esis that, under such a condition, &amp;quot;the taggers must have examined all senses rather
carefully before making a selection&amp;quot; (p. 220). It is surprising to me that the authors
would resort to these two additional hypotheses when the observations could be ex-
plained by a hypothesis they have already made, i.e., taggers examine word senses
in the order they are listed and ignore the rest of the list when they find one sense
that is satisfactory to them. This common search strategy means that taggers tend to
make the same mistakes, and consequently explains the higher intertagger agreement.
The group using frequency-ordered senses got higher tagger-expert agreement than
the other group because this search strategy works better under that condition.
Philip Resnik&apos;s Chapter 10, &amp;quot;WordNet and class-based probabilities,&amp;quot; estimates
the probabilities of the concepts (synsets) in WordNet with an untagged text corpus.
The probability distribution can then be used to determine the &amp;quot;selectional prefer-
ence strength&amp;quot; of verbs. For example, the verb drink has a much stronger selectional
preference on its object than the verb have. The most interesting aspect of this work
is that it combines symbolic knowledge about linguistic relationships with statistical
knowledge about language use. With the addition of statistical knowledge, the rela-
tionships in WordNet can be quantitatively differentiated. With the symbolic taxonomy
in WordNet, probabilities can be distributed over classes, as well as words.
The next four chapters deal with the word sense disambiguation problem in one
way or another. The disambiguation algorithms in the four chapters are based on
the same assumption: in the local Context of the target word (the word to be disam-
biguated), one can expect to find other words that are closely related to the intended
meaning of the target word. Given this assumption, the intended meaning of a word
can be identified by scoring its potential senses with the potential senses of words in
the local context or by finding connections among the senses of the word to senses of
other words in the context and eliminating those senses that are not involved in any
connection.
In &amp;quot;Combining local context and WordNet similarity for word sense identification&amp;quot;
by Leacock and Chodorow, senses of the target word are scored by their similarity
to the senses of other words in the local context (e.g., ±2 words). The authors also
combined this method with a naive Bayes type of algorithm and showed that the
combination resulted in significant improvements (about 5%).
Ellen Voorhees&apos;s Chapter 12, &amp;quot;Using WordNet for text retrieval,&amp;quot; is based on her
earlier work (Voorhees 1993). Potential senses of the target word are scored by totaling
the frequencies of the words in their respective &amp;quot;hoods.&amp;quot; The hood of a word&apos;s sense
is the maximal portion of WordNet that contains the sense but not any other sense
of the word. The main finding in her experiment with word sense disambiguation
in query expansion is the following: when sense disambiguation is perfectly correct,
query expansion with WordNet can improve the performance of short queries, but it
does not make any significant difference with long queries; when the disambiguation
algorithm is less than perfect, query expansion can even hurt the retrieval perfor-
mance.
</bodyText>
<page confidence="0.994821">
294
</page>
<subsectionHeader confidence="0.392498">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999946">
Hirst and St-Onge&apos;s Chapter 12 is titled &amp;quot;Lexical chains as representations of con-
text for the detection and correction of malapropisms.&amp;quot; &amp;quot;A malapropism is the con-
founding of an intended word with another word of similar sound or spelling that
has a quite different and malapropos meaning, for example an ingenuous [for inge-
nious] machine for peeling oranges.&amp;quot; A lexical chain is a path that connects senses of a
group of words in a document. Each link in the path is a lexicosemantic relationship
in WordNet and is associated with a weight that indicates the strength of the relation-
ship. Hirst and St-Onge imposed a set of constraints on the paths to &amp;quot;ensure the path
corresponds to a reasonable relation between the source and the target word.&amp;quot; For
example, a hyponym link cannot be followed later on by a hypernym link. In other
words, after the context is narrowed down, it must not be enlarged again. An alarm
is raised if a word is not connected through a lexical chain to any other word in the
context, but a similarly spelled word would be. Hirst and St-Onge tested their system
on a 322,645-word corpus with 1,409 malapropisms. Their results showed that alarms
were raised for 28.2% of the malapropisms and the false-alarm rate is 87.5%. The basic
idea in this chapter is the same as that of Morris and Hirst (1991), which used Roget&apos;s
Thesaurus instead of WordNet. However, the algorithm of Morris and Hirst was not
implemented due to the lack of lexical resources.
Chapter 14, &amp;quot;Temporal indexing through lexical chaining&amp;quot; by Al-Halimi and Kaz-
man, discusses the use of trees, instead of paths, to connect related words in transcripts
of audio tapes. The goal of their application is to retrieve segments of audio tapes that
are relevant to a query. This is achieved by creating a lexical tree for the query and
retrieving the tape segments with lexical trees that are most similar to it.
Chapter 15, &amp;quot;COLOR-X: Using knowledge from WordNet for conceptual model-
ing&amp;quot; by Burg and van de Riet, is only tangentially related to computational linguistics.
Their basic idea is the following: Since the conceptual models of software systems
involve many classes of entities and relationships that are represented in WordNet,
why not retrieve them from WordNet so that the software designers do not have to
come up with the relationships themselves?
The last chapter, &amp;quot;Knowledge processing on an extended WordNet&amp;quot; by Harabagiu
and Moldovan, treats WordNet as a semantic network. A marker-passing algorithm
similar to that of Charniak (1986) and Norvig (1989) was employed to make &amp;quot;text in-
ferences.&amp;quot; The markers are claimed to be &amp;quot;intelligent markers&amp;quot; that could enforce their
own constraints. However, the &amp;quot;intelligence&amp;quot; of the markers is not explicitly described
in the paper. The paper contains more elaborate examples than earlier marker-passing
papers. Unfortunately, that seems to be all. The algorithm is not implemented nor
tested with real data.
One problem with the last three chapters is the lack of proper evaluation of the
proposed algorithms. Al-Halimi and Kazman evaluated their lexical-tree-building al-
gorithm by comparing its output on a single 1,800-word article with keywords se-
lected by an unspecified number of human subjects. Neither Burg and van de Riet nor
Harabagiu and Moldovan performed any form of evaluation.
Since WordNet is a large-scale lexical resource, without quantitative evaluation, it
may be impossible to predict how well an algorithm will work or even whether or
not it will work at all. The following example is found in Voorhees&apos;s chapter (p. 294):
The nouns nail, hammer, and carpenter are all good hints that the in-
tended sense of board is the &apos;lumber&apos; sense. However, within WordNet
a nail is a fastener, which in turn is a device, so nail would help select
the &apos;control panel&apos; sense of board. Similarly, a hammer is a tool, which
is an implement, which is an article of commerce, so hammer would
</bodyText>
<page confidence="0.992037">
295
</page>
<note confidence="0.705002">
Computational Linguistics Volume 25, Number 2
</note>
<bodyText confidence="0.999985178571429">
help select the &apos;dining table&apos; sense of board. Finally, a carpenter is a
worker, which is a person, which is both an agent and a life form,
which are both things. Thus, carpenter would not help select any sense
of board.
Similarly, as pointed out by Hirst and St-Onge, in WordNet, stew and steak are not
closely related, but public and professional are.
About half of the chapters are revised versions of their authors&apos; earlier publications
in journals or reasonably accessible conference proceedings. Perhaps for this reason,
readers who look for brand-new ideas in the book may feel somewhat disappointed.
On the other hand, given the importance of WordNet, it is convenient to have them
in a single collection. Furthermore, it offers a historical perspective of WordNet and a
relatively complete coverage of its applications.
The book also highlights some common issues that arise from different appli-
cations. For example, all the application papers that are related to word sense dis-
ambiguation expressed the need for what Hirst and St-Onge called &amp;quot;situation rela-
tions&amp;quot; (p. 318), which connect entities involved in the same event or scenario, such
as Nasdaq—share and hospital —physician. One of George Miller&apos;s assumptions about
WordNet is that lexical knowledge can be separated from other types of knowledge.
Incorporation of such relations in WordNet would mean the abandonment of this as-
sumption, as situation relations do not seem to be part of lexical knowledge. Another
possibility is that situation relations should be acquired from corpus data, instead of
being encoded in WordNet. However, none of the chapters explored this idea.
I found the discussions about lexicosemantic relationships in Part I most insightful
and thought-provoking. As a description of the software, however, papers in Part I
are not as systematic and organized as those of Miller (1990) (which are included in
the WordNet software distribution at http.//www.cogsci.princeton.edu/wn/). There is a
great deal of variation in the quality of papers in Parts II and III. Overall, I consider
the book to be worthwhile.
</bodyText>
<sectionHeader confidence="0.977047" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.996510444444445">
Charniak, Eugene. 1986. A neat theory of
marker passing. In Proceedings of the 5th
National Conference on Artificial Intelligence
(AAAI-86), volume 1, pages 584-588.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press.
Miller, George A., editor. 1990. WordNet: An
on-line lexical database. Special issue of
International Journal of Lexicography, 3(4).
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 17: 21-48.
Norvig, Peter. 1989. Marker passing as a
weak method for text inferencing.
Cognitive Science, 13: 569-620.
Voorhees, Ellen M. 1993. Using WordNet to
disambiguate word senses for text
retrieval. In Proceedings of the Sixteenth
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 171-180. ACM Press.
Dekang Lin is Associate Professor of Computer Science at the University of Manitoba. His re-
search interests include principle-based broad-coverage parsing, information extraction, word
sense disambiguation, and learning from parsed corpora. Lin&apos;s address is: Department of Com-
puter Science, University of Manitoba, Winnipeg, Manitoba, Canada, R3T 2N2; e-mail:
lindek@cs.umanitoba.ca
</reference>
<page confidence="0.998496">
296
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.191341">
<title confidence="0.999517">WordNet: An Electronic Lexical Database</title>
<author confidence="0.976997">Christiane Fellbaum</author>
<affiliation confidence="0.99932">(Rider University and Princeton University)</affiliation>
<address confidence="0.975818">Cambridge, MA: The MIT Press</address>
<keyword confidence="0.504659">(Language, speech, and communication series), 1998, xxii+423 pp; hardbound,</keyword>
<note confidence="0.976006">ISBN 0-262-06197-X, $50.00 Reviewed by</note>
<author confidence="0.990145">Dekang Lin</author>
<affiliation confidence="0.990304">University of Manitoba</affiliation>
<abstract confidence="0.8980962">WordNet is perhaps the most important and widely used lexical resource for natural processing systems up to now. An Electronic Lexical Database, by Christiane Fellbaum, discusses the design of WordNet from both theoretical and historical perspectives, provides an up-to-date description of the lexical database, and presents a set of applications of WordNet. The book contains a foreword by George</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A neat theory of marker passing.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th National Conference on Artificial Intelligence (AAAI-86),</booktitle>
<volume>1</volume>
<pages>584--588</pages>
<contexts>
<context position="13205" citStr="Charniak (1986)" startWordPosition="2093" endWordPosition="2094">g knowledge from WordNet for conceptual modeling&amp;quot; by Burg and van de Riet, is only tangentially related to computational linguistics. Their basic idea is the following: Since the conceptual models of software systems involve many classes of entities and relationships that are represented in WordNet, why not retrieve them from WordNet so that the software designers do not have to come up with the relationships themselves? The last chapter, &amp;quot;Knowledge processing on an extended WordNet&amp;quot; by Harabagiu and Moldovan, treats WordNet as a semantic network. A marker-passing algorithm similar to that of Charniak (1986) and Norvig (1989) was employed to make &amp;quot;text inferences.&amp;quot; The markers are claimed to be &amp;quot;intelligent markers&amp;quot; that could enforce their own constraints. However, the &amp;quot;intelligence&amp;quot; of the markers is not explicitly described in the paper. The paper contains more elaborate examples than earlier marker-passing papers. Unfortunately, that seems to be all. The algorithm is not implemented nor tested with real data. One problem with the last three chapters is the lack of proper evaluation of the proposed algorithms. Al-Halimi and Kazman evaluated their lexical-tree-building algorithm by comparing it</context>
</contexts>
<marker>Charniak, 1986</marker>
<rawString>Charniak, Eugene. 1986. A neat theory of marker passing. In Proceedings of the 5th National Conference on Artificial Intelligence (AAAI-86), volume 1, pages 584-588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<marker>Levin, 1993</marker>
<rawString>Levin, Beth. 1993. English Verb Classes and Alternations. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>editor</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>Special issue of International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<marker>Miller, editor, 1990</marker>
<rawString>Miller, George A., editor. 1990. WordNet: An on-line lexical database. Special issue of International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<pages>21--48</pages>
<contexts>
<context position="11990" citStr="Morris and Hirst (1991)" startWordPosition="1897" endWordPosition="1900">tion between the source and the target word.&amp;quot; For example, a hyponym link cannot be followed later on by a hypernym link. In other words, after the context is narrowed down, it must not be enlarged again. An alarm is raised if a word is not connected through a lexical chain to any other word in the context, but a similarly spelled word would be. Hirst and St-Onge tested their system on a 322,645-word corpus with 1,409 malapropisms. Their results showed that alarms were raised for 28.2% of the malapropisms and the false-alarm rate is 87.5%. The basic idea in this chapter is the same as that of Morris and Hirst (1991), which used Roget&apos;s Thesaurus instead of WordNet. However, the algorithm of Morris and Hirst was not implemented due to the lack of lexical resources. Chapter 14, &amp;quot;Temporal indexing through lexical chaining&amp;quot; by Al-Halimi and Kazman, discusses the use of trees, instead of paths, to connect related words in transcripts of audio tapes. The goal of their application is to retrieve segments of audio tapes that are relevant to a query. This is achieved by creating a lexical tree for the query and retrieving the tape segments with lexical trees that are most similar to it. Chapter 15, &amp;quot;COLOR-X: Usin</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, Jane and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17: 21-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Norvig</author>
</authors>
<title>Marker passing as a weak method for text inferencing.</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<volume>13</volume>
<pages>569--620</pages>
<contexts>
<context position="13223" citStr="Norvig (1989)" startWordPosition="2096" endWordPosition="2097">dNet for conceptual modeling&amp;quot; by Burg and van de Riet, is only tangentially related to computational linguistics. Their basic idea is the following: Since the conceptual models of software systems involve many classes of entities and relationships that are represented in WordNet, why not retrieve them from WordNet so that the software designers do not have to come up with the relationships themselves? The last chapter, &amp;quot;Knowledge processing on an extended WordNet&amp;quot; by Harabagiu and Moldovan, treats WordNet as a semantic network. A marker-passing algorithm similar to that of Charniak (1986) and Norvig (1989) was employed to make &amp;quot;text inferences.&amp;quot; The markers are claimed to be &amp;quot;intelligent markers&amp;quot; that could enforce their own constraints. However, the &amp;quot;intelligence&amp;quot; of the markers is not explicitly described in the paper. The paper contains more elaborate examples than earlier marker-passing papers. Unfortunately, that seems to be all. The algorithm is not implemented nor tested with real data. One problem with the last three chapters is the lack of proper evaluation of the proposed algorithms. Al-Halimi and Kazman evaluated their lexical-tree-building algorithm by comparing its output on a sing</context>
</contexts>
<marker>Norvig, 1989</marker>
<rawString>Norvig, Peter. 1989. Marker passing as a weak method for text inferencing. Cognitive Science, 13: 569-620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Using WordNet to disambiguate word senses for text retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>171--180</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="10001" citStr="Voorhees 1993" startWordPosition="1561" endWordPosition="1562">e word to senses of other words in the context and eliminating those senses that are not involved in any connection. In &amp;quot;Combining local context and WordNet similarity for word sense identification&amp;quot; by Leacock and Chodorow, senses of the target word are scored by their similarity to the senses of other words in the local context (e.g., ±2 words). The authors also combined this method with a naive Bayes type of algorithm and showed that the combination resulted in significant improvements (about 5%). Ellen Voorhees&apos;s Chapter 12, &amp;quot;Using WordNet for text retrieval,&amp;quot; is based on her earlier work (Voorhees 1993). Potential senses of the target word are scored by totaling the frequencies of the words in their respective &amp;quot;hoods.&amp;quot; The hood of a word&apos;s sense is the maximal portion of WordNet that contains the sense but not any other sense of the word. The main finding in her experiment with word sense disambiguation in query expansion is the following: when sense disambiguation is perfectly correct, query expansion with WordNet can improve the performance of short queries, but it does not make any significant difference with long queries; when the disambiguation algorithm is less than perfect, query expa</context>
</contexts>
<marker>Voorhees, 1993</marker>
<rawString>Voorhees, Ellen M. 1993. Using WordNet to disambiguate word senses for text retrieval. In Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 171-180. ACM Press.</rawString>
</citation>
<citation valid="false">
<title>Dekang Lin is Associate Professor of Computer Science at the University of Manitoba. His research interests include principle-based broad-coverage parsing, information extraction, word sense disambiguation, and learning from parsed corpora. Lin&apos;s address is:</title>
<institution>Department of Computer Science, University of Manitoba,</institution>
<location>Winnipeg, Manitoba, Canada, R3T</location>
<note>2N2; e-mail: lindek@cs.umanitoba.ca</note>
<marker></marker>
<rawString>Dekang Lin is Associate Professor of Computer Science at the University of Manitoba. His research interests include principle-based broad-coverage parsing, information extraction, word sense disambiguation, and learning from parsed corpora. Lin&apos;s address is: Department of Computer Science, University of Manitoba, Winnipeg, Manitoba, Canada, R3T 2N2; e-mail: lindek@cs.umanitoba.ca</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>