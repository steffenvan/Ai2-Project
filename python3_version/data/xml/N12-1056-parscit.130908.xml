<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010365">
<title confidence="0.994017">
Space Efficiencies in Discourse Modeling via Conditional Random Sampling
</title>
<author confidence="0.957816">
Brian Kjersten Benjamin Van Durme
</author>
<affiliation confidence="0.8204165">
CLSP HLTCOE
Johns Hopkins University Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.985116" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998680933333333">
Recent exploratory efforts in discourse-level
language modeling have relied heavily on cal-
culating Pointwise Mutual Information (PMI),
which involves significant computation when
done over large collections. Prior work has
required aggressive pruning or independence
assumptions to compute scores on large col-
lections. We show the method of Condi-
tional Random Sampling, thus far an underuti-
lized technique, to be a space-efficient means
of representing the sufficient statistics in dis-
course that underly recent PMI-based work.
This is demonstrated in the context of induc-
ing Shankian script-like structures over news
articles.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999857">
It has become common to model the distributional
affinity between some word or phrase pair, (wi, wj),
as a function of co-occurance within some con-
text boundary. Church and Hanks (1990) suggested
pointwise mutual information: PMI(wi, wj) =
</bodyText>
<equation confidence="0.963118">
log Pr(w�,w .)
Pr(w�) Pr(w .), showing linguistically appealing
</equation>
<bodyText confidence="0.99965908">
results using contexts defined by fixed width n-gram
windows, and syntactic dependencies derived from
automatically parsed corpora. Later work such as
by Lin (1999) continued this tradition. Here we con-
sider document, or discourse-level contexts, such as
explored by Rosenfeld (1994) or Church (2000), and
more recently by those such as Chambers and Juraf-
sky (2008) or Van Durme and Lall (2009b).
In the spirit of recent work in randomized algo-
rithms for large-scale HLT (such as by Ravichandran
et al. (2005), Talbot and Osborne (2007), Goyal et
al. (2010), Talbot and Brants (2008),Van Durme and
Lall (2009a), Levenberg and Osborne (2009), Goyal
et al. (2010), Petrovic et al. (2010), Van Durme and
Lall (2010), or Goyal and Daum´e (2011)), we pro-
pose the method of Conditional Random Sampling
(CRS) by Li and Church (2007) as an efficient way
to store approximations of the statistics used to cal-
culate PMI for applications in inducing rudimentary
script-like structures.
Efficiently storing such structures is an impor-
tant step in integrating document-level statistics into
downstream tasks, such as characterizing complex
scenarios (Chambers and Jurafsky, 2011), or story
understanding (Gordon et al., 2011).
</bodyText>
<sectionHeader confidence="0.980631" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.992887">
Conditional Random Sampling (CRS) Li and
Church (2007) proposed CRS to approximate the
contingency table between elements in a query, to
be used in distributional similarity measures such
as cosine similarity, correlation, and PMI. Central
is the idea of the postings list, which is made up
of the identifiers of each document that contains a
given word or phrase. A set of such lists, one per
type in the underlying vocabulary, is known as an
inverted index. To reduce storage costs, a CRS trun-
cates these lists, now called sketches, such that each
sketch is no larger than some length parameter k.
Formally, assume an ordered list of document
identifiers, Q = (1, 2,...), where each referenced
document is a bag of words drawn from a vocabu-
lary of size V . Let Pi C Q be the postings list for
some element wi E V . The function 7r represents a
</bodyText>
<page confidence="0.906204333333333">
513
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 513–517,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999373461538462">
random permutation on the space of identifiers in Q.
The sketch, Si, is defined as the first k elements of
the permuted list: Si = mink(7r(Pi)). 1
Let q be a two-element query, (wi, wj). Given
the postings lists for wi, wj, we can construct
a four-cell contingency table containing the fre-
quency of documents that contained only wi, only
wj, both together, or neither. A CRS allows for
approximating this table in O(k) time by rely-
ing on a sample of Q, specific to q: 7r(Q)q =
(1, 2, 3, ..., min(max(Si), max(Sj))).
The PMI of q, given Q, can be estimated from
7r(Q)q using the approximate word occurrence,
</bodyText>
<equation confidence="0.6563185">
Pr(wi) = |Si ∩7r(Q)q|/|7r(Q)q|, and co-occurrence,
Pr(wi ∩ wj) = |Si ∩ Sj ∩ 7r(Q)q|/|7r(Q)q|.
</equation>
<bodyText confidence="0.999939307692308">
This scheme generalizes to longer queries of
length m, where storage costs remain O(V k), and
query time scales at O(mk). Li and Church (2007)
proved that CRS produces an unbiased estimate of
the probabilities, and showed empirically that vari-
ance is a function of k and m.
Despite its simplicity and promise for large-scale
data mining in NLP, CRS has thus-far seen minimal
application in the community.
Trigger Language Models As here, Rosenfeld
(1994)’s work on trigger language models was con-
cerned with document level context. He identified
trigger pairs: pairs of word sequences where the
presence of the first word sequence affects the prob-
ability of the other, possibly at long distances. He
recommended selecting a small list of trigger pairs
based on the highest average mutual information
(often simply called mutual information), although
intuitively PMI could also be used. Computational
constraints forced him to apply heavy pruning to the
bigrams in his model.
Scripts A script, proposed by Schank (1975), is a
form of Minsky-style frame that captures common-
sense knowledge regarding typical events. For ex-
ample, if a machine were to reason about eating at a
restaurant, it should associate to this event: the ex-
</bodyText>
<footnote confidence="0.566906">
1For example, assume some word wi that appears in doc-
</footnote>
<bodyText confidence="0.98736056">
uments d1, d4, d10 and d12. The identifiers are then randomly
permuted via it such that: d&apos;3 = d1, d&apos;2 = d4, d&apos;7 = d10 and
d&apos;1 = d12. Following permutation, the postings list for wi is
made up of identifiers that map to the same underlying docu-
ments as before, but now in a different order. If we let k = 3,
then Si = (1, 2, 3), corresponding to documents: (d12, d4, d1).
istence of a customer or patron that usually pays for
the meal that is ordered by the patron, then served
by the waiter, etc.
Chambers and Jurafsky (2008) suggested induc-
ing a similar structure called a narrative chain: fo-
cus on the situational descriptions explicitly pertain-
ing to a single protagonist, a series of references
within a document that are automatically labeled
as coreferent. With a large corpus, one can then
find those sets of verbs (as anchors of basic sit-
uational descriptions) which tend to co-occur, and
share a protagonist, leading to an approximate sub-
set of Schank’s original conception.2
Underlying the co-occurrence framework of
Chambers and Jurafsky was finding those verbs with
high PMI. Starting with some initial element, chains
were built greedily by adding the term, x, that max-
imized the average of the pairwise PMI between x
and every term already in the chain:
</bodyText>
<equation confidence="0.9970765">
Wn+1 = arg max
W
</equation>
<bodyText confidence="0.999531">
By relying on the average pairwise PMI, they are
making independence assumptions that are not al-
ways valid. In order to consider more nuanced joint
effects between more than two terms, more efficient
methods would need to be considered.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9991188">
Setup Following Chambers and Jurafsky (2008),
we extracted and lemmatized the verbs from the
New York Times section of the Gigaword Corpus us-
ing the Stanford POS tagger (Toutanova et al., 2004)
and the Morpha lemmatizer (Minnen et al., 2000).
After filtering various POS tagger errors and setting
a minimum document frequency (df) of 50, we went
from a vocabulary of 94,803 words to 8,051.3 For
various values of k we built sketches over 1,655,193
documents, for each resulting word type.
</bodyText>
<construct confidence="0.7612352">
2Given a large collection of news articles, some on the topic
of local crime, one might see a story such as: “... searched for
Michaeli ... hei was arrested ... Mikei plead guilty ... convicted
himi ...”, helping to support an induced chain: (search, arrest,
plead, acquit, convict, sentence).
</construct>
<footnote confidence="0.5190675">
3Types containing punctuation other than hyphens and un-
derscores were discarded as tagger-error.
</footnote>
<equation confidence="0.8323638">
pmi(W, Wj)
1
n
n
j=1
</equation>
<page confidence="0.997111">
514
</page>
<tableCaption confidence="0.999921">
Table 1: Top-n by approximate PMI, for varying k. Subscripts denote rank under true PMI, when less than 50.
</tableCaption>
<table confidence="0.877823333333333">
plead
1 sentence4 sentence4 sentence4
2 commit_ defraud5 misbrand2
3 indict10 indict10 defraud5
4 prosecute33 arraign6 arraign6
5 abuse_ conspire11 manslaughter1
6 convict24 convict24 bilk8
k= 100 1,000 10,000
plead, admit
</table>
<equation confidence="0.993568933333334">
abuse_ sentence5
convict22 prosecute15
owe_ testify20
investigate_ indict10
understand_ defraud7
defraud7 convict22
1,000 10,000
plead, admit, convict
owe_ sentence2
admitt11 prosecute3
engage_ arrest8
investigate28 testify5
prey_ acquit1
defraud_ indict4
1,000 10,000
</equation>
<bodyText confidence="0.9999338">
We use a generalized definition of PMI for three
or more items as the logarithm of the joint probabil-
ity divided by the product of the marginals.
Subjective Quality We first consider the lemma-
tized version of the motivating example by Cham-
bers and Jurafsky (2008): [plead, admit, convict],
breaking it into 1-, 2-, and 3-element seeds. They
reported the top 6 elements that maximize average
pairwise PMI as: sentence, parole, fire, indict, fine,
deny. We see similar results in Table 1, while not-
ing again the distinction in underlying statistics: we
did not restrict ourselves to cooccurrence based on
shared coreferring arguments.
These results show intuitive discourse-level rela-
tionships with a sketch size as small as k = 100
for the unary seed. In addition, when examining the
true PMI rank of each of these terms (reflected as
subscripts), we see that highly ranked items in the
approximate lists come from the set of items highly
ranked in the non-approximate version.4 A major
benefit of the approach is that it allows for approxi-
mate scoring of larger sets of elements jointly, with-
out the traditionally assumed storage penalty.5
Accuracy 1 We measured the trade-off between
PMI approximation accuracy and sketch size.
Triples of verb tokens were sampled at random from
the narrative cloze test set of Chambers and Jurafsky
(2008). Seed terms were limited to verbs with df be-
tween 1,000 and 100,000 to extract lists of the top-
25 candidate verbs by joint, approximate PMI. For
</bodyText>
<footnote confidence="0.887291375">
4The word ”sentence” is consistently higher ranked in the
approximate PMI list than it is in the true PMI list: results stem
from a given shared permutation across the queries, and thus
approximation errors are more likely to be correlated.
5For example, we report that PMI(plead, admit, convict)
&gt; PMI(plead, admit, owe), when k = 1, 000, as com-
pared to: avg(PMI(plead, convict), PMI(admit, convict)) &gt;
avg(PMI(plead, owe), PMI(admit, owe)).
</footnote>
<bodyText confidence="0.999187702702703">
a given rank r, we measured the overlap of the true
top-3 PMI and the approximate list, rank r or higher
(see Figure 1(a)). If query size is 2, k = 10, 000,
the true top-3 true PMI items tend to rank well in
the approximate PMI list. We observe that these
randomly assembled queries tax the sketch-based
approximation, motivating the next experiment on
non-uniformly sampled queries.
Accuracy 2 In a more realistic scenario, we might
have more discretion in selecting terms of interest.
Here we chose the first word of each seed uniformly
at random from each document, and selected subse-
quent seed words to maximize the true PMI with the
established words in the seed. We constrained the
seed terms to have df between 1,000 and 100,000.
Then, for each seed of length 1, 2, and 3 words,
we found the 25-best list of terms using approximate
PMI, considering only terms that occur in more than
50 documents. Figure 1(b) shows the results of this
PMI approximation tradeoff. With a sketch size of
10,000, a rank of 5 is enough to contain two out of
the top three items, and the number gradually con-
tinues to grow as rank size increases.
Memory Analysis Accuracy in a CRS is a func-
tion of the aggressiveness in space savings: as k ap-
proaches the true length of the posting list for wi,
the resulting approximations are closer to truth, at
the cost of increased storage. When k = oc, CRS is
the same as using an inverted index: Fig. 2 shows the
percent memory required for our data, compared to a
standard index, as the sketch size increases. For our
data, a full index involves storing 95 million docu-
ment numbers. For the k = 10, 000 results, we see
that 23% of a full index was needed.
Figure 1(c) shows the quality of approximate best
PMI lists as memory usage is varied. A 2-word
query needs about 20% of the memory for 2.5 of the
</bodyText>
<page confidence="0.990295">
515
</page>
<figure confidence="0.999735587301588">
Mean.Observed
2.5
2.0
0.5
0.0
3.0
1.5
1.0
●
●
● ●
k
1000 10000
● 2 3 4
m
● ● ●
●
● ● ●
●
Mean.Observed
2.5
2.0
0.5
0.0
3.0
1.5
1.0
●
●
●
● 2 3 4
1000 10000
m
k
●
●
●
● ● ●
●
●
Mean.Observed
2.5
2.0
0.5
0.0
3.0
1.5
1.0
0 20 40 60 80 100
●
●
●
●
●
●
m
●
2
3
4
5 10 15 20 25 10 15 20 25 Percent.Memory
Rank Rank (c)
(a) (b)
</figure>
<figureCaption confidence="0.9999028">
Figure 1: (a) Average number of true top-3 PMI items when seed terms have 1,000 &lt; df &lt; 100,000 and are chosen uni-
formly at random from documents. (b) Average number of true top-3 PMI items when seeds are moderate-frequency
high-PMI tuples. (c) Average number of true top-3 PMI items in the top ten approximate PMI list, as a function of
memory usage, when seeds are moderate-frequency high-PMI tuples.
Figure 2: % of inverted index stored, as function of k.
</figureCaption>
<bodyText confidence="0.999692523809524">
top 3 true PMI items to appear in the top 10. Over
40% memory is needed for a 4-word query. 2.5 of
the top 3 true PMI items appear in the top 50 when
the memory is about 35%. This suggests that CRS
allows us to use a fraction of the memory of storing
a full inverted index, but that memory requirements
grow with query size.
Discussion Storing exact PMIs of three or four
words would be expensive to store in memory for
any moderately sized vocabulary, because it would
involve storing on the order of V&apos; count statis-
tics. If we are approximating this with a CRS, we
store sketches of length k or less for every word
in the vocabulary, which is O(kV ). Table 1 and
Fig. 1(b) show that the two-word queries start to
get good performance when k is near 10,000. This
requires 22.7% of the memory of a complete in-
verted index, or 21.5 million postings. The three
and four word queries get good performance near
k = 100, 000. With this sketch size, 60.5 million
postings are stored.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99998125">
We have proposed using Conditional Random Sam-
pling for approximating PMI in the discourse under-
standing community. We have shown that the ap-
proximate PMI rank list produces results that are in-
tuitive and consistent with the exact PMI even with
significant memory savings. This enables us to ap-
proximate PMI for tuples longer than pairs without
undue independence assumptions. One future av-
enue is to explore the use of this structure in appli-
cations such as machine translation, as potentially
enabling greater use of long distance dependencies
than in prior work, such as by Hasan et al. (2008).
</bodyText>
<sectionHeader confidence="0.998888" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99975875">
We acknowledge support from the National Science
Foundation PIRE Grant No. OISE-0530118. We
acknowledge the Army Research Laboratory for its
support to the first author under SCEP (the Student
Career Experience Program). Any opinions, find-
ings, conclusions, or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of the supporting agencies.
</bodyText>
<figure confidence="0.997937777777778">
log10(k)
Percent.Total
100
80
60
40
20
2 3
4 5 6
</figure>
<page confidence="0.993456">
516
</page>
<sectionHeader confidence="0.987422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999563441176471">
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of ACL.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22–29.
Kenneth W. Church. 2000. Empirical estimates of adap-
tation: The chance of two noriegas is closer to p/2 than
p2. In Proceedings of COLING.
Andrew Gordon, Cosmin Bejan, and Kenji Sagae. 2011.
Commonsense causal reasoning using millions of per-
sonal stories. In Proceedings of AAAI.
Amit Goyal and Hal Daum´e. 2011. Lossy conservative
update (lcu) sketch: Succinct approximate count stor-
age. In AAAI.
Amit Goyal, Jagadeesh Jagarlamundi, Hal Daum´e, and
Suresh Venkatasubramanian. 2010. Sketch techniques
for scaling distributional similarity to the web. In 6th
WAC Workshop at NAACL-HLT.
Sasa Hasan, Juri Ganitkevitch, Hermann Ney, and
J. Andr´es-Ferrer. 2008. Triplet lexicon models for
statistical machine translation. In Proceedings of
EMNLP.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of EMNLP.
Ping Li and Kenneth Church. 2007. A sketch algo-
rithm for estimating two-way and multi-way associ-
ations. Computational Linguistics, 33(2):305–354.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings of the 1st International Natural Language
Generation Conference.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of NAACL.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proceedings of ACL.
Ronald Rosenfeld. 1994. Adaptive statistical language
modeling: A maximum entropy approach. Ph.D. the-
sis, Carnegie Mellon University.
Roger C. Schank. 1975. Using knowledge to understand.
In Theoretical Issues in Natural Language Processing.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceeedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proceedings of ACL.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2004. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
Benjamin Van Durme and Ashwin Lall. 2009a. Proba-
bilistic Counting with Randomized Storage. In Pro-
ceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2009b. Stream-
ing pointwise mutual information. In NIPS.
Benjamin Van Durme and Ashwin Lall. 2010. Online
Generation of Locality Sensitive Hash Signatures. In
Proceedings of ACL.
</reference>
<page confidence="0.996868">
517
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.840319">
<title confidence="0.999959">Space Efficiencies in Discourse Modeling via Conditional Random Sampling</title>
<author confidence="0.999933">Brian Kjersten Benjamin Van_Durme</author>
<affiliation confidence="0.98361">CLSP HLTCOE Johns Hopkins University Johns Hopkins University</affiliation>
<abstract confidence="0.9811484375">Recent exploratory efforts in discourse-level language modeling have relied heavily on calculating Pointwise Mutual Information (PMI), which involves significant computation when done over large collections. Prior work has required aggressive pruning or independence assumptions to compute scores on large collections. We show the method of Conditional Random Sampling, thus far an underutilized technique, to be a space-efficient means of representing the sufficient statistics in discourse that underly recent PMI-based work. This is demonstrated in the context of induc- Shankian structures over news articles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1490" citStr="Chambers and Jurafsky (2008)" startWordPosition="213" endWordPosition="217">utional affinity between some word or phrase pair, (wi, wj), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi, wj) = log Pr(w�,w .) Pr(w�) Pr(w .), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like st</context>
<context position="5933" citStr="Chambers and Jurafsky (2008)" startWordPosition="967" endWordPosition="970">aurant, it should associate to this event: the ex1For example, assume some word wi that appears in documents d1, d4, d10 and d12. The identifiers are then randomly permuted via it such that: d&apos;3 = d1, d&apos;2 = d4, d&apos;7 = d10 and d&apos;1 = d12. Following permutation, the postings list for wi is made up of identifiers that map to the same underlying documents as before, but now in a different order. If we let k = 3, then Si = (1, 2, 3), corresponding to documents: (d12, d4, d1). istence of a customer or patron that usually pays for the meal that is ordered by the patron, then served by the waiter, etc. Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain: focus on the situational descriptions explicitly pertaining to a single protagonist, a series of references within a document that are automatically labeled as coreferent. With a large corpus, one can then find those sets of verbs (as anchors of basic situational descriptions) which tend to co-occur, and share a protagonist, leading to an approximate subset of Schank’s original conception.2 Underlying the co-occurrence framework of Chambers and Jurafsky was finding those verbs with high PMI. Starting with some initial element, c</context>
<context position="8712" citStr="Chambers and Jurafsky (2008)" startWordPosition="1409" endWordPosition="1413"> 5 abuse_ conspire11 manslaughter1 6 convict24 convict24 bilk8 k= 100 1,000 10,000 plead, admit abuse_ sentence5 convict22 prosecute15 owe_ testify20 investigate_ indict10 understand_ defraud7 defraud7 convict22 1,000 10,000 plead, admit, convict owe_ sentence2 admitt11 prosecute3 engage_ arrest8 investigate28 testify5 prey_ acquit1 defraud_ indict4 1,000 10,000 We use a generalized definition of PMI for three or more items as the logarithm of the joint probability divided by the product of the marginals. Subjective Quality We first consider the lemmatized version of the motivating example by Chambers and Jurafsky (2008): [plead, admit, convict], breaking it into 1-, 2-, and 3-element seeds. They reported the top 6 elements that maximize average pairwise PMI as: sentence, parole, fire, indict, fine, deny. We see similar results in Table 1, while noting again the distinction in underlying statistics: we did not restrict ourselves to cooccurrence based on shared coreferring arguments. These results show intuitive discourse-level relationships with a sketch size as small as k = 100 for the unary seed. In addition, when examining the true PMI rank of each of these terms (reflected as subscripts), we see that high</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Templatebased information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2291" citStr="Chambers and Jurafsky, 2011" startWordPosition="340" endWordPosition="343">Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding (Gordon et al., 2011). 2 Background Conditional Random Sampling (CRS) Li and Church (2007) proposed CRS to approximate the contingency table between elements in a query, to be used in distributional similarity measures such as cosine similarity, correlation, and PMI. Central is the idea of the postings list, which is made up of the identifiers of each document that contains a given word or phrase. A set of such lists, one per type in the underlying vocabulary, is known as an inverted index. To reduce storage costs, a CRS truncates these lists, now called sketches, such</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Templatebased information extraction without the templates. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="1006" citStr="Church and Hanks (1990)" startWordPosition="142" endWordPosition="145">s. Prior work has required aggressive pruning or independence assumptions to compute scores on large collections. We show the method of Conditional Random Sampling, thus far an underutilized technique, to be a space-efficient means of representing the sufficient statistics in discourse that underly recent PMI-based work. This is demonstrated in the context of inducing Shankian script-like structures over news articles. 1 Introduction It has become common to model the distributional affinity between some word or phrase pair, (wi, wj), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi, wj) = log Pr(w�,w .) Pr(w�) Pr(w .), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as b</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Patrick Hanks. 1990. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Empirical estimates of adaptation: The chance of two noriegas is closer to p/2 than p2.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1425" citStr="Church (2000)" startWordPosition="204" endWordPosition="205">oduction It has become common to model the distributional affinity between some word or phrase pair, (wi, wj), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi, wj) = log Pr(w�,w .) Pr(w�) Pr(w .), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calc</context>
</contexts>
<marker>Church, 2000</marker>
<rawString>Kenneth W. Church. 2000. Empirical estimates of adaptation: The chance of two noriegas is closer to p/2 than p2. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gordon</author>
<author>Cosmin Bejan</author>
<author>Kenji Sagae</author>
</authors>
<title>Commonsense causal reasoning using millions of personal stories.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="2337" citStr="Gordon et al., 2011" startWordPosition="347" endWordPosition="350">e and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding (Gordon et al., 2011). 2 Background Conditional Random Sampling (CRS) Li and Church (2007) proposed CRS to approximate the contingency table between elements in a query, to be used in distributional similarity measures such as cosine similarity, correlation, and PMI. Central is the idea of the postings list, which is made up of the identifiers of each document that contains a given word or phrase. A set of such lists, one per type in the underlying vocabulary, is known as an inverted index. To reduce storage costs, a CRS truncates these lists, now called sketches, such that each sketch is no larger than some lengt</context>
</contexts>
<marker>Gordon, Bejan, Sagae, 2011</marker>
<rawString>Andrew Gordon, Cosmin Bejan, and Kenji Sagae. 2011. Commonsense causal reasoning using millions of personal stories. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
</authors>
<title>Lossy conservative update (lcu) sketch: Succinct approximate count storage.</title>
<date>2011</date>
<booktitle>In AAAI.</booktitle>
<marker>Goyal, Daum´e, 2011</marker>
<rawString>Amit Goyal and Hal Daum´e. 2011. Lossy conservative update (lcu) sketch: Succinct approximate count storage. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Jagadeesh Jagarlamundi</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Sketch techniques for scaling distributional similarity to the web.</title>
<date>2010</date>
<booktitle>In 6th WAC Workshop at NAACL-HLT.</booktitle>
<marker>Goyal, Jagarlamundi, Daum´e, Venkatasubramanian, 2010</marker>
<rawString>Amit Goyal, Jagadeesh Jagarlamundi, Hal Daum´e, and Suresh Venkatasubramanian. 2010. Sketch techniques for scaling distributional similarity to the web. In 6th WAC Workshop at NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Hasan</author>
<author>Juri Ganitkevitch</author>
<author>Hermann Ney</author>
<author>J Andr´es-Ferrer</author>
</authors>
<title>Triplet lexicon models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Hasan, Ganitkevitch, Ney, Andr´es-Ferrer, 2008</marker>
<rawString>Sasa Hasan, Juri Ganitkevitch, Hermann Ney, and J. Andr´es-Ferrer. 2008. Triplet lexicon models for statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Streambased randomised language models for smt.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1765" citStr="Levenberg and Osborne (2009)" startWordPosition="260" endWordPosition="263">g contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding (Gordon et al., 2011). 2 Background Conditional R</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Streambased randomised language models for smt. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Kenneth Church</author>
</authors>
<title>A sketch algorithm for estimating two-way and multi-way associations.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1950" citStr="Li and Church (2007)" startWordPosition="293" endWordPosition="296">er document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding (Gordon et al., 2011). 2 Background Conditional Random Sampling (CRS) Li and Church (2007) proposed CRS to approximate the contingency table between elements in a query, to be used in distributional similarity measures such as cosine </context>
<context position="4269" citStr="Li and Church (2007)" startWordPosition="680" endWordPosition="683"> we can construct a four-cell contingency table containing the frequency of documents that contained only wi, only wj, both together, or neither. A CRS allows for approximating this table in O(k) time by relying on a sample of Q, specific to q: 7r(Q)q = (1, 2, 3, ..., min(max(Si), max(Sj))). The PMI of q, given Q, can be estimated from 7r(Q)q using the approximate word occurrence, Pr(wi) = |Si ∩7r(Q)q|/|7r(Q)q|, and co-occurrence, Pr(wi ∩ wj) = |Si ∩ Sj ∩ 7r(Q)q|/|7r(Q)q|. This scheme generalizes to longer queries of length m, where storage costs remain O(V k), and query time scales at O(mk). Li and Church (2007) proved that CRS produces an unbiased estimate of the probabilities, and showed empirically that variance is a function of k and m. Despite its simplicity and promise for large-scale data mining in NLP, CRS has thus-far seen minimal application in the community. Trigger Language Models As here, Rosenfeld (1994)’s work on trigger language models was concerned with document level context. He identified trigger pairs: pairs of word sequences where the presence of the first word sequence affects the probability of the other, possibly at long distances. He recommended selecting a small list of trig</context>
</contexts>
<marker>Li, Church, 2007</marker>
<rawString>Ping Li and Kenneth Church. 2007. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics, 33(2):305–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1289" citStr="Lin (1999)" startWordPosition="184" endWordPosition="185">rly recent PMI-based work. This is demonstrated in the context of inducing Shankian script-like structures over news articles. 1 Introduction It has become common to model the distributional affinity between some word or phrase pair, (wi, wj), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi, wj) = log Pr(w�,w .) Pr(w�) Pr(w .), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Robust, applied morphological generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Natural Language Generation Conference.</booktitle>
<contexts>
<context position="7185" citStr="Minnen et al., 2000" startWordPosition="1173" endWordPosition="1176">ing the term, x, that maximized the average of the pairwise PMI between x and every term already in the chain: Wn+1 = arg max W By relying on the average pairwise PMI, they are making independence assumptions that are not always valid. In order to consider more nuanced joint effects between more than two terms, more efficient methods would need to be considered. 3 Experiments Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al., 2004) and the Morpha lemmatizer (Minnen et al., 2000). After filtering various POS tagger errors and setting a minimum document frequency (df) of 50, we went from a vocabulary of 94,803 words to 8,051.3 For various values of k we built sketches over 1,655,193 documents, for each resulting word type. 2Given a large collection of news articles, some on the topic of local crime, one might see a story such as: “... searched for Michaeli ... hei was arrested ... Mikei plead guilty ... convicted himi ...”, helping to support an induced chain: (search, arrest, plead, acquit, convict, sentence). 3Types containing punctuation other than hyphens and under</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust, applied morphological generation. In Proceedings of the 1st International Natural Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1810" citStr="Petrovic et al. (2010)" startWordPosition="268" endWordPosition="271">nd syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding (Gordon et al., 2011). 2 Background Conditional Random Sampling (CRS) Li and Church (2007) pro</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2010</marker>
<rawString>Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to twitter. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1634" citStr="Ravichandran et al. (2005)" startWordPosition="240" endWordPosition="243">uggested pointwise mutual information: PMI(wi, wj) = log Pr(w�,w .) Pr(w�) Pr(w .), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as chara</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Adaptive statistical language modeling: A maximum entropy approach.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="1408" citStr="Rosenfeld (1994)" startWordPosition="201" endWordPosition="202">ews articles. 1 Introduction It has become common to model the distributional affinity between some word or phrase pair, (wi, wj), as a function of co-occurance within some context boundary. Church and Hanks (1990) suggested pointwise mutual information: PMI(wi, wj) = log Pr(w�,w .) Pr(w�) Pr(w .), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statis</context>
<context position="4581" citStr="Rosenfeld (1994)" startWordPosition="732" endWordPosition="733">can be estimated from 7r(Q)q using the approximate word occurrence, Pr(wi) = |Si ∩7r(Q)q|/|7r(Q)q|, and co-occurrence, Pr(wi ∩ wj) = |Si ∩ Sj ∩ 7r(Q)q|/|7r(Q)q|. This scheme generalizes to longer queries of length m, where storage costs remain O(V k), and query time scales at O(mk). Li and Church (2007) proved that CRS produces an unbiased estimate of the probabilities, and showed empirically that variance is a function of k and m. Despite its simplicity and promise for large-scale data mining in NLP, CRS has thus-far seen minimal application in the community. Trigger Language Models As here, Rosenfeld (1994)’s work on trigger language models was concerned with document level context. He identified trigger pairs: pairs of word sequences where the presence of the first word sequence affects the probability of the other, possibly at long distances. He recommended selecting a small list of trigger pairs based on the highest average mutual information (often simply called mutual information), although intuitively PMI could also be used. Computational constraints forced him to apply heavy pruning to the bigrams in his model. Scripts A script, proposed by Schank (1975), is a form of Minsky-style frame t</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>Ronald Rosenfeld. 1994. Adaptive statistical language modeling: A maximum entropy approach. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
</authors>
<title>Using knowledge to understand.</title>
<date>1975</date>
<booktitle>In Theoretical Issues in Natural Language Processing.</booktitle>
<contexts>
<context position="5146" citStr="Schank (1975)" startWordPosition="820" endWordPosition="821">rigger Language Models As here, Rosenfeld (1994)’s work on trigger language models was concerned with document level context. He identified trigger pairs: pairs of word sequences where the presence of the first word sequence affects the probability of the other, possibly at long distances. He recommended selecting a small list of trigger pairs based on the highest average mutual information (often simply called mutual information), although intuitively PMI could also be used. Computational constraints forced him to apply heavy pruning to the bigrams in his model. Scripts A script, proposed by Schank (1975), is a form of Minsky-style frame that captures commonsense knowledge regarding typical events. For example, if a machine were to reason about eating at a restaurant, it should associate to this event: the ex1For example, assume some word wi that appears in documents d1, d4, d10 and d12. The identifiers are then randomly permuted via it such that: d&apos;3 = d1, d&apos;2 = d4, d&apos;7 = d10 and d&apos;1 = d12. Following permutation, the postings list for wi is made up of identifiers that map to the same underlying documents as before, but now in a different order. If we let k = 3, then Si = (1, 2, 3), correspond</context>
</contexts>
<marker>Schank, 1975</marker>
<rawString>Roger C. Schank. 1975. Using knowledge to understand. In Theoretical Issues in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized language models via perfect hash functions.</title>
<date>2008</date>
<booktitle>In Proceeedings of ACL.</booktitle>
<contexts>
<context position="1708" citStr="Talbot and Brants (2008)" startWordPosition="252" endWordPosition="255">r(w .), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story unders</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized language models via perfect hash functions. In Proceeedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1661" citStr="Talbot and Osborne (2007)" startWordPosition="244" endWordPosition="247">formation: PMI(wi, wj) = log Pr(w�,w .) Pr(w�) Pr(w .), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Later work such as by Lin (1999) continued this tradition. Here we consider document, or discourse-level contexts, such as explored by Rosenfeld (1994) or Church (2000), and more recently by those such as Chambers and Jurafsky (2008) or Van Durme and Lall (2009b). In the spirit of recent work in randomized algorithms for large-scale HLT (such as by Ravichandran et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daum´e (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efficient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="7137" citStr="Toutanova et al., 2004" startWordPosition="1165" endWordPosition="1168"> initial element, chains were built greedily by adding the term, x, that maximized the average of the pairwise PMI between x and every term already in the chain: Wn+1 = arg max W By relying on the average pairwise PMI, they are making independence assumptions that are not always valid. In order to consider more nuanced joint effects between more than two terms, more efficient methods would need to be considered. 3 Experiments Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al., 2004) and the Morpha lemmatizer (Minnen et al., 2000). After filtering various POS tagger errors and setting a minimum document frequency (df) of 50, we went from a vocabulary of 94,803 words to 8,051.3 For various values of k we built sketches over 1,655,193 documents, for each resulting word type. 2Given a large collection of news articles, some on the topic of local crime, one might see a story such as: “... searched for Michaeli ... hei was arrested ... Mikei plead guilty ... convicted himi ...”, helping to support an induced chain: (search, arrest, plead, acquit, convict, sentence). 3Types con</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2004</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2004. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Probabilistic Counting with Randomized Storage.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009a. Probabilistic Counting with Randomized Storage. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Streaming pointwise mutual information.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009b. Streaming pointwise mutual information. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Online Generation of Locality Sensitive Hash Signatures.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Van Durme, Lall, 2010</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2010. Online Generation of Locality Sensitive Hash Signatures. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>