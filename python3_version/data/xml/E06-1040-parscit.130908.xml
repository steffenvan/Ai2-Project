<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.052427">
<title confidence="0.887806">
Comparing Automatic and Human Evaluation of NLG Systems
</title>
<author confidence="0.60768">
Anja Belz
</author>
<affiliation confidence="0.690641">
Natural Language Technology Group
CMIS, University of Brighton
</affiliation>
<address confidence="0.427417">
UK
</address>
<email confidence="0.992072">
A.S.Belz@brighton.ac.uk
</email>
<author confidence="0.761757">
Ehud Reiter
</author>
<affiliation confidence="0.918638">
Dept of Computing Science
University of Aberdeen
</affiliation>
<address confidence="0.512506">
UK
</address>
<email confidence="0.996757">
ereiter@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.998588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997248">
We consider the evaluation problem in
Natural Language Generation (NLG) and
present results for evaluating several NLG
systems with similar functionality, includ-
ing a knowledge-based generator and sev-
eral statistical systems. We compare eval-
uation results for these systems by human
domain experts, human non-experts, and
several automatic evaluation metrics, in-
cluding NIST, BLEU, and ROUGE. We
find that NIST scores correlate best (&gt;
0.8) with human judgments, but that all
automatic metrics we examined are bi-
ased in favour of generators that select on
the basis of frequency alone. We con-
clude that automatic evaluation of NLG
systems has considerable potential, in par-
ticular where high-quality reference texts
and only a small number of human evalua-
tors are available. However, in general it is
probably best for automatic evaluations to
be supported by human-based evaluations,
or at least by studies that demonstrate that
a particular metric correlates well with hu-
man judgments in a given domain.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891641025641">
Evaluation is becoming an increasingly important
topic in Natural Language Generation (NLG), as
in other fields of computational linguistics. Some
NLG researchers are impressed by the success of
the BLEU evaluation metric (Papineni et al., 2002)
in Machine Translation (MT), which has trans-
formed the MT field by allowing researchers to
quickly and cheaply evaluate the impact of new
ideas, algorithms, and data sets. BLEU and re-
lated metrics work by comparing the output of an
MT system to a set of reference (‘gold standard’)
translations, and in principle this kind of evalua-
tion could be done with NLG systems as well. In-
deed NLG researchers are already starting to use
BLEU (Habash, 2004; Belz, 2005) in their evalua-
tions, as this is much cheaper and easier to organ-
ise than the human evaluations that have tradition-
ally been used to evaluate NLG systems.
However, the use of such corpus-based evalua-
tion metrics is only sensible if they are known to
be correlated with the results of human-based eval-
uations. While studies have shown that ratings of
MT systems by BLEU and similar metrics corre-
late well with human judgments (Papineni et al.,
2002; Doddington, 2002), we are not aware of any
studies that have shown that corpus-based evalu-
ation metrics of NLG systems are correlated with
human judgments; correlation studies have been
made of individual components (Bangalore et al.,
2000), but not of systems.
In this paper we present an empirical study
of how well various corpus-based metrics agree
with human judgments, when evaluating several
NLG systems that generate sentences which de-
scribe changes in the wind (for weather forecasts).
These systems do not perform content determina-
tion (they are limited to microplanning and realisa-
tion), so our study does not address corpus-based
evaluation of content determination.
</bodyText>
<sectionHeader confidence="0.999325" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.998995">
2.1 Evaluation of NLG systems
</subsectionHeader>
<bodyText confidence="0.9996972">
NLG systems have traditionally been evaluated
using human subjects (Mellish and Dale, 1998).
NLG evaluations have tended to be of the intrinsic
type (Sparck Jones and Galliers, 1996), involving
subjects reading and rating texts; usually subjects
</bodyText>
<page confidence="0.998821">
313
</page>
<bodyText confidence="0.999778342105263">
are shown both NLG and human-written texts, and
the NLG system is evaluated by comparing the rat-
ings of its texts and human texts. In some cases,
subjects are shown texts generated by several NLG
systems, including a baseline system which serves
as another point of comparison. This methodology
was first used in NLG in the mid-1990s by Coch
(1996) and Lester and Porter (1997), and contin-
ues to be popular today.
Other, extrinsic, types of human evaluations
of NLG systems include measuring the impact
of different generated texts on task performance
(Young, 1999), measuring how much experts post-
edit generated texts (Sripada et al., 2005), and
measuring how quickly people read generated
texts (Williams and Reiter, 2005).
In recent years there has been growing interest
in evaluating NLG texts by comparing them to a
corpus of human-written texts. As in other ar-
eas of NLP, the advantages of automatic corpus-
based evaluation are that it is potentially much
cheaper and quicker than human-based evaluation,
and also that it is repeatable. Corpus-based evalu-
ation was first used in NLG by Langkilde (1998),
who parsed texts from a corpus, fed the output of
her parser to her NLG system, and then compared
the generated texts to the original corpus texts.
Similar evaluations have been used e.g. by Banga-
lore et al. (2000) and Marciniak and Strube (2004).
Such corpus-based evaluations have sometimes
been criticised in the NLG community, for example
by Reiter and Sripada (2002). Grounds for crit-
icism include the fact that regenerating a parsed
text is not a realistic NLG task; that texts can be
very different from a corpus text but still effec-
tively meet the system’s communicative goal; and
that corpus texts are often not of high enough qual-
ity to form a realistic test.
</bodyText>
<subsectionHeader confidence="0.977271">
2.2 Automatic evaluation of generated texts
in MT and Summarisation
</subsectionHeader>
<bodyText confidence="0.999916745098039">
The MT and document summarisation communi-
ties have developed evaluation metrics based on
comparing output texts to a corpus of human texts,
and have shown that some of these metrics are
highly correlated with human judgments.
The BLEU metric (Papineni et al., 2002) in MT
has been particularly successful; for example MT-
05, the 2005 NIST MT evaluation exercise, used
BLEU-4 as the only method of evaluation. BLEU
is a precision metric that assesses the quality of a
translation in terms of the proportion of its word n-
grams (n = 4 has become standard) that it shares
with one or more high-quality reference transla-
tions. BLEU scores range from 0 to 1, 1 being the
highest which can only be achieved by a transla-
tion if all its substrings can be found in one of the
reference texts (hence a reference text will always
score 1). BLEU should be calculated on a large
test set with several reference translations (four ap-
pears to be standard in MT). Properly calculated
BLEU scores have been shown to correlate reliably
with human judgments (Papineni et al., 2002).
The NIST MT evaluation metric (Doddington,
2002) is an adaptation of BLEU, but where BLEU
gives equal weight to all n-grams, NIST gives more
importance to less frequent (hence more infor-
mative) n-grams. BLEU’s ability to detect subtle
but important differences in translation quality has
been questioned, some research showing NIST to
be more sensitive (Doddington, 2002; Riezler and
Maxwell III, 2005).
The ROUGE metric (Lin and Hovy, 2003) was
conceived as document summarisation’s answer to
BLEU, but it does not appear to have met with the
same degree of enthusiasm. There are several dif-
ferent ROUGE metrics. The simplest is ROUGE-N,
which computes the highest proportion in any ref-
erence summary of n-grams that are matched by
the system-generated summary. A procedure is
applied that averages the score across leave-one-
out subsets of the set of reference texts. ROUGE-
N is an almost straightforward n-gram recall met-
ric between two texts, and has several counter-
intuitive properties, including that even a text com-
posed entirely of sentences from reference texts
cannot score 1 (unless there is only one refer-
ence text). There are several other variants of the
ROUGE metric, and ROUGE-2, along with ROUGE-
SU (based on skip bigrams and unigrams), were
among the official scores for the DUC 2005 sum-
marisation task.
</bodyText>
<subsectionHeader confidence="0.984822">
2.3 SUMTIME
</subsectionHeader>
<bodyText confidence="0.99976775">
The SUMTIME project (Reiter et al., 2005) de-
veloped an NLG system which generated textual
weather forecasts from numerical forecast data.
The SUMTIME system generates specialist fore-
casts for offshore oil rigs. It has two modules:
a content-determination module that determines
the content of the weather forecast by analysing
the numerical data using linear segmentation and
</bodyText>
<page confidence="0.998497">
314
</page>
<bodyText confidence="0.99988946875">
other data analysis techniques; and a microplan-
ning and realisation module which generates texts
based on this content by choosing appropriate
words, deciding on aggregation, enforcing the
sublanguage grammar, and so forth. SUMTIME
generates very high-quality texts, in some cases
forecast users believe SUMTIME texts are better
than human-written texts (Reiter et al., 2005).
SUMTIME is a knowledge-based NLG system.
While its design was informed by corpus analysis
(Reiter et al., 2003), the system is based on manu-
ally authored rules and code.
As part of the project, the SUMTIME team cre-
ated a corpus of 1045 forecasts from the commer-
cial output of five different forecasters and the in-
put data (numerical predictions of wind, tempera-
ture, etc) that the forecasters examined when they
wrote the forecasts (Sripada et al., 2003). In other
words, the SUMTIME corpus contains both the in-
puts (numerical weather predictions) and the out-
puts (forecast texts) of the forecast-generation pro-
cess. The SUMTIME team also derived a con-
tent representation (called ‘tuples’) from the cor-
pus texts similar to that produced by SUMTIME’s
content-determination module. The SUMTIME
microplanner/realiser can be driven by these tu-
ples; this mode (combining human content deter-
mination with SUMTIME microplanning and real-
isation) is called SUMTIME-Hybrid. Table 1 in-
cludes an example of the tuples extracted from the
corpus text (row 1), and a SUMTIME-Hybrid text
produced from the tuples (row 5).
</bodyText>
<subsectionHeader confidence="0.988926">
2.4 pCRU language generation
</subsectionHeader>
<bodyText confidence="0.999948866666667">
Statistical NLG has focused on generate-and-select
models: a set of alternatives is generated and one
is selected with a language model. This technique
is computationally very expensive. Moreover, the
only type of language model used in NLG are n-
gram models which have the additional disadvan-
tage of a general preference for shorter realisa-
tions, which can be harmful in NLG (Belz, 2005).
pCRU1 language generation (Belz, 2006) is a
language generation framework that was designed
to facilitate statistical generation techniques that
are more efficient and less biased. In pCRU gen-
eration, a base generator is encoded as a set of
generation rules made up of relations with zero
or more atomic arguments. The base generator
</bodyText>
<subsectionHeader confidence="0.799414">
1Probabilistic Context-free Representational Underspeci-
fication.
</subsectionHeader>
<bodyText confidence="0.9997658125">
is then trained on raw text corpora to provide a
probability distribution over generation rules. The
resulting PCRU generator can be run in several
modes, including the following:
Random: ignoring pCRU probabilities, randomly
select generation rules.
N-gram: ignoring pCRU probabilities, generate
set of alternatives and select the most likely ac-
cording to a given n-gram language model.
Greedy: select the most likely among each set of
candidate generation rules.
Greedy roulette: select rules with likelihood pro-
portional to their pCRU probability.
The greedy modes are deterministic and there-
fore considerably cheaper in computational terms
than the equivalent n-gram method (Belz, 2005).
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="method">
3 Experimental Procedure
</sectionHeader>
<bodyText confidence="0.999911580645161">
The main goal of our experiments was to deter-
mine how well a variety of automatic evaluation
metrics correlated with human judgments of text
quality in NLG. A secondary goal was to deter-
mine if there were types of NLG systems for which
the correlation of automatic and human evaluation
was particularly good or bad.
Data: We extracted from each forecast in the
SUMTIME corpus the first description of wind (at
10m height) from every morning forecast (the text
shown in Table 1 is a typical example), which re-
sulted in a set of about 500 wind forecasts. We
excluded several forecasts for which we had no in-
put data (numerical weather predictions) or an in-
complete set of system outputs; this left 465 texts,
which we used in our evaluation.
The inputs to the generators were tuples com-
posed of an index, timestamp, wind direction,
wind speed range, and gust speed range (see ex-
amples at top of Table 1).
We randomly selected a subset of 21 forecast
dates for use in human evaluations. For these 21
forecast dates, we also asked two meteorologists
who had not contributed to the original SUMTIME
corpus to write new forecasts texts; we used these
as reference texts for the automatic metrics. The
forecasters created these texts by rewriting the cor-
pus texts, as this was a more natural task for them
than writing texts based on tuples.
500 wind descriptions may seem like a small
corpus, but in fact provides very good coverage as
</bodyText>
<page confidence="0.991996">
315
</page>
<table confidence="0.9998698">
Input [[0, 0600,SSW,16,20,-,-],[1,NOTIME,SSE,-,-,-,-],[2,0000,VAR,04,08,-,-]]
Corpus SSW 16 -20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENING
Human1 SSW’LY 16-20 GRADUALLY BACKING SSE’LY THEN DECREASING VARIABLE 4-8 BY LATE EVENING
Human2 SSW 16 -20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE 4-8 BY LATE EVENING
SumTime SSW 16 -20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT
pCRU
-greedy SSW 16 -20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING
-roulette SSW 16 -20 GRADUALLY BACKING SSE AND VARIABLE 4-8
-2gram SSW 16 -20 BACKING SSE VARIABLE 4-8 LATER
-random SSW 16 -20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8
</table>
<tableCaption confidence="0.954693">
Table 1: Input tuples with corresponding forecasts in corpus, written by two experts and generated by all
systems (for 5 Oct 2000).
</tableCaption>
<bodyText confidence="0.998465470588235">
the domain language is extremely simple, involv-
ing only about 90 word forms (not counting num-
bers and wind directions) and a small handful of
different syntactic structures.
Systems and texts evaluated: We evaluated
four pCRU generators and the SUMTIME system,
operating in Hybrid mode (Section 2.3) for better
comparability because the pCRU generators do not
perform content determination.
A base pCRU generator was created semi-
automatically by running a chunker over the cor-
pus, extracting generation rules and adding some
higher-level rules taking care of aggregation, eli-
sion etc. This base generator was then trained on
9/10 of the corpus (the training data). 5 different
random divisions of the corpus into training and
testing data were used (i.e. all results were val-
idated by 5-fold hold-out cross-validation). Ad-
ditionally, a back-off 2-gram model with Good-
Turing discounting and no lexical classes was built
from the same training data, using the SRILM
toolkit (Stolcke, 2002). Forecasts were then gen-
erated for all corpus inputs, in all four generation
modes (Section 2.4).
Table 1 shows an example of an input to the sys-
tems, along with the three human texts (Corpus,
Human1, Human2) and the texts produced by all
five NLG systems from this data.
Automatic evaluations: We used NIST2,
BLEU3, and ROUGE4 to automatically evaluate the
above systems and texts. We computed BLEU-N
for N = 1..4 (using BLEU-4 as our main BLEU
score). We also computed NIST-5 and ROUGE-4.
As a baseline we used string-edit (SE) distance
</bodyText>
<footnote confidence="0.999901666666667">
2http://cio.nist.gov/esd/emaildir/lists/mt list/bin00000.bin
3ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
4http://www.isi.edu/˜cyl/ROUGE/latest.html
</footnote>
<bodyText confidence="0.999574607142857">
with substitution at cost 2, and deletion and
insertion at cost 1, and normalised to range 0 to
1 (perfect match). When multiple reference texts
are used, the SE score for a generator forecast
is the average of its scores against the reference
texts; the SE score for a set of generator forecasts
is the average of scores for individual forecasts.
Human evaluations: We recruited 9 experts
(people with experience reading forecasts for off-
shore oil rigs) and 21 non-experts (people with no
such experience). Subjects did not have a back-
ground in NLP, and were native speakers of En-
glish. They were shown forecast texts from all the
generators and from the corpus, and asked to score
them on a scale of 0 to 5, for readability, clarity
and general appropriateness. Experts were addi-
tionally shown the numerical weather data that the
forecast text was based on. At the start, subjects
were shown two practice examples. The exper-
iments were carried out over the web. Subjects
completed the experiment unsupervised, at a time
and place of their choosing.
Expert subjects were shown a randomly se-
lected forecast for 18 of the dates. The non-experts
were shown 21 forecast texts, in a repeated Latin
squares (non-repeating column and row entries)
experimental design where each combination of
date and system is assigned one evaluation.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999900285714286">
Table 2 shows evaluation scores for the five NLG
systems and the corpus texts as assessed by ex-
perts, non-experts, NIST-5, BLEU-4, ROUGE-4 and
SE. Scores are averaged over the 18 forecasts that
were used in the expert experiments (for which we
had scores by all metrics and humans) in order
to make results as directly comparable as possi-
</bodyText>
<page confidence="0.992507">
316
</page>
<table confidence="0.999806857142857">
System Experts Non-experts NIST-5 BLEU-4 ROUGE-4 SE
SUMTIME-Hybrid 0.762 (1) 0.77 (1) 5.985 (2) 0.552 (2) 0.192 (3) 0.582 (3)
pCRU-greedy 0.716 (2) 0.68 (3) 6.549 (1) 0.613 (1) 0.315 (1) 0.673 (1)
SUmTImE-Corpus 0.644 (-) 0.736 (-) 8.262 (-) 0.877 (-) 0.569 (-) 0.835 (-)
pCRU-roulette 0.622 (3) 0.714 (2) 5.833 (3) 0.478 (4) 0.156 (4) 0.571 (4)
pCRU-2gram 0.536 (4) 0.65 (4) 5.592 (4) 0.519 (3) 0.223 (2) 0.626 (2)
pCRU-random 0.484 (5) 0.496 (5) 4.287 (5) 0.296 (5) 0.075 (5) 0.464 (5)
</table>
<tableCaption confidence="0.989958">
Table 2: Evaluation scores against 2 reference texts, for set of 18 forecasts used in expert evaluation.
</tableCaption>
<table confidence="0.999884428571429">
Experts Non-experts NIST-5 BLEU-4 ROUGE-4 SE
Experts 1 (0.799) 0.845 (0.510) 0.825 0.791 0.606 0.576
Non-experts 0.845 (0.496) 1 (0.609) 0.836 0.812 0.534 0.627
NIST-5 0.825 (0.822) 0.836 (0.83) 1 (0.991) 0.973 0.884 0.911
BLEU-4 0.791 (0.790) 0.812 (0.808) 0.973 1 (0.995) 0.925 0.949
ROUGE-4 0.606 (0.604) 0.534 (0.534) 0.884 0.925 1 (0.995) 0.974
SE 0.576 (0.568) 0.627 (0.614) 0.911 0.949 0.974 1 (0.984)
</table>
<tableCaption confidence="0.999969">
Table 3: Pearson correlation coefficients between all scores for systems in Table 2.
</tableCaption>
<bodyText confidence="0.992603265625">
ble. Human scores are normalised to range 0 to 1.
Systems are ranked in order of the scores given to
them by experts. All ranks are shown in brackets
behind the absolute scores.
Both experts and non-experts score SUMTIME-
Hybrid the highest, and pCRU-2gram and pCRU-
random the lowest. The experts have pCRU-
greedy in second place, where the non-experts
have pCRU-roulette. The experts rank the corpus
forecasts fourth, the non-experts second.
We used approximate randomisation (AR) as
our significance test, as recommended by Riezler
and Maxwell III (2005). Pair-wise tests between
results in Table 2 showed all but three differences
to be significant with the likelihood of incorrectly
rejecting the null hypothesis p &lt; 0.05 (the stan-
dard threshold in NLP). The exceptions were the
differences in NIST and SE scores for SUMTIME-
Hybrid/pCRU-roulette, and the difference in BLEU
scores for SUMTIME-Hybrid/pCRU-2gram.
Table 3 shows Pearson correlation coefficients
(PCC) for the metrics and humans in Table 2.
The strongest correlation with experts and non-
experts is achieved by NIST-5 (0.82 and 0.83),
with ROUGE-4 and SE showing especially poor
correlation. BLEU-4 correlates fairly well with the
non-experts but less with the experts.
We computed another correlation statistic
(shown in brackets in Table 3) which measures
how well scores by an arbitrary single human or
run of a metric correlate with the average scores by
a set of humans or runs of a metric. This is com-
puted as the average PCC between the scores as-
signed by individual humans/runs of a metric (in-
dexing the rows in Table 3) and the average scores
assigned by a set of humans/runs of a metric (in-
dexing the columns in Table 3). For example, the
PCC for non-experts and experts is 0.845, but the
average PCC between individual non-experts and
average expert judgment is only 0.496, implying
that an arbitrary non-expert is not very likely to
correlate well with average expert judgments. Ex-
perts are better predictors for each other’s judg-
ments (0.799) than non-experts (0.609). Interest-
ingly, it turns out that an arbitrary NIST-5 run is a
better predictor (0.822) of average expert opinion
than an arbitrary single expert (0.799).
The number of forecasts we were able to use
in our human experiments was small, and to back
up the results presented in Table 2 we report
NIST-5, BLEU-4, ROUGE-4 and SE scores aver-
aged across the five test sets from the pCRU val-
idation runs, in Table 4. The picture is similar
to results for the smaller data set: the rankings
assigned by all metrics are the same, except that
NIST-5 and SE have swapped the ranks of SUM-
TIME-Hybrid and pCRU-roulette. Pair-wise AR
tests showed all differences to be significant with
p &lt; 0.05, except for the differences in BLEU, NIST
and ROUGE scores for SUMTIME-Hybrid/pCRU-
roulette, and the difference in BLEU scores for
SUMTIME-Hybrid/pCRU-2gram.
In both Tables 2 and 4, there are two major
differences between the rankings assigned by hu-
</bodyText>
<page confidence="0.996437">
317
</page>
<table confidence="0.999679142857143">
System Experts NIST-5 BLEU-4 ROUGE-4 SE
SUMTIME-Hybrid 1 6.076 (3) 0.527 (2) 0.278 (3) 0.607 (4)
pCRU-greedy 2 6.925 (1) 0.641 (1) 0.425 (1) 0.758 (1)
SUMTIME-Corpus - 9.317 (-) 1 (-) 1 (-) 1 (-)
pCRU-roulette 3 6.175 (2) 0.497 (4) 0.242 (4) 0.679 (3)
pCRU-2gram 4 5.685 (4) 0.519 (3) 0.315 (2) 0.712 (2)
pCRU-random 5 4.515 (5) 0.313 (5) 0.098 (5) 0.551 (5)
</table>
<tableCaption confidence="0.999939">
Table 4: Evaluation scores against the SUMTIME corpus, on 5 test sets from pCRU validation.
</tableCaption>
<bodyText confidence="0.99983528">
man and automatic evaluation: (i) Human evalua-
tors prefer SUMTIME-Hybrid over pCRU-greedy,
whereas all the automatic metrics have it the
other way around; and (ii) human evaluators score
pCRU-roulette highly (second and third respec-
tively), whereas the automatic metrics score it very
low, second worst to random generation (except
for NIST which puts it second).
There are two clear tendencies in scores going
from left (humans) to right (SE) across Tables 2
and 4: SUMTIME-Hybrid goes down in rank, and
pCRU-2gram comes up.
In addition to the BLEU-4 scores shown in the
tables, we also calculated BLEU-1, BLEU-2, BLEU-
3 scores. These give similar results, except that
BLEU-1 and BLEU-2 rank pCRU-roulette as highly
as the human judges.
It is striking how low the experts rank the cor-
pus texts, and to what extent they disagree on their
quality. This appears to indicate that corpus qual-
ity is not ideal. If an imperfect corpus is used
as the gold standard for the automatic metrics,
then high correlation with human judgments is less
likely, and this may explain the difference in hu-
man and automatic scores for SUMTIME-Hybrid.
</bodyText>
<sectionHeader confidence="0.99812" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999980690909091">
If we assume that the human evaluation scores are
the most valid, then the automatic metrics do not
do a good job of comparing the knowledge-based
SUMTIME system to the statistical systems.
One reason for this could be that there are cases
where SUMTIME deliberately does not choose the
most common option in the corpus, because its
developers believed that it was not the best for
readers. For example, in Table 1, the human
forecasters and pCRU-greedy use the phrase by
late evening to refer to 0000, pCRU-2gram uses
the phrase later, while SUMTIME-Hybrid uses the
phrase by midnight. The pCRU choices reflect fre-
quency in the SUMTIME corpus: later (837 in-
stances) and by late evening (327 instances) are
more common than by midnight (184 instances).
However, forecast readers dislike this use of later
(because later is used to mean something else in
a different type of forecast), and also dislike vari-
ants of by evening, because they are unsure how
to interpret them (Reiter et al., 2005); this is why
SUMTIME uses by midnight.
The SUMTIME system builders believe deviat-
ing from corpus frequency in such cases makes
SUMTIME texts better from the reader’s perspec-
tive, and it does appear to increase human ratings
of the system; but deviating from the corpus in
such a way decreases the system’s score under
corpus-similarity metrics. In other words, judg-
ing the output of an NLG system by comparing it
to corpus texts by a method that rewards corpus
similarity will penalise systems which do not base
choice on highest frequency of occurrence in the
corpus, even if this is motivated by careful studies
of what is best for text readers.
The MT community recognises that BLEU is not
effective at evaluating texts which are as good as
(or better than) the reference texts. This is not
a problem for MT, because the output of current
(wide-coverage) MT systems is generally worse
than human translations. But it is an issue for NLG,
where systems are domain-specific and can gen-
erate texts that are judged better by humans than
human-written texts (as seen in Tables 4 and 2).
Although the automatic evaluation metrics gen-
erally replicated human judgments fairly well
when comparing different statistical NLG systems,
there was a discrepancy in the ranking of pCRU-
roulette (ranked high by humans, low by several of
the automatic metrics). pCRU-roulette differs from
the other statistical generators because it does not
always try to make the most common choice (max-
imise the likelihood of the corpus), instead it tries
to vary choices. In particular, if there are several
competing words and phrases with similar prob-
</bodyText>
<page confidence="0.997692">
318
</page>
<bodyText confidence="0.999981048780488">
abilities, pCRU-roulette will tend to use different
words and phrases in different texts, whereas the
other statistical generators will stick to those with
the highest frequency. This behaviour is penalised
by the automatic evaluation metrics, but the hu-
man evaluators do not seem to mind it.
One of the classic rules of writing is to vary lex-
ical and syntactic choices, in order to keep text in-
teresting. However, this behaviour (variation for
variation’s sake) will always reduce a system’s
score under corpus-similarity metrics, even if it
enhances text quality from the perspective of read-
ers. Foster and Oberlander (2006), in their study of
facial gestures, have also noted that humans do not
mind and indeed in some cases prefer variation,
whereas corpus-based evaluations give higher rat-
ings to systems which follow corpus frequency.
Using more reference texts does counteract this
tendency, but only up to a point: no matter how
many reference texts are used, there will still be
one, or a small number of, most frequent variants,
and using anything else will still worsen corpus-
similarity scores.
Canvassing expert opinion of text quality and
averaging the results is also in a sense frequency-
based, as results reflect what the majority of ex-
perts consider good variants. Expert opinions can
vary considerably, as shown by the low correla-
tion among experts in our study (and as seen in
corpus studies, e.g. Reiter et al., 2005), and eval-
uations by a small number of experts may also be
problematic, unless we have good reason to be-
lieve that expert opinions are highly correlated in
the domain (which was certainly not the case in
our weather forecast domain). Ultimately, such
disagreement between experts suggests that (in-
trinsic) judgments of the text quality — whether
by human or metric — really should be be backed
up by (extrinsic) judgments of the effectiveness of
a text in helping real users perform tasks or other-
wise achieving its communicative goal.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999971307692308">
We plan to further investigate the performance of
automatic evaluation measures in NLG in the fu-
ture: (i) performing similar experiments to the
one described here in other domains, and with
more subjects and larger test sets; (ii) investigating
whether automatic corpus-based techniques can
evaluate content determination; (iii) investigating
how well both human ratings and corpus-based
measures correlate with extrinsic evaluations of
the effectiveness of generated texts. Ultimately,
we would like to move beyond critiques of exist-
ing corpus-based metrics to proposing (and vali-
dating) new metrics which work well for NLG.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999926954545455">
Corpus quality plays a significant role in auto-
matic evaluation of NLG texts. Automatic metrics
can be expected to correlate very highly with hu-
man judgments only if the reference texts used are
of high quality, or rather, can be expected to be
judged high quality by the human evaluators. This
is especially important when the generated texts
are of similar quality to human-written texts.
In MT, high-quality texts vary less than gener-
ally in NLG, so BLEU scores against 4 reference
translations from reputable sources (as in MT ’05)
are a feasible evaluation regime. It seems likely
that for automatic evaluation in NLG, a larger num-
ber of reference texts than four are needed.
In our experiments, we have found NIST a more
reliable evaluation metric than BLEU and in par-
ticular ROUGE which did not seem to offer any ad-
vantage over simple string-edit distance. We also
found individual experts’ judgments are not likely
to correlate highly with average expert opinion, in
fact less likely than NIST scores. This seems to
imply that if expert evaluation can only be done
with one or two experts, but a high-quality refer-
ence corpus is available, then a NIST-based eval-
uation may produce more accurate results than an
expert-based evaluation.
It seems clear that for automatic corpus-based
evaluation to work well, we need high-quality
reference texts written by many different authors
and large enough to give reasonable coverage of
phenomena such as variation for variation’s sake.
Metrics that do not exclusively reward similarity
with reference texts (such as NIST) are more likely
to correlate well with human judges, but all of the
existing metrics that we looked at still penalised
generators that do not always choose the most fre-
quent variant.
The results we have reported here are for a
relatively simple sublanguage and domain, and
more empirical research needs to be done on how
well different evaluation metrics and methodolo-
gies (including different types of human evalua-
tions) correlate with each other. In order to es-
tablish reliable and trusted automatic cross-system
</bodyText>
<page confidence="0.99768">
319
</page>
<bodyText confidence="0.9996795">
evaluation methodologies, it seems likely that the
NLG community will need to establish how to col-
lect large amounts of high-quality reference texts
and develop new evaluation metrics specifically
for NLG that correlate more reliably with human
judgments of text quality and appropriateness. Ul-
timately, research should also look at developing
new evaluation techniques that correlate reliably
with the real world usefulness of generated texts.
In the shorter term, we recommend that automatic
evaluations of NLG systems be supported by con-
ventional large-scale human-based evaluations.
</bodyText>
<sectionHeader confidence="0.998466" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8851378">
Anja Belz’s part of the research reported in this
paper was supported under UK EPSRC Grant
GR/S24480/01. Many thanks to John Carroll,
Roger Evans and the anonymous reviewers for
very helpful comments.
</bodyText>
<sectionHeader confidence="0.989552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873012048193">
S. Bangalore, O. Rambow, and S. Whittaker. 2000.
Evaluation metrics for generation. In Proc. 1st In-
ternational Conference on Natural Language Gen-
eration, pages 1–8.
A. Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proc. 10th Euro-
pean Workshop on Natural Language Generation
(ENLG’05), pages 15–23.
A. Belz. 2006. pCRU: Probabilistic generation using
representational underspecification. Technical Re-
port ITRI-06-01, ITRI, University of Brighton.
J. Coch. 1996. Evaluating and comparing three
text production techniques. In Proc. 16th Inter-
national Conference on Computational Linguistics
(COLING-1996).
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proc. ARPA Workshop on
Human Language Technology.
M. E. Foster and J. Oberlander. 2006. Data-driven gen-
eration of emphatic facial displays. In Proceedings
of EACL-2006.
N. Habash. 2004. The use of a structural n-gram lan-
guage model in generation-heavy hybrid machine
translation. In Proc. 3rd International Conference
on Natural Language Generation (INLG ’04), vol-
ume 3123 of LNAI, pages 61–69. Springer.
I. Langkilde. 1998. An empirical verification of cover-
age and correctness for a general-purpose sentence
generator. In Proc. 2nd International Natural Lan-
guage Generation Conference (INLG ’02).
J. Lester and B. Porter. 1997. Developing and empir-
ically evaluating robust explanation generators: The
KNIGHT experiments. Computational Linguistics,
23(1):65–101.
C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proc. HLT-NAACL 2003, pages 71–78.
T. Marciniak and M. Strube. 2004. Classification-
based generation using TAG. In Natural Language
Generation: Proceedings of INLG-2994, pages 100–
109. Springer.
C. Mellish and R. Dale. 1998. Evaluation in the
context of natural language generation. Computer
Speech and Language, 12:349–373.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proc. ACL-2002, pages 311–318.
E. Reiter and S. Sripada. 2002. Should corpora texts be
gold standards for NLG? In Proc. 2nd International
Conference on Natural Language Generation, pages
97–104.
E. Reiter, S. Sripada, and R. Robertson. 2003. Ac-
quiring correct knowledge for natural language gen-
eration. Journal of Artificial Intelligence Research,
18:491–516.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005.
Choosing words in computer-generated weather
forecasts. Artificial Intelligence, 167:137–169.
S. Riezler and J. T. Maxwell III. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proc. ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Sum-
marization, pages 57–64.
K. Sparck Jones and J. R. Galliers. 1996. Evaluating
Natural Language Processing Systems: An Analysis
and Review. Springer Verlag.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2003. Ex-
ploiting a parallel TEXT-DATA corpus. In Proc.
Corpus Linguistics 2003, pages 734–743.
S. Sripada, E. Reiter, and L. Hawizy. 2005. Evalua-
tion of an NLG system used post-edit data: Lessons
learned. In Proc. ENLG-2005, pages 133–139.
A. Stolcke. 2002. SRILM: An extensible language
modeling toolkit. In Proc. 7th International Confer-
ence on Spoken Language Processing (ICSLP ’02),
pages 901–904,.
S. Williams and E. Reiter. 2005. Generating read-
able texts for readers with low basic skills. In Proc.
ENLG-2005, pages 140–147.
M. Young. 1999. Using Grice’s maxim of quantity
to select the content of plan descriptions. Artificial
Intelligence, 115:215–256.
</reference>
<page confidence="0.998253">
320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503011">
<title confidence="0.99992">Comparing Automatic and Human Evaluation of NLG Systems</title>
<author confidence="0.996384">Anja Belz</author>
<affiliation confidence="0.9970865">Natural Language Technology Group CMIS, University of Brighton</affiliation>
<address confidence="0.699422">UK</address>
<email confidence="0.981316">A.S.Belz@brighton.ac.uk</email>
<author confidence="0.934422">Ehud Reiter</author>
<affiliation confidence="0.9999405">Dept of Computing Science University of Aberdeen</affiliation>
<address confidence="0.807502">UK</address>
<email confidence="0.998306">ereiter@csd.abdn.ac.uk</email>
<abstract confidence="0.998927">We consider the evaluation problem in Language Generation and results for evaluating several systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, inand We that correlate best with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conthat automatic evaluation of systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
<author>S Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<booktitle>In Proc. 1st International Conference on Natural Language Generation,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2668" citStr="Bangalore et al., 2000" startWordPosition="418" endWordPosition="421">se than the human evaluations that have traditionally been used to evaluate NLG systems. However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations. While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments; correlation studies have been made of individual components (Bangalore et al., 2000), but not of systems. In this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts). These systems do not perform content determination (they are limited to microplanning and realisation), so our study does not address corpus-based evaluation of content determination. 2 Background 2.1 Evaluation of NLG systems NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tend</context>
<context position="4723" citStr="Bangalore et al. (2000)" startWordPosition="749" endWordPosition="753">lliams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000) and Marciniak and Strube (2004). Such corpus-based evaluations have sometimes been criticised in the NLG community, for example by Reiter and Sripada (2002). Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system’s communicative goal; and that corpus texts are often not of high enough quality to form a realistic test. 2.2 Automatic evaluation of generated texts in MT and Summarisation The MT and document summarisation communities have developed evaluation metr</context>
</contexts>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>S. Bangalore, O. Rambow, and S. Whittaker. 2000. Evaluation metrics for generation. In Proc. 1st International Conference on Natural Language Generation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Statistical generation: Three methods compared and evaluated.</title>
<date>2005</date>
<booktitle>In Proc. 10th European Workshop on Natural Language Generation (ENLG’05),</booktitle>
<pages>15--23</pages>
<contexts>
<context position="1978" citStr="Belz, 2005" startWordPosition="305" endWordPosition="306">s in other fields of computational linguistics. Some NLG researchers are impressed by the success of the BLEU evaluation metric (Papineni et al., 2002) in Machine Translation (MT), which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an MT system to a set of reference (‘gold standard’) translations, and in principle this kind of evaluation could be done with NLG systems as well. Indeed NLG researchers are already starting to use BLEU (Habash, 2004; Belz, 2005) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems. However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations. While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgm</context>
<context position="9919" citStr="Belz, 2005" startWordPosition="1598" endWordPosition="1599">oplanning and realisation) is called SUMTIME-Hybrid. Table 1 includes an example of the tuples extracted from the corpus text (row 1), and a SUMTIME-Hybrid text produced from the tuples (row 5). 2.4 pCRU language generation Statistical NLG has focused on generate-and-select models: a set of alternatives is generated and one is selected with a language model. This technique is computationally very expensive. Moreover, the only type of language model used in NLG are ngram models which have the additional disadvantage of a general preference for shorter realisations, which can be harmful in NLG (Belz, 2005). pCRU1 language generation (Belz, 2006) is a language generation framework that was designed to facilitate statistical generation techniques that are more efficient and less biased. In pCRU generation, a base generator is encoded as a set of generation rules made up of relations with zero or more atomic arguments. The base generator 1Probabilistic Context-free Representational Underspecification. is then trained on raw text corpora to provide a probability distribution over generation rules. The resulting PCRU generator can be run in several modes, including the following: Random: ignoring pC</context>
</contexts>
<marker>Belz, 2005</marker>
<rawString>A. Belz. 2005. Statistical generation: Three methods compared and evaluated. In Proc. 10th European Workshop on Natural Language Generation (ENLG’05), pages 15–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>pCRU: Probabilistic generation using representational underspecification.</title>
<date>2006</date>
<tech>Technical Report ITRI-06-01,</tech>
<institution>ITRI, University of Brighton.</institution>
<contexts>
<context position="9959" citStr="Belz, 2006" startWordPosition="1603" endWordPosition="1604">TIME-Hybrid. Table 1 includes an example of the tuples extracted from the corpus text (row 1), and a SUMTIME-Hybrid text produced from the tuples (row 5). 2.4 pCRU language generation Statistical NLG has focused on generate-and-select models: a set of alternatives is generated and one is selected with a language model. This technique is computationally very expensive. Moreover, the only type of language model used in NLG are ngram models which have the additional disadvantage of a general preference for shorter realisations, which can be harmful in NLG (Belz, 2005). pCRU1 language generation (Belz, 2006) is a language generation framework that was designed to facilitate statistical generation techniques that are more efficient and less biased. In pCRU generation, a base generator is encoded as a set of generation rules made up of relations with zero or more atomic arguments. The base generator 1Probabilistic Context-free Representational Underspecification. is then trained on raw text corpora to provide a probability distribution over generation rules. The resulting PCRU generator can be run in several modes, including the following: Random: ignoring pCRU probabilities, randomly select genera</context>
</contexts>
<marker>Belz, 2006</marker>
<rawString>A. Belz. 2006. pCRU: Probabilistic generation using representational underspecification. Technical Report ITRI-06-01, ITRI, University of Brighton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Coch</author>
</authors>
<title>Evaluating and comparing three text production techniques.</title>
<date>1996</date>
<booktitle>In Proc. 16th International Conference on Computational Linguistics (COLING-1996).</booktitle>
<contexts>
<context position="3748" citStr="Coch (1996)" startWordPosition="591" endWordPosition="592"> NLG systems NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is pote</context>
</contexts>
<marker>Coch, 1996</marker>
<rawString>J. Coch. 1996. Evaluating and comparing three text production techniques. In Proc. 16th International Conference on Computational Linguistics (COLING-1996).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="2445" citStr="Doddington, 2002" startWordPosition="385" endWordPosition="386">ple this kind of evaluation could be done with NLG systems as well. Indeed NLG researchers are already starting to use BLEU (Habash, 2004; Belz, 2005) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems. However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations. While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments; correlation studies have been made of individual components (Bangalore et al., 2000), but not of systems. In this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts). These systems do not perform content determination (they are limited to microplanning and realisation), so our study does </context>
<context position="6357" citStr="Doddington, 2002" startWordPosition="1031" endWordPosition="1032"> proportion of its word ngrams (n = 4 has become standard) that it shares with one or more high-quality reference translations. BLEU scores range from 0 to 1, 1 being the highest which can only be achieved by a translation if all its substrings can be found in one of the reference texts (hence a reference text will always score 1). BLEU should be calculated on a large test set with several reference translations (four appears to be standard in MT). Properly calculated BLEU scores have been shown to correlate reliably with human judgments (Papineni et al., 2002). The NIST MT evaluation metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent (hence more informative) n-grams. BLEU’s ability to detect subtle but important differences in translation quality has been questioned, some research showing NIST to be more sensitive (Doddington, 2002; Riezler and Maxwell III, 2005). The ROUGE metric (Lin and Hovy, 2003) was conceived as document summarisation’s answer to BLEU, but it does not appear to have met with the same degree of enthusiasm. There are several different ROUGE metrics. The simplest is ROUGE-N, which com</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proc. ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>J Oberlander</author>
</authors>
<title>Data-driven generation of emphatic facial displays.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-2006.</booktitle>
<contexts>
<context position="25593" citStr="Foster and Oberlander (2006)" startWordPosition="4167" endWordPosition="4170">8 abilities, pCRU-roulette will tend to use different words and phrases in different texts, whereas the other statistical generators will stick to those with the highest frequency. This behaviour is penalised by the automatic evaluation metrics, but the human evaluators do not seem to mind it. One of the classic rules of writing is to vary lexical and syntactic choices, in order to keep text interesting. However, this behaviour (variation for variation’s sake) will always reduce a system’s score under corpus-similarity metrics, even if it enhances text quality from the perspective of readers. Foster and Oberlander (2006), in their study of facial gestures, have also noted that humans do not mind and indeed in some cases prefer variation, whereas corpus-based evaluations give higher ratings to systems which follow corpus frequency. Using more reference texts does counteract this tendency, but only up to a point: no matter how many reference texts are used, there will still be one, or a small number of, most frequent variants, and using anything else will still worsen corpussimilarity scores. Canvassing expert opinion of text quality and averaging the results is also in a sense frequencybased, as results reflec</context>
</contexts>
<marker>Foster, Oberlander, 2006</marker>
<rawString>M. E. Foster and J. Oberlander. 2006. Data-driven generation of emphatic facial displays. In Proceedings of EACL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
</authors>
<title>The use of a structural n-gram language model in generation-heavy hybrid machine translation.</title>
<date>2004</date>
<booktitle>In Proc. 3rd International Conference on Natural Language Generation (INLG ’04),</booktitle>
<volume>3123</volume>
<pages>61--69</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1965" citStr="Habash, 2004" startWordPosition="303" endWordPosition="304">ation (NLG), as in other fields of computational linguistics. Some NLG researchers are impressed by the success of the BLEU evaluation metric (Papineni et al., 2002) in Machine Translation (MT), which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an MT system to a set of reference (‘gold standard’) translations, and in principle this kind of evaluation could be done with NLG systems as well. Indeed NLG researchers are already starting to use BLEU (Habash, 2004; Belz, 2005) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems. However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations. While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated wit</context>
</contexts>
<marker>Habash, 2004</marker>
<rawString>N. Habash. 2004. The use of a structural n-gram language model in generation-heavy hybrid machine translation. In Proc. 3rd International Conference on Natural Language Generation (INLG ’04), volume 3123 of LNAI, pages 61–69. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>1998</date>
<booktitle>In Proc. 2nd International Natural Language Generation Conference (INLG ’02).</booktitle>
<contexts>
<context position="4507" citStr="Langkilde (1998)" startWordPosition="713" endWordPosition="714"> the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000) and Marciniak and Strube (2004). Such corpus-based evaluations have sometimes been criticised in the NLG community, for example by Reiter and Sripada (2002). Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system’s communicative goal; and that corp</context>
</contexts>
<marker>Langkilde, 1998</marker>
<rawString>I. Langkilde. 1998. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Proc. 2nd International Natural Language Generation Conference (INLG ’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lester</author>
<author>B Porter</author>
</authors>
<title>Developing and empirically evaluating robust explanation generators: The KNIGHT experiments.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="3777" citStr="Lester and Porter (1997)" startWordPosition="594" endWordPosition="597"> systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quic</context>
</contexts>
<marker>Lester, Porter, 1997</marker>
<rawString>J. Lester and B. Porter. 1997. Developing and empirically evaluating robust explanation generators: The KNIGHT experiments. Computational Linguistics, 23(1):65–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>71--78</pages>
<contexts>
<context position="6750" citStr="Lin and Hovy, 2003" startWordPosition="1091" endWordPosition="1094">l reference translations (four appears to be standard in MT). Properly calculated BLEU scores have been shown to correlate reliably with human judgments (Papineni et al., 2002). The NIST MT evaluation metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent (hence more informative) n-grams. BLEU’s ability to detect subtle but important differences in translation quality has been questioned, some research showing NIST to be more sensitive (Doddington, 2002; Riezler and Maxwell III, 2005). The ROUGE metric (Lin and Hovy, 2003) was conceived as document summarisation’s answer to BLEU, but it does not appear to have met with the same degree of enthusiasm. There are several different ROUGE metrics. The simplest is ROUGE-N, which computes the highest proportion in any reference summary of n-grams that are matched by the system-generated summary. A procedure is applied that averages the score across leave-oneout subsets of the set of reference texts. ROUGEN is an almost straightforward n-gram recall metric between two texts, and has several counterintuitive properties, including that even a text composed entirely of sen</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proc. HLT-NAACL 2003, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Marciniak</author>
<author>M Strube</author>
</authors>
<title>Classificationbased generation using TAG.</title>
<date>2004</date>
<booktitle>In Natural Language Generation: Proceedings of INLG-2994,</booktitle>
<pages>100--109</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4755" citStr="Marciniak and Strube (2004)" startWordPosition="755" endWordPosition="758"> recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000) and Marciniak and Strube (2004). Such corpus-based evaluations have sometimes been criticised in the NLG community, for example by Reiter and Sripada (2002). Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system’s communicative goal; and that corpus texts are often not of high enough quality to form a realistic test. 2.2 Automatic evaluation of generated texts in MT and Summarisation The MT and document summarisation communities have developed evaluation metrics based on comparing output te</context>
</contexts>
<marker>Marciniak, Strube, 2004</marker>
<rawString>T. Marciniak and M. Strube. 2004. Classificationbased generation using TAG. In Natural Language Generation: Proceedings of INLG-2994, pages 100– 109. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>R Dale</author>
</authors>
<title>Evaluation in the context of natural language generation. Computer Speech and Language,</title>
<date>1998</date>
<pages>12--349</pages>
<contexts>
<context position="3241" citStr="Mellish and Dale, 1998" startWordPosition="505" endWordPosition="508"> of individual components (Bangalore et al., 2000), but not of systems. In this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts). These systems do not perform content determination (they are limited to microplanning and realisation), so our study does not address corpus-based evaluation of content determination. 2 Background 2.1 Evaluation of NLG systems NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of </context>
</contexts>
<marker>Mellish, Dale, 1998</marker>
<rawString>C. Mellish and R. Dale. 1998. Evaluation in the context of natural language generation. Computer Speech and Language, 12:349–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL-2002,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1518" citStr="Papineni et al., 2002" startWordPosition="223" endWordPosition="226">onsiderable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain. 1 Introduction Evaluation is becoming an increasingly important topic in Natural Language Generation (NLG), as in other fields of computational linguistics. Some NLG researchers are impressed by the success of the BLEU evaluation metric (Papineni et al., 2002) in Machine Translation (MT), which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an MT system to a set of reference (‘gold standard’) translations, and in principle this kind of evaluation could be done with NLG systems as well. Indeed NLG researchers are already starting to use BLEU (Habash, 2004; Belz, 2005) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evalua</context>
<context position="5512" citStr="Papineni et al., 2002" startWordPosition="880" endWordPosition="883">for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system’s communicative goal; and that corpus texts are often not of high enough quality to form a realistic test. 2.2 Automatic evaluation of generated texts in MT and Summarisation The MT and document summarisation communities have developed evaluation metrics based on comparing output texts to a corpus of human texts, and have shown that some of these metrics are highly correlated with human judgments. The BLEU metric (Papineni et al., 2002) in MT has been particularly successful; for example MT05, the 2005 NIST MT evaluation exercise, used BLEU-4 as the only method of evaluation. BLEU is a precision metric that assesses the quality of a translation in terms of the proportion of its word ngrams (n = 4 has become standard) that it shares with one or more high-quality reference translations. BLEU scores range from 0 to 1, 1 being the highest which can only be achieved by a translation if all its substrings can be found in one of the reference texts (hence a reference text will always score 1). BLEU should be calculated on a large t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proc. ACL-2002, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
</authors>
<title>Should corpora texts be gold standards for NLG?</title>
<date>2002</date>
<booktitle>In Proc. 2nd International Conference on Natural Language Generation,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="4880" citStr="Reiter and Sripada (2002)" startWordPosition="773" endWordPosition="776">in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000) and Marciniak and Strube (2004). Such corpus-based evaluations have sometimes been criticised in the NLG community, for example by Reiter and Sripada (2002). Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system’s communicative goal; and that corpus texts are often not of high enough quality to form a realistic test. 2.2 Automatic evaluation of generated texts in MT and Summarisation The MT and document summarisation communities have developed evaluation metrics based on comparing output texts to a corpus of human texts, and have shown that some of these metrics are highly correlated with human judgments. The BLE</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>E. Reiter and S. Sripada. 2002. Should corpora texts be gold standards for NLG? In Proc. 2nd International Conference on Natural Language Generation, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
<author>R Robertson</author>
</authors>
<title>Acquiring correct knowledge for natural language generation.</title>
<date>2003</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>18--491</pages>
<contexts>
<context position="8508" citStr="Reiter et al., 2003" startWordPosition="1368" endWordPosition="1371">nation module that determines the content of the weather forecast by analysing the numerical data using linear segmentation and 314 other data analysis techniques; and a microplanning and realisation module which generates texts based on this content by choosing appropriate words, deciding on aggregation, enforcing the sublanguage grammar, and so forth. SUMTIME generates very high-quality texts, in some cases forecast users believe SUMTIME texts are better than human-written texts (Reiter et al., 2005). SUMTIME is a knowledge-based NLG system. While its design was informed by corpus analysis (Reiter et al., 2003), the system is based on manually authored rules and code. As part of the project, the SUMTIME team created a corpus of 1045 forecasts from the commercial output of five different forecasters and the input data (numerical predictions of wind, temperature, etc) that the forecasters examined when they wrote the forecasts (Sripada et al., 2003). In other words, the SUMTIME corpus contains both the inputs (numerical weather predictions) and the outputs (forecast texts) of the forecast-generation process. The SUMTIME team also derived a content representation (called ‘tuples’) from the corpus texts</context>
</contexts>
<marker>Reiter, Sripada, Robertson, 2003</marker>
<rawString>E. Reiter, S. Sripada, and R. Robertson. 2003. Acquiring correct knowledge for natural language generation. Journal of Artificial Intelligence Research, 18:491–516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
<author>J Hunter</author>
<author>J Yu</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>167--137</pages>
<contexts>
<context position="7681" citStr="Reiter et al., 2005" startWordPosition="1246" endWordPosition="1249">d summary. A procedure is applied that averages the score across leave-oneout subsets of the set of reference texts. ROUGEN is an almost straightforward n-gram recall metric between two texts, and has several counterintuitive properties, including that even a text composed entirely of sentences from reference texts cannot score 1 (unless there is only one reference text). There are several other variants of the ROUGE metric, and ROUGE-2, along with ROUGESU (based on skip bigrams and unigrams), were among the official scores for the DUC 2005 summarisation task. 2.3 SUMTIME The SUMTIME project (Reiter et al., 2005) developed an NLG system which generated textual weather forecasts from numerical forecast data. The SUMTIME system generates specialist forecasts for offshore oil rigs. It has two modules: a content-determination module that determines the content of the weather forecast by analysing the numerical data using linear segmentation and 314 other data analysis techniques; and a microplanning and realisation module which generates texts based on this content by choosing appropriate words, deciding on aggregation, enforcing the sublanguage grammar, and so forth. SUMTIME generates very high-quality t</context>
<context position="23337" citStr="Reiter et al., 2005" startWordPosition="3795" endWordPosition="3798">best for readers. For example, in Table 1, the human forecasters and pCRU-greedy use the phrase by late evening to refer to 0000, pCRU-2gram uses the phrase later, while SUMTIME-Hybrid uses the phrase by midnight. The pCRU choices reflect frequency in the SUMTIME corpus: later (837 instances) and by late evening (327 instances) are more common than by midnight (184 instances). However, forecast readers dislike this use of later (because later is used to mean something else in a different type of forecast), and also dislike variants of by evening, because they are unsure how to interpret them (Reiter et al., 2005); this is why SUMTIME uses by midnight. The SUMTIME system builders believe deviating from corpus frequency in such cases makes SUMTIME texts better from the reader’s perspective, and it does appear to increase human ratings of the system; but deviating from the corpus in such a way decreases the system’s score under corpus-similarity metrics. In other words, judging the output of an NLG system by comparing it to corpus texts by a method that rewards corpus similarity will penalise systems which do not base choice on highest frequency of occurrence in the corpus, even if this is motivated by c</context>
<context position="26403" citStr="Reiter et al., 2005" startWordPosition="4303" endWordPosition="4306">follow corpus frequency. Using more reference texts does counteract this tendency, but only up to a point: no matter how many reference texts are used, there will still be one, or a small number of, most frequent variants, and using anything else will still worsen corpussimilarity scores. Canvassing expert opinion of text quality and averaging the results is also in a sense frequencybased, as results reflect what the majority of experts consider good variants. Expert opinions can vary considerably, as shown by the low correlation among experts in our study (and as seen in corpus studies, e.g. Reiter et al., 2005), and evaluations by a small number of experts may also be problematic, unless we have good reason to believe that expert opinions are highly correlated in the domain (which was certainly not the case in our weather forecast domain). Ultimately, such disagreement between experts suggests that (intrinsic) judgments of the text quality — whether by human or metric — really should be be backed up by (extrinsic) judgments of the effectiveness of a text in helping real users perform tasks or otherwise achieving its communicative goal. 6 Future Work We plan to further investigate the performance of </context>
</contexts>
<marker>Reiter, Sripada, Hunter, Yu, 2005</marker>
<rawString>E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence, 167:137–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>57--64</pages>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>S. Riezler and J. T. Maxwell III. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
<author>J R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review.</title>
<date>1996</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="3332" citStr="Jones and Galliers, 1996" startWordPosition="520" endWordPosition="523">present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts). These systems do not perform content determination (they are limited to microplanning and realisation), so our study does not address corpus-based evaluation of content determination. 2 Background 2.1 Evaluation of NLG systems NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts </context>
</contexts>
<marker>Jones, Galliers, 1996</marker>
<rawString>K. Sparck Jones and J. R. Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sripada</author>
<author>E Reiter</author>
<author>J Hunter</author>
<author>J Yu</author>
</authors>
<title>Exploiting a parallel TEXT-DATA corpus.</title>
<date>2003</date>
<booktitle>In Proc. Corpus Linguistics</booktitle>
<pages>734--743</pages>
<contexts>
<context position="8851" citStr="Sripada et al., 2003" startWordPosition="1428" endWordPosition="1431">and so forth. SUMTIME generates very high-quality texts, in some cases forecast users believe SUMTIME texts are better than human-written texts (Reiter et al., 2005). SUMTIME is a knowledge-based NLG system. While its design was informed by corpus analysis (Reiter et al., 2003), the system is based on manually authored rules and code. As part of the project, the SUMTIME team created a corpus of 1045 forecasts from the commercial output of five different forecasters and the input data (numerical predictions of wind, temperature, etc) that the forecasters examined when they wrote the forecasts (Sripada et al., 2003). In other words, the SUMTIME corpus contains both the inputs (numerical weather predictions) and the outputs (forecast texts) of the forecast-generation process. The SUMTIME team also derived a content representation (called ‘tuples’) from the corpus texts similar to that produced by SUMTIME’s content-determination module. The SUMTIME microplanner/realiser can be driven by these tuples; this mode (combining human content determination with SUMTIME microplanning and realisation) is called SUMTIME-Hybrid. Table 1 includes an example of the tuples extracted from the corpus text (row 1), and a SU</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2003</marker>
<rawString>S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2003. Exploiting a parallel TEXT-DATA corpus. In Proc. Corpus Linguistics 2003, pages 734–743.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sripada</author>
<author>E Reiter</author>
<author>L Hawizy</author>
</authors>
<title>Evaluation of an NLG system used post-edit data: Lessons learned.</title>
<date>2005</date>
<booktitle>In Proc. ENLG-2005,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="4041" citStr="Sripada et al., 2005" startWordPosition="635" endWordPosition="638">nd human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the origina</context>
</contexts>
<marker>Sripada, Reiter, Hawizy, 2005</marker>
<rawString>S. Sripada, E. Reiter, and L. Hawizy. 2005. Evaluation of an NLG system used post-edit data: Lessons learned. In Proc. ENLG-2005, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM: An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. 7th International Conference on Spoken Language Processing (ICSLP ’02),</booktitle>
<pages>901--904</pages>
<contexts>
<context position="14307" citStr="Stolcke, 2002" startWordPosition="2303" endWordPosition="2304">nt determination. A base pCRU generator was created semiautomatically by running a chunker over the corpus, extracting generation rules and adding some higher-level rules taking care of aggregation, elision etc. This base generator was then trained on 9/10 of the corpus (the training data). 5 different random divisions of the corpus into training and testing data were used (i.e. all results were validated by 5-fold hold-out cross-validation). Additionally, a back-off 2-gram model with GoodTuring discounting and no lexical classes was built from the same training data, using the SRILM toolkit (Stolcke, 2002). Forecasts were then generated for all corpus inputs, in all four generation modes (Section 2.4). Table 1 shows an example of an input to the systems, along with the three human texts (Corpus, Human1, Human2) and the texts produced by all five NLG systems from this data. Automatic evaluations: We used NIST2, BLEU3, and ROUGE4 to automatically evaluate the above systems and texts. We computed BLEU-N for N = 1..4 (using BLEU-4 as our main BLEU score). We also computed NIST-5 and ROUGE-4. As a baseline we used string-edit (SE) distance 2http://cio.nist.gov/esd/emaildir/lists/mt list/bin00000.bin</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM: An extensible language modeling toolkit. In Proc. 7th International Conference on Spoken Language Processing (ICSLP ’02), pages 901–904,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>Generating readable texts for readers with low basic skills.</title>
<date>2005</date>
<booktitle>In Proc. ENLG-2005,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="4124" citStr="Williams and Reiter, 2005" startWordPosition="647" endWordPosition="650">gs of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000) </context>
</contexts>
<marker>Williams, Reiter, 2005</marker>
<rawString>S. Williams and E. Reiter. 2005. Generating readable texts for readers with low basic skills. In Proc. ENLG-2005, pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Young</author>
</authors>
<title>Using Grice’s maxim of quantity to select the content of plan descriptions.</title>
<date>1999</date>
<journal>Artificial Intelligence,</journal>
<pages>115--215</pages>
<contexts>
<context position="3965" citStr="Young, 1999" startWordPosition="625" endWordPosition="626">reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her pa</context>
</contexts>
<marker>Young, 1999</marker>
<rawString>M. Young. 1999. Using Grice’s maxim of quantity to select the content of plan descriptions. Artificial Intelligence, 115:215–256.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>