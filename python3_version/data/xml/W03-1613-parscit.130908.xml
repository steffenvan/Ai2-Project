<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994695">
Lexical Paraphrasing for Document Retrieval and Node Identification
</title>
<author confidence="0.996064">
Ingrid Zukerman
</author>
<affiliation confidence="0.992593">
School of Computer Science
and Software Engineering
Monash University
</affiliation>
<address confidence="0.755438">
Clayton, VICTORIA 3800
AUSTRALIA
</address>
<email confidence="0.99548">
ingrid@csse.monash.edu.au
</email>
<author confidence="0.989088">
Sarah George
</author>
<affiliation confidence="0.992497333333333">
School of Computer Science
and Software Engineering
Monash University
</affiliation>
<address confidence="0.755785">
Clayton, VICTORIA 3800
AUSTRALIA
</address>
<email confidence="0.996341">
sarahg@csse.monash.edu.au
</email>
<author confidence="0.997823">
Yingying Wen
</author>
<affiliation confidence="0.992620333333333">
School of Computer Science
and Software Engineering
Monash University
</affiliation>
<address confidence="0.754976">
Clayton, VICTORIA 3800
AUSTRALIA
</address>
<email confidence="0.995017">
ywen@csse.monash.edu.au
</email>
<sectionHeader confidence="0.995586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999782">
We investigate lexical paraphrasing in the
context of two distinct applications: doc-
ument retrieval and node identification.
Document retrieval – the first step in ques-
tion answering – retrieves documents that
contain answers to user queries. Node
identification – performed in the con-
text of a Bayesian argumentation system
– matches users’ Natural Language sen-
tences to nodes in a Bayesian network.
Lexical paraphrases are generated using
syntactic, semantic and corpus-based in-
formation. Our evaluation shows that lex-
ical paraphrasing improves retrieval per-
formance for both applications.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938155555556">
One of the difficulties users face when accessing
a knowledge repository is that of expressing them-
selves in a way that will produce the desired out-
come, e.g., retrieve relevant documents or make
their dialogue contributions understood. The use of
appropriate terminology is one aspect of this prob-
lem: if a user’s vocabulary differs from that within
the resource being accessed, the system may be un-
able to satisfy the user’s requirements. In this pa-
per, we investigate the application of lexical para-
phrasing to two different information access appli-
cations: document retrieval and node identification.
Document retrieval is performed in the context of the
TREC Question Answering task, where the system
retrieves documents that contain answers to users’
queries. Node identification is performed in the
context of a Natural Language (NL) interface to
a Bayesian argumentation system (Zukerman and
George, 2002). Here the system finds the nodes
from a Bayesian network (BN) (Pearl, 1988) that
best match a user’s NL sentences.
Lexical paraphrases replace content words in a
user’s input with their synonyms. We use the follow-
ing information sources to perform this task: syn-
tactic – obtained from Brill’s part-of-speech tag-
ger (Brill, 1992); semantic – obtained from Word-
Net (Miller et al., 1990) and the Webster-1913 on-
line dictionary; and statistical – obtained from our
document collection. The statistical information is
used to moderate the alternatives obtained from the
semantic resources, by preferring query paraphrases
that contain frequent word combinations.
Our evaluation assessed the effect of lexical para-
phrasing on our two applications. Its impact on
document retrieval was evaluated using subsets of
queries from the TREC8, TREC9 and TREC10 col-
lections, and its effect on node identification was
evaluated using paraphrases generated by people for
some nodes in our BN (Section 6.2).
In the next section we discuss related research.
Section 3 describes our two applications. In Sec-
tion 4, we consider the paraphrasing process, and in
Section 5 the information retrieval procedures. Sec-
tion 6 presents the results of our evaluation, followed
by concluding remarks.
</bodyText>
<sectionHeader confidence="0.996242" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.999935894736842">
The vocabulary mis-match between a user’s queries
and indexed documents is often addressed through
query expansion. Query expansion in turn is of-
ten preceded by Word sense disambiguation (WSD)
in order to avoid retrieving irrelevant information.
Mihalcea and Moldovan (1999) and Lytinen et
al. (2000) used WordNet (Miller et al., 1990) to ob-
tain the sense of a word. In contrast, Sch¨utze and
Pedersen (1995) and Lin (1998) used a corpus-based
approach where they automatically constructed a
thesaurus on the basis of contextual information.
The results obtained by Sch¨utze and Pedersen and
by Lytinen et al. are encouraging. However, experi-
mental results reported in (Gonzalo et al., 1998) in-
dicate that the improvement in IR performance due
to WSD is restricted to short queries, and that IR per-
formance is very sensitive to disambiguation errors.
Harabagiu et al. (2001) offered a different form of
query expansion, where they used WordNet to pro-
pose synonyms for the words in a query, and applied
heuristics to select which words to paraphrase.
Our approach to the vocabulary mis-match prob-
lem differs from WSD in that instead of return-
ing the sense of the words in a sentence, we pro-
pose alternative lexical paraphrases. Like Langk-
ilde and Knight (1998), when generating candidate
paraphrases, we use WordNet to propose synonyms
for the words in a sentence. However, they chose
among these synonyms using the word-sense rank-
ings offered by WordNet, while we make our selec-
tion using word-pair frequencies obtained from our
corpus, and word-similarity information obtained
from a thesaurus that is automatically constructed
from the Webster Dictionary. It is worth noting
that the retrieval performance obtained with para-
phrases generated using our dictionary-based the-
saurus compares favorably with that obtained using
Lin’s context-based thesaurus.
</bodyText>
<sectionHeader confidence="0.995983" genericHeader="method">
3 Applications
</sectionHeader>
<bodyText confidence="0.999970816326531">
Our two applications are: document retrieval
(DocRet) and node identification (NodeID).
The document retrieval application consists of re-
trieving documents that are likely to contain the an-
swer to a user’s query. This is the first step in our
Question Answering project, whose aim is to use
these documents to generate answers to queries.
The node identification application is the back-
bone of the NL interface to our Bayesian Interac-
tive Argumentation System (BIAS) (Zukerman and
George, 2002). Here the system matches NL sen-
tences that make up a user’s argument to nodes in a
BN that represents the system’s domain knowledge.
The eventual goal is to understand the user’s argu-
ment (in terms of the system’s domain knowledge)
and generate responses.
There are significant differences between these
applications and their underlying domains:
Our DocRet repository consists of 131,896 arti-
cles from the LA Times portion of the NIST TREC
collection (the other TREC repositories were omit-
ted owing to disk space limitations). Full-text in-
dexing was performed for these documents using
lemmas (uninflected versions of words), rather than
stems or complete words. Our NodeID repository is
an 85-node BN which represents domain knowledge
for a murder mystery. Most nodes are associated
with one “canonical” sentence, and 8 nodes are as-
sociated with two or three sentences. Sample canon-
ical sentences for the nodes in our domain are “Mr
Green murdered Mr Body” and “The blue paint on
the mailbox is one week old”. The sentences in this
domain were also lemmatized, but no indexing was
performed owing to the small size of the BN.
The queries in the DocRet application have an av-
erage length of 7.22 words, and the articles in the
LA Times repository have 242 words on average. In
contrast, in the NodeID application, the user’s sen-
tences and the sentences associated with the nodes
in the BN are of similar length (8.45 words on av-
erage for the user sentences, compared to 7.7 words
for the BN sentences).
The DocRet task is one of containment, i.e., to re-
trieve documents that include the words in a query,
while the NodeID task is one of equivalence, i.e., to
retrieve the node that best matches the user’s sen-
tence. Further, for DocRet there may be several arti-
cles that answer a user’s query, while for NodeID at
most one node matches a user’s sentence.1
</bodyText>
<sectionHeader confidence="0.985068" genericHeader="method">
4 Lexical Paraphrasing
</sectionHeader>
<bodyText confidence="0.854418166666667">
In this section, we discuss the resources used by
our paraphrasing mechanism and describe the para-
phrasing process.
&apos;The user may utter sentences that have no counterpart in the
system’s BN. Also, at present we are not dealing with sentences
that include more than one node.
</bodyText>
<subsectionHeader confidence="0.971045">
4.1 Resources
</subsectionHeader>
<bodyText confidence="0.999989818181818">
Our system uses syntactic, semantic and statistical
information for paraphrase generation.
Syntactic information for each sentence was ob-
tained from Brill’s part-of-speech (PoS) tagger
(Brill, 1992).
Semantic information was obtained from two
sources: WordNet – a knowledge-intensive, hand-
built on-line repository; and Webster – an
on-line version of the Webster-1913 dictionary
(http://www.dict.org). WordNet was used to
generate lemmas for the corpus and the sentences,
and to generate different types of synonyms for the
words in the sentences. Webster was used to auto-
matically build a thesaurus that includes similarity
scores between lemmas, and to automatically con-
struct a list of nominals corresponding to the verbs
in the dictionary, and a list of verbs corresponding
to the nouns in the dictionary. The thesaurus was
used to assign similarity scores to the synonyms re-
turned by WordNet. The nominalization and verbal-
ization lists were used to generate additional syn-
onyms. This was done by activating WordNet for the
nouns corresponding to the verbs in the sentences
and for the verbs corresponding to the nouns in the
sentences.2 The idea was that nominalizations and
verbalizations will help paraphrase queries such as
“who killed Lincoln?” into “who is the murderer of
Lincoln?” (Harabagiu et al., 2001).
The thesaurus, nominal list and verb list were ob-
tained by building a vector from the content lemmas
in the definition of each word in the dictionary, and
applying the cosine similarity measure to calculate
a score for the similarity between the vector corre-
sponding to each word and the vectors correspond-
ing to the other words in the dictionary. The dic-
tionary words that had the highest similarity score
against the original word were retained. To build
the thesaurus, only words with the same PoS as the
original word were considered. To build the nomi-
nalization list, only nouns were considered for each
verb in the dictionary, and to build the verbalization
list, only verbs were considered for each noun. Fur-
ther, for these lists, the retained noun (or verb) had
to have the same stem as the original verb (or noun).
</bodyText>
<footnote confidence="0.728965">
2It was necessary to build nominalization and verbalization
lists because WordNet does not include this information.
</footnote>
<bodyText confidence="0.9999748">
Statistical information was obtained from the LA
Times portion of the NIST Text Research Collection
(http://trec.nist.gov). The statistical informa-
tion was used to estimate the probability of the para-
phrases generated for a sentence (Section 5.1). The
statistical information was stored in a lemma dictio-
nary (202,485 lemmas) and a lemma-pair dictionary
(37,341,156 lemma pairs). Lemma pairs which ap-
pear only once constitute 64% of the pairs, and were
omitted owing to disk space limitations.
</bodyText>
<subsectionHeader confidence="0.990007">
4.2 Procedure
</subsectionHeader>
<bodyText confidence="0.989641">
The following procedure was applied to paraphrase
a sentence:
</bodyText>
<listItem confidence="0.798030777777778">
1. Tokenize, tag and lemmatize the sentence.
2. Generate replacement lemmas for each content
lemma in the sentence.
3. Propose paraphrases for the sentence using dif-
ferent combinations of replacement lemmas, esti-
mate the probability of each paraphrase, and rank
the paraphrases according to their probabilities.
Retain the lemmatized sentence plus the top
paraphrases.
</listItem>
<bodyText confidence="0.99796875">
Documents or nodes are then retrieved for the sen-
tence and its paraphrases, the probability of each re-
trieved item is calculated, and the top items are
retained (Section 5).
</bodyText>
<subsubsectionHeader confidence="0.512222">
4.2.1 Tagging and lemmatizing sentences
</subsubsectionHeader>
<bodyText confidence="0.999841125">
We used Brill’s tagger (Brill, 1992) to obtain the
PoS of a word. This PoS is used to constrain the
number of synonyms generated for a word. Brill’s
tagger incorrectly tagged 16% of the queries in the
DocRet application, which had a marginal detrimen-
tal effect on retrieval performance (Zukerman and
Raskutti, 2002). After tagging, each sentence was
lemmatized using WordNet.
</bodyText>
<subsubsectionHeader confidence="0.587466">
4.2.2 Proposing replacements for each lemma
</subsubsectionHeader>
<bodyText confidence="0.9999875">
We used WordNet and the nominalization and
verbalization lists built from Webster to propose re-
placements for the content lemmas in a sentence.
These resources were used as follows:
</bodyText>
<listItem confidence="0.736370333333333">
1. For each word in the sentence, we determined its
lemma(s) and the lemma(s) that verbalize it (if it
is a noun) or nominalize it (if it is a verb).
</listItem>
<bodyText confidence="0.985258">
2. We then used WordNet to propose different types
of synonyms for the lemmas produced in the
first step. These types of synonyms were: syn-
onyms, attributes, pertainyms and seeal-
sos (Miller et al., 1990). For example, according
to WordNet, a synonym for “high” is “steep”, an
attribute is “height”, and a seealso is “tall”; a
pertainym for “chinese” is “China”. hypernyms
and hyponyms were optionally included depend-
ing on the approach used to represent the similar-
ity between two lemmas (Section 4.2.4).
</bodyText>
<subsectionHeader confidence="0.587038">
4.2.3 Paraphrasing sentences
</subsectionHeader>
<bodyText confidence="0.9999411">
Sentence paraphrases were generated by an iter-
ative process which considers each content lemma
in a sentence in turn, and proposes a replacement
lemma from those collected from our information
sources (Section 4.2.2). Sentences which did not
have sufficient context were not paraphrased. These
are sentences where all the words except one are
closed-class words or stop words (frequently occur-
ring words that are ignored when used as search
terms).
</bodyText>
<subsectionHeader confidence="0.739099">
4.2.4 Probability of a paraphrase
</subsectionHeader>
<bodyText confidence="0.9997">
The probability that a paraphrase is an appropri-
ate rendition of a sentence depends on two factors:
</bodyText>
<listItem confidence="0.985525">
(1) how similar is the paraphrase to the sentence, and
(2) how common are the lemma combinations in the
paraphrase. This may be expressed as follows:
</listItem>
<bodyText confidence="0.998971666666667">
where Para is the th paraphrase of a sentence.
Since the probability of the denominator is constant
for a given sentence, we obtain:
</bodyText>
<table confidence="0.5479382">
Pr Para Sent Pr SentPara Pr Para (2)
where
Pr SentPara (3)
Pr Slem Slem lem lem
Pr Para Pr lem lem (4)
</table>
<bodyText confidence="0.948960166666667">
where is the number of content words in a sen-
tence, Pr Slem✧ is the probability of using Slem –
the th lemma in the sentence, and Pr lem is the
probability of using lem – the th lemma in the th
paraphrase of the sentence.
To calculate Pr SentPara in Eqn. 3 we assume
</bodyText>
<listItem confidence="0.800472">
(1) Pr Slem lem lem is independent of
Pr Slem✧ lem lem for
</listItem>
<bodyText confidence="0.7545733">
and , and
(2) given lem , Slem is independent of the
other lemmas in the sentence paraphrase, i.e.,
Pr Slem lem lem Pr Slem lem .
These assumptions yield
Pr SentPara Pr Slem lem (5)
Eqn. 4 may be rewritten using Bayes rule:
Pr Para Pr lem ctxt (6)
where ctxt is the context for lemma in the th
paraphrase.
</bodyText>
<table confidence="0.684199333333333">
Substituting Eqn. 5 and Eqn. 6 into Eqn. 2 yields
Pr Para Sent (7)
Pr Slem✧ lem Pr lem ctxt
</table>
<bodyText confidence="0.9262736">
Pr Slem lem may be interpreted as the prob-
ability of using Slem instead of lem . Intuitively,
this probability depends on the similarity between
the lemmas. We considered two approaches for rep-
resenting this probability:
</bodyText>
<subsectionHeader confidence="0.560425">
Baseline similarity:
</subsectionHeader>
<bodyText confidence="0.980000333333333">
Pr Slem✧ lem if Slem✧ is a WordNet syn-
onym of lem , and 0 otherwise (where “syn-
onym” encompasses different types of WordNet
similarities).
Webster cosine similarity:
Pr Slem✧ lem SimScore lem Slem✧ if
Slem✧ is a synonym of lem according to the
Webster thesaurus, and 0 otherwise (where Sim-
Score is obtained by applying the cosine similar-
ity measure, Section 4.1). This approach yields
a reduced number of synonyms, which are at the
intersection between the lemmas in the Webster
thesaurus and those returned by WordNet. This
enables us to additionally consider hypernyms
and hyponyms returned by WordNet.3
</bodyText>
<footnote confidence="0.98520175">
3Experiments show that considering hypernyms and hy-
ponyms under the baseline similarity measure exponentially
increases the number of alternative paraphrases without improv-
ing retrieval performance.
</footnote>
<table confidence="0.967831">
Pr SentPara Pr Para (1)
Pr Sent
Pr Para Sent
</table>
<bodyText confidence="0.8789424">
Pr lem ctxt may be represented by
Pr lem lem lem . We consider the
baseline measure Pr lem lem lem ,
and also the following approximation:
Pr lem lem lem Pr lem lem
</bodyText>
<equation confidence="0.569979">
(8)
</equation>
<bodyText confidence="0.9978068">
where Pr lem lem is obtained directly from
the lemma-pair dictionary (Section 4.1). This ap-
proximation, although ad hoc, works well in prac-
tice, yielding a better performance than bi-gram ap-
proximations (Zukerman and Raskutti, 2002).
</bodyText>
<sectionHeader confidence="0.999202" genericHeader="method">
5 Retrieval Procedures
</sectionHeader>
<bodyText confidence="0.99997575">
In this section, we describe the retrieval techniques
used for our two applications, and present proba-
bilistic formulations that incorporate sentence para-
phrasing into our retrieval procedures.
</bodyText>
<subsectionHeader confidence="0.9963">
5.1 Document retrieval
</subsectionHeader>
<bodyText confidence="0.976465375">
The retrieval procedure for the DocRet application
relies on the vector-space model, which calculates
the score of candidate documents given a list of
terms in a query (Salton and McGill, 1983). Nor-
mally, this score is based on the TF.IDF measure
(Term Frequency . Inverse Document Frequency).
If we replace a query with its paraphrase, this score
is represented by the following formula:
where is the number of lemmas in the paraphrase.
By normalizing the scores of the documents, we
obtain the probability that a document contains the
answer to the paraphrase:
We assume that given a paraphrase of a query, a
document retrieved on the basis of the paraphrase
is conditionally independent of the original query.
This yields the following formula:
Pr DocQuery Pr DocPara ✟Pr Para Query
(11)
where is the number of paraphrases. We also adopt
the convention that the 0-th paraphrase is the origi-
nal lemmatized query.
By substituting Eqn. 10 and Eqn. 7 for the first
and second factors in Eqn. 11 we can calculate the
probability of each document given a query.
</bodyText>
<subsectionHeader confidence="0.992253">
5.2 Node identification
</subsectionHeader>
<bodyText confidence="0.99985425">
In order to find the node in the BN that best matches
a user’s sentence, we calculate for each node a score
that reflects its similarity to the user’s sentence.
This score is composed of two elements: SP-score
and NP-score. SP-score is obtained by paraphras-
ing the user’s sentence, and calculating a similar-
ity score between each paraphrase and the canonical
sentence(s) for the node in question. NP-score is ob-
tained by paraphrasing the canonical sentence(s) for
the node, and calculating a similarity score between
each paraphrase and the user’s sentence. These el-
ements are represented by the following formulas:
</bodyText>
<table confidence="0.464930833333333">
SP-score NodeSent (12)
func SimScore Para Sent Node
Pr Para Sent Sent
NP-score NodeSent (13)
func SimScore Sent Para Node
Pr Para Node Node
</table>
<bodyText confidence="0.999755055555556">
where SimScore is calculated using the cosine sim-
ilarity measure (which was also used to build the
Webster thesaurus and the nominal and verb lists,
Section 4.1); Pr Para item item is estimated us-
ing Eqn. 7; is the number of paraphrases gen-
erated for sentences and nodes; and func is either
maximum or average. When func=maximum, the
SP-score of a node is the score of the paraphrase
of the user’s sentence that best matches the node’s
canonical sentence, and the NP-score is the score of
the paraphrase of the node which best matches the
user’s sentence. When func=average, the SP-score
of a node is the weighted average of the scores of
the paraphrases of the user’s sentence, and the NP-
score is the weighted average of the scores of the
paraphrases of the node.
The probability that a node is intended by a user’s
sentence is the normalized value of
</bodyText>
<table confidence="0.720006">
Pr NodeSent (14)
func SP-score NodeSent NP-score NodeSent
Score DocPara tfidf Doc,lem✧ (9)
Pr DocPara tfidf Doc,lem (10)
</table>
<bodyText confidence="0.999979555555556">
where func=maximum retains the best of SP-score
and NP-score, and func=average simply averages
these scores.
We also conducted experiments using SP-score or
NP-score alone, but the best results were obtained by
combining these scores. In the future, we intend to
experiment with a measure that compares the para-
phrases of users’ sentences against the paraphrases
of nodes, instead of using SP- and NP-scores.
</bodyText>
<sectionHeader confidence="0.998592" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999752333333333">
In this section we describe our evaluation metric and
data sets. We then discuss our experiments, and an-
alyze our results.
</bodyText>
<subsectionHeader confidence="0.984893">
6.1 Evaluation metric
</subsectionHeader>
<bodyText confidence="0.999990421052632">
We use the number of matches measure to evalu-
ate retrieval performance. In the DocRet applica-
tion, this measure returns the number of queries for
which the system retrieved at least one document
that contains the answer to a query. In the NodeID
application, this measure returns the number of sen-
tences for which the intended node was among the
returned nodes. The following example illustrates
why this measure was chosen instead of the stan-
dard precision measure. Consider a situation in the
DocRet application where 10 correct documents are
retrieved for each of 2 queries and 0 correct doc-
uments for each of 3 queries, compared to a situ-
ation where 2 correct documents are retrieved for
each of 5 queries. Average precision would yield a
better score for the first situation, failing to address
the question of interest for this application, namely
how many queries have a chance of being answered,
which is 2 in the first case and 5 in the second case.
</bodyText>
<subsectionHeader confidence="0.99883">
6.2 Data
</subsectionHeader>
<bodyText confidence="0.999470916666667">
The data for the DocRet application consisted of
125 queries from the TREC8 collection, 404 queries
from TREC9, and 231 queries from TREC10. These
are queries whose answers reside in the LA Times
portion of the TREC corpus.
The data for the NodeID application was gener-
ated by asking high-school and university students
to modify the canonical sentences corresponding to
7 of the nodes in our BN. No constraints were placed
on the allowed modifications, i.e., students were ad-
vised that they could make any changes to the sen-
tences, provided their meaning was preserved. We
</bodyText>
<tableCaption confidence="0.999547">
Table 1: Configurations of paraphrasing parameters
</tableCaption>
<table confidence="0.99389">
P1 P2 P3
WN-CTXT WordNet baseline ctxt
WNWEB-SIM WordNet+Webster sim baseline
WNWEB-CTXT WordNet+Webster baseline ctxt
WNWEB-SIMCTXT WordNet+Webster sim ctxt
</table>
<bodyText confidence="0.9132345">
received a total of 258 modified sentences from 21
students (an average of 37 sentences per node).
</bodyText>
<subsectionHeader confidence="0.988271">
6.3 Experiments
</subsectionHeader>
<bodyText confidence="0.900112615384615">
Our evaluation determines the effect of paraphras-
ing on retrieval performance, as well as the number
of paraphrases that yields the best performance. For
both applications, we submitted to the retrieval en-
gine increasing sets of paraphrases as follows: first
the lemmatized query or sentence alone (Set 0), next
we added to the query or sentence up to 2 para-
phrases (Set 2), then up to 5 paraphrases (Set 5), up
to 12 paraphrases (Set 12), and up to a maximum of
19 paraphrases (Set 19).4
For the DocRet application, the number of re-
trieved documents was kept constant at 200, as sug-
gested in (Moldovan et al., 2002). Since for the
NodeID application there is only one correct node,
it is critical to return this node most of the time.
Hence, we considered 1 or 3 retrieved nodes.
We experimented with different combinations of
the operating parameters of the paraphrasing pro-
cess as follows: (P1) WordNet alone or Word-
Net+Webster; (P2) Pr Slem✧ lem – baseline
measure or Webster similarity score (sim); and
(P3) Pr lem ctxt – baseline measure or the ap-
proximation in Eqn. 8 (ctxt). The combination of
these parameters yielded the four configurations in
Table 1 (the remaining options are not viable).
In addition, we considered the following options
for the retrieval process: (R1) Weighting – uniform
or weighted; and (R2) func – maximum or average
(only for the NodeID application, Section 5.2).5 The
uniform policy in R1 assigns the same weight (=1)
4These numbers represent the maximum number of para-
phrases – fewer paraphrases were generated if there weren’t
enough synonyms. Also, previous experiments show that
Sets 0, 2, 5, 12 and 19 are significant in terms of retrieval per-
formance, and that there seems to be no advantage in generating
more than 19 paraphrases.
5func=maximum is not suitable for the DocRet application,
as a DocRet document does not contain different versions of
one sentence, as is the case for the NodeID application.
</bodyText>
<figure confidence="0.9946610625">
No. of sentences matched
No. of sentences matched
0 2 4 6 8 10 12 14 16 18
No. of paraphrases
0 2 4 6 8 10 12 14 16 18
No. of paraphrases
230
220
210
200
190
180
170
160
WNWEB-SIMCTXT/UNI/MAX
WNWEB-CTXT/UNI/MAX
WNWEB-SIM/UNI/MAX
WNWEB-CTXT/WGT/AVG
230
220
210
200
190
180
170
16
0
WNWEB-SIMCTXT/UNI/MAX
WNWEB-CTXT/UNI/MAX
WNWEB-SIM/UNI/MAX
WNWEB-CTXT/WGT/AVG
(a) 1 retrieved node (b) 3 retrieved nodes
</figure>
<figureCaption confidence="0.999996">
Figure 1: Effect of number of paraphrases on performance for the NodeID application (258 sentences)
</figureCaption>
<bodyText confidence="0.99854875">
to the paraphrases during retrieval, instead of using
the weights obtained from the paraphrasing process
(weighted policy). The rationale for the uniform pol-
icy is that while a sorting criterion is required to rank
the paraphrases, once a ranking is obtained, the top-
K paraphrases may be equally useful.
The policies in R1 yielded four additional con-
figurations, one for each entry in Table 1. These
configurations were used to generate and rank
the paraphrases, but the top-K paraphrases were
weighed uniformly during retrieval (we distin-
guish the weighted policy from the uniform pol-
icy by appending the suffix WGT to the weighed
options and the suffix UNI to the uniform op-
tions). Finally, the policies in R2 yielded another
eight configurations for the NodeID application only
(each of the above options with func=maximum or
func=average). These configurations are denoted by
appending the suffix MAX or AVG to the above desig-
nations, e.g., WNWEB-SIMCTXT/UNI/AVG.
</bodyText>
<sectionHeader confidence="0.797788" genericHeader="evaluation">
6.4 Results
</sectionHeader>
<bodyText confidence="0.9999">
Paraphrasing improved retrieval performance under
the vast majority of the paraphrasing and retrieval
options for both applications. Performance gen-
erally improved as the number of paraphrases in-
creased, with the largest improvement being ob-
tained for 2-5 paraphrases. However, it is worth not-
ing that when retrieval was performed using the uni-
form R1 policy and func=average, the performance
was sometimes unstable. That is, the number of
matched sentences or answerable queries sometimes
</bodyText>
<figure confidence="0.9251095">
0 2 4 6 8 10 12 14 16 18
No. of paraphrases
</figure>
<figureCaption confidence="0.932463333333333">
Figure 2: Effect of number of paraphrases on per-
formance for the DocRet application (TREC9, 404
queries, 200 retrieved documents)
</figureCaption>
<bodyText confidence="0.999948571428572">
fluctuated as the number of paraphrases increased.
This may be explained by the observation that low
probability paraphrases (i.e., paraphrases with a low
Webster similarity score to the original sentence, or
with a low context-probability) have the same con-
tribution to the score of a node or a retrieved docu-
ment as high-probability paraphrases.
Figures 1(a), 1(b) and 2 depict the performance
of the four best configurations among those dis-
cussed in Section 6.3. Figures 1(a) and 1(b) show
the performance obtained for the NodeID applica-
tion for 1 retrieved node and 3 retrieved nodes
respectively. The retrieval performance obtained
for TREC9 queries in the DocRet application is
</bodyText>
<figure confidence="0.992552833333333">
No. of answerable queries
275
270
265
260
255
250
245
WNWEB-SIMCTXT/UNI/AVG
WNWEB-CTXT/UNI/AVG
WNWEB-SIM/UNI/AVG
WN-CTXT/WGT/AVG
</figure>
<bodyText confidence="0.99996315">
shown in Figure 2 (paraphrasing also improves per-
formance for TREC8 and TREC10, but the im-
provements are more modest than for TREC9). As
can be seen from these Figures, paraphrasing has a
greater impact on the NodeID application than on the
Docxet application. Specifically, the best configu-
ration for NodeID (WNWEB-SIMCTXT/UNI/MAX with 19
paraphrases) improved the number of matched sen-
tences from 65.9% to 82.2% for 1 retrieved node,
and from 70.5% to 85.3% for 3 retrieved nodes. In
contrast, the best configuration for Docxet (WNWEB-
CTXT/UNI/AVG with 19 paraphrases) improved the
number of answerable queries from 62.1% to 66.8%.
Finally, note that the top-4 configurations for our
two applications differ (but with respect to one as-
pect only). This indicates that while paraphrasing
generally improves retrieval performance, the char-
acteristics of the application determine both the im-
pact of paraphrasing and the paraphrasing and re-
trieval configuration that works best.
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999977625">
We have investigated the effect of lexical paraphras-
ing on two applications: document retrieval and
node identification. Paraphrasing was performed us-
ing syntactic, semantic and statistical information.
Our results show that paraphrasing improves re-
trieval performance. However, the improvement is
sensitive to the application at hand and to the para-
phrasing and retrieval operating parameters.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.944263333333333">
This research was supported in part by grants
A49927212 and DP0209565 from the Australian
Research Council.
</bodyText>
<sectionHeader confidence="0.998853" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901521739131">
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In ANLP-92 – Proceedings of the Third Confer-
ence on Applied Natural Language Processing, pages
152–155, Trento, Italy.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan
Cigarran. 1998. Indexing with WordNet synsets can
improve text retrieval. In Proceedings ofthe COLING-
ACL’98 Workshop on Usage of WordNet in Natural
Language Processing Systems, pages 38–44, Mon-
treal, Canada.
Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana
Girju, Vasile Rus, and Paul Morarescu. 2001. The role
of lexico-semantic feedback in open domain textual
question-answering. In ACL01 – Proceedings of the
39th Annual Meeting of the Association for Computa-
tional Linguistics, pages 274–281, Toulouse, France.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL’98 – Proceedings of the International
Conference on Computational Linguistics and the An-
nual Meeting of the Association for Computational
Linguistics, pages 704–710, Montreal, Canada.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL’98 – Proceedings of
the International Conference on Computational Lin-
guistics and the Annual Meeting of the Association for
Computational Linguistics, pages 768–774, Montreal,
Canada.
Steven Lytinen, Noriko Tomuro, and Tom Repede. 2000.
The use of WordNet sense tagging in FAQfinder. In
Proceedings of the AAAI00 Workshop on AI and Web
Search, Austin, Texas.
Rada Mihalcea and Dan Moldovan. 1999. A method
for word sense disambiguation of unrestricted text. In
ACL99 – Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, Balti-
more, Maryland.
George Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Introduc-
tion to WordNet: An on-line lexical database. Journal
ofLexicography, 3(4):235–244.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mi-
hai Surdeanu. 2002. Performance issues and error
analysis in an open domain question answering sys-
tem. In ACL02 – Proceedings of the 40th Annual
Meeting ofthe Association for Computational Linguis-
tics, pages 33–40, Philadelphia, Pennsylvania.
Judea Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems. Morgan Kaufmann Publishers, San Mateo,
California.
G. Salton and M.J. McGill. 1983. An Introduction to
Modern Information Retrieval. McGraw Hill.
Hinrich Sch¨utze and Jan O. Pedersen. 1995. Informa-
tion retrieval based on word senses. In Proceedings of
the Fourth Annual Symposium on Document Analysis
and Information Retrieval, pages 161–175, Las Vegas,
Nevada.
Ingrid Zukerman and Sarah George. 2002. Towards
a noise-tolerant, representation-independent mecha-
nism for argument interpretation. In COLING 2002
– Proceedings of the 19th International Conference on
Computational Linguistics, pages 1170–1176, Taipei,
Taiwan.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In COL-
ING’02 – Proceedings of the 19th International Con-
ference on Computational Linguistics, pages 1177–
1183, Taipei, Taiwan.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.123817">
<title confidence="0.995343">Lexical Paraphrasing for Document Retrieval and Node Identification</title>
<author confidence="0.924878">Ingrid</author>
<affiliation confidence="0.900503">School of Computer and Software Monash</affiliation>
<address confidence="0.922306">Clayton, VICTORIA AUSTRALIA</address>
<email confidence="0.958359">ingrid@csse.monash.edu.au</email>
<author confidence="0.977834">Sarah</author>
<affiliation confidence="0.904015">School of Computer and Software Monash</affiliation>
<address confidence="0.9427995">Clayton, VICTORIA AUSTRALIA</address>
<email confidence="0.993242">sarahg@csse.monash.edu.au</email>
<author confidence="0.578959">Yingying</author>
<affiliation confidence="0.900921333333333">School of Computer and Software Monash</affiliation>
<address confidence="0.9233">Clayton, VICTORIA AUSTRALIA</address>
<email confidence="0.996324">ywen@csse.monash.edu.au</email>
<abstract confidence="0.9985098125">We investigate lexical paraphrasing in the context of two distinct applications: document retrieval and node identification. Document retrieval – the first step in question answering – retrieves documents that contain answers to user queries. Node identification – performed in the context of a Bayesian argumentation system – matches users’ Natural Language sentences to nodes in a Bayesian network. Lexical paraphrases are generated using syntactic, semantic and corpus-based information. Our evaluation shows that lexical paraphrasing improves retrieval performance for both applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In ANLP-92 – Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="2353" citStr="Brill, 1992" startWordPosition="340" endWordPosition="341">formed in the context of the TREC Question Answering task, where the system retrieves documents that contain answers to users’ queries. Node identification is performed in the context of a Natural Language (NL) interface to a Bayesian argumentation system (Zukerman and George, 2002). Here the system finds the nodes from a Bayesian network (BN) (Pearl, 1988) that best match a user’s NL sentences. Lexical paraphrases replace content words in a user’s input with their synonyms. We use the following information sources to perform this task: syntactic – obtained from Brill’s part-of-speech tagger (Brill, 1992); semantic – obtained from WordNet (Miller et al., 1990) and the Webster-1913 online dictionary; and statistical – obtained from our document collection. The statistical information is used to moderate the alternatives obtained from the semantic resources, by preferring query paraphrases that contain frequent word combinations. Our evaluation assessed the effect of lexical paraphrasing on our two applications. Its impact on document retrieval was evaluated using subsets of queries from the TREC8, TREC9 and TREC10 collections, and its effect on node identification was evaluated using paraphrase</context>
<context position="8047" citStr="Brill, 1992" startWordPosition="1254" endWordPosition="1255">al articles that answer a user’s query, while for NodeID at most one node matches a user’s sentence.1 4 Lexical Paraphrasing In this section, we discuss the resources used by our paraphrasing mechanism and describe the paraphrasing process. &apos;The user may utter sentences that have no counterpart in the system’s BN. Also, at present we are not dealing with sentences that include more than one node. 4.1 Resources Our system uses syntactic, semantic and statistical information for paraphrase generation. Syntactic information for each sentence was obtained from Brill’s part-of-speech (PoS) tagger (Brill, 1992). Semantic information was obtained from two sources: WordNet – a knowledge-intensive, handbuilt on-line repository; and Webster – an on-line version of the Webster-1913 dictionary (http://www.dict.org). WordNet was used to generate lemmas for the corpus and the sentences, and to generate different types of synonyms for the words in the sentences. Webster was used to automatically build a thesaurus that includes similarity scores between lemmas, and to automatically construct a list of nominals corresponding to the verbs in the dictionary, and a list of verbs corresponding to the nouns in the </context>
<context position="11298" citStr="Brill, 1992" startWordPosition="1770" endWordPosition="1771">nize, tag and lemmatize the sentence. 2. Generate replacement lemmas for each content lemma in the sentence. 3. Propose paraphrases for the sentence using different combinations of replacement lemmas, estimate the probability of each paraphrase, and rank the paraphrases according to their probabilities. Retain the lemmatized sentence plus the top paraphrases. Documents or nodes are then retrieved for the sentence and its paraphrases, the probability of each retrieved item is calculated, and the top items are retained (Section 5). 4.2.1 Tagging and lemmatizing sentences We used Brill’s tagger (Brill, 1992) to obtain the PoS of a word. This PoS is used to constrain the number of synonyms generated for a word. Brill’s tagger incorrectly tagged 16% of the queries in the DocRet application, which had a marginal detrimental effect on retrieval performance (Zukerman and Raskutti, 2002). After tagging, each sentence was lemmatized using WordNet. 4.2.2 Proposing replacements for each lemma We used WordNet and the nominalization and verbalization lists built from Webster to propose replacements for the content lemmas in a sentence. These resources were used as follows: 1. For each word in the sentence, </context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A simple rule-based part of speech tagger. In ANLP-92 – Proceedings of the Third Conference on Applied Natural Language Processing, pages 152–155, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
<author>Irina Chugur</author>
<author>Juan Cigarran</author>
</authors>
<title>Indexing with WordNet synsets can improve text retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings ofthe COLINGACL’98 Workshop on Usage of WordNet in Natural Language Processing Systems,</booktitle>
<pages>38--44</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4005" citStr="Gonzalo et al., 1998" startWordPosition="593" endWordPosition="596">s is often addressed through query expansion. Query expansion in turn is often preceded by Word sense disambiguation (WSD) in order to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used WordNet to propose synonyms for the words in a query, and applied heuristics to select which words to paraphrase. Our approach to the vocabulary mis-match problem differs from WSD in that instead of returning the sense of the words in a sentence, we propose alternative lexical paraphrases. Like Langkilde and Knight (1998), when generating candidate para</context>
</contexts>
<marker>Gonzalo, Verdejo, Chugur, Cigarran, 1998</marker>
<rawString>Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan Cigarran. 1998. Indexing with WordNet synsets can improve text retrieval. In Proceedings ofthe COLINGACL’98 Workshop on Usage of WordNet in Natural Language Processing Systems, pages 38–44, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>Razvan Bunescu</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
<author>Paul Morarescu</author>
</authors>
<title>The role of lexico-semantic feedback in open domain textual question-answering.</title>
<date>2001</date>
<booktitle>In ACL01 – Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>274--281</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="4188" citStr="Harabagiu et al. (2001)" startWordPosition="624" endWordPosition="627">cea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used WordNet to propose synonyms for the words in a query, and applied heuristics to select which words to paraphrase. Our approach to the vocabulary mis-match problem differs from WSD in that instead of returning the sense of the words in a sentence, we propose alternative lexical paraphrases. Like Langkilde and Knight (1998), when generating candidate paraphrases, we use WordNet to propose synonyms for the words in a sentence. However, they chose among these synonyms using the word-sense rankings offered by WordNet, while we make our s</context>
<context position="9166" citStr="Harabagiu et al., 2001" startWordPosition="1428" endWordPosition="1431">als corresponding to the verbs in the dictionary, and a list of verbs corresponding to the nouns in the dictionary. The thesaurus was used to assign similarity scores to the synonyms returned by WordNet. The nominalization and verbalization lists were used to generate additional synonyms. This was done by activating WordNet for the nouns corresponding to the verbs in the sentences and for the verbs corresponding to the nouns in the sentences.2 The idea was that nominalizations and verbalizations will help paraphrase queries such as “who killed Lincoln?” into “who is the murderer of Lincoln?” (Harabagiu et al., 2001). The thesaurus, nominal list and verb list were obtained by building a vector from the content lemmas in the definition of each word in the dictionary, and applying the cosine similarity measure to calculate a score for the similarity between the vector corresponding to each word and the vectors corresponding to the other words in the dictionary. The dictionary words that had the highest similarity score against the original word were retained. To build the thesaurus, only words with the same PoS as the original word were considered. To build the nominalization list, only nouns were considere</context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Morarescu, 2001</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana Girju, Vasile Rus, and Paul Morarescu. 2001. The role of lexico-semantic feedback in open domain textual question-answering. In ACL01 – Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 274–281, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In COLING-ACL’98 – Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>704--710</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4573" citStr="Langkilde and Knight (1998)" startWordPosition="690" endWordPosition="694">er, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used WordNet to propose synonyms for the words in a query, and applied heuristics to select which words to paraphrase. Our approach to the vocabulary mis-match problem differs from WSD in that instead of returning the sense of the words in a sentence, we propose alternative lexical paraphrases. Like Langkilde and Knight (1998), when generating candidate paraphrases, we use WordNet to propose synonyms for the words in a sentence. However, they chose among these synonyms using the word-sense rankings offered by WordNet, while we make our selection using word-pair frequencies obtained from our corpus, and word-similarity information obtained from a thesaurus that is automatically constructed from the Webster Dictionary. It is worth noting that the retrieval performance obtained with paraphrases generated using our dictionary-based thesaurus compares favorably with that obtained using Lin’s context-based thesaurus. 3 A</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In COLING-ACL’98 – Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics, pages 704–710, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL’98 – Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3737" citStr="Lin (1998)" startWordPosition="555" endWordPosition="556">ider the paraphrasing process, and in Section 5 the information retrieval procedures. Section 6 presents the results of our evaluation, followed by concluding remarks. 2 Related Research The vocabulary mis-match between a user’s queries and indexed documents is often addressed through query expansion. Query expansion in turn is often preceded by Word sense disambiguation (WSD) in order to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used WordNet to propose synonyms for the words in a query, and applied heuristics to select </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL’98 – Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics, pages 768–774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Lytinen</author>
<author>Noriko Tomuro</author>
<author>Tom Repede</author>
</authors>
<title>The use of WordNet sense tagging in FAQfinder.</title>
<date>2000</date>
<booktitle>In Proceedings of the AAAI00 Workshop on AI and Web Search,</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="3614" citStr="Lytinen et al. (2000)" startWordPosition="530" endWordPosition="533">our BN (Section 6.2). In the next section we discuss related research. Section 3 describes our two applications. In Section 4, we consider the paraphrasing process, and in Section 5 the information retrieval procedures. Section 6 presents the results of our evaluation, followed by concluding remarks. 2 Related Research The vocabulary mis-match between a user’s queries and indexed documents is often addressed through query expansion. Query expansion in turn is often preceded by Word sense disambiguation (WSD) in order to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form </context>
</contexts>
<marker>Lytinen, Tomuro, Repede, 2000</marker>
<rawString>Steven Lytinen, Noriko Tomuro, and Tom Repede. 2000. The use of WordNet sense tagging in FAQfinder. In Proceedings of the AAAI00 Workshop on AI and Web Search, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>A method for word sense disambiguation of unrestricted text.</title>
<date>1999</date>
<booktitle>In ACL99 – Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="3588" citStr="Mihalcea and Moldovan (1999)" startWordPosition="525" endWordPosition="528">ated by people for some nodes in our BN (Section 6.2). In the next section we discuss related research. Section 3 describes our two applications. In Section 4, we consider the paraphrasing process, and in Section 5 the information retrieval procedures. Section 6 presents the results of our evaluation, followed by concluding remarks. 2 Related Research The vocabulary mis-match between a user’s queries and indexed documents is often addressed through query expansion. Query expansion in turn is often preceded by Word sense disambiguation (WSD) in order to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001)</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 1999. A method for word sense disambiguation of unrestricted text. In ACL99 – Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>Journal ofLexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="2409" citStr="Miller et al., 1990" startWordPosition="348" endWordPosition="351">ring task, where the system retrieves documents that contain answers to users’ queries. Node identification is performed in the context of a Natural Language (NL) interface to a Bayesian argumentation system (Zukerman and George, 2002). Here the system finds the nodes from a Bayesian network (BN) (Pearl, 1988) that best match a user’s NL sentences. Lexical paraphrases replace content words in a user’s input with their synonyms. We use the following information sources to perform this task: syntactic – obtained from Brill’s part-of-speech tagger (Brill, 1992); semantic – obtained from WordNet (Miller et al., 1990) and the Webster-1913 online dictionary; and statistical – obtained from our document collection. The statistical information is used to moderate the alternatives obtained from the semantic resources, by preferring query paraphrases that contain frequent word combinations. Our evaluation assessed the effect of lexical paraphrasing on our two applications. Its impact on document retrieval was evaluated using subsets of queries from the TREC8, TREC9 and TREC10 collections, and its effect on node identification was evaluated using paraphrases generated by people for some nodes in our BN (Section </context>
<context position="3649" citStr="Miller et al., 1990" startWordPosition="536" endWordPosition="539">ction we discuss related research. Section 3 describes our two applications. In Section 4, we consider the paraphrasing process, and in Section 5 the information retrieval procedures. Section 6 presents the results of our evaluation, followed by concluding remarks. 2 Related Research The vocabulary mis-match between a user’s queries and indexed documents is often addressed through query expansion. Query expansion in turn is often preceded by Word sense disambiguation (WSD) in order to avoid retrieving irrelevant information. Mihalcea and Moldovan (1999) and Lytinen et al. (2000) used WordNet (Miller et al., 1990) to obtain the sense of a word. In contrast, Sch¨utze and Pedersen (1995) and Lin (1998) used a corpus-based approach where they automatically constructed a thesaurus on the basis of contextual information. The results obtained by Sch¨utze and Pedersen and by Lytinen et al. are encouraging. However, experimental results reported in (Gonzalo et al., 1998) indicate that the improvement in IR performance due to WSD is restricted to short queries, and that IR performance is very sensitive to disambiguation errors. Harabagiu et al. (2001) offered a different form of query expansion, where they used</context>
<context position="12217" citStr="Miller et al., 1990" startWordPosition="1923" endWordPosition="1926">h sentence was lemmatized using WordNet. 4.2.2 Proposing replacements for each lemma We used WordNet and the nominalization and verbalization lists built from Webster to propose replacements for the content lemmas in a sentence. These resources were used as follows: 1. For each word in the sentence, we determined its lemma(s) and the lemma(s) that verbalize it (if it is a noun) or nominalize it (if it is a verb). 2. We then used WordNet to propose different types of synonyms for the lemmas produced in the first step. These types of synonyms were: synonyms, attributes, pertainyms and seealsos (Miller et al., 1990). For example, according to WordNet, a synonym for “high” is “steep”, an attribute is “height”, and a seealso is “tall”; a pertainym for “chinese” is “China”. hypernyms and hyponyms were optionally included depending on the approach used to represent the similarity between two lemmas (Section 4.2.4). 4.2.3 Paraphrasing sentences Sentence paraphrases were generated by an iterative process which considers each content lemma in a sentence in turn, and proposes a replacement lemma from those collected from our information sources (Section 4.2.2). Sentences which did not have sufficient context wer</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Introduction to WordNet: An on-line lexical database. Journal ofLexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Sanda Harabagiu</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Performance issues and error analysis in an open domain question answering system.</title>
<date>2002</date>
<booktitle>In ACL02 – Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="21838" citStr="Moldovan et al., 2002" startWordPosition="3544" endWordPosition="3547">ments Our evaluation determines the effect of paraphrasing on retrieval performance, as well as the number of paraphrases that yields the best performance. For both applications, we submitted to the retrieval engine increasing sets of paraphrases as follows: first the lemmatized query or sentence alone (Set 0), next we added to the query or sentence up to 2 paraphrases (Set 2), then up to 5 paraphrases (Set 5), up to 12 paraphrases (Set 12), and up to a maximum of 19 paraphrases (Set 19).4 For the DocRet application, the number of retrieved documents was kept constant at 200, as suggested in (Moldovan et al., 2002). Since for the NodeID application there is only one correct node, it is critical to return this node most of the time. Hence, we considered 1 or 3 retrieved nodes. We experimented with different combinations of the operating parameters of the paraphrasing process as follows: (P1) WordNet alone or WordNet+Webster; (P2) Pr Slem✧ lem – baseline measure or Webster similarity score (sim); and (P3) Pr lem ctxt – baseline measure or the approximation in Eqn. 8 (ctxt). The combination of these parameters yielded the four configurations in Table 1 (the remaining options are not viable). In addition, w</context>
</contexts>
<marker>Moldovan, Pasca, Harabagiu, Surdeanu, 2002</marker>
<rawString>Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mihai Surdeanu. 2002. Performance issues and error analysis in an open domain question answering system. In ACL02 – Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, pages 33–40, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems.</title>
<date>1988</date>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="2100" citStr="Pearl, 1988" startWordPosition="299" endWordPosition="300">d, the system may be unable to satisfy the user’s requirements. In this paper, we investigate the application of lexical paraphrasing to two different information access applications: document retrieval and node identification. Document retrieval is performed in the context of the TREC Question Answering task, where the system retrieves documents that contain answers to users’ queries. Node identification is performed in the context of a Natural Language (NL) interface to a Bayesian argumentation system (Zukerman and George, 2002). Here the system finds the nodes from a Bayesian network (BN) (Pearl, 1988) that best match a user’s NL sentences. Lexical paraphrases replace content words in a user’s input with their synonyms. We use the following information sources to perform this task: syntactic – obtained from Brill’s part-of-speech tagger (Brill, 1992); semantic – obtained from WordNet (Miller et al., 1990) and the Webster-1913 online dictionary; and statistical – obtained from our document collection. The statistical information is used to moderate the alternatives obtained from the semantic resources, by preferring query paraphrases that contain frequent word combinations. Our evaluation as</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann Publishers, San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>An Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="16169" citStr="Salton and McGill, 1983" startWordPosition="2595" endWordPosition="2598">the lemma-pair dictionary (Section 4.1). This approximation, although ad hoc, works well in practice, yielding a better performance than bi-gram approximations (Zukerman and Raskutti, 2002). 5 Retrieval Procedures In this section, we describe the retrieval techniques used for our two applications, and present probabilistic formulations that incorporate sentence paraphrasing into our retrieval procedures. 5.1 Document retrieval The retrieval procedure for the DocRet application relies on the vector-space model, which calculates the score of candidate documents given a list of terms in a query (Salton and McGill, 1983). Normally, this score is based on the TF.IDF measure (Term Frequency . Inverse Document Frequency). If we replace a query with its paraphrase, this score is represented by the following formula: where is the number of lemmas in the paraphrase. By normalizing the scores of the documents, we obtain the probability that a document contains the answer to the paraphrase: We assume that given a paraphrase of a query, a document retrieved on the basis of the paraphrase is conditionally independent of the original query. This yields the following formula: Pr DocQuery Pr DocPara ✟Pr Para Query (11) wh</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. An Introduction to Modern Information Retrieval. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Jan O Pedersen</author>
</authors>
<title>Information retrieval based on word senses.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<location>Las Vegas, Nevada.</location>
<marker>Sch¨utze, Pedersen, 1995</marker>
<rawString>Hinrich Sch¨utze and Jan O. Pedersen. 1995. Information retrieval based on word senses. In Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval, pages 161–175, Las Vegas, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Sarah George</author>
</authors>
<title>Towards a noise-tolerant, representation-independent mechanism for argument interpretation.</title>
<date>2002</date>
<booktitle>In COLING 2002 – Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1170--1176</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="2024" citStr="Zukerman and George, 2002" startWordPosition="284" endWordPosition="287">f this problem: if a user’s vocabulary differs from that within the resource being accessed, the system may be unable to satisfy the user’s requirements. In this paper, we investigate the application of lexical paraphrasing to two different information access applications: document retrieval and node identification. Document retrieval is performed in the context of the TREC Question Answering task, where the system retrieves documents that contain answers to users’ queries. Node identification is performed in the context of a Natural Language (NL) interface to a Bayesian argumentation system (Zukerman and George, 2002). Here the system finds the nodes from a Bayesian network (BN) (Pearl, 1988) that best match a user’s NL sentences. Lexical paraphrases replace content words in a user’s input with their synonyms. We use the following information sources to perform this task: syntactic – obtained from Brill’s part-of-speech tagger (Brill, 1992); semantic – obtained from WordNet (Miller et al., 1990) and the Webster-1913 online dictionary; and statistical – obtained from our document collection. The statistical information is used to moderate the alternatives obtained from the semantic resources, by preferring </context>
<context position="5679" citStr="Zukerman and George, 2002" startWordPosition="856" endWordPosition="859">ated using our dictionary-based thesaurus compares favorably with that obtained using Lin’s context-based thesaurus. 3 Applications Our two applications are: document retrieval (DocRet) and node identification (NodeID). The document retrieval application consists of retrieving documents that are likely to contain the answer to a user’s query. This is the first step in our Question Answering project, whose aim is to use these documents to generate answers to queries. The node identification application is the backbone of the NL interface to our Bayesian Interactive Argumentation System (BIAS) (Zukerman and George, 2002). Here the system matches NL sentences that make up a user’s argument to nodes in a BN that represents the system’s domain knowledge. The eventual goal is to understand the user’s argument (in terms of the system’s domain knowledge) and generate responses. There are significant differences between these applications and their underlying domains: Our DocRet repository consists of 131,896 articles from the LA Times portion of the NIST TREC collection (the other TREC repositories were omitted owing to disk space limitations). Full-text indexing was performed for these documents using lemmas (unin</context>
</contexts>
<marker>Zukerman, George, 2002</marker>
<rawString>Ingrid Zukerman and Sarah George. 2002. Towards a noise-tolerant, representation-independent mechanism for argument interpretation. In COLING 2002 – Proceedings of the 19th International Conference on Computational Linguistics, pages 1170–1176, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Bhavani Raskutti</author>
</authors>
<title>Lexical query paraphrasing for document retrieval.</title>
<date>2002</date>
<booktitle>In COLING’02 – Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1177--1183</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="11577" citStr="Zukerman and Raskutti, 2002" startWordPosition="1815" endWordPosition="1818">phrases according to their probabilities. Retain the lemmatized sentence plus the top paraphrases. Documents or nodes are then retrieved for the sentence and its paraphrases, the probability of each retrieved item is calculated, and the top items are retained (Section 5). 4.2.1 Tagging and lemmatizing sentences We used Brill’s tagger (Brill, 1992) to obtain the PoS of a word. This PoS is used to constrain the number of synonyms generated for a word. Brill’s tagger incorrectly tagged 16% of the queries in the DocRet application, which had a marginal detrimental effect on retrieval performance (Zukerman and Raskutti, 2002). After tagging, each sentence was lemmatized using WordNet. 4.2.2 Proposing replacements for each lemma We used WordNet and the nominalization and verbalization lists built from Webster to propose replacements for the content lemmas in a sentence. These resources were used as follows: 1. For each word in the sentence, we determined its lemma(s) and the lemma(s) that verbalize it (if it is a noun) or nominalize it (if it is a verb). 2. We then used WordNet to propose different types of synonyms for the lemmas produced in the first step. These types of synonyms were: synonyms, attributes, perta</context>
<context position="15734" citStr="Zukerman and Raskutti, 2002" startWordPosition="2531" endWordPosition="2534"> that considering hypernyms and hyponyms under the baseline similarity measure exponentially increases the number of alternative paraphrases without improving retrieval performance. Pr SentPara Pr Para (1) Pr Sent Pr Para Sent Pr lem ctxt may be represented by Pr lem lem lem . We consider the baseline measure Pr lem lem lem , and also the following approximation: Pr lem lem lem Pr lem lem (8) where Pr lem lem is obtained directly from the lemma-pair dictionary (Section 4.1). This approximation, although ad hoc, works well in practice, yielding a better performance than bi-gram approximations (Zukerman and Raskutti, 2002). 5 Retrieval Procedures In this section, we describe the retrieval techniques used for our two applications, and present probabilistic formulations that incorporate sentence paraphrasing into our retrieval procedures. 5.1 Document retrieval The retrieval procedure for the DocRet application relies on the vector-space model, which calculates the score of candidate documents given a list of terms in a query (Salton and McGill, 1983). Normally, this score is based on the TF.IDF measure (Term Frequency . Inverse Document Frequency). If we replace a query with its paraphrase, this score is represe</context>
</contexts>
<marker>Zukerman, Raskutti, 2002</marker>
<rawString>Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical query paraphrasing for document retrieval. In COLING’02 – Proceedings of the 19th International Conference on Computational Linguistics, pages 1177– 1183, Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>