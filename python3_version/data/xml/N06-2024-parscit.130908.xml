<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013030">
<title confidence="0.9946695">
NER Systems that Suit User’s Preferences: Adjusting the Recall-Precision
Trade-off for Entity Extraction
</title>
<author confidence="0.967512">
Einat Minkov, Richard C. Wang
</author>
<affiliation confidence="0.929429666666667">
Language Technologies
Institute
Carnegie Mellon University
</affiliation>
<email confidence="0.997894">
einat,rcwang@cs.cmu.edu
</email>
<author confidence="0.994771">
Anthony Tomasic
</author>
<affiliation confidence="0.978154">
Inst. for Software Research
International
Carnegie Mellon University
</affiliation>
<email confidence="0.996935">
tomasic@cs.cmu.edu
</email>
<author confidence="0.876276">
William W. Cohen
</author>
<affiliation confidence="0.9051705">
Machine Learning Dept.
Carnegie Mellon University
</affiliation>
<email confidence="0.996331">
wcohen@cs.cmu.edu
</email>
<sectionHeader confidence="0.995608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995225">
We describe a method based on “tweak-
ing” an existing learned sequential classi-
fier to change the recall-precision tradeoff,
guided by a user-provided performance
criterion. This method is evaluated on
the task of recognizing personal names in
email and newswire text, and proves to be
both simple and effective.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999679818181818">
Named entity recognition (NER) is the task of iden-
tifying named entities in free text—typically per-
sonal names, organizations, gene-protein entities,
and so on. Recently, sequential learning methods,
such as hidden Markov models (HMMs) and con-
ditional random fields (CRFs), have been used suc-
cessfully for a number of applications, including
NER (Sha and Pereira, 2003; Pinto et al., 2003; Mc-
callum and Lee, 2003). In practice, these methods
provide imperfect performance: precision and re-
call, even for well-studied problems on clean well-
written text, reach at most the mid-90’s. While
performance of NER systems is often evaluated in
terms of F1 measure (a harmonic mean of preci-
sion and recall), this measure may not match user
preferences regarding precision and recall. Further-
more, learned NER models may be sub-optimal also
in terms of F1, as they are trained to optimize other
measures (e.g., loglikelihood of the training data for
CRFs).
Obviously, different applications of NER have
different requirements for precision and recall. A
</bodyText>
<page confidence="0.982451">
93
</page>
<bodyText confidence="0.999952264705882">
system might require high precision if it is designed
to extract entities as one stage of fact-extraction,
where facts are stored directly into a database. On
the other hand, a system that generates candidate ex-
tractions which are passed to a semi-automatic cu-
ration system might prefer higher recall. In some
domains, such as anonymization of medical records,
high recall is essential.
One way to manipulate an extractor’s precision-
recall tradeoff is to assign a confidence score to each
extracted entity and then apply a global threshold to
confidence level. However, confidence thresholding
of this sort cannot increase recall. Also, while confi-
dence scores are straightforward to compute in many
classification settings, there is no inherent mecha-
nism for computing confidence of a sequential ex-
tractor. Culotta and McCallum (2004) suggest sev-
eral methods for doing this with CRFs.
In this paper, we suggest an alternative simple
method for exploring and optimizing the relation-
ship between precision and recall for NER systems.
In particular, we describe and evaluate a technique
called “extractor tweaking” that optimizes a learned
extractor with respect to a specific evaluation met-
ric. In a nutshell, we directly tweak the threashold
term that is part of any linear classifier, including se-
quential extractors. Though simple, this approach
has not been empirically evaluated before, to our
knowledge. Further, although sequential extractors
such as HMMs and CRFs are state-of-the-art meth-
ods for tasks like NER, there has been little prior re-
search about tuning these extractors’ performance to
suit user preferences. The suggested algorithm op-
timizes the system performance per a user-provided
</bodyText>
<note confidence="0.6814015">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 93–96,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99976925">
evaluation criterion, using a linear search procedure.
Applying this procedure is not trivial, since the un-
derlying function is not smooth. However, we show
that the system’s precision-recall rate can indeed be
tuned to user preferences given labelled data using
this method. Empirical results are presented for a
particular NER task—recognizing person names, for
three corpora, including email and newswire text.
</bodyText>
<sectionHeader confidence="0.950199" genericHeader="method">
2 Extractor tweaking
</sectionHeader>
<bodyText confidence="0.9999625">
Learning methods such as VP-HMM and CRFs op-
timize criteria such as margin separation (implicitly
maximized by VP-HMMs) or log-likelihood (ex-
plicitly maximized by CRFs), which are at best indi-
rectly related to precision and recall. Can such learn-
ing methods be modified to more directly reward a
user-provided performance metric?
In a non-sequential classifier, a threshold on confi-
dence can be set to alter the precision-recall tradeoff.
This is nontrivial to do for VP-HMMs and CRFs.
Both learners use dynamic programming to find the
label sequence y = (y1, ... , yi, ... , yN) for a word
sequence x = (x1,... , xi, ... , xN) that maximizes
the function W · Ei f(x, i, yi−1, yi) , where W is
the learned weight vector and f is a vector of fea-
tures computed from x, i, the label yi for xi, and the
previous label yi−1. Dynamic programming finds
the most likely state sequence, and does not output
probability for a particular sub-sequence. (Culotta
and McCallum, 2004) suggest several ways to gen-
erate confidence estimation in this framework. We
propose a simpler approach for directly manipulat-
ing the learned extractor’s precision-recall ratio.
We will assume that the labels y include one label
O for “outside any named entity”, and let w0 be the
weight for the feature f0, defined as follows:
</bodyText>
<equation confidence="0.965889">
�
1 if yi = O
f0(x, i, yi−1, yi) ≡ 0 else
</equation>
<bodyText confidence="0.999749333333333">
If no such feature exists, then we will create one.
The NER based on W will be sensitive to the value
of w0: large negative values will force the dynamic
programming method to label tokens as inside enti-
ties, and large positive values will force it to label
fewer entities1.
</bodyText>
<footnote confidence="0.700388">
1We clarify that w0 will refer to feature f0 only, and not to
other features that may incorporate label information.
</footnote>
<bodyText confidence="0.9999821875">
We thus propose to “tweak” a learned NER by
varying the single parameter w0 systematically so as
to optimize some user-provided performance metric.
Specifically, we tune w0 using a a Gauss-Newton
line search, where the objective function is itera-
tively approximated by quadratics.2 We terminate
the search when two adjacent evaluation results are
within a 0.01% difference3.
A variety of performance metrics might be imag-
ined: for instance, one might wish to optimize re-
call, after applying some sort of penalty for pre-
cision below some fixed threshold. In this paper
we will experiment with performance metrics based
on the (complete) F-measure formula, which com-
bines precision and recall into a single numeric value
based on a user-provided parameter β:
</bodyText>
<equation confidence="0.962484">
Q2P + R
</equation>
<bodyText confidence="0.999976230769231">
A value of β &gt; 1 assigns higher importance to re-
call. In particular, F2 weights recall twice as much
as precision. Similarly, F0.5 weights precision twice
as much as recall.
We consider optimizing both token- and entity-
level Fβ – awarding partial credit for partially ex-
tracted entities and no credit for incorrect entity
boundaries, respectively. Performance is optimized
over the dataset on which W was trained, and tested
on a separate set. A key question our evaluation
should address is whether the values optimized for
the training examples transfer well to unseen test ex-
amples, using the suggested approximate procedure.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999188">
3.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999827888888889">
We experiment with three datasets, of both email
and newswire text. Table 1 gives summary statis-
tics for all datasets. The widely-used MUC-6 dataset
includes news articles drawn from the Wall Street
Journal. The Enron dataset is a collection of emails
extracted from the Enron corpus (Klimt and Yang,
2004), where we use a subcollection of the mes-
sages located in folders named “meetings” or “cal-
endar”. The Mgmt-Groups dataset is a second email
</bodyText>
<footnote confidence="0.9966435">
2from http://billharlan.com/pub/code/inv.
3In the experiments, this is usually within around 10 itera-
tions. Each iteration requires evaluating a “tweaked” extractor
on a training set.
</footnote>
<equation confidence="0.793271">
F(Q, P, R) = (Q2 + 1)PR
</equation>
<page confidence="0.967749">
94
</page>
<figure confidence="0.99713895">
0.5
1.0
0.2
2.0
5.0
F(Beta)
90
85
80
75
70
65
60
55
50
0.2
0.5
1.0
2.0
5.0
</figure>
<bodyText confidence="0.999897125">
collection, extracted from the CSpace email cor-
pus, which contains email messages sent by MBA
students taking a management course conducted at
Carnegie Mellon University in 1997. This data was
split such that its test set contains a different mix of
entity names comparing to training exmaples. Fur-
ther details about these datasets are available else-
where (Minkov et al., 2005).
</bodyText>
<table confidence="0.962003571428571">
Beta
# documents # tokens # names
per doc.
Train Test
MUC-6 347 30 204,071 6.8
Enron 833 143 204,423 3.0
Mgmt-Groups 631 128 104,662 3.7
</table>
<tableCaption confidence="0.999918">
Table 1: Summary of the corpora used in the experiments
</tableCaption>
<bodyText confidence="0.9999724">
We used an implementation of Collins’ voted-
percepton method for discriminatively training
HMMs (henceforth, VP-HMM) (Collins, 2002) as
well as CRF (Lafferty et al., 2001) to learn a NER.
Both VP-HMM and CRF were trained for 20 epochs
on every dataset, using a simple set of features such
as word identity and capitalization patterns for a
window of three words around each word being clas-
sified. Each word is classified as either inside or out-
side a person name.4
</bodyText>
<subsectionHeader confidence="0.997991">
3.2 Extractor tweaking Results
</subsectionHeader>
<bodyText confidence="0.999507333333333">
Figure 1 evaluates the effectiveness of the optimiza-
tion process used by “extractor tweaking” on the
Enron dataset. We optimized models for Fβ with
different values of Q, and also evaluated each op-
timized model with different Fβ metrics. The top
graph shows the results for token-level Fβ, and the
bottom graph shows entity-level Fβ behavior. The
graph illustates that the optimized model does in-
deed roughly maximize performance for the target
Q value: for example, the token-level Fβ curve for
the model optimized for Q = 0.5 indeed peaks at
Q = 0.5 on the test set data. The optimization is
only roughly accurate5 for several possible reasons:
first, there are differences between train and test sets;
in addition, the line search assumes that the perfor-
mance metric is smooth and convex, which need
not be true. Note that evaluation-metric optimiza-
tion is less successful for entity-level performance,
</bodyText>
<footnote confidence="0.796385">
4This problem encoding is basic. However, in the context of
this paper we focus on precision-recall trade-off in the general
case, avoiding settings’ optimization.
5E.g, the token-level F2 curve peaks at β = 5.
</footnote>
<figure confidence="0.467508">
Beta
</figure>
<figureCaption confidence="0.9961335">
Figure 1: Results of token-level (top) and entity-level (bot-
tom) optimization for varying values of β, for the Enron dataset,
VP-HMM. The y-axis gives F in terms of β. β (x-axis) is given
in a logarithmic scale.
</figureCaption>
<bodyText confidence="0.765803">
which behaves less smoothly than token-level per-
formance.
</bodyText>
<table confidence="0.999366375">
β Token Entity
Prec Recall Prec Recall
Baseline 93.3 76.0 93.6 70.6
0.2 100 53.2 98.2 57.0
0.5 95.3 71.1 94.4 67.9
1.0 88.6 79.4 89.2 70.9
2.0 81.0 83.9 81.8 70.9
5.0 65.8 91.3 69.4 71.4
</table>
<tableCaption confidence="0.9677185">
Table 2: Sample optimized CRF results, for the MUC-6
dataset and entity-level optimization.
</tableCaption>
<bodyText confidence="0.999869833333333">
Similar results were obtained optimizing baseline
CRF classifiers. Sample results (for MUC-6 only,
due to space limitations) are given in Table 2, opti-
mizing a CRF baseline for entity-level Fβ. Note that
as Q increases, recall monotonically increases and
precision monotonically falls.
The graphs in Figure 2 present another set of re-
sults with a more traditional recall-precision curves.
The top three graphs are for token-level Fβ opti-
mization, and the bottom three are for entity-level
optimization. The solid lines show the token-level
and entity-level precision-recall tradeoff obtained by
</bodyText>
<figure confidence="0.994890413333333">
0.2 0.5 1.0 2.0 5.0
F(Beta)
90
85
80
75
70
65
60
55
50
0.2
0.5
1.0
2.0
5.0
95
MUC-6 Enron M.Groups
50 60 70 80 90 100
Recall
50 60 70 80 90 100
Recall
100
90
80
70
60
50
100
90
80
70
60
50
50 60 70 80 90 100
Recall
50 60 70 80 90 100
Recall
80
50
100
90
80
70
60
50
100
90
70
60
Token-level
Entity-level
Token-level baseline
Entity-level baseline
50 60 70 80 90 100
Recall
50 60 70 80 90 100
Recall
Precision
100
90
80
70
60
50
Token-level
Entity-level
Token-level baseline
Entity-level baseline
Precision 100
90
80
70
60
50
</figure>
<figureCaption confidence="0.889991666666667">
Figure 2: Results for the evaluation-metric model optimization. The top three graphs are for token-level F(Q) optimization,
and the bottom three are for entity-level optimization. Each graph shows the baseline learned VP-HMM and evaluation-metric
optimization for different values of Q, in terms of both token-level and entity-level performance.
</figureCaption>
<bodyText confidence="0.999839444444444">
varying6 Q and optimizing the relevant measure for
Fβ; the points labeled “baseline” show the precision
and recall in token and entity level of the baseline
model, learned by VP-HMM. These graphs demon-
strate that extractor “tweaking” gives approximately
smooth precision-recall curves, as desired. Again,
we note that the resulting recall-precision trade-
off for entity-level optimization is generally less
smooth.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999295">
We described an approach that is based on mod-
ifying an existing learned sequential classifier to
change the recall-precision tradeoff, guided by a
user-provided performance criterion. This approach
not only allows one to explore a recall-precision
tradeoff, but actually allows the user to specify a
performance metric to optimize, and optimizes a
learned NER system for that metric. We showed
that using a single free parameter and a Gauss-
Newton line search (where the objective is itera-
tively approximated by quadratics), effectively op-
timizes two plausible performance measures, token-
</bodyText>
<footnote confidence="0.972494">
6We varied Q over the values 0.2, 0.5, 0.8, 1, 1.2, 1.5, 2, 3
and 5
</footnote>
<bodyText confidence="0.999929">
level Fβ and entity-level Fβ. This approach is in
fact general, as it is applicable for sequential and/or
structured learning applications other than NER.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993905">
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In EMNLP.
A. Culotta and A. McCallum. 2004. Confidence estimation for
information extraction. In HLT-NAACL.
B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In
CEAS.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML.
A. Mccallum and W. Lee. 2003. early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In CONLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Extracting
personal names from emails: Applying named entity recog-
nition to informal text. In HLT-EMNLP.
D. Pinto, A. Mccallum, X. Wei, and W. B. Croft. 2003. table
extraction using conditional random fields. In ACM SIGIR.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In HLT-NAACL.
</reference>
<page confidence="0.998463">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.166823">
<title confidence="0.984514">NER Systems that Suit User’s Preferences: Adjusting the Trade-off for Entity Extraction</title>
<author confidence="0.629715">Einat Minkov</author>
<author confidence="0.629715">C Richard</author>
<affiliation confidence="0.545776">Language Carnegie Mellon</affiliation>
<email confidence="0.998453">einat,rcwang@cs.cmu.edu</email>
<author confidence="0.704908">Anthony</author>
<affiliation confidence="0.980969">Inst. for Software Carnegie Mellon</affiliation>
<email confidence="0.998592">tomasic@cs.cmu.edu</email>
<author confidence="0.925707">W William</author>
<affiliation confidence="0.7983035">Machine Learning Carnegie Mellon</affiliation>
<email confidence="0.999315">wcohen@cs.cmu.edu</email>
<abstract confidence="0.999369">We describe a method based on “tweaking” an existing learned sequential classifier to change the recall-precision tradeoff, guided by a user-provided performance criterion. This method is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8709" citStr="Collins, 2002" startWordPosition="1391" endWordPosition="1392"> MBA students taking a management course conducted at Carnegie Mellon University in 1997. This data was split such that its test set contains a different mix of entity names comparing to training exmaples. Further details about these datasets are available elsewhere (Minkov et al., 2005). Beta # documents # tokens # names per doc. Train Test MUC-6 347 30 204,071 6.8 Enron 833 143 204,423 3.0 Mgmt-Groups 631 128 104,662 3.7 Table 1: Summary of the corpora used in the experiments We used an implementation of Collins’ votedpercepton method for discriminatively training HMMs (henceforth, VP-HMM) (Collins, 2002) as well as CRF (Lafferty et al., 2001) to learn a NER. Both VP-HMM and CRF were trained for 20 epochs on every dataset, using a simple set of features such as word identity and capitalization patterns for a window of three words around each word being classified. Each word is classified as either inside or outside a person name.4 3.2 Extractor tweaking Results Figure 1 evaluates the effectiveness of the optimization process used by “extractor tweaking” on the Enron dataset. We optimized models for Fβ with different values of Q, and also evaluated each optimized model with different Fβ metrics</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>A McCallum</author>
</authors>
<title>Confidence estimation for information extraction.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="2625" citStr="Culotta and McCallum (2004)" startWordPosition="389" endWordPosition="392"> extractions which are passed to a semi-automatic curation system might prefer higher recall. In some domains, such as anonymization of medical records, high recall is essential. One way to manipulate an extractor’s precisionrecall tradeoff is to assign a confidence score to each extracted entity and then apply a global threshold to confidence level. However, confidence thresholding of this sort cannot increase recall. Also, while confidence scores are straightforward to compute in many classification settings, there is no inherent mechanism for computing confidence of a sequential extractor. Culotta and McCallum (2004) suggest several methods for doing this with CRFs. In this paper, we suggest an alternative simple method for exploring and optimizing the relationship between precision and recall for NER systems. In particular, we describe and evaluate a technique called “extractor tweaking” that optimizes a learned extractor with respect to a specific evaluation metric. In a nutshell, we directly tweak the threashold term that is part of any linear classifier, including sequential extractors. Though simple, this approach has not been empirically evaluated before, to our knowledge. Further, although sequenti</context>
<context position="5077" citStr="Culotta and McCallum, 2004" startWordPosition="782" endWordPosition="785">uential classifier, a threshold on confidence can be set to alter the precision-recall tradeoff. This is nontrivial to do for VP-HMMs and CRFs. Both learners use dynamic programming to find the label sequence y = (y1, ... , yi, ... , yN) for a word sequence x = (x1,... , xi, ... , xN) that maximizes the function W · Ei f(x, i, yi−1, yi) , where W is the learned weight vector and f is a vector of features computed from x, i, the label yi for xi, and the previous label yi−1. Dynamic programming finds the most likely state sequence, and does not output probability for a particular sub-sequence. (Culotta and McCallum, 2004) suggest several ways to generate confidence estimation in this framework. We propose a simpler approach for directly manipulating the learned extractor’s precision-recall ratio. We will assume that the labels y include one label O for “outside any named entity”, and let w0 be the weight for the feature f0, defined as follows: � 1 if yi = O f0(x, i, yi−1, yi) ≡ 0 else If no such feature exists, then we will create one. The NER based on W will be sensitive to the value of w0: large negative values will force the dynamic programming method to label tokens as inside entities, and large positive v</context>
</contexts>
<marker>Culotta, McCallum, 2004</marker>
<rawString>A. Culotta and A. McCallum. 2004. Confidence estimation for information extraction. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Klimt</author>
<author>Y Yang</author>
</authors>
<title>Introducing the Enron corpus.</title>
<date>2004</date>
<booktitle>In CEAS.</booktitle>
<contexts>
<context position="7580" citStr="Klimt and Yang, 2004" startWordPosition="1199" endWordPosition="1202">mance is optimized over the dataset on which W was trained, and tested on a separate set. A key question our evaluation should address is whether the values optimized for the training examples transfer well to unseen test examples, using the suggested approximate procedure. 3 Experiments 3.1 Experimental Settings We experiment with three datasets, of both email and newswire text. Table 1 gives summary statistics for all datasets. The widely-used MUC-6 dataset includes news articles drawn from the Wall Street Journal. The Enron dataset is a collection of emails extracted from the Enron corpus (Klimt and Yang, 2004), where we use a subcollection of the messages located in folders named “meetings” or “calendar”. The Mgmt-Groups dataset is a second email 2from http://billharlan.com/pub/code/inv. 3In the experiments, this is usually within around 10 iterations. Each iteration requires evaluating a “tweaked” extractor on a training set. F(Q, P, R) = (Q2 + 1)PR 94 0.5 1.0 0.2 2.0 5.0 F(Beta) 90 85 80 75 70 65 60 55 50 0.2 0.5 1.0 2.0 5.0 collection, extracted from the CSpace email corpus, which contains email messages sent by MBA students taking a management course conducted at Carnegie Mellon University in 1</context>
</contexts>
<marker>Klimt, Yang, 2004</marker>
<rawString>B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In CEAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8748" citStr="Lafferty et al., 2001" startWordPosition="1397" endWordPosition="1400">nt course conducted at Carnegie Mellon University in 1997. This data was split such that its test set contains a different mix of entity names comparing to training exmaples. Further details about these datasets are available elsewhere (Minkov et al., 2005). Beta # documents # tokens # names per doc. Train Test MUC-6 347 30 204,071 6.8 Enron 833 143 204,423 3.0 Mgmt-Groups 631 128 104,662 3.7 Table 1: Summary of the corpora used in the experiments We used an implementation of Collins’ votedpercepton method for discriminatively training HMMs (henceforth, VP-HMM) (Collins, 2002) as well as CRF (Lafferty et al., 2001) to learn a NER. Both VP-HMM and CRF were trained for 20 epochs on every dataset, using a simple set of features such as word identity and capitalization patterns for a window of three words around each word being classified. Each word is classified as either inside or outside a person name.4 3.2 Extractor tweaking Results Figure 1 evaluates the effectiveness of the optimization process used by “extractor tweaking” on the Enron dataset. We optimized models for Fβ with different values of Q, and also evaluated each optimized model with different Fβ metrics. The top graph shows the results for t</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mccallum</author>
<author>W Lee</author>
</authors>
<title>early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In CONLL.</booktitle>
<contexts>
<context position="1156" citStr="Mccallum and Lee, 2003" startWordPosition="158" endWordPosition="162">deoff, guided by a user-provided performance criterion. This method is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective. 1 Introduction Named entity recognition (NER) is the task of identifying named entities in free text—typically personal names, organizations, gene-protein entities, and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used successfully for a number of applications, including NER (Sha and Pereira, 2003; Pinto et al., 2003; Mccallum and Lee, 2003). In practice, these methods provide imperfect performance: precision and recall, even for well-studied problems on clean wellwritten text, reach at most the mid-90’s. While performance of NER systems is often evaluated in terms of F1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall. Furthermore, learned NER models may be sub-optimal also in terms of F1, as they are trained to optimize other measures (e.g., loglikelihood of the training data for CRFs). Obviously, different applications of NER have different requiremen</context>
</contexts>
<marker>Mccallum, Lee, 2003</marker>
<rawString>A. Mccallum and W. Lee. 2003. early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Minkov</author>
<author>R C Wang</author>
<author>W W Cohen</author>
</authors>
<title>Extracting personal names from emails: Applying named entity recognition to informal text.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP.</booktitle>
<contexts>
<context position="8383" citStr="Minkov et al., 2005" startWordPosition="1336" endWordPosition="1339">In the experiments, this is usually within around 10 iterations. Each iteration requires evaluating a “tweaked” extractor on a training set. F(Q, P, R) = (Q2 + 1)PR 94 0.5 1.0 0.2 2.0 5.0 F(Beta) 90 85 80 75 70 65 60 55 50 0.2 0.5 1.0 2.0 5.0 collection, extracted from the CSpace email corpus, which contains email messages sent by MBA students taking a management course conducted at Carnegie Mellon University in 1997. This data was split such that its test set contains a different mix of entity names comparing to training exmaples. Further details about these datasets are available elsewhere (Minkov et al., 2005). Beta # documents # tokens # names per doc. Train Test MUC-6 347 30 204,071 6.8 Enron 833 143 204,423 3.0 Mgmt-Groups 631 128 104,662 3.7 Table 1: Summary of the corpora used in the experiments We used an implementation of Collins’ votedpercepton method for discriminatively training HMMs (henceforth, VP-HMM) (Collins, 2002) as well as CRF (Lafferty et al., 2001) to learn a NER. Both VP-HMM and CRF were trained for 20 epochs on every dataset, using a simple set of features such as word identity and capitalization patterns for a window of three words around each word being classified. Each word</context>
</contexts>
<marker>Minkov, Wang, Cohen, 2005</marker>
<rawString>E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Extracting personal names from emails: Applying named entity recognition to informal text. In HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pinto</author>
<author>A Mccallum</author>
<author>X Wei</author>
<author>W B Croft</author>
</authors>
<title>table extraction using conditional random fields.</title>
<date>2003</date>
<booktitle>In ACM SIGIR.</booktitle>
<contexts>
<context position="1131" citStr="Pinto et al., 2003" startWordPosition="154" endWordPosition="157">recall-precision tradeoff, guided by a user-provided performance criterion. This method is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective. 1 Introduction Named entity recognition (NER) is the task of identifying named entities in free text—typically personal names, organizations, gene-protein entities, and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used successfully for a number of applications, including NER (Sha and Pereira, 2003; Pinto et al., 2003; Mccallum and Lee, 2003). In practice, these methods provide imperfect performance: precision and recall, even for well-studied problems on clean wellwritten text, reach at most the mid-90’s. While performance of NER systems is often evaluated in terms of F1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall. Furthermore, learned NER models may be sub-optimal also in terms of F1, as they are trained to optimize other measures (e.g., loglikelihood of the training data for CRFs). Obviously, different applications of NER </context>
</contexts>
<marker>Pinto, Mccallum, Wei, Croft, 2003</marker>
<rawString>D. Pinto, A. Mccallum, X. Wei, and W. B. Croft. 2003. table extraction using conditional random fields. In ACM SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="1111" citStr="Sha and Pereira, 2003" startWordPosition="150" endWordPosition="153">assifier to change the recall-precision tradeoff, guided by a user-provided performance criterion. This method is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective. 1 Introduction Named entity recognition (NER) is the task of identifying named entities in free text—typically personal names, organizations, gene-protein entities, and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used successfully for a number of applications, including NER (Sha and Pereira, 2003; Pinto et al., 2003; Mccallum and Lee, 2003). In practice, these methods provide imperfect performance: precision and recall, even for well-studied problems on clean wellwritten text, reach at most the mid-90’s. While performance of NER systems is often evaluated in terms of F1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall. Furthermore, learned NER models may be sub-optimal also in terms of F1, as they are trained to optimize other measures (e.g., loglikelihood of the training data for CRFs). Obviously, different </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>