<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013650">
<title confidence="0.996952">
Using N-gram based Features for Machine Translation
System Combination
</title>
<author confidence="0.99989">
Yong Zhao1 Xiaodong He
</author>
<affiliation confidence="0.999491">
Georgia Institute of Technology Microsoft Research
</affiliation>
<address confidence="0.890369">
Atlanta, GA 30332, USA Redmond, WA 98052, USA
</address>
<email confidence="0.998633">
yongzhao@gatech.edu xiaohe@microsoft.com
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999337">
Conventional confusion network based
system combination for machine translation
(MT) heavily relies on features that are
based on the measure of agreement of
words in different translation hypotheses.
This paper presents two new features that
consider agreement of n-grams in different
hypotheses to improve the performance of
system combination. The first one is based
on a sentence specific online n-gram
language model, and the second one is
based on n-gram voting. Experiments on a
large scale Chinese-to-English MT task
show that both features yield significant
improvements on the translation
performance, and a combination of them
produces even better translation results.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983866666667">
In past years, the confusion network based system
combination approach has been shown with
substantial improvements in various machine
translation (MT) tasks (Bangalore, et. al., 2001,
Matusov, et. al., 2006, Rosti, et. al., 2007, He,
et. al., 2008). Given hypotheses of multiple
systems, a confusion network is built by aligning
all these hypotheses. The resulting network
comprises a sequence of correspondence sets, each
of which contains the alternative words that are
aligned with each other. To derive a consensus
hypothesis from the confusion network, decoding
is performed by selecting a path with the maximum
overall confidence score among all paths that pass
the confusion network (Goel, et. al., 2004).
</bodyText>
<footnote confidence="0.968014">
1 The work was performed when Yong Zhao was an intern at
Microsoft Research
</footnote>
<bodyText confidence="0.999827894736842">
The confidence score of a hypothesis could be
assigned in various ways. Fiscus (1997) used
voting by frequency of word occurrences. Mangu
et. al., (2000) computed a word posterior
probability based on voting of that word in
different hypotheses. Moreover, the overall
confidence score is usually formulated as a log-
linear model including extra features including
language model (LM) score, word count, etc.
Features based on word agreement measure are
extensively studied in past work (Matusov, et. al.,
2006, Rosti, et. al., 2007, He, et. al., 2008).
However, utilization of n-gram agreement
information among the hypotheses has not been
fully explored yet. Moreover, it was argued that
the confusion network decoding may introduce
undesirable spur words that break coherent
phrases (Sim, et. al., 2007). Therefore, we would
prefer the consensus translation that has better n-
gram agreement among outputs of single systems.
In the literature, Zens and Ney (2004)
proposed an n-gram posterior probability based
LM for MT. For each source sentence, a LM is
trained on the n-best list produced by a single MT
system and is used to re-rank that n-best list itself.
On the other hand, Matusov et al. (2008) proposed
an “adapted” LM for system combination, where
this “adapted” LM is trained on translation
hypotheses of the whole test corpus from all single
MT systems involved in system combination.
Inspired by these ideas, we propose two new
features based on n-gram agreement measure to
improve the performance of system combination.
The first one is a sentence specific LM built on
translation hypotheses of multiple systems; the
second one is n-gram-voting-based confidence.
Experimental results are presented in the context of
a large-scale Chinese-English translation task.
</bodyText>
<page confidence="0.984521">
205
</page>
<note confidence="0.5414505">
Proceedings of NAACL HLT 2009: Short Papers, pages 205–208,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.803031" genericHeader="method">
2 System Combination for MT
</sectionHeader>
<bodyText confidence="0.99949768">
One of the most successful approaches for
system combination for MT is based on
confusion network decoding as described in
(Rosti, et. al., 2007). Given translation
hypotheses from multiple MT systems, one of
the hypotheses is selected as the backbone for
the use of hypothesis alignment. This is usually
done by a sentence-level minimum Bayes risk
(MBR) re-ranking method. The confusion
network is constructed by aligning all these
hypotheses against the backbone. Words that
align to each other are grouped into a
correspondence set, constituting competition
links of the confusion network. Each path in the
network passes exactly one link from each
correspondence set. The final consensus output
relies on a decoding procedure that chooses a
path with the maximum confidence score among
all paths that pass the confusion network.
The confidence score of a hypothesis is
usually formalized as a log-linear sum of several
feature functions. Given a source language
sentence 𝐹 , the total confidence of a target
language hypothesis 𝐸 = (𝑒1, ..., 𝑒𝐿) in the
confusion network can be represented as:
</bodyText>
<equation confidence="0.9362022">
𝐿
𝑙𝑜𝑔𝑃 𝐸 𝐹 = 𝑙𝑜𝑔 𝑃 𝑒𝑙 𝑙, 𝐹
𝑙=1
+ 𝜆1𝑙𝑜𝑔𝑃𝐿𝑀 𝐸 (1)
+ 𝜆2𝑁𝑤𝑜𝑟𝑑𝑠 (𝐸)
</equation>
<bodyText confidence="0.999901">
where the feature functions include word
posterior probability 𝑃(𝑒𝑙 |𝑙, 𝐹), LM probability
𝑃𝐿𝑀 (𝐸), and the number of real words 𝑁𝑤𝑜𝑟𝑑𝑠 in
𝐸. Usually, the model parameter λj could be
trained by optimizing an evaluation metric, e.g.,
BLEU score, on a held-out development set.
</bodyText>
<sectionHeader confidence="0.990511" genericHeader="method">
3 N-gram Online Language Model
</sectionHeader>
<bodyText confidence="0.990791333333333">
Given a source sentence 𝐹, the fractional count
𝐶 𝑒1𝑛 𝐹 of an n-gram 𝑒1𝑛 is defined as:
where 𝑬ℎ denotes the hypothesis set, 𝛿 ∙,∙
denotes the Kronecker function, and 𝑃(𝐸′|𝐹) is
the posterior probability of translation
hypothesis 𝐸′ , which is expressed as the
weighted sum of the system specific posterior
probabilities through the systems that contains
hypothesis 𝐸′,
</bodyText>
<equation confidence="0.992925333333333">
𝐾
𝑃 𝐸 𝐹 = 𝑤𝑘𝑃(𝐸 𝑆𝑘,𝐹 1(𝐸 ∈ 𝑬𝑆𝑘) (3)
𝑘=1
</equation>
<bodyText confidence="0.991516125">
where 𝑤𝑘 is the weight for the posterior
probability of the e system 𝑆𝑘, and 1 ∙ is the
indicator function.
Follows Rosti, et. al. (2007), system specific
posteriors are derived based on a rank-based
scoring scheme. I.e., if translation hypothesis 𝐸𝑟
is the rrh best output in the n-best list of system
𝑆𝑘, posterior 𝑃 𝐸𝑟 𝑆𝑘, 𝐹 is approximated as:
</bodyText>
<equation confidence="0.971253">
𝑃 𝐸𝑟 𝑆𝑘, 𝐹 = 1/(1 + 𝑟)𝜂 (4)
𝑟′ =1
||𝑬𝑆𝑘 ||1/(1 + 𝑟′)𝜂
</equation>
<bodyText confidence="0.997217666666667">
where η is a rank smoothing parameter.
Similar to (Zens and Ney, 2004), a
straightforward approach of using n-gram
fractional counts is to formulate it as a sentence
specific online LM. Then the online LM score
of a path in the confusion network will be added
as an additional feature in the log-linear model
for decoding. The online n-gram LM score is
computed by:
</bodyText>
<equation confidence="0.9696156">
𝑙 |𝐹)
𝑃(𝑒𝑙|𝑒𝑙−𝑛+1
𝑙−1 , 𝐹) = 𝐶(𝑒𝑙−𝑛+1 (5)
𝐶(𝑒𝑙−𝑛+1
𝑙−1 |𝐹)
The LM score of hypothesis 𝐸 is obtained by:
𝐿
𝑃𝐿𝑀 𝐸 𝐹 = 𝑃 𝑒𝑙|𝑒𝑙−𝑛+1
𝑙−1 , 𝐹 (6)
𝑙=𝑛
</equation>
<bodyText confidence="0.999957333333333">
Since new n-grams unseen in original
translation hypotheses may be proposed by the
CN decoder, LM smoothing is critical. In our
approach, the score of the online LM is
smoothed by taking a linear interpolation to
combine scores of different orders.
</bodyText>
<equation confidence="0.962317083333333">
𝐿
𝐶 𝑒1 𝑛 𝐹 = 𝑃 𝐸′ 𝐹
𝐸′ ∈𝑬ℎ 𝑙=𝑛
𝛿(𝑒′ 𝑙−𝑛+1
𝑙 , 𝑒1𝑛) (2)
206
𝑙−1
𝑃𝑠𝑚𝑜𝑜𝑡 ℎ 𝑒𝑙  |𝑒𝑙−𝑛+1 , 𝐹
𝑛
= 𝛼𝑚𝑃(𝑒𝑙|𝑒𝑙−𝑚+1
𝑙−1 , 𝐹)
𝑚=1
</equation>
<bodyText confidence="0.9994305">
In our implementation, the interpolation weights
{ 𝛼𝑚 } can be learned along with other
combination parameters in the same Max-BLEU
training scheme via Powell&apos;s search.
</bodyText>
<sectionHeader confidence="0.992576" genericHeader="method">
4 N-gram-Voting-Based Confidence
</sectionHeader>
<bodyText confidence="0.50810625">
Motivated by features based on voting of single
word, we proposed new features based on N-
gram voting. The voting score 𝑉 𝑒1𝑛 𝐹 of an n-
gram 𝑒1𝑛 is computed as:
</bodyText>
<equation confidence="0.999489">
𝑉 𝑒1𝑛 𝐹 = 𝑃 𝐸′ 𝐹 1(𝑒1 𝑛 ∈ 𝐸′)
𝐸′ ∈𝑬ℎ (8)
</equation>
<bodyText confidence="0.998295181818182">
It receives a vote from each hypothesis that
contains that n-gram, and weighted by the
posterior probability of that hypothesis, where
the posterior probability 𝑃 𝐸′ 𝐹 is computed by
(3). Unlike the fractional count, each hypothesis
can vote no more than once on an n-gram.
𝑉 𝑒1𝑛 𝐹 takes a value between 0 and 1. It
can be viewed as the confidence of the n-gram
𝑒1𝑛 . Then the n-gram-voting-based confidence
score of a hypothesis 𝐸 is computed as the
product of confidence scores of n-grams in E:
</bodyText>
<equation confidence="0.9962925">
𝑃𝑁𝑉,𝑛 𝐸 𝐹 = 𝑃𝑁𝑉,𝑛 𝑒1𝑙 𝑙, 𝐹 =
𝑙−𝑛+1 (9)
𝑉(𝑒𝑚 𝑚+𝑛−1|𝐹)
𝑚=1
</equation>
<bodyText confidence="0.99996">
where n can take the value of 2, 3, ..., N. In
order to prevent zero confidence, a small back-
off confidence score is assigned to all n-grams
unseen in original hypotheses.
Augmented with the proposed n-gram based
features, the final log-linear model becomes:
</bodyText>
<equation confidence="0.996790142857143">
𝑙𝑜𝑔𝑃 𝐸 𝐹
𝐿
= 𝑙𝑜𝑔 𝑃 𝑒𝑙 𝑙, 𝐹 + 𝜆1𝑙𝑜𝑔𝑃𝐿𝑀 𝐸
𝑙=1
+ 𝜆2𝑁𝑤𝑜𝑟𝑑𝑠 𝐸 + 𝜆3𝑙𝑜𝑔𝑃𝐿𝑀 𝐸 𝐹 (10)
𝑁
+ 𝜆𝑛+2𝑙𝑜𝑔𝑃𝑁𝑉,𝑛 𝐸 𝐹
</equation>
<page confidence="0.691315">
𝑛=2
</page>
<sectionHeader confidence="0.994992" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999926632653061">
We evaluate the proposed n-gram based features
on the Chinese-to-English (C2E) test in the past
NIST Open MT Evaluations. The experimental
results are reported in case sensitive BLEU
score (Papineni, et. al., 2002).
The dev set, which is used for system
combination parameter training, is the newswire
and newsgroup parts of NIST MT06, which
contains a total of 1099 sentences. The test set is
the &amp;quot;current&amp;quot; test set of NIST MT08, which
contains 1357 sentences of newswire and web-
blog data. Both dev and test sets have four
reference translations per sentence.
Outputs from a total of eight single MT
systems were combined for consensus
translations. These selected systems are based
on various translation paradigms, such as
phrasal, hierarchical, and syntax-based systems.
Each system produces 10-best hypotheses per
translation. The BLEU score range for the eight
individual systems are from 26.11% to 31.09%
on the dev set and from 20.42% to 26.24% on
the test set. In our experiments, a state-of-the-art
system combination method proposed by He, et.
al. (2008) is implemented as the baseline. The
true-casing model proposed by Toutanova et al.
(2008) is used.
Table 1 shows results of adding the online
LM feature. Different LM orders up to four are
tested. Results show that using a 2-gram online
LM yields a half BLEU point gain over the
baseline. However, the gain is saturated after a
LM order of three, and fluctuates after that.
Table 2 shows the performance of using n-
gram-voting-based confidence features. The best
result of 31.01% is achieved when up to 4-gram
confidence features are used. The BLEU score
keeps improving when longer n-gram
confidence features are added. This indicates
that the n-gram voting based confidence feature
is robust to high order n-grams.
We further experimented with incorporating
both features in the log-linear model and
reported the results in Table 3. Given the
observation that the n-gram voting based
confidence feature is more robust to high order
n-grams, we further tested using different n-
gram orders for them. As shown in Table 3,
using 3-gram online LM plus 2~4-gram voting
</bodyText>
<equation confidence="0.702586">
(7)
</equation>
<page confidence="0.991063">
207
</page>
<bodyText confidence="0.9980492">
based confidence scores yields the best BLEU
scores on both dev and test sets, which are
37.98% and 31.35%, respectively. This is a 0.84
BLEU point gain over the baseline on the MT08
test set.
</bodyText>
<tableCaption confidence="0.99958">
Table 1: Results of adding the n-gram online LM.
</tableCaption>
<table confidence="0.999716666666667">
BLEU % Dev Test
Baseline 37.34 30.51
1-gram online LM 37.34 30.51
2-gram online LM 37.86 31.02
3-gram online LM 37.87 31.08
4-gram online LM 37.86 31.01
</table>
<tableCaption confidence="0.992615">
Table 2: Results of adding n-gram voting based
confidence features.
</tableCaption>
<table confidence="0.9995384">
BLEU % Dev Test
Baseline 37.34 30.51
+ 2-gram voting 37.58 30.88
+ 2~3-gram voting 37.66 30.96
+ 2~4-gram voting 37.77 31.01
</table>
<tableCaption confidence="0.997184">
Table 3: Results of using both n-gram online LM
and n-gram voting based confidence features
</tableCaption>
<table confidence="0.999044666666667">
BLEU % Dev Test
Baseline 37.34 30.51
2-gram LM + 2-gram voting 37.78 30.98
3-gram LM + 2~3-gram voting 37.89 31.21
4-gram LM + 2~4-gram voting 37.93 31.08
3-gram LM + 2~4-gram voting 37.98 31.35
</table>
<sectionHeader confidence="0.998395" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999918705882353">
This work explored utilization of n-gram
agreement information among translation
outputs of multiple MT systems to improve the
performance of system combination. This is an
extension of an earlier idea presented at the
NIPS 2008 Workshop on Speech and Language
(Yong and He 2008). Two kinds of n-gram based
features were proposed. The first is based on an
online LM using n-gram fractional counts, and
the second is a confidence feature based on n-
gram voting scores. Our experiments on the
NIST MT08 Chinese-English task showed that
both methods yield nice improvements on the
translation results, and incorporating both kinds
of features produced the best translation result
with a BLEU score of 31.35%, which is a 0.84%
improvement.
</bodyText>
<sectionHeader confidence="0.989962" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999308294117647">
J.G. Fiscus, 1997. A post-processing system to yield
reduced word error rates: Recognizer Output Voting
Error Reduction (ROVER), in Proc. ASRU.
S. Bangalore, G. Bordel, and G. Riccardi, 2001.
Computing consensus translation from multiple
machine translation systems, in Proc. ASRU.
E. Matusov, N. Ueffing, and H. Ney, 2006.
Computing consensus translation from multiple
machine translation systems using enhanced
hypotheses alignment, in Proc. EACL.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz, 2007.
Improved Word-Level System Combination for
Machine Translation. In Proc. ACL.
X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore,
2008. Indirect-HMM-based hypothesis alignment for
combining outputs from machine translation
systems, in Proc. EMNLP.
L. Mangu, E. Brill, and A. Stolcke, 2000. Finding
Consensus in Speech Recognition: Word Error
Minimization and Other Applications of Confusion
Networks, Computer Speech and Language,
14(4):373-400.
R. Zens and H. Ney, 2004. N-Gram posterior
probabilities for statistical machine translation, in
Proc. HLT-NAACL.
K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi and
P.C. Woodland, 2007. Consensus network decoding
for statistical machine translation system
combination. in Proc. ICASSP.
V. Goel, S. Kumar, and W. Byrne, 2004. Segmental
minimum Bayes-risk decoding for automatic speech
recognition. IEEE transactions on Speech and Audio
Processing, vol. 12, no. 3.
K. Papineni, S. Roukos, T. Ward, and W. Zhu, 2002.
BLEU: a method for automatic evaluation of
machine translation. in Proc. ACL.
K. Toutanova, H. Suzuki and A. Ruopp. 2008. Applying
Morphology Generation Models to Machine
Translation. In Proc. of ACL.
Yong Zhao and Xiaodong He. 2008. System
Combination for Machine Translation Using N-Gram
Posterior Probabilities. NIPS 2008 WORKSHOP on
Speech and Language: Learning-based Methods and
Systems. Dec. 2008
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D.
Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B.
Marino, M. Paulik, S. Roukos, H. Schwenk, and H.
Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language
Processing, Sept. 2008.
</reference>
<page confidence="0.997783">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002225">
<title confidence="0.999791">Using N-gram based Features for Machine System Combination</title>
<author confidence="0.98886">Xiaodong He</author>
<affiliation confidence="0.999975">Georgia Institute of Technology Microsoft Research</affiliation>
<address confidence="0.999353">Atlanta, GA 30332, USA Redmond, WA 98052, USA</address>
<email confidence="0.999281">yongzhao@gatech.eduxiaohe@microsoft.com</email>
<abstract confidence="0.993373223776224">Conventional confusion network based system combination for machine translation (MT) heavily relies on features that are based on the measure of agreement of words in different translation hypotheses. This paper presents two new features that consider agreement of n-grams in different hypotheses to improve the performance of system combination. The first one is based on a sentence specific online n-gram language model, and the second one is based on n-gram voting. Experiments on a large scale Chinese-to-English MT task show that both features yield significant improvements on the translation performance, and a combination of them produces even better translation results. In past years, the confusion network based system combination approach has been shown with substantial improvements in various machine translation (MT) tasks (Bangalore, et. al., 2001, Matusov, et. al., 2006, Rosti, et. al., 2007, He, et. al., 2008). Given hypotheses of multiple systems, a confusion network is built by aligning all these hypotheses. The resulting network comprises a sequence of correspondence sets, each of which contains the alternative words that are aligned with each other. To derive a consensus hypothesis from the confusion network, decoding is performed by selecting a path with the maximum overall confidence score among all paths that pass the confusion network (Goel, et. al., 2004). work was performed when Yong Zhao was an intern at Microsoft Research The confidence score of a hypothesis could be assigned in various ways. Fiscus (1997) used voting by frequency of word occurrences. Mangu et. al., (2000) computed a word posterior probability based on voting of that word in different hypotheses. Moreover, the overall confidence score is usually formulated as a loglinear model including extra features including language model (LM) score, word count, etc. Features based on word agreement measure are extensively studied in past work (Matusov, et. al., 2006, Rosti, et. al., 2007, He, et. al., 2008). However, utilization of n-gram agreement information among the hypotheses has not been fully explored yet. Moreover, it was argued that the confusion network decoding may introduce undesirable spur words that break coherent phrases (Sim, et. al., 2007). Therefore, we would prefer the consensus translation that has better ngram agreement among outputs of single systems. In the literature, Zens and Ney (2004) proposed an n-gram posterior probability based LM for MT. For each source sentence, a LM is trained on the n-best list produced by a single MT system and is used to re-rank that n-best list itself. On the other hand, Matusov et al. (2008) proposed for system combination, where LM is on translation hypotheses of the whole test corpus from all single MT systems involved in system combination. Inspired by these ideas, we propose two new features based on n-gram agreement measure to improve the performance of system combination. The first one is a sentence specific LM built on translation hypotheses of multiple systems; the second one is n-gram-voting-based confidence. Experimental results are presented in the context of a large-scale Chinese-English translation task. 205 of NAACL HLT 2009: Short pages Colorado, June 2009. Association for Computational Linguistics 2 System Combination for MT One of the most successful approaches for system combination for MT is based on confusion network decoding as described in (Rosti, et. al., 2007). Given translation hypotheses from multiple MT systems, one of the hypotheses is selected as the backbone for the use of hypothesis alignment. This is usually done by a sentence-level minimum Bayes risk (MBR) re-ranking method. The confusion network is constructed by aligning all these hypotheses against the backbone. Words that align to each other are grouped into a correspondence set, constituting competition links of the confusion network. Each path in the network passes exactly one link from each correspondence set. The final consensus output relies on a decoding procedure that chooses a path with the maximum confidence score among all paths that pass the confusion network. The confidence score of a hypothesis is usually formalized as a log-linear sum of several feature functions. Given a source language the total confidence of a target hypothesis the confusion network can be represented as: 𝐿 𝐸 𝐹 𝑃 + where the feature functions include word probability LM probability and the number of real words Usually, the model parameter be trained by optimizing an evaluation metric, e.g., BLEU score, on a held-out development set. 3 N-gram Online Language Model a source sentence the fractional count 𝐹 an n-gram is defined as: the hypothesis set, the Kronecker function, and the posterior probability of translation which is expressed as the weighted sum of the system specific posterior probabilities through the systems that contains 𝐾 𝐸 𝐹 ∈ the weight for the posterior of the and the indicator function. Follows Rosti, et. al. (2007), system specific posteriors are derived based on a rank-based scheme. I.e., if translation hypothesis the best output in the n-best list of system posterior approximated as: + (4) + a rank smoothing parameter. Similar to (Zens and Ney, 2004), a straightforward approach of using n-gram fractional counts is to formulate it as a sentence specific online LM. Then the online LM score of a path in the confusion network will be added as an additional feature in the log-linear model for decoding. The online n-gram LM score is computed by: = (5) LM score of hypothesis obtained by: 𝐿 𝐹 Since new n-grams unseen in original translation hypotheses may be proposed by the CN decoder, LM smoothing is critical. In our approach, the score of the online LM is smoothed by taking a linear interpolation to combine scores of different orders. 𝐿 206 ℎ 𝑛 In our implementation, the interpolation weights can be learned along with other combination parameters in the same Max-BLEU training scheme via Powell&apos;s search. 4 N-gram-Voting-Based Confidence Motivated by features based on voting of single word, we proposed new features based on Nvoting. The voting score 𝐹 an nis computed as: 𝐹 It receives a vote from each hypothesis that contains that n-gram, and weighted by the posterior probability of that hypothesis, where posterior probability computed by (3). Unlike the fractional count, each hypothesis can vote no more than once on an n-gram. 𝐹 a value between 0 and 1. It can be viewed as the confidence of the n-gram . Then the n-gram-voting-based confidence of a hypothesis computed as the of confidence scores of n-grams in 𝐹 the value of 2, 3, ..., In order to prevent zero confidence, a small backoff confidence score is assigned to all n-grams unseen in original hypotheses. Augmented with the proposed n-gram based features, the final log-linear model becomes: 𝑙𝑜𝑔𝑃 𝐸 𝐹 𝐿 𝑃 𝐹 𝑁 𝐹 5 Evaluation We evaluate the proposed n-gram based features on the Chinese-to-English (C2E) test in the past NIST Open MT Evaluations. The experimental results are reported in case sensitive BLEU et. al., 2002). The dev set, which is used for system combination parameter training, is the newswire and newsgroup parts of NIST MT06, which contains a total of 1099 sentences. The test set is test set of NIST MT08, which contains 1357 sentences of newswire and webblog data. Both dev and test sets have four reference translations per sentence. Outputs from a total of eight single MT systems were combined for consensus translations. These selected systems are based on various translation paradigms, such as phrasal, hierarchical, and syntax-based systems. Each system produces 10-best hypotheses per translation. The BLEU score range for the eight individual systems are from 26.11% to 31.09% on the dev set and from 20.42% to 26.24% on the test set. In our experiments, a state-of-the-art system combination method proposed by He, et. al. (2008) is implemented as the baseline. The true-casing model proposed by Toutanova et al. (2008) is used. Table 1 shows results of adding the online LM feature. Different LM orders up to four are tested. Results show that using a 2-gram online LM yields a half BLEU point gain over the baseline. However, the gain is saturated after a LM order of three, and fluctuates after that. Table 2 shows the performance of using ngram-voting-based confidence features. The best result of 31.01% is achieved when up to 4-gram confidence features are used. The BLEU score keeps improving when longer n-gram confidence features are added. This indicates that the n-gram voting based confidence feature is robust to high order n-grams. We further experimented with incorporating both features in the log-linear model and reported the results in Table 3. Given the observation that the n-gram voting based confidence feature is more robust to high order n-grams, we further tested using different ngram orders for them. As shown in Table 3, using 3-gram online LM plus 2~4-gram voting (7) 207 based confidence scores yields the best BLEU scores on both dev and test sets, which are 37.98% and 31.35%, respectively. This is a 0.84 BLEU point gain over the baseline on the MT08 test set. Table 1: Results of adding the n-gram online LM. BLEU % Dev Test Baseline 37.34 30.51 1-gram online LM 37.34 30.51 2-gram online LM 37.86 31.02 3-gram online LM 37.87 31.08 4-gram online LM 37.86 31.01 Table 2: Results of adding n-gram voting based confidence features. BLEU % Dev Test Baseline 37.34 30.51 + 2-gram voting 37.58 30.88 + 2~3-gram voting 37.66 30.96 + 2~4-gram voting 37.77 31.01 Table 3: Results of using both n-gram online LM and n-gram voting based confidence features BLEU % Dev Test Baseline 37.34 30.51 2-gram LM + 2-gram voting 37.78 30.98 3-gram LM + 2~3-gram voting 37.89 31.21 4-gram LM + 2~4-gram voting 37.93 31.08 3-gram LM + 2~4-gram voting 37.98 31.35 6 Conclusion This work explored utilization of n-gram agreement information among translation outputs of multiple MT systems to improve the performance of system combination. This is an extension of an earlier idea presented at the NIPS 2008 Workshop on Speech and Language (Yong and He 2008). Two kinds of n-gram based features were proposed. The first is based on an online LM using n-gram fractional counts, and the second is a confidence feature based on ngram voting scores. Our experiments on the NIST MT08 Chinese-English task showed that both methods yield nice improvements on the translation results, and incorporating both kinds of features produced the best translation result with a BLEU score of 31.35%, which is a 0.84% improvement.</abstract>
<note confidence="0.825749083333333">References J.G. Fiscus, 1997. A post-processing system to yield reduced word error rates: Recognizer Output Voting Reduction (ROVER), in S. Bangalore, G. Bordel, and G. Riccardi, 2001. Computing consensus translation from multiple translation systems, in E. Matusov, N. Ueffing, and H. Ney, 2006. Computing consensus translation from multiple machine translation systems using enhanced alignment, in A.-V.I. Rosti, S. Matsoukas, and R. Schwartz, 2007.</note>
<title confidence="0.950514">Improved Word-Level System Combination for</title>
<author confidence="0.712804">In X He</author>
<author confidence="0.712804">M Yang</author>
<author confidence="0.712804">J Gao</author>
<author confidence="0.712804">P Nguyen</author>
<author confidence="0.712804">R Moore</author>
<abstract confidence="0.914251043478261">2008. Indirect-HMM-based hypothesis alignment for combining outputs from machine translation in L. Mangu, E. Brill, and A. Stolcke, 2000. Finding Consensus in Speech Recognition: Word Error Minimization and Other Applications of Confusion Speech and 14(4):373-400. R. Zens and H. Ney, 2004. N-Gram posterior probabilities for statistical machine translation, in K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi and P.C. Woodland, 2007. Consensus network decoding for statistical machine translation system in V. Goel, S. Kumar, and W. Byrne, 2004. Segmental minimum Bayes-risk decoding for automatic speech transactions on Speech and Audio vol. 12, no. 3. K. Papineni, S. Roukos, T. Ward, and W. Zhu, 2002. BLEU: a method for automatic evaluation of translation. in K. Toutanova, H. Suzuki and A. Ruopp. 2008. Applying Morphology Generation Models to Machine</abstract>
<note confidence="0.522342642857143">In of Yong Zhao and Xiaodong He. 2008. System Combination for Machine Translation Using N-Gram Probabilities. 2008 WORKSHOP on Speech and Language: Learning-based Methods and 2008 E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B. Marino, M. Paulik, S. Roukos, H. Schwenk, and H. Ney. 2008. System Combination for Machine Translation of Spoken and Written Language. IEEE Transactions on Audio, Speech and Language Processing, Sept. 2008. 208</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER), in</title>
<date>1997</date>
<booktitle>Proc. ASRU.</booktitle>
<contexts>
<context position="1811" citStr="Fiscus (1997)" startWordPosition="267" endWordPosition="268">heses of multiple systems, a confusion network is built by aligning all these hypotheses. The resulting network comprises a sequence of correspondence sets, each of which contains the alternative words that are aligned with each other. To derive a consensus hypothesis from the confusion network, decoding is performed by selecting a path with the maximum overall confidence score among all paths that pass the confusion network (Goel, et. al., 2004). 1 The work was performed when Yong Zhao was an intern at Microsoft Research The confidence score of a hypothesis could be assigned in various ways. Fiscus (1997) used voting by frequency of word occurrences. Mangu et. al., (2000) computed a word posterior probability based on voting of that word in different hypotheses. Moreover, the overall confidence score is usually formulated as a loglinear model including extra features including language model (LM) score, word count, etc. Features based on word agreement measure are extensively studied in past work (Matusov, et. al., 2006, Rosti, et. al., 2007, He, et. al., 2008). However, utilization of n-gram agreement information among the hypotheses has not been fully explored yet. Moreover, it was argued th</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>J.G. Fiscus, 1997. A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER), in Proc. ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Bordel</author>
<author>G Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems, in</title>
<date>2001</date>
<booktitle>Proc. ASRU.</booktitle>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>S. Bangalore, G. Bordel, and G. Riccardi, 2001. Computing consensus translation from multiple machine translation systems, in Proc. ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment, in</title>
<date>2006</date>
<booktitle>Proc. EACL.</booktitle>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>E. Matusov, N. Ueffing, and H. Ney, 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment, in Proc. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-V I Rosti</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
</authors>
<title>Improved Word-Level System Combination for Machine Translation. In</title>
<date>2007</date>
<booktitle>Proc. ACL.</booktitle>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>A.-V.I. Rosti, S. Matsoukas, and R. Schwartz, 2007. Improved Word-Level System Combination for Machine Translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X He</author>
<author>M Yang</author>
<author>J Gao</author>
<author>P Nguyen</author>
<author>R Moore</author>
</authors>
<title>Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems, in</title>
<date>2008</date>
<booktitle>Proc. EMNLP.</booktitle>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore, 2008. Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems, in Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
<author>A Stolcke</author>
</authors>
<title>Finding Consensus in Speech Recognition:</title>
<date>2000</date>
<booktitle>Word Error Minimization and Other Applications of Confusion Networks, Computer Speech and Language,</booktitle>
<pages>14--4</pages>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>L. Mangu, E. Brill, and A. Stolcke, 2000. Finding Consensus in Speech Recognition: Word Error Minimization and Other Applications of Confusion Networks, Computer Speech and Language, 14(4):373-400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>N-Gram posterior probabilities for statistical machine translation, in</title>
<date>2004</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="2688" citStr="Zens and Ney (2004)" startWordPosition="400" endWordPosition="403"> features including language model (LM) score, word count, etc. Features based on word agreement measure are extensively studied in past work (Matusov, et. al., 2006, Rosti, et. al., 2007, He, et. al., 2008). However, utilization of n-gram agreement information among the hypotheses has not been fully explored yet. Moreover, it was argued that the confusion network decoding may introduce undesirable spur words that break coherent phrases (Sim, et. al., 2007). Therefore, we would prefer the consensus translation that has better ngram agreement among outputs of single systems. In the literature, Zens and Ney (2004) proposed an n-gram posterior probability based LM for MT. For each source sentence, a LM is trained on the n-best list produced by a single MT system and is used to re-rank that n-best list itself. On the other hand, Matusov et al. (2008) proposed an “adapted” LM for system combination, where this “adapted” LM is trained on translation hypotheses of the whole test corpus from all single MT systems involved in system combination. Inspired by these ideas, we propose two new features based on n-gram agreement measure to improve the performance of system combination. The first one is a sentence s</context>
<context position="6024" citStr="Zens and Ney, 2004" startWordPosition="959" endWordPosition="962">ighted sum of the system specific posterior probabilities through the systems that contains hypothesis 𝐸′, 𝐾 𝑃 𝐸 𝐹 = 𝑤𝑘𝑃(𝐸 𝑆𝑘,𝐹 1(𝐸 ∈ 𝑬𝑆𝑘) (3) 𝑘=1 where 𝑤𝑘 is the weight for the posterior probability of the e system 𝑆𝑘, and 1 ∙ is the indicator function. Follows Rosti, et. al. (2007), system specific posteriors are derived based on a rank-based scoring scheme. I.e., if translation hypothesis 𝐸𝑟 is the rrh best output in the n-best list of system 𝑆𝑘, posterior 𝑃 𝐸𝑟 𝑆𝑘, 𝐹 is approximated as: 𝑃 𝐸𝑟 𝑆𝑘, 𝐹 = 1/(1 + 𝑟)𝜂 (4) 𝑟′ =1 ||𝑬𝑆𝑘 ||1/(1 + 𝑟′)𝜂 where η is a rank smoothing parameter. Similar to (Zens and Ney, 2004), a straightforward approach of using n-gram fractional counts is to formulate it as a sentence specific online LM. Then the online LM score of a path in the confusion network will be added as an additional feature in the log-linear model for decoding. The online n-gram LM score is computed by: 𝑙 |𝐹) 𝑃(𝑒𝑙|𝑒𝑙−𝑛+1 𝑙−1 , 𝐹) = 𝐶(𝑒𝑙−𝑛+1 (5) 𝐶(𝑒𝑙−𝑛+1 𝑙−1 |𝐹) The LM score of hypothesis 𝐸 is obtained by: 𝐿 𝑃𝐿𝑀 𝐸 𝐹 = 𝑃 𝑒𝑙|𝑒𝑙−𝑛+1 𝑙−1 , 𝐹 (6) 𝑙=𝑛 Since new n-grams unseen in original translation hypotheses may be proposed by the CN decoder, LM smoothing is critical. In our approach, the score of the onlin</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney, 2004. N-Gram posterior probabilities for statistical machine translation, in Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Sim</author>
<author>W J Byrne</author>
<author>M J F Gales</author>
<author>H Sahbi</author>
<author>P C Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination. in</title>
<date>2007</date>
<booktitle>Proc. ICASSP.</booktitle>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi and P.C. Woodland, 2007. Consensus network decoding for statistical machine translation system combination. in Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Segmental minimum Bayes-risk decoding for automatic speech recognition.</title>
<date>2004</date>
<booktitle>IEEE transactions on Speech and Audio Processing,</booktitle>
<volume>12</volume>
<marker>Goel, Kumar, Byrne, 2004</marker>
<rawString>V. Goel, S. Kumar, and W. Byrne, 2004. Segmental minimum Bayes-risk decoding for automatic speech recognition. IEEE transactions on Speech and Audio Processing, vol. 12, no. 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation. in</title>
<date>2002</date>
<booktitle>Proc. ACL.</booktitle>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu, 2002. BLEU: a method for automatic evaluation of machine translation. in Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>H Suzuki</author>
<author>A Ruopp</author>
</authors>
<title>Applying Morphology Generation Models to Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="9316" citStr="Toutanova et al. (2008)" startWordPosition="1553" endWordPosition="1556">e translations per sentence. Outputs from a total of eight single MT systems were combined for consensus translations. These selected systems are based on various translation paradigms, such as phrasal, hierarchical, and syntax-based systems. Each system produces 10-best hypotheses per translation. The BLEU score range for the eight individual systems are from 26.11% to 31.09% on the dev set and from 20.42% to 26.24% on the test set. In our experiments, a state-of-the-art system combination method proposed by He, et. al. (2008) is implemented as the baseline. The true-casing model proposed by Toutanova et al. (2008) is used. Table 1 shows results of adding the online LM feature. Different LM orders up to four are tested. Results show that using a 2-gram online LM yields a half BLEU point gain over the baseline. However, the gain is saturated after a LM order of three, and fluctuates after that. Table 2 shows the performance of using ngram-voting-based confidence features. The best result of 31.01% is achieved when up to 4-gram confidence features are used. The BLEU score keeps improving when longer n-gram confidence features are added. This indicates that the n-gram voting based confidence feature is rob</context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>K. Toutanova, H. Suzuki and A. Ruopp. 2008. Applying Morphology Generation Models to Machine Translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong Zhao</author>
<author>Xiaodong He</author>
</authors>
<title>System Combination for Machine Translation Using N-Gram Posterior Probabilities.</title>
<date>2008</date>
<booktitle>NIPS 2008 WORKSHOP on Speech and Language: Learning-based Methods and Systems.</booktitle>
<marker>Zhao, He, 2008</marker>
<rawString>Yong Zhao and Xiaodong He. 2008. System Combination for Machine Translation Using N-Gram Posterior Probabilities. NIPS 2008 WORKSHOP on Speech and Language: Learning-based Methods and Systems. Dec. 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>G Leusch</author>
<author>R E Banchs</author>
<author>N Bertoldi</author>
<author>D Dechelotte</author>
<author>M Federico</author>
<author>M Kolss</author>
<author>Y Lee</author>
<author>J B Marino</author>
<author>M Paulik</author>
<author>S Roukos</author>
<author>H Schwenk</author>
<author>H Ney</author>
</authors>
<date>2008</date>
<booktitle>System Combination for Machine Translation of Spoken and Written Language. IEEE Transactions on Audio, Speech and Language Processing,</booktitle>
<contexts>
<context position="2927" citStr="Matusov et al. (2008)" startWordPosition="444" endWordPosition="447">am agreement information among the hypotheses has not been fully explored yet. Moreover, it was argued that the confusion network decoding may introduce undesirable spur words that break coherent phrases (Sim, et. al., 2007). Therefore, we would prefer the consensus translation that has better ngram agreement among outputs of single systems. In the literature, Zens and Ney (2004) proposed an n-gram posterior probability based LM for MT. For each source sentence, a LM is trained on the n-best list produced by a single MT system and is used to re-rank that n-best list itself. On the other hand, Matusov et al. (2008) proposed an “adapted” LM for system combination, where this “adapted” LM is trained on translation hypotheses of the whole test corpus from all single MT systems involved in system combination. Inspired by these ideas, we propose two new features based on n-gram agreement measure to improve the performance of system combination. The first one is a sentence specific LM built on translation hypotheses of multiple systems; the second one is n-gram-voting-based confidence. Experimental results are presented in the context of a large-scale Chinese-English translation task. 205 Proceedings of NAACL</context>
</contexts>
<marker>Matusov, Leusch, Banchs, Bertoldi, Dechelotte, Federico, Kolss, Lee, Marino, Paulik, Roukos, Schwenk, Ney, 2008</marker>
<rawString>E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. Dechelotte, M. Federico, M. Kolss, Y. Lee, J. B. Marino, M. Paulik, S. Roukos, H. Schwenk, and H. Ney. 2008. System Combination for Machine Translation of Spoken and Written Language. IEEE Transactions on Audio, Speech and Language Processing, Sept. 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>