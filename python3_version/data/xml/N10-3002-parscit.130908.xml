<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002319">
<title confidence="0.9975295">
On Automated Evaluation of Readability of Summaries:
Capturing Grammaticality, Focus, Structure and Coherence
</title>
<author confidence="0.992246">
Raviliiran Vadlapudi
</author>
<affiliation confidence="0.8306285">
Language Technologies Research Center
IIIT Hyderabad
</affiliation>
<email confidence="0.992875">
ravikiranv@research.iiit.ac.in
</email>
<author confidence="0.987143">
Rahul Katragadda
</author>
<affiliation confidence="0.8295665">
Language Technologies Research Center
IIIT Hyderabad
</affiliation>
<email confidence="0.989519">
rahul k@research.iiit.ac.in
</email>
<sectionHeader confidence="0.993632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99943047826087">
Readability of a summary is usually graded
manually on five aspects of readability: gram-
maticality, coherence and structure, focus,
referential clarity and non-redundancy. In the
context of automated metrics for evaluation
of summary quality, content evaluations have
been presented through the last decade and
continue to evolve, however a careful exami-
nation of readability aspects of summary qual-
ity has not been as exhaustive. In this paper
we explore alternative evaluation metrics for
‘grammaticality’ and ‘coherence and struc-
ture’ that are able to strongly correlate with
manual ratings. Our results establish that our
methods are able to perform pair-wise rank-
ing of summaries based on grammaticality, as
strongly as ROUGE is able to distinguish for
content evaluations. We observed that none
of the five aspects of readability are indepen-
dent of each other, and hence by addressing
the individual criterion of evaluation we aim to
achieve automated appreciation of readability
of summaries.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99949025">
Automated text summarization deals with both the
problem of identifying relevant snippets of informa-
tion and presenting it in a pertinent format. Auto-
mated evaluation is crucial to automatic text sum-
marization to be used both to rank multiple partic-
ipant systems in shared tasks1, and to developers
whose goal is to improve the summarization sys-
tems. Summarization evaluations help in the cre-
ation of reusable resources and infrastructure; it sets
up the stage for comparison and replication of re-
sults by introducing an element of competition to
produce better results (Hirschman and Mani, 2001).
</bodyText>
<footnote confidence="0.958957">
1The summarization tracks at Text Analysis Conference
(TAC) 2009, 2008 and its predecessors at Document Under-
standing Conferences (DUC).
</footnote>
<page confidence="0.996509">
7
</page>
<bodyText confidence="0.999805025641026">
Readability or Fluency of a summary is categor-
ically measured based on a set of linguistic qual-
ity questions that manual assessors answer for each
summary. The linguistic quality markers are: gram-
maticality, Non-Redundancy, Referential Clarity,
Focus and Structure and Coherence. Hence read-
ability assessment is a manual method where ex-
pert assessors give a rating for each summary on
the Likert Scale for each of the linguistic quality
markers. Manual evaluation being time-consuming
and expensive doesn’t help system developers —
who appreciate fast, reliable and most importantly
automated evaluation metric. So despite having a
sound manual evaluation methodology for readabil-
ity, there is an need for reliable automatic metrics.
All the early approaches like Flesch Reading Ease
(Flesch, 1948) were developed for general texts and
none of these techniques have tried to character-
ize themselves as approximations to grammatical-
ity or structure or coherence. In assessing read-
ability of summaries, there hasn’t been much of
dedicated analysis with text summaries, except in
(Barzilay and Lapata, 2005) where local coherence
was modeled for text summaries and in (Vadlapudi
and Katragadda, 2010) where grammaticality of text
summaries were explored. In a marginally related
work in Natural Language Generation, (Mutton et
al., 2007) addresses sentence level fluency regard-
less of content, while recent work in (Chae and
Nenkova, 2009) gives a systematic study on how
syntactic features were able to distinguish machine
generated translations from human translations. In
another related work, (Pitler and Nenkova, 2008)
investigated the impact of certain surface linguistic
features, syntactic, entity coherence and discourse
features on the readability of Wall Street Journal
(WSJ) Corpus. We use the syntactic features used
in (Pitler and Nenkova, 2008) as baselines for our
experiments on grammaticality in this paper.
</bodyText>
<subsectionHeader confidence="0.3264525">
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 7–12,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999965923076923">
While studying the coherence patterns in student
essays, (Higgins et al., 2004) identified that gram-
matical errors affect the overall expressive quality
of the essays. In this paper, due to the lack of an ap-
propriate baseline and due to the interesting-ness of
the above observation we use metrics for grammat-
icality as a baseline measure for structure and co-
herence. Focus of a summary, is the only aspect of
readability that relies to a larger extent on the content
of the summary. In this paper, we use Recall Ori-
ented Understudy of Gisting Evaluation (ROUGE)
(Lin, 2004) based metrics as one of the baselines to
capture focus in a summary.
</bodyText>
<sectionHeader confidence="0.943045" genericHeader="method">
2 Summary Grammaticality
</sectionHeader>
<bodyText confidence="0.9996474">
Grammaticality of summaries, in this paper, is de-
fined based on the grammaticality of its sentences,
since it is more a sentence level syntactic property.
A sentence can either be grammatically correct or
grammatically incorrect. The problem of grammati-
cal incorrectness should not occur in summaries be-
ing evaluated because they are generated mostly by
extract based summarization systems.
But as the distribution of grammaticality scores
in Table 1 shows, there are a lot of summaries that
obtain very low scores. Hence, We model the prob-
lem of grammaticality as “how suitable or accept-
able are the sentence structures to be a part of a
summary?”.
The acceptance or non acceptance of sentence
structures varies across reviewers because of vari-
ous factors like usage, style and dialects. Hence, we
define a degree to which a sentence structure is ac-
ceptable to the reviewers, this is called the degree of
acceptance throughout this paper.
</bodyText>
<tableCaption confidence="0.971128">
Table 1: Percentage distribution of grammaticality scores
in system summaries
</tableCaption>
<bodyText confidence="0.99995975862069">
In this paper, the degree of acceptance of sen-
tence structures is estimated using language mod-
els trained on a corpus of human written sum-
maries. Considering the sentence structures in ref-
erence summaries as the best accepted ones (with
highest degree of acceptance), we estimate the de-
gree of acceptance of sentences in system gener-
ated summaries by quantifying the amount of sim-
ilarity/digression from the references using the lan-
guage models.
The structure of the sentences can be represented
by sequences of parts-of-speech (POS) tags and
chunk tags. Our previous observations (Vadlapudi
and Katragadda, 2010) show that the tagset size
plays an important role in determining the degree
of acceptance. In this paper, we combine the two
features of a sentence — the POS-tag sequence and
chunk-tag sequence — to generate the POS-Chunk-
tag training corpus.
Some aspects of grammatical structure are well
identifiable at the level of POS tags, while some
other aspects (such as distinguishing between appos-
itives and lists for eg.) need the power of chunk tags,
the combination of these two tag-sequences provides
the power of both.
Hence, the following approaches use probabilis-
tic models, learned on POS tag corpus and POS-
Chunk tag corpus, in 3 different ways to determine
the grammaticality of a sentence.
</bodyText>
<subsectionHeader confidence="0.881913">
2.1 Enhanced Ngram model
</subsectionHeader>
<bodyText confidence="0.999952375">
As described in our previous work, the Ngram
model estimates the probability of a sentence to be
grammatically acceptable with respect to the corpus
using language models. Sentences constructed us-
ing frequent grammar rules would have higher prob-
ability and are said to have a well accepted sentence
structure. The grammaticality of a summary is com-
puted as
</bodyText>
<equation confidence="0.99530225">
� G(Sum) = AV G(P(Seqi)) ; P(Seqi) = log( n P(Kj))
j=1
P(Kj) = P(tj−2tj−1tj)
P(t1t2t3) = ,\1 ∗ P(t3|t1t2) + A2 ∗ P(t3|t2) + A3 ∗ P(t3)
</equation>
<bodyText confidence="0.999984357142857">
where G(Sum) is grammaticality score of a sum-
mary Sum and G(Si) is grammaticality of sentence
Si which is estimated by the probability (P(Segi))
of its POS-tag sequence (Segi). P(Kj) is proba-
bility of POS-tag trigram Kj which is tj−2tj−1tj
and Vtj, tj E POS tags. The additional tags
t−1, to and t,,,+1 are the beginning-of-sequence and
end-of-sequence markers. The average AVG of
the grammaticality scores of sentences P(Segi) in
a summary gives the final grammaticality score of
the summary. In the prior work, arithmetic mean
was used as the averaging technique, which per-
forms consistently well. However, here two other
averaging techniques namely geometric mean and
</bodyText>
<figure confidence="0.837963">
Grammaticality Score
Percentage Distribution (in %)
10 13 15 37 25
1 2 3 4 5
</figure>
<page confidence="0.983232">
8
</page>
<bodyText confidence="0.999878090909091">
harmonic mean are experimented and based on our
experiments, we found geometric mean perform-
ing better than the other two averaging techniques.
All the results reported in this paper are based on
geometric mean. The above procedure estimates
grammaticality of sentence using its POS tags and
we call this run ‘Ngram (POS)’. A similar proce-
dure is followed to estimate grammaticality using its
POS-Chunk tags (language models trained on POS-
chunk-tag training corpus). The corresponding run
is called ‘Ngram (POS-Chunk)’ in the results.
</bodyText>
<subsectionHeader confidence="0.997749">
2.2 Multi-Level Class model
</subsectionHeader>
<bodyText confidence="0.999912652173913">
In this model, we view the task of scoring grammati-
cality as a n-level classification problem. Grammat-
icality of summaries is manually scored on a scale
of 1 to 5, which means the summaries are classi-
fied into 5 classes. We assume that each sentence of
the summary is also rated on a similar scale which
cumulatively decides to which class the summary
must belong. In our approach, sentences are classi-
fied into 5 classes on the basis of frequencies of un-
derlying grammar rules (trigram) by defining class
boundaries on frequencies. Hence, the cumulative
score of the rules estimate the score of grammatical-
ity of a sentence and inturn the summary.
Similar to (Vadlapudi and Katragadda, 2010), tri-
grams are classified into 5 classes C1, C2, C3, C�
and C5 and each class is assigned a score on a sim-
ilar scale (Vjscore(Cj) = j) and class boundaries
are estimated using the frequencies of trigrams in
the training corpus. The most frequent trigram, for
example, would fall into class C5. POS-Class se-
quences are generated from POS-tag sequences us-
ing class boundaries as shown in Figure 1. This is
the first level of classification.
</bodyText>
<figureCaption confidence="0.999256">
Figure 1: Two-level class model
</figureCaption>
<bodyText confidence="0.9971389">
Like the first level of classification, a series of
classifications are performed upto ‘k’ levels. At each
level we apply the scoring method described below
to evaluate the grammaticality of summaries. We
observed that from 3rd level onwards the structural
dissimilarity disappears and the ability to distinguish
different structures is lost. Hence, we report on the
second level of classification, that captures the gram-
matical acceptability of summaries very well, and
Figure 1 explains the two level classification.
</bodyText>
<equation confidence="0.947471">
G(Si) = AV G(H(Cw1), H(Cw2),...., H(Cw,,)) (1)
</equation>
<bodyText confidence="0.999090545454545">
AVG is the average of H(Cwi), where w1, w2,
... wn are class trigrams, Cwi is the class into which
class trigram wi falls into and H(Cwi) is score as-
signed to the class Cwi. The AVG is computed us-
ing geometric mean and this run is referred as ‘Class
(POS 2 level)’ in the results.
Similar to above approach, the grammaticality
of a sentence can also be estimated using POS-
Chunk tag sequence and POS-Chunk Class training
data, and the corresponding run is referred as ‘Class
(POS-Chunk 2 level)’.
</bodyText>
<subsectionHeader confidence="0.954674">
2.3 Hybrid Model
</subsectionHeader>
<bodyText confidence="0.9998778125">
As would be later seen in Table 2, the Ngram (POS)
and Class (POS 2 level) runs are able to distin-
guish various systems based on grammaticality. We
also note that these runs are able to very finely
distinguish the degree of grammaticality at sum-
mary level. This is a very positive result, one that
shows the applicability of applying these methods
to any test summaries in this genre. To fully uti-
lize these methods we combine the two methods
by a linear combination of their scores to form a
‘hybrid model’. As seen with earlier approaches,
both the POS-tag sequences and POS-Chunk-tag se-
quences could be used to estimate the grammatical-
ity of a sentence, and hence the summary. These two
runs are called ‘Hybrid (POS)’ and ‘Hybrid (POS-
Chunk)’, respectively.
</bodyText>
<sectionHeader confidence="0.986575" genericHeader="method">
3 Structure and Coherence
</sectionHeader>
<bodyText confidence="0.998872375">
Most automated systems generate summaries from
multiple documents by extracting relevant sentences
and concatenating them. For these summaries to be
comprehensible they must also be cohesive and co-
herent, apart from being content bearing and gram-
matical. Lexical cohesion is a type of cohesion that
arises from links between words in a text (Halliday
and Hasan, 1976).A Lexical chain is a sequence of
</bodyText>
<page confidence="0.989286">
9
</page>
<bodyText confidence="0.999257179487179">
such related words spanning a unit of text. Lexi-
cal cohesion along with presuppositions and impli-
cations with world knowledge achieves coherence in
texts. Hence, coherence is what makes text semanti-
cally meaningful, and in this paper, we also attempt
to automate the evaluation of the “structure and co-
herence” of summaries.
We capture the structure or lexical cohesion of a
summary by constructing a lexical chain that spans
the summary. The relation between entities (noun
phrases) in adjacent sentences could be of type
center-reference (pronoun reference or reiteration),
or based on semantic relatedness (Morris and Hirst,
1991). A center-reference relation exists if an en-
tity in a sentence is a reference to center in adjacent
sentence. Identifying centers of reference expres-
sions can be done using a co-reference resolution
tool. Performance of co-reference resolution tools in
summaries, being evaluated, is not as good as their
performance on generic texts. Semantic relatedness
relation cannot be captured by using tools like Word-
net because they are not very exhaustive and hence
are not effective. We use a much richer knowledge
base to define this relation – Wikipedia.
Coherence of a summary is modelled by its struc-
ture and content together. Structure is captured by
lexical chains which also give information about fo-
cus of each sentence which inturn contribute to the
topic focus of the summary. Content presented in
the summary must be semantically relevant to the
topic focus of the summary. If the content presented
by each sentence is semantically relevant to the fo-
cus of the sentence, then it would be semantically
relevant to the topic focus of the summary. As the
foci of sentences are closely related, a prerequisite
for being a part of a lexical chain, the summary is
said to be coherent. In this paper, the semantic relat-
edness of topic focus and content is captured using
Wikipedia as elaborated in Section 3.1 of this paper.
</bodyText>
<subsectionHeader confidence="0.999907">
3.1 Construction of lexical chains
</subsectionHeader>
<bodyText confidence="0.99928">
In this approach, we identify the strongest lexical
chain possible which would capture the structure of
the summary. We define this problem of finding the
strongest possible lexical chain as that of finding the
best possible parts-of-speech tag sequence for a sen-
tence using the Viterbi algorithm shown in (Brants,
2000). The entities (noun phrases) of each sentence
are the nodes and transition probabilities are defined
as relatedness score (Figure 2). The strongest lex-
ical chain would have the highest score than other
possible lexical chains obtained.
Consider sentence 5k with entity set (e11, e12,
e13.... e1n) and sentence 5k+1 with entity set (e21,
e22, e23, . . . e2.). Sentences 5k and 5k+1 are said
to be strongly connected if there exists entities e1i E
5k and e2j E 5k+1 that are closely related. e1i and
e2j are considered closely related if
</bodyText>
<listItem confidence="0.990432333333333">
• e2j is a pronoun reference of the center e1i
• e2j is a reiteration of e1i
• e2j and e1i are semantically related
</listItem>
<bodyText confidence="0.9810506">
Pronoun reference In this approach, we resolve
the reference automatically by finding more than
one possible center for the reference expression us-
ing Wikipedia. Since the summaries are generated
from news articles, we make a fair assumption that
related articles are present in Wikipedia. We en-
sure that the correct center is one among the pos-
sible centers through which 5k+1 and 5k+2 might
be strongly connected. Entities with query hits ra-
tio &gt; A are considered as possible centers and entity
e2j is replaced by entities that act as the possible
centers. Since the chain with the identified correct
center is likely to have the highest score, our final
lexical chain would contain the correct center.
Query hits(e1i and e2j)
</bodyText>
<equation confidence="0.6566515">
Query hit ratio =
Query hits(e1i)
</equation>
<bodyText confidence="0.9993301">
Reiteration Generally, an entity with a determiner
can be treated as reiteration expression but not vice
versa. Therefore, we check whether e2j is actually
a reiteration expression or not, using query hits on
Wikipedia. If Query hits (e2j) &gt; Q then we con-
sider it to be a reiteration expression. A reiterating
expression of a named entity is generally a common
noun that occurs in many documents. After identify-
ing a reiteration expression we estimate relatedness
using semantic relatedness approach.
</bodyText>
<figureCaption confidence="0.976755">
Figure 2: Viterbi trace for identifying lexical chain
</figureCaption>
<page confidence="0.995382">
10
</page>
<bodyText confidence="0.999949523809524">
Semantic relatedness By using query hits over
Wikipedia we estimate the semantic relatedness of
two entities. Such an approach has been previously
attempted in (Strube and Ponzetto, 2006). Based on
our experiments on grammaticality 2.2, classifying
into 5 classes is better suited for evaluation tasks,
hence we follow suit and classify semantic related-
ness into 5 classes. These classes indicate how se-
mantically related the entities are. Each class is as-
signed a value that is given to the hits which fall into
the class. For example, if query hits lie in the range
(-y1, -y2) or if query hit ratio is &gt; � then it falls into
class k and is assigned a score equal to k.
Now that we have computed semantic connect-
edness between adjacent sentences using the meth-
ods explained above, we identify the output node
with maximum score (node V2 in Figure 2). This
node with best score is selected and by backtack-
ing the Viterbi path we generate the lexical chain for
the summary. The constants A, -y1, -y2and� are deter-
mined based on empirical tuning.
</bodyText>
<subsectionHeader confidence="0.99942">
3.2 Coherence
</subsectionHeader>
<bodyText confidence="0.999964285714286">
We estimate coherence of the summary by estimat-
ing how the sentences stick together and the seman-
tic relevance of their collocation. In a sentence, the
semantic relatedness of entities with the focus esti-
mates score for the meaningfulness of the sentence,
and the average score of all the sentences estimates
the coherence of the summary.
</bodyText>
<equation confidence="0.997959166666667">
EN i�1G(si)
C(Summary) _
N
Ek�1
j�1 H(Q(F and eij))
k
</equation>
<bodyText confidence="0.997159">
Where C(Summary) is the coherence of sum-
mary Summary, and G(si) is the semantic relat-
edness of a sentence si in Summary, while Q(q)
denotes the number of query hits of query q. F is
the focus of si and eij is an entity in si, and H(Q)
is the score of class into which query falls.
</bodyText>
<sectionHeader confidence="0.997797" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.997796">
evaluations. We use 3 types of correlation evalu-
ations — Spearman’s Rank Correlation, Pearson’s
Correlation and Kendall’s Tau — each describing
some aspect of ordering problems.
We used reference summaries from TAC 2008,
2009 for the reference corpus and the experiments
described were tested on DUC 2007 query-focused
multi-document summarization datasets which have
45 topics and 32 system summaries for each topic
apart from 4 human reference summaries.
Table 2 shows the system level correlations of
our approaches to grammaticality assessment with
that of human ratings. We have used four base-
line approaches: AverageNPs, AverageVPs, Aver-
ageSBARs and AverageParseTreeHeight. Our ap-
proaches constitute of the following runs: Ngram
(POS), Ngram (POS-Chunk), Class (POS 2 level),
Class (POS-Chunk 2 level), Hybrid (POS), Hybrid
(POS-Chunk).
</bodyText>
<table confidence="0.999365923076923">
RUN Spearman’s p Pearson’s r Kendall’s T
Baselines
AverageNPs 0.1971 0.2378 0.1577
AverageSBARs 0.2923 0.4167 0.2138
AverageVPs 0.3118 0.3267 0.2225
ParseTreeHeight 0.2483 0.3759 0.1922
Our experiments
Ngram (POS) 0.7366 0.7411 0.5464
Ngram (POS+Chunk) 0.7247 0.6903 0.5421
Class (POS 2level) 0.7168 0.7592 0.5464
Class (POS+Chunk 2level) 0.7061 0.7409 0.5290
Hybrid (POS) 0.7273 0.7845 0.5205
Hybrid (POS+Chunk) 0.7733 0.7485 0.5810
</table>
<tableCaption confidence="0.999428">
Table 2: System level correlations of automated and man-
ual metrics for grammaticality.
</tableCaption>
<table confidence="0.985578428571429">
Experiments
Ngram (POS) 0.4319 0.4171 0.3165
Ngram (POS+Chunk) 0.4132 0.4086 0.3124
Class (POS 2level) 0.3022 0.3036 0.2275
Class (POS+Chunk 2level) 0.2698 0.2650 0.2015
Hybrid (POS) 0.3652 0.3483 0.2747
Hybrid (POS+Chunk) 0.3351 0.3083 0.2498
</table>
<tableCaption confidence="0.997546">
Table 3: Summary level correlations of automated and manual met-
rics for grammaticality .
</tableCaption>
<figure confidence="0.916463">
RUN
Human Grammaticality rating
Ngram(POS)
RUN
Kendall’s T
Pearson’s r
Spearman’s p
G(si) _
Spearman’s p Pearson’s r Kendall’s T
Baselines
0.5546 0.6034 0.4152
0.3236 0.4765 0.2229
Experiments
Our coherence model 0.7133 0.5379 0.5173
</figure>
<bodyText confidence="0.999201666666667">
This paper deals with methods that imitate manual
evaluation metric for grammaticality and structure
and coherence by producing a score for each sum-
mary. An evaluation of these new summarization
evaluation metrics is based on how well the system
rankings produced by them correlate with manual
</bodyText>
<tableCaption confidence="0.7842755">
Table 4: System level correlations of automated and manual metrics
for coherence .
</tableCaption>
<bodyText confidence="0.88072425">
Table 4 shows the system level correlations of
our approach to structure and coherence assessment
with that of human ratings. As mentioned earlier in
Section 1, human ratings for grammaticality and our
</bodyText>
<page confidence="0.99088">
11
</page>
<table confidence="0.97708625">
Spearman’s p Pearson’s r Kendall’s T
RUN
Baselines
Human Grammaticality rating 0.5979 0.6463 0.4360
Human Coherence rating 0.9400 0.9108 0.8196
Ngram(POS) 0.4336 0.6578 0.3175
Our coherence model 0.5900 0.5331 0.4125
ROUGE-2 0.3574 0.4237 0.2681
</table>
<tableCaption confidence="0.9800765">
Table 5: System level correlations of automated and manual metrics
forfocus
</tableCaption>
<bodyText confidence="0.999930545454545">
best performing system for grammaticality are used
as baselines for structure and coherence assessment.
Again, like we previously mentioned, focus can be
easily characterized using structure and coherence,
and to an extent the grammatical well-formedness.
Also the focus of a summary is also dependent on
content of the summary. Hence, we use ROUGE-
2, manual rating for grammaticality, manual rating
for coherence, and our approaches to both grammat-
icality and structure and coherence as baselines as
shown in Table 5.
</bodyText>
<sectionHeader confidence="0.997851" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999979162162162">
In this paper, we addressed the problem of identi-
fying the degree of acceptance of grammatical for-
mations at sentence level using surface features like
Ngrams probabilities (in Section 2.1), and trigrams
based class Ngrams (in Section 2.2) and a hybrid
model using both Ngram and Class model (in Sec-
tion 2.3), on the POS-tag sequences and POS-chunk-
tag sequences which have produced impressive re-
sults improving upon our previous work.
Our approaches have produced high correlations
to human judgment on grammaticality. Results in
Table 2 show that the Hybrid approach on the POS-
Chunk tag sequences outperforms all the other ap-
proaches. Our approaches to grammaticality assess-
ment have performed decently at pair-wise ranking
of summaries, shown by correlations of the order of
0.4 for many runs. This correlation is of the same
order as that of similar figure for content evaluations
using ROUGE and Basic Elements.
Table 4 shows that our approach to the ‘structure
and coherence’ assessment outperforms the base-
lines set and has an impressive correlation with man-
ual ratings. From Table 5 we found that grammati-
cality is a good indicator of focus while we also ob-
serve that structure and coherence forms a strong
alternative to focus.
The focus of this paper was on providing a com-
plete picture on capturing the grammaticality as-
pects of readability of a summary using relatively
shallow features as POS-tags and POS-Chunk-tags.
We used lexical chains to capture structure and co-
herence of summaries, whose performance also cor-
related with focus of summaries. None of the five
aspects of readability are completely independent of
each other, and by addressing the individual criteria
for evaluation we aim to achieve overall appreciation
of readability of summary.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914022727273">
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In ACL.
Thorsten Brants. 2000. Tnt: a statistical part-of-speech
tagger. In Proceedings of the sixth conference on
Applied natural language processing, pages 224–231,
Morristown, NJ, USA. Association for Computational
Linguistics.
Jieun Chae and Ani Nenkova. 2009. Predicting the
fluency of text with shallow structural features: Case
studies of machine translation and human-written text.
In EACL, pages 139–147. The Association for Com-
puter Linguistics.
Rudolf Flesch. 1948. A new readability yardstick. Jour-
nal of Applied Psychology, 32:221–233.
M.A.K Halliday and Ruqayia Hasan. 1976. Longman
publishers.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In HLT-NAACL 2004: Main
Proceedings, pages 185–192, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
Lynette Hirschman and Inderjeet Mani. 2001. Evalua-
tion.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In the proceedings of ACL
Workshop on Text Summarization Branches Out. ACL.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Comput. Linguist., 17(1):21–48.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In ACL. The Association for Computer
Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In EMNLP, pages 186–195. ACL.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In 21. AAAI / 18. IAAI 2006. AAAI Press,
july.
Ravikiran Vadlapudi and Rahul Katragadda. 2010.
Quantitative evaluation of grammaticality of sum-
maries. In CICLing.
</reference>
<page confidence="0.998462">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.163653">
<title confidence="0.997718">On Automated Evaluation of Readability of Capturing Grammaticality, Focus, Structure and Coherence</title>
<author confidence="0.861191">Raviliiran</author>
<affiliation confidence="0.852945">Language Technologies Research</affiliation>
<address confidence="0.492171">IIIT Hyderabad</address>
<email confidence="0.934029">ravikiranv@research.iiit.ac.in</email>
<author confidence="0.906886">Rahul Katragadda</author>
<affiliation confidence="0.7465305">Language Technologies Research IIIT Hyderabad</affiliation>
<email confidence="0.849389">rahulk@research.iiit.ac.in</email>
<abstract confidence="0.999528958333333">Readability of a summary is usually graded on five aspects of readability: gramand clarity In the context of automated metrics for evaluation of summary quality, content evaluations have been presented through the last decade and continue to evolve, however a careful examination of readability aspects of summary quality has not been as exhaustive. In this paper we explore alternative evaluation metrics for and and structhat are able to strongly correlate with manual ratings. Our results establish that our methods are able to perform pair-wise ranking of summaries based on grammaticality, as strongly as ROUGE is able to distinguish for content evaluations. We observed that none of the five aspects of readability are independent of each other, and hence by addressing the individual criterion of evaluation we aim to achieve automated appreciation of readability of summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3178" citStr="Barzilay and Lapata, 2005" startWordPosition="465" endWordPosition="468">suming and expensive doesn’t help system developers — who appreciate fast, reliable and most importantly automated evaluation metric. So despite having a sound manual evaluation methodology for readability, there is an need for reliable automatic metrics. All the early approaches like Flesch Reading Ease (Flesch, 1948) were developed for general texts and none of these techniques have tried to characterize themselves as approximations to grammaticality or structure or coherence. In assessing readability of summaries, there hasn’t been much of dedicated analysis with text summaries, except in (Barzilay and Lapata, 2005) where local coherence was modeled for text summaries and in (Vadlapudi and Katragadda, 2010) where grammaticality of text summaries were explored. In a marginally related work in Natural Language Generation, (Mutton et al., 2007) addresses sentence level fluency regardless of content, while recent work in (Chae and Nenkova, 2009) gives a systematic study on how syntactic features were able to distinguish machine generated translations from human translations. In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity c</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Tnt: a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the sixth conference on Applied natural language processing,</booktitle>
<pages>224--231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14722" citStr="Brants, 2000" startWordPosition="2352" endWordPosition="2353">sentences are closely related, a prerequisite for being a part of a lexical chain, the summary is said to be coherent. In this paper, the semantic relatedness of topic focus and content is captured using Wikipedia as elaborated in Section 3.1 of this paper. 3.1 Construction of lexical chains In this approach, we identify the strongest lexical chain possible which would capture the structure of the summary. We define this problem of finding the strongest possible lexical chain as that of finding the best possible parts-of-speech tag sequence for a sentence using the Viterbi algorithm shown in (Brants, 2000). The entities (noun phrases) of each sentence are the nodes and transition probabilities are defined as relatedness score (Figure 2). The strongest lexical chain would have the highest score than other possible lexical chains obtained. Consider sentence 5k with entity set (e11, e12, e13.... e1n) and sentence 5k+1 with entity set (e21, e22, e23, . . . e2.). Sentences 5k and 5k+1 are said to be strongly connected if there exists entities e1i E 5k and e2j E 5k+1 that are closely related. e1i and e2j are considered closely related if • e2j is a pronoun reference of the center e1i • e2j is a reite</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. Tnt: a statistical part-of-speech tagger. In Proceedings of the sixth conference on Applied natural language processing, pages 224–231, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jieun Chae</author>
<author>Ani Nenkova</author>
</authors>
<title>Predicting the fluency of text with shallow structural features: Case studies of machine translation and human-written text.</title>
<date>2009</date>
<booktitle>In EACL,</booktitle>
<pages>139--147</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="3510" citStr="Chae and Nenkova, 2009" startWordPosition="515" endWordPosition="518"> for general texts and none of these techniques have tried to characterize themselves as approximations to grammaticality or structure or coherence. In assessing readability of summaries, there hasn’t been much of dedicated analysis with text summaries, except in (Barzilay and Lapata, 2005) where local coherence was modeled for text summaries and in (Vadlapudi and Katragadda, 2010) where grammaticality of text summaries were explored. In a marginally related work in Natural Language Generation, (Mutton et al., 2007) addresses sentence level fluency regardless of content, while recent work in (Chae and Nenkova, 2009) gives a systematic study on how syntactic features were able to distinguish machine generated translations from human translations. In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 7–12, Los Angeles, California, June 2010. c�20</context>
</contexts>
<marker>Chae, Nenkova, 2009</marker>
<rawString>Jieun Chae and Ani Nenkova. 2009. Predicting the fluency of text with shallow structural features: Case studies of machine translation and human-written text. In EACL, pages 139–147. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Flesch</author>
</authors>
<title>A new readability yardstick.</title>
<date>1948</date>
<journal>Journal of Applied Psychology,</journal>
<pages>32--221</pages>
<contexts>
<context position="2872" citStr="Flesch, 1948" startWordPosition="420" endWordPosition="421">s are: grammaticality, Non-Redundancy, Referential Clarity, Focus and Structure and Coherence. Hence readability assessment is a manual method where expert assessors give a rating for each summary on the Likert Scale for each of the linguistic quality markers. Manual evaluation being time-consuming and expensive doesn’t help system developers — who appreciate fast, reliable and most importantly automated evaluation metric. So despite having a sound manual evaluation methodology for readability, there is an need for reliable automatic metrics. All the early approaches like Flesch Reading Ease (Flesch, 1948) were developed for general texts and none of these techniques have tried to characterize themselves as approximations to grammaticality or structure or coherence. In assessing readability of summaries, there hasn’t been much of dedicated analysis with text summaries, except in (Barzilay and Lapata, 2005) where local coherence was modeled for text summaries and in (Vadlapudi and Katragadda, 2010) where grammaticality of text summaries were explored. In a marginally related work in Natural Language Generation, (Mutton et al., 2007) addresses sentence level fluency regardless of content, while r</context>
</contexts>
<marker>Flesch, 1948</marker>
<rawString>Rudolf Flesch. 1948. A new readability yardstick. Journal of Applied Psychology, 32:221–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqayia Hasan</author>
</authors>
<date>1976</date>
<note>Longman publishers.</note>
<contexts>
<context position="12374" citStr="Halliday and Hasan, 1976" startWordPosition="1967" endWordPosition="1970">r approaches, both the POS-tag sequences and POS-Chunk-tag sequences could be used to estimate the grammaticality of a sentence, and hence the summary. These two runs are called ‘Hybrid (POS)’ and ‘Hybrid (POSChunk)’, respectively. 3 Structure and Coherence Most automated systems generate summaries from multiple documents by extracting relevant sentences and concatenating them. For these summaries to be comprehensible they must also be cohesive and coherent, apart from being content bearing and grammatical. Lexical cohesion is a type of cohesion that arises from links between words in a text (Halliday and Hasan, 1976).A Lexical chain is a sequence of 9 such related words spanning a unit of text. Lexical cohesion along with presuppositions and implications with world knowledge achieves coherence in texts. Hence, coherence is what makes text semantically meaningful, and in this paper, we also attempt to automate the evaluation of the “structure and coherence” of summaries. We capture the structure or lexical cohesion of a summary by constructing a lexical chain that spans the summary. The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or re</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M.A.K Halliday and Ruqayia Hasan. 1976. Longman publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Claudia Gentile</author>
</authors>
<title>Evaluating multiple aspects of coherence in student essays.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>185--192</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="4234" citStr="Higgins et al., 2004" startWordPosition="618" endWordPosition="621">ions from human translations. In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 7–12, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics While studying the coherence patterns in student essays, (Higgins et al., 2004) identified that grammatical errors affect the overall expressive quality of the essays. In this paper, due to the lack of an appropriate baseline and due to the interesting-ness of the above observation we use metrics for grammaticality as a baseline measure for structure and coherence. Focus of a summary, is the only aspect of readability that relies to a larger extent on the content of the summary. In this paper, we use Recall Oriented Understudy of Gisting Evaluation (ROUGE) (Lin, 2004) based metrics as one of the baselines to capture focus in a summary. 2 Summary Grammaticality Grammatica</context>
</contexts>
<marker>Higgins, Burstein, Marcu, Gentile, 2004</marker>
<rawString>Derrick Higgins, Jill Burstein, Daniel Marcu, and Claudia Gentile. 2004. Evaluating multiple aspects of coherence in student essays. In HLT-NAACL 2004: Main Proceedings, pages 185–192, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Inderjeet Mani</author>
</authors>
<date>2001</date>
<publisher>Evaluation.</publisher>
<contexts>
<context position="1933" citStr="Hirschman and Mani, 2001" startWordPosition="278" endWordPosition="281">of summaries. 1 Introduction Automated text summarization deals with both the problem of identifying relevant snippets of information and presenting it in a pertinent format. Automated evaluation is crucial to automatic text summarization to be used both to rank multiple participant systems in shared tasks1, and to developers whose goal is to improve the summarization systems. Summarization evaluations help in the creation of reusable resources and infrastructure; it sets up the stage for comparison and replication of results by introducing an element of competition to produce better results (Hirschman and Mani, 2001). 1The summarization tracks at Text Analysis Conference (TAC) 2009, 2008 and its predecessors at Document Understanding Conferences (DUC). 7 Readability or Fluency of a summary is categorically measured based on a set of linguistic quality questions that manual assessors answer for each summary. The linguistic quality markers are: grammaticality, Non-Redundancy, Referential Clarity, Focus and Structure and Coherence. Hence readability assessment is a manual method where expert assessors give a rating for each summary on the Likert Scale for each of the linguistic quality markers. Manual evalua</context>
</contexts>
<marker>Hirschman, Mani, 2001</marker>
<rawString>Lynette Hirschman and Inderjeet Mani. 2001. Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In the proceedings of ACL Workshop on Text Summarization Branches Out. ACL.</booktitle>
<contexts>
<context position="4729" citStr="Lin, 2004" startWordPosition="706" endWordPosition="707">ation for Computational Linguistics While studying the coherence patterns in student essays, (Higgins et al., 2004) identified that grammatical errors affect the overall expressive quality of the essays. In this paper, due to the lack of an appropriate baseline and due to the interesting-ness of the above observation we use metrics for grammaticality as a baseline measure for structure and coherence. Focus of a summary, is the only aspect of readability that relies to a larger extent on the content of the summary. In this paper, we use Recall Oriented Understudy of Gisting Evaluation (ROUGE) (Lin, 2004) based metrics as one of the baselines to capture focus in a summary. 2 Summary Grammaticality Grammaticality of summaries, in this paper, is defined based on the grammaticality of its sentences, since it is more a sentence level syntactic property. A sentence can either be grammatically correct or grammatically incorrect. The problem of grammatical incorrectness should not occur in summaries being evaluated because they are generated mostly by extract based summarization systems. But as the distribution of grammaticality scores in Table 1 shows, there are a lot of summaries that obtain very l</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In the proceedings of ACL Workshop on Text Summarization Branches Out. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Comput. Linguist.,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="13043" citStr="Morris and Hirst, 1991" startWordPosition="2072" endWordPosition="2075">ed words spanning a unit of text. Lexical cohesion along with presuppositions and implications with world knowledge achieves coherence in texts. Hence, coherence is what makes text semantically meaningful, and in this paper, we also attempt to automate the evaluation of the “structure and coherence” of summaries. We capture the structure or lexical cohesion of a summary by constructing a lexical chain that spans the summary. The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst, 1991). A center-reference relation exists if an entity in a sentence is a reference to center in adjacent sentence. Identifying centers of reference expressions can be done using a co-reference resolution tool. Performance of co-reference resolution tools in summaries, being evaluated, is not as good as their performance on generic texts. Semantic relatedness relation cannot be captured by using tools like Wordnet because they are not very exhaustive and hence are not effective. We use a much richer knowledge base to define this relation – Wikipedia. Coherence of a summary is modelled by its struct</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Comput. Linguist., 17(1):21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Mutton</author>
<author>Mark Dras</author>
<author>Stephen Wan</author>
<author>Robert Dale</author>
</authors>
<title>Gleu: Automatic evaluation of sentencelevel fluency.</title>
<date>2007</date>
<booktitle>In ACL. The Association</booktitle>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="3408" citStr="Mutton et al., 2007" startWordPosition="499" endWordPosition="502"> automatic metrics. All the early approaches like Flesch Reading Ease (Flesch, 1948) were developed for general texts and none of these techniques have tried to characterize themselves as approximations to grammaticality or structure or coherence. In assessing readability of summaries, there hasn’t been much of dedicated analysis with text summaries, except in (Barzilay and Lapata, 2005) where local coherence was modeled for text summaries and in (Vadlapudi and Katragadda, 2010) where grammaticality of text summaries were explored. In a marginally related work in Natural Language Generation, (Mutton et al., 2007) addresses sentence level fluency regardless of content, while recent work in (Chae and Nenkova, 2009) gives a systematic study on how syntactic features were able to distinguish machine generated translations from human translations. In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. Proceedings</context>
</contexts>
<marker>Mutton, Dras, Wan, Dale, 2007</marker>
<rawString>Andrew Mutton, Mark Dras, Stephen Wan, and Robert Dale. 2007. Gleu: Automatic evaluation of sentencelevel fluency. In ACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>186--195</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3694" citStr="Pitler and Nenkova, 2008" startWordPosition="541" endWordPosition="544">ries, there hasn’t been much of dedicated analysis with text summaries, except in (Barzilay and Lapata, 2005) where local coherence was modeled for text summaries and in (Vadlapudi and Katragadda, 2010) where grammaticality of text summaries were explored. In a marginally related work in Natural Language Generation, (Mutton et al., 2007) addresses sentence level fluency regardless of content, while recent work in (Chae and Nenkova, 2009) gives a systematic study on how syntactic features were able to distinguish machine generated translations from human translations. In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 7–12, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics While studying the coherence patterns in student essays, (Higgins et al., 2004) identified that grammatical errors affect the overall expre</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In EMNLP, pages 186–195. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Wikirelate! computing semantic relatedness using wikipedia.</title>
<date>2006</date>
<booktitle>In 21. AAAI / 18. IAAI</booktitle>
<publisher>AAAI Press,</publisher>
<contexts>
<context position="16882" citStr="Strube and Ponzetto, 2006" startWordPosition="2710" endWordPosition="2713">re, we check whether e2j is actually a reiteration expression or not, using query hits on Wikipedia. If Query hits (e2j) &gt; Q then we consider it to be a reiteration expression. A reiterating expression of a named entity is generally a common noun that occurs in many documents. After identifying a reiteration expression we estimate relatedness using semantic relatedness approach. Figure 2: Viterbi trace for identifying lexical chain 10 Semantic relatedness By using query hits over Wikipedia we estimate the semantic relatedness of two entities. Such an approach has been previously attempted in (Strube and Ponzetto, 2006). Based on our experiments on grammaticality 2.2, classifying into 5 classes is better suited for evaluation tasks, hence we follow suit and classify semantic relatedness into 5 classes. These classes indicate how semantically related the entities are. Each class is assigned a value that is given to the hits which fall into the class. For example, if query hits lie in the range (-y1, -y2) or if query hit ratio is &gt; � then it falls into class k and is assigned a score equal to k. Now that we have computed semantic connectedness between adjacent sentences using the methods explained above, we id</context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Paolo Ponzetto. 2006. Wikirelate! computing semantic relatedness using wikipedia. In 21. AAAI / 18. IAAI 2006. AAAI Press, july.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravikiran Vadlapudi</author>
<author>Rahul Katragadda</author>
</authors>
<title>Quantitative evaluation of grammaticality of summaries.</title>
<date>2010</date>
<booktitle>In CICLing.</booktitle>
<contexts>
<context position="3271" citStr="Vadlapudi and Katragadda, 2010" startWordPosition="479" endWordPosition="482">most importantly automated evaluation metric. So despite having a sound manual evaluation methodology for readability, there is an need for reliable automatic metrics. All the early approaches like Flesch Reading Ease (Flesch, 1948) were developed for general texts and none of these techniques have tried to characterize themselves as approximations to grammaticality or structure or coherence. In assessing readability of summaries, there hasn’t been much of dedicated analysis with text summaries, except in (Barzilay and Lapata, 2005) where local coherence was modeled for text summaries and in (Vadlapudi and Katragadda, 2010) where grammaticality of text summaries were explored. In a marginally related work in Natural Language Generation, (Mutton et al., 2007) addresses sentence level fluency regardless of content, while recent work in (Chae and Nenkova, 2009) gives a systematic study on how syntactic features were able to distinguish machine generated translations from human translations. In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. We us</context>
<context position="6455" citStr="Vadlapudi and Katragadda, 2010" startWordPosition="980" endWordPosition="983"> system summaries In this paper, the degree of acceptance of sentence structures is estimated using language models trained on a corpus of human written summaries. Considering the sentence structures in reference summaries as the best accepted ones (with highest degree of acceptance), we estimate the degree of acceptance of sentences in system generated summaries by quantifying the amount of similarity/digression from the references using the language models. The structure of the sentences can be represented by sequences of parts-of-speech (POS) tags and chunk tags. Our previous observations (Vadlapudi and Katragadda, 2010) show that the tagset size plays an important role in determining the degree of acceptance. In this paper, we combine the two features of a sentence — the POS-tag sequence and chunk-tag sequence — to generate the POS-Chunktag training corpus. Some aspects of grammatical structure are well identifiable at the level of POS tags, while some other aspects (such as distinguishing between appositives and lists for eg.) need the power of chunk tags, the combination of these two tag-sequences provides the power of both. Hence, the following approaches use probabilistic models, learned on POS tag corpu</context>
<context position="9671" citStr="Vadlapudi and Katragadda, 2010" startWordPosition="1514" endWordPosition="1517">a n-level classification problem. Grammaticality of summaries is manually scored on a scale of 1 to 5, which means the summaries are classified into 5 classes. We assume that each sentence of the summary is also rated on a similar scale which cumulatively decides to which class the summary must belong. In our approach, sentences are classified into 5 classes on the basis of frequencies of underlying grammar rules (trigram) by defining class boundaries on frequencies. Hence, the cumulative score of the rules estimate the score of grammaticality of a sentence and inturn the summary. Similar to (Vadlapudi and Katragadda, 2010), trigrams are classified into 5 classes C1, C2, C3, C� and C5 and each class is assigned a score on a similar scale (Vjscore(Cj) = j) and class boundaries are estimated using the frequencies of trigrams in the training corpus. The most frequent trigram, for example, would fall into class C5. POS-Class sequences are generated from POS-tag sequences using class boundaries as shown in Figure 1. This is the first level of classification. Figure 1: Two-level class model Like the first level of classification, a series of classifications are performed upto ‘k’ levels. At each level we apply the sco</context>
</contexts>
<marker>Vadlapudi, Katragadda, 2010</marker>
<rawString>Ravikiran Vadlapudi and Rahul Katragadda. 2010. Quantitative evaluation of grammaticality of summaries. In CICLing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>