<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032879">
<title confidence="0.996615">
Question Detection in Spoken Conversations Using Textual Conversations
</title>
<author confidence="0.994639">
Anna Margolis and Mari Ostendorf
</author>
<affiliation confidence="0.9982705">
Department of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.948747">
Seattle, WA, USA
</address>
<email confidence="0.999839">
{amargoli,mo}@ee.washington.edu
</email>
<sectionHeader confidence="0.998607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956615384615">
We investigate the use of textual Internet con-
versations for detecting questions in spoken
conversations. We compare the text-trained
model with models trained on manually-
labeled, domain-matched spoken utterances
with and without prosodic features. Over-
all, the text-trained model achieves over 90%
of the performance (measured in Area Under
the Curve) of the domain-matched model in-
cluding prosodic features, but does especially
poorly on declarative questions. We describe
efforts to utilize unlabeled spoken utterances
and prosodic features via domain adaptation.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999758607142857">
Automatic speech recognition systems, which tran-
scribe words, are often augmented by subsequent
processing for inserting punctuation or labeling
speech acts. Both prosodic features (extracted from
the acoustic signal) and lexical features (extracted
from the word sequence) have been shown to be
useful for these tasks (Shriberg et al., 1998; Kim
and Woodland, 2003; Ang et al., 2005). However,
access to labeled speech training data is generally
required in order to use prosodic features. On the
other hand, the Internet contains large quantities of
textual data that is already labeled with punctua-
tion, and which can be used to train a system us-
ing lexical features. In this work, we focus on ques-
tion detection in the Meeting Recorder Dialog Act
corpus (MRDA) (Shriberg et al., 2004), using text
sentences with question marks in Wikipedia “talk”
pages. We compare the performance of a ques-
tion detector trained on the text domain using lex-
ical features with one trained on MRDA using lex-
ical features and/or prosodic features. In addition,
we experiment with two unsupervised domain adap-
tation methods to incorporate unlabeled MRDA ut-
terances into the text-based question detector. The
goal is to use the unlabeled domain-matched data to
bridge stylistic differences as well as to incorporate
the prosodic features, which are unavailable in the
labeled text data.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999876761904762">
Question detection can be viewed as a subtask of
speech act or dialogue act tagging, which aims
to label functions of utterances in conversations,
with categories as question/statement/backchannel,
or more specific categories such as request or com-
mand (e.g., Core and Allen (1997)). Previous work
has investigated the utility of various feature types;
Boakye et al. (2009), Shriberg et al. (1998) and Stol-
cke et al. (2000) showed that prosodic features were
useful for question detection in English conversa-
tional speech, but (at least in the absence of recog-
nition errors) most of the performance was achieved
with words alone. There has been some previous
investigation of domain adaptation for dialogue act
classification, including adaptation between: differ-
ent speech corpora (MRDA and Switchboard) (Guz
et al., 2010), speech corpora in different languages
(Margolis et al., 2010), and from a speech domain
(MRDA/Switchboard) to text domains (emails and
forums) (Jeong et al., 2009). These works did
not use prosodic features, although Venkataraman
</bodyText>
<page confidence="0.969956">
118
</page>
<note confidence="0.5893235">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 118–124,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999897614678899">
et al. (2003) included prosodic features in a semi- we compare two domain adaptation approaches to
supervised learning approach for dialogue act la- utilize unlabeled speech data: bootstrapping, and
beling within a single spoken domain. Also rele- Blitzer et al.’s Structural Correspondence Learning
vant is the work of Moniz et al. (2011), who com- (SCL) (Blitzer et al., 2006). SCL is a feature-
pared question types in different Portuguese cor- learning method that uses unlabeled data from both
pora, including text and speech. For question de- domains. Although it has been applied to several
tection on speech, they compared performance of a NLP tasks, to our knowledge we are the first to apply
lexical model trained with newspaper text to models SCL to both lexical and prosodic features in order to
trained with speech including acoustic and prosodic adapt from text to speech.
features, where the speech-trained model also uti- 3 Experiments
lized the text-based model predictions as a feature. 3.1 Data
They reported that the lexical model mainly iden- The Wiki talk pages consist of threaded posts by
tified wh questions, while the speech data helped different authors about a particular Wikipedia entry.
identify yes-no and tag questions, although results While these lack certain properties of spontaneous
for specific categories were not included. speech (such as backchannels, disfluencies, and in-
Question detection is related to the task of auto- terruptions), they are more conversational than news
matic punctuation annotation, for which the contri- articles, containing utterances such as: “Are you se-
butions of lexical and prosodic features have been rious?” or “Hey, that’s a really good point.” We
explored in other works, e.g. Christensen et al. first cleaned the posts (to remove URLs, images,
(2001) and Huang and Zweig (2002). Kim and signatures, Wiki markup, and duplicate posts) and
Woodland (2003) and Liu et al. (2006) used auxil- then performed automatic segmentation of the posts
iary text corpora to train lexical models for punc- into sentences using MXTERMINATOR (Reynar
tuation annotation or sentence segmentation, which and Ratnaparkhi, 1997). We labeled each sentence
were used along with speech-trained prosodic mod- ending in a question mark (followed optionally by
els; the text corpora consisted of broadcast news or other punctuation) as a question; we also included
telephone conversation transcripts. More recently, parentheticals ending in question marks. All other
Gravano et al. (2009) used lexical models built from sentences were labeled as non-questions. We then
web news articles on broadcast news speech, and removed all punctuation and capitalization from the
compared their performance on written news; Shen resulting sentences and performed some additional
et al. (2009) trained models on an online encyclo- text normalization to match the MRDA transcripts,
pedia, for punctuation annotation of news podcasts. such as number and date expansion.
Web text was also used in a domain adaptation For the MRDA corpus, we use the manually-
strategy for prosodic phrase prediction in news text transcribed sentences with utterance time align-
(Chen et al., 2010). ments. The corpus has been hand-annotated with
In our work, we focus on spontaneous conversa- detailed dialogue act tags, using a hierarchical la-
tional speech, and utilize a web text source that is beling scheme in which each utterance receives one
somewhat matched in style: both domains consist of “general” label plus a variable number of “specific”
goal-directed multi-party conversations. We focus labels (Dhillon et al., 2004). In this work we are
specifically on question detection in pre-segmented only looking at the problem of discriminating ques-
utterances. This differs from punctuation annota- tions from non-questions; we consider as questions
tion or segmentation, which is usually seen as a se- all complete utterances labeled with one of the gen-
quence tagging or classification task at word bound- eral labels wh, yes-no, open-ended, or, or-after-yes-
aries, and uses mostly local features. Our focus also no, or rhetorical question. (To derive the question
allows us to clearly analyze the performance on dif- categories below, we also consider the specific la-
ferent question types, in isolation from segmenta- bels tag and declarative, which are appended to one
tion issues. We compare performance of textual- of the general labels.) All remaining utterances, in-
and speech-trained lexical models, and examine the
detection accuracy of each question type. Finally,
119
cluding backchannels and incomplete questions, are and number of words per frame. All 16 features
considered as non-questions, although we removed were z-normalized using speaker-level parameters,
utterances that are very short (less than 200ms), have or gender-level parameters if the speaker had less
no transcribed words, or are missing segmentation than 10 utterances.
times or dialogue act label. We performed minor text For all experiments we used logistic regression
normalization on the transcriptions, such as mapping models trained with the LIBLINEAR package (Fan
all word fragments to a single token. et al., 2008). Prosodic and lexical features were
The Wiki training set consists of close to 46k combined by concatenation into a single feature vec-
utterances, with 8.0% questions. We derived an tor; prosodic features and the number-of-words were
MRDA training set of the same size from the train- z-normalized to place them roughly on the same
ing division of the original corpus; it consists of scale as the binary ngram features. (We substituted 0
6.6% questions. For the adaptation experiments, we for missing prosody features due to, e.g., no voiced
used the full MRDA training set of 72k utterances frames detected, segmentation errors, utterance too
as unlabeled adaptation data. We used two meet- short.) Our setup is similar to (Surendran and
ings (3k utterances) from the original MRDA devel- Levow, 2006), who combined ngram and prosodic
opment set for model selection and parameter tun- features for dialogue act classification using a lin-
ing. The remaining meetings (in the original devel- ear SVM. Since ours is a detection problem, with
opment and test divisions; 26k utterances) were used questions much less frequent than non-questions,
as our test set. we present results in terms of ROC curves, which
3.2 Features and Classifier were computed from the probability scores of the
Lexical features consisted of unigrams through tri- classifier. The cost parameter C was tuned to opti-
grams including start- and end-utterance tags, repre- mize Area Under the Curve (AUC) on the develop-
sented as binary features (presence/absence), plus a ment set (C = 0.01 for prosodic features only and
total-number-of-words feature. All ngram features C = 0.1 in all other cases.)
were required to occur at least twice in the training 3.3 Baseline Results
set. The MRDA training set contained on the order Figure 1 shows the ROC curves for the baseline
of 65k ngram features while the Wiki training set Wiki-trained lexical system and the MRDA-trained
contained over 205k. Although some previous work systems with different feature sets. Table 2 com-
has used part-of-speech or parse features in related pares performance across different question cate-
tasks, Boakye et al. (2009) showed no clear benefit gories at a fixed false positive rate (16.7%) near the
of these features for question detection on MRDA equal error rate of the MRDA (lex) case. For analy-
beyond the ngram features. sis purposes we defined the categories in Table 2 as
We extracted 16 prosody features from the speech follows: tag includes any yes-no question given the
waveforms defined by the given utterance times, us- additional tag label; declarative includes any ques-
ing stylized F0 contours computed based on S¨onmez tion category given the declarative label that is not
et al. (1998) and Lei (2006). The features are de- a tag question; the remaining categories (yes-no, or,
signed to be useful for detecting questions and are etc.) include utterances in those categories but not
similar or identical to some of those in Boakye et included in declarative or tag. Table 1 gives exam-
al. (2009) or Shriberg et al. (1998). They include: ple sentences for each category.
F0 statistics (mean, stdev, max, min) computed over As expected, the Wiki-trained system does worst
the whole utterance and over the last 200ms; slopes on declarative, which have the syntactic form of
computed from a linear regression to the F0 contour statements. For the MRDA-trained system, prosody
(over the whole utterance and last 200ms); initial alone does best on yes-no and declarative. Along
and final slope values output from the stylizer; ini- with lexical features, prosody is more useful for
tial intercept value from the whole utterance linear declarative, while it appears to be somewhat re-
regression; ratio of mean F0 in the last 400-200ms dundant with lexical features for yes-no. Ideally,
to that in the last 200ms; number of voiced frames; such redundancy can be used together with unla-
120
yes-no did did you do that?
declarative you’re not going to be around
this afternoon?
wh what do you mean um reference
frames?
tag you know?
rhetorical why why don’t we do that?
open-ended do we have anything else to say
about transcription?
or and @frag@ did they use sig-
moid or a softmax type thing?
or-after-YN or should i collect it all?
</bodyText>
<tableCaption confidence="0.918088">
Table 1: Examples for each MRDA question category as
defined in this paper, based on Dhillon et al. (2004).
</tableCaption>
<bodyText confidence="0.95325075">
beled spoken utterances to incorporate prosodic fea-
tures into the Wiki system, which may improve de-
tection of some kinds of questions.
false pos rate
</bodyText>
<figureCaption confidence="0.816338">
Figure 1: ROC curves with AUC values for question de-
</figureCaption>
<bodyText confidence="0.958197666666667">
tection on MRDA; comparison between systems trained
on MRDA using lexical and/or prosodic features, and
Wiki talk pages using lexical features.
</bodyText>
<subsectionHeader confidence="0.995941">
3.4 Adaptation Results
</subsectionHeader>
<bodyText confidence="0.999224625">
For bootstrapping, we first train an initial baseline
classifier using the Wiki training data, then use it to
label MRDA data from the unlabeled adaptation set.
We select the k most confident examples for each
of the two classes and add them to the training set
using the guessed labels, then retrain the classifier
using the new training set. This is repeated for r
rounds. In order to use prosodic features, which are
</bodyText>
<table confidence="0.9998072">
type (count) MRDA MRDA MRDA Wiki
(L+P) (L) (P) (L)
yes-no (526) 89.4 86.1 59.3 77.2
declar. (417) 69.8 59.2 49.4 25.9
wh (415) 95.4 93.0 42.2 92.8
tag (358) 89.7 90.5 26.0 79.1
rhetorical (75) 88.0 90.7 25.3 93.3
open-ended (50) 88.0 92.0 16.0 80.0
or (38) 97.4 100 29.0 89.5
or-after-YN (32) 96.9 96.9 25.0 90.6
</table>
<tableCaption confidence="0.994453">
Table 2: Question detection rates (%) by question type for
</tableCaption>
<bodyText confidence="0.5330434">
each system (L=lexical features, P=prosodic features.)
Detection rates are given at a false positive rate of 16.7%
(starred points in Figure 1), which is the equal error rate
point for the MRDA (L) system. Boldface gives best re-
sult for each type.
</bodyText>
<table confidence="0.999796555555555">
type (count) baseline bootstrap SCL
yes-no (526) 77.2 81.4 83.5
declar. (417) 25.9 30.5 32.1
wh (415) 92.8 92.8 93.5
tag (358) 79.1 79.3 80.7
rhetorical (75) 93.3 88.0 92.0
open-ended (50) 80.0 76.0 80.0
or (38) 89.5 89.5 89.5
or-after-YN (32) 90.6 90.6 90.6
</table>
<tableCaption confidence="0.99966">
Table 3: Adaptation performance by question type, at
</tableCaption>
<bodyText confidence="0.948948421052631">
false positive rate of 16.7% (starred points in Figure 2.)
Boldface indicates adaptation results better than baseline;
italics indicate worse than baseline.
available only in the bootstrapped MRDA data, we
simply add 16 zeros onto the Wiki examples in place
of the missing prosodic features. The values k = 20
and r = 6 were selected on the dev set.
In contrast with bootstrapping, SCL (Blitzer et al.,
2006) uses the unlabeled target data to learn domain-
independent features. SCL has generated much in-
terest lately because of the ability to incorporate fea-
tures not seen in the training data. The main idea is
to use unlabeled data in both domains to learn linear
predictors for many “auxiliary” tasks, which should
be somewhat related to the task of interest. In par-
ticular, if z is a row vector representing the original
feature vector and yz represents the label for auxil-
iary task i, the linear predictor wz is learned to pre-
dict 9z = wz · z′ (where z′ is a modified version of
</bodyText>
<figure confidence="0.997672">
detection rate
0.8
0.6
0.4
0.2
00 0.2 0.4 0.6 0.8 1
1
0.925
0.912
0.833
0.696
</figure>
<figureCaption confidence="0.9618095">
train meetings (lex+pros)
train meetings (lex only)
train meetings (pros only)
train wiki (lex only)
</figureCaption>
<page confidence="0.994803">
121
</page>
<bodyText confidence="0.975220725806452">
x that excludes any features completely predictive
of yz.) The learned predictors for all tasks {wz} are
then collected into the columns of a matrix W, on
which singular value decomposition USVT = W
is performed. Ideally, features that behave simi-
larly across many yz will be represented in the same
singular vector; thus, the auxiliary tasks can tie to-
gether features which may never occur together in
the same example. Projection of the original feature
vector onto the top h left singular vectors gives an
h−dimensional feature vector z - UT��h · x′. The
model is then trained on the concatenated feature
representation [x, z] using the labeled source data.
As auxiliary tasks yz, we identify all initial words
that begin an utterance at least 5 times in each do-
main’s training set, and predict the presence of each
initial word (yz = 0 or 1). The idea of using the
initial words is that they may be related to the inter-
rogative status of an utterance— utterances starting
with “do” or “what” are more often questions, while
those starting with “i” are usually not. There were
about 250 auxiliary tasks. The prediction features x′
used in SCL include all ngrams occuring at least 5
times in the unlabeled Wiki or MRDA data, except
those over the first word, as well as prosody features
(which are zero in the Wiki data.) We tuned h = 100
and the scale factor of z (to 1) on the dev set.
Figure 2 compares the results using the boot-
strapping and SCL approaches, and the baseline un-
adapted Wiki system. Table 3 shows results by ques-
tion type at the fixed false positive point chosen
for analysis. At this point, both adaptation meth-
ods improved detection of declarative and yes-no
questions, although they decreased detection of sev-
eral other types. Note that we also experimented
with other adaptation approaches on the dev set:
bootstrapping without the prosodic features did not
lead to an improvement, nor did training on Wiki
using “fake” prosody features predicted based on
MRDA examples. We also tried a co-training ap-
proach using separate prosodic and lexical classi-
fiers, inspired by the work of Guz et al. (2007) on
semi-supervised sentence segmentation; this led to
a smaller improvement than bootstrapping. Since
we tuned and selected adaptation methods on the
MRDA dev set, we compare to training with the la-
beled MRDA dev (with prosodic features) and Wiki
data together. This gives superior results compared
to adaptation; but note that the adaptation process
did not use labeled MRDA data to train, but merely
for model selection. Analysis of the adapted sys-
tems suggests prosody features are being utilized to
improve performance in both methods, but clearly
the effect is small, and the need to tune parame-
ters would present a challenge if no labeled speech
data were available. Finally, while the benefit from
3k labeled MRDA utterances added to the Wiki ut-
terances is encouraging, we found that most of the
MRDA training utterances (with prosodic features)
had to be added to match the MRDA-only result in
Figure 1, although perhaps training separate lexical
and prosodic models would be useful in this respect.
</bodyText>
<sectionHeader confidence="0.997968" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999708">
This work explored the use of conversational web
text to detect questions in conversational speech.
We found that the web text does especially poorly
on declarative questions, which can potentially be
improved using prosodic features. Unsupervised
adaptation methods utilizing unlabeled speech and
a small labeled development set are shown to im-
prove performance slightly, although training with
the small development set leads to bigger gains.
Our work suggests approaches for combining large
amounts of “naturally” annotated web text with
unannotated speech data, which could be useful in
other spoken language processing tasks, e.g. sen-
tence segmentation or emphasis detection.
</bodyText>
<figure confidence="0.971912538461538">
1
0.884
0.859
0.850
0.833
0.6
0.4
SCL
bootstrap
baseline (no adapt)
include MRDA dev
00 0.2 0.4 0.6 0.8 1
false pos rate
</figure>
<figureCaption confidence="0.9976515">
Figure 2: ROC curves and AUC values for adaptation,
baseline Wiki, and Wiki + MRDA dev.
</figureCaption>
<figure confidence="0.973786">
detection rate
0.8
0.2
</figure>
<page confidence="0.986519">
122
</page>
<sectionHeader confidence="0.98228" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997917238095238">
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classification
in multiparty meetings. In Proc. Int. Conference on
Acoustics, Speech, and Signal Processing.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120–128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Kofi Boakye, Benoit Favre, and Dilek Hakkini-t¨ur. 2009.
Any questions? Automatic question detection in meet-
ings. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding.
Zhigang Chen, Guoping Hu, and Wei Jiang. 2010. Im-
proving prosodic phrase prediction by unsupervised
adaptation and syntactic features extraction. In Proc.
Interspeech.
Heidi Christensen, Yoshihiko Gotoh, and Steve Renals.
2001. Punctuation annotation using statistical prosody
models. In in Proc. ISCA Workshop on Prosody in
Speech Recognition and Understanding, pages 35–40.
Mark G. Core and James F. Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proc. of the
Working Notes of the AAAI Fall Symposium on Com-
municative Action in Humans and Machines, Cam-
bridge, MA, November.
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Eliz-
abeth Shriberg. 2004. Meeting recorder project: Di-
alog act labeling guide. Technical report, ICSI Tech.
Report.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874, August.
Agustin Gravano, Martin Jansche, and Michiel Bacchi-
ani. 2009. Restoring punctuation and capitalization in
transcribed speech. In Proc. Int. Conference on Acous-
tics, Speech, and Signal Processing.
Umit Guz, S´ebastien Cuendet, Dilek Hakkani-T¨ur, and
Gokhan Tur. 2007. Co-training using prosodic and
lexical information for sentence segmentation. In
Proc. Interspeech.
Umit Guz, Gokhan Tur, Dilek Hakkani-T¨ur, and
S´ebastien Cuendet. 2010. Cascaded model adaptation
for dialog act segmentation and tagging. Computer
Speech &amp; Language, 24(2):289–306, April.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proc. Int. Conference on Spoken Language Process-
ing, pages 917–920.
Minwoo Jeong, Chin-Yew Lin, and Gary G. Lee. 2009.
Semi-supervised speech act recognition in emails and
forums. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1250–1259, Singapore, August. Association for
Computational Linguistics.
Ji-Hwan Kim and Philip C. Woodland. 2003. A
combined punctuation generation and speech recog-
nition system and its performance enhancement us-
ing prosody. Speech Communication, 41(4):563–577,
November.
Xin Lei. 2006. Modeling lexical tones for Man-
darin large vocabulary continuous speech recognition.
Ph.D. thesis, Department of Electrical Engineering,
University of Washington.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Trans. Audio, Speech, and Language Processing,
14(5):1526–1540, September.
Anna Margolis, Karen Livescu, and Mari Ostendorf.
2010. Domain adaptation with unlabeled data for dia-
log act tagging. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Process-
ing, pages 45–52, Uppsala, Sweden, July. Association
for Computational Linguistics.
Helena Moniz, Fernando Batista, Isabel Trancoso, and
Ana Mata. 2011. Analysis of interrogatives in dif-
ferent domains. In Toward Autonomous, Adaptive,
and Context-Aware Multimodal Interfaces. Theoret-
ical and Practical Issues, volume 6456 of Lecture
Notes in Computer Science, chapter 12, pages 134–
146. Springer Berlin / Heidelberg.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. 5th Conf. on Applied Natural
Language Processing, April.
Wenzhu Shen, Roger P. Yu, Frank Seide, and Ji Wu.
2009. Automatic punctuation generation for speech.
In Proc. IEEE Workshop on Automatic Speech Recog-
nition and Understanding, pages 586–589, December.
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke,
Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coc-
caro, Rachel Martin, Marie Meteer, and Carol Van Ess-
Dykema. 1998. Can prosody aid the automatic classi-
fication of dialog acts in conversational speech? Lan-
guage and Speech (Special Double Issue on Prosody
and Conversation), 41(3-4):439–487.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meet-
ing recorder dialog act (MRDA) corpus. In Proc. of
the 5th SIGdial Workshop on Discourse and Dialogue,
pages 97–100.
</reference>
<page confidence="0.98735">
123
</page>
<reference confidence="0.99658945">
Kemal S¨onmez, Elizabeth Shriberg, Larry Heck, and
Mitchel Weintraub. 1998. Modeling dynamic
prosodic variation for speaker verification. In Proc.
Int. Conference on Spoken Language Processing,
pages 3189–3192.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339–373.
Dinoj Surendran and Gina-Anne Levow. 2006. Dialog
act tagging with support vector machines and hidden
Markov models. In Proc. Interspeech, pages 1950–
1953.
Anand Venkataraman, Luciana Ferrer, Andreas Stolcke,
and Elizabeth Shriberg. 2003. Training a prosody-
based dialog act tagger from unlabeled data. In Proc.
Int. Conference on Acoustics, Speech, and Signal Pro-
cessing, volume 1, pages 272–275, April.
</reference>
<page confidence="0.998308">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945820">
<title confidence="0.999862">Question Detection in Spoken Conversations Using Textual Conversations</title>
<author confidence="0.981507">Margolis</author>
<affiliation confidence="0.9996435">Department of Electrical University of</affiliation>
<address confidence="0.970761">Seattle, WA,</address>
<abstract confidence="0.999492357142857">We investigate the use of textual Internet conversations for detecting questions in spoken conversations. We compare the text-trained model with models trained on manuallylabeled, domain-matched spoken utterances with and without prosodic features. Overall, the text-trained model achieves over 90% of the performance (measured in Area Under the Curve) of the domain-matched model including prosodic features, but does especially poorly on declarative questions. We describe efforts to utilize unlabeled spoken utterances and prosodic features via domain adaptation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeremy Ang</author>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Automatic dialog act segmentation and classification in multiparty meetings.</title>
<date>2005</date>
<booktitle>In Proc. Int. Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="1190" citStr="Ang et al., 2005" startWordPosition="161" endWordPosition="164">ve) of the domain-matched model including prosodic features, but does especially poorly on declarative questions. We describe efforts to utilize unlabeled spoken utterances and prosodic features via domain adaptation. 1 Introduction Automatic speech recognition systems, which transcribe words, are often augmented by subsequent processing for inserting punctuation or labeling speech acts. Both prosodic features (extracted from the acoustic signal) and lexical features (extracted from the word sequence) have been shown to be useful for these tasks (Shriberg et al., 1998; Kim and Woodland, 2003; Ang et al., 2005). However, access to labeled speech training data is generally required in order to use prosodic features. On the other hand, the Internet contains large quantities of textual data that is already labeled with punctuation, and which can be used to train a system using lexical features. In this work, we focus on question detection in the Meeting Recorder Dialog Act corpus (MRDA) (Shriberg et al., 2004), using text sentences with question marks in Wikipedia “talk” pages. We compare the performance of a question detector trained on the text domain using lexical features with one trained on MRDA u</context>
</contexts>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005. Automatic dialog act segmentation and classification in multiparty meetings. In Proc. Int. Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="3828" citStr="Blitzer et al., 2006" startWordPosition="569" endWordPosition="572">sodic features, although Venkataraman 118 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 118–124, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics et al. (2003) included prosodic features in a semi- we compare two domain adaptation approaches to supervised learning approach for dialogue act la- utilize unlabeled speech data: bootstrapping, and beling within a single spoken domain. Also rele- Blitzer et al.’s Structural Correspondence Learning vant is the work of Moniz et al. (2011), who com- (SCL) (Blitzer et al., 2006). SCL is a featurepared question types in different Portuguese cor- learning method that uses unlabeled data from both pora, including text and speech. For question de- domains. Although it has been applied to several tection on speech, they compared performance of a NLP tasks, to our knowledge we are the first to apply lexical model trained with newspaper text to models SCL to both lexical and prosodic features in order to trained with speech including acoustic and prosodic adapt from text to speech. features, where the speech-trained model also uti- 3 Experiments lized the text-based model p</context>
<context position="15183" citStr="Blitzer et al., 2006" startWordPosition="2397" endWordPosition="2400">5) 92.8 92.8 93.5 tag (358) 79.1 79.3 80.7 rhetorical (75) 93.3 88.0 92.0 open-ended (50) 80.0 76.0 80.0 or (38) 89.5 89.5 89.5 or-after-YN (32) 90.6 90.6 90.6 Table 3: Adaptation performance by question type, at false positive rate of 16.7% (starred points in Figure 2.) Boldface indicates adaptation results better than baseline; italics indicate worse than baseline. available only in the bootstrapped MRDA data, we simply add 16 zeros onto the Wiki examples in place of the missing prosodic features. The values k = 20 and r = 6 were selected on the dev set. In contrast with bootstrapping, SCL (Blitzer et al., 2006) uses the unlabeled target data to learn domainindependent features. SCL has generated much interest lately because of the ability to incorporate features not seen in the training data. The main idea is to use unlabeled data in both domains to learn linear predictors for many “auxiliary” tasks, which should be somewhat related to the task of interest. In particular, if z is a row vector representing the original feature vector and yz represents the label for auxiliary task i, the linear predictor wz is learned to predict 9z = wz · z′ (where z′ is a modified version of detection rate 0.8 0.6 0.</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kofi Boakye</author>
<author>Benoit Favre</author>
<author>Dilek Hakkini-t¨ur</author>
</authors>
<title>Any questions? Automatic question detection in meetings.</title>
<date>2009</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<marker>Boakye, Favre, Hakkini-t¨ur, 2009</marker>
<rawString>Kofi Boakye, Benoit Favre, and Dilek Hakkini-t¨ur. 2009. Any questions? Automatic question detection in meetings. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhigang Chen</author>
<author>Guoping Hu</author>
<author>Wei Jiang</author>
</authors>
<title>Improving prosodic phrase prediction by unsupervised adaptation and syntactic features extraction.</title>
<date>2010</date>
<booktitle>In Proc. Interspeech.</booktitle>
<contexts>
<context position="6655" citStr="Chen et al., 2010" startWordPosition="1004" endWordPosition="1008">as non-questions. We then web news articles on broadcast news speech, and removed all punctuation and capitalization from the compared their performance on written news; Shen resulting sentences and performed some additional et al. (2009) trained models on an online encyclo- text normalization to match the MRDA transcripts, pedia, for punctuation annotation of news podcasts. such as number and date expansion. Web text was also used in a domain adaptation For the MRDA corpus, we use the manuallystrategy for prosodic phrase prediction in news text transcribed sentences with utterance time align(Chen et al., 2010). ments. The corpus has been hand-annotated with In our work, we focus on spontaneous conversa- detailed dialogue act tags, using a hierarchical lational speech, and utilize a web text source that is beling scheme in which each utterance receives one somewhat matched in style: both domains consist of “general” label plus a variable number of “specific” goal-directed multi-party conversations. We focus labels (Dhillon et al., 2004). In this work we are specifically on question detection in pre-segmented only looking at the problem of discriminating quesutterances. This differs from punctuation </context>
</contexts>
<marker>Chen, Hu, Jiang, 2010</marker>
<rawString>Zhigang Chen, Guoping Hu, and Wei Jiang. 2010. Improving prosodic phrase prediction by unsupervised adaptation and syntactic features extraction. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Christensen</author>
<author>Yoshihiko Gotoh</author>
<author>Steve Renals</author>
</authors>
<title>Punctuation annotation using statistical prosody models.</title>
<date>2001</date>
<booktitle>In in Proc. ISCA Workshop on Prosody in Speech Recognition and Understanding,</booktitle>
<pages>35--40</pages>
<marker>Christensen, Gotoh, Renals, 2001</marker>
<rawString>Heidi Christensen, Yoshihiko Gotoh, and Steve Renals. 2001. Punctuation annotation using statistical prosody models. In in Proc. ISCA Workshop on Prosody in Speech Recognition and Understanding, pages 35–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James F Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>In Proc. of the Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="2471" citStr="Core and Allen (1997)" startWordPosition="367" endWordPosition="370"> we experiment with two unsupervised domain adaptation methods to incorporate unlabeled MRDA utterances into the text-based question detector. The goal is to use the unlabeled domain-matched data to bridge stylistic differences as well as to incorporate the prosodic features, which are unavailable in the labeled text data. 2 Related Work Question detection can be viewed as a subtask of speech act or dialogue act tagging, which aims to label functions of utterances in conversations, with categories as question/statement/backchannel, or more specific categories such as request or command (e.g., Core and Allen (1997)). Previous work has investigated the utility of various feature types; Boakye et al. (2009), Shriberg et al. (1998) and Stolcke et al. (2000) showed that prosodic features were useful for question detection in English conversational speech, but (at least in the absence of recognition errors) most of the performance was achieved with words alone. There has been some previous investigation of domain adaptation for dialogue act classification, including adaptation between: different speech corpora (MRDA and Switchboard) (Guz et al., 2010), speech corpora in different languages (Margolis et al., </context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James F. Allen. 1997. Coding dialogs with the DAMSL annotation scheme. In Proc. of the Working Notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines, Cambridge, MA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajdip Dhillon</author>
<author>Sonali Bhagat</author>
<author>Hannah Carvey</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Meeting recorder project: Dialog act labeling guide.</title>
<date>2004</date>
<tech>Technical report, ICSI Tech. Report.</tech>
<contexts>
<context position="7089" citStr="Dhillon et al., 2004" startWordPosition="1072" endWordPosition="1075">used in a domain adaptation For the MRDA corpus, we use the manuallystrategy for prosodic phrase prediction in news text transcribed sentences with utterance time align(Chen et al., 2010). ments. The corpus has been hand-annotated with In our work, we focus on spontaneous conversa- detailed dialogue act tags, using a hierarchical lational speech, and utilize a web text source that is beling scheme in which each utterance receives one somewhat matched in style: both domains consist of “general” label plus a variable number of “specific” goal-directed multi-party conversations. We focus labels (Dhillon et al., 2004). In this work we are specifically on question detection in pre-segmented only looking at the problem of discriminating quesutterances. This differs from punctuation annota- tions from non-questions; we consider as questions tion or segmentation, which is usually seen as a se- all complete utterances labeled with one of the genquence tagging or classification task at word bound- eral labels wh, yes-no, open-ended, or, or-after-yesaries, and uses mostly local features. Our focus also no, or rhetorical question. (To derive the question allows us to clearly analyze the performance on dif- categor</context>
<context position="13052" citStr="Dhillon et al. (2004)" startWordPosition="2033" endWordPosition="2036"> the last 400-200ms dundant with lexical features for yes-no. Ideally, to that in the last 200ms; number of voiced frames; such redundancy can be used together with unla120 yes-no did did you do that? declarative you’re not going to be around this afternoon? wh what do you mean um reference frames? tag you know? rhetorical why why don’t we do that? open-ended do we have anything else to say about transcription? or and @frag@ did they use sigmoid or a softmax type thing? or-after-YN or should i collect it all? Table 1: Examples for each MRDA question category as defined in this paper, based on Dhillon et al. (2004). beled spoken utterances to incorporate prosodic features into the Wiki system, which may improve detection of some kinds of questions. false pos rate Figure 1: ROC curves with AUC values for question detection on MRDA; comparison between systems trained on MRDA using lexical and/or prosodic features, and Wiki talk pages using lexical features. 3.4 Adaptation Results For bootstrapping, we first train an initial baseline classifier using the Wiki training data, then use it to label MRDA data from the unlabeled adaptation set. We select the k most confident examples for each of the two classes </context>
</contexts>
<marker>Dhillon, Bhagat, Carvey, Shriberg, 2004</marker>
<rawString>Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Elizabeth Shriberg. 2004. Meeting recorder project: Dialog act labeling guide. Technical report, ICSI Tech. Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agustin Gravano</author>
<author>Martin Jansche</author>
<author>Michiel Bacchiani</author>
</authors>
<title>Restoring punctuation and capitalization in transcribed speech.</title>
<date>2009</date>
<booktitle>In Proc. Int. Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="5982" citStr="Gravano et al. (2009)" startWordPosition="901" endWordPosition="904">oodland (2003) and Liu et al. (2006) used auxil- then performed automatic segmentation of the posts iary text corpora to train lexical models for punc- into sentences using MXTERMINATOR (Reynar tuation annotation or sentence segmentation, which and Ratnaparkhi, 1997). We labeled each sentence were used along with speech-trained prosodic mod- ending in a question mark (followed optionally by els; the text corpora consisted of broadcast news or other punctuation) as a question; we also included telephone conversation transcripts. More recently, parentheticals ending in question marks. All other Gravano et al. (2009) used lexical models built from sentences were labeled as non-questions. We then web news articles on broadcast news speech, and removed all punctuation and capitalization from the compared their performance on written news; Shen resulting sentences and performed some additional et al. (2009) trained models on an online encyclo- text normalization to match the MRDA transcripts, pedia, for punctuation annotation of news podcasts. such as number and date expansion. Web text was also used in a domain adaptation For the MRDA corpus, we use the manuallystrategy for prosodic phrase prediction in new</context>
</contexts>
<marker>Gravano, Jansche, Bacchiani, 2009</marker>
<rawString>Agustin Gravano, Martin Jansche, and Michiel Bacchiani. 2009. Restoring punctuation and capitalization in transcribed speech. In Proc. Int. Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umit Guz</author>
<author>S´ebastien Cuendet</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Gokhan Tur</author>
</authors>
<title>Co-training using prosodic and lexical information for sentence segmentation. In</title>
<date>2007</date>
<booktitle>Proc. Interspeech.</booktitle>
<marker>Guz, Cuendet, Hakkani-T¨ur, Tur, 2007</marker>
<rawString>Umit Guz, S´ebastien Cuendet, Dilek Hakkani-T¨ur, and Gokhan Tur. 2007. Co-training using prosodic and lexical information for sentence segmentation. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umit Guz</author>
<author>Gokhan Tur</author>
<author>Dilek Hakkani-T¨ur</author>
<author>S´ebastien Cuendet</author>
</authors>
<title>Cascaded model adaptation for dialog act segmentation and tagging.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Guz, Tur, Hakkani-T¨ur, Cuendet, 2010</marker>
<rawString>Umit Guz, Gokhan Tur, Dilek Hakkani-T¨ur, and S´ebastien Cuendet. 2010. Cascaded model adaptation for dialog act segmentation and tagging. Computer Speech &amp; Language, 24(2):289–306, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Huang</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Maximum entropy model for punctuation annotation from speech.</title>
<date>2002</date>
<booktitle>In Proc. Int. Conference on Spoken Language Processing,</booktitle>
<pages>917--920</pages>
<contexts>
<context position="5300" citStr="Huang and Zweig (2002)" startWordPosition="801" endWordPosition="804">no and tag questions, although results While these lack certain properties of spontaneous for specific categories were not included. speech (such as backchannels, disfluencies, and inQuestion detection is related to the task of auto- terruptions), they are more conversational than news matic punctuation annotation, for which the contri- articles, containing utterances such as: “Are you sebutions of lexical and prosodic features have been rious?” or “Hey, that’s a really good point.” We explored in other works, e.g. Christensen et al. first cleaned the posts (to remove URLs, images, (2001) and Huang and Zweig (2002). Kim and signatures, Wiki markup, and duplicate posts) and Woodland (2003) and Liu et al. (2006) used auxil- then performed automatic segmentation of the posts iary text corpora to train lexical models for punc- into sentences using MXTERMINATOR (Reynar tuation annotation or sentence segmentation, which and Ratnaparkhi, 1997). We labeled each sentence were used along with speech-trained prosodic mod- ending in a question mark (followed optionally by els; the text corpora consisted of broadcast news or other punctuation) as a question; we also included telephone conversation transcripts. More </context>
</contexts>
<marker>Huang, Zweig, 2002</marker>
<rawString>Jing Huang and Geoffrey Zweig. 2002. Maximum entropy model for punctuation annotation from speech. In Proc. Int. Conference on Spoken Language Processing, pages 917–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Chin-Yew Lin</author>
<author>Gary G Lee</author>
</authors>
<title>Semi-supervised speech act recognition in emails and forums.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1250--1259</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3178" citStr="Jeong et al., 2009" startWordPosition="476" endWordPosition="479">009), Shriberg et al. (1998) and Stolcke et al. (2000) showed that prosodic features were useful for question detection in English conversational speech, but (at least in the absence of recognition errors) most of the performance was achieved with words alone. There has been some previous investigation of domain adaptation for dialogue act classification, including adaptation between: different speech corpora (MRDA and Switchboard) (Guz et al., 2010), speech corpora in different languages (Margolis et al., 2010), and from a speech domain (MRDA/Switchboard) to text domains (emails and forums) (Jeong et al., 2009). These works did not use prosodic features, although Venkataraman 118 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 118–124, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics et al. (2003) included prosodic features in a semi- we compare two domain adaptation approaches to supervised learning approach for dialogue act la- utilize unlabeled speech data: bootstrapping, and beling within a single spoken domain. Also rele- Blitzer et al.’s Structural Correspondence Learning vant is the work of Moniz et</context>
</contexts>
<marker>Jeong, Lin, Lee, 2009</marker>
<rawString>Minwoo Jeong, Chin-Yew Lin, and Gary G. Lee. 2009. Semi-supervised speech act recognition in emails and forums. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250–1259, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji-Hwan Kim</author>
<author>Philip C Woodland</author>
</authors>
<title>A combined punctuation generation and speech recognition system and its performance enhancement using prosody.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="1171" citStr="Kim and Woodland, 2003" startWordPosition="157" endWordPosition="160">ed in Area Under the Curve) of the domain-matched model including prosodic features, but does especially poorly on declarative questions. We describe efforts to utilize unlabeled spoken utterances and prosodic features via domain adaptation. 1 Introduction Automatic speech recognition systems, which transcribe words, are often augmented by subsequent processing for inserting punctuation or labeling speech acts. Both prosodic features (extracted from the acoustic signal) and lexical features (extracted from the word sequence) have been shown to be useful for these tasks (Shriberg et al., 1998; Kim and Woodland, 2003; Ang et al., 2005). However, access to labeled speech training data is generally required in order to use prosodic features. On the other hand, the Internet contains large quantities of textual data that is already labeled with punctuation, and which can be used to train a system using lexical features. In this work, we focus on question detection in the Meeting Recorder Dialog Act corpus (MRDA) (Shriberg et al., 2004), using text sentences with question marks in Wikipedia “talk” pages. We compare the performance of a question detector trained on the text domain using lexical features with on</context>
</contexts>
<marker>Kim, Woodland, 2003</marker>
<rawString>Ji-Hwan Kim and Philip C. Woodland. 2003. A combined punctuation generation and speech recognition system and its performance enhancement using prosody. Speech Communication, 41(4):563–577, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Lei</author>
</authors>
<title>Modeling lexical tones for Mandarin large vocabulary continuous speech recognition.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Electrical Engineering, University of Washington.</institution>
<contexts>
<context position="11426" citStr="Lei (2006)" startWordPosition="1760" endWordPosition="1761"> et al. (2009) showed no clear benefit gories at a fixed false positive rate (16.7%) near the of these features for question detection on MRDA equal error rate of the MRDA (lex) case. For analybeyond the ngram features. sis purposes we defined the categories in Table 2 as We extracted 16 prosody features from the speech follows: tag includes any yes-no question given the waveforms defined by the given utterance times, us- additional tag label; declarative includes any quesing stylized F0 contours computed based on S¨onmez tion category given the declarative label that is not et al. (1998) and Lei (2006). The features are de- a tag question; the remaining categories (yes-no, or, signed to be useful for detecting questions and are etc.) include utterances in those categories but not similar or identical to some of those in Boakye et included in declarative or tag. Table 1 gives examal. (2009) or Shriberg et al. (1998). They include: ple sentences for each category. F0 statistics (mean, stdev, max, min) computed over As expected, the Wiki-trained system does worst the whole utterance and over the last 200ms; slopes on declarative, which have the syntactic form of computed from a linear regressi</context>
</contexts>
<marker>Lei, 2006</marker>
<rawString>Xin Lei. 2006. Modeling lexical tones for Mandarin large vocabulary continuous speech recognition. Ph.D. thesis, Department of Electrical Engineering, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dustin Hillard</author>
<author>Mari Ostendorf</author>
<author>Mary Harper</author>
</authors>
<title>Enriching speech recognition with automatic detection of sentence boundaries and disfluencies.</title>
<date>2006</date>
<journal>IEEE Trans. Audio, Speech, and Language Processing,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="5397" citStr="Liu et al. (2006)" startWordPosition="817" endWordPosition="820"> categories were not included. speech (such as backchannels, disfluencies, and inQuestion detection is related to the task of auto- terruptions), they are more conversational than news matic punctuation annotation, for which the contri- articles, containing utterances such as: “Are you sebutions of lexical and prosodic features have been rious?” or “Hey, that’s a really good point.” We explored in other works, e.g. Christensen et al. first cleaned the posts (to remove URLs, images, (2001) and Huang and Zweig (2002). Kim and signatures, Wiki markup, and duplicate posts) and Woodland (2003) and Liu et al. (2006) used auxil- then performed automatic segmentation of the posts iary text corpora to train lexical models for punc- into sentences using MXTERMINATOR (Reynar tuation annotation or sentence segmentation, which and Ratnaparkhi, 1997). We labeled each sentence were used along with speech-trained prosodic mod- ending in a question mark (followed optionally by els; the text corpora consisted of broadcast news or other punctuation) as a question; we also included telephone conversation transcripts. More recently, parentheticals ending in question marks. All other Gravano et al. (2009) used lexical m</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Hillard, Ostendorf, Harper, 2006</marker>
<rawString>Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin Hillard, Mari Ostendorf, and Mary Harper. 2006. Enriching speech recognition with automatic detection of sentence boundaries and disfluencies. IEEE Trans. Audio, Speech, and Language Processing, 14(5):1526–1540, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Margolis</author>
<author>Karen Livescu</author>
<author>Mari Ostendorf</author>
</authors>
<title>Domain adaptation with unlabeled data for dialog act tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,</booktitle>
<pages>45--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3076" citStr="Margolis et al., 2010" startWordPosition="460" endWordPosition="463"> and Allen (1997)). Previous work has investigated the utility of various feature types; Boakye et al. (2009), Shriberg et al. (1998) and Stolcke et al. (2000) showed that prosodic features were useful for question detection in English conversational speech, but (at least in the absence of recognition errors) most of the performance was achieved with words alone. There has been some previous investigation of domain adaptation for dialogue act classification, including adaptation between: different speech corpora (MRDA and Switchboard) (Guz et al., 2010), speech corpora in different languages (Margolis et al., 2010), and from a speech domain (MRDA/Switchboard) to text domains (emails and forums) (Jeong et al., 2009). These works did not use prosodic features, although Venkataraman 118 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 118–124, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics et al. (2003) included prosodic features in a semi- we compare two domain adaptation approaches to supervised learning approach for dialogue act la- utilize unlabeled speech data: bootstrapping, and beling within a single spok</context>
</contexts>
<marker>Margolis, Livescu, Ostendorf, 2010</marker>
<rawString>Anna Margolis, Karen Livescu, and Mari Ostendorf. 2010. Domain adaptation with unlabeled data for dialog act tagging. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 45–52, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helena Moniz</author>
<author>Fernando Batista</author>
<author>Isabel Trancoso</author>
<author>Ana Mata</author>
</authors>
<title>Analysis of interrogatives in different domains.</title>
<date>2011</date>
<booktitle>In Toward Autonomous, Adaptive, and Context-Aware Multimodal Interfaces. Theoretical and Practical Issues,</booktitle>
<volume>6456</volume>
<pages>134--146</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="3789" citStr="Moniz et al. (2011)" startWordPosition="562" endWordPosition="565">., 2009). These works did not use prosodic features, although Venkataraman 118 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 118–124, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics et al. (2003) included prosodic features in a semi- we compare two domain adaptation approaches to supervised learning approach for dialogue act la- utilize unlabeled speech data: bootstrapping, and beling within a single spoken domain. Also rele- Blitzer et al.’s Structural Correspondence Learning vant is the work of Moniz et al. (2011), who com- (SCL) (Blitzer et al., 2006). SCL is a featurepared question types in different Portuguese cor- learning method that uses unlabeled data from both pora, including text and speech. For question de- domains. Although it has been applied to several tection on speech, they compared performance of a NLP tasks, to our knowledge we are the first to apply lexical model trained with newspaper text to models SCL to both lexical and prosodic features in order to trained with speech including acoustic and prosodic adapt from text to speech. features, where the speech-trained model also uti- 3 E</context>
</contexts>
<marker>Moniz, Batista, Trancoso, Mata, 2011</marker>
<rawString>Helena Moniz, Fernando Batista, Isabel Trancoso, and Ana Mata. 2011. Analysis of interrogatives in different domains. In Toward Autonomous, Adaptive, and Context-Aware Multimodal Interfaces. Theoretical and Practical Issues, volume 6456 of Lecture Notes in Computer Science, chapter 12, pages 134– 146. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proc. 5th Conf. on Applied Natural Language Processing,</booktitle>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proc. 5th Conf. on Applied Natural Language Processing, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhu Shen</author>
<author>Roger P Yu</author>
<author>Frank Seide</author>
<author>Ji Wu</author>
</authors>
<title>Automatic punctuation generation for speech.</title>
<date>2009</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>586--589</pages>
<marker>Shen, Yu, Seide, Wu, 2009</marker>
<rawString>Wenzhu Shen, Roger P. Yu, Frank Seide, and Ji Wu. 2009. Automatic punctuation generation for speech. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding, pages 586–589, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Andreas Stolcke</author>
<author>Paul Taylor</author>
<author>Daniel Jurafsky</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
<author>Carol Van EssDykema</author>
</authors>
<title>Can prosody aid the automatic classification of dialog acts in conversational speech?</title>
<date>1998</date>
<booktitle>Language and Speech (Special Double Issue on Prosody and Conversation),</booktitle>
<pages>41--3</pages>
<marker>Shriberg, Bates, Stolcke, Taylor, Jurafsky, Ries, Coccaro, Martin, Meteer, Van EssDykema, 1998</marker>
<rawString>Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke, Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coccaro, Rachel Martin, Marie Meteer, and Carol Van EssDykema. 1998. Can prosody aid the automatic classification of dialog acts in conversational speech? Language and Speech (Special Double Issue on Prosody and Conversation), 41(3-4):439–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Raj Dhillon</author>
<author>Sonali Bhagat</author>
<author>Jeremy Ang</author>
<author>Hannah Carvey</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus.</title>
<date>2004</date>
<booktitle>In Proc. of the 5th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="1594" citStr="Shriberg et al., 2004" startWordPosition="230" endWordPosition="233">sodic features (extracted from the acoustic signal) and lexical features (extracted from the word sequence) have been shown to be useful for these tasks (Shriberg et al., 1998; Kim and Woodland, 2003; Ang et al., 2005). However, access to labeled speech training data is generally required in order to use prosodic features. On the other hand, the Internet contains large quantities of textual data that is already labeled with punctuation, and which can be used to train a system using lexical features. In this work, we focus on question detection in the Meeting Recorder Dialog Act corpus (MRDA) (Shriberg et al., 2004), using text sentences with question marks in Wikipedia “talk” pages. We compare the performance of a question detector trained on the text domain using lexical features with one trained on MRDA using lexical features and/or prosodic features. In addition, we experiment with two unsupervised domain adaptation methods to incorporate unlabeled MRDA utterances into the text-based question detector. The goal is to use the unlabeled domain-matched data to bridge stylistic differences as well as to incorporate the prosodic features, which are unavailable in the labeled text data. 2 Related Work Ques</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy Ang, and Hannah Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In Proc. of the 5th SIGdial Workshop on Discourse and Dialogue, pages 97–100.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kemal S¨onmez</author>
</authors>
<location>Elizabeth Shriberg, Larry Heck, and</location>
<marker>S¨onmez, </marker>
<rawString>Kemal S¨onmez, Elizabeth Shriberg, Larry Heck, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchel Weintraub</author>
</authors>
<title>Modeling dynamic prosodic variation for speaker verification.</title>
<date>1998</date>
<booktitle>In Proc. Int. Conference on Spoken Language Processing,</booktitle>
<pages>3189--3192</pages>
<marker>Weintraub, 1998</marker>
<rawString>Mitchel Weintraub. 1998. Modeling dynamic prosodic variation for speaker verification. In Proc. Int. Conference on Spoken Language Processing, pages 3189–3192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--339</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26:339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dinoj Surendran</author>
<author>Gina-Anne Levow</author>
</authors>
<title>Dialog act tagging with support vector machines and hidden Markov models.</title>
<date>2006</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>pages</pages>
<marker>Surendran, Levow, 2006</marker>
<rawString>Dinoj Surendran and Gina-Anne Levow. 2006. Dialog act tagging with support vector machines and hidden Markov models. In Proc. Interspeech, pages 1950– 1953.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
<author>Luciana Ferrer</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Training a prosodybased dialog act tagger from unlabeled data. In</title>
<date>2003</date>
<booktitle>Proc. Int. Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>272--275</pages>
<marker>Venkataraman, Ferrer, Stolcke, Shriberg, 2003</marker>
<rawString>Anand Venkataraman, Luciana Ferrer, Andreas Stolcke, and Elizabeth Shriberg. 2003. Training a prosodybased dialog act tagger from unlabeled data. In Proc. Int. Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 272–275, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>