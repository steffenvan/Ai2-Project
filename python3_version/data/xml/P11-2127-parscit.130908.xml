<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001859">
<title confidence="0.959546">
The Surprising Variance in Shortest-Derivation Parsing
</title>
<author confidence="0.991937">
Mohit Bansal and Dan Klein
</author>
<affiliation confidence="0.9978935">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.999386">
{mbansal,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998500294117647">
We investigate full-scale shortest-derivation
parsing (SDP), wherein the parser selects an
analysis built from the fewest number of train-
ing fragments. Shortest derivation parsing
exhibits an unusual range of behaviors. At
one extreme, in the fully unpruned case, it
is neither fast nor accurate. At the other ex-
treme, when pruned with a coarse unlexical-
ized PCFG, the shortest derivation criterion
becomes both fast and surprisingly effective,
rivaling more complex weighted-fragment ap-
proaches. Our analysis includes an investi-
gation of tie-breaking and associated dynamic
programs. At its best, our parser achieves an
accuracy of 87% F1 on the English WSJ task
with minimal annotation, and 90% F1 with
richer annotation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997028125">
One guiding intuition in parsing, and data-driven
NLP more generally, is that, all else equal, it is ad-
vantageous to memorize large fragments of training
examples. Taken to the extreme, this intuition sug-
gests shortest derivation parsing (SDP), wherein a
test sentence is analyzed in a way which uses as few
training fragments as possible (Bod, 2000; Good-
man, 2003). SDP certainly has appealing properties:
it is simple and parameter free – there need not even
be an explicit lexicon. However, SDP may be too
simple to be competitive.
In this paper, we consider SDP in both its pure
form and with several direct modifications, finding a
range of behaviors. In its pure form, with no prun-
ing or approximation, SDP is neither fast nor accu-
rate, achieving less than 70% F1 on the English WSJ
task. Moreover, basic tie-breaking variants and lexi-
cal augmentation are insufficient to achieve compet-
itive accuracies.1 On the other hand, SDP is dramat-
ically improved in both speed and accuracy when
a simple, unlexicalized PCFG is used for coarse-
to-fine pruning (and tie-breaking). On the English
WSJ, the coarse PCFG and the fine SDP together
achieve 87% F1 with basic treebank annotation (see
Table 2) and up to 90% F1 with richer treebank an-
notation (see Table 4).
The main contribution of this work is to analyze
the behavior of shortest derivation parsing, showing
both when it fails and when it succeeds. Our final
parser, which combines a simple PCFG coarse pass
with an otherwise pure SPD fine pass, can be quite
accurate while being straightforward to implement.
</bodyText>
<sectionHeader confidence="0.969077" genericHeader="method">
2 Implicit Grammar for SDP
</sectionHeader>
<bodyText confidence="0.999432875">
The all-fragments grammar (AFG) for a (binarized)
treebank is formally the tree-substitution grammar
(TSG) (Resnik, 1992; Bod, 1993) that consists of
all fragments (elementary trees) of all training trees
in the treebank, with some weighting on each frag-
ment. AFGs are too large to fully extract explicitly;
researchers therefore either work with a tractable
subset of the fragments (Sima’an, 2000; Bod, 2001;
Post and Gildea, 2009; Cohn and Blunsom, 2010) or
use a PCFG reduction like that of Goodman (1996a),
in which each treebank node token Xi is given its
own unique grammar symbol.
We follow Bansal and Klein (2010) in choosing
the latter, both to permit comparison to their results
and because SDP is easily phrased as a PCFG re-
duction. Bansal and Klein (2010) use a carefully pa-
</bodyText>
<footnote confidence="0.9934465">
1Bod (2000) presented another SDP parser, but with a sam-
pled subset of the training fragments.
</footnote>
<page confidence="0.895662">
720
</page>
<note confidence="0.608591">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720–725,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.985088846153846">
rameterized weighting of the substructures in their
grammar in an effort to extend the original DOP1
model (Bod, 1993; Goodman, 1996a). However, for
SDP, the grammar is even simpler (Goodman, 2003).
In principle, the implicit SDP grammar needs just
two rule schemas: CONTINUE (Xp —* YQ Zr) and
SWITCH (Xp —* XQ), with additive costs 0 and 1,
respectively. CONTINUE rules walk along training
trees, while SWITCH rules change between trees for
a unit cost.2 Assuming that the SWITCH rules are in
practice broken down into BEGIN and END sub-rules
as in Bansal and Klein (2010), the grammar is linear
in the size of the treebank.3 Note that no lexicon
is needed in this grammar: lexical switches are like
any other.
A derivation in our grammar has weight (cost) w
where w is the number of switches (or the num-
ber of training fragments minus one) used to build
the derivation (see Figure 1). The Viterbi dy-
namic program for finding the shortest derivation is
quite simple: it requires CKY to store only byte-
valued switch-counts s(Xp, i, j) (i.e., the number
of switches) for each chart item and compute the
derivation with the least switch-count. Specifically,
in the dynamic program, if we use a SWITCH rule
Xp —* XQ, then we update
</bodyText>
<equation confidence="0.754379">
s(Xp, i, j) := s(XQ, i, j) + 1.
</equation>
<bodyText confidence="0.977566">
If we use a continue rule Xp —* YQ Zr, then the up-
date is
</bodyText>
<equation confidence="0.512894">
s(Xp, i, j) := s(YQ, i, k) + s(Zr, k, j),
</equation>
<bodyText confidence="0.9999798">
where k is a split point in the chart. Using this
dynamic program, we compute the exact shortest
derivation parse in the full all-fragments grammar
(which is reduced to a PCFG with 2 rules schemas
as described above).
</bodyText>
<sectionHeader confidence="0.97396" genericHeader="method">
3 Basic SDP: Inaccurate and Slow
</sectionHeader>
<bodyText confidence="0.998522">
SDP in its most basic form is appealingly simple,
but has two serious issues: it is both slow and in-
accurate. Because there are millions of grammar
</bodyText>
<footnote confidence="0.891211142857143">
2This grammar is a very minor variant of the reduction of
SDP suggested by Goodman (2003).
3For a compact WSJ training set with graph packing (see
Bansal and Klein (2010)) and one level of parent annotation
and markovization, our grammar has 0.9 million indexed sym-
bols compared to 7.5 million unbinarized (and 0.75 million bi-
narized) explicitly-extracted fragments of just depth 1 and 2.
</footnote>
<figureCaption confidence="0.9957675">
Figure 1: SDP - the best parse corresponds to the shortest
derivation (fewest switches).
</figureCaption>
<bodyText confidence="0.999977636363636">
symbols, exact SDP parsing takes more than 45 sec-
onds per sentence in our implementation (in addition
to being highly memory-intensive). Many methods
exist for speeding up parsing through approxima-
tion, but basic SDP is too inaccurate to merit them.
When implemented as described in Section 2, SDP
achieves only 66% F1 on the WSJ task (dev set, &lt;
40 words).
Why does SDP perform so poorly? One reason
for low accuracy may be that there are many short-
est derivations, i.e. derivations that are all built with
the fewest number of fragments, and that tie break-
ing could be at fault. To investigate this, we tried
various methods for tie-breaking: FIRST/LAST (pro-
cedurally break ties), UNIFORM (sample derivations
equally), FREQ (use the frequency of local rules).
However, none of these methods help much, giv-
ing results within a percentage of F1. In fact, even
oracle tie-breaking, where ties are broken to favor
the number of gold constituents in the derivation
achieves only 80% F1, indicating that correct deriva-
tions are often not the shortest ones. Another rea-
son for the poor performance of SDP may be that
the parameter-free treatment of the lexical layer is
particularly pathological. Indeed, this hypothesis is
partially verified by the result that using a lexicon
(similar to that in Petrov et al. (2006)) at the termi-
nal layer brings the uniform tie-breaking result up to
80% F1. However, combining a lexicon with oracle
tie-breaking yields only 81.8% F1.
These results at first seem quite discouraging, but
we will show that they can be easily improved with
information from even a simple PCFG.
</bodyText>
<figure confidence="0.996601043478261">
The girl
The girl
Test Parse
NP-1
DT-2 NN-3
NP-4
DT-5 NN-6
SWITCH
DT-2
NP
DT NN
The girl
Derivation 1 Derivation 2
Training Data
NP-1
DT-2 NN-3
The girl
DT-5
A girl
NP-4
NN-6
Test Sentence
The girl
</figure>
<page confidence="0.956605">
721
</page>
<sectionHeader confidence="0.962414" genericHeader="method">
4 Improvements from a Coarse PCFG
</sectionHeader>
<bodyText confidence="0.999979441860465">
The additional information that makes shortest
derivation parsing work comes from a coarse un-
lexicalized PCFG. In the standard way, our PCFG
consists of the local (depth-1) rules X —* Y Z with
probability P(Y Z|X) computed using the count
of the rule and the count of the nonterminal X in
the given treebank (no smoothing was used). Our
coarse grammar uses a lexicon with unknown word
classes, similar to that in Petrov et al. (2006). When
taken from a binarized treebank with one level of
parent annotation (Johnson, 1998) and horizontal
markovization, the PCFG is quite small, with around
3500 symbols and 25000 rules; it achieves an accu-
racy of 84% on its own (see Table 2), so the PCFG
on its own is better than the basic SDP, but still rela-
tively weak.
When filtered by a coarse PCFG pass, how-
ever, SDP becomes both fast and accurate, even for
the basic, lexicon-free SDP formulation. Summed
marginals (posteriors) are computed in the coarse
PCFG and used for pruning and tie-breaking in the
SDP chart, as described next. Pruning works in the
standard coarse-to-fine (CTF) way (see Charniak et
al. (2006)). If a particular base symbol X is pruned
by the PCFG coarse pass for a particular span (i, j)
(i.e., the posterior marginal P(X, i, j|s) is less than
a certain threshold), then in the full SDP pass we do
not allow building any indexed symbol Xl of type X
for span (i, j). In all our pruning-based experiments,
we use a log posterior threshold of −3.8, tuned on
the WSJ development set.
We also use the PCFG coarse pass for tie-
breaking. During Viterbi shortest-derivation pars-
ing (after coarse-pruning), if two derivations have
the same cost (i.e., the number of switches), then we
break the tie between them by choosing the deriva-
tion which has a higher sum of coarse posteriors
(i.e., the sum of the coarse PCFG chart-cell pos-
teriors P(X, i, j|s) used to build the derivation).4
The coarse PCFG has an extremely beneficial in-
teraction with the fine all-fragments SDP grammar,
wherein the accuracy of the combined grammars
is significantly higher than either individually (see
</bodyText>
<footnote confidence="0.992727333333333">
4This is similar to the maximum recall objective for approx-
imate inference (Goodman, 1996b). The product of posteriors
also works equally well.
</footnote>
<table confidence="0.998726">
dev (&lt; 40) test (&lt; 40)
Model F1 EX F1 EX
B&amp;K2010 pruned 88.4 33.7 88.5 33.0
B&amp;K2010 unpruned 87.9 32.4 88.1 31.9
</table>
<tableCaption confidence="0.962644666666667">
Table 1: Accuracy (F1) and exact match (EX) for Bansal and
Klein (2010). The pruned row shows their original results with
coarse-to-fine pruning. The unpruned row shows new results
for an unpruned version of their parser; these accuracies are
very similar to their pruned counterparts.
Table 2). In addition, the speed of parsing and
</tableCaption>
<bodyText confidence="0.9948199">
memory-requirements improve by more than an or-
der of magnitude over the exact SDP pass alone.
It is perhaps surprising that coarse-pass pruning
improves accuracy by such a large amount for SDP.
Indeed, given that past all-fragments work has used
a coarse pass for speed, and that we are the first (to
our knowledge) to actually parse at scale with an
implicit grammar without such a coarse pass, it is
a worry that previous results could be crucially de-
pendent on fortuitous coarse-pass pruning. To check
one such result, we ran the full, weighted AFG con-
struction of Bansal and Klein (2010) without any
pruning (using the maximum recall objective as they
did). Their results hold up without pruning: the re-
sults of the unpruned version are only around 0.5%
less (in parsing F1) than the results achieved with
pruning (see Table 1). However, in the case of our
shortest-derivation parser, the coarse-pass is essen-
tial for high accuracies (and for speed and memory,
as always).
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999961166666667">
We have seen that basic, unpruned SDP is both slow
and inaccurate, but improves greatly when comple-
mented by a coarse PCFG pass; these results are
shown in Table 2. Shortest derivation parsing with a
PCFG coarse-pass (PCFG+SDP) achieves an accu-
racy of nearly 87% F1 (on the WSJ test set, &lt; 40
word sentences), which is significantly higher than
the accuracy of the PCFG or SDP alone.5 When
the coarse PCFG is combined with basic SDP, the
majority of the improvement comes from pruning
with the coarse-posteriors; tie-breaking with coarse-
posteriors contributes around 0.5% F1 over pruning.
</bodyText>
<footnote confidence="0.7251025">
5PCFG+SDP accuracies are around 3% higher in F1 and
10% higher in EX than the PCFG-only accuracies.
</footnote>
<page confidence="0.986916">
722
</page>
<table confidence="0.9986556">
dev (&lt; 40) test (&lt; 40) test (all)
Model F1 EX F1 EX F1 EX
SDP 66.2 18.0 66.9 18.4 64.9 17.3
PCFG 83.8 20.0 84.0 21.6 83.2 20.1
PCFG+SDP 86.4 30.6 86.9 31.5 86.0 29.4
</table>
<tableCaption confidence="0.554999333333333">
Table 2: Our primary results on the WSJ task. SDP is the
basic unpruned shortest derivation parser. PCFG results are
with one level of parent annotation and horizontal markoviza-
tion. PCFG+SDP incorporates the coarse PCFG posteriors into
SDP. See end of Section 5 for a comparison to other parsing
approaches.
</tableCaption>
<bodyText confidence="0.997204393939394">
Figure 2 shows the number of fragments for short-
est derivation parsing (averaged for each sentence
length). Note that the number of fragments is of
course greater for the combined PCFG+SDP model
than the exact basic SDP model (which is guaranteed
to be minimal). This result provides some analysis
of how coarse-pruning helps SDP: it illustrates that
the coarse-pass filters out certain short but inaccu-
rate derivations (that the minimal SDP on its own is
forced to choose) to improve performance.
Figure 3 shows the parsing accuracy of the
PCFG+SDP model for various pruning thresholds
in coarse-to-fine pruning. Note how this is differ-
ent from the standard coarse-pass pruning graphs
(see Charniak et al. (1998), Petrov and Klein (2007),
Bansal and Klein (2010)) where only a small im-
provement is achieved from pruning. In contrast,
coarse-pass pruning provides large accuracy benefits
here, perhaps because of the unusual complementar-
ity of the two grammars (typical coarse passes are
designed to be as similar as possible to their fine
counterparts, even explicitly so in Petrov and Klein
(2007)).
Our PCFG+SDP parser is more accurate than re-
cent sampling-based TSG’s (Post and Gildea, 2009;
Cohn and Blunsom, 2010), who achieve 83-85% F1,
and it is competitive with more complex weighted-
fragment approaches.6 See Bansal and Klein (2010)
for a more thorough comparison to other parsing
work. In addition to being accurate, the PCFG+SDP
parser is simple and fast, requiring negligible train-
ing and tuning. It takes 2 sec/sentence, less than 2
GB of memory and is written in less than 2000 lines
</bodyText>
<tableCaption confidence="0.562893333333333">
6Bansal and Klein (2010) achieve around 1.0% higher F1
than our results without a lexicon (character-level parsing) and
1.5% higher F1 with a lexicon.
</tableCaption>
<figure confidence="0.970507">
0 4 8 12 16 20 24 28 32 36 40
sentence length
</figure>
<figureCaption confidence="0.993838666666667">
Figure 2: The average number of fragments in shortest deriva-
tion parses, computed using the basic version (SDP) and the
pruned version (PCFG+SDP), for WSJ dev-set (&lt; 40 words).
</figureCaption>
<figure confidence="0.998245777777778">
90.0
85.0
80.0
75.0
70.0 No Pruning
(PT = -inf)
65.0
-3 -5 -7 -9 -11 -13 -15 -17
Coarse-pass Log Posterior Threshold (PT)
</figure>
<figureCaption confidence="0.99444975">
Figure 3: Parsing accuracy for various coarse-pass pruning
thresholds (on WSJ dev-set &lt; 40 words). A larger threshold
means more pruning. These are results without the coarse-
posterior tie-breaking to illustrate the sole effect of pruning.
</figureCaption>
<bodyText confidence="0.844495">
of Java code, including I/O.7
</bodyText>
<subsectionHeader confidence="0.985653">
5.1 Other Treebanks
</subsectionHeader>
<bodyText confidence="0.998870454545455">
One nice property of the parameter-free, all-
fragments SDP approach is that we can easily trans-
fer it to any new domain with a treebank, or any
new annotation of an existing treebank. Table 3
shows domain adaptation performance by the re-
sults for training and testing on the Brown and
German datasets.8 On Brown, we perform better
than the relatively complex lexicalized Model 1 of
Collins (1999). For German, our parser outperforms
Dubey (2005) and we are not far behind latent-
variable parsers, for which parsing is substantially
</bodyText>
<footnote confidence="0.97610925">
7These statistics can be further improved with standard pars-
ing micro-optimization.
8See Gildea (2001) and Petrov and Klein (2007) for the ex-
act experimental setup that we followed here.
</footnote>
<figure confidence="0.984868555555555">
PCFG + SDP
SDP
25
20
# of fragments 15
10
5
0
F 1
</figure>
<page confidence="0.992132">
723
</page>
<table confidence="0.9989293">
test (&lt; 40) test (all)
F1 EX F1 EX
Model
BROWN
Gildea (2001) 84.1 – – –
This Paper (PCFG+SDP) 84.7 34.6 83.1 32.6
GERMAN
Dubey (2005) 76.3 – – –
Petrov and Klein (2007) 80.8 40.8 80.1 39.1
This Paper (PCFG+SDP) 78.1 39.3 77.1 38.2
</table>
<tableCaption confidence="0.884022">
Table 3: Results for training and testing on the Brown and
German treebanks. Gildea (2001) uses the lexicalized Collins’
Model 1 (Collins, 1999).
</tableCaption>
<table confidence="0.99932825">
test (&lt; 40) test (all)
Annotation F1 EX F1 EX
STAN-ANNOTATION 88.1 34.3 87.4 32.2
BERK-ANNOTATION 90.0 38.9 89.5 36.8
</table>
<tableCaption confidence="0.955253">
Table 4: Results with richer WSJ-annotations from Stanford
and Berkeley parsers.
</tableCaption>
<bodyText confidence="0.933208">
more complex.
</bodyText>
<subsectionHeader confidence="0.992476">
5.2 Treebank Annotations
</subsectionHeader>
<bodyText confidence="0.999976615384615">
PCFG+SDP achieves 87% F1 on the English WSJ
task using basic annotation only (i.e., one level
of parent annotation and horizontal markoviza-
tion). Table 4 shows that by pre-transforming the
WSJ treebank with richer annotation from previ-
ous work, we can obtain state-of-the-art accuracies
of up to 90% F1 with no change to our simple
parser. In STAN-ANNOTATION, we annotate the
treebank symbols with annotations from the Stan-
ford parser (Klein and Manning, 2003). In BERK-
ANNOTATION, we annotate with the splits learned
via hard-EM and 5 split-merge rounds of the Berke-
ley parser (Petrov et al., 2006).
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999846">
Our investigation of shortest-derivation parsing
showed that, in the exact case, SDP performs poorly.
When pruned (and, to a much lesser extent, tie-
broken) by a coarse PCFG, however, it is competi-
tive with a range of other, more complex techniques.
An advantage of this approach is that the fine SDP
pass is actually quite simple compared to typical fine
passes, while still retaining enough complementarity
to the coarse PCFG to increase final accuracies. One
aspect of our findings that may apply more broadly
is the caution that coarse-to-fine methods may some-
times be more critical to end system quality than
generally thought.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999901166666667">
We would like to thank Adam Pauls, Slav Petrov
and the anonymous reviewers for their helpful sug-
gestions. This research is supported by BBN un-
der DARPA contract HR0011-06-C-0022 and by the
Office of Naval Research under MURI Grant No.
N000140911081.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999553057142857">
Mohit Bansal and Dan Klein. 2010. Simple, Accurate
Parsing with an All-Fragments Grammar. In Proceed-
ings of ACL.
Rens Bod. 1993. Using an Annotated Corpus as a
Stochastic Grammar. In Proceedings of EACL.
Rens Bod. 2000. Parsing with the Shortest Derivation.
In Proceedings of COLING.
Rens Bod. 2001. What is the Minimal Set of Fragments
that Achieves Maximum Parse Accuracy? In Proceed-
ings of ACL.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-Based Best-First Chart Parsing. In Pro-
ceedings of the 6th Workshop on Very Large Corpora.
Eugene Charniak, Mark Johnson, et al. 2006. Multi-
level Coarse-to-fine PCFG Parsing. In Proceedings of
HLT-NAACL.
Trevor Cohn and Phil Blunsom. 2010. Blocked Inference
in Bayesian Tree Substitution Grammars. In Proceed-
ings of NAACL.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
A. Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In ACL ’05.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP.
Joshua Goodman. 1996a. Efficient Algorithms for Pars-
ing the DOP Model. In Proceedings of EMNLP.
Joshua Goodman. 1996b. Parsing Algorithms and Met-
rics. In Proceedings of ACL.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod R, Scha R, Sima’an K (eds.)
Data-Oriented Parsing. University of Chicago Press,
Chicago, IL.
</reference>
<page confidence="0.980904">
724
</page>
<reference confidence="0.999902047619048">
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24:613–
632.
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of ACL.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of NAACL-
HLT.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In Proceedings of
ACL-IJCNLP.
Philip Resnik. 1992. Probabilistic Tree-Adjoining
Grammar as a Framework for Statistical Natural Lan-
guage Processing. In Proceedings of COLING.
Khalil Sima’an. 2000. Tree-gram Parsing: Lexical De-
pendencies and Structural Relations. In Proceedings
of ACL.
</reference>
<page confidence="0.998434">
725
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939528">
<title confidence="0.998813">The Surprising Variance in Shortest-Derivation Parsing</title>
<author confidence="0.96021">Bansal</author>
<affiliation confidence="0.9998755">Computer Science University of California,</affiliation>
<abstract confidence="0.998815944444444">We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments. Shortest derivation parsing exhibits an unusual range of behaviors. At one extreme, in the fully unpruned case, it is neither fast nor accurate. At the other extreme, when pruned with a coarse unlexicalized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Simple, Accurate Parsing with an All-Fragments Grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3135" citStr="Bansal and Klein (2010)" startWordPosition="497" endWordPosition="500">P The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully pa1Bod (2000) presented another SDP parser, but with a sampled subset of the training fragments. 720 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720–725, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics rameterized weighting of the substructures in their grammar in an effort to extend the original DOP1 model (Bod, 1993; Goodman, 1996a).</context>
<context position="5530" citStr="Bansal and Klein (2010)" startWordPosition="917" endWordPosition="920">e update is s(Xp, i, j) := s(YQ, i, k) + s(Zr, k, j), where k is a split point in the chart. Using this dynamic program, we compute the exact shortest derivation parse in the full all-fragments grammar (which is reduced to a PCFG with 2 rules schemas as described above). 3 Basic SDP: Inaccurate and Slow SDP in its most basic form is appealingly simple, but has two serious issues: it is both slow and inaccurate. Because there are millions of grammar 2This grammar is a very minor variant of the reduction of SDP suggested by Goodman (2003). 3For a compact WSJ training set with graph packing (see Bansal and Klein (2010)) and one level of parent annotation and markovization, our grammar has 0.9 million indexed symbols compared to 7.5 million unbinarized (and 0.75 million binarized) explicitly-extracted fragments of just depth 1 and 2. Figure 1: SDP - the best parse corresponds to the shortest derivation (fewest switches). symbols, exact SDP parsing takes more than 45 seconds per sentence in our implementation (in addition to being highly memory-intensive). Many methods exist for speeding up parsing through approximation, but basic SDP is too inaccurate to merit them. When implemented as described in Section 2</context>
<context position="10096" citStr="Bansal and Klein (2010)" startWordPosition="1698" endWordPosition="1701">e., the sum of the coarse PCFG chart-cell posteriors P(X, i, j|s) used to build the derivation).4 The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (see 4This is similar to the maximum recall objective for approximate inference (Goodman, 1996b). The product of posteriors also works equally well. dev (&lt; 40) test (&lt; 40) Model F1 EX F1 EX B&amp;K2010 pruned 88.4 33.7 88.5 33.0 B&amp;K2010 unpruned 87.9 32.4 88.1 31.9 Table 1: Accuracy (F1) and exact match (EX) for Bansal and Klein (2010). The pruned row shows their original results with coarse-to-fine pruning. The unpruned row shows new results for an unpruned version of their parser; these accuracies are very similar to their pruned counterparts. Table 2). In addition, the speed of parsing and memory-requirements improve by more than an order of magnitude over the exact SDP pass alone. It is perhaps surprising that coarse-pass pruning improves accuracy by such a large amount for SDP. Indeed, given that past all-fragments work has used a coarse pass for speed, and that we are the first (to our knowledge) to actually parse at </context>
<context position="13277" citStr="Bansal and Klein (2010)" startWordPosition="2232" endWordPosition="2235">s is of course greater for the combined PCFG+SDP model than the exact basic SDP model (which is guaranteed to be minimal). This result provides some analysis of how coarse-pruning helps SDP: it illustrates that the coarse-pass filters out certain short but inaccurate derivations (that the minimal SDP on its own is forced to choose) to improve performance. Figure 3 shows the parsing accuracy of the PCFG+SDP model for various pruning thresholds in coarse-to-fine pruning. Note how this is different from the standard coarse-pass pruning graphs (see Charniak et al. (1998), Petrov and Klein (2007), Bansal and Klein (2010)) where only a small improvement is achieved from pruning. In contrast, coarse-pass pruning provides large accuracy benefits here, perhaps because of the unusual complementarity of the two grammars (typical coarse passes are designed to be as similar as possible to their fine counterparts, even explicitly so in Petrov and Klein (2007)). Our PCFG+SDP parser is more accurate than recent sampling-based TSG’s (Post and Gildea, 2009; Cohn and Blunsom, 2010), who achieve 83-85% F1, and it is competitive with more complex weightedfragment approaches.6 See Bansal and Klein (2010) for a more thorough c</context>
</contexts>
<marker>Bansal, Klein, 2010</marker>
<rawString>Mohit Bansal and Dan Klein. 2010. Simple, Accurate Parsing with an All-Fragments Grammar. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Using an Annotated Corpus as a Stochastic Grammar.</title>
<date>1993</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="2646" citStr="Bod, 1993" startWordPosition="417" endWordPosition="418"> the fine SDP together achieve 87% F1 with basic treebank annotation (see Table 2) and up to 90% F1 with richer treebank annotation (see Table 4). The main contribution of this work is to analyze the behavior of shortest derivation parsing, showing both when it fails and when it succeeds. Our final parser, which combines a simple PCFG coarse pass with an otherwise pure SPD fine pass, can be quite accurate while being straightforward to implement. 2 Implicit Grammar for SDP The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG</context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>Rens Bod. 1993. Using an Annotated Corpus as a Stochastic Grammar. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Parsing with the Shortest Derivation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1272" citStr="Bod, 2000" startWordPosition="187" endWordPosition="188">gment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation. 1 Introduction One guiding intuition in parsing, and data-driven NLP more generally, is that, all else equal, it is advantageous to memorize large fragments of training examples. Taken to the extreme, this intuition suggests shortest derivation parsing (SDP), wherein a test sentence is analyzed in a way which uses as few training fragments as possible (Bod, 2000; Goodman, 2003). SDP certainly has appealing properties: it is simple and parameter free – there need not even be an explicit lexicon. However, SDP may be too simple to be competitive. In this paper, we consider SDP in both its pure form and with several direct modifications, finding a range of behaviors. In its pure form, with no pruning or approximation, SDP is neither fast nor accurate, achieving less than 70% F1 on the English WSJ task. Moreover, basic tie-breaking variants and lexical augmentation are insufficient to achieve competitive accuracies.1 On the other hand, SDP is dramatically</context>
<context position="3311" citStr="Bod (2000)" startWordPosition="530" endWordPosition="532">ll training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully pa1Bod (2000) presented another SDP parser, but with a sampled subset of the training fragments. 720 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720–725, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics rameterized weighting of the substructures in their grammar in an effort to extend the original DOP1 model (Bod, 1993; Goodman, 1996a). However, for SDP, the grammar is even simpler (Goodman, 2003). In principle, the implicit SDP grammar needs just two rule schemas: CONTINUE (Xp —* YQ Zr) and SWITCH (Xp —* XQ)</context>
</contexts>
<marker>Bod, 2000</marker>
<rawString>Rens Bod. 2000. Parsing with the Shortest Derivation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>What is the Minimal Set of Fragments that Achieves Maximum Parse Accuracy?</title>
<date>2001</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2922" citStr="Bod, 2001" startWordPosition="461" endWordPosition="462">it succeeds. Our final parser, which combines a simple PCFG coarse pass with an otherwise pure SPD fine pass, can be quite accurate while being straightforward to implement. 2 Implicit Grammar for SDP The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully pa1Bod (2000) presented another SDP parser, but with a sampled subset of the training fragments. 720 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720–725, Portlan</context>
</contexts>
<marker>Bod, 2001</marker>
<rawString>Rens Bod. 2001. What is the Minimal Set of Fragments that Achieves Maximum Parse Accuracy? In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Edge-Based Best-First Chart Parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="13227" citStr="Charniak et al. (1998)" startWordPosition="2224" endWordPosition="2227">entence length). Note that the number of fragments is of course greater for the combined PCFG+SDP model than the exact basic SDP model (which is guaranteed to be minimal). This result provides some analysis of how coarse-pruning helps SDP: it illustrates that the coarse-pass filters out certain short but inaccurate derivations (that the minimal SDP on its own is forced to choose) to improve performance. Figure 3 shows the parsing accuracy of the PCFG+SDP model for various pruning thresholds in coarse-to-fine pruning. Note how this is different from the standard coarse-pass pruning graphs (see Charniak et al. (1998), Petrov and Klein (2007), Bansal and Klein (2010)) where only a small improvement is achieved from pruning. In contrast, coarse-pass pruning provides large accuracy benefits here, perhaps because of the unusual complementarity of the two grammars (typical coarse passes are designed to be as similar as possible to their fine counterparts, even explicitly so in Petrov and Klein (2007)). Our PCFG+SDP parser is more accurate than recent sampling-based TSG’s (Post and Gildea, 2009; Cohn and Blunsom, 2010), who achieve 83-85% F1, and it is competitive with more complex weightedfragment approaches.6</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998. Edge-Based Best-First Chart Parsing. In Proceedings of the 6th Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Multilevel Coarse-to-fine PCFG Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<marker>Charniak, Johnson, 2006</marker>
<rawString>Eugene Charniak, Mark Johnson, et al. 2006. Multilevel Coarse-to-fine PCFG Parsing. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>Blocked Inference in Bayesian Tree Substitution Grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2970" citStr="Cohn and Blunsom, 2010" startWordPosition="467" endWordPosition="470">ch combines a simple PCFG coarse pass with an otherwise pure SPD fine pass, can be quite accurate while being straightforward to implement. 2 Implicit Grammar for SDP The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully pa1Bod (2000) presented another SDP parser, but with a sampled subset of the training fragments. 720 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720–725, Portland, Oregon, June 19-24, 2011. c�2011 Association </context>
<context position="13733" citStr="Cohn and Blunsom, 2010" startWordPosition="2304" endWordPosition="2307">-to-fine pruning. Note how this is different from the standard coarse-pass pruning graphs (see Charniak et al. (1998), Petrov and Klein (2007), Bansal and Klein (2010)) where only a small improvement is achieved from pruning. In contrast, coarse-pass pruning provides large accuracy benefits here, perhaps because of the unusual complementarity of the two grammars (typical coarse passes are designed to be as similar as possible to their fine counterparts, even explicitly so in Petrov and Klein (2007)). Our PCFG+SDP parser is more accurate than recent sampling-based TSG’s (Post and Gildea, 2009; Cohn and Blunsom, 2010), who achieve 83-85% F1, and it is competitive with more complex weightedfragment approaches.6 See Bansal and Klein (2010) for a more thorough comparison to other parsing work. In addition to being accurate, the PCFG+SDP parser is simple and fast, requiring negligible training and tuning. It takes 2 sec/sentence, less than 2 GB of memory and is written in less than 2000 lines 6Bansal and Klein (2010) achieve around 1.0% higher F1 than our results without a lexicon (character-level parsing) and 1.5% higher F1 with a lexicon. 0 4 8 12 16 20 24 28 32 36 40 sentence length Figure 2: The average nu</context>
</contexts>
<marker>Cohn, Blunsom, 2010</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2010. Blocked Inference in Bayesian Tree Substitution Grammars. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="15292" citStr="Collins (1999)" startWordPosition="2572" endWordPosition="2573">(on WSJ dev-set &lt; 40 words). A larger threshold means more pruning. These are results without the coarseposterior tie-breaking to illustrate the sole effect of pruning. of Java code, including I/O.7 5.1 Other Treebanks One nice property of the parameter-free, allfragments SDP approach is that we can easily transfer it to any new domain with a treebank, or any new annotation of an existing treebank. Table 3 shows domain adaptation performance by the results for training and testing on the Brown and German datasets.8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latentvariable parsers, for which parsing is substantially 7These statistics can be further improved with standard parsing micro-optimization. 8See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. PCFG + SDP SDP 25 20 # of fragments 15 10 5 0 F 1 723 test (&lt; 40) test (all) F1 EX F1 EX Model BROWN Gildea (2001) 84.1 – – – This Paper (PCFG+SDP) 84.7 34.6 83.1 32.6 GERMAN Dubey (2005) 76.3 – – – Petrov and Klein (2007) 80.8 40.8 80.1 39.1 This Paper (PCFG+SDP) 78.1 39.3 77.</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
</authors>
<title>What to do when lexicalization fails: parsing German with suffix analysis and smoothing.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<contexts>
<context position="15341" citStr="Dubey (2005)" startWordPosition="2579" endWordPosition="2580">ns more pruning. These are results without the coarseposterior tie-breaking to illustrate the sole effect of pruning. of Java code, including I/O.7 5.1 Other Treebanks One nice property of the parameter-free, allfragments SDP approach is that we can easily transfer it to any new domain with a treebank, or any new annotation of an existing treebank. Table 3 shows domain adaptation performance by the results for training and testing on the Brown and German datasets.8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latentvariable parsers, for which parsing is substantially 7These statistics can be further improved with standard parsing micro-optimization. 8See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. PCFG + SDP SDP 25 20 # of fragments 15 10 5 0 F 1 723 test (&lt; 40) test (all) F1 EX F1 EX Model BROWN Gildea (2001) 84.1 – – – This Paper (PCFG+SDP) 84.7 34.6 83.1 32.6 GERMAN Dubey (2005) 76.3 – – – Petrov and Klein (2007) 80.8 40.8 80.1 39.1 This Paper (PCFG+SDP) 78.1 39.3 77.1 38.2 Table 3: Results for training and testing </context>
</contexts>
<marker>Dubey, 2005</marker>
<rawString>A. Dubey. 2005. What to do when lexicalization fails: parsing German with suffix analysis and smoothing. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="15529" citStr="Gildea (2001)" startWordPosition="2607" endWordPosition="2608">f the parameter-free, allfragments SDP approach is that we can easily transfer it to any new domain with a treebank, or any new annotation of an existing treebank. Table 3 shows domain adaptation performance by the results for training and testing on the Brown and German datasets.8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latentvariable parsers, for which parsing is substantially 7These statistics can be further improved with standard parsing micro-optimization. 8See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. PCFG + SDP SDP 25 20 # of fragments 15 10 5 0 F 1 723 test (&lt; 40) test (all) F1 EX F1 EX Model BROWN Gildea (2001) 84.1 – – – This Paper (PCFG+SDP) 84.7 34.6 83.1 32.6 GERMAN Dubey (2005) 76.3 – – – Petrov and Klein (2007) 80.8 40.8 80.1 39.1 This Paper (PCFG+SDP) 78.1 39.3 77.1 38.2 Table 3: Results for training and testing on the Brown and German treebanks. Gildea (2001) uses the lexicalized Collins’ Model 1 (Collins, 1999). test (&lt; 40) test (all) Annotation F1 EX F1 EX STAN-ANNOTATION 88.1 34.3 87.4 32.2 BE</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient Algorithms for Parsing the DOP Model.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP. Joshua Goodman. 1996b. Parsing Algorithms and Metrics. In Proceedings of ACL.</booktitle>
<contexts>
<context position="3021" citStr="Goodman (1996" startWordPosition="479" endWordPosition="480">SPD fine pass, can be quite accurate while being straightforward to implement. 2 Implicit Grammar for SDP The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully pa1Bod (2000) presented another SDP parser, but with a sampled subset of the training fragments. 720 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720–725, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics rameterized weighting</context>
<context position="9857" citStr="Goodman, 1996" startWordPosition="1656" endWordPosition="1657">ortest-derivation parsing (after coarse-pruning), if two derivations have the same cost (i.e., the number of switches), then we break the tie between them by choosing the derivation which has a higher sum of coarse posteriors (i.e., the sum of the coarse PCFG chart-cell posteriors P(X, i, j|s) used to build the derivation).4 The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (see 4This is similar to the maximum recall objective for approximate inference (Goodman, 1996b). The product of posteriors also works equally well. dev (&lt; 40) test (&lt; 40) Model F1 EX F1 EX B&amp;K2010 pruned 88.4 33.7 88.5 33.0 B&amp;K2010 unpruned 87.9 32.4 88.1 31.9 Table 1: Accuracy (F1) and exact match (EX) for Bansal and Klein (2010). The pruned row shows their original results with coarse-to-fine pruning. The unpruned row shows new results for an unpruned version of their parser; these accuracies are very similar to their pruned counterparts. Table 2). In addition, the speed of parsing and memory-requirements improve by more than an order of magnitude over the exact SDP pass alone. It i</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996a. Efficient Algorithms for Parsing the DOP Model. In Proceedings of EMNLP. Joshua Goodman. 1996b. Parsing Algorithms and Metrics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2003</date>
<editor>In Bod R, Scha R, Sima’an K (eds.) Data-Oriented Parsing.</editor>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="1288" citStr="Goodman, 2003" startWordPosition="189" endWordPosition="191">aches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation. 1 Introduction One guiding intuition in parsing, and data-driven NLP more generally, is that, all else equal, it is advantageous to memorize large fragments of training examples. Taken to the extreme, this intuition suggests shortest derivation parsing (SDP), wherein a test sentence is analyzed in a way which uses as few training fragments as possible (Bod, 2000; Goodman, 2003). SDP certainly has appealing properties: it is simple and parameter free – there need not even be an explicit lexicon. However, SDP may be too simple to be competitive. In this paper, we consider SDP in both its pure form and with several direct modifications, finding a range of behaviors. In its pure form, with no pruning or approximation, SDP is neither fast nor accurate, achieving less than 70% F1 on the English WSJ task. Moreover, basic tie-breaking variants and lexical augmentation are insufficient to achieve competitive accuracies.1 On the other hand, SDP is dramatically improved in bot</context>
<context position="3797" citStr="Goodman, 2003" startWordPosition="601" endWordPosition="602">on to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully pa1Bod (2000) presented another SDP parser, but with a sampled subset of the training fragments. 720 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720–725, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics rameterized weighting of the substructures in their grammar in an effort to extend the original DOP1 model (Bod, 1993; Goodman, 1996a). However, for SDP, the grammar is even simpler (Goodman, 2003). In principle, the implicit SDP grammar needs just two rule schemas: CONTINUE (Xp —* YQ Zr) and SWITCH (Xp —* XQ), with additive costs 0 and 1, respectively. CONTINUE rules walk along training trees, while SWITCH rules change between trees for a unit cost.2 Assuming that the SWITCH rules are in practice broken down into BEGIN and END sub-rules as in Bansal and Klein (2010), the grammar is linear in the size of the treebank.3 Note that no lexicon is needed in this grammar: lexical switches are like any other. A derivation in our grammar has weight (cost) w where w is the number of switches (or</context>
<context position="5449" citStr="Goodman (2003)" startWordPosition="905" endWordPosition="906">i, j) := s(XQ, i, j) + 1. If we use a continue rule Xp —* YQ Zr, then the update is s(Xp, i, j) := s(YQ, i, k) + s(Zr, k, j), where k is a split point in the chart. Using this dynamic program, we compute the exact shortest derivation parse in the full all-fragments grammar (which is reduced to a PCFG with 2 rules schemas as described above). 3 Basic SDP: Inaccurate and Slow SDP in its most basic form is appealingly simple, but has two serious issues: it is both slow and inaccurate. Because there are millions of grammar 2This grammar is a very minor variant of the reduction of SDP suggested by Goodman (2003). 3For a compact WSJ training set with graph packing (see Bansal and Klein (2010)) and one level of parent annotation and markovization, our grammar has 0.9 million indexed symbols compared to 7.5 million unbinarized (and 0.75 million binarized) explicitly-extracted fragments of just depth 1 and 2. Figure 1: SDP - the best parse corresponds to the shortest derivation (fewest switches). symbols, exact SDP parsing takes more than 45 seconds per sentence in our implementation (in addition to being highly memory-intensive). Many methods exist for speeding up parsing through approximation, but basi</context>
</contexts>
<marker>Goodman, 2003</marker>
<rawString>Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Bod R, Scha R, Sima’an K (eds.) Data-Oriented Parsing. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG Models of Linguistic Tree Representations. Computational Linguistics,</title>
<date>1998</date>
<pages>24--613</pages>
<contexts>
<context position="8203" citStr="Johnson, 1998" startWordPosition="1371" endWordPosition="1372">he girl DT-5 A girl NP-4 NN-6 Test Sentence The girl 721 4 Improvements from a Coarse PCFG The additional information that makes shortest derivation parsing work comes from a coarse unlexicalized PCFG. In the standard way, our PCFG consists of the local (depth-1) rules X —* Y Z with probability P(Y Z|X) computed using the count of the rule and the count of the nonterminal X in the given treebank (no smoothing was used). Our coarse grammar uses a lexicon with unknown word classes, similar to that in Petrov et al. (2006). When taken from a binarized treebank with one level of parent annotation (Johnson, 1998) and horizontal markovization, the PCFG is quite small, with around 3500 symbols and 25000 rules; it achieves an accuracy of 84% on its own (see Table 2), so the PCFG on its own is better than the basic SDP, but still relatively weak. When filtered by a coarse PCFG pass, however, SDP becomes both fast and accurate, even for the basic, lexicon-free SDP formulation. Summed marginals (posteriors) are computed in the coarse PCFG and used for pruning and tie-breaking in the SDP chart, as described next. Pruning works in the standard coarse-to-fine (CTF) way (see Charniak et al. (2006)). If a partic</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24:613– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16743" citStr="Klein and Manning, 2003" startWordPosition="2815" endWordPosition="2818">.4 32.2 BERK-ANNOTATION 90.0 38.9 89.5 36.8 Table 4: Results with richer WSJ-annotations from Stanford and Berkeley parsers. more complex. 5.2 Treebank Annotations PCFG+SDP achieves 87% F1 on the English WSJ task using basic annotation only (i.e., one level of parent annotation and horizontal markovization). Table 4 shows that by pre-transforming the WSJ treebank with richer annotation from previous work, we can obtain state-of-the-art accuracies of up to 90% F1 with no change to our simple parser. In STAN-ANNOTATION, we annotate the treebank symbols with annotations from the Stanford parser (Klein and Manning, 2003). In BERKANNOTATION, we annotate with the splits learned via hard-EM and 5 split-merge rounds of the Berkeley parser (Petrov et al., 2006). 6 Conclusion Our investigation of shortest-derivation parsing showed that, in the exact case, SDP performs poorly. When pruned (and, to a much lesser extent, tiebroken) by a coarse PCFG, however, it is competitive with a range of other, more complex techniques. An advantage of this approach is that the fine SDP pass is actually quite simple compared to typical fine passes, while still retaining enough complementarity to the coarse PCFG to increase final ac</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACLHLT.</booktitle>
<contexts>
<context position="13252" citStr="Petrov and Klein (2007)" startWordPosition="2228" endWordPosition="2231">at the number of fragments is of course greater for the combined PCFG+SDP model than the exact basic SDP model (which is guaranteed to be minimal). This result provides some analysis of how coarse-pruning helps SDP: it illustrates that the coarse-pass filters out certain short but inaccurate derivations (that the minimal SDP on its own is forced to choose) to improve performance. Figure 3 shows the parsing accuracy of the PCFG+SDP model for various pruning thresholds in coarse-to-fine pruning. Note how this is different from the standard coarse-pass pruning graphs (see Charniak et al. (1998), Petrov and Klein (2007), Bansal and Klein (2010)) where only a small improvement is achieved from pruning. In contrast, coarse-pass pruning provides large accuracy benefits here, perhaps because of the unusual complementarity of the two grammars (typical coarse passes are designed to be as similar as possible to their fine counterparts, even explicitly so in Petrov and Klein (2007)). Our PCFG+SDP parser is more accurate than recent sampling-based TSG’s (Post and Gildea, 2009; Cohn and Blunsom, 2010), who achieve 83-85% F1, and it is competitive with more complex weightedfragment approaches.6 See Bansal and Klein (20</context>
<context position="15557" citStr="Petrov and Klein (2007)" startWordPosition="2610" endWordPosition="2613">ee, allfragments SDP approach is that we can easily transfer it to any new domain with a treebank, or any new annotation of an existing treebank. Table 3 shows domain adaptation performance by the results for training and testing on the Brown and German datasets.8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latentvariable parsers, for which parsing is substantially 7These statistics can be further improved with standard parsing micro-optimization. 8See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. PCFG + SDP SDP 25 20 # of fragments 15 10 5 0 F 1 723 test (&lt; 40) test (all) F1 EX F1 EX Model BROWN Gildea (2001) 84.1 – – – This Paper (PCFG+SDP) 84.7 34.6 83.1 32.6 GERMAN Dubey (2005) 76.3 – – – Petrov and Klein (2007) 80.8 40.8 80.1 39.1 This Paper (PCFG+SDP) 78.1 39.3 77.1 38.2 Table 3: Results for training and testing on the Brown and German treebanks. Gildea (2001) uses the lexicalized Collins’ Model 1 (Collins, 1999). test (&lt; 40) test (all) Annotation F1 EX F1 EX STAN-ANNOTATION 88.1 34.3 87.4 32.2 BERK-ANNOTATION 90.0 38.9 89.5</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceedings of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="7151" citStr="Petrov et al. (2006)" startWordPosition="1184" endWordPosition="1187">ample derivations equally), FREQ (use the frequency of local rules). However, none of these methods help much, giving results within a percentage of F1. In fact, even oracle tie-breaking, where ties are broken to favor the number of gold constituents in the derivation achieves only 80% F1, indicating that correct derivations are often not the shortest ones. Another reason for the poor performance of SDP may be that the parameter-free treatment of the lexical layer is particularly pathological. Indeed, this hypothesis is partially verified by the result that using a lexicon (similar to that in Petrov et al. (2006)) at the terminal layer brings the uniform tie-breaking result up to 80% F1. However, combining a lexicon with oracle tie-breaking yields only 81.8% F1. These results at first seem quite discouraging, but we will show that they can be easily improved with information from even a simple PCFG. The girl The girl Test Parse NP-1 DT-2 NN-3 NP-4 DT-5 NN-6 SWITCH DT-2 NP DT NN The girl Derivation 1 Derivation 2 Training Data NP-1 DT-2 NN-3 The girl DT-5 A girl NP-4 NN-6 Test Sentence The girl 721 4 Improvements from a Coarse PCFG The additional information that makes shortest derivation parsing work </context>
<context position="16881" citStr="Petrov et al., 2006" startWordPosition="2839" endWordPosition="2842"> Treebank Annotations PCFG+SDP achieves 87% F1 on the English WSJ task using basic annotation only (i.e., one level of parent annotation and horizontal markovization). Table 4 shows that by pre-transforming the WSJ treebank with richer annotation from previous work, we can obtain state-of-the-art accuracies of up to 90% F1 with no change to our simple parser. In STAN-ANNOTATION, we annotate the treebank symbols with annotations from the Stanford parser (Klein and Manning, 2003). In BERKANNOTATION, we annotate with the splits learned via hard-EM and 5 split-merge rounds of the Berkeley parser (Petrov et al., 2006). 6 Conclusion Our investigation of shortest-derivation parsing showed that, in the exact case, SDP performs poorly. When pruned (and, to a much lesser extent, tiebroken) by a coarse PCFG, however, it is competitive with a range of other, more complex techniques. An advantage of this approach is that the fine SDP pass is actually quite simple compared to typical fine passes, while still retaining enough complementarity to the coarse PCFG to increase final accuracies. One aspect of our findings that may apply more broadly is the caution that coarse-to-fine methods may sometimes be more critical</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian Learning of a Tree Substitution Grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="2945" citStr="Post and Gildea, 2009" startWordPosition="463" endWordPosition="466">. Our final parser, which combines a simple PCFG coarse pass with an otherwise pure SPD fine pass, can be quite accurate while being straightforward to implement. 2 Implicit Grammar for SDP The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully pa1Bod (2000) presented another SDP parser, but with a sampled subset of the training fragments. 720 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720–725, Portland, Oregon, June 19-24, </context>
<context position="13708" citStr="Post and Gildea, 2009" startWordPosition="2300" endWordPosition="2303">ng thresholds in coarse-to-fine pruning. Note how this is different from the standard coarse-pass pruning graphs (see Charniak et al. (1998), Petrov and Klein (2007), Bansal and Klein (2010)) where only a small improvement is achieved from pruning. In contrast, coarse-pass pruning provides large accuracy benefits here, perhaps because of the unusual complementarity of the two grammars (typical coarse passes are designed to be as similar as possible to their fine counterparts, even explicitly so in Petrov and Klein (2007)). Our PCFG+SDP parser is more accurate than recent sampling-based TSG’s (Post and Gildea, 2009; Cohn and Blunsom, 2010), who achieve 83-85% F1, and it is competitive with more complex weightedfragment approaches.6 See Bansal and Klein (2010) for a more thorough comparison to other parsing work. In addition to being accurate, the PCFG+SDP parser is simple and fast, requiring negligible training and tuning. It takes 2 sec/sentence, less than 2 GB of memory and is written in less than 2000 lines 6Bansal and Klein (2010) achieve around 1.0% higher F1 than our results without a lexicon (character-level parsing) and 1.5% higher F1 with a lexicon. 0 4 8 12 16 20 24 28 32 36 40 sentence length</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian Learning of a Tree Substitution Grammar. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2634" citStr="Resnik, 1992" startWordPosition="415" endWordPosition="416">oarse PCFG and the fine SDP together achieve 87% F1 with basic treebank annotation (see Table 2) and up to 90% F1 with richer treebank annotation (see Table 4). The main contribution of this work is to analyze the behavior of shortest derivation parsing, showing both when it fails and when it succeeds. Our final parser, which combines a simple PCFG coarse pass with an otherwise pure SPD fine pass, can be quite accurate while being straightforward to implement. 2 Implicit Grammar for SDP The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phras</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Tree-gram Parsing: Lexical Dependencies and Structural Relations.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Sima’an, 2000</marker>
<rawString>Khalil Sima’an. 2000. Tree-gram Parsing: Lexical Dependencies and Structural Relations. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>