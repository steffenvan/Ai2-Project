<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002935">
<title confidence="0.9966405">
Combining Multiple Knowledge Sources for Dialogue Segmentation in
Multimedia Archives
</title>
<author confidence="0.994996">
Pei-Yun Hsueh
</author>
<affiliation confidence="0.998318">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.859844">
Edinburgh, UK EH8 9WL
</address>
<email confidence="0.999103">
p.hsueh@ed.ac.uk
</email>
<author confidence="0.990651">
Johanna D. Moore
</author>
<affiliation confidence="0.998216">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.859857">
Edinburgh, UK EH8 9WL
</address>
<email confidence="0.999197">
J.Moore@ed.ac.uk
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998675">
Automatic segmentation is important for
making multimedia archives comprehensi-
ble, and for developing downstream infor-
mation retrieval and extraction modules. In
this study, we explore approaches that can
segment multiparty conversational speech
by integrating various knowledge sources
(e.g., words, audio and video recordings,
speaker intention and context). In particu-
lar, we evaluate the performance of a Max-
imum Entropy approach, and examine the
effectiveness of multimodal features on the
task of dialogue segmentation. We also pro-
vide a quantitative account of the effect of
using ASR transcription as opposed to hu-
man transcripts.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997547613636364">
Recent advances in multimedia technologies have
led to huge archives of audio-video recordings of
multiparty conversations in a wide range of areas
including clinical use, online video sharing ser-
vices, and meeting capture and analysis. While it
is straightforward to replay such recordings, find-
ing information from the often lengthy archives is a
more challenging task. Annotating implicit seman-
tics to enhance browsing and searching of recorded
conversational speech has therefore posed new chal-
lenges to the field of multimedia information re-
trieval.
One critical problem is how to divide unstructured
conversational speech into a number of locally co-
herent segments. The problem is important for two
1016
reasons: First, empirical analysis has shown that an-
notating transcripts with semantic information (e.g.,
topics) enables users to browse and find information
from multimedia archives more efficiently (Baner-
jee et al., 2005). Second, because the automatically
generated segments make up for the lack of explicit
orthographic cues (e.g., story and paragraph breaks)
in conversational speech, dialogue segmentation
is useful in many spoken language understanding
tasks, including anaphora resolution (Grosz and Sid-
ner, 1986), information retrieval (e.g., as input for
the TREC Spoken Document Retrieval (SDR) task),
and summarization (Zechner and Waibel, 2000).
This study therefore aims to explore whether a
Maximum Entropy (MaxEnt) classifier can inte-
grate multiple knowledge sources for segmenting
recorded speech. In this paper, we first evaluate the
effectiveness of features that have been proposed in
previous work, with a focus on features that can be
extracted automatically. Second, we examine other
knowledge sources that have not been studied sys-
tematically in previous work, but which we expect
to be good predictors of dialogue segments. In ad-
dition, as our ultimate goal is to develop an infor-
mation retrieval module that can be operated in a
fully automatic fashion, we also investigate the im-
pact of automatic speech recognition (ASR) errors
on the task of dialogue segmentation.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.997644">
In previous work, the problem of automatic dia-
logue segmentation is often considered as similar to
the problem of topic segmentation. Therefore, re-
search has adopted techniques previously developed
</bodyText>
<note confidence="0.9026605">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016–1023,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.995415469387756">
to segment topics in text (Kozima, 1993; Hearst,
1997; Reynar, 1998) and in read speech (e.g., broad-
cast news) (Ponte and Croft, 1997; Allan et al.,
1998). For example, lexical cohesion-based algo-
rithms, such as LCSEG (Galley et al., 2003), or its
word frequency-based predecessor TextTile (Hearst,
1997) capture topic shifts by modeling the similarity
of word repetition in adjacent windows.
However, recent work has shown that LCSEG is
less successful in identifying “agenda-based conver-
sation segments” (e.g., presentation, group discus-
sion) that are typically signalled by differences in
group activity (Hsueh and Moore, 2006). This is
not surprising since LCSEG considers only lexical
cohesion. Previous work has shown that training a
segmentation model with features that are extracted
from knowledge sources other than words, such as
speaker interaction (e.g., overlap rate, pause, and
speaker change) (Galley et al., 2003), or partici-
pant behaviors, e.g., note taking cues (Banerjee and
Rudnicky, 2006), can outperform LCSEG on similar
tasks.
In many other fields of research, a variety of fea-
tures have been identified as indicative of segment
boundaries in different types of recorded speech.
For example, Brown et al. (1980) have shown that
a discourse segment often starts with relatively high
pitched sounds and ends with sounds of pitch within
a more compressed range. Passonneau and Lit-
man (1993) identified that topic shifts often occur
after a pause of relatively long duration. Other
prosodic cues (e.g., pitch contour, energy) have been
studied for their correlation with story segments in
read speech (Tur et al., 2001; Levow, 2004; Chris-
tensen et al., 2005) and with theory-based discourse
segments in spontaneous speech (e.g., direction-
given monologue) (Hirschberg and Nakatani, 1996).
In addition, head and hand/forearm movements are
used to detect group-action based segments (Mc-
Cowan et al., 2005; Al-Hames et al., 2005).
However, many other features that we expect to
signal segment boundaries have not been studied
systematically. For instance, speaker intention (i.e.,
dialogue act types) and conversational context (e.g.,
speaker role). In addition, although these features
are expected to be complementary to one another,
few of the previous studies have looked at the ques-
tion how to use conditional approaches to model the
correlation among features.
</bodyText>
<sectionHeader confidence="0.995381" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.99905">
3.1 Meeting Corpus
</subsectionHeader>
<bodyText confidence="0.99965204">
This study aims to explore approaches that can in-
tegrate multimodal information to discover implicit
semantics from conversation archives. As our goal
is to identify multimodal cues of segmentation in
face-to-face conversation, we use the AMI meeting
corpus (Carletta et al., 2006), which includes audio-
video recordings, to test our approach. In particu-
lar, we are using 50 scenario-based meetings from
the AMI corpus, in which participants are assigned
to different roles and given specific tasks related to
designing a remote control. On average, AMI meet-
ings last 26 minutes, with over 4,700 words tran-
spired. This corpus includes annotation for dialogue
segmentation and topic labels. In the annotation pro-
cess, annotators were given the freedom to subdi-
vide a segment into subsegments to indicate when
the group was discussing a subtopic. Annotators
were also given a set of segment descriptions to be
used as labels. Annotators were instructed to add a
new label only if they could not find a match in the
standard set. The set of segment descriptions can
be divided to three categories: activity-based (e.g.,
presentation, discussion), issue-based (e.g., budget,
usability), and functional segments (e.g., chitchat,
opening, closing).
</bodyText>
<subsectionHeader confidence="0.999358">
3.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999045142857143">
The first step is to break a recorded meeting into
minimal units, which can vary from sentence chunks
to blocks of sentences. In this study, we use spurts,
that is, consecutive speech with no pause longer than
0.5 seconds, as minimal units.
Then, to examine the difference between the set
of features that are characteristic of segmentation at
both coarse and fine levels of granularity, this study
characterizes a dialogue as a sequence of segments
that may be further divided into sub-segments. We
take the theory-free dialogue segmentation annota-
tions in the corpus and flatten the sub-segment struc-
ture and consider only two levels of segmentation:
top-level segments and all sub-level segments.1 We
</bodyText>
<footnote confidence="0.753009">
1We take the spurts which the annotators choose as the be-
ginning of a segment as the topic boundaries. On average,
</footnote>
<page confidence="0.991124">
1017
</page>
<bodyText confidence="0.999804125">
observed that annotators tended to annotate activity-
based segments only at the top level, whereas they
often included sub-topics when segmenting issue-
based segments. For example, a top-level interface
specialist presentation segment can be divided into
agenda/equipment issues, user requirements, exist-
ing products, and look and usability sub-level seg-
ments.
</bodyText>
<subsectionHeader confidence="0.990229">
3.3 Intercoder Agreement
</subsectionHeader>
<bodyText confidence="0.999639136363636">
To measure intercoder agreement, we employ three
different metrics: the kappa coefficient, PK, and
WD. Kappa values measure how well a pair of an-
notators agree on where the segments break. PK is
the probability that two spurts drawn randomly from
a document are incorrectly identified as belonging
to the same segment. WindowDiff (WD) calculates
the error rate by moving a sliding window across the
transcript counting the number of times the hypoth-
esized and reference segment boundaries are differ-
ent. While not uncontroversial, the use of these met-
rics is widespread. Table 1 shows the intercoder
agreement of the top-level and sub-level segmenta-
tion respectively.
It is unclear whether the kappa values shown here
indicate reliable intercoder agreement.2 But given
the low disagreement rate among codings in terms
of the PK and WD scores, we will argue for the reli-
ability of the annotation procedure used in this study.
Also, to our knowledge the reported degree of agree-
ment is the best in the field of meeting dialogue seg-
mentation.3
</bodyText>
<table confidence="0.990756">
Intercoder Kappa PK WD
TOP 0.66 0.11 0.17
SUB 0.59 0.23 0.28
</table>
<tableCaption confidence="0.933795">
Table 1: Intercoder agreement of annotations at the
top-level (TOP) and sub-level (SUB) segments.
</tableCaption>
<bodyText confidence="0.9469131">
the annotators marked 8.7 top-level segments and 14.6 sub-
segments per meeting.
2In computational linguistics, kappa values over 0.67
point to reliable intercoder agreement. But Di Eugenio and
Glass (2004) have found that this interpretation does not hold
true for all tasks.
3For example, Gruenstein et al.(2005) report kappa
(PK/WD) of 0.41(0.28/0.34) for determining the top-level and
0.45(0.27/0.35) for the sub-level segments in the ICSI meeting
corpus.
</bodyText>
<subsectionHeader confidence="0.991117">
3.4 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.996208142857143">
As reported in Section 2, there is a wide range of
features that are potentially characteristic of segment
boundaries, and we expect to find some of them use-
ful for automatic recognition of segment boundaries.
The features we explore can be divided into the fol-
lowing five classes:
Conversational Features: We follow Galley et
al. (2003) and extracted a set of conversational fea-
tures, including the amount of overlapping speech,
the amount of silence between speaker segments,
speaker activity change, the number of cue words,
and the predictions of LCSEG (i.e., the lexical co-
hesion statistics, the estimated posterior probability,
the predicted class).
Lexical Features: We compile the list of words
that occur more than once in the spurts that have
been marked as a top-level or sub-segment boundary
in the training set. Each spurt is then represented as
a vector space of unigrams from this list.
Prosodic Features: We use the direct modelling
approach proposed in Shriberg and Stolcke (2001)
and include maximum F0 and energy of the spurt,
mean F0 and energy of the spurt, pitch contour (i.e.,
slope) and energy at multiple points (e.g., the first
and last 100 and 200 ms, the first and last quarter,
the first and second half) of a spurt. We also include
rate of speech, in-spurt silence, preceding and sub-
sequent pauses, and duration. The rate of speech is
calculated as both the number of words and the num-
ber of syllables spoken per second.
Motion Features: We measure the magnitude
of relevant movements in the meeting room using
methods that detect movements directly from video
recordings in frames of 40 ms. Of special interest are
the frontal shots as recorded by the close up cameras,
the hand movements as recorded by the overview
cameras, and shots of the areas of the room where
presentations are made. We then average the magni-
tude of movements over the frames within a spurt as
its feature value.
Contextual Features: These include dialogue act
type4 and speaker role (e.g., project manager, mar-
</bodyText>
<footnote confidence="0.935745">
4In the annotations, each dialogue act is classified as one
of 15 types, including acts about information exchange (e.g.,
Inform), acts about possible actions (e.g., Suggest), acts whose
primary purpose is to smooth the social functioning (e.g., Be-
positive), acts that are commenting on previous discussion (e.g.,
</footnote>
<page confidence="0.992867">
1018
</page>
<bodyText confidence="0.99978225">
keting expert). As each spurt may consist of multiple
dialogue acts, we represent each spurt as a vector of
dialogue act types, wherein a component is 1 or 0
depending on whether the type occurs in the spurt.
</bodyText>
<subsectionHeader confidence="0.766769">
3.5 Multimodal Integration Using Maximum
Entropy Models
</subsectionHeader>
<bodyText confidence="0.9997410625">
Previous work has used MaxEnt models for sentence
and topic segmentation and shown that conditional
approaches can yield competitive results on these
tasks (Christensen et al., 2005; Hsueh and Moore,
2006). In this study, we also use a MaxEnt clas-
sifier5 for dialogue segmentation under the typical
supervised learning scheme, that is, to train the clas-
sifier to maximize the conditional likelihood over
the training data and then to use the trained model
to predict whether an unseen spurt in the test set
is a segment boundary or not. Because continuous
features have to be discretized for MaxEnt, we ap-
plied a histogram binning approach, which divides
the value range into N intervals that contain an equal
number of counts as specified in the histogram, to
discretize the data.
</bodyText>
<sectionHeader confidence="0.997684" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.930673">
4.1 Probabilistic Models
</subsectionHeader>
<bodyText confidence="0.94254925">
The first question we want to address is whether
the different types of characteristic multimodal fea-
tures can be integrated, using the conditional Max-
Ent model, to automatically detect segment bound-
aries. In this study, we use a set of 50 meet-
ings, which consists of 17,977 spurts. Among these
spurts, only 1.7% and 3.3% are top-level and sub-
segment boundaries. For our experiments we use
10-fold cross validation. The baseline is the re-
sult obtained by using LCSEG, an unsupervised ap-
proach exploiting only lexical cohesion statistics.
Table 2 shows the results obtained by using the
same set of conversational (CONV) features used
in previous work (Galley et al., 2003; Hsueh and
Moore, 2006), and results obtained by using all the
available features (ALL). The evaluation metrics PK
and WD are conventional measures of error rates in
segmentation (see Section 3.3). In Row 2, we see
Elicit-Assessment), and acts that allow complete segmentation
(e.g., Stall).
</bodyText>
<footnote confidence="0.8019915">
5The parameters of the MaxEnt classifier are optimized us-
ing Limited-Memory Variable Metrics.
</footnote>
<table confidence="0.9986828">
TOP SUB
Error Rate PK WD PK WD
BASELINE(LCSEG) 0.40 0.49 0.40 0.47
MAXENT(CONV) 0.34 0.34 0.37 0.37
MAXENT(ALL) 0.30 0.33 0.34 0.36
</table>
<tableCaption confidence="0.70792">
Table 2: Compare the result of MaxEnt models
trained with only conversational features (CONV)
and with all available features (ALL).
</tableCaption>
<bodyText confidence="0.998499454545455">
that using a MaxEnt classifier trained on the conver-
sational features (CONV) alone improves over the
LCSEG baseline by 15.3% for top-level segments
and 6.8% for sub-level segements. Row 3 shows
that combining additional knowledge sources, in-
cluding lexical features (LX1) and the non-verbal
features, prosody (PROS), motion (MOT), and con-
text (CTXT), yields a further improvement (of 8.8%
for top-level segmentation and 5.4% for sub-level
segmentation) over the model trained on conversa-
tional features.
</bodyText>
<subsectionHeader confidence="0.98136">
4.2 Feature Effects
</subsectionHeader>
<bodyText confidence="0.999969083333333">
The second question we want to address is which
knowledge sources (and combinations) are good
predictors for segment boundaries. In this round of
experiments, we evaluate the performance of differ-
ent feature combinations. Table 3 further illustrates
the impact of each feature class on the error rate
metrics (PK/WD). In addition, as the PK and WD
score do not reflect the magnitude of over- or under-
prediction, we also report on the average number of
hypothesized segment boundaries (Hyp). The num-
ber of reference segments in the annotations is 8.7 at
the top-level and 14.6 at the sub-level.
Rows 2-6 in Table 3 show the results of models
trained with each individual feature class. We per-
formed a one-way ANOVA to examine the effect
of different feature classes. The ANOVA suggests
a reliable effect of feature class (F(5, 54) = 36.1;
p &lt; .001). We performed post-hoc tests (Tukey
HSD) to test for significant differences. Analysis
shows that the model that is trained with lexical
features alone (LX1) performs significantly worse
than the LCSEG baseline (p &lt; .001). This is
due to the fact that cue words, such as okay and
now, learned from the training data to signal seg-
</bodyText>
<page confidence="0.993418">
1019
</page>
<table confidence="0.9993804">
TOP SUB
Hyp PK WD Hyp PK WD
BASELINE 17.6 0.40 0.49 17.6 0.40 0.47
(LCSEG)
LX1 61.2 0.53 0.72 65.1 0.49 0.66
CONV 3.1 0.34 0.34 2.9 0.37 0.37
PROS 2.3 0.35 0.35 2.5 0.37 0.37
MOT 96.2 0.36 0.40 96.2 0.38 0.41
CTXT 2.6 0.34 0.34 2.2 0.37 0.37
ALL 7.7 0.29 0.33 7.6 0.35 0.38
</table>
<tableCaption confidence="0.972234">
Table 3: Effects of individual feature classes and
their combination on detecting segment boundaries.
</tableCaption>
<bodyText confidence="0.998753222222222">
ment boundaries, are often used for non-discourse
purposes, such as making a semantic contribution to
an utterance.6 Thus, we hypothesize that these am-
biguous cue words have led the LX1 model to over-
predict. Row 7 further shows that when all avail-
able features (including LX1) are used, the com-
bined model (ALL) yields performance that is sig-
nificantly better than that obtained with individual
feature classes (F(5, 54) = 32.2; p &lt; .001).
</bodyText>
<table confidence="0.999058625">
TOP SUB
Hyp PK WD Hyp PK WD
ALL 7.7 0.29 0.33 7.6 0.35 0.38
ALL-LX1 3.9 0.35 0.35 3.5 0.37 0.38
ALL-CONV 6.6 0.30 0.34 6.8 0.35 0.37
ALL-PROS 5.6 0.29 0.31 7.4 0.33 0.35
ALL-MOTION 7.5 0.30 0.35 7.3 0.35 0.37
ALL-CTXT 7.2 0.29 0.33 6.7 0.36 0.38
</table>
<tableCaption confidence="0.933137">
Table 4: Performance change of taking out each
individual feature class from the ALL model.
</tableCaption>
<bodyText confidence="0.917042714285714">
Table 4 illustrates the error rate change (i.e., in-
creased or decreased PK and WD score)7 that is
incurred by leaving out one feature class from the
ALL model. Results show that CONV, PROS, MO-
TION and CTXT can be taken out from the ALL
model individually without increasing the error rate
significantly.8 Morevoer, the combined models al-
</bodyText>
<footnote confidence="0.986467333333333">
6Hirschberg and Litman (1987) have proposed to discrimi-
nate the different uses intonationally.
7Note that the increase in error rate indicates performance
degradation, and vice versa.
8Sign tests were used to test for significant differences be-
tween means in each fold of cross validation.
</footnote>
<bodyText confidence="0.998935">
ways perform better than the LX1 model (p &lt; .01),
cf. Table 3.
This suggests that the non-lexical feature classes
are complementary to LX1, and thus it is essential
to incorporate some, but not necessarily all, of the
non-lexical classes into the model.
</bodyText>
<table confidence="0.9992958">
TOP SUB
Hyp PK WD Hyp PK WD
LX1 61.2 0.53 0.72 65.1 0.49 0.66
MOT 96.2 0.36 0.40 96.2 0.38 0.41
LX1+CONV 5.3 0.27 0.30 6.9 0.32 0.35
LX1+PROS 6.2 0.30 0.33 7.3 0.36 0.38
LX1+MOT 20.2 0.39 0.49 24.8 0.39 0.47
LX1+CTXT 6.3 0.28 0.31 7.2 0.33 0.35
MOT+PROS 62.0 0.34 0.34 62.1 0.37 0.37
MOT+CTXT 2.7 0.33 0.33 2.3 0.37 0.37
</table>
<tableCaption confidence="0.943181333333333">
Table 5: Effects of combining complementary fea-
tures on detecting segment boundaries.
Table 5 further illustrates the performance of dif-
</tableCaption>
<bodyText confidence="0.993390166666667">
ferent feature combinations on detecting segment
boundaries. By subtracting the PK or WD score in
Row 1, the LX1 model, from that in Rows 3-6, we
can tell how essential each of the non-lexical classes
is to be combined with LX1 into one model. Results
show that CONV is the most essential, followed by
CTXT, PROS and MOT. The advantage of incorpo-
rating the non-lexical feature classes is also shown
in the noticeably reduced number of overpredictions
as compared to that of the LX1 model.
To analyze whether there is a significant interac-
tion between feature classes, we performed another
round of ANOVA tests to examine the effect of LX1
and each of the non-lexical feature classes on de-
tecting segment boundaries. This analysis shows
that there is a significant interaction effect on de-
tecting both top-level and sub-level segment bound-
aries (p &lt; .01), suggesting that the performance of
LX1 is significantly improved when combined with
any non-lexical feature class. Also, among the non-
lexical feature classes, combining prosodic features
significantly improves the performance of the model
in which the motion features are combined to detect
top-level segment boundaries (p &lt; .05).
</bodyText>
<page confidence="0.981653">
1020
</page>
<table confidence="0.99878125">
TOP SUB
Error Rate PK WD PK WD
LCSEG(REF) 0.45 0.57 0.42 0.47
LCSEG(ASR) 0.45 0.58 0.40 0.47
MAXENT-CONV(REF) 0.34 0.34 0.37 0.37
MAXENT-CONV(ASR) 0.34 0.33 0.38 0.38
MAXENT-ALL(REF) 0.30 0.33 0.34 0.36
MAXENT-ALL(ASR) 0.31 0.34 0.34 0.37
</table>
<subsectionHeader confidence="0.993485">
4.3 Degradation Using ASR
</subsectionHeader>
<bodyText confidence="0.999995756756757">
The third question we want to address here is
whether using the output of ASR will cause sig-
nificant degradation to the performance of the seg-
mentation approaches. The ASR transcripts used in
this experiment are obtained using standard technol-
ogy including HMM based acoustic modeling and
N-gram based language models (Hain et al., 2005).
The average word error rates (WER) are 39.1%. We
also applied a word alignment algorithm to ensure
that the number of words in the ASR transcripts is
the same as that in the human-produced transcripts.
In this way we can compare the PK and WD metrics
obtained on the ASR outputs directly with that on
the human transcripts.
In this study, we again use a set of 50 meetings
and 10-fold cross validation. We compare the per-
formance of the reference models, which are trained
on human transcripts and tested on human tran-
scripts, with that of the ASR models, which are
trained on ASR transcripts and tested on ASR tran-
scripts. Table 6 shows that despite the word recogni-
tion errors, none of the LCSEG, the MaxEnt models
trained with conversational features, and the Max-
Ent models trained with all available features per-
form significantly worse on ASR transcripts than on
reference transcripts. One possible explanation for
this, which we have observed in our corpus, is that
the ASR system is likely to mis-recognize different
occurences of words in the same way, and thus the
lexical cohesion statistic, which captures the similar-
ity of word repetition between two adjacency win-
dows, is also likely to remain unchanged. In addi-
tion, when the models are trained with other features
that are not affected by the recognition errors, such
as pause and overlap, the negative impacts of recog-
nition errors are further reduced to an insignificant
level.
</bodyText>
<sectionHeader confidence="0.99907" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.989424909090909">
The results in Section 4 show the benefits of includ-
ing additional knowledge sources for recognizing
segment boundaries. The next question to be ad-
dressed is what features in these sources are most
useful for recognition. To provide a qualitative ac-
count of the segmentation cues, we performed an
analysis to determine whether each proposed feature
Table 6: Effects of word recognition errors on de-
tecting segments boundaries.
discriminates the class of segment boundaries. Pre-
vious work has identified statistical measures (e.g.,
Log Likelihood ratio) that are useful for determin-
ing the statistical association strength (relevance) of
the occurrence of an n-gram feature to target class
(Hsueh and Moore, 2006). Here we extend that
study to calculate the LogLikelihood relevance of all
of the features used in the experiments, and use the
statistics to rank the features.
Our analysis shows that people do speak and be-
have differently near segment boundaries. Some
of the identified segmentation cues match previous
findings. For example, a segment is likely to start
with higher pitched sounds (Brown et al., 1980; Ay-
ers, 1994) and a lower rate of speech (Lehiste, 1980).
Also, interlocutors pause longer than usual to make
sure that everyone is ready to move on to a new dis-
cussion (Brown et al., 1980; Passonneau and Lit-
man, 1993) and use some conventional expressions
(e.g., now, okay, let’s, um, so).
Our analysis also identified segmentation cues
that have not been mentioned in previous research.
For example, interlocutors do not move around a lot
when a new discussion is brought up; interlocutors
mention agenda items (e.g., presentation, meeting)
or content words more often when initiating a new
discussion. Also, from the analysis of current di-
alogue act types and their immediate contexts, we
also observe that at segment boundaries interlocu-
tors do the following more often than usual: start
speaking before they are ready (Stall), give infor-
mation (Inform), elicit an assessment of what has
been said so far (Elicit-assessment), or act to smooth
social functioning and make the group happier (Be-
positive).
</bodyText>
<page confidence="0.979573">
1021
</page>
<sectionHeader confidence="0.701708" genericHeader="conclusions">
6 Conclusions and Future Work ing question is whether it is possible to automat-
</sectionHeader>
<bodyText confidence="0.985285829787234">
This study explores the use of features from mul- ically select the discriminative features for recog-
tiple knowledge sources (i.e., words, prosody, mo- nition. This is particularly important for prosodic
tion, interaction cues, speaker intention and role) for features, because the direct modelling approach we
developing an automatic segmentation component adopted resulted in a large number of features. We
in spontaneous, multiparty conversational speech. expect that by applying feature selection methods
In particular, we addressed the following questions: we can further improve the performance of auto-
(1) Can a MaxEnt classifier integrate the potentially matic segmentation models. In the field of machine
characteristic multimodal features for automatic di- learning and pattern analysis, many methods and se-
alogue segmentation? (2) What are the most dis- lection criteria have been proposed. Our next step
criminative knowledge sources for detecting seg- will be to examine the effectiveness of these meth-
ment boundaries? (3) Does the use of ASR tran- ods for the task of automatic segmentation. Also, we
scription significantly degrade the performance of a will further explore how to choose the best perform-
segmentation model? ing ensemble of knowledge sources so as to facili-
First of all, our results show that a well perform- tate automatic selection of knowledge sources to be
ing MaxEnt model can be trained with available included.
knowledge sources. Our results improve on previous Acknowledgement
work, which uses only conversational features, by This work was supported by the EU 6th FWP IST In-
8.8% for top-level segmentation and 5.4% for sub- tegrated Project AMI (Augmented Multi-party Inter-
level segmentation. Analysis of the effectiveness of action, FP6-506811). Our special thanks to Wessel
the various features shows that lexical features (i.e., Kraaij, Stephan Raaijmakers, Steve Renals, Gabriel
cue words) are the most essential feature class to Murray, Jean Carletta, and the anonymous review-
be combined into the segmentation model. How- ers for valuable comments. Thanks also to the AMI
ever, lexical features must be combined with other ASR group for producing the ASR transcriptions,
features, in particular, conversational features (i.e., and to our research partners in TNO for generating
lexical cohesion, overlap, pause, speaker change), to motion features.
train well performing models. References
In addition, many of the non-lexical feature M. Al-Hames, A. Dielmann, D. GaticaPerez, S. Reiter,
classes, including those that have been identified as S. Renals, and D. Zhang. 2005. Multimodal integra-
indicative of segment boundaries in previous work tion for meeting group action segmentation and recog-
(e.g., prosody) and those that we hypothesized as nition. In Proc. ofMLMI 2005.
good predictors of segment boundaries (e.g., mo- J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
tion, context), are not beneficial for recognizing Y. Yang. 1998. Topic detection and tracking pilot
boundaries when used in isolation. However, these study: Final report. In Proc. of the DARPA Broadcast
non-lexical features are useful when combined with News Transcription and Understanding Workshop.
lexical features, as the presence of the non-lexical G. M. Ayers. 1994. Discourse functions of pitch range in
features can balance the tendency of models trained spontaneous and read speech. In Jennifer J. Venditti,
with lexical cues alone to overpredict. editor, OSU Working Papers in Linguistics, volume 44,
Experiments also show that it is possible to seg- pages 1–49.
ment conversational speech directly on the ASR out- S. Banerjee and A. Rudnicky. 2006. Segmenting meet-
puts. These results encouragingly show that we ings into agenda items by extracting implicit supervi-
can segment conversational speech using features sion from human note-taking. In Proc. ofIUI2006.
extracted from different knowledge sources, and in S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
turn, facilitate the development of a fully automatic necessity of a meeting recording and playback system,
segmentation component for multimedia archives. and the benefit of topic-level annotations to meeting
With the segmentation models developed and dis-
criminative knowledge sources identified, a remain-
1022
</bodyText>
<reference confidence="0.999913070422535">
browsing. In Proc. of the Tenth International Confer-
ence on Human-Computer Interaction.
G. Brown, K. L. Currie, and J. Kenworthe. 1980. Ques-
tions ofIntonation. University Park Press.
J. Carletta et al. 2006. The AMI meeting corpus: A pre-
announcement. In Steve Renals and Samy Bengio, ed-
itors, Springer-Verlag Lecture Notes in Computer Sci-
ence, volume 3869. Springer-Verlag.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2005. Maximum entropy segmentation of broadcast
news. In Proc. ofICASP, Philadelphia USA.
B. Di Eugenio and M. G. Glass. 2004. The kappa
statistic: A second look. Computational Linguistics,
30(1):95–101.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proc. ofACL 2003.
B. Grosz and C. Sidner. 1986. Attention, intentions, and
the structure of discourse. Computational Linguistics,
12(3).
A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meet-
ing structure annotation: Data and tools. In Proc. of
the SIGdial Workshop on Discourse and Dialogue.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, and S. Renals. 2005. Tran-
scription of conference room meetings: An investiga-
tion. In Proc. ofInterspeech 2005.
M. Hearst. 1997. TextTiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguis-
tics, 25(3):527–571.
J. Hirschberg and D. Litman. 1987. Now let’s talk about
now: identifying cue phrases intonationally. In Proc.
ofACL 1987.
J. Hirschberg and C. H. Nakatani. 1996. A prosodic anal-
ysis of discourse segments in direction-giving mono-
logues. In Proc. ofACL 1996.
P. Hsueh and J.D. Moore. 2006. Automatic topic seg-
mentation and lablelling in multiparty dialogue. In the
first IEEE/ACM workshop on Spoken Language Tech-
nology (SLT) 2006.
H. Kozima. 1993. Text segmentation based on similarity
between words. In Proc. ofACL 1993.
I. Lehiste. 1980. Phonetic characteristics of discourse.
In the Meeting of the Committee on Speech Research,
Acoustical Society ofJapan.
G. Levow. 2004. Prosody-based topic segmentation for
mandarin broadcast news. In Proc. ofHLT 2004.
I. McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud,
M. Barnard, and D. Zhang. 2005. Automatic analysis
of multimodal group actions in meetings. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
(PAMI), 27(3):305–317.
R. Passonneau and D. Litman. 1993. Intention-based
segmentation: Human reliability and correlation with
linguistic cues. In Proc. ofACL 1993.
J. Ponte and W. Croft. 1997. Text segmentation by topic.
In Proc. of the Conference on Research and Advanced
Technologyfor Digital Libraries 1997.
J. Reynar. 1998. Topic Segmentation: Algorithms and
Applications. Ph.D. thesis, UPenn, PA USA.
E. Shriberg and A. Stolcke. 2001. Direct modeling of
prosody: An overview of applications in automatic
speech processing. In Proc. International Conference
on Speech Prosody 2004.
G. Tur, D. Hakkani-Tur, A. Stolcke, and E. Shriberg.
2001. Integrating prosodic and lexical cues for auto-
matic topic segmentation. Computational Linguistics,
27(1):31–57.
K. Zechner and A. Waibel. 2000. DIASUMM: Flexi-
ble summarization of spontaneous dialogues in unre-
stricted domains. In Proc. of COLING-2000.
</reference>
<page confidence="0.960121">
1023
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882268">
<title confidence="0.995875">Combining Multiple Knowledge Sources for Dialogue Segmentation in Multimedia Archives</title>
<author confidence="0.999707">Pei-Yun Hsueh</author>
<affiliation confidence="0.999959">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.962219">Edinburgh, UK EH8 9WL</address>
<email confidence="0.988551">p.hsueh@ed.ac.uk</email>
<author confidence="0.99898">Johanna D Moore</author>
<affiliation confidence="0.999945">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.966049">Edinburgh, UK EH8 9WL</address>
<email confidence="0.99684">J.Moore@ed.ac.uk</email>
<abstract confidence="0.998300411764706">Automatic segmentation is important for making multimedia archives comprehensible, and for developing downstream information retrieval and extraction modules. In this study, we explore approaches that can segment multiparty conversational speech by integrating various knowledge sources (e.g., words, audio and video recordings, speaker intention and context). In particular, we evaluate the performance of a Maximum Entropy approach, and examine the effectiveness of multimodal features on the task of dialogue segmentation. We also provide a quantitative account of the effect of using ASR transcription as opposed to human transcripts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>browsing</author>
</authors>
<booktitle>In Proc. of the Tenth International Conference on Human-Computer Interaction.</booktitle>
<marker>browsing, </marker>
<rawString>browsing. In Proc. of the Tenth International Conference on Human-Computer Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Brown</author>
<author>K L Currie</author>
<author>J Kenworthe</author>
</authors>
<title>Questions ofIntonation.</title>
<date>1980</date>
<publisher>University Park Press.</publisher>
<contexts>
<context position="4693" citStr="Brown et al. (1980)" startWordPosition="692" endWordPosition="695">d Moore, 2006). This is not surprising since LCSEG considers only lexical cohesion. Previous work has shown that training a segmentation model with features that are extracted from knowledge sources other than words, such as speaker interaction (e.g., overlap rate, pause, and speaker change) (Galley et al., 2003), or participant behaviors, e.g., note taking cues (Banerjee and Rudnicky, 2006), can outperform LCSEG on similar tasks. In many other fields of research, a variety of features have been identified as indicative of segment boundaries in different types of recorded speech. For example, Brown et al. (1980) have shown that a discourse segment often starts with relatively high pitched sounds and ends with sounds of pitch within a more compressed range. Passonneau and Litman (1993) identified that topic shifts often occur after a pause of relatively long duration. Other prosodic cues (e.g., pitch contour, energy) have been studied for their correlation with story segments in read speech (Tur et al., 2001; Levow, 2004; Christensen et al., 2005) and with theory-based discourse segments in spontaneous speech (e.g., directiongiven monologue) (Hirschberg and Nakatani, 1996). In addition, head and hand/</context>
<context position="23343" citStr="Brown et al., 1980" startWordPosition="3733" endWordPosition="3736">fied statistical measures (e.g., Log Likelihood ratio) that are useful for determining the statistical association strength (relevance) of the occurrence of an n-gram feature to target class (Hsueh and Moore, 2006). Here we extend that study to calculate the LogLikelihood relevance of all of the features used in the experiments, and use the statistics to rank the features. Our analysis shows that people do speak and behave differently near segment boundaries. Some of the identified segmentation cues match previous findings. For example, a segment is likely to start with higher pitched sounds (Brown et al., 1980; Ayers, 1994) and a lower rate of speech (Lehiste, 1980). Also, interlocutors pause longer than usual to make sure that everyone is ready to move on to a new discussion (Brown et al., 1980; Passonneau and Litman, 1993) and use some conventional expressions (e.g., now, okay, let’s, um, so). Our analysis also identified segmentation cues that have not been mentioned in previous research. For example, interlocutors do not move around a lot when a new discussion is brought up; interlocutors mention agenda items (e.g., presentation, meeting) or content words more often when initiating a new discus</context>
</contexts>
<marker>Brown, Currie, Kenworthe, 1980</marker>
<rawString>G. Brown, K. L. Currie, and J. Kenworthe. 1980. Questions ofIntonation. University Park Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>The AMI meeting corpus: A preannouncement.</title>
<date>2006</date>
<booktitle>In Steve Renals and Samy Bengio, editors, Springer-Verlag Lecture Notes in Computer Science,</booktitle>
<volume>volume</volume>
<pages>3869</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Carletta, 2006</marker>
<rawString>J. Carletta et al. 2006. The AMI meeting corpus: A preannouncement. In Steve Renals and Samy Bengio, editors, Springer-Verlag Lecture Notes in Computer Science, volume 3869. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Christensen</author>
<author>B Kolluru</author>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>Maximum entropy segmentation of broadcast news.</title>
<date>2005</date>
<booktitle>In Proc. ofICASP,</booktitle>
<location>Philadelphia USA.</location>
<contexts>
<context position="5136" citStr="Christensen et al., 2005" startWordPosition="763" endWordPosition="767"> many other fields of research, a variety of features have been identified as indicative of segment boundaries in different types of recorded speech. For example, Brown et al. (1980) have shown that a discourse segment often starts with relatively high pitched sounds and ends with sounds of pitch within a more compressed range. Passonneau and Litman (1993) identified that topic shifts often occur after a pause of relatively long duration. Other prosodic cues (e.g., pitch contour, energy) have been studied for their correlation with story segments in read speech (Tur et al., 2001; Levow, 2004; Christensen et al., 2005) and with theory-based discourse segments in spontaneous speech (e.g., directiongiven monologue) (Hirschberg and Nakatani, 1996). In addition, head and hand/forearm movements are used to detect group-action based segments (McCowan et al., 2005; Al-Hames et al., 2005). However, many other features that we expect to signal segment boundaries have not been studied systematically. For instance, speaker intention (i.e., dialogue act types) and conversational context (e.g., speaker role). In addition, although these features are expected to be complementary to one another, few of the previous studie</context>
<context position="12804" citStr="Christensen et al., 2005" startWordPosition="1979" endWordPosition="1982"> possible actions (e.g., Suggest), acts whose primary purpose is to smooth the social functioning (e.g., Bepositive), acts that are commenting on previous discussion (e.g., 1018 keting expert). As each spurt may consist of multiple dialogue acts, we represent each spurt as a vector of dialogue act types, wherein a component is 1 or 0 depending on whether the type occurs in the spurt. 3.5 Multimodal Integration Using Maximum Entropy Models Previous work has used MaxEnt models for sentence and topic segmentation and shown that conditional approaches can yield competitive results on these tasks (Christensen et al., 2005; Hsueh and Moore, 2006). In this study, we also use a MaxEnt classifier5 for dialogue segmentation under the typical supervised learning scheme, that is, to train the classifier to maximize the conditional likelihood over the training data and then to use the trained model to predict whether an unseen spurt in the test set is a segment boundary or not. Because continuous features have to be discretized for MaxEnt, we applied a histogram binning approach, which divides the value range into N intervals that contain an equal number of counts as specified in the histogram, to discretize the data.</context>
</contexts>
<marker>Christensen, Kolluru, Gotoh, Renals, 2005</marker>
<rawString>H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals. 2005. Maximum entropy segmentation of broadcast news. In Proc. ofICASP, Philadelphia USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Di Eugenio</author>
<author>M G Glass</author>
</authors>
<title>The kappa statistic: A second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>B. Di Eugenio and M. G. Glass. 2004. The kappa statistic: A second look. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>E Fosler-Lussier</author>
<author>H Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="3697" citStr="Galley et al., 2003" startWordPosition="543" endWordPosition="546">vious work, the problem of automatic dialogue segmentation is often considered as similar to the problem of topic segmentation. Therefore, research has adopted techniques previously developed Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016–1023, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics to segment topics in text (Kozima, 1993; Hearst, 1997; Reynar, 1998) and in read speech (e.g., broadcast news) (Ponte and Croft, 1997; Allan et al., 1998). For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al., 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. However, recent work has shown that LCSEG is less successful in identifying “agenda-based conversation segments” (e.g., presentation, group discussion) that are typically signalled by differences in group activity (Hsueh and Moore, 2006). This is not surprising since LCSEG considers only lexical cohesion. Previous work has shown that training a segmentation model with features that are extracted from knowledge sources other than words, such a</context>
<context position="10360" citStr="Galley et al. (2003)" startWordPosition="1577" endWordPosition="1580">(2004) have found that this interpretation does not hold true for all tasks. 3For example, Gruenstein et al.(2005) report kappa (PK/WD) of 0.41(0.28/0.34) for determining the top-level and 0.45(0.27/0.35) for the sub-level segments in the ICSI meeting corpus. 3.4 Feature Extraction As reported in Section 2, there is a wide range of features that are potentially characteristic of segment boundaries, and we expect to find some of them useful for automatic recognition of segment boundaries. The features we explore can be divided into the following five classes: Conversational Features: We follow Galley et al. (2003) and extracted a set of conversational features, including the amount of overlapping speech, the amount of silence between speaker segments, speaker activity change, the number of cue words, and the predictions of LCSEG (i.e., the lexical cohesion statistics, the estimated posterior probability, the predicted class). Lexical Features: We compile the list of words that occur more than once in the spurts that have been marked as a top-level or sub-segment boundary in the training set. Each spurt is then represented as a vector space of unigrams from this list. Prosodic Features: We use the direc</context>
<context position="14124" citStr="Galley et al., 2003" startWordPosition="2198" endWordPosition="2201"> different types of characteristic multimodal features can be integrated, using the conditional MaxEnt model, to automatically detect segment boundaries. In this study, we use a set of 50 meetings, which consists of 17,977 spurts. Among these spurts, only 1.7% and 3.3% are top-level and subsegment boundaries. For our experiments we use 10-fold cross validation. The baseline is the result obtained by using LCSEG, an unsupervised approach exploiting only lexical cohesion statistics. Table 2 shows the results obtained by using the same set of conversational (CONV) features used in previous work (Galley et al., 2003; Hsueh and Moore, 2006), and results obtained by using all the available features (ALL). The evaluation metrics PK and WD are conventional measures of error rates in segmentation (see Section 3.3). In Row 2, we see Elicit-Assessment), and acts that allow complete segmentation (e.g., Stall). 5The parameters of the MaxEnt classifier are optimized using Limited-Memory Variable Metrics. TOP SUB Error Rate PK WD PK WD BASELINE(LCSEG) 0.40 0.49 0.40 0.47 MAXENT(CONV) 0.34 0.34 0.37 0.37 MAXENT(ALL) 0.30 0.33 0.34 0.36 Table 2: Compare the result of MaxEnt models trained with only conversational fea</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing. 2003. Discourse segmentation of multi-party conversation. In Proc. ofACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="2182" citStr="Grosz and Sidner, 1986" startWordPosition="309" endWordPosition="313">versational speech into a number of locally coherent segments. The problem is important for two 1016 reasons: First, empirical analysis has shown that annotating transcripts with semantic information (e.g., topics) enables users to browse and find information from multimedia archives more efficiently (Banerjee et al., 2005). Second, because the automatically generated segments make up for the lack of explicit orthographic cues (e.g., story and paragraph breaks) in conversational speech, dialogue segmentation is useful in many spoken language understanding tasks, including anaphora resolution (Grosz and Sidner, 1986), information retrieval (e.g., as input for the TREC Spoken Document Retrieval (SDR) task), and summarization (Zechner and Waibel, 2000). This study therefore aims to explore whether a Maximum Entropy (MaxEnt) classifier can integrate multiple knowledge sources for segmenting recorded speech. In this paper, we first evaluate the effectiveness of features that have been proposed in previous work, with a focus on features that can be extracted automatically. Second, we examine other knowledge sources that have not been studied systematically in previous work, but which we expect to be good predi</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B. Grosz and C. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruenstein</author>
<author>J Niekrasz</author>
<author>M Purver</author>
</authors>
<title>Meeting structure annotation: Data and tools.</title>
<date>2005</date>
<booktitle>In Proc. of the SIGdial Workshop on Discourse</booktitle>
<marker>Gruenstein, Niekrasz, Purver, 2005</marker>
<rawString>A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meeting structure annotation: Data and tools. In Proc. of the SIGdial Workshop on Discourse and Dialogue. T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore, V. Wan, R. Ordelman, and S. Renals. 2005. Transcription of conference room meetings: An investigation. In Proc. ofInterspeech 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multiparagraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="3511" citStr="Hearst, 1997" startWordPosition="514" endWordPosition="515">be operated in a fully automatic fashion, we also investigate the impact of automatic speech recognition (ASR) errors on the task of dialogue segmentation. 2 Previous Work In previous work, the problem of automatic dialogue segmentation is often considered as similar to the problem of topic segmentation. Therefore, research has adopted techniques previously developed Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016–1023, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics to segment topics in text (Kozima, 1993; Hearst, 1997; Reynar, 1998) and in read speech (e.g., broadcast news) (Ponte and Croft, 1997; Allan et al., 1998). For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al., 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. However, recent work has shown that LCSEG is less successful in identifying “agenda-based conversation segments” (e.g., presentation, group discussion) that are typically signalled by differences in group activity (Hsueh and Moore, 2006). This is not surprisin</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M. Hearst. 1997. TextTiling: Segmenting text into multiparagraph subtopic passages. Computational Linguistics, 25(3):527–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
</authors>
<title>Now let’s talk about now: identifying cue phrases intonationally.</title>
<date>1987</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="18002" citStr="Hirschberg and Litman (1987)" startWordPosition="2850" endWordPosition="2854"> 0.35 3.5 0.37 0.38 ALL-CONV 6.6 0.30 0.34 6.8 0.35 0.37 ALL-PROS 5.6 0.29 0.31 7.4 0.33 0.35 ALL-MOTION 7.5 0.30 0.35 7.3 0.35 0.37 ALL-CTXT 7.2 0.29 0.33 6.7 0.36 0.38 Table 4: Performance change of taking out each individual feature class from the ALL model. Table 4 illustrates the error rate change (i.e., increased or decreased PK and WD score)7 that is incurred by leaving out one feature class from the ALL model. Results show that CONV, PROS, MOTION and CTXT can be taken out from the ALL model individually without increasing the error rate significantly.8 Morevoer, the combined models al6Hirschberg and Litman (1987) have proposed to discriminate the different uses intonationally. 7Note that the increase in error rate indicates performance degradation, and vice versa. 8Sign tests were used to test for significant differences between means in each fold of cross validation. ways perform better than the LX1 model (p &lt; .01), cf. Table 3. This suggests that the non-lexical feature classes are complementary to LX1, and thus it is essential to incorporate some, but not necessarily all, of the non-lexical classes into the model. TOP SUB Hyp PK WD Hyp PK WD LX1 61.2 0.53 0.72 65.1 0.49 0.66 MOT 96.2 0.36 0.40 96.2</context>
</contexts>
<marker>Hirschberg, Litman, 1987</marker>
<rawString>J. Hirschberg and D. Litman. 1987. Now let’s talk about now: identifying cue phrases intonationally. In Proc. ofACL 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>C H Nakatani</author>
</authors>
<title>A prosodic analysis of discourse segments in direction-giving monologues.</title>
<date>1996</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="5264" citStr="Hirschberg and Nakatani, 1996" startWordPosition="780" endWordPosition="783"> types of recorded speech. For example, Brown et al. (1980) have shown that a discourse segment often starts with relatively high pitched sounds and ends with sounds of pitch within a more compressed range. Passonneau and Litman (1993) identified that topic shifts often occur after a pause of relatively long duration. Other prosodic cues (e.g., pitch contour, energy) have been studied for their correlation with story segments in read speech (Tur et al., 2001; Levow, 2004; Christensen et al., 2005) and with theory-based discourse segments in spontaneous speech (e.g., directiongiven monologue) (Hirschberg and Nakatani, 1996). In addition, head and hand/forearm movements are used to detect group-action based segments (McCowan et al., 2005; Al-Hames et al., 2005). However, many other features that we expect to signal segment boundaries have not been studied systematically. For instance, speaker intention (i.e., dialogue act types) and conversational context (e.g., speaker role). In addition, although these features are expected to be complementary to one another, few of the previous studies have looked at the question how to use conditional approaches to model the correlation among features. 3 Methodology 3.1 Meeti</context>
</contexts>
<marker>Hirschberg, Nakatani, 1996</marker>
<rawString>J. Hirschberg and C. H. Nakatani. 1996. A prosodic analysis of discourse segments in direction-giving monologues. In Proc. ofACL 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hsueh</author>
<author>J D Moore</author>
</authors>
<title>Automatic topic segmentation and lablelling in multiparty dialogue.</title>
<date>2006</date>
<booktitle>In the first IEEE/ACM workshop on Spoken Language Technology (SLT)</booktitle>
<contexts>
<context position="4088" citStr="Hsueh and Moore, 2006" startWordPosition="598" endWordPosition="601">nt topics in text (Kozima, 1993; Hearst, 1997; Reynar, 1998) and in read speech (e.g., broadcast news) (Ponte and Croft, 1997; Allan et al., 1998). For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al., 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. However, recent work has shown that LCSEG is less successful in identifying “agenda-based conversation segments” (e.g., presentation, group discussion) that are typically signalled by differences in group activity (Hsueh and Moore, 2006). This is not surprising since LCSEG considers only lexical cohesion. Previous work has shown that training a segmentation model with features that are extracted from knowledge sources other than words, such as speaker interaction (e.g., overlap rate, pause, and speaker change) (Galley et al., 2003), or participant behaviors, e.g., note taking cues (Banerjee and Rudnicky, 2006), can outperform LCSEG on similar tasks. In many other fields of research, a variety of features have been identified as indicative of segment boundaries in different types of recorded speech. For example, Brown et al. (</context>
<context position="12828" citStr="Hsueh and Moore, 2006" startWordPosition="1983" endWordPosition="1986">uggest), acts whose primary purpose is to smooth the social functioning (e.g., Bepositive), acts that are commenting on previous discussion (e.g., 1018 keting expert). As each spurt may consist of multiple dialogue acts, we represent each spurt as a vector of dialogue act types, wherein a component is 1 or 0 depending on whether the type occurs in the spurt. 3.5 Multimodal Integration Using Maximum Entropy Models Previous work has used MaxEnt models for sentence and topic segmentation and shown that conditional approaches can yield competitive results on these tasks (Christensen et al., 2005; Hsueh and Moore, 2006). In this study, we also use a MaxEnt classifier5 for dialogue segmentation under the typical supervised learning scheme, that is, to train the classifier to maximize the conditional likelihood over the training data and then to use the trained model to predict whether an unseen spurt in the test set is a segment boundary or not. Because continuous features have to be discretized for MaxEnt, we applied a histogram binning approach, which divides the value range into N intervals that contain an equal number of counts as specified in the histogram, to discretize the data. 4 Experimental Results </context>
<context position="14148" citStr="Hsueh and Moore, 2006" startWordPosition="2202" endWordPosition="2205">haracteristic multimodal features can be integrated, using the conditional MaxEnt model, to automatically detect segment boundaries. In this study, we use a set of 50 meetings, which consists of 17,977 spurts. Among these spurts, only 1.7% and 3.3% are top-level and subsegment boundaries. For our experiments we use 10-fold cross validation. The baseline is the result obtained by using LCSEG, an unsupervised approach exploiting only lexical cohesion statistics. Table 2 shows the results obtained by using the same set of conversational (CONV) features used in previous work (Galley et al., 2003; Hsueh and Moore, 2006), and results obtained by using all the available features (ALL). The evaluation metrics PK and WD are conventional measures of error rates in segmentation (see Section 3.3). In Row 2, we see Elicit-Assessment), and acts that allow complete segmentation (e.g., Stall). 5The parameters of the MaxEnt classifier are optimized using Limited-Memory Variable Metrics. TOP SUB Error Rate PK WD PK WD BASELINE(LCSEG) 0.40 0.49 0.40 0.47 MAXENT(CONV) 0.34 0.34 0.37 0.37 MAXENT(ALL) 0.30 0.33 0.34 0.36 Table 2: Compare the result of MaxEnt models trained with only conversational features (CONV) and with al</context>
<context position="22939" citStr="Hsueh and Moore, 2006" startWordPosition="3667" endWordPosition="3670">ecognizing segment boundaries. The next question to be addressed is what features in these sources are most useful for recognition. To provide a qualitative account of the segmentation cues, we performed an analysis to determine whether each proposed feature Table 6: Effects of word recognition errors on detecting segments boundaries. discriminates the class of segment boundaries. Previous work has identified statistical measures (e.g., Log Likelihood ratio) that are useful for determining the statistical association strength (relevance) of the occurrence of an n-gram feature to target class (Hsueh and Moore, 2006). Here we extend that study to calculate the LogLikelihood relevance of all of the features used in the experiments, and use the statistics to rank the features. Our analysis shows that people do speak and behave differently near segment boundaries. Some of the identified segmentation cues match previous findings. For example, a segment is likely to start with higher pitched sounds (Brown et al., 1980; Ayers, 1994) and a lower rate of speech (Lehiste, 1980). Also, interlocutors pause longer than usual to make sure that everyone is ready to move on to a new discussion (Brown et al., 1980; Passo</context>
</contexts>
<marker>Hsueh, Moore, 2006</marker>
<rawString>P. Hsueh and J.D. Moore. 2006. Automatic topic segmentation and lablelling in multiparty dialogue. In the first IEEE/ACM workshop on Spoken Language Technology (SLT) 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kozima</author>
</authors>
<title>Text segmentation based on similarity between words.</title>
<date>1993</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="3497" citStr="Kozima, 1993" startWordPosition="512" endWordPosition="513">dule that can be operated in a fully automatic fashion, we also investigate the impact of automatic speech recognition (ASR) errors on the task of dialogue segmentation. 2 Previous Work In previous work, the problem of automatic dialogue segmentation is often considered as similar to the problem of topic segmentation. Therefore, research has adopted techniques previously developed Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016–1023, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics to segment topics in text (Kozima, 1993; Hearst, 1997; Reynar, 1998) and in read speech (e.g., broadcast news) (Ponte and Croft, 1997; Allan et al., 1998). For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al., 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. However, recent work has shown that LCSEG is less successful in identifying “agenda-based conversation segments” (e.g., presentation, group discussion) that are typically signalled by differences in group activity (Hsueh and Moore, 2006). This is</context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>H. Kozima. 1993. Text segmentation based on similarity between words. In Proc. ofACL 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Lehiste</author>
</authors>
<title>Phonetic characteristics of discourse.</title>
<date>1980</date>
<booktitle>In the Meeting of the Committee on Speech Research, Acoustical Society ofJapan.</booktitle>
<contexts>
<context position="23400" citStr="Lehiste, 1980" startWordPosition="3746" endWordPosition="3747">re useful for determining the statistical association strength (relevance) of the occurrence of an n-gram feature to target class (Hsueh and Moore, 2006). Here we extend that study to calculate the LogLikelihood relevance of all of the features used in the experiments, and use the statistics to rank the features. Our analysis shows that people do speak and behave differently near segment boundaries. Some of the identified segmentation cues match previous findings. For example, a segment is likely to start with higher pitched sounds (Brown et al., 1980; Ayers, 1994) and a lower rate of speech (Lehiste, 1980). Also, interlocutors pause longer than usual to make sure that everyone is ready to move on to a new discussion (Brown et al., 1980; Passonneau and Litman, 1993) and use some conventional expressions (e.g., now, okay, let’s, um, so). Our analysis also identified segmentation cues that have not been mentioned in previous research. For example, interlocutors do not move around a lot when a new discussion is brought up; interlocutors mention agenda items (e.g., presentation, meeting) or content words more often when initiating a new discussion. Also, from the analysis of current dialogue act typ</context>
</contexts>
<marker>Lehiste, 1980</marker>
<rawString>I. Lehiste. 1980. Phonetic characteristics of discourse. In the Meeting of the Committee on Speech Research, Acoustical Society ofJapan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Levow</author>
</authors>
<title>Prosody-based topic segmentation for mandarin broadcast news.</title>
<date>2004</date>
<booktitle>In Proc. ofHLT</booktitle>
<contexts>
<context position="5109" citStr="Levow, 2004" startWordPosition="761" endWordPosition="762">lar tasks. In many other fields of research, a variety of features have been identified as indicative of segment boundaries in different types of recorded speech. For example, Brown et al. (1980) have shown that a discourse segment often starts with relatively high pitched sounds and ends with sounds of pitch within a more compressed range. Passonneau and Litman (1993) identified that topic shifts often occur after a pause of relatively long duration. Other prosodic cues (e.g., pitch contour, energy) have been studied for their correlation with story segments in read speech (Tur et al., 2001; Levow, 2004; Christensen et al., 2005) and with theory-based discourse segments in spontaneous speech (e.g., directiongiven monologue) (Hirschberg and Nakatani, 1996). In addition, head and hand/forearm movements are used to detect group-action based segments (McCowan et al., 2005; Al-Hames et al., 2005). However, many other features that we expect to signal segment boundaries have not been studied systematically. For instance, speaker intention (i.e., dialogue act types) and conversational context (e.g., speaker role). In addition, although these features are expected to be complementary to one another,</context>
</contexts>
<marker>Levow, 2004</marker>
<rawString>G. Levow. 2004. Prosody-based topic segmentation for mandarin broadcast news. In Proc. ofHLT 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I McCowan</author>
<author>D Gatica-Perez</author>
<author>S Bengio</author>
<author>G Lathoud</author>
<author>M Barnard</author>
<author>D Zhang</author>
</authors>
<title>Automatic analysis of multimodal group actions in meetings.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="5379" citStr="McCowan et al., 2005" startWordPosition="797" endWordPosition="801">high pitched sounds and ends with sounds of pitch within a more compressed range. Passonneau and Litman (1993) identified that topic shifts often occur after a pause of relatively long duration. Other prosodic cues (e.g., pitch contour, energy) have been studied for their correlation with story segments in read speech (Tur et al., 2001; Levow, 2004; Christensen et al., 2005) and with theory-based discourse segments in spontaneous speech (e.g., directiongiven monologue) (Hirschberg and Nakatani, 1996). In addition, head and hand/forearm movements are used to detect group-action based segments (McCowan et al., 2005; Al-Hames et al., 2005). However, many other features that we expect to signal segment boundaries have not been studied systematically. For instance, speaker intention (i.e., dialogue act types) and conversational context (e.g., speaker role). In addition, although these features are expected to be complementary to one another, few of the previous studies have looked at the question how to use conditional approaches to model the correlation among features. 3 Methodology 3.1 Meeting Corpus This study aims to explore approaches that can integrate multimodal information to discover implicit sema</context>
</contexts>
<marker>McCowan, Gatica-Perez, Bengio, Lathoud, Barnard, Zhang, 2005</marker>
<rawString>I. McCowan, D. Gatica-Perez, S. Bengio, G. Lathoud, M. Barnard, and D. Zhang. 2005. Automatic analysis of multimodal group actions in meetings. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 27(3):305–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
<author>D Litman</author>
</authors>
<title>Intention-based segmentation: Human reliability and correlation with linguistic cues.</title>
<date>1993</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="4869" citStr="Passonneau and Litman (1993)" startWordPosition="720" endWordPosition="724">xtracted from knowledge sources other than words, such as speaker interaction (e.g., overlap rate, pause, and speaker change) (Galley et al., 2003), or participant behaviors, e.g., note taking cues (Banerjee and Rudnicky, 2006), can outperform LCSEG on similar tasks. In many other fields of research, a variety of features have been identified as indicative of segment boundaries in different types of recorded speech. For example, Brown et al. (1980) have shown that a discourse segment often starts with relatively high pitched sounds and ends with sounds of pitch within a more compressed range. Passonneau and Litman (1993) identified that topic shifts often occur after a pause of relatively long duration. Other prosodic cues (e.g., pitch contour, energy) have been studied for their correlation with story segments in read speech (Tur et al., 2001; Levow, 2004; Christensen et al., 2005) and with theory-based discourse segments in spontaneous speech (e.g., directiongiven monologue) (Hirschberg and Nakatani, 1996). In addition, head and hand/forearm movements are used to detect group-action based segments (McCowan et al., 2005; Al-Hames et al., 2005). However, many other features that we expect to signal segment bo</context>
<context position="23562" citStr="Passonneau and Litman, 1993" startWordPosition="3773" endWordPosition="3777">2006). Here we extend that study to calculate the LogLikelihood relevance of all of the features used in the experiments, and use the statistics to rank the features. Our analysis shows that people do speak and behave differently near segment boundaries. Some of the identified segmentation cues match previous findings. For example, a segment is likely to start with higher pitched sounds (Brown et al., 1980; Ayers, 1994) and a lower rate of speech (Lehiste, 1980). Also, interlocutors pause longer than usual to make sure that everyone is ready to move on to a new discussion (Brown et al., 1980; Passonneau and Litman, 1993) and use some conventional expressions (e.g., now, okay, let’s, um, so). Our analysis also identified segmentation cues that have not been mentioned in previous research. For example, interlocutors do not move around a lot when a new discussion is brought up; interlocutors mention agenda items (e.g., presentation, meeting) or content words more often when initiating a new discussion. Also, from the analysis of current dialogue act types and their immediate contexts, we also observe that at segment boundaries interlocutors do the following more often than usual: start speaking before they are r</context>
</contexts>
<marker>Passonneau, Litman, 1993</marker>
<rawString>R. Passonneau and D. Litman. 1993. Intention-based segmentation: Human reliability and correlation with linguistic cues. In Proc. ofACL 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ponte</author>
<author>W Croft</author>
</authors>
<title>Text segmentation by topic.</title>
<date>1997</date>
<booktitle>In Proc. of the Conference on Research and Advanced Technologyfor Digital Libraries</booktitle>
<contexts>
<context position="3591" citStr="Ponte and Croft, 1997" startWordPosition="526" endWordPosition="529">t of automatic speech recognition (ASR) errors on the task of dialogue segmentation. 2 Previous Work In previous work, the problem of automatic dialogue segmentation is often considered as similar to the problem of topic segmentation. Therefore, research has adopted techniques previously developed Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016–1023, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics to segment topics in text (Kozima, 1993; Hearst, 1997; Reynar, 1998) and in read speech (e.g., broadcast news) (Ponte and Croft, 1997; Allan et al., 1998). For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al., 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. However, recent work has shown that LCSEG is less successful in identifying “agenda-based conversation segments” (e.g., presentation, group discussion) that are typically signalled by differences in group activity (Hsueh and Moore, 2006). This is not surprising since LCSEG considers only lexical cohesion. Previous work has shown that trai</context>
</contexts>
<marker>Ponte, Croft, 1997</marker>
<rawString>J. Ponte and W. Croft. 1997. Text segmentation by topic. In Proc. of the Conference on Research and Advanced Technologyfor Digital Libraries 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
</authors>
<title>Topic Segmentation: Algorithms and Applications.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>UPenn, PA USA.</institution>
<contexts>
<context position="3526" citStr="Reynar, 1998" startWordPosition="516" endWordPosition="517"> a fully automatic fashion, we also investigate the impact of automatic speech recognition (ASR) errors on the task of dialogue segmentation. 2 Previous Work In previous work, the problem of automatic dialogue segmentation is often considered as similar to the problem of topic segmentation. Therefore, research has adopted techniques previously developed Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016–1023, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics to segment topics in text (Kozima, 1993; Hearst, 1997; Reynar, 1998) and in read speech (e.g., broadcast news) (Ponte and Croft, 1997; Allan et al., 1998). For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al., 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. However, recent work has shown that LCSEG is less successful in identifying “agenda-based conversation segments” (e.g., presentation, group discussion) that are typically signalled by differences in group activity (Hsueh and Moore, 2006). This is not surprising since LCSEG c</context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>J. Reynar. 1998. Topic Segmentation: Algorithms and Applications. Ph.D. thesis, UPenn, PA USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Direct modeling of prosody: An overview of applications in automatic speech processing.</title>
<date>2001</date>
<booktitle>In Proc. International Conference on Speech Prosody</booktitle>
<contexts>
<context position="11020" citStr="Shriberg and Stolcke (2001)" startWordPosition="1682" endWordPosition="1685">ional features, including the amount of overlapping speech, the amount of silence between speaker segments, speaker activity change, the number of cue words, and the predictions of LCSEG (i.e., the lexical cohesion statistics, the estimated posterior probability, the predicted class). Lexical Features: We compile the list of words that occur more than once in the spurts that have been marked as a top-level or sub-segment boundary in the training set. Each spurt is then represented as a vector space of unigrams from this list. Prosodic Features: We use the direct modelling approach proposed in Shriberg and Stolcke (2001) and include maximum F0 and energy of the spurt, mean F0 and energy of the spurt, pitch contour (i.e., slope) and energy at multiple points (e.g., the first and last 100 and 200 ms, the first and last quarter, the first and second half) of a spurt. We also include rate of speech, in-spurt silence, preceding and subsequent pauses, and duration. The rate of speech is calculated as both the number of words and the number of syllables spoken per second. Motion Features: We measure the magnitude of relevant movements in the meeting room using methods that detect movements directly from video record</context>
</contexts>
<marker>Shriberg, Stolcke, 2001</marker>
<rawString>E. Shriberg and A. Stolcke. 2001. Direct modeling of prosody: An overview of applications in automatic speech processing. In Proc. International Conference on Speech Prosody 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>D Hakkani-Tur</author>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Integrating prosodic and lexical cues for automatic topic segmentation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="5096" citStr="Tur et al., 2001" startWordPosition="757" endWordPosition="760">form LCSEG on similar tasks. In many other fields of research, a variety of features have been identified as indicative of segment boundaries in different types of recorded speech. For example, Brown et al. (1980) have shown that a discourse segment often starts with relatively high pitched sounds and ends with sounds of pitch within a more compressed range. Passonneau and Litman (1993) identified that topic shifts often occur after a pause of relatively long duration. Other prosodic cues (e.g., pitch contour, energy) have been studied for their correlation with story segments in read speech (Tur et al., 2001; Levow, 2004; Christensen et al., 2005) and with theory-based discourse segments in spontaneous speech (e.g., directiongiven monologue) (Hirschberg and Nakatani, 1996). In addition, head and hand/forearm movements are used to detect group-action based segments (McCowan et al., 2005; Al-Hames et al., 2005). However, many other features that we expect to signal segment boundaries have not been studied systematically. For instance, speaker intention (i.e., dialogue act types) and conversational context (e.g., speaker role). In addition, although these features are expected to be complementary to</context>
</contexts>
<marker>Tur, Hakkani-Tur, Stolcke, Shriberg, 2001</marker>
<rawString>G. Tur, D. Hakkani-Tur, A. Stolcke, and E. Shriberg. 2001. Integrating prosodic and lexical cues for automatic topic segmentation. Computational Linguistics, 27(1):31–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
<author>A Waibel</author>
</authors>
<title>DIASUMM: Flexible summarization of spontaneous dialogues in unrestricted domains.</title>
<date>2000</date>
<booktitle>In Proc. of COLING-2000.</booktitle>
<contexts>
<context position="2318" citStr="Zechner and Waibel, 2000" startWordPosition="329" endWordPosition="332">s has shown that annotating transcripts with semantic information (e.g., topics) enables users to browse and find information from multimedia archives more efficiently (Banerjee et al., 2005). Second, because the automatically generated segments make up for the lack of explicit orthographic cues (e.g., story and paragraph breaks) in conversational speech, dialogue segmentation is useful in many spoken language understanding tasks, including anaphora resolution (Grosz and Sidner, 1986), information retrieval (e.g., as input for the TREC Spoken Document Retrieval (SDR) task), and summarization (Zechner and Waibel, 2000). This study therefore aims to explore whether a Maximum Entropy (MaxEnt) classifier can integrate multiple knowledge sources for segmenting recorded speech. In this paper, we first evaluate the effectiveness of features that have been proposed in previous work, with a focus on features that can be extracted automatically. Second, we examine other knowledge sources that have not been studied systematically in previous work, but which we expect to be good predictors of dialogue segments. In addition, as our ultimate goal is to develop an information retrieval module that can be operated in a fu</context>
</contexts>
<marker>Zechner, Waibel, 2000</marker>
<rawString>K. Zechner and A. Waibel. 2000. DIASUMM: Flexible summarization of spontaneous dialogues in unrestricted domains. In Proc. of COLING-2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>