<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<note confidence="0.5996565">
Relating Probabilistic Grammars and Automata
Steven Abney David McAllester Fernando Pereira
AT&amp;T Labs-Research
180 Park Ave
</note>
<address confidence="0.668682">
Florham Park NJ 07932
</address>
<email confidence="0.638936">
fabney, dmac, pereiralOresearch.att.com
</email>
<sectionHeader confidence="0.991098" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796444444444">
Both probabilistic context-free grammars
(PCFGs) and shift-reduce probabilistic push-
down automata (PPDAs) have been used for
language modeling and maximum likelihood
parsing. We investigate the precise relationship
between these two formalisms, showing that,
while they define the same classes of probabilis-
tic languages, they appear to impose different
inductive biases.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899463768116">
Current work in stochastic language models
and maximum likelihood parsers falls into two
main approaches. The first approach (Collins,
1998; Charniak, 1997) uses directly the defini-
tion of stochastic grammar, defining the prob-
ability of a parse tree as the probability that
a certain top-down stochastic generative pro-
cess produces that tree. The second approach
(Briscoe and Carroll, 1993; Black et al., 1992;
Magerman, 1994; Ratnaparkhi, 1997; Chelba
and Jelinek, 1998) defines the probability of a
parse tree as the probability that a certain shift-
reduce stochastic parsing automaton outputs
that tree. These two approaches correspond to
the classical notions of context-free grammars
and nondeterministic pushdown automata re-
spectively. It is well known that these two clas-
sical formalisms define the same language class.
In this paper, we show that probabilistic context-
free grammars (PCFGs) and probabilistic push-
down automata (PPDAs) define the same class
of distributions on strings, thus extending the
classical result to the stochastic case. We also
touch on the perhaps more interesting ques-
tion of whether PCFGs and shift-reduce pars-
ing models have the same inductive bias with
respect to the automatic learning of model pa-
rameters from data. Though we cannot provide
a definitive answer, the constructions we use to
answer the equivalence question involve blow-
ups in the number of parameters in both direc-
tions, suggesting that the two models impose
different inductive biases.
We are concerned here with probabilistic
shift-reduce parsing models that define prob-
ability distributions over word sequences, and
in particular the model of Chelba and Je-
linek (1998). Most other probabilistic shift-
reduce parsing models (Briscoe and Carroll,
1993; Black et al., 1992; Magerman, 1994; Rat-
naparkhi, 1997) give only the conditional prob-
ability of a parse tree given a word sequence.
Collins (1998) has argued that those models fail
to capture the appropriate dependency relations
of natural language. Furthermore, they are not
directly comparable to PCFGs, which define
probability distributions over word sequences.
To make the discussion somewhat more con-
crete, we now present a simplified version of the
Chelba-Jelinek model. Consider the following
sentence:
The small woman gave the fat man her
sandwich.
The model under discussion is based on shift-
reduce PPDAs. In such a model, shift transi-
tions generate the next word w and its associ-
ated syntactic category X and push the pair
(X, w) on the stack. Each shift transition
is followed by zero or more reduce transitions
that combine topmost stack entries. For exam-
ple the stack elements (Det, the), (Adj, small),
(N, woman) can be combined to form the single
entry (NP, woman) representing the phrase &amp;quot;the
small woman&amp;quot;. In general each stack entry con-
sists of a syntactic category and a head word.
After generating the prefix &amp;quot;The small woman
gave the fat man&amp;quot; the stack might contain the
sequence (NP, woman)(V, gave)(NP, man). The
Chelba-Jelinek model then executes a shift tran-
</bodyText>
<page confidence="0.980368">
542
</page>
<note confidence="0.751791">
S-4 (S, admired)
</note>
<bodyText confidence="0.940702125">
(S, admired) -4 (NP, Mary) (VP, admired)
(VP, admired) --+ (V, admired)(NP, oak)
(NP, oak) -4 (Det, the)(N, oak)
(N, oak) --+ (Adj, towering)(N, oak)
(N, oak) -4 (Adj, strong)(N, oak)
(N, oak) -4 (Adj, old)(N, oak)
(NP, Mary) --* Mary
(N, oak) --. oak
</bodyText>
<figureCaption confidence="0.999219">
Figure 1: Lexicalized context-free grammar
</figureCaption>
<bodyText confidence="0.999744606741573">
sition by generating the next word. This is
done in a manner similar to that of a trigram
model except that, rather than generate the
next word based on the two preceding words, it
generates the next word based on the two top-
most stack entries. In this example the Chelba-
Jelinek model generates the word &amp;quot;her&amp;quot; from
(V, gave)(NP, man) while a classical trigram
model would generate &amp;quot;her&amp;quot; from &amp;quot;fat man&amp;quot;.
We now contrast Chelba-Jelinek style mod-
els with lexicalized PCFG models. A PCFG is
a context-free grammar in which each produc-
tion is associated with a weight in the interval
[0, 1] and such that the weights of the produc-
tions from any given nonterminal sum to 1. For
instance, the sentence
Mary admired the towering strong old oak
can be derived using a lexicalized PCFG based
on the productions in Figure 1. Production
probabilities in the PCFG would reflect the like-
lihood that a phrase headed by a certain word
can be expanded in a certain way. Since it can
be difficult to estimate fully these likelihoods,
we might restrict ourselves to models based on
bilexical relationships (Eisner, 1997), those be-
tween pairs of words. The simplest bilexical re-
lationship is a bigram statistic, the fraction of
times that &amp;quot;oak&amp;quot; follows &amp;quot;old&amp;quot;. Bilexical rela-
tionships for a PCFG include that between the
head-word of a phrase and the head-word of a
non-head immediate constituent, for instance.
In particular, the generation of the above sen-
tence using a PCFG based on Figure 1 would
exploit a bilexical statistic between &amp;quot;towering&amp;quot;
and &amp;quot;oak&amp;quot; contained in the weight of the fifth
production. This bilexical relationship between
&amp;quot;towering&amp;quot; and &amp;quot;oak&amp;quot; would not be exploited in
either a trigram model or in a Chelba-Jelinek
style model. In a Chelba-Jelinek style model
one must generate &amp;quot;towering&amp;quot; before generating
&amp;quot;oak&amp;quot; and then &amp;quot;oak&amp;quot; must be generated from
(Adj, strong), (Adj, old). In this example the
Chelba-Jelinek model behaves more like a clas-
sical trigram model than like a PCFG model.
This contrast between PPDAs and PCFGs
is formalized in theorem 1, which exhibits a
PCFG for which no stochastic parameterization
of the corresponding shift-reduce parser yields
the same probability distribution over strings.
That is, the standard shift-reduce translation
from CFGs to PDAs cannot be generalized to
the stochastic case.
We give two ways of getting around the above
difficulty. The first is to construct a top-down
PPDA that mimics directly the process of gen-
erating a PCFG derivation from the start sym-
bol by repeatedly replacing the leftmost non-
terminal in a sentential form by the right-hand
side of one of its rules. Theorem 2 states
that any PCFG can be translated into a top-
down PPDA. Conversely, theorem 3 states that
any PPDA can be translated to a PCFG, not
just those that are top-down PPDAs for some
PCFG. Hence PCFGs and general PPDAs de-
fine the same class of stochastic languages.
Unfortunately, top-down PPDAs do not al-
low the simple left-to-right processing that mo-
tivates shift-reduce PPDAs. A second way
around the difficulty formalized in theorem 1
is to encode additional information about the
derivation context with richer stack and state
alphabets. Theorem 7 shows that it is thus
possible to translate an arbitrary PCFG to a
shift-reduce PPDA . The construction requires a
fair amount of machinery including proofs that
any PCFG can be put in Chomsky normal form,
that weights can be renormalized to ensure that
the result of grammar transformations can be
made into PCFGs, that any PCFG can be put
in Greibach normal form, and, finally, that a
Greibach normal form PCFG can be converted
to a shift-reduce PPDA.
The construction also involves a blow-up in
the size of the shift-reduce parsing automaton.
This suggests that some languages that are con-
cisely describable by a PCFG are not concisely
describable by a shift-reduce PPDA, hence that
the class of PCFGs and the class of shift-reduce
PPDAs impose different inductive biases on the
</bodyText>
<page confidence="0.997513">
543
</page>
<bodyText confidence="0.9997896">
CF languages. In the conversion from shift-
reduce PPDAs to PCFGs, there is also a blow-
up, if a less dramatic one, leaving open the pos-
sibility that the biases are incomparable, and
that neither formalism is inherently more con-
cise.
Our main conclusion is then that, while the
generative and shift-reduce parsing approaches
are weakly equivalent, they impose different in-
ductive biases.
</bodyText>
<sectionHeader confidence="0.858548" genericHeader="introduction">
2 Probabilistic and Weighted
Grammars
</sectionHeader>
<bodyText confidence="0.999881609375">
For the remainder of the paper, we fix a terminal
alphabet E and a nonterminal alphabet N, to
which we may add auxiliary symbols as needed.
A weighted context-free grammar (WCFG)
consists of a distinguished start symbol S E N
plus a finite set of weighted productions of the
form X 4 a, (alternately, u : X -4 a), where
X E N,aE (NUE)* and the weight u is a non-
negative real number. A probabilistic context-
free grammar (PCFG) is a WCFG such that for
all X, Eu:x_+au = 1. Since weights are non-
negative, this also implies that u &lt; 1 for any
individual production.
A PCFG defines a stochastic process with
sentential forms as states, and leftmost rewrit-
ing steps as transitions. In the more general
case of WCFGs, we can no longer speak of
stochastic processes; but weighted parse trees
and sets of weighted parse trees are still well-
defined notions.
We define a parse tree to be a tree whose
nodes are labeled with productions. Suppose
node e is labeled X 4 a[Yi, ,17], where we
write a[Yi, , Yn] for a string whose nonter-
minal symbols are Y1, , Y. We say that e&apos;s
nonterminal label is X and its weight is u. The
subtree rooted at e is said to be rooted in X. e is
well-labeled just in case it has n children, whose
nonterminal labels are , Yri, respectively.
Note that a terminal node is well-labeled only
if a is empty or consists exclusively of terminal
symbols. We say a WCFG G admits a tree d
just in case all nodes of d are well-labeled, and
all labels are productions of G. Note that no
requirement is placed on the nonterminal of the
root node of d; in particular, it need not be S.
We define the weight of a tree d, denoted
WG(d), or W(d) if G is clear from context, to be
the product of weights of its nodes. The depth
r(d) of d is the length of the longest path from
root to leaf in d. The root production r(d) is the
label of the root node. The root symbol p(d) is
the left-hand side of 7r(d). The yield c(d) of
the tree d is defined in the standard way as the
string of terminal symbols &amp;quot;parsed&amp;quot; by the tree.
It is convenient to treat the functions 7r, p,
a, and 7 as random variables over trees. We
write, for example, fp = X} as an abbreviation
for fdlp(d) = Xl; and WG(p = X) represents
the sum of weights of such trees. If the sum
diverges, we set WG(p = X) = oo. We call
iiXiiG = W G(p = X) the norm of X, and GI =
the norm of the grammar.
A WCFG G is called convergent if IGII &lt; oo.
If G is a PCFG then IIGII WG(p = 5) &lt; 1,
that is, all PCFGs are convergent. A PCFG
G is called consistent if IIGII = 1. A sufficient
condition for the consistency of a PCFG is given
in (Booth and Thompson, 1973). If 4) and kIf are
two sets of parse trees such that 0 &lt; WG(T) &lt;
oo we define PG (CT) to be WG(nklf) W G (41)
For any terminal string y and grammar G such
that 0 &lt; WG(p = 5) &lt; oo we define PG(y) to
be PG(a = YIP = S).
</bodyText>
<sectionHeader confidence="0.922996" genericHeader="method">
3 Stochastic Push-Down Automata
</sectionHeader>
<bodyText confidence="0.99548925">
We use a somewhat nonstandard definition of
pushdown automaton for convenience, but all
our results hold for a variety of essentially equiv-
alent definitions. In addition to the terminal
alphabet E, we will use sets of stack symbols
and states as needed. A weighted push-down
automaton (WPDA) consists of a distinguished
start state qo, a distinguished start stack symbol
Xo and a finite set of transitions of the following
form where p and q are states, a E EU{ e}, X
and Z1, Zi, are stack symbols, and w is a
nonnegative real weight:
x , p a4v • Zn ,q
A WPDA is a probabilistic push-down automa-
ton (PPDA) if all weights are in the interval
[0, 1] and for each pair of a stack symbol X and
a state q the sum of the weights of all transitions
of the form X , p a4u q equals 1. A ma-
chine configuration is a pair (0, q) of a finite
sequence 13 of stack symbols (a stack) and a ma-
chine state q. A machine configuration is called
halting if the stack is empty. If M is a PPDA
containing the transition X ,p c!4v Z1
then any configuration of the form (#X, p) has
</bodyText>
<page confidence="0.990846">
544
</page>
<bodyText confidence="0.989820795698925">
probability w of being transformed into the con-
figuration (0Z1 Zn, q) where this transfor-
mation has the effect of &amp;quot;outputting&amp;quot; a if a 0 €.
A complete execution of M is a sequence of tran-
sitions between configurations starting in the
initial configuration (X0, q0) and ending in a
configuration with an empty stack. The prob-
ability of a complete execution is the product
of the probabilities of the individual transitions
between configurations in that execution. For
any PPDA M and y E E* we define Pm(y) to
be the sum of the probabilities of all complete
executions outputting y. A PPDA M is called
consistent if EyEE. Pm(y) = I.
We first show that the well known shift-
reduce conversion of CFGs into PDAs can not
be made to handle the stochastic case. Given a
(non-probabilistic) CFG G in Chomsky normal
form we define a (non-probabilistic) shift-reduce
PDA SR(G) as follows. The stack symbols of
SR(G) are taken to be nonterminals of G plus
the special symbols T and I. The states of
SR(G) are in one-to-one correspondence with
the stack symbols and we will abuse notation
by using the same symbols for both states and
stack symbols. The initial stack symbol is 1
and the initial state is (the state corresponding
to) _L. For each production of the form X —&gt; a
in G the PDA SR(G) contains all shift transi-
tions of the following form
Y, Z YZ, X
The PDA SR(G) also contains the following ter-
mination transitions where S is the start symbol
of G.
Note that if G consists entirely of productions of
the form S -4 a these transitions suffice. More
generally, for each production of the form X -4
Y Z in G the PDA SR(G) contains the following
reduce transitions.
All reachable configurations are in one of the
following four forms where the first is the initial
configuration, the second is a template for all
intermediate configurations with a E N*, and
the last two are terminal configurations.
(1, I), (11a, X), (1, T), (E, T)
Furthermore, a configuration of the form
(11a, X) can be reached after outputting y if
and only if aX 4 y. In particular, the machine
can reach configuration (11, S) outputting y
if and only if S 4 y. So the machine SR(G)
generates the same language as G.
We now show that the shift-reduce transla-
tion of CFGs into PDAs does not generalize to
the stochastic case. For any PCFG G we define
the underlying CFG to be the result of erasing
all weights from the productions of G.
Theorem 1 There exists a consistent PCFG G
in Chomsky normal form with underlying CFG
G&apos; such that no consistent weighting M of the
PDA SR(G&apos;) has the property that Pm(y) =
PG(y) for all y E E*.
To prove the theorem take G to be the fol-
lowing grammar.
S AX1, S -4 BYi
xi 4 CX2, X2 4 CA
li. CY2, Y2 4 CB
A ±). a, B b, C c
Note that G generates acca and bccb each
with probability 1. Let M be a consistent
PPDA whose transitions consist of some weight-
ing of the transitions of SR(G&apos;). We will as-
sume that Pm(y) = PG(y) for all y E E*
and derive a contradiction. Call the nonter-
minals A, B, and C preterminals. Note that
the only reduce transitions in SR(G&apos;) com-
bining two preterminals are C, A 4, X2 and
C, B Y2. Hence the only machine configu-
ration reachable after outputting the sequence
acc is (11AC, C). If Pm(acca) and
Pm(accb) 0 then the machine in configuration
(1_LAC, C) must deterministically move to con-
figuration (11ACC, A). But this implies that
configuration (I_LBC , C) also deterministically
moves to configuration (11BCC, A) so we have
Pm(bccb) 0 which violates the assumptions
about M..
Although the standard shift-reduce transla-
tion of CFGs into PDAs fails to generalize to
the stochastic case, the standard top-down con-
version easily generalizes. A top-down PPDA
is one in which only € transitions can cause the
stack to grow and transitions which output a
word must pop the stack.
</bodyText>
<page confidence="0.992363">
545
</page>
<bodyText confidence="0.997186860465116">
Theorem 2 Any string distribution definable
by a consistent PCFG is also definable by a top-
down PPDA.
Theorem 4 For any consistent PCFG G with
PG(E) &lt; 1 there exists a consistent PCFG C(G)
in Chomsky normal form such that, for all y E
Here we consider only PCFGs in Chom-
sky normal form—the generalization to arbi-
trary PCFGs is straightforward. Any PCFG
in Chomsky normal form can be translated to
a top-down PPDA by translating each weighted
production of the form X 4&apos; YZ to the set of
expansion moves of the form W, X 641 WZ, Y
and each production of the form X 4&apos; a to the
set of pop moves of the form Z, X a-4°. , Z.
We also have the following converse of the
above theorem.
Theorem 3 Any string distribution definable
by a consistent PPDA is definable by a PCFG.
The proof, omitted here, uses a weighted ver-
sion of the standard translation of a PDA into
a CFG followed by a renormalization step using
lemma 5. We note that it does in general in-
volve an increase in the number of parameters
in the derived PCFG.
In this paper we are primarily interested in
shift-reduce PPDAs which we now define for-
mally. In a shift-reduce PPDA there is a one-
to-one correspondence between states and stack
symbols and every transition has one of the fol-
lowing two forms.
Y, Z c4u YZ, X a
Transitions of the first type are called shift
transitions and transitions of the second type
are called reduce transitions. Shift transitions
output a terminal symbol and push a single
symbol on the stack. Reduce transitions are
&amp;transitions that combine two stack symbols.
The above theorems leave open the question of
whether shift-reduce PPDAs can express arbi-
trary context-free distributions. Our main the-
orem is that they can. To prove this some ad-
ditional machinery is needed.
</bodyText>
<sectionHeader confidence="0.989377" genericHeader="method">
4 Chomsky Normal Form
</sectionHeader>
<bodyText confidence="0.9823592">
A PCFG is in Chomsky normal form (CNF) if
all productions are either of the form X a,
aEorX4Y1Y2,Yi,Y2EN. Our next
theorem states, in essence, that any PCFG can
be converted to Chomsky normal form.
</bodyText>
<equation confidence="0.9999035">
Pc(G)(Y) = 1 pG(E)
PG(Y) = PG(YIY f)
</equation>
<bodyText confidence="0.999980555555556">
To prove the theorem, note first that, without
loss of generality, we can assume that all pro-
ductions in G are of one of the forms X -u-+ YZ,
X 4 Y, X 4 a, or X e. More specifi-
cally, any production not in one of these forms
must have the form X 4 ai3 where a and P
are nonempty strings. Such a production can
be replaced by X -4 AB, A -1&amp;, and B
where A and B are fresh nonterminal symbols.
By repeatedly applying this binarization trans-
formation we get a grammar in the desired form
defining the same distribution on strings.
We now assume that all productions of G
are in one of the above four forms. This im-
plies that a node in a G-derivation has at most
two children. A node with two children will
be called a branching node. Branching nodes
must be labeled with a production of the form
X 4 YZ. Because G can contain produc-
tions of the form X 4 e there may be ar-
bitrarily large G-derivations with empty yield.
Even G-derivations with nonempty yield may
contain arbitrarily large subtrees with empty
yield. A branching node in the G-derivation
will be called ephemeral if either of its chil-
dren has empty yield. Any G-derivation d with
Icr(d)I &gt; 2 must contain a unique shallowest
non-ephemeral branching node, labeled by some
production X 4 YZ. In this case, define
P(d) = YZ. Otherwise (Icr(d)1 &lt; 2), let i3(d) =-
0-(d). We say that a nonterminal X is nontrivial
in the grammar G if P0(a0elp.X)&gt; 0.
We now define the grammar G&apos; to consist of all
productions of the following form where X, Y,
and Z are nontrivial nonterminals of G and a is
a terminal symbol appearing in G.
</bodyText>
<equation confidence="0.980379333333333">
x PG(0=1&apos; z I P=X, er0c) y z
PG(f3=a p=X, cr0e)
X a
</equation>
<bodyText confidence="0.99928175">
We leave it to the reader to verify that G&apos; has
the property stated in theorem 4. •
The above proof of theorem 4 is non-
constructive in that it does not provide any
</bodyText>
<page confidence="0.984959">
546
</page>
<bodyText confidence="0.740109">
way of computing the conditional probabilities
</bodyText>
<equation confidence="0.957011">
PG(0 = YZ I p = X, cr c) and PG(i3 =
</equation>
<bodyText confidence="0.867821428571428">
a I p = X, a 0 e). However, it is not
difficult to compute probabilities of the form
PG( 4)I p = X, T &lt; t + 1) from probabili-
ties of the form PG(43 I p = X, T &lt; and
PG( 4)I p= X) is the limit as t goes to infinity
of PG(4. I p = X, 7 &lt; t). We omit the details
here.
</bodyText>
<sectionHeader confidence="0.991422" genericHeader="method">
5 Renormalization
</sectionHeader>
<bodyText confidence="0.963071448275862">
A nonterminal X is called reachable in a gram-
mar G if either X is S or there is some (re-
cursively) reachable nonterminal Y such that G
contains a production of the form Y 4 a where
a contains X. A nonterminal X is nonempty
in G if G contains X 4 a where u &gt; 0 and a
contains only terminal symbols, or G contains
X 4 a[Yi, , Yk] where u &gt; 0 and each
Yi is (recursively) nonempty. A WCFG G is
proper if every nonterminal is both reachable
and nonempty. It is possible to efficiently com-
pute the set of reachable and nonempty non-
terminals in any grammar. Furthermore, the
subset of productions involving only nontermi-
nals that are both reachable and nonempty de-
fines the same weight distribution on strings.
So without loss of generality we need only con-
sider proper WCFGs. A reweighting of G is any
WCFG derived from G by changing the weights
of the productions of G.
Lemma 5 For any convergent proper WCFG
G, there exists a reweighting G&apos; of G such that
G&apos; is a consistent PCFG such that for all ter-
minal strings y we have PGI(y) = PG(y).
Proof: Since G is convergent, and every non-
terminal X is reachable, we must have IIXIIG &lt;
oo. We now renormalize all the productions
from X as follows. For each production X 4
a[Yi, Yn] we replace u by
</bodyText>
<equation confidence="0.570116">
u, _ IlYilIG
u 11x1IG
</equation>
<bodyText confidence="0.9813955">
To show that G&apos; is a PCFG we must show
that the sum of the weights of all productions
</bodyText>
<equation confidence="0.979535875">
from X equals 1:
Eu:X-+arYi,•••, n ily.11.
Yni u iixfiG
On- Eu:X--+a[Y1,...,Yn] u Ili IlYilIG
1 E • y u II. wG(P = Yi)
&apos;pay u..X--).a[y 2 ,..., n] 2
WG(p=X)WG(P := X)
1
</equation>
<bodyText confidence="0.999861">
For any parse tree d admitted by G let
d&apos; be the corresponding tree admitted by G&apos;,
that is, the result of reweighting the pro-
ductions in d. One can show by induc-
tion on the depth of parse trees that if
</bodyText>
<equation confidence="0.894506454545455">
p(d) = X then Wo(d&apos;) = W (d
Therefore E{ellp(d)=X} (d&apos;) =
1111IGE{dip(d)=x)WG(d) = 1411WG) = 1. In par-
ticular, HMI = = 1, that is, G&apos; is consis-
tent. This implies that for any terminal string
y we have Po(Y) = thWGi (a = y, p= S)=
WG,(0* = y, p= S). Furthermore, for any tree
d with p(d) = S we have Wo(d&apos;) = 1 W
11,911G— G
and so WG,(cr = y, p = S) = isHKWG(a =
y, p = S) = PG(y). •
</equation>
<sectionHeader confidence="0.962206" genericHeader="method">
6 Greibach Normal Form
</sectionHeader>
<bodyText confidence="0.935073818181818">
A PCFG is in Greibach normal form (GNF) if
every production X 4 a satisfies a E EN*.
The following holds:
Theorem 6 For any consistent PCFG G in
CNF there exists a consistent PCFG G&apos; in GNF
such that Po(y) = PG(y) for y E E*.
Proof: A left corner G-derivation from X to
Y is a G-derivation from X where the leftmost
leaf, rather than being labeled with a produc-
tion, is simply labeled with the nonterminal
Y. For example, if G contains the productions
X &apos;4 YZ and Z 4 a then we can construct a
left corner G-derivation from X to Y by build-
ing a tree with a root labeled by X YZ, a
left child labeled with Y and a right child la-
beled with Zti4 a. The weight of a left corner
G-derivation is the product of the productions
on the nodes. A tree consisting of a single node
labeled with X is a left corner G-derivation from
X to X.
For each pair of nonterminals X, Y in G
we introduce a new nonterminal symbol X/Y.
</bodyText>
<page confidence="0.993532">
547
</page>
<bodyText confidence="0.974519129032258">
The H-derivations from X/Y will be in one
to one correspondence with the left-corner G-
derivations from X to Y. For each production
in G of the form X 4 a we include the following
in H where S is the start symbol of G:
S 4 a S/X .
We also include in H all productions of the fol-
lowing form where X is any nonterminal in G:
XIX 1).
If G consists only of productions of the form
S 4 a these productions suffice. More gener-
ally, for each nonterminal X/Y of H and each
pair of productions U 114 YZ, W 14 a we in-
clude in H the following:
X/Y w42 a Z/W X/U
Because of the productions x/x 4 f, WH(p =
XI X) &gt; 1, and H is not quite in GNF. These
two issues will be addressed momentarily.
Standard arguments can be used to show
that the H-derivations from X/Y are in one-
to-one correspondence with the left corner
derivations from from X to Y. Furthermore, this one-
to-one correspondence preserves weight—if d is
the H-derivation rooted at X/Y corresponding
to the left corner G-derivation from X to Y then
WH (d) is the product of the weights of the pro-
ductions in the G-derivation.
The weight-preserving one-to-one correspon-
dence between left-corner G-derivations from X
to Y and H-derivations from X/Y yields the
following.
</bodyText>
<equation confidence="0.95139725">
WH (ace)
E(S4aS1 X)EH UVVII (Cr = I p= SIX)
E(x4a)eGul/V11(&amp;quot; I P= SIX)
= PG (aa)
</equation>
<bodyText confidence="0.998543571428571">
Theorem 5 implies that we can reweight the
proper subset of H (the reachable and nonempty
productions of H) so as to construct a consistent
PCFG J with Pj(ce) = P(a). To prove theo-
rem 6 it now suffices to show that the produc-
tions of the form XIX 4 c can be eliminated
from the PCFG J. Indeed, we can eliminate
the E productions from J in a manner similar
to that used in the proof of theorem 4. A node
in an J-derivation is ephemeral if it is labeled
X 4 c for some X. We now define a function 7
on J-derivations d as follows. If the root of d is
labeled with X 4 aYZ then we have four sub-
cases. If neither child of the root is ephemeral
then 7(d) is the string aYZ. If only the left child
is ephemeral then -y(d) is aZ. If only the right
child is ephemeral then -y(d) is aY and if both
children are ephemeral then 7(d) is a. Analo-
gously, if the root is labeled with X 4 aY, then
7(d) is aY if the child is not ephemeral and a
otherwise. If the root is labeled with X 4 c
then 7(d) is c.
A nonterminal X in K will be called trivial
if Pj(7 = I p = 1. We now define the
final grammar G&apos; to consist of all productions
of the following form where X, Y, and Z are
nontrivial nonterminals appearing in J and a is
a terminal symbol appearing in J.
</bodyText>
<equation confidence="0.990648166666667">
X PJ(P=a I P=x 700
a
X P.,(a=a17 .14=X, -y0e)
aY
Pj(a=aY Z I p=X,-y0f)
X aYZ
</equation>
<bodyText confidence="0.9264196">
As in section 4, for every nontrivial nonterminal
X in K and terminal string a we have PK (t7 =
alP=X)=PJ(0.=a1P=X,(706).In
particular, since Pj(c) = PG(E) = 0, we have
the following:
</bodyText>
<equation confidence="0.927189">
PK(a) =
= PG (a)
</equation>
<bodyText confidence="0.999454571428572">
The PCFG K is the desired PCFG in Greibach
normal form. •
The construction in this proof is essen-
tially the standard left-corner transformation
(Rosenkrantz and II, 1970), as extended by Sa-
lomaa and Soittola (1978, theorem 2.3) to alge-
braic formal power series.
</bodyText>
<sectionHeader confidence="0.99519" genericHeader="method">
7 The Main Theorem
</sectionHeader>
<bodyText confidence="0.978387">
We can now prove our main theorem.
</bodyText>
<construct confidence="0.955974333333333">
Theorem 7 For any consistent PCFG G there
exists a shift-reduce PPDA M such that
PAI(y) = PG(y) for ally E E*.
</construct>
<bodyText confidence="0.999756666666667">
Let G be an arbitrary consistent PCFG. By
theorems 4 and 6, we can assume that G con-
sists of productions of the form S 4 E and
</bodyText>
<figure confidence="0.534903666666667">
Pj(a=odp=5,a0E)
13:7(a =cdp=--S)
Pj(a)
</figure>
<page confidence="0.919589">
548
</page>
<bodyText confidence="0.980694409090909">
S 1-=*w S&apos; plus productions in Greibach normal
form not mentioning S. We can then replace
the rule S 14u S&apos; with all rules of the form
s -4 a where G contains S&apos; a. We now
assume without loss of generality that G con-
sists of a single production of the form S e
plus productions in Greibach normal form not
mentioning S on the right hand side.
The stack symbols of M are of the form W,
where a E N* is a proper suffix of the right hand
side of some production in G. For example, if
G contains the production X 4 aYZ then the
symbols of M include Wyz, Wy, and W. The
initial state is Ws and the initial stack symbol is
I. We have assumed that G contains a unique
production of the form S 4 E. We include the
following transition in M corresponding to this
production.
-I-, Ws €4v, T
Then, for each rule of the form X 4 a0 in G
and each symbol of the form Wx„ we include
the following in M:
</bodyText>
<equation confidence="0.834874">
Z,Wxaa4 ZWx«,W0
</equation>
<bodyText confidence="0.9992195">
We also include all &amp;quot;post-processing&amp;quot; rules of
the following form:
</bodyText>
<equation confidence="0.937117">
Wxa147€
±,w€ 4,T
1 T , T
</equation>
<bodyText confidence="0.997795666666667">
Note that all reduction transitions are determin-
istic with the single exception of the first rule
listed above. The nondeterministic shift tran-
sitions of M are in one-to-one correspondence
with the productions of G. This yields the prop-
erty that Pm (y) = PG(Y)• •
</bodyText>
<sectionHeader confidence="0.999055" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9999910625">
The relationship between PCFGs and PPDAs
is subtler than a direct application of the clas-
sical constructions relating general CFGs and
PDAs. Although PCFGs can be concisely trans-
lated into top-down PPDAs, we conjecture that
there is no concise translation of PCFGs into
shift-reduce PPDAs. Conversely, there appears
to be no concise translation of shift-reduce PP-
DAs to PCFGs. Our main result is that PCFGs
and shift-reduce PPDAs are intertranslatable,
hence weakly equivalent. However, the non-
conciseness of our translations is consistent with
the view that stochastic top-down generation
models are significantly different from shift-
reduce stochastic parsing models, affecting the
ability to learn a model from examples.
</bodyText>
<sectionHeader confidence="0.9979" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99974854">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling,
volume I. Prentice-Hall, Englewood Cliffs, New
Jersey.
Ezra Black, Fred Jelinek, John Lafferty, David
Magerman, Robert Mercer, and Salim Roukos.
1992. Towards history-based grammars: Using
richer models for probabilistic parsing. In Pro-
ceedings of the 5th DARPA Speech and Natural
Language Workshop.
Taylor Booth and Richard Thompson. 1973. Apply-
ing probability measures to abstract languages.
IEEE Transactions on Computers, C-22(5):442-
450.
Ted Briscoe and John Carroll. 1993. Generalized
probabilistic LR parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):25-59.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics.
In Fourteenth National Conference on Artificial
Intelligence, pages 598-603. AAAI Press/MIT
Press.
Ciprian Chelba and Fred Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In
COLING-ACL &apos;98, pages 225-231.
Michael Collins. 1998. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Jason Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proceedings of the
International Workshop on Parsing Technologies.
David M. Magerman. 1994. Natural Language Pars-
ing as Statistical Pattern Recognition. Ph.D. the-
sis, Department of Computer Science, Stanford
University.
Adwait Ratnaparkhi. 1997. A linear oberved time
statistical parser based on maximum entropy
models. In Claire Cardie and Ralph Weischedel,
editors, Second Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-2),
Somerset, New Jersey. Association For Computa-
tional Linguistics.
Daniel J. Rosenkrantz and Philip M. Lewis II. 1970.
Deterministic left corner parser. In IEEE Con-
ference Record of the 11th Annual Symposium on
Switching and Automata Theory, pages 139-152.
Arto Salomaa and Matti Soittola. 1978. Automata-
Theoretic Aspects of Formal Power Series.
Springer-Verlag, New York.
</reference>
<page confidence="0.998735">
549
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972909">
<title confidence="0.999938">Relating Probabilistic Grammars and Automata</title>
<author confidence="0.999877">Steven Abney David McAllester Fernando Pereira</author>
<affiliation confidence="0.999923">AT&amp;T Labs-Research</affiliation>
<address confidence="0.994826">180 Park Ave Florham Park NJ 07932</address>
<email confidence="0.999739">fabney,dmac,pereiralOresearch.att.com</email>
<abstract confidence="0.9983213">Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing. We investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>The Theory of Parsing, Translation and Compiling, volume I. Prentice-Hall, Englewood Cliffs,</title>
<date>1972</date>
<location>New Jersey.</location>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation and Compiling, volume I. Prentice-Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Fred Jelinek</author>
<author>John Lafferty</author>
<author>David Magerman</author>
<author>Robert Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards history-based grammars: Using richer models for probabilistic parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the 5th DARPA Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="991" citStr="Black et al., 1992" startWordPosition="135" endWordPosition="138">investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on strings, thus extending th</context>
<context position="2357" citStr="Black et al., 1992" startWordPosition="345" endWordPosition="348">ave the same inductive bias with respect to the automatic learning of model parameters from data. Though we cannot provide a definitive answer, the constructions we use to answer the equivalence question involve blowups in the number of parameters in both directions, suggesting that the two models impose different inductive biases. We are concerned here with probabilistic shift-reduce parsing models that define probability distributions over word sequences, and in particular the model of Chelba and Jelinek (1998). Most other probabilistic shiftreduce parsing models (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997) give only the conditional probability of a parse tree given a word sequence. Collins (1998) has argued that those models fail to capture the appropriate dependency relations of natural language. Furthermore, they are not directly comparable to PCFGs, which define probability distributions over word sequences. To make the discussion somewhat more concrete, we now present a simplified version of the Chelba-Jelinek model. Consider the following sentence: The small woman gave the fat man her sandwich. The model under discussion is based on shiftreduce PPDAs. In</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1992</marker>
<rawString>Ezra Black, Fred Jelinek, John Lafferty, David Magerman, Robert Mercer, and Salim Roukos. 1992. Towards history-based grammars: Using richer models for probabilistic parsing. In Proceedings of the 5th DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Booth</author>
<author>Richard Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<pages>22--5</pages>
<contexts>
<context position="10944" citStr="Booth and Thompson, 1973" startWordPosition="1860" endWordPosition="1863">nal symbols &amp;quot;parsed&amp;quot; by the tree. It is convenient to treat the functions 7r, p, a, and 7 as random variables over trees. We write, for example, fp = X} as an abbreviation for fdlp(d) = Xl; and WG(p = X) represents the sum of weights of such trees. If the sum diverges, we set WG(p = X) = oo. We call iiXiiG = W G(p = X) the norm of X, and GI = the norm of the grammar. A WCFG G is called convergent if IGII &lt; oo. If G is a PCFG then IIGII WG(p = 5) &lt; 1, that is, all PCFGs are convergent. A PCFG G is called consistent if IIGII = 1. A sufficient condition for the consistency of a PCFG is given in (Booth and Thompson, 1973). If 4) and kIf are two sets of parse trees such that 0 &lt; WG(T) &lt; oo we define PG (CT) to be WG(nklf) W G (41) For any terminal string y and grammar G such that 0 &lt; WG(p = 5) &lt; oo we define PG(y) to be PG(a = YIP = S). 3 Stochastic Push-Down Automata We use a somewhat nonstandard definition of pushdown automaton for convenience, but all our results hold for a variety of essentially equivalent definitions. In addition to the terminal alphabet E, we will use sets of stack symbols and states as needed. A weighted push-down automaton (WPDA) consists of a distinguished start state qo, a distinguish</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>Taylor Booth and Richard Thompson. 1973. Applying probability measures to abstract languages. IEEE Transactions on Computers, C-22(5):442-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="971" citStr="Briscoe and Carroll, 1993" startWordPosition="131" endWordPosition="134">mum likelihood parsing. We investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on string</context>
<context position="2337" citStr="Briscoe and Carroll, 1993" startWordPosition="341" endWordPosition="344">ift-reduce parsing models have the same inductive bias with respect to the automatic learning of model parameters from data. Though we cannot provide a definitive answer, the constructions we use to answer the equivalence question involve blowups in the number of parameters in both directions, suggesting that the two models impose different inductive biases. We are concerned here with probabilistic shift-reduce parsing models that define probability distributions over word sequences, and in particular the model of Chelba and Jelinek (1998). Most other probabilistic shiftreduce parsing models (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997) give only the conditional probability of a parse tree given a word sequence. Collins (1998) has argued that those models fail to capture the appropriate dependency relations of natural language. Furthermore, they are not directly comparable to PCFGs, which define probability distributions over word sequences. To make the discussion somewhat more concrete, we now present a simplified version of the Chelba-Jelinek model. Consider the following sentence: The small woman gave the fat man her sandwich. The model under discussion is based on s</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<contexts>
<context position="738" citStr="Charniak, 1997" startWordPosition="96" endWordPosition="97"> Florham Park NJ 07932 fabney, dmac, pereiralOresearch.att.com Abstract Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing. We investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well know</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Fourteenth National Conference on Artificial Intelligence, pages 598-603. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Fred Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In COLING-ACL &apos;98,</booktitle>
<pages>225--231</pages>
<contexts>
<context position="1053" citStr="Chelba and Jelinek, 1998" startWordPosition="143" endWordPosition="146">formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on strings, thus extending the classical result to the stochastic case. We also touch on th</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Fred Jelinek. 1998. Exploiting syntactic structure for language modeling. In COLING-ACL &apos;98, pages 225-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="721" citStr="Collins, 1998" startWordPosition="94" endWordPosition="95">ch 180 Park Ave Florham Park NJ 07932 fabney, dmac, pereiralOresearch.att.com Abstract Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing. We investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively</context>
<context position="2485" citStr="Collins (1998)" startWordPosition="368" endWordPosition="369">ive answer, the constructions we use to answer the equivalence question involve blowups in the number of parameters in both directions, suggesting that the two models impose different inductive biases. We are concerned here with probabilistic shift-reduce parsing models that define probability distributions over word sequences, and in particular the model of Chelba and Jelinek (1998). Most other probabilistic shiftreduce parsing models (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997) give only the conditional probability of a parse tree given a word sequence. Collins (1998) has argued that those models fail to capture the appropriate dependency relations of natural language. Furthermore, they are not directly comparable to PCFGs, which define probability distributions over word sequences. To make the discussion somewhat more concrete, we now present a simplified version of the Chelba-Jelinek model. Consider the following sentence: The small woman gave the fat man her sandwich. The model under discussion is based on shiftreduce PPDAs. In such a model, shift transitions generate the next word w and its associated syntactic category X and push the pair (X, w) on th</context>
</contexts>
<marker>Collins, 1998</marker>
<rawString>Michael Collins. 1998. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and a cubictime probabilistic parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="5040" citStr="Eisner, 1997" startWordPosition="801" endWordPosition="802"> context-free grammar in which each production is associated with a weight in the interval [0, 1] and such that the weights of the productions from any given nonterminal sum to 1. For instance, the sentence Mary admired the towering strong old oak can be derived using a lexicalized PCFG based on the productions in Figure 1. Production probabilities in the PCFG would reflect the likelihood that a phrase headed by a certain word can be expanded in a certain way. Since it can be difficult to estimate fully these likelihoods, we might restrict ourselves to models based on bilexical relationships (Eisner, 1997), those between pairs of words. The simplest bilexical relationship is a bigram statistic, the fraction of times that &amp;quot;oak&amp;quot; follows &amp;quot;old&amp;quot;. Bilexical relationships for a PCFG include that between the head-word of a phrase and the head-word of a non-head immediate constituent, for instance. In particular, the generation of the above sentence using a PCFG based on Figure 1 would exploit a bilexical statistic between &amp;quot;towering&amp;quot; and &amp;quot;oak&amp;quot; contained in the weight of the fifth production. This bilexical relationship between &amp;quot;towering&amp;quot; and &amp;quot;oak&amp;quot; would not be exploited in either a trigram model or in a</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Jason Eisner. 1997. Bilexical grammars and a cubictime probabilistic parser. In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Natural Language Parsing as Statistical Pattern Recognition.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Stanford University.</institution>
<contexts>
<context position="1007" citStr="Magerman, 1994" startWordPosition="139" endWordPosition="140">ise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on strings, thus extending the classical resu</context>
<context position="2373" citStr="Magerman, 1994" startWordPosition="349" endWordPosition="350">ve bias with respect to the automatic learning of model parameters from data. Though we cannot provide a definitive answer, the constructions we use to answer the equivalence question involve blowups in the number of parameters in both directions, suggesting that the two models impose different inductive biases. We are concerned here with probabilistic shift-reduce parsing models that define probability distributions over word sequences, and in particular the model of Chelba and Jelinek (1998). Most other probabilistic shiftreduce parsing models (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997) give only the conditional probability of a parse tree given a word sequence. Collins (1998) has argued that those models fail to capture the appropriate dependency relations of natural language. Furthermore, they are not directly comparable to PCFGs, which define probability distributions over word sequences. To make the discussion somewhat more concrete, we now present a simplified version of the Chelba-Jelinek model. Consider the following sentence: The small woman gave the fat man her sandwich. The model under discussion is based on shiftreduce PPDAs. In such a model, s</context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>David M. Magerman. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. thesis, Department of Computer Science, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear oberved time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Claire Cardie and Ralph Weischedel, editors, Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2),</booktitle>
<publisher>Association For Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="1026" citStr="Ratnaparkhi, 1997" startWordPosition="141" endWordPosition="142"> between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on strings, thus extending the classical result to the stochasti</context>
<context position="2393" citStr="Ratnaparkhi, 1997" startWordPosition="351" endWordPosition="353">pect to the automatic learning of model parameters from data. Though we cannot provide a definitive answer, the constructions we use to answer the equivalence question involve blowups in the number of parameters in both directions, suggesting that the two models impose different inductive biases. We are concerned here with probabilistic shift-reduce parsing models that define probability distributions over word sequences, and in particular the model of Chelba and Jelinek (1998). Most other probabilistic shiftreduce parsing models (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997) give only the conditional probability of a parse tree given a word sequence. Collins (1998) has argued that those models fail to capture the appropriate dependency relations of natural language. Furthermore, they are not directly comparable to PCFGs, which define probability distributions over word sequences. To make the discussion somewhat more concrete, we now present a simplified version of the Chelba-Jelinek model. Consider the following sentence: The small woman gave the fat man her sandwich. The model under discussion is based on shiftreduce PPDAs. In such a model, shift transitions gen</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear oberved time statistical parser based on maximum entropy models. In Claire Cardie and Ralph Weischedel, editors, Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2), Somerset, New Jersey. Association For Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel J Rosenkrantz</author>
<author>Philip M Lewis</author>
</authors>
<title>Deterministic left corner parser.</title>
<date>1970</date>
<booktitle>In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory,</booktitle>
<pages>139--152</pages>
<marker>Rosenkrantz, Lewis, 1970</marker>
<rawString>Daniel J. Rosenkrantz and Philip M. Lewis II. 1970. Deterministic left corner parser. In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory, pages 139-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arto Salomaa</author>
<author>Matti Soittola</author>
</authors>
<title>AutomataTheoretic Aspects of Formal Power Series.</title>
<date>1978</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="26317" citStr="Salomaa and Soittola (1978" startWordPosition="4842" endWordPosition="4846">ductions of the following form where X, Y, and Z are nontrivial nonterminals appearing in J and a is a terminal symbol appearing in J. X PJ(P=a I P=x 700 a X P.,(a=a17 .14=X, -y0e) aY Pj(a=aY Z I p=X,-y0f) X aYZ As in section 4, for every nontrivial nonterminal X in K and terminal string a we have PK (t7 = alP=X)=PJ(0.=a1P=X,(706).In particular, since Pj(c) = PG(E) = 0, we have the following: PK(a) = = PG (a) The PCFG K is the desired PCFG in Greibach normal form. • The construction in this proof is essentially the standard left-corner transformation (Rosenkrantz and II, 1970), as extended by Salomaa and Soittola (1978, theorem 2.3) to algebraic formal power series. 7 The Main Theorem We can now prove our main theorem. Theorem 7 For any consistent PCFG G there exists a shift-reduce PPDA M such that PAI(y) = PG(y) for ally E E*. Let G be an arbitrary consistent PCFG. By theorems 4 and 6, we can assume that G consists of productions of the form S 4 E and Pj(a=odp=5,a0E) 13:7(a =cdp=--S) Pj(a) 548 S 1-=*w S&apos; plus productions in Greibach normal form not mentioning S. We can then replace the rule S 14u S&apos; with all rules of the form s -4 a where G contains S&apos; a. We now assume without loss of generality that G con</context>
</contexts>
<marker>Salomaa, Soittola, 1978</marker>
<rawString>Arto Salomaa and Matti Soittola. 1978. AutomataTheoretic Aspects of Formal Power Series. Springer-Verlag, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>