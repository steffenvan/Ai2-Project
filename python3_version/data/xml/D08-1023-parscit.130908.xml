<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001628">
<title confidence="0.997885">
Probabilistic Inference for Machine Translation
</title>
<author confidence="0.992424">
Phil Blunsom and Miles Osborne
</author>
<affiliation confidence="0.999787">
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.983775">
10 Crichton Street, Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.999764">
{pblunsom,miles}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.996675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999488357142857">
We advance the state-of-the-art for discrimi-
natively trained machine translation systems
by presenting novel probabilistic inference
and search methods for synchronous gram-
mars. By approximating the intractable space
of all candidate translations produced by inter-
secting an ngram language model with a
synchronous grammar, we are able to train
and decode models incorporating millions of
sparse, heterogeneous features. Further, we
demonstrate the power of the discriminative
training paradigm by extracting structured
syntactic features, and achieving increases in
translation performance.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977519230769">
The goal of creating statistical machine translation
(SMT) systems incorporating rich, sparse, features
over syntax and morphology has consumed much
recent research attention. Discriminative approaches
are widely seen as a promising technique, poten-
tially allowing us to further the state-of-the-art.
Most work on discriminative training for SMT has
focussed on linear models, often with margin based
algorithms (Liang et al., 2006; Watanabe et al.,
2006), or rescaling a product of sub-models (Och,
2003; Ittycheriah and Roukos, 2007).
Recent work by Blunsom et al. (2008) has shown
how translation can be framed as a probabilistic
log-linear model, where the distribution over trans-
lations is modelled in terms of a latent variable
on derivations. Their approach was globally opti-
mised and discriminative trained. However, a lan-
guage model, an information source known to be
crucial for obtaining good performance in SMT, was
notably omitted. This was because adding a lan-
guage model would mean that the normalising parti-
tion function could no longer be exactly calculated,
thereby preventing efficient parameter estimation.
Here, we show how language models can be
incorporated into large-scale discriminative transla-
tion models, without losing the probabilistic inter-
pretation of the model. The key insight is that we
can use Monte-Carlo methods to approximate the
partition function, thereby allowing us to tackle the
extra computational burden associated with adding
the language model. This approach is theoreti-
cally justified and means that the model contin-
ues to be both probabilistic and globally optimised.
As expected, using a language model dramatically
increases translation performance.
Our second major contribution is an exploita-
tion of syntactic features. By encoding source syn-
tax as features allows the model to use, or ignore,
this information as it sees fit, thereby avoiding the
problems of coverage and sparsity associated with
directly incorporating the syntax into the grammar
(Huang et al., 2006; Mi et al., 2008). We report on
translation gains using this approach.
We begin by introducing the synchronous gram-
mar approach to SMT in Section 2. In Section
3 we define the parametric form of our model
and describe techniques for approximating the
intractable space of all translations for a given
source sentence. In Section 4 we evaluate the abil-
ity of our model to effectively estimate the highly
dependent weights for the sparse features and real-
valued language model. In addition we describe how
</bodyText>
<page confidence="0.987076">
215
</page>
<note confidence="0.9551595">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215–223,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<equation confidence="0.994342333333333">
S � 〈X1 n o , X1 .〉
X1 � 〈X2 是 昨天 深夜 X3 , X2 X3 late last night〉
X2 � 〈布朗 , Brown〉
X3 � 〈从 X4 抵e X5 , arrived in X5 from X4〉
X4 � 〈�54 , Shanghai〉
X5 � 〈北京 , Beijing〉
</equation>
<figureCaption confidence="0.918111">
Figure 1. An example SCFG derivation from a Chi-
nese source sentence which yields the English sentence:
“Brown arrived in Shanghai from Beijing late last night.”
</figureCaption>
<bodyText confidence="0.986873666666667">
our model can easily integrate rich features over
source syntax trees and compare our training meth-
ods to a state-of-the-art benchmark.
</bodyText>
<sectionHeader confidence="0.868019" genericHeader="method">
2 Synchronous context free grammar
</sectionHeader>
<bodyText confidence="0.999981692307692">
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) describes the gener-
ation of pairs of strings. A string pair is generated
by applying a series of paired context-free rewrite
rules of the form, X —* (a, -y, —), where X is a non-
terminal, a and -y are strings of terminals and non-
terminals and — specifies a one-to-one alignment
between non-terminals in a and -y. In the context of
SMT, by assigning the source and target languages
to the respective sides of a SCFG it is possible to
describe translation as the process of parsing the
source sentence, while generating the target trans-
lation (Chiang, 2007).
In this paper we only consider grammars
extracted using the heuristics described for the Hiero
SMT system (Chiang, 2007). Note however that our
approach is general and could be used with other
synchronous grammar transducers (e.g., (Galley et
al., 2006)). SCFG productions can specify that the
order of the child non-terminals is the same in both
languages (a monotone production), or is reversed (a
reordering production). Without loss of generality,
here we add the restriction that non-terminals on the
source and target sides of the grammar must have the
same category. Figure 1 shows an example deriva-
tion for Chinese to English translation.
</bodyText>
<sectionHeader confidence="0.987075" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999354583333333">
We start by defining a log-linear model for the con-
ditional probability distribution over target transla-
tions of a given source sentence. A sequence of
SCFG rule applications which produce a translation
from a source sentence is referred to as a derivation,
and each translation may be produced by many dif-
ferent derivations. As the training data only provides
source and target sentences, the derivations are mod-
elled as a latent variable.
The conditional probability of a derivation, d, for
a target translation, e, conditioned on the source, f,
is given by:
</bodyText>
<equation confidence="0.9467788">
exp Ek AkHk(d, e, f)
pA(d, e|f) = (1)
ZA(f)
1: where Hk(d, e, f) = hk(f, r, q(r, d)) (2)
r∈d
</equation>
<bodyText confidence="0.938633">
Using Equation (1), the conditional probability of
a target translation given the source is the sum over
all of its derivations:
</bodyText>
<equation confidence="0.999462">
pA(e|f) = 1: pA(d, e|f)
d∈0(e,f)
</equation>
<bodyText confidence="0.960258333333333">
where A(e, f) is the set of all derivations of the
target sentence e from the source f.
Here k ranges over the model’s features, and
A = {AkI are the model parameters (weights for
their corresponding features). The function q(r, d)
returns the target ngram context, for a language
model with order m, of rule r in derivation d.
For a rule which spans the target words (i, j) and
target yield(d) = {t0, · · · , tlI:
</bodyText>
<equation confidence="0.8106055">
q(r, d = I ti ··· ti+m−2*tj−m+2 ··· tj if j — i &gt; m
ti···tj otherwise
</equation>
<bodyText confidence="0.9997255">
The feature functions hk are real-valued functions
over the source and target sentences, and can include
overlapping and non-independent features of the
data. The features must decompose with the deriva-
tion and the ngram context defined by the function q,
as shown in Equation (2). The features can reference
the entire source sentence coupled with each rule, r,
and its target context, in a derivation.
By directly incorporating the language model
context q into the model formulation, we will not
</bodyText>
<page confidence="0.998118">
216
</page>
<bodyText confidence="0.983430307692308">
be able to exactly compute the partition function
ZA(f), which sums over all possible derivations.
Even though a dynamic program over this space
would still run in polynomial time, as shown by Chi-
ang (2007), a packed chart representation of the par-
tition function for the binary Hiero grammars used
in this work would require O(n3JT J4(m−1)) space,1
which is far too large to be practical.
Instead we approximate the partition function
using a sum over a large subset of the possible
derivations (A(e, f)):
Algorithm 1 Top-down recursive derivation sam-
pling algorithm.
</bodyText>
<listItem confidence="0.966057888888889">
1: procedure SAMPLE(X, i, k)
2: rule — MULTI(inside chart(X, i, k))
3: c = �
4: for (child category, x, y) E CHILDREN(rule)
do
5: c — c U SAMPLE(child category, x, y)
6: end for
7: return DERIVATION(X, children)
8: end procedure
</listItem>
<bodyText confidence="0.99951325">
first called on a category and chart cell spanning the
entire chart, and then proceeds top down by using
the function MULTI to draw the next rule to expand
from the distribution defined by the inside scores.
</bodyText>
<equation confidence="0.994978">
ZA(f) � � � �exp AkHk(d, e, f)
e dE{CO(e,f)} k
= ZA(f)
</equation>
<bodyText confidence="0.999919333333333">
This model formulation raises the questions of
what an appropriate large subset of derivations for
training is, and how to efficiently calculate the sum
over all derivations in decoding. In the following
sections we elucidate and evaluate our solutions to
these problems.
</bodyText>
<subsectionHeader confidence="0.999758">
3.1 Sampling Derivations
</subsectionHeader>
<bodyText confidence="0.999865772727273">
The training and decoding algorithms presented in
the following sections rely upon Monte-Carlo tech-
niques, which in turn require the ability to draw
derivation samples from the probability distribution
defined by our log-linear model. Here we adapt
previously presented algorithms for sampling from
a PCFG (Goodman, 1998) for use with our syn-
chronous grammar model. Algorithm 1 describes the
algorithm for sampling derivations. The sampling
algorithm assumes the pre-existance of a packed
chart representation of all derivations for a given
source sentence. The inside algorithm is then used
to calculate the scores needed to define a multino-
mial distribution over all partial derivations associ-
ated with expanding a given child rule. These ini-
tial steps are performed once and then an unlim-
ited number of samples can be drawn by calling the
recursive SAMPLE procedure. MULTI draws a sample
from the distribution over rules for a given chart cell,
CHILDREN enumerates the chart cells connected to
a rule as variables, and DERIVATION is a recursive
tree data structure for derivations. The algorithm is
</bodyText>
<footnote confidence="0.9282865">
1where IT is the size of the terminal alphabet, i.e. the num-
ber of unique English words.
</footnote>
<subsectionHeader confidence="0.998477">
3.2 Approximate Inference
</subsectionHeader>
<bodyText confidence="0.999940129032258">
Approximating the partition function with �ZA(f)
could introduce biases into inference and in the fol-
lowing discussion we describe measures taken to
minimise the effects of the approximation bias.
An obvious approach to approximating the parti-
tion function, and the feature expectations required
for calculating the derivative in training, is to use
the packed chart of derivations produced by running
the cube pruning beam search algorithm of Chiang
(2007) on the source sentence. In this case ZA(f)
includes all the derivations that fall within the cube
pruning beam, hopefully representing the majority
of the probability mass. We denote the partition
function estimated with this cube beam approxima-
tion as �ZA (f). This approach has the advantage of
using the same beam search dynamic program dur-
ing training as is used for decoding. As the approxi-
mated partition function does not contain all deriva-
tions, it is possible that some, or all, of the deriva-
tions of the reference translation from the parallel
corpus may be excluded. We must therefore intersect
the packed chart built from the cube beam with that
of the reference derivations to ensure consistency.
Although, as would be done using cube-pruning,
it would seem intuitively sensible to approximate
the partition function using only high probability
derivations, it is possible that doing so will bias
our model in odd ways. The space of derivations
contained within the beam will be tightly clustered
about a maximum, and thus a model trained with
such an approximation will only see a very small
</bodyText>
<page confidence="0.996686">
217
</page>
<figureCaption confidence="0.99993">
Figure 2. A German-English translation example of building �Z��m
</figureCaption>
<bodyText confidence="0.97655675">
� (f) from samples. (a) Two sample derivations are
drawn from the model, (b) these samples are then combined into a packed representation, here represented by a
hypergraph with target translations elided for a bigram language model. The derivation in (c) is contained within the
hypergraph even though it was never explicitly inserted.
</bodyText>
<figure confidence="0.999498209876543">
1 2 3 4 5 6
(a)
(c)
1 2 3 4 5 6
1 2 3 4 5 6
Alles /
Everything
Alles /
Everything
Alles /
Everything
X
und /
and
X
und /
and
X
und /
and
X
X
X
jedes /
anything
jedes /
everyone
jedes /
anything
S
X
S
X
S
X
ist /
is
ist /
is
ist /
is
vorstellbar /
conceivable
vorstellbar /
possible
vorstellbar /
conceivable
X[1,2]
Everything
(b)
X[1,4]
Everything
and
X[2,3]
and
X[3,4]
anything
X[1,3]
Everything *
anything
X[3,4]
everyone
X[1,3]
Everything *
everyone
s
S[1,6]
Everything *
possible
X[4,5]
is
X[1,5]
Everything *
is
X[5,6]
possible
S[1,6]
Everything *
conceivable
X[5,6]
conceivable
</figure>
<bodyText confidence="0.999860703703704">
part of the overall distribution, possibly leading it
astray. Consider the example of a language model
feature: as this is a very strong indicator of transla-
tion quality, we would expect all derivations within
the beam to have a similar (high) language model
score, thereby robbing this feature of its discriminat-
ing power. However if our model could also see the
low probability derivations it would be clear that this
feature is indeed very strongly correlated with good
translations. Thus a good approximation of the space
of derivations is one that includes both good and bad
examples, not just a cluster around the maximum.
A principled solution to the problem of approx-
imating the partition function would be to use a
Markov Chain Monte Carlo (MCMC) sampler to
estimate the sum with a large number of samples.
Most of the sampled derivations would be in the
high probability region of the distribution, however
there would also be a number of samples drawn
from the rest of the space, giving the model a more
global view of the distribution, avoiding the pit-
falls of the narrow view obtained by a beam search.
Although effective, the computational cost of such
an approach is prohibitive as we would need to draw
hundreds of thousands of samples to obtain conver-
gence, for every training iteration.
Here we mediate between the computational
advantages of a beam and the broad view of the dis-
tribution provided by sampling. Using the algorithm
outlined in Section 3.1 we draw samples from the
distribution of derivations and then insert these sam-
ples into a packed chart representation. This process
is illustrated in Figure 2. The packed chart created
by intersecting the sample derivations represents a
space of derivations much greater than the original
samples. In Figure 2 the chart is built from the first
two sampled derivations, while the third derivation
can be extracted from the chart even though it was
never explicitly entered. This approximation of the
partition function (denoted �Z��&apos;
� (f)) allows us to
build an efficient packed chart representation of a
large number of derivations, centred on those with
high probability while still including a significant
representation of the low probability space. Deriva-
tions corresponding to the reference can be detected
during sampling and thus we can build the chart
for the reference derivations at the same time as
the one approximating the partition function. This
could lead to some, or none of, the possible ref-
erence derivations being included, as they may not
have been sampled. Although we could intersect all
of the reference derivations with the sampled chart,
this could distort the distribution over derivations,
</bodyText>
<page confidence="0.993112">
218
</page>
<bodyText confidence="0.99961605882353">
This results in the following log-likelihood objec-
tive and corresponding gradient:
and we believe it to be advantageous to keep the
distributions between the partition function and ref-
erence charts consistent.
Both of the approximations proposed above,
Z&apos; (f) and �Z���
� (f), rely on the pre-existence of a
trained translation model in order to either guide the
cube-pruning beam, or define the probability distri-
bution from which we draw samples. We solve this
chicken and egg problem by first training an exact
translation model without a language model, and
then use this model to create the partition function
approximations for training with a language model.
We denote the distribution without a language model
as p−LM
</bodyText>
<equation confidence="0.583156">
� (e|f) and that with as p+LM
� (e|f).
</equation>
<bodyText confidence="0.999942733333333">
A final training problem that we need to address
is the appropriate initialisation of the model param-
eters. In theory we could simply randomly initialise
A for p+LM
� (e|f), however in practice we found that
this resulted in poor performance on the develop-
ment data. This is due to the complex non-convex
optimisation function, and the fact that many fea-
tures will fall outside the approximated charts result-
ing in random, or zero, weights in testing. We intro-
duce a novel solution in which we use the Gaus-
sian prior over model weights to tie the exact model
trained without a language model, which assigns
sensible values to all rule features, with the approx-
imated model. The prior over model parameters for
</bodyText>
<equation confidence="0.995775333333333">
p+LM
� (e|f) is defined as:
p+LM(Ak) ∝ e−
</equation>
<bodyText confidence="0.9999891">
Here we have set the mean parameters of the Gaus-
sian distribution for the approximated model to
those learnt for the exact one. This has the effect
that any features that fall outside the approximated
model will simply retain the weight assigned by the
exact model. While for other feature weights the
prior will penalise substantial deviations away from
A−LM, essentially encoding the intuition that the
rule rule parameters should not change substantially
with the inclusion of language model features.
</bodyText>
<equation confidence="0.965192833333333">
= ��+LM
Λ (d|ei,fi)Lhk� − ��+LM
Λ (e|fi)Lhk�
A+LM − A−LM
k k
− Q2
</equation>
<subsectionHeader confidence="0.992958">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.977466853658537">
As stated in Equation 3 the probability of a given
translation string is calculated as the sum of the
probabilities of all the derivations that yield that
string. In decoding, where the reference translation
is not known, the exact calculation of this summa-
tion is NP-Hard. This problem also arises in mono-
lingual parsing with probabilistic tree substitution
grammars and has been tackled in the literature
using Monte-Carlo sampling methods (Chappelier
and Rajman, 2000). Their approach is directly appli-
cable to our SCFG decoding problem and we can use
Algorithm 1 to draw sample translation derivations
for the source sentence. The probability of a trans-
lation can be calculated simply from the number of
times a derivation that yields it was sampled, divided
by the total number of samples. For the p−LM
� (e|f)
model we can build the full chart of all possible
derivations and thus sample from the true distribu-
tion over derivations. For the p+LM
� (e|f) model we
suffer the same problem as in training and cannot
build the full chart. Instead a chart is built using
the cube-pruning algorithm with a wide beam and
we then draw samples from this chart. Although
sampling from a reduced chart will result in biased
samples, in Section 4 we show this approach to be
effective in practice.2 In Section 4 we compare our
sampling approach to the heuristic beam search pro-
posed by Blunsom et al. (2008).
It is of interest to compare our proposed decoding
algorithms to minimum Bayes risk (MBR) decoding
(Kumar and Byrne, 2004), a commonly used decod-
ing method. From a theoretical standpoint, the sum-
ming of derivations for a given translation is exactly
2We have experimented with using a Metropolis Hastings
sampler, with p−LM
Λ (elf) as the proposal distribution, to sam-
ple from the true distribution with the language model. Unfor-
tunately the sample rejection rate was very high such that this
method proved infeasibly slow.
</bodyText>
<figure confidence="0.90402">
� � log p+LM
L = log p+LM 0 (Ak)
(ei,fi)∈D � (ei|fi) +
k
k4 −&apos;\k LM k2
2σ2
aL
aAk
</figure>
<page confidence="0.998284">
219
</page>
<bodyText confidence="0.999914090909091">
equivalent to performing MBR with a 0/1 loss func-
tion over derivations. From a practical perspective,
MBR is normally performed with BLEU as the loss
and approximated using n-best lists. These n-best
lists are produced using algorithms tuned to remove
multiple derivations of the same translation (which
have previously been seen as undesirable). However,
it would be simple to extend our sampling based
decoding algorithm to calculate the MBR estimate
using BLEU, in theory providing a lower variance
estimate than attained with n-best lists.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999968766666667">
We evaluate our model on the IWSLT 2005 Chinese
to English translation task (Eck and Hori, 2005),
using the 2004 test set as development data for
tuning the hyperparameters and MERT training the
benchmark systems. The statistics for this data are
presented in Table 1.3 The training data made avail-
able for this task consisted of 40k pairs of tran-
scribed utterances, drawn from the travel domain.
The development and test data for this task are some-
what unusual in that each sentence has a single
human translated reference, and fifteen paraphrases
of this reference, provided by monolingual anno-
tators. Model performance is evaluated using the
standard BLEU metric (Papineni et al., 2002) which
measures average n-gram precision, n &lt; 4, and we
use the NIST definition of the brevity penalty for
multiple reference test sets. We provide evaluation
against both the entire multi-reference sets, and the
single human translation.
Our translation grammar is induced using the
standard alignment and rule extraction heuristics
used in hierarchical translation models (Chiang,
2007).4 As these heuristics aren’t based on a genera-
tive model, and don’t guarantee that the target trans-
lation will be reachable from the source, we discard
those sentence pairs for which we cannot produce a
derivation, leaving 38,405 sentences for training.
Our base model contains a single feature for each
rule which counts the number of times it appeared in
a particular derivation. For models which include a
</bodyText>
<footnote confidence="0.93793675">
3Development and test set statistics are for the single human
translation reference.
4With the exception that we allow unaligned words at the
boundary of rules. This improves training set coverage.
</footnote>
<bodyText confidence="0.999820384615385">
language model, we train a standard Kneser-Ney tri-
gram model on the target side of the training corpus.
We also include a word penalty feature to compen-
sate for the shortening effect of the language model.
In total our model contains 2.9M features.
The aims of our evaluation are: (1) to deter-
mine that our proposed training regimes are able to
realise performance increase when training sparse
rule features and a real valued language model fea-
ture together, (2) that the model is able to effectively
use rich features over the source sentence, and (3)
to confirm that our model obtains performance com-
petitive with the current state-of-the-art.
</bodyText>
<subsectionHeader confidence="0.924965">
4.1 Inference and Decoding
</subsectionHeader>
<bodyText confidence="0.983352060606061">
We have described a number of modelling choices
which aim to compensate for the training biases
introduced by incorporating a language model fea-
ture through approximate inference. Our a priori
knowledge from other SMT systems suggests that
incorporating a language model should lead to large
increases in BLEU score. In this evaluation we aim
to determine whether our training regimes are able
to realises these expected gains.
Table 2 shows a matrix of development BLEU
scores achieved by varying the approximation of the
partition function in training, and varying the decod-
ing algorithm. If we consider the vertical axis we
can see that the sampling method for approximat-
ing the partition function has a small but consistent
advantage over using the cube-pruning beam. The
charts produced by the sampling approach occupy
roughly half the disc space as those produced by
the beam search, so in subsequent experiments we
present results using the �Z���
� (f) approximation.
Comparing the decoding algorithms on the hori-
zontal axis we can reconfirm the findings of Blun-
som et al. (2008) that the max-translation decod-
ing outperforms the Viterbi max-derivation approx-
imation. It is also of note that this BLEU increase
is robust to the introduction of the language model
feature, assuaging fears that the max-translation
approach may have been doing the job of the lan-
guage model. We also compare using Monte-Carlo
sampling for decoding with the previously pro-
posed heuristic beam search algorithm. The differ-
ence between the two algorithms is small, however
</bodyText>
<page confidence="0.989738">
220
</page>
<table confidence="0.999845">
Training Development Test
Chinese English Chinese English Chinese English
Utterances 38405 500 506
Segments/Words 317621 353116 3464 3752 3784 3823
Av. Utterances Length 8 9 6 7 7 7
Longest Utterance 55 68 58 62 61 56
</table>
<tableCaption confidence="0.995326">
Table 1. IWSLT Chinese to English translation corpus statistics.
</tableCaption>
<table confidence="0.999775">
Model Max-derivation Max-translation(Beam) Max-translation(Sampling)
P�LM 31.0 32.5 32.6
� (e|f)
p�LM 39.1 39.8 39.8
� (eIf) ( 2� (f)) 39.9 40.5 40.6
p�LM
(elf) ( �Z��—
� � (f))
</table>
<tableCaption confidence="0.655098">
Table 2. Development set results for varying the approximation of the partition function in training, 2� � (f) vs. �Z��—
� (f),
</tableCaption>
<bodyText confidence="0.969806454545455">
and decoding using the Viterbi max-derivation algorithm, or the max-translation algorithm with either a beam approxi-
mation or Monte-Carlo sampling.
we feeling the sampling approach is more theoreti-
cally justified and adopt it for our later experiments.
The most important result from this evaluation
is that both our training regimes realise substantial
gains from the introduction of the language model
feature. Thus we can be confident that our model
is capable of modelling the distribution over trans-
lations even when the space over all derivations is
intractable to dynamically program exactly.
</bodyText>
<subsectionHeader confidence="0.9770785">
4.2 A Discriminative Syntactic Translation
Model
</subsectionHeader>
<bodyText confidence="0.999381421052632">
In the previous sections we’ve described and evalu-
ated a statistical model of translation that is able to
estimate a probability distribution over translations
using millions of sparse features. A prime motiva-
tion for such a model is the ability to define com-
plex features over more than just the surface forms
of the source and target strings. There are limit-
less options for such features, and previous work
has focused on defining token based features such
as part-of-speech and morphology (Ittycheriah and
Roukos, 2007). Although such features are applica-
ble to our model, here we attempt to test the model’s
ability to incorporate complex features over source-
side syntax trees, essentially subsuming and extend-
ing previous work on tree-to-string translation mod-
els (Huang et al., 2006; Mi et al., 2008).
We first parse the source side of our training,
development and test corpora using the Stanford
parser.5 Next, while building the synchronous charts
</bodyText>
<footnote confidence="0.72498">
5http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<bodyText confidence="0.999596772727273">
required for training, whenever a rule is used in a
derivation a feature is activated which captures: (1)
the constituent spanning the rule’s source side in the
syntax tree (if any) (2) constituents spanning any
variables in the rule, and (3) the rule’s target side
surface form. Figure 3 illustrates this process.
These syntactic features are equivalent to the
grammar rules extracted for tree-to-string translation
systems. The key difference in our model is that the
source syntax tree is treated as conditioning context
and it’s information encoded as features, thus this
information can be used or ignored as the model sees
fit. This avoids the problems associated with explic-
itly encoding the source syntax in the grammar, such
as sparsity and overly constraining the model. In
addition we could easily incorporate features over
multiple source trees, for example mixing labelled
syntax trees with dependency graphs.
We limit the extraction of syntactic features to
those that appear in at least two training derivations,
giving a total of 4.2M syntactic features, for an over-
all total of 7.1M features.
</bodyText>
<subsectionHeader confidence="0.944392">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999957285714286">
Table 3 shows the results from applying our
described models to the test set. We benchmark our
results against a model (Hiero) which was directly
trained to optimise BLEU&amp;quot;ST using the standard
MERT algorithm (Och, 2003) and the full set of
translation and lexical weight features described
for the Hiero model (Chiang, 2007). As well as
</bodyText>
<page confidence="0.992608">
221
</page>
<table confidence="0.99994475">
Model BLEUNIST BLEUIBM BLEUHumanRef
p�LM 33.5 35.2 25.2
Λ (e|f) 44.6 44.6 31.2
p�LM 45.3 45.2 31.8
Λ (e|f)
p�LM
Λ (e|f) + syntax
MERT (BLEUNIST) 46.2 44.5 30.2
</table>
<tableCaption confidence="0.999161">
Table 3. Test set results.
</tableCaption>
<figure confidence="0.999394555555555">
(a) (b)
SQ
VP
NP
(c)
N N V WH ?
_PhF5 AWE ?t �里 ?
Where is the currency exchange ofÞce ?
Example Derivation:
(Step 1) X1 -&gt; &lt;_PhF5 AWE, currency exchange office&gt;
(Step 2) X2 -&gt; &lt; [X1] ?t �里 ?, Where is the [X1] ?&gt;
Example Syntax feature =
for Step 2
SQ
Where is the [X1] ?
?t �里 ?
NP
X1
</figure>
<figureCaption confidence="0.980948333333333">
Figure 3. Syntax feature example: For the parsed source and candidate translation (a), with the derivation (b), we
extract the syntax feature in (c) by combining the grammar rule with the source syntax of the constituents contained
within that rule.
</figureCaption>
<table confidence="0.998623888888889">
Source RA ±_ &amp;--h Z% W17 N A K - 4K NT_ W17 _WL _595o
p�LM don ’t have enough bag on me change please go purchase a new by plane .
Λ (e|f) i have enough money to buy a new one by air.
p�LM i don ’t have enough money to buy a new airline ticket.
Λ (e|f)
p�LM
Λ (e|f) + syntax
MERT (BLEUNIST) i don ’t have enough money to buy a new ticket.
Reference i do n’t have enough money with me to buy a new airplane ticket.
</table>
<tableCaption confidence="0.998179">
Table 4. Example test set output produced when: not using a language model, using a language model, also using
syntax, output optimised using MERT and finally the reference
</tableCaption>
<bodyText confidence="0.999762733333333">
BLEUNIST (brevity penalty uses the shortest ref-
erence), we also include results from the IBM
(BLEUIBM) metric (brevity penalty uses the closest
reference), and using only the actual human transla-
tion in the test set, not the monolingual paraphrase
multiple references (BLEUHumanRef).
The first result of interest is that we see an
increase in performance through the incorporation
of the source syntax features. This is an encourag-
ing result as the transcribed speech source sentences
are well out of the domain of the data on which the
parser was trained, suggesting that our model is able
to sift the good information from the noisy in the
unreliable source syntax trees. Table 4 shows illus-
trative system output on the test set.
On the BLEUNIST metric we see that our mod-
els under-perform the MERT trained system. We
hypothesise that this is predominately due to the
interaction of the brevity penalty with the unusual
nature of the multiple paraphrase reference train-
ing and development data. Their performance is
however quite consistent across the different inter-
pretations of the brevity penalty (NIST vs. IBM).
This contrasts with the MERT trained model, which
clearly over-fits to the NIST metric that it was
trained on and underperforms our models when eval-
uated on the single human test translations. If we
directly compare the brevity penalties of the MERT
model (0.868) and our discriminative model incor-
porating source syntax (0.942), on the these single
</bodyText>
<page confidence="0.98868">
222
</page>
<bodyText confidence="0.999904777777778">
references, we can see that the MERT training has
optimised to the shortest paraphrase reference.
From these results it is difficult to draw any hard
conclusions on the relative performance of the dif-
ferent training regimes. However we feel confident
in claiming that we have achieved our goal of train-
ing a probabilistic model on millions of sparse fea-
tures which obtains performance competitive with
the current state-of-the-art training algorithm.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999080363636364">
In this paper we have shown that statistical machine
translation can be effectively modelled as a well
posed machine learning task. In doing so we have
described a model capable of estimating a probabil-
ity distribution over translations using sparse com-
plex features, and achieving performance compara-
ble to the state-of-the-art on standard metrics. With
further work on scaling these models to large data
sets, and engineering high performance features, we
believe this research has the potential to provide sig-
nificant increases in translation quality.
</bodyText>
<sectionHeader confidence="0.996537" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999107">
The authors acknowledge the support of the EPSRC
grant EP/D074959/1.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879837837838">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of the 46th Annual Con-
ference of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08:HLT),
pages 200–208, Columbus, Ohio, June.
Jean-C´edric Chappelier and Martin Rajman. 2000.
Monte-carlo sampling for np-hard maximization prob-
lems in the framework of weighted parsing. In NLP
’00: Proceedings of the Second International Confer-
ence on Natural Language Processing, pages 106–
117, London, UK.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proc. of the
International Workshop on Spoken Language Trans-
lation, Pittsburgh, October.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguis-
tics (COLING/ACL-2006), pages 961–968, Sydney,
Australia, July.
Joshua T. Goodman. 1998. Parsing inside-out. Ph.D.
thesis, Cambridge, MA, USA. Adviser-Stuart Shieber.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In In Proceedings of the 7th Bien-
nial Conference of the Association for Machine Trans-
lation in the Americas (AMTA), Boston, MA.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proc. of the 7th International
Conference on Human Language Technology Research
and 8th Annual Meeting of the NAACL (HLT-NAACL
2007), pages 57–64, Rochester, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. of the 4th International Conference on Human
Language Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 169–
176.
Philip M. Lewis II and Richard E. Stearns. 1968. Syntax-
directed transduction. J. ACM, 15(3):465–488.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of the
44th Annual Meeting of the ACL and 21st Inter-
national Conference on Computational Linguistics
(COLING/ACL-2006), pages 761–768, Sydney, Aus-
tralia, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of the 46th Annual Confer-
ence of the Association for Computational Linguistics:
Human Language Technologies (ACL-08:HLT), pages
192–199, Columbus, Ohio, June.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160–
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the ACL and 3rd Annual Meeting of
the NAACL (ACL-2002), pages 311–318, Philadelphia,
Pennsylvania.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of the 44th Annual
Meeting of the ACL and 21st International Conference
on Computational Linguistics (COLING/ACL-2006),
pages 777–784, Sydney, Australia.
</reference>
<page confidence="0.999167">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.557619">
<title confidence="0.99857">Probabilistic Inference for Machine Translation</title>
<author confidence="0.764614">Blunsom</author>
<affiliation confidence="0.908835">School of Informatics, University of</affiliation>
<address confidence="0.691295">10 Crichton Street, Edinburgh, EH8 9AB,</address>
<abstract confidence="0.998953933333333">We advance the state-of-the-art for discriminatively trained machine translation systems by presenting novel probabilistic inference and search methods for synchronous grammars. By approximating the intractable space of all candidate translations produced by intersecting an ngram language model with a synchronous grammar, we are able to train and decode models incorporating millions of sparse, heterogeneous features. Further, we demonstrate the power of the discriminative training paradigm by extracting structured syntactic features, and achieving increases in translation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT),</booktitle>
<pages>200--208</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1388" citStr="Blunsom et al. (2008)" startWordPosition="186" endWordPosition="189">ases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be crucial for obtaining good performance in SMT, was notably omitted. This was because adding a language model would mean that the normalising partition function could no longer be exactly calculated, thereby preventing efficient parameter estimation. Here, we show how language models can be incor</context>
<context position="18433" citStr="Blunsom et al. (2008)" startWordPosition="3040" endWordPosition="3043">er of samples. For the p−LM � (e|f) model we can build the full chart of all possible derivations and thus sample from the true distribution over derivations. For the p+LM � (e|f) model we suffer the same problem as in training and cannot build the full chart. Instead a chart is built using the cube-pruning algorithm with a wide beam and we then draw samples from this chart. Although sampling from a reduced chart will result in biased samples, in Section 4 we show this approach to be effective in practice.2 In Section 4 we compare our sampling approach to the heuristic beam search proposed by Blunsom et al. (2008). It is of interest to compare our proposed decoding algorithms to minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004), a commonly used decoding method. From a theoretical standpoint, the summing of derivations for a given translation is exactly 2We have experimented with using a Metropolis Hastings sampler, with p−LM Λ (elf) as the proposal distribution, to sample from the true distribution with the language model. Unfortunately the sample rejection rate was very high such that this method proved infeasibly slow. � � log p+LM L = log p+LM 0 (Ak) (ei,fi)∈D � (ei|fi) + k k4 −&apos;\k LM k2 2σ2</context>
<context position="23047" citStr="Blunsom et al. (2008)" startWordPosition="3789" endWordPosition="3793"> BLEU scores achieved by varying the approximation of the partition function in training, and varying the decoding algorithm. If we consider the vertical axis we can see that the sampling method for approximating the partition function has a small but consistent advantage over using the cube-pruning beam. The charts produced by the sampling approach occupy roughly half the disc space as those produced by the beam search, so in subsequent experiments we present results using the �Z��� � (f) approximation. Comparing the decoding algorithms on the horizontal axis we can reconfirm the findings of Blunsom et al. (2008) that the max-translation decoding outperforms the Viterbi max-derivation approximation. It is also of note that this BLEU increase is robust to the introduction of the language model feature, assuaging fears that the max-translation approach may have been doing the job of the language model. We also compare using Monte-Carlo sampling for decoding with the previously proposed heuristic beam search algorithm. The difference between the two algorithms is small, however 220 Training Development Test Chinese English Chinese English Chinese English Utterances 38405 500 506 Segments/Words 317621 353</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), pages 200–208, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>Monte-carlo sampling for np-hard maximization problems in the framework of weighted parsing.</title>
<date>2000</date>
<booktitle>In NLP ’00: Proceedings of the Second International Conference on Natural Language Processing,</booktitle>
<pages>106--117</pages>
<location>London, UK.</location>
<contexts>
<context position="17503" citStr="Chappelier and Rajman, 2000" startWordPosition="2875" endWordPosition="2878">hould not change substantially with the inclusion of language model features. = ��+LM Λ (d|ei,fi)Lhk� − ��+LM Λ (e|fi)Lhk� A+LM − A−LM k k − Q2 3.3 Decoding As stated in Equation 3 the probability of a given translation string is calculated as the sum of the probabilities of all the derivations that yield that string. In decoding, where the reference translation is not known, the exact calculation of this summation is NP-Hard. This problem also arises in monolingual parsing with probabilistic tree substitution grammars and has been tackled in the literature using Monte-Carlo sampling methods (Chappelier and Rajman, 2000). Their approach is directly applicable to our SCFG decoding problem and we can use Algorithm 1 to draw sample translation derivations for the source sentence. The probability of a translation can be calculated simply from the number of times a derivation that yields it was sampled, divided by the total number of samples. For the p−LM � (e|f) model we can build the full chart of all possible derivations and thus sample from the true distribution over derivations. For the p+LM � (e|f) model we suffer the same problem as in training and cannot build the full chart. Instead a chart is built using</context>
</contexts>
<marker>Chappelier, Rajman, 2000</marker>
<rawString>Jean-C´edric Chappelier and Martin Rajman. 2000. Monte-carlo sampling for np-hard maximization problems in the framework of weighted parsing. In NLP ’00: Proceedings of the Second International Conference on Natural Language Processing, pages 106– 117, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4646" citStr="Chiang, 2007" startWordPosition="726" endWordPosition="727"> context free grammar (SCFG, (Lewis II and Stearns, 1968)) describes the generation of pairs of strings. A string pair is generated by applying a series of paired context-free rewrite rules of the form, X —* (a, -y, —), where X is a nonterminal, a and -y are strings of terminals and nonterminals and — specifies a one-to-one alignment between non-terminals in a and -y. In the context of SMT, by assigning the source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence, while generating the target translation (Chiang, 2007). In this paper we only consider grammars extracted using the heuristics described for the Hiero SMT system (Chiang, 2007). Note however that our approach is general and could be used with other synchronous grammar transducers (e.g., (Galley et al., 2006)). SCFG productions can specify that the order of the child non-terminals is the same in both languages (a monotone production), or is reversed (a reordering production). Without loss of generality, here we add the restriction that non-terminals on the source and target sides of the grammar must have the same category. Figure 1 shows an exampl</context>
<context position="7313" citStr="Chiang (2007)" startWordPosition="1184" endWordPosition="1186"> include overlapping and non-independent features of the data. The features must decompose with the derivation and the ngram context defined by the function q, as shown in Equation (2). The features can reference the entire source sentence coupled with each rule, r, and its target context, in a derivation. By directly incorporating the language model context q into the model formulation, we will not 216 be able to exactly compute the partition function ZA(f), which sums over all possible derivations. Even though a dynamic program over this space would still run in polynomial time, as shown by Chiang (2007), a packed chart representation of the partition function for the binary Hiero grammars used in this work would require O(n3JT J4(m−1)) space,1 which is far too large to be practical. Instead we approximate the partition function using a sum over a large subset of the possible derivations (A(e, f)): Algorithm 1 Top-down recursive derivation sampling algorithm. 1: procedure SAMPLE(X, i, k) 2: rule — MULTI(inside chart(X, i, k)) 3: c = � 4: for (child category, x, y) E CHILDREN(rule) do 5: c — c U SAMPLE(child category, x, y) 6: end for 7: return DERIVATION(X, children) 8: end procedure first ca</context>
<context position="10139" citStr="Chiang (2007)" startWordPosition="1644" endWordPosition="1645">ucture for derivations. The algorithm is 1where IT is the size of the terminal alphabet, i.e. the number of unique English words. 3.2 Approximate Inference Approximating the partition function with �ZA(f) could introduce biases into inference and in the following discussion we describe measures taken to minimise the effects of the approximation bias. An obvious approach to approximating the partition function, and the feature expectations required for calculating the derivative in training, is to use the packed chart of derivations produced by running the cube pruning beam search algorithm of Chiang (2007) on the source sentence. In this case ZA(f) includes all the derivations that fall within the cube pruning beam, hopefully representing the majority of the probability mass. We denote the partition function estimated with this cube beam approximation as �ZA (f). This approach has the advantage of using the same beam search dynamic program during training as is used for decoding. As the approximated partition function does not contain all derivations, it is possible that some, or all, of the derivations of the reference translation from the parallel corpus may be excluded. We must therefore int</context>
<context position="20679" citStr="Chiang, 2007" startWordPosition="3406" endWordPosition="3407"> that each sentence has a single human translated reference, and fifteen paraphrases of this reference, provided by monolingual annotators. Model performance is evaluated using the standard BLEU metric (Papineni et al., 2002) which measures average n-gram precision, n &lt; 4, and we use the NIST definition of the brevity penalty for multiple reference test sets. We provide evaluation against both the entire multi-reference sets, and the single human translation. Our translation grammar is induced using the standard alignment and rule extraction heuristics used in hierarchical translation models (Chiang, 2007).4 As these heuristics aren’t based on a generative model, and don’t guarantee that the target translation will be reachable from the source, we discard those sentence pairs for which we cannot produce a derivation, leaving 38,405 sentences for training. Our base model contains a single feature for each rule which counts the number of times it appeared in a particular derivation. For models which include a 3Development and test set statistics are for the single human translation reference. 4With the exception that we allow unaligned words at the boundary of rules. This improves training set co</context>
<context position="27219" citStr="Chiang, 2007" startWordPosition="4448" endWordPosition="4449">ultiple source trees, for example mixing labelled syntax trees with dependency graphs. We limit the extraction of syntactic features to those that appear in at least two training derivations, giving a total of 4.2M syntactic features, for an overall total of 7.1M features. 4.3 Discussion Table 3 shows the results from applying our described models to the test set. We benchmark our results against a model (Hiero) which was directly trained to optimise BLEU&amp;quot;ST using the standard MERT algorithm (Och, 2003) and the full set of translation and lexical weight features described for the Hiero model (Chiang, 2007). As well as 221 Model BLEUNIST BLEUIBM BLEUHumanRef p�LM 33.5 35.2 25.2 Λ (e|f) 44.6 44.6 31.2 p�LM 45.3 45.2 31.8 Λ (e|f) p�LM Λ (e|f) + syntax MERT (BLEUNIST) 46.2 44.5 30.2 Table 3. Test set results. (a) (b) SQ VP NP (c) N N V WH ? _PhF5 AWE ?t �里 ? Where is the currency exchange ofÞce ? Example Derivation: (Step 1) X1 -&gt; &lt;_PhF5 AWE, currency exchange office&gt; (Step 2) X2 -&gt; &lt; [X1] ?t �里 ?, Where is the [X1] ?&gt; Example Syntax feature = for Step 2 SQ Where is the [X1] ? ?t �里 ? NP X1 Figure 3. Syntax feature example: For the parsed source and candidate translation (a), with the derivation (b</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Chiori Hori</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2005</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<location>Pittsburgh,</location>
<contexts>
<context position="19698" citStr="Eck and Hori, 2005" startWordPosition="3251" endWordPosition="3254">a 0/1 loss function over derivations. From a practical perspective, MBR is normally performed with BLEU as the loss and approximated using n-best lists. These n-best lists are produced using algorithms tuned to remove multiple derivations of the same translation (which have previously been seen as undesirable). However, it would be simple to extend our sampling based decoding algorithm to calculate the MBR estimate using BLEU, in theory providing a lower variance estimate than attained with n-best lists. 4 Evaluation We evaluate our model on the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), using the 2004 test set as development data for tuning the hyperparameters and MERT training the benchmark systems. The statistics for this data are presented in Table 1.3 The training data made available for this task consisted of 40k pairs of transcribed utterances, drawn from the travel domain. The development and test data for this task are somewhat unusual in that each sentence has a single human translated reference, and fifteen paraphrases of this reference, provided by monolingual annotators. Model performance is evaluated using the standard BLEU metric (Papineni et al., 2002) which </context>
</contexts>
<marker>Eck, Hori, 2005</marker>
<rawString>Matthias Eck and Chiori Hori. 2005. Overview of the IWSLT 2005 evaluation campaign. In Proc. of the International Workshop on Spoken Language Translation, Pittsburgh, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006),</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4901" citStr="Galley et al., 2006" startWordPosition="764" endWordPosition="767">and -y are strings of terminals and nonterminals and — specifies a one-to-one alignment between non-terminals in a and -y. In the context of SMT, by assigning the source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence, while generating the target translation (Chiang, 2007). In this paper we only consider grammars extracted using the heuristics described for the Hiero SMT system (Chiang, 2007). Note however that our approach is general and could be used with other synchronous grammar transducers (e.g., (Galley et al., 2006)). SCFG productions can specify that the order of the child non-terminals is the same in both languages (a monotone production), or is reversed (a reordering production). Without loss of generality, here we add the restriction that non-terminals on the source and target sides of the grammar must have the same category. Figure 1 shows an example derivation for Chinese to English translation. 3 Model We start by defining a log-linear model for the conditional probability distribution over target translations of a given source sentence. A sequence of SCFG rule applications which produce a transla</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006), pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>Parsing inside-out.</title>
<date>1998</date>
<booktitle>Ph.D. thesis,</booktitle>
<publisher>Adviser-Stuart Shieber.</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="8786" citStr="Goodman, 1998" startWordPosition="1429" endWordPosition="1430"> model formulation raises the questions of what an appropriate large subset of derivations for training is, and how to efficiently calculate the sum over all derivations in decoding. In the following sections we elucidate and evaluate our solutions to these problems. 3.1 Sampling Derivations The training and decoding algorithms presented in the following sections rely upon Monte-Carlo techniques, which in turn require the ability to draw derivation samples from the probability distribution defined by our log-linear model. Here we adapt previously presented algorithms for sampling from a PCFG (Goodman, 1998) for use with our synchronous grammar model. Algorithm 1 describes the algorithm for sampling derivations. The sampling algorithm assumes the pre-existance of a packed chart representation of all derivations for a given source sentence. The inside algorithm is then used to calculate the scores needed to define a multinomial distribution over all partial derivations associated with expanding a given child rule. These initial steps are performed once and then an unlimited number of samples can be drawn by calling the recursive SAMPLE procedure. MULTI draws a sample from the distribution over rul</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>Joshua T. Goodman. 1998. Parsing inside-out. Ph.D. thesis, Cambridge, MA, USA. Adviser-Stuart Shieber.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2837" citStr="Huang et al., 2006" startWordPosition="407" endWordPosition="410">g us to tackle the extra computational burden associated with adding the language model. This approach is theoretically justified and means that the model continues to be both probabilistic and globally optimised. As expected, using a language model dramatically increases translation performance. Our second major contribution is an exploitation of syntactic features. By encoding source syntax as features allows the model to use, or ignore, this information as it sees fit, thereby avoiding the problems of coverage and sparsity associated with directly incorporating the syntax into the grammar (Huang et al., 2006; Mi et al., 2008). We report on translation gains using this approach. We begin by introducing the synchronous grammar approach to SMT in Section 2. In Section 3 we define the parametric form of our model and describe techniques for approximating the intractable space of all translations for a given source sentence. In Section 4 we evaluate the ability of our model to effectively estimate the highly dependent weights for the sparse features and realvalued language model. In addition we describe how 215 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pag</context>
<context position="25550" citStr="Huang et al., 2006" startWordPosition="4186" endWordPosition="4189">ons of sparse features. A prime motivation for such a model is the ability to define complex features over more than just the surface forms of the source and target strings. There are limitless options for such features, and previous work has focused on defining token based features such as part-of-speech and morphology (Ittycheriah and Roukos, 2007). Although such features are applicable to our model, here we attempt to test the model’s ability to incorporate complex features over sourceside syntax trees, essentially subsuming and extending previous work on tree-to-string translation models (Huang et al., 2006; Mi et al., 2008). We first parse the source side of our training, development and test corpora using the Stanford parser.5 Next, while building the synchronous charts 5http://nlp.stanford.edu/software/lex-parser.shtml required for training, whenever a rule is used in a derivation a feature is activated which captures: (1) the constituent spanning the rule’s source side in the syntax tree (if any) (2) constituents spanning any variables in the rule, and (3) the rule’s target side surface form. Figure 3 illustrates this process. These syntactic features are equivalent to the grammar rules extr</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Direct translation model 2.</title>
<date>2007</date>
<booktitle>In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL (HLT-NAACL</booktitle>
<pages>57--64</pages>
<location>Rochester, USA.</location>
<contexts>
<context position="1350" citStr="Ittycheriah and Roukos, 2007" startWordPosition="179" endWordPosition="182">ctured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be crucial for obtaining good performance in SMT, was notably omitted. This was because adding a language model would mean that the normalising partition function could no longer be exactly calculated, thereby preventing efficient parameter estimation. Here, we</context>
<context position="25284" citStr="Ittycheriah and Roukos, 2007" startWordPosition="4144" endWordPosition="4147">rivations is intractable to dynamically program exactly. 4.2 A Discriminative Syntactic Translation Model In the previous sections we’ve described and evaluated a statistical model of translation that is able to estimate a probability distribution over translations using millions of sparse features. A prime motivation for such a model is the ability to define complex features over more than just the surface forms of the source and target strings. There are limitless options for such features, and previous work has focused on defining token based features such as part-of-speech and morphology (Ittycheriah and Roukos, 2007). Although such features are applicable to our model, here we attempt to test the model’s ability to incorporate complex features over sourceside syntax trees, essentially subsuming and extending previous work on tree-to-string translation models (Huang et al., 2006; Mi et al., 2008). We first parse the source side of our training, development and test corpora using the Stanford parser.5 Next, while building the synchronous charts 5http://nlp.stanford.edu/software/lex-parser.shtml required for training, whenever a rule is used in a derivation a feature is activated which captures: (1) the cons</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2007. Direct translation model 2. In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL (HLT-NAACL 2007), pages 57–64, Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of the 4th International Conference on Human Language Technology Research and 5th Annual Meeting of the NAACL (HLT-NAACL 2004),</booktitle>
<pages>169--176</pages>
<contexts>
<context position="18557" citStr="Kumar and Byrne, 2004" startWordPosition="3060" endWordPosition="3063">true distribution over derivations. For the p+LM � (e|f) model we suffer the same problem as in training and cannot build the full chart. Instead a chart is built using the cube-pruning algorithm with a wide beam and we then draw samples from this chart. Although sampling from a reduced chart will result in biased samples, in Section 4 we show this approach to be effective in practice.2 In Section 4 we compare our sampling approach to the heuristic beam search proposed by Blunsom et al. (2008). It is of interest to compare our proposed decoding algorithms to minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004), a commonly used decoding method. From a theoretical standpoint, the summing of derivations for a given translation is exactly 2We have experimented with using a Metropolis Hastings sampler, with p−LM Λ (elf) as the proposal distribution, to sample from the true distribution with the language model. Unfortunately the sample rejection rate was very high such that this method proved infeasibly slow. � � log p+LM L = log p+LM 0 (Ak) (ei,fi)∈D � (ei|fi) + k k4 −&apos;\k LM k2 2σ2 aL aAk 219 equivalent to performing MBR with a 0/1 loss function over derivations. From a practical perspective, MBR is nor</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proc. of the 4th International Conference on Human Language Technology Research and 5th Annual Meeting of the NAACL (HLT-NAACL 2004), pages 169– 176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M Lewis</author>
<author>Richard E Stearns</author>
</authors>
<title>Syntaxdirected transduction.</title>
<date>1968</date>
<journal>J. ACM,</journal>
<volume>15</volume>
<issue>3</issue>
<marker>Lewis, Stearns, 1968</marker>
<rawString>Philip M. Lewis II and Richard E. Stearns. 1968. Syntaxdirected transduction. J. ACM, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006),</booktitle>
<pages>761--768</pages>
<location>Sydney, Australia,</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006), pages 761–768, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT),</booktitle>
<pages>192--199</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2855" citStr="Mi et al., 2008" startWordPosition="411" endWordPosition="414">xtra computational burden associated with adding the language model. This approach is theoretically justified and means that the model continues to be both probabilistic and globally optimised. As expected, using a language model dramatically increases translation performance. Our second major contribution is an exploitation of syntactic features. By encoding source syntax as features allows the model to use, or ignore, this information as it sees fit, thereby avoiding the problems of coverage and sparsity associated with directly incorporating the syntax into the grammar (Huang et al., 2006; Mi et al., 2008). We report on translation gains using this approach. We begin by introducing the synchronous grammar approach to SMT in Section 2. In Section 3 we define the parametric form of our model and describe techniques for approximating the intractable space of all translations for a given source sentence. In Section 4 we evaluate the ability of our model to effectively estimate the highly dependent weights for the sparse features and realvalued language model. In addition we describe how 215 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215–223, Honolu</context>
<context position="25568" citStr="Mi et al., 2008" startWordPosition="4190" endWordPosition="4193">es. A prime motivation for such a model is the ability to define complex features over more than just the surface forms of the source and target strings. There are limitless options for such features, and previous work has focused on defining token based features such as part-of-speech and morphology (Ittycheriah and Roukos, 2007). Although such features are applicable to our model, here we attempt to test the model’s ability to incorporate complex features over sourceside syntax trees, essentially subsuming and extending previous work on tree-to-string translation models (Huang et al., 2006; Mi et al., 2008). We first parse the source side of our training, development and test corpora using the Stanford parser.5 Next, while building the synchronous charts 5http://nlp.stanford.edu/software/lex-parser.shtml required for training, whenever a rule is used in a derivation a feature is activated which captures: (1) the constituent spanning the rule’s source side in the syntax tree (if any) (2) constituents spanning any variables in the rule, and (3) the rule’s target side surface form. Figure 3 illustrates this process. These syntactic features are equivalent to the grammar rules extracted for tree-to-</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), pages 192–199, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the ACL (ACL-2003),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1319" citStr="Och, 2003" startWordPosition="177" endWordPosition="178">acting structured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be crucial for obtaining good performance in SMT, was notably omitted. This was because adding a language model would mean that the normalising partition function could no longer be exactly calculated, thereby preventing efficient</context>
<context position="27114" citStr="Och, 2003" startWordPosition="4431" endWordPosition="4432">as sparsity and overly constraining the model. In addition we could easily incorporate features over multiple source trees, for example mixing labelled syntax trees with dependency graphs. We limit the extraction of syntactic features to those that appear in at least two training derivations, giving a total of 4.2M syntactic features, for an overall total of 7.1M features. 4.3 Discussion Table 3 shows the results from applying our described models to the test set. We benchmark our results against a model (Hiero) which was directly trained to optimise BLEU&amp;quot;ST using the standard MERT algorithm (Och, 2003) and the full set of translation and lexical weight features described for the Hiero model (Chiang, 2007). As well as 221 Model BLEUNIST BLEUIBM BLEUHumanRef p�LM 33.5 35.2 25.2 Λ (e|f) 44.6 44.6 31.2 p�LM 45.3 45.2 31.8 Λ (e|f) p�LM Λ (e|f) + syntax MERT (BLEUNIST) 46.2 44.5 30.2 Table 3. Test set results. (a) (b) SQ VP NP (c) N N V WH ? _PhF5 AWE ?t �里 ? Where is the currency exchange ofÞce ? Example Derivation: (Step 1) X1 -&gt; &lt;_PhF5 AWE, currency exchange office&gt; (Step 2) X2 -&gt; &lt; [X1] ?t �里 ?, Where is the [X1] ?&gt; Example Syntax feature = for Step 2 SQ Where is the [X1] ? ?t �里 ? NP X1 Figu</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the ACL (ACL-2003), pages 160– 167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the ACL and 3rd Annual Meeting of the NAACL (ACL-2002),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="20291" citStr="Papineni et al., 2002" startWordPosition="3347" endWordPosition="3350">ion task (Eck and Hori, 2005), using the 2004 test set as development data for tuning the hyperparameters and MERT training the benchmark systems. The statistics for this data are presented in Table 1.3 The training data made available for this task consisted of 40k pairs of transcribed utterances, drawn from the travel domain. The development and test data for this task are somewhat unusual in that each sentence has a single human translated reference, and fifteen paraphrases of this reference, provided by monolingual annotators. Model performance is evaluated using the standard BLEU metric (Papineni et al., 2002) which measures average n-gram precision, n &lt; 4, and we use the NIST definition of the brevity penalty for multiple reference test sets. We provide evaluation against both the entire multi-reference sets, and the single human translation. Our translation grammar is induced using the standard alignment and rule extraction heuristics used in hierarchical translation models (Chiang, 2007).4 As these heuristics aren’t based on a generative model, and don’t guarantee that the target translation will be reachable from the source, we discard those sentence pairs for which we cannot produce a derivati</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting of the ACL and 3rd Annual Meeting of the NAACL (ACL-2002), pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006),</booktitle>
<pages>777--784</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1270" citStr="Watanabe et al., 2006" startWordPosition="167" endWordPosition="170">rate the power of the discriminative training paradigm by extracting structured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be crucial for obtaining good performance in SMT, was notably omitted. This was because adding a language model would mean that the normalising partition function could no longer be</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006), pages 777–784, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>