<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.981507">
Partial Parsing from Bitext Projections
</title>
<author confidence="0.954919">
Prashanth Mannem and Aswarth Dara
</author>
<affiliation confidence="0.9834185">
Language Technologies Research Center
International Institute of Information Technology
</affiliation>
<address confidence="0.515661">
Hyderabad, AP, India - 500032
</address>
<email confidence="0.998575">
{prashanth,abhilash.d}@research.iiit.ac.in
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999329">
Recent work has shown how a parallel
corpus can be leveraged to build syntac-
tic parser for a target language by project-
ing automatic source parse onto the target
sentence using word alignments. The pro-
jected target dependency parses are not al-
ways fully connected to be useful for train-
ing traditional dependency parsers. In this
paper, we present a greedy non-directional
parsing algorithm which doesn’t need a
fully connected parse and can learn from
partial parses by utilizing available struc-
tural and syntactic information in them.
Our parser achieved statistically signifi-
cant improvements over a baseline system
that trains on only fully connected parses
for Bulgarian, Spanish and Hindi. It also
gave a significant improvement over pre-
viously reported results for Bulgarian and
set a benchmark for Hindi.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979804411764706">
Parallel corpora have been used to transfer in-
formation from source to target languages for
Part-Of-Speech (POS) tagging, word sense disam-
biguation (Yarowsky et al., 2001), syntactic pars-
ing (Hwa et al., 2005; Ganchev et al., 2009; Jiang
and Liu, 2010) and machine translation (Koehn,
2005; Tiedemann, 2002). Analysis on the source
sentences was induced onto the target sentence via
projections across word aligned parallel corpora.
Equipped with a source language parser and a
word alignment tool, parallel data can be used to
build an automatic treebank for a target language.
The parse trees given by the parser on the source
sentences in the parallel data are projected onto the
target sentence using the word alignments from
the alignment tool. Due to the usage of automatic
source parses, automatic word alignments and dif-
ferences in the annotation schemes of source and
target languages, the projected parses are not al-
ways fully connected and can have edges missing
(Hwa et al., 2005; Ganchev et al., 2009). Non-
literal translations and divergences in the syntax
of the two languages also lead to incomplete pro-
jected parse trees.
Figure 1 shows an English-Hindi parallel sen-
tence with correct source parse, alignments and
target dependency parse. For the same sentence,
Figure 2 is a sample partial dependency parse pro-
jected using an automatic source parser on aligned
text. This parse is not fully connected with the
words banaa, kottaige and dikhataa left without
any parents.
The cottage built on the hill looks very beautiful
pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
</bodyText>
<figureCaption confidence="0.995317">
Figure 1: Word alignment with dependency
</figureCaption>
<bodyText confidence="0.9908455">
parses for an English-Hindi parallel sentence
To train the traditional dependency parsers (Ya-
mada and Matsumoto, 2003; Eisner, 1996; Nivre,
2003), the dependency parse has to satisfy four
constraints: connectedness, single-headedness,
acyclicity and projectivity (Kuhlmann and Nivre,
2006). Projectivity can be relaxed in some parsers
(McDonald et al., 2005; Nivre, 2009). But these
parsers can not directly be used to learn from par-
tially connected parses (Hwa et al., 2005; Ganchev
et al., 2009).
In the projected Hindi treebank (section 4) that
was extracted from English-Hindi parallel text,
only 5.9% of the sentences had full trees. In
</bodyText>
<page confidence="0.947572">
1597
</page>
<note confidence="0.9801725">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597–1606,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.907374583333334">
Spanish and Bulgarian projected data extracted by
Ganchev et al. (2009), the figures are 3.2% and
12.9% respectively. Learning from data with such
high proportions of partially connected depen-
dency parses requires special parsing algorithms
which are not bound by connectedness. Its only
during learning that the constraint doesn’t satisfy.
For a new sentence (i.e. during inference), the
parser should output fully connected dependency
tree.
hill on build PastPart. cottage very beautiful look Be.Pres.
pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
</bodyText>
<figureCaption confidence="0.986578">
Figure 2: A sample dependency parse with partial
parses
</figureCaption>
<bodyText confidence="0.99994762962963">
In this paper, we present a dependency pars-
ing algorithm which can train on partial projected
parses and can take rich syntactic information as
features for learning. The parsing algorithm con-
structs the partial parses in a bottom-up manner by
performing a greedy search over all possible rela-
tions and choosing the best one at each step with-
out following either left-to-right or right-to-left
traversal. The algorithm is inspired by earlier non-
directional parsing works of Shen and Joshi (2008)
and Goldberg and Elhadad (2010). We also pro-
pose an extended partial parsing algorithm that can
learn from partial parses whose yields are partially
contiguous.
Apart from bitext projections, this work can be
extended to other cases where learning from par-
tial structures is required. For example, while
bootstrapping parsers high confidence parses are
extracted and trained upon (Steedman et al., 2003;
Reichart and Rappoport, 2007). In cases where
these parses are few, learning from partial parses
might be beneficial.
We train our parser on projected Hindi, Bulgar-
ian and Spanish treebanks and show statistically
significant improvements in accuracies between
training on fully connected trees and learning from
partial parses.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999817510638298">
Learning from partial parses has been dealt in dif-
ferent ways in the literature. Hwa et al. (2005)
used post-projection completion/transformation
rules to get full parse trees from the projections
and train Collin’s parser (Collins, 1999) on them.
Ganchev et al. (2009) handle partial projected
parses by avoiding committing to entire projected
tree during training. The posterior regularization
based framework constrains the projected syntac-
tic relations to hold approximately and only in ex-
pectation. Jiang and Liu (2010) refer to align-
ment matrix and a dynamic programming search
algorithm to obtain better projected dependency
trees. They deal with partial projections by break-
ing down the projected parse into a set of edges
and training on the set of projected relations rather
than on trees.
While Hwa et al. (2005) requires full projected
parses to train their parser, Ganchev et al. (2009)
and Jiang and Liu (2010) can learn from partially
projected trees. However, the discriminative train-
ing in (Ganchev et al., 2009) doesn’t allow for
richer syntactic context and it doesn’t learn from
all the relations in the partial dependency parse.
By treating each relation in the projected depen-
dency data independently as a classification in-
stance for parsing, Jiang and Liu (2010) sacrifice
the context of the relations such as global struc-
tural context, neighboring relations that are crucial
for dependency analysis. Due to this, they report
that the parser suffers from local optimization dur-
ing training.
The parser proposed in this work (section 3)
learns from partial trees by using the available
structural information in it and also in neighbor-
ing partial parses. We evaluated our system (sec-
tion 5) on Bulgarian and Spanish projected depen-
dency data used in (Ganchev et al., 2009) for com-
parison. The same could not be carried out for
Chinese (which was the language (Jiang and Liu,
2010) worked on) due to the unavailability of pro-
jected data used in their work. Comparison with
the traditional dependency parsers (McDonald et
al., 2005; Yamada and Matsumoto, 2003; Nivre,
2003; Goldberg and Elhadad, 2010) which train on
complete dependency parsers is out of the scope of
this work.
</bodyText>
<sectionHeader confidence="0.988472" genericHeader="method">
3 Partial Parsing
</sectionHeader>
<bodyText confidence="0.9820894">
A standard dependency graph satisfies four graph
constraints: connectedness, single-headedness,
acyclicity and projectivity (Kuhlmann and Nivre,
2006). In our work, we assume the dependency
graph for a sentence only satisfies the single-
</bodyText>
<page confidence="0.971929">
1598
</page>
<figure confidence="0.990924">
a) hill on build PastPart. cottage very beautiful look Be.Pres. b) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
c) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai d) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
e) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai f) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
g) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai h) pahaada para banaa huaa kottaige bahuta dikhataa hai
sundara
</figure>
<figureCaption confidence="0.810387">
Figure 3: Steps taken by GNPPA. The dashed arcs indicate the unconnected words in unConn. The
dotted arcs indicate the candidate arcs in candidateArcs and the solid arcs are the high scoring arcs that
are stored in builtPPs
</figureCaption>
<bodyText confidence="0.993220777777778">
headedness, acyclicity and projectivity constraints
while not necessarily being connected i.e. all the
words need not have parents.
Given a sentence W=w0 · · · w,,, with a set of
directed arcs A on the words in W, w2 → wj de-
notes a dependency arc from w2 to wj, (w2,wj) c
A. w2 is the parent in the arc and wj is the child in
�
the arc. →− denotes the reflexive and transitive clo-
sure of the arc. w2 -*4 wj says that w2 dominates
wj, i.e. there is (possibly empty) path from w2 to
wj.
A node w2 is unconnected if it does not have
an incoming arc. R is the set of all such uncon-
nected nodes in the dependency graph. For the
example in Figure 2, R={banaa, kottaige,
dikhataa}. A partial parse rooted at node w2
denoted by p(w2) is the set of arcs that can be tra-
versed from node w2. The yield of a partial parse
p(w2) is the set of nodes dominated by it. We
use 7r(w2) to refer to the yield of p(w2) arranged
in the linear order of their occurrence in the sen-
tence. The span of the partial tree is the first and
last words in its yield.
The dependency graph D can now be rep-
resented in terms of partial parses by D =
(W, R, o(R)) where W={w0 · · · w,,,} is the sen-
tence, R={r1 · · · r..} is the set of unconnected
nodes and o(R)= {p(r1) · · · p(r..)} is the set of
partial parses rooted at these unconnected nodes.
w0 is a dummy word added at the beginning of
W to behave as a root of a fully connected parse.
A fully connected dependency graph would have
only one element w0 in R and the dependency
graph rooted at w0 as the only (fully connected)
parse in p(R).
We assume the combined yield of p(R) spans
the entire sentence and each of the partial parses in
p(R) to be contiguous and non-overlapping with
one another. A partial parse is contiguous if its
yield is contiguous i.e. if a node wj c 7r(w2), then
all the words between w2 and wj also belong to
7r(w2). A partial parse p(w2) is non-overlapping if
the intersection of its yield 7r(w2) with yields of all
other partial parses is empty.
</bodyText>
<subsectionHeader confidence="0.9962705">
3.1 Greedy Non-directional Partial Parsing
Algorithm (GNPPA)
</subsectionHeader>
<bodyText confidence="0.9999746">
Given the sentence W and the set of unconnected
nodes R, the parser follows a non-directional
greedy approach to establish relations in a bottom
up manner. The parser does a greedy search over
all the possible relations and picks the one with
</bodyText>
<page confidence="0.987467">
1599
</page>
<bodyText confidence="0.969450421052632">
the highest score at each stage. This process is re-
peated until parents for all the nodes that do not
belong to R are chosen.
Algorithm 1 lists the outline of the greedy non-
directional partial parsing algorithm (GNPPA).
builtPPs maintains a list of all the partial
parses that have been built. It is initialized
in line 1 by considering each word as a sep-
arate partial parse with just one node. can-
didateArcs stores all the arcs that are possi-
ble at each stage of the parsing process in a
bottom up strategy. It is initialized in line 2
using the method initCandidateArcs(w0 · · · wn).
initCandidateArcs(w0 · · · wn) adds two candidate
arcs for each pair of consecutive words with each
other as parent (see Figure 3b). If an arc has one
of the nodes in R as the child, it isn’t included in
candidateArcs.
Algorithm 1 Partial Parsing Algorithm
</bodyText>
<equation confidence="0.6692534">
Input: sentence w0 · · · wn and set of partial tree roots un-
Conn={r1 · · · rm}
Output: set of partial parses whose roots are in unConn
(builtPPs = {p(r1) · · · p(rm)})
1: builtPPs = {p(r1) · · · p(rn)} — {w0 · · · wn}
</equation>
<listItem confidence="0.946051727272727">
2: candidateArcs = initCandidateArcs(w0 · · · wn)
3: while candidateArcs.isNotEmpty() do
4: bestArc = argmax score(ci, ��w)
ci a candidateArcs
5: builtPPs.remove(bestArc.child)
6: builtPPs.remove(bestArc.parent)
7: builtPPs.add(bestArc)
8: updateCandidateArcs(bestArc,
candidateArcs, builtPPs, unConn)
9: end while
10: return builtPPs
</listItem>
<bodyText confidence="0.99366406060606">
Once initialized, the candidate arc with the
highest score (line 4) is chosen and accepted
into builtPPs. This involves replacing the best
arc’s child partial parse p(arc.child) and parent
partial parse p(arc.parent) over which the arc
has been formed with the arc p(arc.parent) →
p(arc.child) itself in builtPPs (lines 5-7). In Figure
3f, to accept the best candidate arc p(banaa) →
p(pahaada), the parser would remove the nodes
p(banaa) and p(pahaada) in builtPPs and add
p(banaa) → p(pahaada) to builtPPs (see Fig-
ure 3g).
After the best arc is accepted, the candidateArcs
has to be updated (line 8) to remove the arcs that
are no longer valid and add new arcs in the con-
text of the updated builtPPs. Algorithm 2 shows
the update procedure. First, all the arcs that end
on the child are removed (lines 3-7) along with
the arc from child to parent. Then, the immedi-
ately previous and next partial parses of the best
arc in builtPPs are retrieved (lines 8-9) to add pos-
sible candidate arcs between them and the partial
parse representing the best arc (lines 10-23). In
the example, between Figures 3b and 3c, the arcs
p(kottaige) → p(bahuta) and p(bahuta)
→ p(sundara) are first removed and the arc
p(kottaige) → p(sundara) is added to can-
didateArcs. Care is taken to avoid adding arcs that
end on unconnected nodes listed in R.
The entire GNPPA parsing process for the ex-
ample sentence in Figure 2 is shown in Figure 3.
Algorithm 2 updateCandidateArcs(bestArc, can-
didateArcs, builtPPs, unConn)
</bodyText>
<listItem confidence="0.896980055555556">
1: baChild = bestArc.child
2: baParent = bestArc.parent
3: for all arc a candidateArcs do
4: if arc.child = baChild or
(arc.parent = baChild and
arc.child = baParent) then
5: remove arc
6: end if
7: end for
8: prevPP = builtPPs.previousPP(bestArc)
9: nextPP = builtPPs.nextPP(bestArc)
10: if bestArc.direction ==LEFT then
11: newArc1 = new Arc(prevPP,baParent)
12: newArc2 = new Arc(baParent,prevPP)
13: end if
14: if bestArc.direction == RIGHT then
15: newArc1 = new Arc(nextPP,baParent)
16: newArc2 = new Arc(baParent,nextPP)
</listItem>
<reference confidence="0.31645575">
17: end if
18: if newArc1.parent � unConn then
19: candidateArcs.add(newArc1)
20: end if
21: if newArc2.parent � unConn then
22: candidateArcs.add(newArc2)
23: end if
24: return candidateArcs
</reference>
<subsectionHeader confidence="0.999353">
3.2 Learning
</subsectionHeader>
<bodyText confidence="0.999933866666667">
The algorithm described in the previous section
uses a weight vector →−w to compute the best arc
from the list of candidate arcs. This weight vec-
tor is learned using a simple Perceptron like algo-
rithm similar to the one used in (Shen and Joshi,
2008). Algorithm 3 lists the learning framework
for GNPPA.
For a training sample with sentence wo · · · wn,
projected partial parses projectedPPs={p(ri) · · ·
p(r..)}, unconnected words unConn and weight
vector →−w , the builtPPs and candidateArcs are ini-
tiated as in algorithm 1. Then the arc with the
highest score is selected. If this arc belongs to
the parses in projectedPPs, builtPPs and candi-
dateArcs are updated similar to the operations in
</bodyText>
<page confidence="0.849535">
1600
</page>
<figure confidence="0.777989333333333">
a) hill on build PastPart. cottage very beautiful look Be.Pres. b) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
c) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai d) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
</figure>
<figureCaption confidence="0.648756833333333">
Figure 4: First four steps taken by E-GNPPA. The blue colored dotted arcs are the additional candidate
arcs that are added to candidateArcs
algorithm 1. If it doesn’t, it is treated as a neg-
ative sample and a corresponding positive candi-
date arc which is present both projectedPPs and
candidateArcs is selected (lines 11-12).
</figureCaption>
<bodyText confidence="0.844000333333333">
The weights of the positive candidate arc are in-
creased while that of the negative sample (best arc)
are decreased. To reduce over fitting, we use aver-
aged weights (Collins, 2002) in algorithm 1.
Algorithm 3 Learning for Non-directional Greedy
Partial Parsing Algorithm
Input: sentence wo · · · wn, projected partial parses project-
edPPs, unconnected words unConn, current w
Output: updated w
</bodyText>
<listItem confidence="0.901663785714286">
1: builtPPs = {p(rl) · · · p(rn)} +— {w0 · · · wn}
2: candidateArcs = initCandidateArcs(w0 · · · wn)
3: while candidateArcs.isNotEmpty() do
4: bestArc = argmax score(ci, w)
ci a candidateArca
5: if bestArc E projectedPPs then
6: builtPPs.remove(bestArc.child)
7: builtPPs.remove(bestArc.parent)
8: builtPPs.add(bestArc)
9: updateCandidateArcs(bestArc,
candidateArcs, builtPPs, unConn)
10: else
11: allowedArcs = {ci I ci a candidateArcs &amp;&amp; ci e
12: compatArc = argmax
</listItem>
<figure confidence="0.925300571428571">
projectedArcs} score(ci, &apos;w)
ci a allowedArca
13: promote(compatArc,w)
14: demote(bestArc,w )
15: end if
16: end while
17: return builtPPs
</figure>
<subsectionHeader confidence="0.979782">
3.3 Extended GNPPA (E-GNPPA)
</subsectionHeader>
<bodyText confidence="0.920054461538462">
The GNPPA described in section 3.1 assumes that
the partial parses are contiguous. The exam-
ple in Figure 5 has a partial tree p(dikhataa)
which isn’t contiguous. Its yield doesn’t con-
tain bahuta and sundara. We call such non-
contiguous partial parses whose yields encompass
the yield of an other partial parse as partially con-
tiguous. Partially contiguous parses are common
in the projected data and would not be parsable by
the algorithm 1 (p(dikhataa) —4 p(kottaige)
would not be identified).
hill on build PastPart. cottage very beautiful look Be.Pres.
pahaada para banaa huaa kottaige bahuta sundara dikhataa hai
</bodyText>
<figureCaption confidence="0.9865805">
Figure 5: Dependency parse with a partially con-
tiguous partial parse
</figureCaption>
<bodyText confidence="0.999347315789474">
In order to identify and learn from relations
which are part of partially contiguous partial
parses, we propose an extension to GNPPA. The
extended GNPAA (E-GNPPA) broadens its scope
while searching for possible candidate arcs given
R and builtPPs. If the immediate previous or
the next partial parses over which arcs are to
be formed are designated unconnected nodes, the
parser looks further for a partial parse over which
it can form arcs. For example, in Figure 4b, the
arc p(para) -4 p(banaa) can not be added to
the candidateArcs since banaa is a designated
unconnected node in unConn. The E-GNPPA
looks over the unconnected node and adds the arc
p(para) -4 p(huaa) to the candidate arcs list
candidateArcs.
E-GNPPA differs from algorithm 1 in lines 2
and 8. The E-GNPPA uses an extended initializa-
tion method initCandidateArcsExtended(w0) for
</bodyText>
<page confidence="0.869485">
1601
</page>
<table confidence="0.999329833333333">
Parent and Child par.pos, chd.pos, par.lex, chd.lex
Sentence Context par-1.pos, par-2.pos, par+1.pos, par+2.pos, par-1.lex, par+1.lex
chd-1.pos, chd-2.pos, chd+1.pos, chd+2.pos, chd-1.lex, chd+1.lex
Structural Info leftMostChild(par).pos, rightMostChild(par).pos, leftSibling(chd).pos,
rightSibling(chd).pos
Partial Parse Context previousPP().pos, previousPP().lex, nextPP().pos, nextPP().lex
</table>
<tableCaption confidence="0.999597">
Table 1: Information on which features are defined. par denotes the parent in the relation and chd the
</tableCaption>
<bodyText confidence="0.988022181818182">
child. .pos and .lex is the POS and word-form of the corresponding node. +/-i is the previous/next
ith word in the sentence. leftMostChild() and rightMostChild() denote the left most and right most
children of a node. leftSibling() and rightSibling() get the immediate left and right siblings of a node.
previousPP() and nextPP() return the immediate previous and next partial parses of the arc in builtPPs at
the state.
candidateArcs in line 2 and an extended proce-
dure updateCandidateArcsExtended to update the
candidateArcs after each step in line 8. Algorithm
4 shows the changes w.r.t algorithm 2. Figure 4
presents the steps taken by the E-GNPPA parser
for the example parse in Figure 5.
</bodyText>
<equation confidence="0.9191102">
Algorithm 4 updateCandidateArcsExtended
( bestArc, candidateArcs, builtPPs,unConn )
· · · lines 1 to 7 of Algorithm 2 · · ·
prevPP = builtPPs.previousPP(bestArc)
while prevPP ∈ unConn do
prevPP = builtPPs.previousPP(prevPP)
end while
nextPP = builtPPs.nextPP(bestArc)
while nextPP ∈ unConn do
nextPP = builtPPs.nextPP(nextPP)
</equation>
<listItem confidence="0.376864">
end while
· · · lines 10 to 24 of Algorithm 2 · · ·
</listItem>
<subsectionHeader confidence="0.679958">
3.4 Features
</subsectionHeader>
<bodyText confidence="0.9998086">
Features for a relation (candidate arc) are defined
on the POS tags and lexical items of the nodes in
the relation and those in its context. Two kinds
of context are used a) context from the input sen-
tence (sentence context) b) context in builtPPs i.e.
nearby partial parses (partial parse context). In-
formation from the partial parses (structural info)
such as left and right most children of the par-
ent node in the relation, left and right siblings of
the child node in the relation are also used. Ta-
ble 1 lists the information on which features are
defined in the various configurations of the three
language parsers. The actual features are combi-
nations of the information present in the table. The
set varies depending on the language and whether
its GNPPA or E-GNPPA approach.
While training, no features are defined on
whether a node is unconnected (present in un-
Conn) or not as this information isn’t available
during testing.
</bodyText>
<sectionHeader confidence="0.997058" genericHeader="method">
4 Hindi Projected Dependency Treebank
</sectionHeader>
<bodyText confidence="0.999867827586207">
We conducted experiments on English-Hindi par-
allel data by transferring syntactic information
from English to Hindi to build a projected depen-
dency treebank for Hindi.
The TIDES English-Hindi parallel data con-
taining 45,000 sentences was used for this pur-
pose 1 (Venkatapathy, 2008). Word alignments
for these sentences were obtained using the widely
used GIZA++ toolkit in grow-diag-final-and mode
(Och and Ney, 2003). Since Hindi is a morpho-
logically rich language, root words were used in-
stead of the word forms. A bidirectional English
POS tagger (Shen et al., 2007) was used to POS
tag the source sentences and the parses were ob-
tained using the first order MST parser (McDon-
ald et al., 2005) trained on dependencies extracted
from Penn treebank using the head rules of Ya-
mada and Matsumoto (2003). A CRF based Hindi
POS tagger (PVS. and Gali, 2007) was used to
POS tag the target sentences.
English and Hindi being morphologically and
syntactically divergent makes the word alignment
and dependency projection a challenging task.
The source dependencies are projected using an
approach similar to (Hwa et al., 2005). While
they use post-projection transformations on the
projected parse to account for annotation differ-
ences, we use pre-projection transformations on
the source parse. The projection algorithm pro-
</bodyText>
<footnote confidence="0.99687925">
1The original data had 50,000 parallel sentences. It was
later refined by IIIT-Hyderabad to remove repetitions and
other trivial errors. The corpus is still noisy with typographi-
cal errors, mismatched sentences and unfaithful translations.
</footnote>
<page confidence="0.997533">
1602
</page>
<bodyText confidence="0.9813935">
duces acyclic parses which could be unconnected
and non-projective.
</bodyText>
<subsectionHeader confidence="0.9880845">
4.1 Annotation Differences in Hindi and
English
</subsectionHeader>
<bodyText confidence="0.999990714285714">
Before projecting the source parses onto the tar-
get sentence, the parses are transformed to reflect
the annotation scheme differences in English and
Hindi. While English dependency parses reflect
the PTB annotation style (Marcus et al., 1994),
we project them to Hindi to reflect the annotation
scheme described in (Begum et al., 2008). The
differences in the annotation schemes are with re-
spect to three phenomena: a) head of a verb group
containing auxiliary and main verbs, b) preposi-
tions in a prepositional phrase (PP) and c) coordi-
nation structures.
In the English parses, the auxiliary verb is the
head of the main verb while in Hindi, the main
verb is the head of the auxiliary in the verb group.
For example, in the Hindi parse in Figure 1,
dikhataa is the head of the auxiliary verb hai.
The prepositions in English are realized as post-
positions in Hindi. While prepositions are the
heads in a preposition phrase, post-positions are
the modifiers of the preceding nouns in Hindi. In
pahaada para (on the hill), hill is the head
of para. In coordination structures, while En-
glish differentiates between how NP coordination
and VP coordination structures behave, Hindi an-
notation scheme is consistent in its handling. Left-
most verb is the head of a VP coordination struc-
ture in English whereas the rightmost noun is the
head in case of NP coordination. In Hindi, the con-
junct is the head of the two verbs/nouns in the co-
ordination structure.
These three cases are identified in the source
tree and appropriate transformations are made to
the source parse itself before projecting the rela-
tions using word alignments.
</bodyText>
<sectionHeader confidence="0.998176" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999720444444444">
We carried out all our experiments on paral-
lel corpora belonging to English-Hindi, English-
Bulgarian and English-Spanish language pairs.
While the Hindi projected treebank was obtained
using the method described in section 4, Bulgar-
ian and Spanish projected datasets were obtained
using the approach in (Ganchev et al., 2009). The
datasets of Bulgarian and Spanish that contributed
to the best accuracies for Ganchev et al. (2009)
</bodyText>
<table confidence="0.999358222222222">
Statistic Hindi Bulgarian Spanish
N(Words) 226852 71986 133124
N(Parent==-1) 44607 30268 54815
P(Parent==-1) 19.7 42.0 41.1
N(Full trees) 593 1299 327
N(GNPPA) 30063 10850 19622
P(GNPPA) 16.4 26.0 25.0
N(E-GNPPA) 35389 12281 24577
P(E-GNPPA) 19.3 29.4 30.0
</table>
<tableCaption confidence="0.897075333333333">
Table 2: Statistics of the Hindi, Bulgarian and Spanish
projected treebanks used for experiments. Each of them has
10,000 randomly picked parses. N(X) denotes number of X
</tableCaption>
<bodyText confidence="0.972349567567568">
and P(X) denotes percentage of X. N(Words) is the number
of words. N(Parents==-1) is the number of words without a
parent. N(Full trees) is the number of parses which are fully
connected. N(GNPPA) is the number of relations learnt by
GNPPA parser and N(E-GNPPA) is the number of relations
learnt by E-GNPPA parser. Note that P(GNPPA) is calculated
as N(GNPPA)/(N(Words) - N(Parents==-1)).
were used in our work (7 rules dataset for Bulgar-
ian and 3 rules dataset for Spanish). The Hindi,
Bulgarian and Spanish projected dependency tree-
banks have 44760, 39516 and 76958 sentences re-
spectively. Since we don’t have confidence scores
for the projections on the sentences, we picked
10,000 sentences randomly in each of the three
datasets for training the parsers2. Other methods
of choosing the 10K sentences such as those with
the max. no. of relations, those with least no. of
unconnected words, those with max. no. of con-
tiguous partial trees that can be learned by GNPPA
parser etc. were tried out. Among all these, ran-
dom selection was consistent and yielded the best
results. The errors introduced in the projected
parses by errors in word alignment, source parser
and projection are not consistent enough to be ex-
ploited to select the better parses from the entire
projected data.
Table 2 gives an account of the randomly cho-
sen 10k sentences in terms of the number of words,
words without parents etc. Around 40% of the
words spread over 88% of sentences in Bulgarian
and 97% of sentences in Spanish have no parents.
Traditional dependency parsers which only train
from fully connected trees would not be able to
learn from these sentences. P(GNPPA) is the per-
centage of relations in the data that are learned by
the GNPPA parser satisfying the contiguous par-
tial tree constraint and P(E-GNPPA) is the per-
</bodyText>
<footnote confidence="0.98096">
2Exactly 10K sentences were selected in order to compare
our results with those of (Ganchev et al., 2009).
</footnote>
<page confidence="0.791702">
1603
</page>
<table confidence="0.999840857142857">
Parser Hindi Bulgarian Spanish
Punct NoPunct Punct NoPunct Punct NoPunct
Baseline 78.70 77.39 51.85 55.15 41.60 45.61
GNPPA
E-GNPPA
80.03* 78.81* 77.03* 79.06* 65.49* 68.70*
81.10*† 79.94*† 78.93*† 80.11*† 67.69*† 70.90*†
</table>
<tableCaption confidence="0.7731065">
Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained
on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates
without punctuation. * next to an accuracy denotes statistically significant (McNemar’s and p &lt; 0.05)
improvement over the baseline. † denotes significance over GNPPA
</tableCaption>
<bodyText confidence="0.9999321875">
centage that satisfies the partially contiguous con-
straint. E-GNPPA parser learns around 2-5% more
no. of relations than GNPPA due to the relaxation
in the constraints.
The Hindi test data that was released as part of
the ICON-2010 Shared Task (Husain et al., 2010)
was used for evaluation. For Bulgarian and Span-
ish, we used the same test data that was used in
the work of Ganchev et al. (2009). These test
datasets had sentences from the training section of
the CoNLL Shared Task (Nivre et al., 2007) that
had lengths less than or equal to 10. All the test
datasets have gold POS tags.
A baseline parser was built to compare learning
from partial parses with learning from fully con-
nected parses. Full parses are constructed from
partial parses in the projected data by randomly
assigning parents to unconnected parents, similar
to the work in (Hwa et al., 2005). The uncon-
nected words in the parse are selected randomly
one by one and are assigned parents randomly to
complete the parse. This process is repeated for all
the sentences in the three language datasets. The
parser is then trained with the GNPPA algorithm
on these fully connected parses to be used as the
baseline.
Table 3 lists the accuracies of the baseline,
GNPPA and E-GNPPA parsers. The accuracies
are unlabeled attachment scores (UAS): the per-
centage of words with the correct head. Table
4 compares our accuracies with those reported in
(Ganchev et al., 2009) for Bulgarian and Spanish.
</bodyText>
<subsectionHeader confidence="0.970654">
5.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999653666666667">
The baseline reported in (Ganchev et al., 2009)
significantly outperforms our baseline (see Table
4) due to the different baselines used in both the
works. In our work, while creating the data for
the baseline by assigning random parents to un-
connected words, acyclicity and projectivity con-
</bodyText>
<table confidence="0.999178">
Parser Bulgarian Spanish
Ganchev-Baseline 72.6 69.0
Baseline 55.15 45.61
Ganchev-Discriminative 78.3 72.3
GNPPA 79.06 68.70
E-GNPPA 80.11 70.90
</table>
<tableCaption confidence="0.99697">
Table 4: Comparison of baseline, GNPPA and E-
</tableCaption>
<bodyText confidence="0.973184166666666">
GNPPA with baseline and discriminative model
from (Ganchev et al., 2009) for Bulgarian and
Spanish. Evaluation didn’t include punctuation.
straints are not enforced. Ganchev et al. (2009)’s
baseline is similar to the first iteration of their dis-
criminative model and hence performs better than
ours. Our Bulgarian E-GNPPA parser achieved a
1.8% gain over theirs while the Spanish results are
lower. Though their training data size is also 10K,
the training data is different in both our works due
to the difference in the method of choosing 10K
sentences from the large projected treebanks.
The GNPPA accuracies (see table 3) for all the
three languages are significant improvements over
the baseline accuracies. This shows that learning
from partial parses is effective when compared to
imposing the connected constraint on the partially
projected dependency parse. Even while project-
ing source dependencies during data creation, it
is better to project high confidence relations than
look to project more relations and thereby intro-
duce noise.
The E-GNPPA which also learns from partially
contiguous partial parses achieved statistically sig-
nificant gains for all the three languages. The
gains across languages is due to the fact that in
the 10K data that was used for training, E-GNPPA
parser could learn 2 − 5% more relations over
GNPPA (see Table 2).
Figure 6 shows the accuracies of baseline and E-
</bodyText>
<page confidence="0.989983">
1604
</page>
<figure confidence="0.996737571428571">
80
70
60
50
40
30
0
</figure>
<figureCaption confidence="0.932181666666667">
Figure 6: Accuracies (without punctuation) w.r.t
varying training data sizes for baseline and E-
GNPPA parsers.
</figureCaption>
<bodyText confidence="0.97633225">
GNPPA parser for the three languages when train-
ing data size is varied. The parsers peak early with
less than 1000 sentences and make small gains
with the addition of more data.
</bodyText>
<sectionHeader confidence="0.998964" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997875">
We presented a non-directional parsing algorithm
that can learn from partial parses using syntac-
tic and contextual information as features. A
Hindi projected dependency treebank was devel-
oped from English-Hindi bilingual data and ex-
periments were conducted for three languages
Hindi, Bulgarian and Spanish. Statistically sig-
nificant improvements were achieved by our par-
tial parsers over the baseline system. The partial
parsing algorithms presented in this paper are not
specific to bitext projections and can be used for
learning from partial parses in any setting.
</bodyText>
<sectionHeader confidence="0.997788" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995553882352941">
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai,
and R. Sangal. 2008. Dependency annotation
scheme for indian languages. In In Proceedings of
The Third International Joint Conference on Natural
Language Processing (IJCNLP), Hyderabad, India.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
AAI9926110.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
’02, pages 1–8, Morristown, NJ, USA. Association
for Computational Linguistics.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 1, pages 340–345, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1, ACL-IJCNLP ’09, pages 369–
377, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 742–750, Morristown, NJ,
USA. Association for Computational Linguistics.
Samar Husain, Prashanth Mannem, Bharath Ambati,
and Phani Gadde. 2010. Icon 2010 tools contest on
indian language dependency parsing. In Proceed-
ings of ICON 2010 NLP Tools Contest.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11:311–325, September.
Wenbin Jiang and Qun Liu. 2010. Dependency parsing
and projection based on word-pair classification. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ’10,
pages 12–20, Morristown, NJ, USA. Association for
Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Citeseer.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL on Main conference poster
sessions, pages 507–514, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
</reference>
<page confidence="0.805157">
1605
</page>
<reference confidence="0.999842743589744">
Jens Nilsson and Joakim Nivre. 2008. Malteval:
an evaluation and visualization tool for dependency
parsing. In Proceedings of the Sixth International
Language Resources and Evaluation (LREC’08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague, Czech Republic. Association for
Computational Linguistics.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Eighth International
Workshop on Parsing Technologies, Nancy, France.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
351–359, Suntec, Singapore, August. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Avinesh PVS. and Karthik Gali. 2007. Part-Of-Speech
Tagging and Chunking using Conditional Random
Fields and Transformation-Based Learning. In Pro-
ceedings of the IJCAI and the Workshop On Shallow
Parsing for South Asian Languages (SPSAL), pages
21–24.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statisti-
cal parsers trained on small datasets. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 616–623,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 495–504, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the tenth conference on
European chapter of the Association for Computa-
tional Linguistics - Volume 1, EACL ’03, pages 331–
338, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Jrg Tiedemann. 2002. MatsLex - a multilingual lex-
ical database for machine translation. In Proceed-
ings of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC’2002), vol-
ume VI, pages 1909–1912, Las Palmas de Gran Ca-
naria, Spain, 29-31 May.
Sriram Venkatapathy. 2008. Nlp tools contest - 2008:
Summary. In Proceedings of ICON 2008 NLP Tools
Contest.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector Ma-
chines. In In Proceedings of IWPT, pages 195–206.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, HLT ’01,
pages 1–8, Morristown, NJ, USA. Association for
Computational Linguistics.
</reference>
<page confidence="0.991948">
1606
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.240396">
<title confidence="0.997875">Partial Parsing from Bitext Projections</title>
<author confidence="0.601074">Prashanth Mannem</author>
<author confidence="0.601074">Aswarth</author>
<affiliation confidence="0.689805666666667">Language Technologies Research International Institute of Information Hyderabad, AP, India -</affiliation>
<abstract confidence="0.997419190476191">Recent work has shown how a parallel corpus can be leveraged to build syntactic parser for a target language by projecting automatic source parse onto the target sentence using word alignments. The projected target dependency parses are not always fully connected to be useful for training traditional dependency parsers. In this paper, we present a greedy non-directional parsing algorithm which doesn’t need a fully connected parse and can learn from partial parses by utilizing available structural and syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>R Begum</author>
<author>S Husain</author>
<author>A Dhwaj</author>
<author>D Sharma</author>
<author>L Bai</author>
<author>R Sangal</author>
</authors>
<title>17: end if 18: if newArc1.parent � unConn then 19: candidateArcs.add(newArc1) 20: end if 21: if newArc2.parent � unConn then 22: candidateArcs.add(newArc2) 23: end if 24: return candidateArcs</title>
<date>2008</date>
<booktitle>In Proceedings of The Third International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<location>Hyderabad, India.</location>
<marker>Begum, Husain, Dhwaj, Sharma, Bai, Sangal, 2008</marker>
<rawString>17: end if 18: if newArc1.parent � unConn then 19: candidateArcs.add(newArc1) 20: end if 21: if newArc2.parent � unConn then 22: candidateArcs.add(newArc2) 23: end if 24: return candidateArcs R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and R. Sangal. 2008. Dependency annotation scheme for indian languages. In In Proceedings of The Third International Joint Conference on Natural Language Processing (IJCNLP), Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5653" citStr="Collins, 1999" startWordPosition="863" endWordPosition="864">ned upon (Steedman et al., 2003; Reichart and Rappoport, 2007). In cases where these parses are few, learning from partial parses might be beneficial. We train our parser on projected Hindi, Bulgarian and Spanish treebanks and show statistically significant improvements in accuracies between training on fully connected trees and learning from partial parses. 2 Related Work Learning from partial parses has been dealt in different ways in the literature. Hwa et al. (2005) used post-projection completion/transformation rules to get full parse trees from the projections and train Collin’s parser (Collins, 1999) on them. Ganchev et al. (2009) handle partial projected parses by avoiding committing to entire projected tree during training. The posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation. Jiang and Liu (2010) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. They deal with partial projections by breaking down the projected parse into a set of edges and training on the set of projected relations rather than on trees. While Hwa et al. (2005) requires full </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, USA. AAI9926110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02, pages 1–8, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: an exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics -</booktitle>
<volume>1</volume>
<pages>340--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2844" citStr="Eisner, 1996" startWordPosition="439" endWordPosition="440">ish-Hindi parallel sentence with correct source parse, alignments and target dependency parse. For the same sentence, Figure 2 is a sample partial dependency parse projected using an automatic source parser on aligned text. This parse is not fully connected with the words banaa, kottaige and dikhataa left without any parents. The cottage built on the hill looks very beautiful pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 1: Word alignment with dependency parses for an English-Hindi parallel sentence To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003), the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009). But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In 1597 Proceedings of the 49th Annual Meeting of the Association for Computational Linguist</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: an exploration. In Proceedings of the 16th conference on Computational linguistics - Volume 1, pages 340–345, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL-IJCNLP ’09,</booktitle>
<pages>369--377</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1302" citStr="Ganchev et al., 2009" startWordPosition="190" endWordPosition="193">can learn from partial parses by utilizing available structural and syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. 1 Introduction Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001), syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002). Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora. Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank for a target language. The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool. Due to the usage of automatic source parses, automatic word alignments and diffe</context>
<context position="3210" citStr="Ganchev et al., 2009" startWordPosition="492" endWordPosition="495"> looks very beautiful pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 1: Word alignment with dependency parses for an English-Hindi parallel sentence To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003), the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009). But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In 1597 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597–1606, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Spanish and Bulgarian projected data extracted by Ganchev et al. (2009), the figures are 3.2% and 12.9% respectively. Learning from data with such high proportions of partially connected dependency parses requires special parsing algorithms which are not boun</context>
<context position="5684" citStr="Ganchev et al. (2009)" startWordPosition="867" endWordPosition="870">., 2003; Reichart and Rappoport, 2007). In cases where these parses are few, learning from partial parses might be beneficial. We train our parser on projected Hindi, Bulgarian and Spanish treebanks and show statistically significant improvements in accuracies between training on fully connected trees and learning from partial parses. 2 Related Work Learning from partial parses has been dealt in different ways in the literature. Hwa et al. (2005) used post-projection completion/transformation rules to get full parse trees from the projections and train Collin’s parser (Collins, 1999) on them. Ganchev et al. (2009) handle partial projected parses by avoiding committing to entire projected tree during training. The posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation. Jiang and Liu (2010) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. They deal with partial projections by breaking down the projected parse into a set of edges and training on the set of projected relations rather than on trees. While Hwa et al. (2005) requires full projected parses to train their</context>
<context position="7209" citStr="Ganchev et al., 2009" startWordPosition="1111" endWordPosition="1114">on in the projected dependency data independently as a classification instance for parsing, Jiang and Liu (2010) sacrifice the context of the relations such as global structural context, neighboring relations that are crucial for dependency analysis. Due to this, they report that the parser suffers from local optimization during training. The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al., 2009) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work. 3 Partial Parsing A standard dependency graph satisfies four graph constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). In our work, we assume th</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL-IJCNLP ’09, pages 369– 377, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>742--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4699" citStr="Goldberg and Elhadad (2010)" startWordPosition="717" endWordPosition="720">a huaa kottaige bahuta sundara dikhataa hai Figure 2: A sample dependency parse with partial parses In this paper, we present a dependency parsing algorithm which can train on partial projected parses and can take rich syntactic information as features for learning. The parsing algorithm constructs the partial parses in a bottom-up manner by performing a greedy search over all possible relations and choosing the best one at each step without following either left-to-right or right-to-left traversal. The algorithm is inspired by earlier nondirectional parsing works of Shen and Joshi (2008) and Goldberg and Elhadad (2010). We also propose an extended partial parsing algorithm that can learn from partial parses whose yields are partially contiguous. Apart from bitext projections, this work can be extended to other cases where learning from partial structures is required. For example, while bootstrapping parsers high confidence parses are extracted and trained upon (Steedman et al., 2003; Reichart and Rappoport, 2007). In cases where these parses are few, learning from partial parses might be beneficial. We train our parser on projected Hindi, Bulgarian and Spanish treebanks and show statistically significant im</context>
<context position="7536" citStr="Goldberg and Elhadad, 2010" startWordPosition="1165" endWordPosition="1168">on during training. The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al., 2009) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work. 3 Partial Parsing A standard dependency graph satisfies four graph constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). In our work, we assume the dependency graph for a sentence only satisfies the single1598 a) hill on build PastPart. cottage very beautiful look Be.Pres. b) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai pahaada para banaa huaa kottaige bahuta sundara dikhataa hai c) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai d) pahaada</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 742–750, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samar Husain</author>
<author>Prashanth Mannem</author>
<author>Bharath Ambati</author>
<author>Phani Gadde</author>
</authors>
<title>Icon</title>
<date>2010</date>
<booktitle>In Proceedings of ICON 2010 NLP Tools Contest.</booktitle>
<marker>Husain, Mannem, Ambati, Gadde, 2010</marker>
<rawString>Samar Husain, Prashanth Mannem, Bharath Ambati, and Phani Gadde. 2010. Icon 2010 tools contest on indian language dependency parsing. In Proceedings of ICON 2010 NLP Tools Contest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>11--311</pages>
<contexts>
<context position="1280" citStr="Hwa et al., 2005" startWordPosition="186" endWordPosition="189">nnected parse and can learn from partial parses by utilizing available structural and syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. 1 Introduction Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001), syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002). Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora. Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank for a target language. The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool. Due to the usage of automatic source parses, automatic wor</context>
<context position="3187" citStr="Hwa et al., 2005" startWordPosition="488" endWordPosition="491"> built on the hill looks very beautiful pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 1: Word alignment with dependency parses for an English-Hindi parallel sentence To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003), the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009). But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In 1597 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597–1606, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Spanish and Bulgarian projected data extracted by Ganchev et al. (2009), the figures are 3.2% and 12.9% respectively. Learning from data with such high proportions of partially connected dependency parses requires special parsing algori</context>
<context position="5513" citStr="Hwa et al. (2005)" startWordPosition="843" endWordPosition="846">ases where learning from partial structures is required. For example, while bootstrapping parsers high confidence parses are extracted and trained upon (Steedman et al., 2003; Reichart and Rappoport, 2007). In cases where these parses are few, learning from partial parses might be beneficial. We train our parser on projected Hindi, Bulgarian and Spanish treebanks and show statistically significant improvements in accuracies between training on fully connected trees and learning from partial parses. 2 Related Work Learning from partial parses has been dealt in different ways in the literature. Hwa et al. (2005) used post-projection completion/transformation rules to get full parse trees from the projections and train Collin’s parser (Collins, 1999) on them. Ganchev et al. (2009) handle partial projected parses by avoiding committing to entire projected tree during training. The posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation. Jiang and Liu (2010) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. They deal with partial projections by breaking down the pro</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Nat. Lang. Eng., 11:311–325, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Dependency parsing and projection based on word-pair classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>12--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1324" citStr="Jiang and Liu, 2010" startWordPosition="194" endWordPosition="197"> parses by utilizing available structural and syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. 1 Introduction Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001), syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002). Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora. Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank for a target language. The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool. Due to the usage of automatic source parses, automatic word alignments and differences in the annotati</context>
<context position="5939" citStr="Jiang and Liu (2010)" startWordPosition="903" endWordPosition="906">cies between training on fully connected trees and learning from partial parses. 2 Related Work Learning from partial parses has been dealt in different ways in the literature. Hwa et al. (2005) used post-projection completion/transformation rules to get full parse trees from the projections and train Collin’s parser (Collins, 1999) on them. Ganchev et al. (2009) handle partial projected parses by avoiding committing to entire projected tree during training. The posterior regularization based framework constrains the projected syntactic relations to hold approximately and only in expectation. Jiang and Liu (2010) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. They deal with partial projections by breaking down the projected parse into a set of edges and training on the set of projected relations rather than on trees. While Hwa et al. (2005) requires full projected parses to train their parser, Ganchev et al. (2009) and Jiang and Liu (2010) can learn from partially projected trees. However, the discriminative training in (Ganchev et al., 2009) doesn’t allow for richer syntactic context and it doesn’t learn from all the relations in the </context>
<context position="7317" citStr="Jiang and Liu, 2010" startWordPosition="1131" endWordPosition="1134">10) sacrifice the context of the relations such as global structural context, neighboring relations that are crucial for dependency analysis. Due to this, they report that the parser suffers from local optimization during training. The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al., 2009) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work. 3 Partial Parsing A standard dependency graph satisfies four graph constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). In our work, we assume the dependency graph for a sentence only satisfies the single1598 a) hill on build PastPart. cottage very beau</context>
</contexts>
<marker>Jiang, Liu, 2010</marker>
<rawString>Wenbin Jiang and Qun Liu. 2010. Dependency parsing and projection based on word-pair classification. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 12–20, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit,</booktitle>
<volume>5</volume>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1361" citStr="Koehn, 2005" startWordPosition="201" endWordPosition="202">syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. 1 Introduction Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001), syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002). Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora. Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank for a target language. The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool. Due to the usage of automatic source parses, automatic word alignments and differences in the annotation schemes of source and target langu</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly non-projective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>507--514</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3002" citStr="Kuhlmann and Nivre, 2006" startWordPosition="456" endWordPosition="459">l dependency parse projected using an automatic source parser on aligned text. This parse is not fully connected with the words banaa, kottaige and dikhataa left without any parents. The cottage built on the hill looks very beautiful pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 1: Word alignment with dependency parses for an English-Hindi parallel sentence To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003), the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009). But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In 1597 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597–1606, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Spanish and Bulgarian projected data extracted by G</context>
<context position="7782" citStr="Kuhlmann and Nivre, 2006" startWordPosition="1199" endWordPosition="1202">ected dependency data used in (Ganchev et al., 2009) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work. 3 Partial Parsing A standard dependency graph satisfies four graph constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). In our work, we assume the dependency graph for a sentence only satisfies the single1598 a) hill on build PastPart. cottage very beautiful look Be.Pres. b) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai pahaada para banaa huaa kottaige bahuta sundara dikhataa hai c) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai d) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai e) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai f) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai g) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai </context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-projective dependency structures. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 507–514, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3070" citStr="McDonald et al., 2005" startWordPosition="467" endWordPosition="470"> text. This parse is not fully connected with the words banaa, kottaige and dikhataa left without any parents. The cottage built on the hill looks very beautiful pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 1: Word alignment with dependency parses for an English-Hindi parallel sentence To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003), the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009). But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In 1597 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597–1606, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Spanish and Bulgarian projected data extracted by Ganchev et al. (2009), the figures are 3.2% and 12.9% respectively. L</context>
<context position="7466" citStr="McDonald et al., 2005" startWordPosition="1155" endWordPosition="1158"> this, they report that the parser suffers from local optimization during training. The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al., 2009) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work. 3 Partial Parsing A standard dependency graph satisfies four graph constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). In our work, we assume the dependency graph for a sentence only satisfies the single1598 a) hill on build PastPart. cottage very beautiful look Be.Pres. b) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai pahaada para banaa huaa kottaige bahuta sundara dikhataa hai c) p</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
</authors>
<title>Malteval: an evaluation and visualization tool for dependency parsing.</title>
<date>2008</date>
<journal>European Language Resources Association</journal>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<marker>Nilsson, Nivre, 2008</marker>
<rawString>Jens Nilsson and Joakim Nivre. 2008. Malteval: an evaluation and visualization tool for dependency parsing. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http://www.lrecconf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan Mcdonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<marker>Nivre, Hall, K¨ubler, Mcdonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mcdonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An Efficient Algorithm for Projective Dependency Parsing.</title>
<date>2003</date>
<booktitle>In Eighth International Workshop on Parsing Technologies,</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="2858" citStr="Nivre, 2003" startWordPosition="441" endWordPosition="442">llel sentence with correct source parse, alignments and target dependency parse. For the same sentence, Figure 2 is a sample partial dependency parse projected using an automatic source parser on aligned text. This parse is not fully connected with the words banaa, kottaige and dikhataa left without any parents. The cottage built on the hill looks very beautiful pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 1: Word alignment with dependency parses for an English-Hindi parallel sentence To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003), the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009). But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In 1597 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 159</context>
<context position="7507" citStr="Nivre, 2003" startWordPosition="1163" endWordPosition="1164">al optimization during training. The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al., 2009) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work. 3 Partial Parsing A standard dependency graph satisfies four graph constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). In our work, we assume the dependency graph for a sentence only satisfies the single1598 a) hill on build PastPart. cottage very beautiful look Be.Pres. b) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai pahaada para banaa huaa kottaige bahuta sundara dikhataa hai c) pahaada para banaa huaa kottaige bahuta su</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An Efficient Algorithm for Projective Dependency Parsing. In Eighth International Workshop on Parsing Technologies, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-projective dependency parsing in expected linear time.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>351--359</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="3084" citStr="Nivre, 2009" startWordPosition="471" endWordPosition="472">t fully connected with the words banaa, kottaige and dikhataa left without any parents. The cottage built on the hill looks very beautiful pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 1: Word alignment with dependency parses for an English-Hindi parallel sentence To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003), the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009). But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In 1597 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597–1606, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Spanish and Bulgarian projected data extracted by Ganchev et al. (2009), the figures are 3.2% and 12.9% respectively. Learning from d</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 351–359, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Gali</author>
</authors>
<title>Part-Of-Speech Tagging and Chunking using Conditional Random Fields and Transformation-Based Learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the IJCAI and the Workshop On Shallow Parsing for South Asian Languages (SPSAL),</booktitle>
<pages>21--24</pages>
<marker>Gali, 2007</marker>
<rawString>Avinesh PVS. and Karthik Gali. 2007. Part-Of-Speech Tagging and Chunking using Conditional Random Fields and Transformation-Based Learning. In Proceedings of the IJCAI and the Workshop On Shallow Parsing for South Asian Languages (SPSAL), pages 21–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>616--623</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5101" citStr="Reichart and Rappoport, 2007" startWordPosition="778" endWordPosition="781">nd choosing the best one at each step without following either left-to-right or right-to-left traversal. The algorithm is inspired by earlier nondirectional parsing works of Shen and Joshi (2008) and Goldberg and Elhadad (2010). We also propose an extended partial parsing algorithm that can learn from partial parses whose yields are partially contiguous. Apart from bitext projections, this work can be extended to other cases where learning from partial structures is required. For example, while bootstrapping parsers high confidence parses are extracted and trained upon (Steedman et al., 2003; Reichart and Rappoport, 2007). In cases where these parses are few, learning from partial parses might be beneficial. We train our parser on projected Hindi, Bulgarian and Spanish treebanks and show statistically significant improvements in accuracies between training on fully connected trees and learning from partial parses. 2 Related Work Learning from partial parses has been dealt in different ways in the literature. Hwa et al. (2005) used post-projection completion/transformation rules to get full parse trees from the projections and train Collin’s parser (Collins, 1999) on them. Ganchev et al. (2009) handle partial p</context>
</contexts>
<marker>Reichart, Rappoport, 2007</marker>
<rawString>Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616–623, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind Joshi</author>
</authors>
<title>LTAG dependency parsing with bidirectional incremental construction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>495--504</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="4667" citStr="Shen and Joshi (2008)" startWordPosition="712" endWordPosition="715">Be.Pres. pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 2: A sample dependency parse with partial parses In this paper, we present a dependency parsing algorithm which can train on partial projected parses and can take rich syntactic information as features for learning. The parsing algorithm constructs the partial parses in a bottom-up manner by performing a greedy search over all possible relations and choosing the best one at each step without following either left-to-right or right-to-left traversal. The algorithm is inspired by earlier nondirectional parsing works of Shen and Joshi (2008) and Goldberg and Elhadad (2010). We also propose an extended partial parsing algorithm that can learn from partial parses whose yields are partially contiguous. Apart from bitext projections, this work can be extended to other cases where learning from partial structures is required. For example, while bootstrapping parsers high confidence parses are extracted and trained upon (Steedman et al., 2003; Reichart and Rappoport, 2007). In cases where these parses are few, learning from partial parses might be beneficial. We train our parser on projected Hindi, Bulgarian and Spanish treebanks and s</context>
</contexts>
<marker>Shen, Joshi, 2008</marker>
<rawString>Libin Shen and Aravind Joshi. 2008. LTAG dependency parsing with bidirectional incremental construction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>G Satta</author>
<author>A Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>L. Shen, G. Satta, and A. Joshi. 2007. Guided learning for bidirectional sequence classification. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mark Steedman</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Stephen Clark</author>
<author>Rebecca Hwa</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume 1, EACL ’03,</booktitle>
<pages>331--338</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5070" citStr="Steedman et al., 2003" startWordPosition="774" endWordPosition="777">ll possible relations and choosing the best one at each step without following either left-to-right or right-to-left traversal. The algorithm is inspired by earlier nondirectional parsing works of Shen and Joshi (2008) and Goldberg and Elhadad (2010). We also propose an extended partial parsing algorithm that can learn from partial parses whose yields are partially contiguous. Apart from bitext projections, this work can be extended to other cases where learning from partial structures is required. For example, while bootstrapping parsers high confidence parses are extracted and trained upon (Steedman et al., 2003; Reichart and Rappoport, 2007). In cases where these parses are few, learning from partial parses might be beneficial. We train our parser on projected Hindi, Bulgarian and Spanish treebanks and show statistically significant improvements in accuracies between training on fully connected trees and learning from partial parses. 2 Related Work Learning from partial parses has been dealt in different ways in the literature. Hwa et al. (2005) used post-projection completion/transformation rules to get full parse trees from the projections and train Collin’s parser (Collins, 1999) on them. Ganchev</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume 1, EACL ’03, pages 331– 338, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jrg Tiedemann</author>
</authors>
<title>MatsLex - a multilingual lexical database for machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC’2002),</booktitle>
<volume>volume VI,</volume>
<pages>1909--1912</pages>
<contexts>
<context position="1379" citStr="Tiedemann, 2002" startWordPosition="203" endWordPosition="204">ormation in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. 1 Introduction Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001), syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002). Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora. Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank for a target language. The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool. Due to the usage of automatic source parses, automatic word alignments and differences in the annotation schemes of source and target languages, the projecte</context>
</contexts>
<marker>Tiedemann, 2002</marker>
<rawString>Jrg Tiedemann. 2002. MatsLex - a multilingual lexical database for machine translation. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC’2002), volume VI, pages 1909–1912, Las Palmas de Gran Canaria, Spain, 29-31 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
</authors>
<title>Nlp tools contest - 2008: Summary.</title>
<date>2008</date>
<journal>NLP Tools Contest.</journal>
<booktitle>In Proceedings of ICON</booktitle>
<marker>Venkatapathy, 2008</marker>
<rawString>Sriram Venkatapathy. 2008. Nlp tools contest - 2008: Summary. In Proceedings of ICON 2008 NLP Tools Contest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines. In</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="2830" citStr="Yamada and Matsumoto, 2003" startWordPosition="434" endWordPosition="438">rees. Figure 1 shows an English-Hindi parallel sentence with correct source parse, alignments and target dependency parse. For the same sentence, Figure 2 is a sample partial dependency parse projected using an automatic source parser on aligned text. This parse is not fully connected with the words banaa, kottaige and dikhataa left without any parents. The cottage built on the hill looks very beautiful pahaada para banaa huaa kottaige bahuta sundara dikhataa hai Figure 1: Word alignment with dependency parses for an English-Hindi parallel sentence To train the traditional dependency parsers (Yamada and Matsumoto, 2003; Eisner, 1996; Nivre, 2003), the dependency parse has to satisfy four constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). Projectivity can be relaxed in some parsers (McDonald et al., 2005; Nivre, 2009). But these parsers can not directly be used to learn from partially connected parses (Hwa et al., 2005; Ganchev et al., 2009). In the projected Hindi treebank (section 4) that was extracted from English-Hindi parallel text, only 5.9% of the sentences had full trees. In 1597 Proceedings of the 49th Annual Meeting of the Association for Computat</context>
<context position="7494" citStr="Yamada and Matsumoto, 2003" startWordPosition="1159" endWordPosition="1162"> the parser suffers from local optimization during training. The parser proposed in this work (section 3) learns from partial trees by using the available structural information in it and also in neighboring partial parses. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al., 2009) for comparison. The same could not be carried out for Chinese (which was the language (Jiang and Liu, 2010) worked on) due to the unavailability of projected data used in their work. Comparison with the traditional dependency parsers (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre, 2003; Goldberg and Elhadad, 2010) which train on complete dependency parsers is out of the scope of this work. 3 Partial Parsing A standard dependency graph satisfies four graph constraints: connectedness, single-headedness, acyclicity and projectivity (Kuhlmann and Nivre, 2006). In our work, we assume the dependency graph for a sentence only satisfies the single1598 a) hill on build PastPart. cottage very beautiful look Be.Pres. b) pahaada para banaa huaa kottaige bahuta sundara dikhataa hai pahaada para banaa huaa kottaige bahuta sundara dikhataa hai c) pahaada para banaa huaa kotta</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In In Proceedings of IWPT, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the first international conference on Human language technology research, HLT ’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1243" citStr="Yarowsky et al., 2001" startWordPosition="179" endWordPosition="182">ing algorithm which doesn’t need a fully connected parse and can learn from partial parses by utilizing available structural and syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi. 1 Introduction Parallel corpora have been used to transfer information from source to target languages for Part-Of-Speech (POS) tagging, word sense disambiguation (Yarowsky et al., 2001), syntactic parsing (Hwa et al., 2005; Ganchev et al., 2009; Jiang and Liu, 2010) and machine translation (Koehn, 2005; Tiedemann, 2002). Analysis on the source sentences was induced onto the target sentence via projections across word aligned parallel corpora. Equipped with a source language parser and a word alignment tool, parallel data can be used to build an automatic treebank for a target language. The parse trees given by the parser on the source sentences in the parallel data are projected onto the target sentence using the word alignments from the alignment tool. Due to the usage of a</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the first international conference on Human language technology research, HLT ’01, pages 1–8, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>