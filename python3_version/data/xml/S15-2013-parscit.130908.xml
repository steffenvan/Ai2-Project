<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006207">
<title confidence="0.987646">
CDTDS: Predicting Paraphrases in Twitter via Support Vector Regression
</title>
<author confidence="0.996298">
Rafael Michael Karampatsis
</author>
<affiliation confidence="0.998455">
Department of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.989544">
10 Crichton Street, EH8 9AB Edinburgh, United Kingdom
</address>
<email confidence="0.997503">
mpatsis13@gmail.com
</email>
<sectionHeader confidence="0.99582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9921209">
In this paper we describe a system that rec-
ognizes paraphrases in Twitter for tweets that
refer to the same topic. The system partici-
pated in Task1 of SEMEVAL-2015 and uses a
support vector regression machine to predict
the degree of similarity. The similarity is then
thresholded to create a binary prediction. The
model and experimental results are discussed
along with future work that could improve the
method.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980280952381">
Recently, Twitter has gained significant popularity
among the social network services. Lots of users of-
ten use Twitter to express feelings or opinions about
a variety of subjects. Analysing this kind of content
can lead to useful information for fields such as per-
sonalized marketing or social profiling. However,
such a task is not trivial, because the language used
on Twitter is often informal, presenting new chal-
lenges to text analysis.
Task1 of SEMEVAL-2015 (Xu et al., 2015) fo-
cuses on recognition of paraphrases and semantic
similarity in Twitter i.e., recognizing if two tweets
are alternative linguistic expressions of the same, or
similar, meaning (Bhagat and Hovy, 2013). The task
is based on a crowdsourced corpus of 18000 pairs
of paraphrases and non-paraphrased sentences from
Twitter (Xu et al., 2014) and each pair consists of
two tweets from the same topic. A label is provided
with each pair, which is the number of yes votes
from 5 crowdsourced annotators when asked if the
second tweet is a paraphrase of the first one.
</bodyText>
<figure confidence="0.8321478">
Paraphrase Example:
Roberto Mancini gets the boot from Man City
Non-Paraphrase Example:
WORLD OF JENKS IS ON AT 11
World of Jenks is my favorite show on tv
</figure>
<figureCaption confidence="0.9782675">
Figure 1: Examples of both a paraphrase and a non-
paraphrase pair of the data.
</figureCaption>
<bodyText confidence="0.999979727272727">
The method utilizes a support vector regression
machine (SVR). The regression model tries to pre-
dict the degree of semantic similarity between two
tweets, by assuming that it can be represented by the
probability that random human annotators would an-
notate the pair as a paraphrase. The predicted value
is transformed into a binary decision via a threshold.
Section 2 describes the data provided by the or-
ganizers. Sections 4 and 5 present the system and
its performance respectively. Finally, Section 6 pro-
vides ideas for future work and Section 7 concludes.
</bodyText>
<sectionHeader confidence="0.988849" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.998101222222222">
The objective of this task is to predict whether two
sentences from Twitter sharing the same topic, im-
ply the same or very similar meaning and optionally
a degree score between 0 and 1. In Figure 1, a para-
phrase and a non-paraphrase example taken from the
task website are illustrated.
The organizers released a training (Train) and a
development set (Dev), both labeled and they also
provided a test set (Test) for the task. To collect
</bodyText>
<footnote confidence="0.8240905">
Roberto Mancini has been sacked by Manchester
City with the Blues saying
</footnote>
<page confidence="0.994307">
75
</page>
<note confidence="0.6879905">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 75–79,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.9995115">
Set Size Paraphrase Non-Paraphrase Debatable
Train 13693 3996 7534 1533
Dev 724 948 2672 585
Test 972 175 663 134
</table>
<tableCaption confidence="0.826749">
Table 1: Class distribution of the train, development
and test sets.
</tableCaption>
<bodyText confidence="0.99730675">
the data they used Twitter’s public API1 to crawl
trending topics and their associated tweets (Xu et
al., 2014). Annotation of the collected tweets was
performed via crowdsourcing (Amazon Mechanical
Turk). From each topic, 11 random tweets were se-
lected and 5 different annotators were used. One of
the 11 tweets was randomly selected as the original
sentence. The annotators were asked to select which
of the remaining 10 tweets have the same meaning
as the original one. Each topic’s pairs are annotated
with the number of annotators that voted for them.
Problematic annotators were removed by checking
their Cohen’s Kappa agreement (Artstein and Poe-
sio, 2008) with other annotators. Agreement with
an expert annotator on 972 sentence pairs (test set)
was also measured and the Pearson correlation coef-
ficient was 0.735 although the expert annotator had
actually used a different scale for the annotation.
Both Train and Dev were collected from the same
time period while Test was collected from a differ-
ent time period.
Table 1 illustrates the class distribution of the
data. The task organizers have stated that when a
pair has either 1 or 0 votes it should be considered
a non-paraphrase, while pairs that have 3, 4, and 5
votes should be considered as paraphrases. Pairs that
have exactly 2 votes are assumed debatable and the
organizers suggest that they should be discarded. We
can observe that all the data sets have a very similar
distribution and that the majority class is in all cases
the non-paraphrase one with about 60% of the data
(debatable instances included).
</bodyText>
<sectionHeader confidence="0.99838" genericHeader="method">
3 Previous Work
</sectionHeader>
<bodyText confidence="0.99773225">
Measuring semantic text similarity has been a re-
search subject in natural language processing, infor-
mation retrieval and artificial intelligence for many
years. Most works have focused on the document
</bodyText>
<footnote confidence="0.845587">
1https://dev.twitter.com/docs/api/1.1/overview
</footnote>
<bodyText confidence="0.99999127027027">
level (i.e., comparing two long texts or comparing
a small text with a long one). Recently, there has
been growing interest at the sentence level, specifi-
cally on computing the similarity of two sentences.
The most related task to computing tweets similarity
is the computation of sentence similarity.
According to (Han et al., 2012), there are three
main approaches for sentence similarity. The first
is based on a vector space model (Meadow, 1992)
that models the text as a “bag of words” and rep-
resents it using a vector, and the similarity between
the two texts is computed as the cosine similarity
of their vectors. The second approach relies on the
assumption that the words or expressions of two se-
mantically equivalent sentences should be able to be
aligned. The quality of this alignment can then be
used as a similarity measure. When this approach
is utilized, words from the two sentences are paired
(Mihalcea et al., 2006) by maximizing the summa-
tion of the word similarity of the resulting pairs. Fi-
nally, the third and final approach utilizes machine
learning and combines different measures and fea-
tures (such as lexical, semantic and syntactic fea-
tures) which are supplied to a classifier that learns
a model on the training data.
The unique characteristics of Twitter present new
challenges and opportunities for paraphrase research
(Xu et al., 2013; Ling et al., 2013). Most of the
work has focused on paraphrase generation (Xu et
al., 2013; Ling et al., 2013) in order to use it for text
normalization. However, the task organizers (Xu et
al., 2014) created a dataset, implemented a system
and reimplemented several baselines and state-of-
the-art systems for sentence paraphrase recognition.
They showed that their method, which combines a
previous system with latent space achieves at least
as good results as state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.978569" genericHeader="method">
4 System Overview
</sectionHeader>
<bodyText confidence="0.999966625">
The main objective of the implemented system is to
classify pairs of tweets from the same topic as se-
mantically similar or not. The approach used differs
from previous works because it models the problem
as a regression task first and then as a classification
task, while typical approaches treat the problem as
a classification task (usually binary since debatable
pairs are discarded). The main inspiration for this
</bodyText>
<page confidence="0.983881">
76
</page>
<tableCaption confidence="0.926483">
Table 2: Mapping of the number of positive votes
</tableCaption>
<bodyText confidence="0.986119636363637">
from the annotators to real valued labels.
approach comes from the observation that for ex-
ample, pairs that got voted from 3 of the annota-
tors will not be as similar as pairs that got voted
from 5. Treating these instances in the same way
is very likely to confuse the model. The regression
approach is a possible way to avoid this effect since
instances with different number of votes will not just
use different values but will have a relation between
their values. For example, instances that got 5 votes
will use a better score as their label than instances
that got 4 or 3.
To extract this relation from Train data, the ra-
tio of positive votes against the total number of an-
notators (5) for each pair was used to create the la-
bels. The debatable instances correspond to exactly
2 votes from the human annotators, which maps to
0.4. These instances were discarded as the organiz-
ers suggested. This resulted in the use of the values
shown in Table 2 as labels.
An SVR with a linear kernel function2 was used
to predict the degree of similarity between the tweet
pairs. For each training instance (i.e. a tweet pair)
a feature vector is supplied to the regression model3
along with the corresponding label. The output of
the SVR can be used for classification by using
a threshold. 0.35 was chosen as the classification
threshold as it belongs to the debatable space and it
is slightly less than 0.4, in order to increase the re-
call of the minority class (Paraphrases). However,
the threshold could be tuned using cross validation
on the training data or by testing on the development
set for better results.
</bodyText>
<subsectionHeader confidence="0.993336">
4.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.98661075">
Preprocessing can greatly affect the performance of
a system. The tweets were passed through a Twitter
specific tokenizer and part-of-speech (POS) tagger
(Ritter et al., 2011) by the organizers. We converted
</bodyText>
<footnote confidence="0.969237">
2The LIBLINEAR distribution (Fan et al., 2008)
3The regression model uses L2-regularized regression with
the default parameter C=1.
</footnote>
<bodyText confidence="0.999563666666667">
all the tweets to lower case and stopwords were re-
moved using the NLTK (Loper and Bird, 2002) stop-
words list. Moreover, we removed the tokens of the
topic since they always exist in both tweets. Finally,
we applied stemming to each one of the remaining
tokens and the stemmed representations are stored.
</bodyText>
<subsectionHeader confidence="0.994938">
4.2 Feature Engineering
</subsectionHeader>
<bodyText confidence="0.999671">
In this section the features used in the model will
be described in detail. We made two submissions.
Both share the same features except for the senti-
ment matching feature.
</bodyText>
<subsectionHeader confidence="0.801534">
4.2.1 Lexical Overlap Features
</subsectionHeader>
<bodyText confidence="0.999980416666667">
A very popular and competitive baseline is to
use lexical overlap features (Das and Smith, 2009).
These features use the unigrams, bigrams and tri-
grams of the sentences, both with and without stem-
ming. The cardinality of the intersection of the n-
grams between each pair of tweets as a proportion
of the length of each tweet is used as a feature. The
harmonic mean of these two values is also calculated
and used as a feature. These three types of features
for each n-gram size were named precision, recall
and F1 (harmonic mean of precision and recall) by
Das and Smith (2009).
</bodyText>
<subsubsectionHeader confidence="0.732904">
4.2.2 Ratio of the Tweets Length in Tokens
</subsubsectionHeader>
<bodyText confidence="0.9999528">
The ratio of the length of the shortest tweet in
the pair divided by the length of the longer tweet is
used as a feature. This feature is used because if the
tweets differ a lot in length then they will probably
not have similar meaning.
</bodyText>
<subsectionHeader confidence="0.966258">
4.2.3 Overlap of POS Tags
</subsectionHeader>
<bodyText confidence="0.9999764">
Similar to the lexical overlap features the overlap
of POS tags of unigrams, bigrams and trigrams is
checked and a total of 9 features is created. For ex-
ample the tweet “Wow EJ Manuel” contains the fol-
lowing two POS bigrams: UH NNP and NNP NNP.
</bodyText>
<subsectionHeader confidence="0.912953">
4.2.4 Overlap of Named Entities Tags (NE)
</subsectionHeader>
<bodyText confidence="0.998971">
Similar to the lexical overlap features the overlap
of NE is checked and three features are created.
</bodyText>
<subsectionHeader confidence="0.731833">
4.2.5 GloVe Word Vectors Similarity
</subsectionHeader>
<bodyText confidence="0.998222333333333">
Vector space representations of words have suc-
ceeded in capturing semantic and syntactic regulari-
ties using vector arithmetic (Pennington et al., 2014;
</bodyText>
<figure confidence="0.996808375">
Number of Votes
Label Value
1 2 3 4
0.2 0.4 0.6 0.8
0
0
5
1
</figure>
<page confidence="0.993091">
77
</page>
<bodyText confidence="0.99763319047619">
Mikolov et al., 2013). The word vectors from GloVe
(Pennington et al., 2014) were used to calculate the
semantic similarity between tokens of the two sen-
tences by measuring their cosine similarity. The
word vectors utilized were created from a corpus of
2B tweets which contains 27B tokens. Experiments
on the development set were also done with vectors
from Wikipedia2014 + Gigawords5 (about 6B to-
kens) but were not used for submission since they
performed worse than the Twitter ones.
The calculation of these features is based on the
alignment algorithm described by (Han et al., 2013).
For each of the two tweets we iterate over its to-
kens. For each token the similarity to all the tokens
of the other tweet that exist in the model is calcu-
lated and the maximum is returned. When the algo-
rithm finishes, the maximum, minimum and average
values of the matched similarities for each tweet are
returned as features. This makes a total of 6 fea-
tures. An additional feature is calculated by finding
the similarity of the centroids of the two tweets.
</bodyText>
<subsectionHeader confidence="0.823948">
4.2.6 Sentiment Matching
</subsectionHeader>
<bodyText confidence="0.99998275">
A Twitter sentiment classifier was used to pre-
dict the sentiment of the tweets (Karampatsis et al.,
2014). The feature has a value of 1 if both tweets of
the pair have the same sentiment and 0 otherwise.
</bodyText>
<sectionHeader confidence="0.991242" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999954764705882">
Each system had to submit for each test set instance
its prediction (paraphrase or not) (subtask1) and op-
tionally a degree of semantic similarity (subtask2).
To evaluate system performance for subtask1 the
organizers used Fl against human judgements on
the predictions. While for subtask2 they used the
Pearson correlation of the predicted similarity scores
with the human scores. Our team was ranked 9th
on both subtasks4 and our systems were ranked 13th
and 14th on subtask1 and 15th and 16th on sub-
task2. Table 3 illustrates the results and rankings
of our systems and the baselines. The results in-
dicate that the sentiment feature decreases perfor-
mance and should be removed from our system.
We used the official evaluation script to assess the
performance of our systems on the test set for dif-
ferent threshold values. The results are illustrated in
</bodyText>
<page confidence="0.734382">
46 teams did not participate in subtask2
</page>
<figureCaption confidence="0.614982">
Figure 2. We used thresholds from 0 to 1 with a step
</figureCaption>
<bodyText confidence="0.855530166666667">
of 0.05 except for the space [0.3, 0.4] where we used
a step of 0.01. The two systems behave similarly and
the best performance (0.622) was achieved from the
All-Sentiment system using a threshold of 0.36.
Figure 2: F1 for subtask1 on the test set for our sys-
tems using different threshold values.
</bodyText>
<sectionHeader confidence="0.997896" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999973636363636">
A possible direction would be to use locality sen-
sitive hashing on the tweets (Petrovic et al., 2012)
to create more features. Moreover, ordinal regres-
sion could be used to train the model (Hardin and
Hilbe, 2001). The addition of a text normalization
algorithm in the preprocessing step could enhance
the performance of lexical overlap features and that
of other methods such as wordnet, LDA (Blei et al.,
2003) or LSA (Deerwester et al., 1990). Finally, the
overlap of character n-grams could also be used as
features.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999976818181818">
We described a system that predicts semantic simi-
larity between tweets from the same topic. The sys-
tem’s aim is to identify paraphrases of a tweet on a
specific topic, which is really useful in event recog-
nition systems. It employs a support vector regres-
sion to predict the probability that human annotators
would annotate a pair of tweets as a paraphrase. The
predicted value is then used for binary classification
by using a threshold. The system’s performance was
measured on SEMEVAL-2015 Task1 and it achieves
better results than the task baselines.
</bodyText>
<page confidence="0.993415">
78
</page>
<table confidence="0.999420142857143">
System F1 F1 Rank Precision Recall Pearson Pearson Rank maxF1 mPrecision mRecall
All Features 0.613 13/38 0.547 0.697 0.494 15/28 0.626 0.675 0.583
All-Sentiment 0.612 14/38 0.542 0.703 0.491 16/28 0.624 0.589 0.663
LR Baseline 0.589 21/38 0.679 0.520 0.511 11/28 0.601 0.674 0.543
WTMF Baseline 0.536 28/38 0.450 0.663 0.350 26/28 0.587 0.570 0.606
Random 0.266 38/38 0.192 0.434 0.350 28/28 0.350 0.215 0.949
Human Bound 0.823 - 0.752 0.908 0.017 - - - -
</table>
<tableCaption confidence="0.99983">
Table 3: Results of our systems, baselines and human annotators on the test set.
</tableCaption>
<sectionHeader confidence="0.995088" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996601125">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput. Lin-
guist., 34(4):555–596, December.
Rahul Bhagat and Eduard H. Hovy. 2013. What is a
paraphrase? Computational Linguistics, 39(3):463–
472.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In In Proc. of ACL-IJCNLP.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871–1874.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Auto-
matically constructing a normalisation dictionary for
microblogs. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2012), pages 421–432, Jeju Is-
land, Korea.
Lushan Han, Abhay Kashyap, Tim Finin, James May-
field, and Jonathan Weese. 2013. Umbc ebiquity-
core: Semantic textual similarity systems.
James W. Hardin and Joseph Hilbe. 2001. General-
ized Linear Models and Extensions. College Station,
Texas: Stata Press.
Rafael Michael Karampatsis, John Pavlopoulos, and Pro-
dromos Malakasiotis. 2014. Aueb: Two stage senti-
ment analysis of social network messages. In Proceed-
ings of the 8th International Workshop on Semantic
Evaluation (SemEval 2014), pages 114–118, Dublin,
Ireland, August. Association for Computational Lin-
guistics and Dublin City University.
Wang Ling, Chris Dyer, Alan W. Black, and Isabel Tran-
coso. 2013. Paraphrasing 4 microblog normalization.
In EMNLP, pages 73–84. ACL.
Edward Loper and Steven Bird. 2002. NLTK: The Nat-
ural Language Toolkit. ETMTNLP ’02, pages 63–70,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Charles T. Meadow. 1992. Text Information Retrieval
Systems. Academic Press, Inc.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In AAAI06, pages 775–
780.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013.
Linguistic regularities in continuous space word repre-
sentations. In HLT-NAACL, pages 746–751.
Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of EMNLP.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2012. Using paraphrases for improving first story de-
tection in news and twitter. In HLT-NAACL, pages
338–346. The Association for Computational Linguis-
tics.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In EMNLP, pages 1524–1534.
Wei Xu, Alan Ritter, and Ralph Grishman. 2013. Gath-
ering and generating paraphrases from twitter with ap-
plication to normalization.
Wei Xu, Alan Ritter, Chris Callison-Burch, William
Dolan, and Yangfeng Ji. 2014. Extracting lexically
divergent paraphrases from twitter. Transactions of
the Association for Computational Linguistics, 2:435–
448.
Wei Xu, Chris Callison-Burch, and William B. Dolan.
2015. SemEval-2015 Task 1: Paraphrase and semantic
similarity in Twitter (PIT). In Proceedings of the 9th
International Workshop on Semantic Evaluation (Se-
mEval).
</reference>
<page confidence="0.999021">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888886">
<title confidence="0.999602">CDTDS: Predicting Paraphrases in Twitter via Support Vector Regression</title>
<author confidence="0.999952">Rafael Michael</author>
<affiliation confidence="0.9984005">Department of University of</affiliation>
<address confidence="0.916565">10 Crichton Street, EH8 9AB Edinburgh, United</address>
<email confidence="0.988478">mpatsis13@gmail.com</email>
<abstract confidence="0.997606454545455">In this paper we describe a system that recognizes paraphrases in Twitter for tweets that refer to the same topic. The system particiin Task1 of and uses a support vector regression machine to predict the degree of similarity. The similarity is then thresholded to create a binary prediction. The model and experimental results are discussed along with future work that could improve the method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="4042" citStr="Artstein and Poesio, 2008" startWordPosition="653" endWordPosition="657">API1 to crawl trending topics and their associated tweets (Xu et al., 2014). Annotation of the collected tweets was performed via crowdsourcing (Amazon Mechanical Turk). From each topic, 11 random tweets were selected and 5 different annotators were used. One of the 11 tweets was randomly selected as the original sentence. The annotators were asked to select which of the remaining 10 tweets have the same meaning as the original one. Each topic’s pairs are annotated with the number of annotators that voted for them. Problematic annotators were removed by checking their Cohen’s Kappa agreement (Artstein and Poesio, 2008) with other annotators. Agreement with an expert annotator on 972 sentence pairs (test set) was also measured and the Pearson correlation coefficient was 0.735 although the expert annotator had actually used a different scale for the annotation. Both Train and Dev were collected from the same time period while Test was collected from a different time period. Table 1 illustrates the class distribution of the data. The task organizers have stated that when a pair has either 1 or 0 votes it should be considered a non-paraphrase, while pairs that have 3, 4, and 5 votes should be considered as para</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist., 34(4):555–596, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Eduard H Hovy</author>
</authors>
<title>What is a paraphrase?</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<pages>472</pages>
<contexts>
<context position="1343" citStr="Bhagat and Hovy, 2013" startWordPosition="202" endWordPosition="205">e social network services. Lots of users often use Twitter to express feelings or opinions about a variety of subjects. Analysing this kind of content can lead to useful information for fields such as personalized marketing or social profiling. However, such a task is not trivial, because the language used on Twitter is often informal, presenting new challenges to text analysis. Task1 of SEMEVAL-2015 (Xu et al., 2015) focuses on recognition of paraphrases and semantic similarity in Twitter i.e., recognizing if two tweets are alternative linguistic expressions of the same, or similar, meaning (Bhagat and Hovy, 2013). The task is based on a crowdsourced corpus of 18000 pairs of paraphrases and non-paraphrased sentences from Twitter (Xu et al., 2014) and each pair consists of two tweets from the same topic. A label is provided with each pair, which is the number of yes votes from 5 crowdsourced annotators when asked if the second tweet is a paraphrase of the first one. Paraphrase Example: Roberto Mancini gets the boot from Man City Non-Paraphrase Example: WORLD OF JENKS IS ON AT 11 World of Jenks is my favorite show on tv Figure 1: Examples of both a paraphrase and a nonparaphrase pair of the data. The met</context>
</contexts>
<marker>Bhagat, Hovy, 2013</marker>
<rawString>Rahul Bhagat and Eduard H. Hovy. 2013. What is a paraphrase? Computational Linguistics, 39(3):463– 472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="14571" citStr="Blei et al., 2003" startWordPosition="2444" endWordPosition="2447"> the best performance (0.622) was achieved from the All-Sentiment system using a threshold of 0.36. Figure 2: F1 for subtask1 on the test set for our systems using different threshold values. 6 Future Work A possible direction would be to use locality sensitive hashing on the tweets (Petrovic et al., 2012) to create more features. Moreover, ordinal regression could be used to train the model (Hardin and Hilbe, 2001). The addition of a text normalization algorithm in the preprocessing step could enhance the performance of lexical overlap features and that of other methods such as wordnet, LDA (Blei et al., 2003) or LSA (Deerwester et al., 1990). Finally, the overlap of character n-grams could also be used as features. 7 Conclusion We described a system that predicts semantic similarity between tweets from the same topic. The system’s aim is to identify paraphrases of a tweet on a specific topic, which is really useful in event recognition systems. It employs a support vector regression to predict the probability that human annotators would annotate a pair of tweets as a paraphrase. The predicted value is then used for binary classification by using a threshold. The system’s performance was measured o</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition. In</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="10169" citStr="Das and Smith, 2009" startWordPosition="1673" endWordPosition="1676">tweets to lower case and stopwords were removed using the NLTK (Loper and Bird, 2002) stopwords list. Moreover, we removed the tokens of the topic since they always exist in both tweets. Finally, we applied stemming to each one of the remaining tokens and the stemmed representations are stored. 4.2 Feature Engineering In this section the features used in the model will be described in detail. We made two submissions. Both share the same features except for the sentiment matching feature. 4.2.1 Lexical Overlap Features A very popular and competitive baseline is to use lexical overlap features (Das and Smith, 2009). These features use the unigrams, bigrams and trigrams of the sentences, both with and without stemming. The cardinality of the intersection of the ngrams between each pair of tweets as a proportion of the length of each tweet is used as a feature. The harmonic mean of these two values is also calculated and used as a feature. These three types of features for each n-gram size were named precision, recall and F1 (harmonic mean of precision and recall) by Das and Smith (2009). 4.2.2 Ratio of the Tweets Length in Tokens The ratio of the length of the shortest tweet in the pair divided by the le</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="14604" citStr="Deerwester et al., 1990" startWordPosition="2450" endWordPosition="2453">2) was achieved from the All-Sentiment system using a threshold of 0.36. Figure 2: F1 for subtask1 on the test set for our systems using different threshold values. 6 Future Work A possible direction would be to use locality sensitive hashing on the tweets (Petrovic et al., 2012) to create more features. Moreover, ordinal regression could be used to train the model (Hardin and Hilbe, 2001). The addition of a text normalization algorithm in the preprocessing step could enhance the performance of lexical overlap features and that of other methods such as wordnet, LDA (Blei et al., 2003) or LSA (Deerwester et al., 1990). Finally, the overlap of character n-grams could also be used as features. 7 Conclusion We described a system that predicts semantic similarity between tweets from the same topic. The system’s aim is to identify paraphrases of a tweet on a specific topic, which is really useful in event recognition systems. It employs a support vector regression to predict the probability that human annotators would annotate a pair of tweets as a paraphrase. The predicted value is then used for binary classification by using a threshold. The system’s performance was measured on SEMEVAL-2015 Task1 and it achie</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="9455" citStr="Fan et al., 2008" startWordPosition="1556" endWordPosition="1559">a threshold. 0.35 was chosen as the classification threshold as it belongs to the debatable space and it is slightly less than 0.4, in order to increase the recall of the minority class (Paraphrases). However, the threshold could be tuned using cross validation on the training data or by testing on the development set for better results. 4.1 Data Preprocessing Preprocessing can greatly affect the performance of a system. The tweets were passed through a Twitter specific tokenizer and part-of-speech (POS) tagger (Ritter et al., 2011) by the organizers. We converted 2The LIBLINEAR distribution (Fan et al., 2008) 3The regression model uses L2-regularized regression with the default parameter C=1. all the tweets to lower case and stopwords were removed using the NLTK (Loper and Bird, 2002) stopwords list. Moreover, we removed the tokens of the topic since they always exist in both tweets. Finally, we applied stemming to each one of the remaining tokens and the stemmed representations are stored. 4.2 Feature Engineering In this section the features used in the model will be described in detail. We made two submissions. Both share the same features except for the sentiment matching feature. 4.2.1 Lexical</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatically constructing a normalisation dictionary for microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012),</booktitle>
<pages>421--432</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5557" citStr="Han et al., 2012" startWordPosition="897" endWordPosition="900">nstances included). 3 Previous Work Measuring semantic text similarity has been a research subject in natural language processing, information retrieval and artificial intelligence for many years. Most works have focused on the document 1https://dev.twitter.com/docs/api/1.1/overview level (i.e., comparing two long texts or comparing a small text with a long one). Recently, there has been growing interest at the sentence level, specifically on computing the similarity of two sentences. The most related task to computing tweets similarity is the computation of sentence similarity. According to (Han et al., 2012), there are three main approaches for sentence similarity. The first is based on a vector space model (Meadow, 1992) that models the text as a “bag of words” and represents it using a vector, and the similarity between the two texts is computed as the cosine similarity of their vectors. The second approach relies on the assumption that the words or expressions of two semantically equivalent sentences should be able to be aligned. The quality of this alignment can then be used as a similarity measure. When this approach is utilized, words from the two sentences are paired (Mihalcea et al., 2006</context>
</contexts>
<marker>Han, Cook, Baldwin, 2012</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Automatically constructing a normalisation dictionary for microblogs. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012), pages 421–432, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Jonathan Weese</author>
</authors>
<title>Umbc ebiquitycore: Semantic textual similarity systems.</title>
<date>2013</date>
<contexts>
<context position="12185" citStr="Han et al., 2013" startWordPosition="2031" endWordPosition="2034">4 0.2 0.4 0.6 0.8 0 0 5 1 77 Mikolov et al., 2013). The word vectors from GloVe (Pennington et al., 2014) were used to calculate the semantic similarity between tokens of the two sentences by measuring their cosine similarity. The word vectors utilized were created from a corpus of 2B tweets which contains 27B tokens. Experiments on the development set were also done with vectors from Wikipedia2014 + Gigawords5 (about 6B tokens) but were not used for submission since they performed worse than the Twitter ones. The calculation of these features is based on the alignment algorithm described by (Han et al., 2013). For each of the two tweets we iterate over its tokens. For each token the similarity to all the tokens of the other tweet that exist in the model is calculated and the maximum is returned. When the algorithm finishes, the maximum, minimum and average values of the matched similarities for each tweet are returned as features. This makes a total of 6 features. An additional feature is calculated by finding the similarity of the centroids of the two tweets. 4.2.6 Sentiment Matching A Twitter sentiment classifier was used to predict the sentiment of the tweets (Karampatsis et al., 2014). The fea</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. Umbc ebiquitycore: Semantic textual similarity systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Hardin</author>
<author>Joseph Hilbe</author>
</authors>
<title>Generalized Linear Models and Extensions. College Station,</title>
<date>2001</date>
<publisher>Stata Press.</publisher>
<location>Texas:</location>
<contexts>
<context position="14372" citStr="Hardin and Hilbe, 2001" startWordPosition="2412" endWordPosition="2415"> in 46 teams did not participate in subtask2 Figure 2. We used thresholds from 0 to 1 with a step of 0.05 except for the space [0.3, 0.4] where we used a step of 0.01. The two systems behave similarly and the best performance (0.622) was achieved from the All-Sentiment system using a threshold of 0.36. Figure 2: F1 for subtask1 on the test set for our systems using different threshold values. 6 Future Work A possible direction would be to use locality sensitive hashing on the tweets (Petrovic et al., 2012) to create more features. Moreover, ordinal regression could be used to train the model (Hardin and Hilbe, 2001). The addition of a text normalization algorithm in the preprocessing step could enhance the performance of lexical overlap features and that of other methods such as wordnet, LDA (Blei et al., 2003) or LSA (Deerwester et al., 1990). Finally, the overlap of character n-grams could also be used as features. 7 Conclusion We described a system that predicts semantic similarity between tweets from the same topic. The system’s aim is to identify paraphrases of a tweet on a specific topic, which is really useful in event recognition systems. It employs a support vector regression to predict the prob</context>
</contexts>
<marker>Hardin, Hilbe, 2001</marker>
<rawString>James W. Hardin and Joseph Hilbe. 2001. Generalized Linear Models and Extensions. College Station, Texas: Stata Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Michael Karampatsis</author>
<author>John Pavlopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>Aueb: Two stage sentiment analysis of social network messages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>114--118</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="12776" citStr="Karampatsis et al., 2014" startWordPosition="2136" endWordPosition="2139">hm described by (Han et al., 2013). For each of the two tweets we iterate over its tokens. For each token the similarity to all the tokens of the other tweet that exist in the model is calculated and the maximum is returned. When the algorithm finishes, the maximum, minimum and average values of the matched similarities for each tweet are returned as features. This makes a total of 6 features. An additional feature is calculated by finding the similarity of the centroids of the two tweets. 4.2.6 Sentiment Matching A Twitter sentiment classifier was used to predict the sentiment of the tweets (Karampatsis et al., 2014). The feature has a value of 1 if both tweets of the pair have the same sentiment and 0 otherwise. 5 Experimental Results Each system had to submit for each test set instance its prediction (paraphrase or not) (subtask1) and optionally a degree of semantic similarity (subtask2). To evaluate system performance for subtask1 the organizers used Fl against human judgements on the predictions. While for subtask2 they used the Pearson correlation of the predicted similarity scores with the human scores. Our team was ranked 9th on both subtasks4 and our systems were ranked 13th and 14th on subtask1 a</context>
</contexts>
<marker>Karampatsis, Pavlopoulos, Malakasiotis, 2014</marker>
<rawString>Rafael Michael Karampatsis, John Pavlopoulos, and Prodromos Malakasiotis. 2014. Aueb: Two stage sentiment analysis of social network messages. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 114–118, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan W Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Paraphrasing 4 microblog normalization.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>73--84</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6610" citStr="Ling et al., 2013" startWordPosition="1074" endWordPosition="1077">he quality of this alignment can then be used as a similarity measure. When this approach is utilized, words from the two sentences are paired (Mihalcea et al., 2006) by maximizing the summation of the word similarity of the resulting pairs. Finally, the third and final approach utilizes machine learning and combines different measures and features (such as lexical, semantic and syntactic features) which are supplied to a classifier that learns a model on the training data. The unique characteristics of Twitter present new challenges and opportunities for paraphrase research (Xu et al., 2013; Ling et al., 2013). Most of the work has focused on paraphrase generation (Xu et al., 2013; Ling et al., 2013) in order to use it for text normalization. However, the task organizers (Xu et al., 2014) created a dataset, implemented a system and reimplemented several baselines and state-ofthe-art systems for sentence paraphrase recognition. They showed that their method, which combines a previous system with latent space achieves at least as good results as state-of-the-art systems. 4 System Overview The main objective of the implemented system is to classify pairs of tweets from the same topic as semantically s</context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang Ling, Chris Dyer, Alan W. Black, and Isabel Trancoso. 2013. Paraphrasing 4 microblog normalization. In EMNLP, pages 73–84. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: The Natural Language Toolkit.</title>
<date>2002</date>
<journal>ETMTNLP</journal>
<volume>02</volume>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9634" citStr="Loper and Bird, 2002" startWordPosition="1585" endWordPosition="1588">rity class (Paraphrases). However, the threshold could be tuned using cross validation on the training data or by testing on the development set for better results. 4.1 Data Preprocessing Preprocessing can greatly affect the performance of a system. The tweets were passed through a Twitter specific tokenizer and part-of-speech (POS) tagger (Ritter et al., 2011) by the organizers. We converted 2The LIBLINEAR distribution (Fan et al., 2008) 3The regression model uses L2-regularized regression with the default parameter C=1. all the tweets to lower case and stopwords were removed using the NLTK (Loper and Bird, 2002) stopwords list. Moreover, we removed the tokens of the topic since they always exist in both tweets. Finally, we applied stemming to each one of the remaining tokens and the stemmed representations are stored. 4.2 Feature Engineering In this section the features used in the model will be described in detail. We made two submissions. Both share the same features except for the sentiment matching feature. 4.2.1 Lexical Overlap Features A very popular and competitive baseline is to use lexical overlap features (Das and Smith, 2009). These features use the unigrams, bigrams and trigrams of the se</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit. ETMTNLP ’02, pages 63–70, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles T Meadow</author>
</authors>
<title>Text Information Retrieval Systems.</title>
<date>1992</date>
<publisher>Academic Press, Inc.</publisher>
<contexts>
<context position="5673" citStr="Meadow, 1992" startWordPosition="918" endWordPosition="919">rocessing, information retrieval and artificial intelligence for many years. Most works have focused on the document 1https://dev.twitter.com/docs/api/1.1/overview level (i.e., comparing two long texts or comparing a small text with a long one). Recently, there has been growing interest at the sentence level, specifically on computing the similarity of two sentences. The most related task to computing tweets similarity is the computation of sentence similarity. According to (Han et al., 2012), there are three main approaches for sentence similarity. The first is based on a vector space model (Meadow, 1992) that models the text as a “bag of words” and represents it using a vector, and the similarity between the two texts is computed as the cosine similarity of their vectors. The second approach relies on the assumption that the words or expressions of two semantically equivalent sentences should be able to be aligned. The quality of this alignment can then be used as a similarity measure. When this approach is utilized, words from the two sentences are paired (Mihalcea et al., 2006) by maximizing the summation of the word similarity of the resulting pairs. Finally, the third and final approach u</context>
</contexts>
<marker>Meadow, 1992</marker>
<rawString>Charles T. Meadow. 1992. Text Information Retrieval Systems. Academic Press, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI06,</booktitle>
<pages>775--780</pages>
<contexts>
<context position="6158" citStr="Mihalcea et al., 2006" startWordPosition="1001" endWordPosition="1004">to (Han et al., 2012), there are three main approaches for sentence similarity. The first is based on a vector space model (Meadow, 1992) that models the text as a “bag of words” and represents it using a vector, and the similarity between the two texts is computed as the cosine similarity of their vectors. The second approach relies on the assumption that the words or expressions of two semantically equivalent sentences should be able to be aligned. The quality of this alignment can then be used as a similarity measure. When this approach is utilized, words from the two sentences are paired (Mihalcea et al., 2006) by maximizing the summation of the word similarity of the resulting pairs. Finally, the third and final approach utilizes machine learning and combines different measures and features (such as lexical, semantic and syntactic features) which are supplied to a classifier that learns a model on the training data. The unique characteristics of Twitter present new challenges and opportunities for paraphrase research (Xu et al., 2013; Ling et al., 2013). Most of the work has focused on paraphrase generation (Xu et al., 2013; Ling et al., 2013) in order to use it for text normalization. However, the</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI06, pages 775– 780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="11618" citStr="Mikolov et al., 2013" startWordPosition="1938" endWordPosition="1941"> the overlap of POS tags of unigrams, bigrams and trigrams is checked and a total of 9 features is created. For example the tweet “Wow EJ Manuel” contains the following two POS bigrams: UH NNP and NNP NNP. 4.2.4 Overlap of Named Entities Tags (NE) Similar to the lexical overlap features the overlap of NE is checked and three features are created. 4.2.5 GloVe Word Vectors Similarity Vector space representations of words have succeeded in capturing semantic and syntactic regularities using vector arithmetic (Pennington et al., 2014; Number of Votes Label Value 1 2 3 4 0.2 0.4 0.6 0.8 0 0 5 1 77 Mikolov et al., 2013). The word vectors from GloVe (Pennington et al., 2014) were used to calculate the semantic similarity between tokens of the two sentences by measuring their cosine similarity. The word vectors utilized were created from a corpus of 2B tweets which contains 27B tokens. Experiments on the development set were also done with vectors from Wikipedia2014 + Gigawords5 (about 6B tokens) but were not used for submission since they performed worse than the Twitter ones. The calculation of these features is based on the alignment algorithm described by (Han et al., 2013). For each of the two tweets we i</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="11532" citStr="Pennington et al., 2014" startWordPosition="1916" endWordPosition="1919"> have similar meaning. 4.2.3 Overlap of POS Tags Similar to the lexical overlap features the overlap of POS tags of unigrams, bigrams and trigrams is checked and a total of 9 features is created. For example the tweet “Wow EJ Manuel” contains the following two POS bigrams: UH NNP and NNP NNP. 4.2.4 Overlap of Named Entities Tags (NE) Similar to the lexical overlap features the overlap of NE is checked and three features are created. 4.2.5 GloVe Word Vectors Similarity Vector space representations of words have succeeded in capturing semantic and syntactic regularities using vector arithmetic (Pennington et al., 2014; Number of Votes Label Value 1 2 3 4 0.2 0.4 0.6 0.8 0 0 5 1 77 Mikolov et al., 2013). The word vectors from GloVe (Pennington et al., 2014) were used to calculate the semantic similarity between tokens of the two sentences by measuring their cosine similarity. The word vectors utilized were created from a corpus of 2B tweets which contains 27B tokens. Experiments on the development set were also done with vectors from Wikipedia2014 + Gigawords5 (about 6B tokens) but were not used for submission since they performed worse than the Twitter ones. The calculation of these features is based on th</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Using paraphrases for improving first story detection in news and twitter.</title>
<date>2012</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>338--346</pages>
<contexts>
<context position="14260" citStr="Petrovic et al., 2012" startWordPosition="2393" endWordPosition="2396">sess the performance of our systems on the test set for different threshold values. The results are illustrated in 46 teams did not participate in subtask2 Figure 2. We used thresholds from 0 to 1 with a step of 0.05 except for the space [0.3, 0.4] where we used a step of 0.01. The two systems behave similarly and the best performance (0.622) was achieved from the All-Sentiment system using a threshold of 0.36. Figure 2: F1 for subtask1 on the test set for our systems using different threshold values. 6 Future Work A possible direction would be to use locality sensitive hashing on the tweets (Petrovic et al., 2012) to create more features. Moreover, ordinal regression could be used to train the model (Hardin and Hilbe, 2001). The addition of a text normalization algorithm in the preprocessing step could enhance the performance of lexical overlap features and that of other methods such as wordnet, LDA (Blei et al., 2003) or LSA (Deerwester et al., 1990). Finally, the overlap of character n-grams could also be used as features. 7 Conclusion We described a system that predicts semantic similarity between tweets from the same topic. The system’s aim is to identify paraphrases of a tweet on a specific topic,</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2012</marker>
<rawString>Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2012. Using paraphrases for improving first story detection in news and twitter. In HLT-NAACL, pages 338–346. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1524--1534</pages>
<contexts>
<context position="9376" citStr="Ritter et al., 2011" startWordPosition="1544" endWordPosition="1547">orresponding label. The output of the SVR can be used for classification by using a threshold. 0.35 was chosen as the classification threshold as it belongs to the debatable space and it is slightly less than 0.4, in order to increase the recall of the minority class (Paraphrases). However, the threshold could be tuned using cross validation on the training data or by testing on the development set for better results. 4.1 Data Preprocessing Preprocessing can greatly affect the performance of a system. The tweets were passed through a Twitter specific tokenizer and part-of-speech (POS) tagger (Ritter et al., 2011) by the organizers. We converted 2The LIBLINEAR distribution (Fan et al., 2008) 3The regression model uses L2-regularized regression with the default parameter C=1. all the tweets to lower case and stopwords were removed using the NLTK (Loper and Bird, 2002) stopwords list. Moreover, we removed the tokens of the topic since they always exist in both tweets. Finally, we applied stemming to each one of the remaining tokens and the stemmed representations are stored. 4.2 Feature Engineering In this section the features used in the model will be described in detail. We made two submissions. Both s</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In EMNLP, pages 1524–1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Ralph Grishman</author>
</authors>
<title>Gathering and generating paraphrases from twitter with application to normalization.</title>
<date>2013</date>
<contexts>
<context position="6590" citStr="Xu et al., 2013" startWordPosition="1070" endWordPosition="1073"> to be aligned. The quality of this alignment can then be used as a similarity measure. When this approach is utilized, words from the two sentences are paired (Mihalcea et al., 2006) by maximizing the summation of the word similarity of the resulting pairs. Finally, the third and final approach utilizes machine learning and combines different measures and features (such as lexical, semantic and syntactic features) which are supplied to a classifier that learns a model on the training data. The unique characteristics of Twitter present new challenges and opportunities for paraphrase research (Xu et al., 2013; Ling et al., 2013). Most of the work has focused on paraphrase generation (Xu et al., 2013; Ling et al., 2013) in order to use it for text normalization. However, the task organizers (Xu et al., 2014) created a dataset, implemented a system and reimplemented several baselines and state-ofthe-art systems for sentence paraphrase recognition. They showed that their method, which combines a previous system with latent space achieves at least as good results as state-of-the-art systems. 4 System Overview The main objective of the implemented system is to classify pairs of tweets from the same top</context>
</contexts>
<marker>Xu, Ritter, Grishman, 2013</marker>
<rawString>Wei Xu, Alan Ritter, and Ralph Grishman. 2013. Gathering and generating paraphrases from twitter with application to normalization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Chris Callison-Burch</author>
<author>William Dolan</author>
<author>Yangfeng Ji</author>
</authors>
<title>Extracting lexically divergent paraphrases from twitter.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<pages>448</pages>
<contexts>
<context position="1478" citStr="Xu et al., 2014" startWordPosition="224" endWordPosition="227">f content can lead to useful information for fields such as personalized marketing or social profiling. However, such a task is not trivial, because the language used on Twitter is often informal, presenting new challenges to text analysis. Task1 of SEMEVAL-2015 (Xu et al., 2015) focuses on recognition of paraphrases and semantic similarity in Twitter i.e., recognizing if two tweets are alternative linguistic expressions of the same, or similar, meaning (Bhagat and Hovy, 2013). The task is based on a crowdsourced corpus of 18000 pairs of paraphrases and non-paraphrased sentences from Twitter (Xu et al., 2014) and each pair consists of two tweets from the same topic. A label is provided with each pair, which is the number of yes votes from 5 crowdsourced annotators when asked if the second tweet is a paraphrase of the first one. Paraphrase Example: Roberto Mancini gets the boot from Man City Non-Paraphrase Example: WORLD OF JENKS IS ON AT 11 World of Jenks is my favorite show on tv Figure 1: Examples of both a paraphrase and a nonparaphrase pair of the data. The method utilizes a support vector regression machine (SVR). The regression model tries to predict the degree of semantic similarity between</context>
<context position="3491" citStr="Xu et al., 2014" startWordPosition="566" endWordPosition="569">nd they also provided a test set (Test) for the task. To collect Roberto Mancini has been sacked by Manchester City with the Blues saying 75 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 75–79, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Set Size Paraphrase Non-Paraphrase Debatable Train 13693 3996 7534 1533 Dev 724 948 2672 585 Test 972 175 663 134 Table 1: Class distribution of the train, development and test sets. the data they used Twitter’s public API1 to crawl trending topics and their associated tweets (Xu et al., 2014). Annotation of the collected tweets was performed via crowdsourcing (Amazon Mechanical Turk). From each topic, 11 random tweets were selected and 5 different annotators were used. One of the 11 tweets was randomly selected as the original sentence. The annotators were asked to select which of the remaining 10 tweets have the same meaning as the original one. Each topic’s pairs are annotated with the number of annotators that voted for them. Problematic annotators were removed by checking their Cohen’s Kappa agreement (Artstein and Poesio, 2008) with other annotators. Agreement with an expert </context>
<context position="6792" citStr="Xu et al., 2014" startWordPosition="1107" endWordPosition="1110"> summation of the word similarity of the resulting pairs. Finally, the third and final approach utilizes machine learning and combines different measures and features (such as lexical, semantic and syntactic features) which are supplied to a classifier that learns a model on the training data. The unique characteristics of Twitter present new challenges and opportunities for paraphrase research (Xu et al., 2013; Ling et al., 2013). Most of the work has focused on paraphrase generation (Xu et al., 2013; Ling et al., 2013) in order to use it for text normalization. However, the task organizers (Xu et al., 2014) created a dataset, implemented a system and reimplemented several baselines and state-ofthe-art systems for sentence paraphrase recognition. They showed that their method, which combines a previous system with latent space achieves at least as good results as state-of-the-art systems. 4 System Overview The main objective of the implemented system is to classify pairs of tweets from the same topic as semantically similar or not. The approach used differs from previous works because it models the problem as a regression task first and then as a classification task, while typical approaches trea</context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Wei Xu, Alan Ritter, Chris Callison-Burch, William Dolan, and Yangfeng Ji. 2014. Extracting lexically divergent paraphrases from twitter. Transactions of the Association for Computational Linguistics, 2:435– 448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<title>SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT).</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).</booktitle>
<contexts>
<context position="1142" citStr="Xu et al., 2015" startWordPosition="172" endWordPosition="175">nary prediction. The model and experimental results are discussed along with future work that could improve the method. 1 Introduction Recently, Twitter has gained significant popularity among the social network services. Lots of users often use Twitter to express feelings or opinions about a variety of subjects. Analysing this kind of content can lead to useful information for fields such as personalized marketing or social profiling. However, such a task is not trivial, because the language used on Twitter is often informal, presenting new challenges to text analysis. Task1 of SEMEVAL-2015 (Xu et al., 2015) focuses on recognition of paraphrases and semantic similarity in Twitter i.e., recognizing if two tweets are alternative linguistic expressions of the same, or similar, meaning (Bhagat and Hovy, 2013). The task is based on a crowdsourced corpus of 18000 pairs of paraphrases and non-paraphrased sentences from Twitter (Xu et al., 2014) and each pair consists of two tweets from the same topic. A label is provided with each pair, which is the number of yes votes from 5 crowdsourced annotators when asked if the second tweet is a paraphrase of the first one. Paraphrase Example: Roberto Mancini gets</context>
</contexts>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>