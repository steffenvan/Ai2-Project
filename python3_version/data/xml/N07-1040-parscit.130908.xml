<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.98413">
Whose idea was this, and why does it matter?
Attributing scientific work to citations
</title>
<author confidence="0.983448">
Advaith Siddharthan &amp; Simone Teufel
</author>
<affiliation confidence="0.9889375">
Natural Language and Information Processing Group
University of Cambridge Computer Laboratory
</affiliation>
<email confidence="0.992834">
{as372,sht25}@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996705625">
Scientific papers revolve around cita-
tions, and for many discourse level
tasks one needs to know whose work
is being talked about at any point in
the discourse. In this paper, we in-
troduce the scientific attribution task,
which links different linguistic expres-
sions to citations. We discuss the
suitability of different evaluation met-
rics and evaluate our classification ap-
proach to deciding attribution both in-
trinsically and in an extrinsic evalua-
tion where information about scientific
attribution is shown to improve per-
formance on Argumentative Zoning, a
rhetorical classification task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999323866666667">
In the recent past, there has been a focus on
information management from scientific litera-
ture. In the genetics domain, for instance, in-
formation extraction of genes and gene–protein
interactions helps geneticists scan large amounts
of information (e.g., as explored in the TREC
Genomics track (Hersh et al., 2004)). Elsewhere,
citation indexes (Garfield, 1979) provide biblio-
metric data about the frequency with which par-
ticular papers are cited. The success of citation
indexers such as CiteSeer (Giles et al., 1998) and
Google Scholar relies on the robust detection
of formal citations in arbitrary text. In bibli-
ographic information retrieval, anchor text, i.e.,
the context of a citation can be used to charac-
terise (index) the cited paper using terms out-
side of that paper (Bradshaw, 2003); O’Connor
(1982) presents an approach for identifying the
area around citations where the text focuses on
that citation. And automatic citation classifi-
cation (Nanba and Okumura, 1999; Teufel et
al., 2006) determines the function that a cita-
tion plays in the discourse.
For such information access and retrieval pur-
poses, the relevance of a citation within a paper
is often crucial. One can estimate how impor-
tant a citation is by simply counting how often
it occurs in the paper. But as Kim and Webber
(2006) argue, this ignores many expressions in
text which refer to the cited author’s work but
which are not as easy to recognise as citations.
They address the resolution of instances of the
third person personal pronoun “they” in astron-
omy papers: it can either refer to a citation or to
some entities that are part of research within the
paper (e.g., planets or galaxies). Several appli-
cations should profit in principle from detecting
connections between referring expressions and
citations. For instance, in citation function clas-
sification, the task is to find out if a citation is
described as flawed or as useful. Consider:
Most computational models of discourse
are based primarily on an analysis of
the intentions of the speakers [Cohen and
Perrault, 1979][Allen and Perrault,
1980][Grosz and Sidner, 1986]WEAx.
The speaker will form intentions based on
his goals and then act on these intentions, pro-
ducing utterances. The hearer will then re-
construct a model of the speaker’s intentions
upon hearing the utterance. This approach
has many strong points, but does not
provide a very satisfactory account of
the adherence to discourse conventions in di-
alogue.
The three citations above are described as flawed
(detectable by “does not provide a very satis-
factory account”), and thus receive the label
WEAK. However, in order to detect this, one
must first realise that “this approach” refers to
</bodyText>
<page confidence="0.975834">
316
</page>
<note confidence="0.7992995">
Proceedings of NAACL HLT 2007, pages 316–323,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.971526">
the three cited papers. A contrasting hypoth-
esis could be that the citations are used (thus
deserving the label USE; the cue phrase “based
on&amp;quot; might make us think so (as in the context
“our work is based on&amp;quot;). This, however, can be
ruled out if we know that “the speaker” is not
referring to some aspect of the current paper.
</bodyText>
<sectionHeader confidence="0.786262" genericHeader="introduction">
2 The scientific attribution task
</sectionHeader>
<bodyText confidence="0.990861461538461">
We define an attribution task where possible ref-
erents are members of the reference list (i.e.,
each cited paper), the CURRENT-PAPER, and
a back-off category NO-SPECIFIC-PAPER for
markables that are not attributable to any spe-
cific paper(s). Our markables are as follows:
all definite descriptions (e.g., “the hearer”, and
including demonstrative noun phrases such as
“these intentions”), all “work” nouns i, and all
pronouns (possessive, personal and demonstra-
tive); c.f., underlined strings in the above exam-
ple. Our notion of attribution link encompasses
two relations:
</bodyText>
<listItem confidence="0.9819146">
1. ANAPHORIC: The referents are entire re-
search papers, or the papers’ authors
2. SUBPART: The referents are some compo-
nent of an approach/argument/claim in the
research paper
</listItem>
<bodyText confidence="0.998908924528302">
There are two tasks: attributing a linguistic
expression to the right paper (including the cur-
rent paper) – a task we call scientific attribution
– and deciding whether or not the expression is
anaphoric to the entirety of the paper, or just to
some subpart of it.
Kim and Webber (2006) solve the problem of
distinguishing between these relations for one
case. They decide whether the pronoun “they”
anaphorically refers to the authors of a cited pa-
per, or whether it refers to some entity that is
discussed in (a subpart of) a paper (e.g., “galax-
ies&amp;quot;). In this paper, we tackle the other problem
of scientific attribution.
We do not distinguish between the two types
of links stated above, but only identify which ci-
tation(s) a linguistic expression is attributable
&apos;We use a list of around 40 research methodology re-
lated nouns from Teufel and Moens (2002), such as e.g.,
“study, account, investigation, result” etc. These are
nouns we are particularly interested in.
to. For tasks of interest to us, it is not enough
to only consider anaphoric references to entire
papers; authors often make statements compar-
ing/using/criticising aspects or subparts of cited
work. We therefore consider a far wider range
of markables than Kim and Webber&apos;s single pro-
noun “they”.
Our attribution task differs from the tradi-
tional anaphora resolution task in that we have
a fixed list of possible referents (the reference
list items, CURRENT-PAPER or NO-SPECIFIC-
PAPER) that are known upfront. Also, we do
not form co-reference chains; we attribute a re-
ferring expression directly to one or more ref-
erents. Ours is therefore a multi-label classi-
fication task, where the citations, CURRENT-
PAPER and NO-SPECIFIC-PAPER are the labels,
and where one or more labels are assigned to
each markable.
We evaluate intrinsically by comparing to
human-annotated attribution, and extrinsically
by showing that automatically acquired knowl-
edge about scientific attribution improves per-
formance on a discourse classification task—
Argumentative Zoning (Teufel and Moens,
2002), where sentences are labelled as one
of {OWN, OTHER, BACKGROUND, TEXTUAL,
AIM, BASIS, CONTRAST} according to their role
in the author’s argument.
We describe our data in §3 and methodology
in §4, discuss evaluation metrics in §5, and eval-
uate intrinsically in §6 and extrinsically in §7.
</bodyText>
<sectionHeader confidence="0.996382" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999835">
We used data from the CmpLg (Computation
and Language archive; 320 conference articles
in computational linguistics). The articles are
in XML format.
We produced an annotated corpus (10 arti-
cles, 4290 data points, i.e., markables) based on
written guidelines. The task was found to be
quite intuitive by our annotators, and this was
reflected in high agreement - Krippendorff&apos;s al-
pha2 of more than 0.8 (2 annotators, 3 papers,
1429 data points) on the attribution task. The
distribution of classes was, as expected, quite
skewed: 69% of markables are attributable to
</bodyText>
<footnote confidence="0.743899">
2see description in §5.2
</footnote>
<page confidence="0.997486">
317
</page>
<bodyText confidence="0.9992992">
CURRENT-PAPER, 7% to no specific paper and
24% to specific references (on average, 1.7 per
reference). Details about the annotation pro-
cess and human agreement figures can be found
in Siddharthan and Teufel (2007).
</bodyText>
<sectionHeader confidence="0.991512" genericHeader="method">
4 Machine Learning Approach
</sectionHeader>
<bodyText confidence="0.999988882352941">
We frame the attribution problem as a classi-
fication task: Given a markable (the definite
description/pronoun/work noun under consid-
eration), a binary yes/no decision is made for
each cited paper, and a binary yes/no decision
is made for whether the markable is attributable
to the current paper. The list of labels for the
markable is compiled by including all the cita-
tions for which the machine learner returns yes,
and CURRENT-PAPER if the learner returns yes.
If the list is empty (learner returns no for every-
thing), the label is No-SPECIFIC-PAPER.
Since the model for whether a markable is at-
tributable to the current work is likely to be
different from the model for whether it is at-
tributable to a citation, we trained separate
models for the two problems.
</bodyText>
<subsectionHeader confidence="0.998992">
4.1 Deciding attribution to a citation
</subsectionHeader>
<bodyText confidence="0.9995492">
For each data point to be classified (called NP
below), we create a machine learning instance
for each reference list item by automatically
computing the following features from POS-
tagged text:
</bodyText>
<subsectionHeader confidence="0.67908">
1. Properties of data point (NP) and the closest Cita-
</subsectionHeader>
<bodyText confidence="0.750342">
tion instance (CIT) of the reference list item:
</bodyText>
<listItem confidence="0.91648796875">
(a) Type of NP (Definite Description/Work
Noun/Pronoun)
(b) CIT is a self Citation or not
(c) CIT is syntactic (in running text) or paren-
thetical
(d) Is CIT Hobbs’ prediction (searching left–right
starting from current sentence and then con-
sidering previous sentences, is CIT the first
citation or reference to current work found)?
2. Distance measures:
(a) Dist. between NP and CIT measured in words
(b) Dist. between NP and CIT measured in sen-
tences
(c) Dist. between NP and CIT measured in para-
graphs
(d) Is CIT after NP in the discourse (cataphor)?
(e) Distance between CIT and the closest first
person pronoun or “this paper” in words
3. Contextual:
(a) Rank of CIT (how many other reference list
items are closer)
(b) Number of times CIT is cited in the paragraph
(c) Number of times CIT is cited in the whole
paper
(d) Current Section heading (this feature has 5
values: Introduction, Methods, Results, Con-
clusions, Unrecognised)
4. Agreement:
(a) Agreement Number (He/She &amp; single author
non-self citation)
(b) Agreement Person (First &amp; Current/Self Ci-
tation, Third and Not-Current)
</listItem>
<bodyText confidence="0.999979111111111">
We have a chicken and egg problem with cal-
culating the distance of a reference to current
work in 2(e). Unlike citations, these are not un-
ambiguously marked in the text. We calculate
distance from the closest first person pronoun
(even though these could possibly refer to a self
citation, rather than current work) or the phrase
“this paper”, which can again refer to other cita-
tions but predominantly refers to current work.
</bodyText>
<subsectionHeader confidence="0.973566">
4.2 Deciding attribution to current work
</subsectionHeader>
<bodyText confidence="0.999990066666667">
We use the same features for the second clas-
sifier that makes the decision on whether the
data point refers to CURRENT-PAPER, with the
following changes: Features 1(b,c) are removed
as they are meaningless; 1(d) checks Hobbs’
prediction for a first person pronoun/“this pa-
per&amp;quot;, rather than CIT; in 2(a–d), the distance is
measured between the closest first person pro-
noun/“this paper” and the markable, rather
than a citation and the markable; similarly, in
3(b,c) we count instances of first person pro-
noun/“this paper”; for 2(e), we now calculate
the distance of the closest citation instance. In
short, the same features are used, but current
work and citations are swapped.
</bodyText>
<sectionHeader confidence="0.993467" genericHeader="method">
5 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.9998678">
We consider two evaluation metrics. The first
is the scoring system used for the co-reference
task in the Message Understanding Conferences
MUC-6 and MUC-7. The second is Krippen-
dorff&apos;s α. We briefly discuss both below.
</bodyText>
<subsectionHeader confidence="0.997517">
5.1 The MUC-6/MUC-7 Metric
</subsectionHeader>
<bodyText confidence="0.999670666666667">
The MUC-6/MUC-7 Co-reference evaluation
metric (Vilain et al., 1995) works by compar-
ing co-reference classes across two annotated
</bodyText>
<page confidence="0.994921">
318
</page>
<bodyText confidence="0.999965153846154">
files. Calling one annotation the “model” and
the other the “system”, for each co-reference
class S in the model, c(S) is the minimal num-
ber of co-reference links needed to generate the
class (this is one less than the cardinality of the
class; c(S) = JSJ − 1). m(S) is the number of
“missing” links in the system annotation rela-
tive to the co-reference class as marked up in
the model. In other words, this is the minimum
number of co-reference links that need to be
added to the system annotation to fully gener-
ate the co-reference class S in the model. Recall
error is then RE(S) = m(S)/c(S) and Recall is
</bodyText>
<equation confidence="0.7825455">
R(S) = 1 − RE = c(S)�m(S)
c(S) . Recall for the en-
</equation>
<bodyText confidence="0.993016">
tire file (or set of files) is calculated by summing
over all co-reference classes in the model:
</bodyText>
<equation confidence="0.89359">
�i c(Si) − m(Si)
R =
Ei c(Si)
Precision (P) is calculated by swapping the
</equation>
<bodyText confidence="0.596720333333333">
model and system and the f-measure (F =
2R x P/(R + P)) is symmetric with respect to
both annotations.
</bodyText>
<subsectionHeader confidence="0.988097">
5.2 Krippendorff&apos;s Alpha
</subsectionHeader>
<bodyText confidence="0.999959">
We follow Passonneau (2004) and Poesio and
Artstein (2005) in using Krippendorff (1980)&apos;s
a metric to compute agreement between anno-
tations. The advantage of a over the more com-
monly used r. metric is that a allows for par-
tial agreement when annotators assign multiple
labels to the same markable; in this case calcu-
lating agreement on a markable requires a more
graded agreement calculation than the “1 if sets
are identical and 0 otherwise” provided for by
r.. Krippendorff&apos;s a measures disagreement, and
allows for the use of distance metrics to calculate
partial disagreement. Following Passonneau, we
present results using four distance metrics:
</bodyText>
<listItem confidence="0.9827681875">
1. (N)ominal: Two sets have distance N = 0
if they are identical and N = 1 if they are
not. a calculated using the nominal dis-
tance metric is equivalent to r..
2. (J)accard: Two sets A and B have dis-
tance J = 1 − JA fl BJ/JA U BJ. In other
words, the distance between two sets is
larger, the smaller their intersection and the
larger their union.
3. (D)ice: Two sets A and B have distance
D = 1 − 2 x JA fl BJ/(JAJ + JBJ). In prac-
tice, the Dice distance metric behaves simi-
larly to the Jaccard metric, but tends to be
smaller, resulting in slightly higher a.
4. (M)ASI: This is the Jaccard distance J
weighted by a monotonicity distance m
</listItem>
<bodyText confidence="0.929413538461539">
where, m = 0 if two sets are identical;
m = 0.33 if one is a subset of the other;
m = 0.67 if the intersection and the two
set differences are all non-null; m = 1 if the
two sets are disjoint. Formally, the MASI
metric is M = m x J.
As an example, consider two sets {a, b, c} and
{b, c, d}. The distances between these sets are
N = 1, J = 1−2/4 = 0.5, D = 1−2x2/(3+3) =
0.33 and M = 0.67 x 0.5 = 0.33.
Krippendorff&apos;s a is defined as a = 1−Do/De,
where Do is the observed disagreement and De
is the disagreement that is expected by chance:
</bodyText>
<equation confidence="0.9463915">
1
Do = ( ) E E E njknjk0dkk0
cc − 1
j k k0
1
_
De c(c − 1) E E nknk0dkk0
k k0
</equation>
<bodyText confidence="0.999930636363636">
In the above formulae, c is the number of
coders, njk is the number of times item j is
classed as category k, nk is the number of times
any item is classed as category k and dkk0 is the
distance between categories k and k�.
Like r., Krippendorff&apos;s a is 1 when there is
perfect agreement, 0 when the observed agree-
ment is only what was expected by chance, neg-
ative when observed agreement is less than ex-
pected by chance and positive when observed
agreement is greater than expected by chance.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="method">
6 Intrinsic Evaluation Results
</sectionHeader>
<bodyText confidence="0.999505">
We ran a machine learning experiment us-
ing 10-fold cross-validation and the memory-
based learner IBk3 (with k=6), using the WEKA
toolkit (Witten and Frank, 2000). The perfor-
mance is shown in Tables 1 and 2. To position
these results we compare them with three base-
line lower bounds and the human performance
upper bound in Table 3. We use three baselines:
</bodyText>
<footnote confidence="0.8630475">
3Memory based learning gave better results on this
task than other learners (NB, HNB, IBk, J48, cf. § 7.3.
</footnote>
<page confidence="0.979251">
319
</page>
<table confidence="0.968865142857143">
Paper Items α-N α-J α-D α-M %A*
0003055 446 .601 .606 .607 .610 85%
0005006 446 .670 .704 .711 .715 81%
0005015 462 .679 .696 .701 .706 81%
0005025 277 .707 .707 .707 .707 86%
0006011 393 .766 .771 .772 .775 88%
0006038 578 .551 .568 .573 .578 79%
0007035 393 .570 .590 .600 .609 90%
0008026 449 .700 .700 .700 .700 87%
0001001 420 .564 .565 .569 .571 88%
0001020 429 .730 .778 .790 .801 88%
AVG. 429 .654 .669 .673 .677 85%
*% Agreement, the conservative estimate measured
using the Nominal metric
</table>
<tableCaption confidence="0.999925">
Table 1: Agreement with Human Gold Standard
</tableCaption>
<listItem confidence="0.989433555555556">
• BASEM (Major Class): All data points are
labelled CURRENT-WORK
• BASEP (Previous): Data points are tagged
with the most recent label
• BASEH (Hobbs’ Prediction): Data points
are tagged with the label found by Hobbs’
(1986) search (Search left to right in each
sentence, starting from current sentence,
then considering previous sentences)
</listItem>
<bodyText confidence="0.999971916666667">
As Table 3 shows, our machine learning ap-
proach performs much better than the base-
lines on all the agreement metrics, and is indeed
closer to human performance than to any of the
baselines. The MUC evaluation appears to pro-
duce highly inflated results on our task – when
there is a small set of co-reference classes and
one of these classes contains 70% of data points,
it takes only a small number of missing links to
correct annotations. This results in unreason-
ably high values, particularly for the majority
class baseline of labelling every data point as
CURRENT-PAPER. We believe that the α met-
rics provide a much more realistic estimate of
the difficulty of the task and the relative perfor-
mances of different approaches.
Table 4 shows the performance of the ma-
chine learner for each of the three types of lin-
guistic expressions considered. Pronouns are
the easiest to resolve, with on average 90% re-
solved correctly (an agreement with the human
gold standard of α = .71). This drops to 85%
(α = .68) for definite descriptions and demon-
stratives, and further to 78% (α = .63) for re-
</bodyText>
<table confidence="0.999883083333333">
Paper No. Classes Recall Precision F
0003055 14 .934 .886 .910
0005006 17 .875 .870 .872
0005015 19 .897 .876 .886
0005025 16 .903 .874 .888
0006011 14 .942 .909 .925
0006038 25 .905 .893 .899
0007035 18 .957 .926 .941
0008026 9 .966 .962 .964
0001001 14 .949 .908 .928
0001020 18 .924 .926 .925
TOTAL 164 .924 .903 .913
</table>
<tableCaption confidence="0.977641">
Table 2: Evaluation using MUC-6/7 software
</tableCaption>
<table confidence="0.9999265">
Algo α-N α-J α-D α-M %Agr*muc -f
Basem .002 .001 .001 .001 69% .934
BaseP -.101 -.083 -.081 -.077 19% .894
BaseH .387 .397 .399 .407 72% .910
IBk .654 .669 .673 .677 85% .913
Hum** .806 .808 .808 .809 91% .965
</table>
<tableCaption confidence="0.943519333333333">
*% Agreement, the conservative estimate measured
using the Nominal metric
**Agreement between two human annotators over a
subset of the corpus (3 files, 1429 data points)
Table 3: Comparison with Baselines and Human
Performance (Averaged results)
</tableCaption>
<bodyText confidence="0.998709916666667">
maining work nouns (i.e., those not already in a
definite noun phrase).
While all the features contributed to the re-
ported results, the most important features (in
terms of information gain) for deciding attribu-
tion to a citation were the paragraph level cita-
tion count 3(b), the distance features 2(a,b,c,d),
the rank 3(a) and the Hobbs’ prediction 1(d).
The most important features for deciding attri-
bution to the current paper were the distance
features 2(a,c,e), the rank 3(a) and the Hobbs’
prediction 1(d).
</bodyText>
<sectionHeader confidence="0.992718" genericHeader="method">
7 Extrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.99979">
To demonstrate the use of automatic scientific
attribution classification, we studied its util-
ity for one well known discourse annotation
task: Argumentative Zoning (Teufel and Moens,
2002). Argumentative Zoning (AZ) is the task of
applying one of seven discourse level tags (Fig-
ure 1) to each sentence in a scientific paper.
These categories model several aspects of sci-
entific papers: from the distinction of segments
by who an idea is attributed to (Own – Other –
Background), to the judgement of how the au-
</bodyText>
<page confidence="0.994105">
320
</page>
<table confidence="0.999899384615385">
Paper Pronouns Definites Work Nouns
αm %N αm %N αm %N
0003055 .746 94% .556 83% .735 87%
0005006 .846 91% .703 85% .700 78%
0005015 .662 83% .692 79% .787 86%
0005025 .804 89% .717 87% .514 78%
0006011 .824 91% .807 91% .615 76%
0006038 .603 90% .609 81% .430 66%
0007035 .577 94% .507 91% .770 87%
0008026 .678 88% .726 87% .551 78%
0011001 .562 97% .633 87% .377 81%
0011020 .792 90% .798 92% .808 89%
AVG. .709 90% .675 85% .629 78%
</table>
<tableCaption confidence="0.922684">
Table 4: Results for different markable types
</tableCaption>
<table confidence="0.999024272727273">
Category Description
Background Generally accepted background knowl-
edge
Other Specific other work
Own Own work: method, results, future
work
Aim Specific research goal
Textual Textual section structure
Contrast Contrast, comparison, weakness of
other solution
Basis Other work provides basis for own work
</table>
<figureCaption confidence="0.765493444444444">
Figure 1: AZ Annotation scheme
thors relate to other work (Contrast - Basis)
to the rhetorical status of high-level discourse
goals (statement of Aim; overview of section
structure (Textual)). Some of these categories
(Background, Other and Own) occur in zones
that span many sentences. Other categories typ-
ically occur in short zones, often just a single
sentence (Textual, Aim, Contrast, Basis).
</figureCaption>
<bodyText confidence="0.999927777777778">
In all work to date, classification of sentences
into one of the AZ categories has been performed
on the basis of features extracted from within
the sentence, and a few contextual features such
as section heading and location in document.
Scientific attribution links previously unresolved
noun phrases or pronouns in the sentence to cita-
tions. As this provides the machine learner with
more information, AZ results should improve.
</bodyText>
<subsectionHeader confidence="0.998411">
7.1 AZ Data
</subsectionHeader>
<bodyText confidence="0.986421">
The evaluation corpus used is the one from
Teufel and Moens (2002). It consists of 80 con-
ference papers in computational linguistics, con-
taining around 12000 sentences. Each of these
is manually tagged as one of {OWN, OTH, BKG,
BAS, AIM, CTR, TXT}. The reliability observed
is reasonable (Kappa=0.71)).
</bodyText>
<subsectionHeader confidence="0.981039">
7.2 Features
</subsectionHeader>
<bodyText confidence="0.989758333333333">
Following Teufel and Moens (2002), we used su-
pervised ML using features extracted by shallow
processing (POS tagging and pattern matching):
</bodyText>
<listItem confidence="0.935398">
• Lexical (cue phrase) features consist
of three features: the first models occur-
</listItem>
<bodyText confidence="0.911101470588235">
rence of about 1700 manually identified sci-
entific cue phrases (such as “in this paper”).
The cue phrases are classified into semantic
groups. The second models the main verb
of the sentence, by lookup in a verb lexicon
organised by 13 main clusters of verb types
(e.g. “change verbs”), and the third models
the likely subject of the sentence, by clas-
sifying them either as the authors, or other
researchers, or none of the above, using an
extensive lexicon of regular expressions.
• Content word features model occurrence
and density of content words in the sen-
tences, where content words are either de-
fined as non-stoplist words in the subsection
heading preceding the sentence, or as words
with a high TF*IDF score.
</bodyText>
<listItem confidence="0.9755594">
• Linguistic features include (complex)
tense, voice, and presence of an auxiliary.
• Citation features detect properties of for-
mal citations in text, such as the occurrence
of authors’ names in text, the position of a
citation in text, and whether the citation
is a self citation (i.e., includes any of the
authors of the paper itself).
• Location features: Rhetorical roles are
expected at certain places in the document,
</listItem>
<bodyText confidence="0.92354375">
for instance, background sentences are more
likely to occur at the beginning of the text,
and goal statements often occur after about
a fifth of the paper. We model this by split-
ting the text into ten segments and assign-
ing each sentence to the segment it is lo-
cated in. We also use the section heading
as a contextual feature.
Some categories tend to occur in blocks (e.g.,
OWN, OTHER, BACKGROUND), and the context
in terms of the label of the previous sentence
has good predictive value. We model this (the
</bodyText>
<page confidence="0.995076">
321
</page>
<table confidence="0.999582714285714">
Learner kappa Macro-F
No Attrib With Attrib No With
NB .45 .46 .53 .53
HNB .42 .45 .51 .53
IBk .34 .36 .39 .39
J48 .38 .41 .41 .48
Stacking .45 .48 .51 .53
</table>
<tableCaption confidence="0.9984945">
Table 5: Improvement on AZ from using auto-
matic scientific attribution classification.
</tableCaption>
<bodyText confidence="0.9971518">
so-called History feature) by running the clas-
sifier twice, and including the prediction for the
previous sentence as a feature the second time.
Due to practical considerations, we obtained
our linguistic features using the RASP part of
speech tagger (Briscoe and Carroll, 1995), when
in previous work we used the IT TTT (Grover
et al., 2000). We would not expect this to in-
fluence results much, however. Another differ-
ence is that we use around 1700 additional cue
phrases acquired from previous work on another
discourse task4 (Teufel et al., 2006).
In addition to these features, we use four
features obtained from the scientific attribution
task described in this paper:
</bodyText>
<subsectionHeader confidence="0.893009">
Scientific Attribution Features:
</subsectionHeader>
<listItem confidence="0.9836405">
• Whether there is any reference to current
work in the sentence
• Whether there is any reference to any spe-
cific citation in the sentence
• Whether there is any reference in the sen-
tence to work that is in neither the current
paper nor any specific citation
• Which of these, if any, is in subject position
</listItem>
<bodyText confidence="0.999253666666667">
Our aim is to explore whether these features
obtained from the scientific attribution task in-
fluence machine learning performance on AZ.
</bodyText>
<subsectionHeader confidence="0.992193">
7.3 AZ results
</subsectionHeader>
<bodyText confidence="0.973502125">
We ran five different machine learners with and
without the four scientific attribution features
(c.f., §7.2). Note that our labelled data for the
attribution task does not overlap with the 80 pa-
pers in the AZ corpus, and all attribution pre-
dictions used in features for this AZ experiment
¢These cues are acquired manually from files that are
not part of the AZ evaluation corpus.
</bodyText>
<table confidence="0.99902325">
Without Attribution Features
Aim Ctr Txt Own Bkg Bas Oth
P .44 .42 .52 .84 .46 .34 .47
R .61 .30 .68 .88 .45 .37 .37
F .52 .35 .59 .86 .46 .35 .42
Correctly Classified Instances 73.0%
Kappa statistic 0.45
Macro-F 0.51
With Attribution Features
Aim Ctr Txt Own Bkg Bas Oth
P .57 .42 .57 .84 .44 .40 .55
R .61 .27 .66 .90 .47 .43 .42
F .59 .33 .61 .87 .46 .41 .47
Correctly Classified Instances 74.7%
Kappa statistic 0.48
Macro-F 0.53
</table>
<tableCaption confidence="0.950262333333333">
Table 6: Best AZ results using Stacked classifier:
with and without Attribution Features.
are obtained entirely from unseen (and indeed
unlabelled) data based on the model learnt on
10 papers (c.f., §6). The learners we used (with
default WEKA settings) are:
</tableCaption>
<listItem confidence="0.9993996">
• NB: Naive Bayes learner
• HNB: Hidden Naive Bayes learner
• IBk: Memory based learner
• J48: Decision tree based learner
• STACKING: combining NB and J48 classi-
</listItem>
<bodyText confidence="0.98314515">
fiers with the stacking method
As mentioned under History feature above, we
run each learner twice, the second time includ-
ing the machine learning prediction for the pre-
vious sentence (as we found in Teufel and Moens
(2002) for NB, we noticed a slight improvement
in performance when using the history feature
(between .005 and .01 on both r. and Macro-
F for all learners)). We found an improvement
from including the four reference features with
all the learners, as shown in Table 5.
For a more detailed view of where the im-
provement comes from, refer to Table 6, which
shows precision, recall and f-measure per cate-
gory for our best learner. The biggest improve-
ments from using attribution features are for the
categories OTHER, Aim and BAs. The improve-
ment in OTHER was to be be expected, as this
zone is directly related to the attribution classi-
fication. The large improvements in Aim and
</bodyText>
<page confidence="0.992236">
322
</page>
<table confidence="0.997108142857143">
Aim Ctr pct Own Bkg Bas Oth
P .44 .34 .57 .84 .40 .37 .52
R .65 .20 .66 .88 .50 .40 .39
F .52 .26 .61 .86 .44 .38 .44
Correctly Classified Instances 72.5%
Kappa statistic 0.45
Macro-F 0.50
</table>
<tableCaption confidence="0.8253535">
Table 7: Teufel and Moens (2002)&apos;s best AZ re-
sults (Naive Bayes Classifier).
</tableCaption>
<bodyText confidence="0.999742555555556">
Bas is good news, as these are amongst our
most informative rhetorical categories for down-
stream tasks. Our best results of Kappa=0.48
and Macro-F=0.53 are better than the best pre-
viously published results on task (Kappa=0.45
and Macro-F=0.50 in Teufel and Moens (2002)).
Our results improve on the results of Teufel and
Moens (2002) (reproduced in Table 7) – both
overall and for each individual category.
</bodyText>
<sectionHeader confidence="0.999362" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9999593125">
We have described a new reference task - decid-
ing scientific attribution, and demonstrated high
human agreement (α &gt; 0.8) on this task. Our
machine learning solution using shallow features
achieves an agreement of αm = 0.68 with the
human gold standard, increasing to αm = 0.71
if only pronouns need to be resolved. We have
also demonstrated that information about scien-
tific attribution improves results for a discourse
classification task (Argumentative Zoning).
We believe that similar improvements can be
achieved on other discourse annotation tasks in
the scientific literature domain. In particular,
we plan to investigate the use of scientific at-
tribution information for the citation function
classification task.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.973586666666666">
This work was funded by the EPSRC project
SciBorg (EP/C010035/1, Extracting the Science
from Scientific Publications).
</bodyText>
<sectionHeader confidence="0.998645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999942177419355">
S. Bradshaw. 2003. Reference directed indexing: Re-
deeming relevance for subject search in citation
indexes. In Proc. of ECDL.
T. Briscoe and J. Carroll. 1995. Developing and
evaluating a probabilistic LR parser of part-of-
speech and punctuation labels. In Proc. of IWPT-
95, Prague / Karlovy Vary, Czech Republic.
E. Garfield. 1979. Citation Indexing: Its Theory and
Application in Science, Technology and Humani-
ties. J. Wiley, New York, NY.
C. L. Giles, K. Bollacker, and S. Lawrence. 1998.
Citeseer: An automatic citation indexing system.
In Proc. of the Third ACM Conference on Digital
Libraries.
C. Grover, C. Matheson, A. Mikheev, and M. Moens.
2000. LT TTT - A flexible tokenisation tool. In
Proc. of LREC-00, Athens, Greece.
W. Hersh, R. Bhuptiraju, L. Ross, P. Johnson, A.
Cohen, and D. Kraemer. 2004. Trec 2004 ge-
nomics track overview. In Proc. of TREC.
J. Hobbs. 1986. Resolving Pronoun References. In
Readings in Natural Language, Grosz, B., Sparck-
Jones, K. and Webber, B. (eds.) Morgan Kauf-
man.
Y. Kim and B. Webber. 2006. Automatic refer-
ence resolution in astronomy articles. In Proc. of
20th International CODATA Conference, Beijing,
China.
K. Krippendorff. 1980. Content Analysis: An in-
troduction to its methodology. Sage Publications,
Beverly Hills.
H. Nanba and M. Okumura. 1999. Towards multi-
paper summarization using reference information.
In Proc. of IJCAI-99.
J. O’Connor. 1982. Citing statements: Computer
recognition and use to improve retrieval. Informa-
tion Processing and Management, 18(3):125–131.
R. Passonneau. 2004. Computing reliability for
coreference annotation. In Proc. of LREC-04, Lis-
bon, Portugal.
M. Poesio and R. Artstein. 2005. Annotating
(anaphoric) ambiguity. In Proc. of the Corpus
Linguistics Conference, Birmingham, UK.
A. Siddharthan and S. Teufel. 2007. Whose
idea was this? Deciding attribution in scien-
tific literature. In Proc. of the 6th Discourse
Anaphora and Anaphor Resolution Colloquium
(DAARC’07), Lagos, Portugal.
S. Teufel and M. Moens. 2002. Summarising sci-
entific articles — experiments with relevance an
d rhetorical status. Computational Linguistics,
28(4):409–446.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Au-
tomatic classification of citation function. In Proc.
of EMNLP-06, Sydney, Australia.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. In Proc. of the 6th Message
Understanding Conference, San Francisco.
I. Witten and E. Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
</reference>
<page confidence="0.999393">
323
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900213">
<title confidence="0.9927605">Whose idea was this, and why does it Attributing scientific work to citations</title>
<author confidence="0.951965">Advaith Siddharthan</author>
<author confidence="0.951965">Simone</author>
<affiliation confidence="0.9606965">Natural Language and Information Processing University of Cambridge Computer</affiliation>
<abstract confidence="0.999521352941176">Scientific papers revolve around citations, and for many discourse level tasks one needs to know whose work is being talked about at any point in the discourse. In this paper, we inthe attribution which links different linguistic expressions to citations. We discuss the suitability of different evaluation metrics and evaluate our classification approach to deciding attribution both intrinsically and in an extrinsic evaluation where information about scientific attribution is shown to improve performance on Argumentative Zoning, a rhetorical classification task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bradshaw</author>
</authors>
<title>Reference directed indexing: Redeeming relevance for subject search in citation indexes.</title>
<date>2003</date>
<booktitle>In Proc. of ECDL.</booktitle>
<contexts>
<context position="1656" citStr="Bradshaw, 2003" startWordPosition="249" endWordPosition="250">in interactions helps geneticists scan large amounts of information (e.g., as explored in the TREC Genomics track (Hersh et al., 2004)). Elsewhere, citation indexes (Garfield, 1979) provide bibliometric data about the frequency with which particular papers are cited. The success of citation indexers such as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citation. And automatic citation classification (Nanba and Okumura, 1999; Teufel et al., 2006) determines the function that a citation plays in the discourse. For such information access and retrieval purposes, the relevance of a citation within a paper is often crucial. One can estimate how important a citation is by simply counting how often it occurs in the paper. But as Kim and Webber (2006) argue, this ignores many expressions in text which refer to the cited author’s work but w</context>
</contexts>
<marker>Bradshaw, 2003</marker>
<rawString>S. Bradshaw. 2003. Reference directed indexing: Redeeming relevance for subject search in citation indexes. In Proc. of ECDL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Developing and evaluating a probabilistic LR parser of part-ofspeech and punctuation labels.</title>
<date>1995</date>
<booktitle>In Proc. of IWPT95, Prague / Karlovy Vary,</booktitle>
<location>Czech Republic.</location>
<contexts>
<context position="23761" citStr="Briscoe and Carroll, 1995" startWordPosition="4008" endWordPosition="4011">context in terms of the label of the previous sentence has good predictive value. We model this (the 321 Learner kappa Macro-F No Attrib With Attrib No With NB .45 .46 .53 .53 HNB .42 .45 .51 .53 IBk .34 .36 .39 .39 J48 .38 .41 .41 .48 Stacking .45 .48 .51 .53 Table 5: Improvement on AZ from using automatic scientific attribution classification. so-called History feature) by running the classifier twice, and including the prediction for the previous sentence as a feature the second time. Due to practical considerations, we obtained our linguistic features using the RASP part of speech tagger (Briscoe and Carroll, 1995), when in previous work we used the IT TTT (Grover et al., 2000). We would not expect this to influence results much, however. Another difference is that we use around 1700 additional cue phrases acquired from previous work on another discourse task4 (Teufel et al., 2006). In addition to these features, we use four features obtained from the scientific attribution task described in this paper: Scientific Attribution Features: • Whether there is any reference to current work in the sentence • Whether there is any reference to any specific citation in the sentence • Whether there is any referenc</context>
</contexts>
<marker>Briscoe, Carroll, 1995</marker>
<rawString>T. Briscoe and J. Carroll. 1995. Developing and evaluating a probabilistic LR parser of part-ofspeech and punctuation labels. In Proc. of IWPT95, Prague / Karlovy Vary, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Garfield</author>
</authors>
<title>Citation Indexing: Its Theory and Application</title>
<date>1979</date>
<booktitle>in Science, Technology</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="1222" citStr="Garfield, 1979" startWordPosition="177" endWordPosition="178">r classification approach to deciding attribution both intrinsically and in an extrinsic evaluation where information about scientific attribution is shown to improve performance on Argumentative Zoning, a rhetorical classification task. 1 Introduction In the recent past, there has been a focus on information management from scientific literature. In the genetics domain, for instance, information extraction of genes and gene–protein interactions helps geneticists scan large amounts of information (e.g., as explored in the TREC Genomics track (Hersh et al., 2004)). Elsewhere, citation indexes (Garfield, 1979) provide bibliometric data about the frequency with which particular papers are cited. The success of citation indexers such as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citation. And automatic citation classification (Nanba</context>
</contexts>
<marker>Garfield, 1979</marker>
<rawString>E. Garfield. 1979. Citation Indexing: Its Theory and Application in Science, Technology and Humanities. J. Wiley, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Giles</author>
<author>K Bollacker</author>
<author>S Lawrence</author>
</authors>
<title>Citeseer: An automatic citation indexing system.</title>
<date>1998</date>
<booktitle>In Proc. of the Third ACM Conference on Digital Libraries.</booktitle>
<contexts>
<context position="1379" citStr="Giles et al., 1998" startWordPosition="201" endWordPosition="204">wn to improve performance on Argumentative Zoning, a rhetorical classification task. 1 Introduction In the recent past, there has been a focus on information management from scientific literature. In the genetics domain, for instance, information extraction of genes and gene–protein interactions helps geneticists scan large amounts of information (e.g., as explored in the TREC Genomics track (Hersh et al., 2004)). Elsewhere, citation indexes (Garfield, 1979) provide bibliometric data about the frequency with which particular papers are cited. The success of citation indexers such as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citation. And automatic citation classification (Nanba and Okumura, 1999; Teufel et al., 2006) determines the function that a citation plays in the discourse. For such information access and retrieval purposes, </context>
</contexts>
<marker>Giles, Bollacker, Lawrence, 1998</marker>
<rawString>C. L. Giles, K. Bollacker, and S. Lawrence. 1998. Citeseer: An automatic citation indexing system. In Proc. of the Third ACM Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>C Matheson</author>
<author>A Mikheev</author>
<author>M Moens</author>
</authors>
<title>LT TTT - A flexible tokenisation tool.</title>
<date>2000</date>
<booktitle>In Proc. of LREC-00,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="23825" citStr="Grover et al., 2000" startWordPosition="4021" endWordPosition="4024">tive value. We model this (the 321 Learner kappa Macro-F No Attrib With Attrib No With NB .45 .46 .53 .53 HNB .42 .45 .51 .53 IBk .34 .36 .39 .39 J48 .38 .41 .41 .48 Stacking .45 .48 .51 .53 Table 5: Improvement on AZ from using automatic scientific attribution classification. so-called History feature) by running the classifier twice, and including the prediction for the previous sentence as a feature the second time. Due to practical considerations, we obtained our linguistic features using the RASP part of speech tagger (Briscoe and Carroll, 1995), when in previous work we used the IT TTT (Grover et al., 2000). We would not expect this to influence results much, however. Another difference is that we use around 1700 additional cue phrases acquired from previous work on another discourse task4 (Teufel et al., 2006). In addition to these features, we use four features obtained from the scientific attribution task described in this paper: Scientific Attribution Features: • Whether there is any reference to current work in the sentence • Whether there is any reference to any specific citation in the sentence • Whether there is any reference in the sentence to work that is in neither the current paper n</context>
</contexts>
<marker>Grover, Matheson, Mikheev, Moens, 2000</marker>
<rawString>C. Grover, C. Matheson, A. Mikheev, and M. Moens. 2000. LT TTT - A flexible tokenisation tool. In Proc. of LREC-00, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hersh</author>
<author>R Bhuptiraju</author>
<author>L Ross</author>
<author>P Johnson</author>
<author>A Cohen</author>
<author>D Kraemer</author>
</authors>
<title>Trec</title>
<date>2004</date>
<booktitle>In Proc. of TREC.</booktitle>
<contexts>
<context position="1175" citStr="Hersh et al., 2004" startWordPosition="170" endWordPosition="173">ity of different evaluation metrics and evaluate our classification approach to deciding attribution both intrinsically and in an extrinsic evaluation where information about scientific attribution is shown to improve performance on Argumentative Zoning, a rhetorical classification task. 1 Introduction In the recent past, there has been a focus on information management from scientific literature. In the genetics domain, for instance, information extraction of genes and gene–protein interactions helps geneticists scan large amounts of information (e.g., as explored in the TREC Genomics track (Hersh et al., 2004)). Elsewhere, citation indexes (Garfield, 1979) provide bibliometric data about the frequency with which particular papers are cited. The success of citation indexers such as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citatio</context>
</contexts>
<marker>Hersh, Bhuptiraju, Ross, Johnson, Cohen, Kraemer, 2004</marker>
<rawString>W. Hersh, R. Bhuptiraju, L. Ross, P. Johnson, A. Cohen, and D. Kraemer. 2004. Trec 2004 genomics track overview. In Proc. of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>Resolving Pronoun References.</title>
<date>1986</date>
<booktitle>In Readings in Natural Language,</booktitle>
<editor>Grosz, B., SparckJones, K. and Webber, B. (eds.)</editor>
<publisher>Morgan Kaufman.</publisher>
<marker>Hobbs, 1986</marker>
<rawString>J. Hobbs. 1986. Resolving Pronoun References. In Readings in Natural Language, Grosz, B., SparckJones, K. and Webber, B. (eds.) Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kim</author>
<author>B Webber</author>
</authors>
<title>Automatic reference resolution in astronomy articles.</title>
<date>2006</date>
<booktitle>In Proc. of 20th International CODATA Conference,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2166" citStr="Kim and Webber (2006)" startWordPosition="333" endWordPosition="336">citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citation. And automatic citation classification (Nanba and Okumura, 1999; Teufel et al., 2006) determines the function that a citation plays in the discourse. For such information access and retrieval purposes, the relevance of a citation within a paper is often crucial. One can estimate how important a citation is by simply counting how often it occurs in the paper. But as Kim and Webber (2006) argue, this ignores many expressions in text which refer to the cited author’s work but which are not as easy to recognise as citations. They address the resolution of instances of the third person personal pronoun “they” in astronomy papers: it can either refer to a citation or to some entities that are part of research within the paper (e.g., planets or galaxies). Several applications should profit in principle from detecting connections between referring expressions and citations. For instance, in citation function classification, the task is to find out if a citation is described as flawe</context>
<context position="5085" citStr="Kim and Webber (2006)" startWordPosition="806" endWordPosition="809">ll pronouns (possessive, personal and demonstrative); c.f., underlined strings in the above example. Our notion of attribution link encompasses two relations: 1. ANAPHORIC: The referents are entire research papers, or the papers’ authors 2. SUBPART: The referents are some component of an approach/argument/claim in the research paper There are two tasks: attributing a linguistic expression to the right paper (including the current paper) – a task we call scientific attribution – and deciding whether or not the expression is anaphoric to the entirety of the paper, or just to some subpart of it. Kim and Webber (2006) solve the problem of distinguishing between these relations for one case. They decide whether the pronoun “they” anaphorically refers to the authors of a cited paper, or whether it refers to some entity that is discussed in (a subpart of) a paper (e.g., “galaxies&amp;quot;). In this paper, we tackle the other problem of scientific attribution. We do not distinguish between the two types of links stated above, but only identify which citation(s) a linguistic expression is attributable &apos;We use a list of around 40 research methodology related nouns from Teufel and Moens (2002), such as e.g., “study, acco</context>
</contexts>
<marker>Kim, Webber, 2006</marker>
<rawString>Y. Kim and B. Webber. 2006. Automatic reference resolution in astronomy articles. In Proc. of 20th International CODATA Conference, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Content Analysis: An introduction to its methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills.</location>
<contexts>
<context position="12764" citStr="Krippendorff (1980)" startWordPosition="2077" endWordPosition="2078">ber of co-reference links that need to be added to the system annotation to fully generate the co-reference class S in the model. Recall error is then RE(S) = m(S)/c(S) and Recall is R(S) = 1 − RE = c(S)�m(S) c(S) . Recall for the entire file (or set of files) is calculated by summing over all co-reference classes in the model: �i c(Si) − m(Si) R = Ei c(Si) Precision (P) is calculated by swapping the model and system and the f-measure (F = 2R x P/(R + P)) is symmetric with respect to both annotations. 5.2 Krippendorff&apos;s Alpha We follow Passonneau (2004) and Poesio and Artstein (2005) in using Krippendorff (1980)&apos;s a metric to compute agreement between annotations. The advantage of a over the more commonly used r. metric is that a allows for partial agreement when annotators assign multiple labels to the same markable; in this case calculating agreement on a markable requires a more graded agreement calculation than the “1 if sets are identical and 0 otherwise” provided for by r.. Krippendorff&apos;s a measures disagreement, and allows for the use of distance metrics to calculate partial disagreement. Following Passonneau, we present results using four distance metrics: 1. (N)ominal: Two sets have distance</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>K. Krippendorff. 1980. Content Analysis: An introduction to its methodology. Sage Publications, Beverly Hills.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nanba</author>
<author>M Okumura</author>
</authors>
<title>Towards multipaper summarization using reference information.</title>
<date>1999</date>
<booktitle>In Proc. of IJCAI-99.</booktitle>
<contexts>
<context position="1840" citStr="Nanba and Okumura, 1999" startWordPosition="274" endWordPosition="277">1979) provide bibliometric data about the frequency with which particular papers are cited. The success of citation indexers such as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citation. And automatic citation classification (Nanba and Okumura, 1999; Teufel et al., 2006) determines the function that a citation plays in the discourse. For such information access and retrieval purposes, the relevance of a citation within a paper is often crucial. One can estimate how important a citation is by simply counting how often it occurs in the paper. But as Kim and Webber (2006) argue, this ignores many expressions in text which refer to the cited author’s work but which are not as easy to recognise as citations. They address the resolution of instances of the third person personal pronoun “they” in astronomy papers: it can either refer to a citat</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>H. Nanba and M. Okumura. 1999. Towards multipaper summarization using reference information. In Proc. of IJCAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J O’Connor</author>
</authors>
<title>Citing statements: Computer recognition and use to improve retrieval.</title>
<date>1982</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>18</volume>
<issue>3</issue>
<marker>O’Connor, 1982</marker>
<rawString>J. O’Connor. 1982. Citing statements: Computer recognition and use to improve retrieval. Information Processing and Management, 18(3):125–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
</authors>
<title>Computing reliability for coreference annotation.</title>
<date>2004</date>
<booktitle>In Proc. of LREC-04,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="12704" citStr="Passonneau (2004)" startWordPosition="2068" endWordPosition="2069">d up in the model. In other words, this is the minimum number of co-reference links that need to be added to the system annotation to fully generate the co-reference class S in the model. Recall error is then RE(S) = m(S)/c(S) and Recall is R(S) = 1 − RE = c(S)�m(S) c(S) . Recall for the entire file (or set of files) is calculated by summing over all co-reference classes in the model: �i c(Si) − m(Si) R = Ei c(Si) Precision (P) is calculated by swapping the model and system and the f-measure (F = 2R x P/(R + P)) is symmetric with respect to both annotations. 5.2 Krippendorff&apos;s Alpha We follow Passonneau (2004) and Poesio and Artstein (2005) in using Krippendorff (1980)&apos;s a metric to compute agreement between annotations. The advantage of a over the more commonly used r. metric is that a allows for partial agreement when annotators assign multiple labels to the same markable; in this case calculating agreement on a markable requires a more graded agreement calculation than the “1 if sets are identical and 0 otherwise” provided for by r.. Krippendorff&apos;s a measures disagreement, and allows for the use of distance metrics to calculate partial disagreement. Following Passonneau, we present results using</context>
</contexts>
<marker>Passonneau, 2004</marker>
<rawString>R. Passonneau. 2004. Computing reliability for coreference annotation. In Proc. of LREC-04, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Artstein</author>
</authors>
<title>Annotating (anaphoric) ambiguity.</title>
<date>2005</date>
<booktitle>In Proc. of the Corpus Linguistics Conference,</booktitle>
<location>Birmingham, UK.</location>
<contexts>
<context position="12735" citStr="Poesio and Artstein (2005)" startWordPosition="2071" endWordPosition="2074">other words, this is the minimum number of co-reference links that need to be added to the system annotation to fully generate the co-reference class S in the model. Recall error is then RE(S) = m(S)/c(S) and Recall is R(S) = 1 − RE = c(S)�m(S) c(S) . Recall for the entire file (or set of files) is calculated by summing over all co-reference classes in the model: �i c(Si) − m(Si) R = Ei c(Si) Precision (P) is calculated by swapping the model and system and the f-measure (F = 2R x P/(R + P)) is symmetric with respect to both annotations. 5.2 Krippendorff&apos;s Alpha We follow Passonneau (2004) and Poesio and Artstein (2005) in using Krippendorff (1980)&apos;s a metric to compute agreement between annotations. The advantage of a over the more commonly used r. metric is that a allows for partial agreement when annotators assign multiple labels to the same markable; in this case calculating agreement on a markable requires a more graded agreement calculation than the “1 if sets are identical and 0 otherwise” provided for by r.. Krippendorff&apos;s a measures disagreement, and allows for the use of distance metrics to calculate partial disagreement. Following Passonneau, we present results using four distance metrics: 1. (N)o</context>
</contexts>
<marker>Poesio, Artstein, 2005</marker>
<rawString>M. Poesio and R. Artstein. 2005. Annotating (anaphoric) ambiguity. In Proc. of the Corpus Linguistics Conference, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
<author>S Teufel</author>
</authors>
<title>Whose idea was this? Deciding attribution in scientific literature.</title>
<date>2007</date>
<booktitle>In Proc. of the 6th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC’07),</booktitle>
<location>Lagos, Portugal.</location>
<contexts>
<context position="7950" citStr="Siddharthan and Teufel (2007)" startWordPosition="1262" endWordPosition="1265">s (10 articles, 4290 data points, i.e., markables) based on written guidelines. The task was found to be quite intuitive by our annotators, and this was reflected in high agreement - Krippendorff&apos;s alpha2 of more than 0.8 (2 annotators, 3 papers, 1429 data points) on the attribution task. The distribution of classes was, as expected, quite skewed: 69% of markables are attributable to 2see description in §5.2 317 CURRENT-PAPER, 7% to no specific paper and 24% to specific references (on average, 1.7 per reference). Details about the annotation process and human agreement figures can be found in Siddharthan and Teufel (2007). 4 Machine Learning Approach We frame the attribution problem as a classification task: Given a markable (the definite description/pronoun/work noun under consideration), a binary yes/no decision is made for each cited paper, and a binary yes/no decision is made for whether the markable is attributable to the current paper. The list of labels for the markable is compiled by including all the citations for which the machine learner returns yes, and CURRENT-PAPER if the learner returns yes. If the list is empty (learner returns no for everything), the label is No-SPECIFIC-PAPER. Since the model</context>
</contexts>
<marker>Siddharthan, Teufel, 2007</marker>
<rawString>A. Siddharthan and S. Teufel. 2007. Whose idea was this? Deciding attribution in scientific literature. In Proc. of the 6th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC’07), Lagos, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Summarising scientific articles — experiments with relevance an d rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="5657" citStr="Teufel and Moens (2002)" startWordPosition="903" endWordPosition="906">r just to some subpart of it. Kim and Webber (2006) solve the problem of distinguishing between these relations for one case. They decide whether the pronoun “they” anaphorically refers to the authors of a cited paper, or whether it refers to some entity that is discussed in (a subpart of) a paper (e.g., “galaxies&amp;quot;). In this paper, we tackle the other problem of scientific attribution. We do not distinguish between the two types of links stated above, but only identify which citation(s) a linguistic expression is attributable &apos;We use a list of around 40 research methodology related nouns from Teufel and Moens (2002), such as e.g., “study, account, investigation, result” etc. These are nouns we are particularly interested in. to. For tasks of interest to us, it is not enough to only consider anaphoric references to entire papers; authors often make statements comparing/using/criticising aspects or subparts of cited work. We therefore consider a far wider range of markables than Kim and Webber&apos;s single pronoun “they”. Our attribution task differs from the traditional anaphora resolution task in that we have a fixed list of possible referents (the reference list items, CURRENT-PAPER or NO-SPECIFICPAPER) tha</context>
<context position="19093" citStr="Teufel and Moens, 2002" startWordPosition="3222" endWordPosition="3225">uted to the reported results, the most important features (in terms of information gain) for deciding attribution to a citation were the paragraph level citation count 3(b), the distance features 2(a,b,c,d), the rank 3(a) and the Hobbs’ prediction 1(d). The most important features for deciding attribution to the current paper were the distance features 2(a,c,e), the rank 3(a) and the Hobbs’ prediction 1(d). 7 Extrinsic Evaluation To demonstrate the use of automatic scientific attribution classification, we studied its utility for one well known discourse annotation task: Argumentative Zoning (Teufel and Moens, 2002). Argumentative Zoning (AZ) is the task of applying one of seven discourse level tags (Figure 1) to each sentence in a scientific paper. These categories model several aspects of scientific papers: from the distinction of segments by who an idea is attributed to (Own – Other – Background), to the judgement of how the au320 Paper Pronouns Definites Work Nouns αm %N αm %N αm %N 0003055 .746 94% .556 83% .735 87% 0005006 .846 91% .703 85% .700 78% 0005015 .662 83% .692 79% .787 86% 0005025 .804 89% .717 87% .514 78% 0006011 .824 91% .807 91% .615 76% 0006038 .603 90% .609 81% .430 66% 0007035 .57</context>
<context position="21113" citStr="Teufel and Moens (2002)" startWordPosition="3561" endWordPosition="3564">. Other categories typically occur in short zones, often just a single sentence (Textual, Aim, Contrast, Basis). In all work to date, classification of sentences into one of the AZ categories has been performed on the basis of features extracted from within the sentence, and a few contextual features such as section heading and location in document. Scientific attribution links previously unresolved noun phrases or pronouns in the sentence to citations. As this provides the machine learner with more information, AZ results should improve. 7.1 AZ Data The evaluation corpus used is the one from Teufel and Moens (2002). It consists of 80 conference papers in computational linguistics, containing around 12000 sentences. Each of these is manually tagged as one of {OWN, OTH, BKG, BAS, AIM, CTR, TXT}. The reliability observed is reasonable (Kappa=0.71)). 7.2 Features Following Teufel and Moens (2002), we used supervised ML using features extracted by shallow processing (POS tagging and pattern matching): • Lexical (cue phrase) features consist of three features: the first models occurrence of about 1700 manually identified scientific cue phrases (such as “in this paper”). The cue phrases are classified into sem</context>
<context position="26110" citStr="Teufel and Moens (2002)" startWordPosition="4420" endWordPosition="4423">lts using Stacked classifier: with and without Attribution Features. are obtained entirely from unseen (and indeed unlabelled) data based on the model learnt on 10 papers (c.f., §6). The learners we used (with default WEKA settings) are: • NB: Naive Bayes learner • HNB: Hidden Naive Bayes learner • IBk: Memory based learner • J48: Decision tree based learner • STACKING: combining NB and J48 classifiers with the stacking method As mentioned under History feature above, we run each learner twice, the second time including the machine learning prediction for the previous sentence (as we found in Teufel and Moens (2002) for NB, we noticed a slight improvement in performance when using the history feature (between .005 and .01 on both r. and MacroF for all learners)). We found an improvement from including the four reference features with all the learners, as shown in Table 5. For a more detailed view of where the improvement comes from, refer to Table 6, which shows precision, recall and f-measure per category for our best learner. The biggest improvements from using attribution features are for the categories OTHER, Aim and BAs. The improvement in OTHER was to be be expected, as this zone is directly relate</context>
<context position="27384" citStr="Teufel and Moens (2002)" startWordPosition="4646" endWordPosition="4649">ovements in Aim and 322 Aim Ctr pct Own Bkg Bas Oth P .44 .34 .57 .84 .40 .37 .52 R .65 .20 .66 .88 .50 .40 .39 F .52 .26 .61 .86 .44 .38 .44 Correctly Classified Instances 72.5% Kappa statistic 0.45 Macro-F 0.50 Table 7: Teufel and Moens (2002)&apos;s best AZ results (Naive Bayes Classifier). Bas is good news, as these are amongst our most informative rhetorical categories for downstream tasks. Our best results of Kappa=0.48 and Macro-F=0.53 are better than the best previously published results on task (Kappa=0.45 and Macro-F=0.50 in Teufel and Moens (2002)). Our results improve on the results of Teufel and Moens (2002) (reproduced in Table 7) – both overall and for each individual category. 8 Conclusions We have described a new reference task - deciding scientific attribution, and demonstrated high human agreement (α &gt; 0.8) on this task. Our machine learning solution using shallow features achieves an agreement of αm = 0.68 with the human gold standard, increasing to αm = 0.71 if only pronouns need to be resolved. We have also demonstrated that information about scientific attribution improves results for a discourse classification task (Argumentative Zoning). We believe that similar improvements can be ach</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>S. Teufel and M. Moens. 2002. Summarising scientific articles — experiments with relevance an d rhetorical status. Computational Linguistics, 28(4):409–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>D Tidhar</author>
</authors>
<title>Automatic classification of citation function.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP-06,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1862" citStr="Teufel et al., 2006" startWordPosition="278" endWordPosition="281">c data about the frequency with which particular papers are cited. The success of citation indexers such as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citation. And automatic citation classification (Nanba and Okumura, 1999; Teufel et al., 2006) determines the function that a citation plays in the discourse. For such information access and retrieval purposes, the relevance of a citation within a paper is often crucial. One can estimate how important a citation is by simply counting how often it occurs in the paper. But as Kim and Webber (2006) argue, this ignores many expressions in text which refer to the cited author’s work but which are not as easy to recognise as citations. They address the resolution of instances of the third person personal pronoun “they” in astronomy papers: it can either refer to a citation or to some entitie</context>
<context position="24033" citStr="Teufel et al., 2006" startWordPosition="4056" endWordPosition="4059">ent on AZ from using automatic scientific attribution classification. so-called History feature) by running the classifier twice, and including the prediction for the previous sentence as a feature the second time. Due to practical considerations, we obtained our linguistic features using the RASP part of speech tagger (Briscoe and Carroll, 1995), when in previous work we used the IT TTT (Grover et al., 2000). We would not expect this to influence results much, however. Another difference is that we use around 1700 additional cue phrases acquired from previous work on another discourse task4 (Teufel et al., 2006). In addition to these features, we use four features obtained from the scientific attribution task described in this paper: Scientific Attribution Features: • Whether there is any reference to current work in the sentence • Whether there is any reference to any specific citation in the sentence • Whether there is any reference in the sentence to work that is in neither the current paper nor any specific citation • Which of these, if any, is in subject position Our aim is to explore whether these features obtained from the scientific attribution task influence machine learning performance on A</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Automatic classification of citation function. In Proc. of EMNLP-06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proc. of the 6th Message Understanding Conference,</booktitle>
<location>San Francisco.</location>
<contexts>
<context position="11653" citStr="Vilain et al., 1995" startWordPosition="1872" endWordPosition="1875">arkable, rather than a citation and the markable; similarly, in 3(b,c) we count instances of first person pronoun/“this paper”; for 2(e), we now calculate the distance of the closest citation instance. In short, the same features are used, but current work and citations are swapped. 5 Evaluation Metrics We consider two evaluation metrics. The first is the scoring system used for the co-reference task in the Message Understanding Conferences MUC-6 and MUC-7. The second is Krippendorff&apos;s α. We briefly discuss both below. 5.1 The MUC-6/MUC-7 Metric The MUC-6/MUC-7 Co-reference evaluation metric (Vilain et al., 1995) works by comparing co-reference classes across two annotated 318 files. Calling one annotation the “model” and the other the “system”, for each co-reference class S in the model, c(S) is the minimal number of co-reference links needed to generate the class (this is one less than the cardinality of the class; c(S) = JSJ − 1). m(S) is the number of “missing” links in the system annotation relative to the co-reference class as marked up in the model. In other words, this is the minimum number of co-reference links that need to be added to the system annotation to fully generate the co-reference </context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proc. of the 6th Message Understanding Conference, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="15268" citStr="Witten and Frank, 2000" startWordPosition="2561" endWordPosition="2564">number of times item j is classed as category k, nk is the number of times any item is classed as category k and dkk0 is the distance between categories k and k�. Like r., Krippendorff&apos;s a is 1 when there is perfect agreement, 0 when the observed agreement is only what was expected by chance, negative when observed agreement is less than expected by chance and positive when observed agreement is greater than expected by chance. 6 Intrinsic Evaluation Results We ran a machine learning experiment using 10-fold cross-validation and the memorybased learner IBk3 (with k=6), using the WEKA toolkit (Witten and Frank, 2000). The performance is shown in Tables 1 and 2. To position these results we compare them with three baseline lower bounds and the human performance upper bound in Table 3. We use three baselines: 3Memory based learning gave better results on this task than other learners (NB, HNB, IBk, J48, cf. § 7.3. 319 Paper Items α-N α-J α-D α-M %A* 0003055 446 .601 .606 .607 .610 85% 0005006 446 .670 .704 .711 .715 81% 0005015 462 .679 .696 .701 .706 81% 0005025 277 .707 .707 .707 .707 86% 0006011 393 .766 .771 .772 .775 88% 0006038 578 .551 .568 .573 .578 79% 0007035 393 .570 .590 .600 .609 90% 0008026 44</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>I. Witten and E. Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>