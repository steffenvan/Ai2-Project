<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000364">
<title confidence="0.987602">
Using the text to evaluate short answers for reading comprehension exercises
</title>
<author confidence="0.999624">
Andrea Horbach, Alexis Palmer and Manfred Pinkal
</author>
<affiliation confidence="0.999122">
Department of Computational Linguistics, Saarland University, Saarbr¨ucken, Germany
</affiliation>
<email confidence="0.997625">
(andrea|apalmer|pinkal)@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999666655172414">
Short answer questions for reading compre-
hension are a common task in foreign lan-
guage learning. Automatic short answer scor-
ing is the task of automatically assessing the
semantic content of a student’s answer, mark-
ing it e.g. as correct or incorrect. While pre-
vious approaches mainly focused on compar-
ing a learner answer to some reference an-
swer provided by the teacher, we explore the
use of the underlying reading texts as addi-
tional evidence for the classification. First, we
conduct a corpus study targeting the links be-
tween sentences in reading texts for learners of
German and answers to reading comprehen-
sion questions based on those texts. Second,
we use the reading text directly for classifi-
cation, considering three different models: an
answer-based classifier extended with textual
features, a simple text-based classifier, and a
model that combines the two according to con-
fidence of the text-based classification. The
most promising approach is the first one, re-
sults for which show that textual features im-
prove classification accuracy. While the other
two models do not improve classification ac-
curacy, they do investigate the role of the text
and suggest possibilities for developing auto-
matic answer scoring systems with less super-
vision needed from instructors.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933105263158">
Reading comprehension exercises are a common
means of assessment for language teaching: students
read a text in the language they are learning and are
then asked to answer questions about the text. The
types of questions asked of the learner may vary in
their scope and in the type of answers they are de-
signed to elicit; in this work we focus on “short an-
swer” responses, which are generally in the range of
1–3 sentences.
The nature of the reading comprehension task is
that the student is asked to show that he or she has
understood the text at hand. Questions focus on one
or more pieces of information from the text, and cor-
rect responses should contain the relevant semantic
content. In the language learning context, responses
classified as correct might still contain grammatical
or spelling errors; the focus lies on the content rather
than the form of the learner answer.
Automatic scoring of short answer responses to
reading comprehension questions is in essence a tex-
tual entailment task, with the additional complica-
tion that, in order to answer a question correctly, the
learner must have identified the right portion of the
text. It isn’t enough that a student answer is en-
tailed by some part of the reading text; it must be
entailed by the part of the text which is responsive
to the question under discussion.
Previous approaches to automatic short answer
scoring have seldom considered the reading text it-
self, instead comparing student answers to target an-
swers supplied by instructors; we will refer to these
as answer-based models. In this paper we explore
the role of the text for short answer scoring, evalu-
ating several models for considering the text in au-
tomatic scoring, and presenting results of an anno-
tation study regarding the semantic links between
reading texts and answers to reading comprehension
questions.
</bodyText>
<page confidence="0.955843">
286
</page>
<note confidence="0.957351">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 286–295, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.604819" genericHeader="introduction">
TEXT: SCHLOSS PILLNITZ
</sectionHeader>
<bodyText confidence="0.997817666666667">
This palace, which lies in the east of Dresden, is to me the most beautiful palace in the Dresden area. (... ) One special
attraction in the park is the camellia tree. In 1992, the camellia, which is more than 230 years old and 8.90 meters tall, got a
new, moveable home, in which temperature, ventilation, humidity, and shade are controlled by a climate regulation computer.
In the warm seasons, the house is rolled away from the tree. During the Blossom Time, from the middle of February until April,
the camellia has tens of thousands of crimson red blossoms. Every year, a limited number of shoots from the Pillnitz camellia
are sold during the Blossom Time, making it an especially worthwhile time to visit.
</bodyText>
<sectionHeader confidence="0.45684" genericHeader="method">
QUESTION:
</sectionHeader>
<bodyText confidence="0.459176">
A friend of yours would like to see the historic camellia tree. When should he go to Pillnitz, and why exactly at this time?
</bodyText>
<sectionHeader confidence="0.434973" genericHeader="method">
TARGET ANSWERS:
</sectionHeader>
<listItem confidence="0.982234142857143">
• From the middle of February until April is the Blossom Time.
• In spring the camellia has tens of thousands of crimson red blossoms.
LEARNER ANSWERS:
• [correct] He should go from the middle of February until April, because then the historic camellia hastens of thousands
of crimson red blossoms.
• [incorrect] Every year, a limited number of Pillnitz camellia are sold during the Blossom Time.
• [incorrect] All year round against temperature and humidity are controlled by a climate regulation computer.
</listItem>
<figureCaption confidence="0.999741">
Figure 1: Example of reading text with question and answers (translation by authors)
</figureCaption>
<bodyText confidence="0.999950791666667">
These investigations are done for German lan-
guage texts, questions, and answers. Figure 1 shows
a (translated) sample reading text, question, set of
target answers, and set of learner answers.
We show that the use of text-based features
improves classification performance over purely
answer-based models. We also show that a very sim-
ple text-based classifier, while it does not achieve
the same performance as the answer-based classifier,
does reach an accuracy of 76% for binary classifica-
tion (correct/incorrect) of student answers. The im-
plication of this for automatic scoring is that reason-
able results may be achievable with much less effort
on the part of instructors; namely, a classifier trained
on the supervision provided by marking the region
of a text relevant to a given question performs rea-
sonably well, though not as well as one trained on
full target answers.
The paper proceeds as follows: in Section 2 we
discuss the task and related approaches. In Sec-
tion 3, we describe our baseline model and the data
set we use. In Section 4 and Section 5 we discuss
our text-based models and present experiments and
results.
</bodyText>
<sectionHeader confidence="0.751376" genericHeader="method">
2 Approaches to short answer scoring
</sectionHeader>
<bodyText confidence="0.999888434782609">
In short answer scoring (SAS) the task is to auto-
matically assign labels to individual learner answers.
Those labels can either be binary, a value on some
scale of points or grades, or a more fine-grained di-
agnosis. For example, one fine-grained set of labels
(Bailey, 2008) classifies answers as (among others)
correct, as missing a necessary concept or concepts,
containing extra content, or as failing to answer the
question. Our present study is restricted to binary
classification.
Previous work on SAS, including early systems
like (Leacock and Chodorow, 2003; Pulman and
Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is
of course not only in the domain of foreign lan-
guage learning. For example, Mohler et al. (2011)
and Mohler and Mihalcea (2009) use semantic graph
alignments and semantic similarity measures to as-
sess student answers to computer science questions,
comparing them to sample solutions provided by a
teacher. Accordingly, not all SAS settings include
reading or other reference texts; many involve only
questions, target answers, and learner answers. Our
approach is relevant for scenarios in which some sort
</bodyText>
<page confidence="0.99336">
287
</page>
<bodyText confidence="0.99994678125">
of reference text is available.
The work we present here is strongly based on
approaches towards SAS by Meurers and colleagues
(Bailey and Meurers, 2008; Meurers et al., 2011a;
Meurers et al., 2011b; Ziai et al., 2012). Specifically,
the sentence alignment model described in Section 3
(and again discussed in Section 4) is modeled after
the one used by Meurers et al. to align target answers
and student answers.
Rather than using answers provided by instruc-
tors, Nielsen et al. (2008) represent target answers to
science questions as a set of hand-annotated facets,
i.e. important aspects of the answer, typically repre-
sented by a pair of words and the relation that con-
nects them. Student answers, and consequently stu-
dents’ understanding of target science concepts, are
then assessed by determining whether the relevant
facets are addressed by the learner answers.
Evaluating short answers on the basis of associ-
ated reading texts, as we do here, is a task related
to textual entailment. In the context of tutoring sys-
tems, Bethard et al. (2012) identify students’ mis-
conceptions of science concepts in essay writing us-
ing textual entailment techniques. They align stu-
dents’ writings to extracted science concepts in or-
der to identify misconceptions, using a similar ap-
proach to identify the correct underlying concept.
An excellent and more detailed overview of re-
lated work can be found in Ziai et al. (2012).
To our knowledge, there is no previous work that
uses reading texts as evidence for short answer scor-
ing in the context of foreign language learning.
</bodyText>
<sectionHeader confidence="0.996744" genericHeader="method">
3 Answer-based models
</sectionHeader>
<bodyText confidence="0.988488955555556">
In order to compare to previous work, we first im-
plement an alignment-based model following that
proposed in (Meurers et al., 2011b). We refer to
this class of models as answer-based because they
function by aligning learner answers to instructor-
supplied target answers along several different di-
mensions, discussed below. Answers are then clas-
sified as correct or incorrect on the basis of features
derived from these alignments.
Wherever possible/practical, we directly re-
implement the Meurers model for German data.
In this section we describe relevant aspects of the
Meurers model, along with modifications and exten-
sions in our implementation of that model.1
Preprocessing
We preprocess all material (learner answers, target
answers, questions and reading texts) using stan-
dard NLP tools for sentence splitting and tokeniza-
tion (both OpenNLP2), POS tagging and stemming
(both Treetagger (Schmid, 1994)), NP chunking
(OpenNLP), and dependency parsing (Zurich Parser
(Sennrich et al., 2009)). We use an NE Tagger
(Faruqui and Pad´o, 2010) to annotate named enti-
ties. Synonyms and semantic types are extracted
from GermaNet (Hamp and Feldweg, 1997).
For keywords, which serve to give more emphasis
to content words in the target answer, we extract all
nouns from the target answer.
Given that we are dealing with learner language,
but do not want to penalize answers for typical
learner errors, spellchecking (and subsequent cor-
rection of spelling errors) is especially important for
this task. Our approach is as follows: we first iden-
tify all words from the learner answers that are not
accepted by a German spellchecker (aspell3). We
then check for each word whether the word never-
theless occurs in the target answer, question or read-
ing text. If so, we accept it as correct. Otherwise, we
try to identify (using Levenshtein distance) which
word from the target answer, question, or reading
text is most likely to be the form intended by the
student.
Prior to alignment, we remove from the answer
all punctuation, stopwords (restricted to determiners
and auxiliaries), and material present in the question.
</bodyText>
<subsectionHeader confidence="0.826296">
Alignment
</subsectionHeader>
<bodyText confidence="0.958535636363636">
The alignment process in short answer scoring ap-
proximates determination of semantic equivalence
between target answer and learner answer. Dur-
ing alignment, we identify matches between an-
swer pairs on a number of linguistic levels: tokens,
chunks, and dependency triples.
On the token level, we consider a number of dif-
ferent metrics for identity between tokens, with each
1Some extensions were made in order to bring performance
of our re-implementation closer to the figures reported in previ-
ous work.
</bodyText>
<footnote confidence="0.9998835">
2http://opennlp.apache.org/index.html
3http://aspell.net/
</footnote>
<page confidence="0.995085">
288
</page>
<bodyText confidence="0.992671075">
metric associated with a certain alignment weight.
After weights have been determined for all possible
token pairs, the best applicable weight is used as in-
put for a traditional marriage alignment algorithm
(Gale and Shapley, 1962).
We use the following types of identity (id),
weighted in the following order:
token id &gt; lemma id &gt;
spelling id &gt; synonym &amp; NE id &gt;
similarity id&gt;
NE type, semantic type &amp; POS id
For synonym identity, we take a broad notion of
synonymy, extracting (from GermaNet) as potential
synonyms all words which are at most two levels (in
either direction) away from the target word. Simi-
larity identity is defined as two words having a Ger-
maNet path relatedness above some threshhold. In
order to have semantic type identity, two words must
have a common GermaNet hypernym (from a pre-
determined set of relevant hypernyms). Only some
closed-class words are eligible for POS identity. We
treat e.g. all types of determiners as POS identical.
Unlike, for example, alignment in machine trans-
lation, in which every token pair is considered a can-
didate for alignment, under the Meurers model only
candidates with at least one type of token identity are
available for alignment. This aims to prevent com-
pletely unrelated word pairs from being considered
for alignment.
In order to favor alignment of content words over
alignment of function words, and in departure from
the Meurers model, we use a content word multiplier
for alignment weights.
Chunks can only be aligned if at least one pair
of tokens within the respective chunks has been
aligned, and the percentage of aligned tokens be-
tween learner and target answer chunks is used as
input for the alignment process. Dependency triple
pairs are aligned when they share dependency rela-
tion, head lemma, and dependent lemma.
</bodyText>
<subsectionHeader confidence="0.970154">
Features and classifier
</subsectionHeader>
<bodyText confidence="0.999926260869565">
After answers have been aligned, the following fea-
tures are extracted as input for the classifier: key-
word overlap (percentage of aligned keywords), tar-
get token overlap (percentage of aligned target to-
kens), learner token overlap (percentage of aligned
learner tokens), token match (percentage of token
alignments that are token identical), lemma match,
synonym match, type match, target triple overlap,
learner triple overlap, target chunk overlap, learner
chunk overlap, target bigram overlap, learner bigram
overlap, target trigram overlap, learner trigram over-
lap, and variety of alignment (number of different
token alignment types).
The n-gram features are the only new features in
our re-implementation of the Meurers model, hoping
to capture the influence of linear ordering of aligned
tokens. These features did not in the end improve
the model’s performance.
For classification, we use the timbl toolkit (Daele-
mans et al., 2009) for k-nearest neighbors classi-
fication. We treat all features as numeric values
and evaluate performance via leave-one-out cross-
validation. Further details appear in Section 5.
</bodyText>
<subsectionHeader confidence="0.879497">
Data
</subsectionHeader>
<bodyText confidence="0.999929">
For all work reported in this paper, we use the Ger-
man CREG corpus (Ott et al., 2012) of short answers
to questions for reading comprehension tasks. More
specifically, we use a balanced subset of the CREG
corpus containing a total of 1032 learner answers.
This corpus consists of 30 reading texts with an av-
erage of 5.9 questions per text. Each question is as-
sociated with one or more target answers, specified
by a teacher. For each question in turn there are an
average of 5.8 learner answers, each manually anno-
tated according to both binary and fine-grained la-
beling schemes. When there are several target an-
swers for a question, the best target answer for each
learner answer is indicated.
</bodyText>
<sectionHeader confidence="0.991681" genericHeader="method">
4 Text-based approach
</sectionHeader>
<bodyText confidence="0.999943272727273">
Previous approaches to this task take the instructor-
supplied target answer(s) as a sort of supervision;
the target answer is meant to indicate the seman-
tic content necessary for a correct student answer.
Alignment between student answer and target an-
swer is then taken as a way of approximating se-
mantic equivalence. The key innovation of the cur-
rent study is to incorporate the reading text into the
evaluation of student answers. In this section we de-
scribe and evaluate three approaches to incorporat-
ing the text. The aim is to consider the semantic
</bodyText>
<page confidence="0.990815">
289
</page>
<bodyText confidence="0.9997862">
relationships between target answer, learner answer,
and the text itself.4
A target answer is in fact just one way of ex-
pressing the requisite semantic content. Teachers
who create such exercises are obviously looking at
the text while creating target answers, and target an-
swers are often paraphrases of one or more sentences
of the reading text. Some learner answers which are
scored as incorrect by the answer-based system may
in fact be variant expressions of the same semantic
content as the target answer. Due to the nature of the
reading comprehension task, in which students are
able to view the text while answering questions, we
might expect students to express things in a man-
ner similar to the text. This is especially true for
language learners, as they are likely to have a lim-
ited range of options both for lexical expression and
grammatical constructions.
Along similar lines, one potential source of incor-
rect answers is an inability on the part of the stu-
dent to correctly identify the portion of the text that
is relevant to the question at hand. Our hypothesis
therefore is that a learner answer which links to the
same portion of the reading text as the target answer
is likely to be a correct answer. Similarly, a learner
answer which closely matches some part of the text
that is not related to the target answer is likely to be
incorrect.
Our text-based models investigate this hypothesis
in several different ways, described in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.998914">
4.1 Annotation study
</subsectionHeader>
<bodyText confidence="0.999977461538461">
The CREG data includes questions, learner answers,
target answers, and reading texts; associations be-
tween text and answers are not part of the anno-
tations. We undertook an annotation project in or-
der to have gold-standard source sentences for both
learner and target answers. This gold-standard is
then used to inform the text-based models described
below.
After removing a handful of problematic ques-
tions and their associated answers, we acquired hu-
man annotations for 889 of the 1032 learner an-
swers from the balanced subset of the CREG cor-
pus, in addition to 294 target answers. Each answer
</bodyText>
<footnote confidence="0.9346885">
4In future work we will also consider semantic relationships
between the question and the text.
</footnote>
<bodyText confidence="0.999977083333333">
was labeled separately by two (of three) annotators,
who were given the reading text and the question
and asked to identify the single best source sentence
from the text. Annotators were not told whether any
given instance was a target or learner answer, nor
whether learner answers were correct or incorrect.
Although we expected most answers to corre-
spond directly to a single text passage (Meurers et
al., 2011b), annotators were asked to look for (and
annotate appropriately) two different conditions in
which more than one source sentence may be rele-
vant. We refer to these as the repeated content con-
dition and the distributed content condition.
In the repeated content condition, the same se-
mantic content may be fully represented in more
than one sentence from the original text. In such
cases, we would expect the text to contain sen-
tences that are paraphrases or near-paraphrases of
one another. The distributed content condition oc-
curs when the relevant semantic content spans multi-
ple sentences, and some degree of synthesis or even
inference may be required to arrive at the answer.
Annotators were instructed to assume that pronouns
had been resolved; in other words, a sentence should
not be considered necessary semantic content sim-
ply because it contains the NP to which a pronoun in
another sentence refers. For both of these multiple-
sentence conditions, annotators were asked to select
one single-best source sentence from among the set
and also to mark the alternative source sentences.
For 31.2% of the answers annotated, one or more
annotator provided more than one possible source
sentence. Upon closer inspection, though, the an-
notations for these conditions are not very consis-
tent. In the repeated content condition, there is very
little agreement between annotators regarding when
the text contains more than one full-sentence source
for the answer. In the distributed content condition,
sometimes annotators disagree on the primary sen-
tence, and in many instances, one annotator identi-
fied multiple sentences and the other only one. Due
to these inconsistencies, for the purpose of this study
we decided to treat the multiple-sentence conditions
in an underspecified fashion. When an annotator has
identified either of these conditions, we convert the
annotations to a single-best sentence and a set of al-
ternatives.
The annotations were processed to automatically
</bodyText>
<page confidence="0.966454">
290
</page>
<table confidence="0.9998044">
Answer type agree altagree disagree nolink
Learner answers (all) 70.3% 9.4% 16.9% 3.4%
Learner answers (correct) 75.1% 11.7% 12.7% 0.5%
Learner answers (incorrect) 65.9% 7.3% 20.7% 6.4%
Target 73.1% 8.1% 17.3% 1.4%
</table>
<tableCaption confidence="0.999946">
Table 1: Inter-annotator agreement for linking answers to source sentences in text
</tableCaption>
<bodyText confidence="0.97444970212766">
produce a gold-standard set of source sentence IDs,
indicating the single sentence in the reading text to
which each answer is most closely linked. We iden-
tify four distinct categories with respect to agree-
ment between annotators. Agreement figures appear
in Table 1.
** agree: In this case, both annotators linked the
answer to the same source sentence, and that sen-
tence is identified as the gold-standard link to the
answer.
** altagree: This category covers two different
situations in which the two annotators fail to agree
on the single-best sentence. First, there are cases in
which the best sentence selected by one annotator
is a member of the set of alternatives indicated by
the other. Second, in a small number of cases, both
annotators agree on one member of the set of alter-
natives. In other words, the single sentence in the
intersection of the sets of sentences identified by the
two annotators is taken as the gold-standard annota-
tion. There was no (non-agree) case in which that
intersection contained more than one sentence.
** disagree: This category also includes two dif-
ferent types of cases. In the first, one of the two
annotators failed to identify a source sentence to
link with the answer. In that case, we consider
the annotators to be in disagreement, and for the
gold-standard we use the sentence ID provided by
the one responding annotator. In the second case,
the annotators disagree on the single-best sentence
and there is no overlap between indicated alterna-
tive sentences. In those cases, for the gold standard
we choose from the two source sentences that which
appears first in the reading text.5
** nolink: For a small number of answers (n=34),
5This is a relatively arbitrary decision motivated by the de-
sire to have a source sentence associated with as many answers
as possible. Future work may include adjudication of annota-
tions to reduce the noise introduced to the gold standard by this
category of responses.
both annotators found no link to the text. One ex-
ample of such a case is an answer given entirely in
English. For these cases, the gold standard provides
no best source sentence.
If we consider both altagree and nolink to be
forms of agreement, interannotator agreement is
about 74% for both learner and target answers.
</bodyText>
<subsectionHeader confidence="0.915273">
4.2 Text-based models
</subsectionHeader>
<bodyText confidence="0.999798">
In this paper we consider two different models for
incorporating the reading text into automatic short
answer scoring. In the first approach, we employ
a purely text-based model. The second combines
either text-based features or the text-based model
with the answer-based model described in Section 3.
Evaluation of all three approaches appears in Sec-
tion 5.
</bodyText>
<subsectionHeader confidence="0.843135">
4.2.1 Simple text-based model
</subsectionHeader>
<bodyText confidence="0.999523952380952">
This model classifies student answers by compar-
ing the source sentence most closely associated with
the student answer to that associated with the tar-
get answer. If the two sentences are identical, the
answer is classified as correct, and otherwise as in-
correct.
We consider both the annotated best sentences
(goldlink) and automatically-identified answer-
sentence pairs (autolink). For automatic identifica-
tion, we use the alignment model described in Sec-
tion 3 to identify the best matching source sentence
in the text for both learner and target answers. We
use the token alignment process to align a given an-
swer with each sentence from its respective reading
text; the best-matching source sentence is that with
the highest alignment weight. Chunk alignments are
used only for correction of token alignments, and
dependency alignments are not considered.
This model takes an extremely simple approach to
answer classification, and could certainly be refined
and improved. At the same time, its relatively strong
</bodyText>
<page confidence="0.994061">
291
</page>
<bodyText confidence="0.9996676">
performance (see Table 3) suggests that the mini-
mal level of supervision offered by teachers simply
marking the sentence of a text most relevant to a
given reading comprehension question may be ben-
eficial for automatic answer scoring.
</bodyText>
<subsectionHeader confidence="0.954453">
4.2.2 Combining text-based and answer-based
models
</subsectionHeader>
<bodyText confidence="0.996728583333333">
In addition to the purely text-based model, we ex-
plore two ways of combining text- and answer-based
models.
Textual features in the answer-based model In
the first, we extract four features from the align-
ments between answers and source sentences and in-
corporate these as additional features in the answer-
based model.
Features 1, 3, and 4 are each computed in two
versions, using source sentences from either the an-
notated gold standard (goldlink), or the alignment
model (autolink).
</bodyText>
<listItem confidence="0.89405005882353">
1. SourceAgree This boolean feature is true if
both learner and target answer link to the same
source sentence, and false otherwise (also if no
source sentence was annotated or automatically
found).
2. SourceEntropy For this feature we look at the
two most-likely source sentences for the learner
answer, as determined by automatic alignment
scores. We treat the alignment weights as
probabilities, normalizing so that they sum up
to one. We then take the entropy between
these two alignment weights as indicative of the
confidence of the automatic alignment for the
learner answer.
3. AgreeEntropy Here we weight the first feature
according to the second, taking the entropy as a
confidence score for the binary feature. Specif-
</listItem>
<bodyText confidence="0.915248285714286">
ically, we value SourceAgree at 0.5 when the
feature is true, −0.5 when false, and multiply
this with (1 − entropy).
4. TextAdjacency This feature captures the dis-
tance (in number of sentences) between the
source sentence linked to the learner answer
and that linked to the target answer. With this
feature we aim to capture the tendency of adja-
cent passages in a text to exhibit topical coher-
ence (Mirkin et al., 2010).
Classifier combination In the second approach,
we combine the output of the answer-based and
text-based classifiers to arrive at a final classifica-
tion system, allowing the text-based classifier to pre-
dominate in those cases for which it is most con-
fident and falling back to the answer-based classi-
fier for other cases. Confidence of the text-based
classifier is determined based on entropy of the two
highest-scoring alignments between learner answer
and source sentence. The entropy threshhold was
determined empirically to 0.5.
</bodyText>
<sectionHeader confidence="0.979792" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.999952166666667">
This section discusses experiments on short an-
swer scoring (binary classification) for German, in
the context of reading comprehension for language
learning. Specifically, we investigate the text-based
models described in Section 4.2. In all cases, fea-
tures and parameter settings were tuned on a de-
velopment set which was extracted from the larger
CREG corpus. In other words, there is no over-
lap between test and development data. For test-
ing, we perform leave-one-out cross-validation on
the slightly-smaller subset of the corpus which was
used for annotation.
</bodyText>
<subsectionHeader confidence="0.997955">
5.1 Answer-based baseline
</subsectionHeader>
<bodyText confidence="0.997777928571429">
As a baseline for our text-based models we take
our implementation of the answer-based model from
(Meurers et al., 2011b). As previously mentioned,
our implementation diverges from theirs at some
points, and we do not quite reach the performance
reported for their model (accuracy of 84.6% on the
balanced CREG corpus) and are far from reaching
the current state of the art accuracy of 86.3%, as re-
ported in Hahn and Meurers (2012).
Our answer-based model appears as baseline in
Table 2. During development, the one extension to
the baseline which helped most was the use of ex-
tended synonyms. This variant of the model appears
in the results table with the annotation +syn.
</bodyText>
<page confidence="0.990717">
292
</page>
<table confidence="0.999753333333333">
model k=5 k=15 k=30
baseline 0.817 0.820 0.822
baseline+syn 0.822 0.826 0.825
text: goldlink 0.827 0.827 0.829
text+syn:goldlink 0.830 0.835* 0.837*
text:autolink 0.837* 0.836* 0.825
text+syn:autolink 0.844* 0.836* 0.832
combined 0.810 0.819 0.816
combined+syn 0.817 0.822 0.818
</table>
<tableCaption confidence="0.952227555555555">
Table 2: Classification accuracy for answer-based base-
line (baseline), answer-based plus textual features (text),
and classifier combination (combined). +syn indicates
expanded synonymy features, goldlink indicates identi-
fying the source sentences via annotated links, autolink
indicates determining source sentences using the align-
ment model, k=number of neighbors. Results marked
with * are significant compared to the best baseline
model. See Section 5.2.1 for details.
</tableCaption>
<subsectionHeader confidence="0.994402">
5.2 Text-based models
</subsectionHeader>
<bodyText confidence="0.999678166666667">
As described in Section 4.2, we consider three dif-
ferent approaches for incorporating the reading text
into answer classification: use of textual features
in the answer-based model, combination of separate
answer-based and text-based models, and a simple
text-based classifier.
</bodyText>
<subsectionHeader confidence="0.9900055">
5.2.1 Combining text-based and answer-based
models
</subsectionHeader>
<bodyText confidence="0.977914941176471">
We explore two ways of combining text- and
answer-based models.
Adding textual features to the answer-based
model
We evaluate the contribution of the four new text-
based features, computed in two variations: with
source sentences as they are identified in the gold
standard (goldlink) and as they are computed using
the alignment model (autolink). We add those ad-
ditional features to the two answer-based systems:
the baseline (text) and the baseline with extended
synonym set (text+syn). Results are presented in
Table 2.
We present results for using the 5, 15, and 30 near-
est neighbors for classification, as the influence of
various features changes with the number of neigh-
bors. We calculate the significance for the difference
</bodyText>
<table confidence="0.99841575">
autolink goldlink alt-set
Accuracy 0.762 0.722 0.747
P correct 0.805 0.781 0.753
R correct 0.667 0.585 0.702
F correct 0.729 0.668 0.727
P incorrect 0.735 0.689 0.742
R incorrect 0.851 0.849 0.788
F incorrect 0.789 0.761 0.764
</table>
<tableCaption confidence="0.989924">
Table 3: Classification accuracy, precision, recall, and f-
score for simple text-based classifier, under three differ-
ent conditions. See Section 5.2.2 for details.
</tableCaption>
<bodyText confidence="0.9993754">
between the best baseline model (0.826) and each
model which uses textual features, using a resam-
pling test (Edgington, 1986). The results marked
with a * in the Table 2 are significant at p &lt; 0.01.
Although the impact of the textual features is
clearly not as big with a stronger baseline model,
we still see a clear pattern of improved accuracy.
We may expect this difference to increase with more
data and with additional and/or improved text-based
features.
</bodyText>
<subsectionHeader confidence="0.937564">
Classifier combination
</subsectionHeader>
<bodyText confidence="0.9998645">
Combining the two classifiers (answer-based and
text-based) according to confidence levels results in
decreased performance compared to the baseline.
These results appear in Table 2 as combined.
</bodyText>
<subsectionHeader confidence="0.608478">
5.2.2 Simple text-based classification
</subsectionHeader>
<bodyText confidence="0.999864882352941">
We have seen that textual features improve clas-
sification accuracy over the answer-driven model,
yet this approach still requires the supervision pro-
vided by teacher-supplied target answers. In our
third model, we investigate how the system performs
without this degree of supervision, considering how
far we can get by using only the text.
The simple text-based classifier, rather than tak-
ing a feature-based approach to classification, bases
its decision solely on whether or not the learner and
target answers link to the same source sentence. We
compare three different methods for obtaining these
links. The first approach (autolink) automatically
links each answer to a source sentence from the
text, based on alignments as described in Section 3.
The second (goldlink) uses links as provided by the
gold standard; in this case, learner answers without
</bodyText>
<page confidence="0.9943">
293
</page>
<bodyText confidence="0.9999065">
a linked sentence (e.g. nolink cases) are immedi-
ately classified as incorrect. The third approach (alt-
set) exploits that fact that in many cases annotators
provided alternate source sentences. Under this ap-
proach, an answer is classified as correct provided
that there is a non-empty intersection between the
set of possible source sentences for the learner an-
swer and that for the target answer. For the second
and third approaches, we classify as incorrect those
learner answers lacking a gold-standard annotation
for the corresponding target answer.
In Table 3 we present classification accuracy, pre-
cision, recall, and f-score for the three different con-
ditions. Precision, recall, and f-score are reported
separately for correct and incorrect learner answers.
The 76% accuracy reached using the simple text-
based classifier suggests that a system which has
teachers supply source sentences instead of target
answers and then automatically aligns learner an-
swers to the text, while nowhere near comparable to
the state-of-the-art supervised system, still achieves
a reasonably accurate classification.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99976578125">
In this paper we have presented the first use of
reading texts for automatic short answer scoring in
the context of foreign language learning. We show
that, for German, the use of simple text-based fea-
tures improves classification accuracy over purely
answer-based models. We plan in the future to inves-
tigate a wider range of text-based features. We have
also shown that a simple classification model based
only on linking answers to source sentences in the
text achieves a reasonable classification accuracy.
This finding has the potential to reduce the amount
of teacher supervision necessary for authoring short
answer exercises within automatic answer scoring
systems. In addition to these findings, we have pre-
sented the results of an annotation study linking both
target and learner answers to source sentences.
In the near-term future we plan to further inves-
tigate the role of the reading text for short answer
scoring along three lines. First, we will address the
question of the best size of text unit for alignment.
In many cases, the best answers are linked not to
entire sentences but to regions of sentences; in oth-
ers, answers may correspond to more than one sen-
tence. Our current approach ignores this issue. Sec-
ond, we are interested in the variety of semantic re-
lationships holding between questions, answers and
texts. Along these lines, we will further investigate
the sets of alternatives provided by annotators, as
well as bringing in notions from work on paraphras-
ing and recognizing textual entailment. Finally, we
are interested in moving from simple binary classi-
fication to the fine-grained level of diagnosis.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990787666666667">
We would like to thank Erik Hahn, David Alejandro
Przybilla and Jonas Sunde for carrying out the an-
notations. We thank the three anonymous reviewers
for their helpful comments. This work was funded
by the Cluster of Excellence “Multimodal Comput-
ing and Interaction” of the German Excellence Ini-
tiative and partially funded through the INTERREG
IV A programme project ALLEGRO (Project No.:
67 SMLW 11137).
</bodyText>
<sectionHeader confidence="0.997233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996635178571429">
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading comprehen-
sion questions. In Proceedings of the Third Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 107–115, Columbus, Ohio, June.
Stacey Bailey. 2008. Content Assessment in Intelligent
Computer-Aided Language Learning: Meaning Error
Diagnosis for English as a Second Language. Ph.D.
thesis, The Ohio State University.
Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James H.
Martin, Md. Arafat Sultan, and Tamara Sumner. 2012.
Identifying science concepts and student misconcep-
tions in an interactive essay writing tutor. In Proceed-
ings of the Seventh Workshop on Building Educational
Applications Using NLP, pages 12–21.
Walter Daelemans, Jakub Zavrel, Ko Sloot, and Antal
Van Den Bosch. 2009. TiMBL: Tilburg Memory-
Based Learner, version 6.2, Reference Guide. ILK
Technical Report 09-01.
Eugene S Edgington. 1986. Randomization tests. Mar-
cel Dekker, Inc., New York, NY, USA.
Manaal Faruqui and Sebastian Pad´o. 2010. Training and
evaluating a German named entity recognizer with se-
mantic generalization. In Proceedings of KONVENS
2010, Saarbr¨ucken, Germany.
David Gale and Lloyd S. Shapley. 1962. College ad-
missions and the stability of marriage. The American
Mathematical Monthly, 69(1):9–15.
</reference>
<page confidence="0.976784">
294
</page>
<reference confidence="0.999799860759493">
Michael Hahn and Detmar Meurers. 2012. Evaluat-
ing the meaning of answers to reading comprehension
questions: A semantics-based approach. In Proceed-
ings of the 7th Workshop on Innovative Use of NLP
for Building Educational Applications (BEA7), pages
326–336, Montreal, Canada. Association for Compu-
tational Linguistics.
Birgit Hamp and Helmut Feldweg. 1997. Germanet - a
lexical-semantic net for German. In In Proceedings of
ACL workshop Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP Ap-
plications, pages 9–15.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389–405.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bai-
ley. 2011a. Integrating parallel analysis modules to
evaluate the meaning of answers to reading compre-
hension questions. Special Issue on Free-text Auto-
matic Evaluation. International Journal of Continu-
ing Engineering Education and Life-Long Learning
(IJCEELL), 21(4):355–369.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina
Kopp. 2011b. Evaluating answers to reading com-
prehension questions in context: Results for German
and the role of information structure. In Proceedings
of the TextInfer 2011 Workshop on Textual Entailment,
pages 1–9, Edinburgh, Scottland, UK.
Shachar Mirkin, Ido Dagan, and Sebastian Pad´o. 2010.
Assessing the role of discourse references in entail-
ment inference. In ACL.
Michael Mohler and Rada Mihalcea. 2009. Text-to-text
semantic similarity for automatic short answer grad-
ing. In Alex Lascarides, Claire Gardent, and Joakim
Nivre, editors, EACL, pages 567–575.
Michael Mohler, Razvan C. Bunescu, and Rada Mihal-
cea. 2011. Learning to grade short answer questions
using semantic similarity measures and dependency
graph alignments. In Dekang Lin, Yuji Matsumoto,
and Rada Mihalcea, editors, ACL, pages 752–762.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Learning to assess low-level conceptual under-
standing. In David Wilson and H. Chad Lane, editors,
FLAIRS Conference, pages 427–432.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Cre-
ation and analysis of a reading comprehension exer-
cise corpus: Towards evaluating meaning in context.
In Thomas Schmidt and Kai W¨orner, editors, Mul-
tilingual Corpora and Multilingual Corpus Analysis,
Hamburg Studies in Multilingualism (HSM), pages
47–69. Benjamins, Amsterdam.
Stephen G. Pulman and Jana Z. Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
second workshop on Building Educational Applica-
tions Using NLP, EdAppsNLP 05, pages 9–16.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, United Kingdom.
Rico Sennrich, Gerold Schneider, Martin Volk, and Mar-
tin Warin. 2009. A new hybrid dependency parser
for German. In Christian Chiarcos, Richard Eckart
de Castilho, and Manfred Stede, editors, Von der Form
zur Bedeutung: Texte automatisch verarbeiten ? From
Form to Meaning: Processing Texts Automatically.
Proceedings of the Biennial GSCL Conference 2009,
pages 115–124. Narr, T¨ubingen.
Jana Z. Sukkarieh and Stephen G. Pulman. 2005. In-
formation extraction and machine learning: Auto-
marking short free text responses to science questions.
In Chee-Kit Looi, Gordon I. McCalla, Bert Bredeweg,
and Joost Breuker, editors, AIED, volume 125 of Fron-
tiers in Artificial Intelligence and Applications, pages
629–637.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012.
Short answer assessment: Establishing links between
research strands. In Proceedings of the 7th Workshop
on Innovative Use of NLP for Building Educational
Applications (BEA7), Montreal, Canada.
</reference>
<page confidence="0.998563">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934876">
<title confidence="0.99996">Using the text to evaluate short answers for reading comprehension exercises</title>
<author confidence="0.999953">Andrea Horbach</author>
<author confidence="0.999953">Alexis Palmer</author>
<author confidence="0.999953">Manfred Pinkal</author>
<affiliation confidence="0.998881">Department of Computational Linguistics, Saarland University, Saarbr¨ucken,</affiliation>
<email confidence="0.992069">(andrea|apalmer|pinkal)@coli.uni-saarland.de</email>
<abstract confidence="0.9981007">Short answer questions for reading comprehension are a common task in foreign language learning. Automatic short answer scoring is the task of automatically assessing the semantic content of a student’s answer, marking it e.g. as correct or incorrect. While previous approaches mainly focused on comparing a learner answer to some reference answer provided by the teacher, we explore the use of the underlying reading texts as additional evidence for the classification. First, we conduct a corpus study targeting the links between sentences in reading texts for learners of German and answers to reading comprehension questions based on those texts. Second, we use the reading text directly for classification, considering three different models: an answer-based classifier extended with textual features, a simple text-based classifier, and a model that combines the two according to confidence of the text-based classification. The most promising approach is the first one, results for which show that textual features improve classification accuracy. While the other two models do not improve classification accuracy, they do investigate the role of the text and suggest possibilities for developing automatic answer scoring systems with less supervision needed from instructors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stacey Bailey</author>
<author>Detmar Meurers</author>
</authors>
<title>Diagnosing meaning errors in short answers to reading comprehension questions.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>107--115</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7587" citStr="Bailey and Meurers, 2008" startWordPosition="1222" endWordPosition="1225">reign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target </context>
</contexts>
<marker>Bailey, Meurers, 2008</marker>
<rawString>Stacey Bailey and Detmar Meurers. 2008. Diagnosing meaning errors in short answers to reading comprehension questions. In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications, pages 107–115, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stacey Bailey</author>
</authors>
<title>Content Assessment in Intelligent Computer-Aided Language Learning: Meaning Error Diagnosis for English as a Second Language.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>The Ohio State University.</institution>
<contexts>
<context position="6573" citStr="Bailey, 2008" startWordPosition="1067" endWordPosition="1068">as well as one trained on full target answers. The paper proceeds as follows: in Section 2 we discuss the task and related approaches. In Section 3, we describe our baseline model and the data set we use. In Section 4 and Section 5 we discuss our text-based models and present experiments and results. 2 Approaches to short answer scoring In short answer scoring (SAS) the task is to automatically assign labels to individual learner answers. Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, </context>
</contexts>
<marker>Bailey, 2008</marker>
<rawString>Stacey Bailey. 2008. Content Assessment in Intelligent Computer-Aided Language Learning: Meaning Error Diagnosis for English as a Second Language. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arafat Sultan</author>
<author>Tamara Sumner</author>
</authors>
<title>Identifying science concepts and student misconceptions in an interactive essay writing tutor.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>12--21</pages>
<marker>Sultan, Sumner, 2012</marker>
<rawString>Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James H. Martin, Md. Arafat Sultan, and Tamara Sumner. 2012. Identifying science concepts and student misconceptions in an interactive essay writing tutor. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 12–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko Sloot</author>
<author>Antal Van Den Bosch</author>
</authors>
<title>TiMBL: Tilburg MemoryBased Learner, version 6.2, Reference Guide.</title>
<date>2009</date>
<tech>ILK Technical Report 09-01.</tech>
<marker>Daelemans, Zavrel, Sloot, Van Den Bosch, 2009</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko Sloot, and Antal Van Den Bosch. 2009. TiMBL: Tilburg MemoryBased Learner, version 6.2, Reference Guide. ILK Technical Report 09-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene S Edgington</author>
</authors>
<title>Randomization tests.</title>
<date>1986</date>
<publisher>Marcel Dekker, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="30600" citStr="Edgington, 1986" startWordPosition="4915" endWordPosition="4916">ious features changes with the number of neighbors. We calculate the significance for the difference autolink goldlink alt-set Accuracy 0.762 0.722 0.747 P correct 0.805 0.781 0.753 R correct 0.667 0.585 0.702 F correct 0.729 0.668 0.727 P incorrect 0.735 0.689 0.742 R incorrect 0.851 0.849 0.788 F incorrect 0.789 0.761 0.764 Table 3: Classification accuracy, precision, recall, and fscore for simple text-based classifier, under three different conditions. See Section 5.2.2 for details. between the best baseline model (0.826) and each model which uses textual features, using a resampling test (Edgington, 1986). The results marked with a * in the Table 2 are significant at p &lt; 0.01. Although the impact of the textual features is clearly not as big with a stronger baseline model, we still see a clear pattern of improved accuracy. We may expect this difference to increase with more data and with additional and/or improved text-based features. Classifier combination Combining the two classifiers (answer-based and text-based) according to confidence levels results in decreased performance compared to the baseline. These results appear in Table 2 as combined. 5.2.2 Simple text-based classification We hav</context>
</contexts>
<marker>Edgington, 1986</marker>
<rawString>Eugene S Edgington. 1986. Randomization tests. Marcel Dekker, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Training and evaluating a German named entity recognizer with semantic generalization.</title>
<date>2010</date>
<booktitle>In Proceedings of KONVENS</booktitle>
<location>Saarbr¨ucken, Germany.</location>
<marker>Faruqui, Pad´o, 2010</marker>
<rawString>Manaal Faruqui and Sebastian Pad´o. 2010. Training and evaluating a German named entity recognizer with semantic generalization. In Proceedings of KONVENS 2010, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Gale</author>
<author>Lloyd S Shapley</author>
</authors>
<title>College admissions and the stability of marriage.</title>
<date>1962</date>
<journal>The American Mathematical Monthly,</journal>
<volume>69</volume>
<issue>1</issue>
<contexts>
<context position="11942" citStr="Gale and Shapley, 1962" startWordPosition="1911" endWordPosition="1914">es between answer pairs on a number of linguistic levels: tokens, chunks, and dependency triples. On the token level, we consider a number of different metrics for identity between tokens, with each 1Some extensions were made in order to bring performance of our re-implementation closer to the figures reported in previous work. 2http://opennlp.apache.org/index.html 3http://aspell.net/ 288 metric associated with a certain alignment weight. After weights have been determined for all possible token pairs, the best applicable weight is used as input for a traditional marriage alignment algorithm (Gale and Shapley, 1962). We use the following types of identity (id), weighted in the following order: token id &gt; lemma id &gt; spelling id &gt; synonym &amp; NE id &gt; similarity id&gt; NE type, semantic type &amp; POS id For synonym identity, we take a broad notion of synonymy, extracting (from GermaNet) as potential synonyms all words which are at most two levels (in either direction) away from the target word. Similarity identity is defined as two words having a GermaNet path relatedness above some threshhold. In order to have semantic type identity, two words must have a common GermaNet hypernym (from a predetermined set of relev</context>
</contexts>
<marker>Gale, Shapley, 1962</marker>
<rawString>David Gale and Lloyd S. Shapley. 1962. College admissions and the stability of marriage. The American Mathematical Monthly, 69(1):9–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hahn</author>
<author>Detmar Meurers</author>
</authors>
<title>Evaluating the meaning of answers to reading comprehension questions: A semantics-based approach.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications (BEA7),</booktitle>
<pages>326--336</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Canada.</location>
<contexts>
<context position="28003" citStr="Hahn and Meurers (2012)" startWordPosition="4526" endWordPosition="4529">between test and development data. For testing, we perform leave-one-out cross-validation on the slightly-smaller subset of the corpus which was used for annotation. 5.1 Answer-based baseline As a baseline for our text-based models we take our implementation of the answer-based model from (Meurers et al., 2011b). As previously mentioned, our implementation diverges from theirs at some points, and we do not quite reach the performance reported for their model (accuracy of 84.6% on the balanced CREG corpus) and are far from reaching the current state of the art accuracy of 86.3%, as reported in Hahn and Meurers (2012). Our answer-based model appears as baseline in Table 2. During development, the one extension to the baseline which helped most was the use of extended synonyms. This variant of the model appears in the results table with the annotation +syn. 292 model k=5 k=15 k=30 baseline 0.817 0.820 0.822 baseline+syn 0.822 0.826 0.825 text: goldlink 0.827 0.827 0.829 text+syn:goldlink 0.830 0.835* 0.837* text:autolink 0.837* 0.836* 0.825 text+syn:autolink 0.844* 0.836* 0.832 combined 0.810 0.819 0.816 combined+syn 0.817 0.822 0.818 Table 2: Classification accuracy for answer-based baseline (baseline), an</context>
</contexts>
<marker>Hahn, Meurers, 2012</marker>
<rawString>Michael Hahn and Detmar Meurers. 2012. Evaluating the meaning of answers to reading comprehension questions: A semantics-based approach. In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications (BEA7), pages 326–336, Montreal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>Germanet - a lexical-semantic net for German. In</title>
<date>1997</date>
<booktitle>In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>9--15</pages>
<contexts>
<context position="10173" citStr="Hamp and Feldweg, 1997" startWordPosition="1633" endWordPosition="1636">section we describe relevant aspects of the Meurers model, along with modifications and extensions in our implementation of that model.1 Preprocessing We preprocess all material (learner answers, target answers, questions and reading texts) using standard NLP tools for sentence splitting and tokenization (both OpenNLP2), POS tagging and stemming (both Treetagger (Schmid, 1994)), NP chunking (OpenNLP), and dependency parsing (Zurich Parser (Sennrich et al., 2009)). We use an NE Tagger (Faruqui and Pad´o, 2010) to annotate named entities. Synonyms and semantic types are extracted from GermaNet (Hamp and Feldweg, 1997). For keywords, which serve to give more emphasis to content words in the target answer, we extract all nouns from the target answer. Given that we are dealing with learner language, but do not want to penalize answers for typical learner errors, spellchecking (and subsequent correction of spelling errors) is especially important for this task. Our approach is as follows: we first identify all words from the learner answers that are not accepted by a German spellchecker (aspell3). We then check for each word whether the word nevertheless occurs in the target answer, question or reading text. I</context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. Germanet - a lexical-semantic net for German. In In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 9–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>C-rater: Automated scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="6864" citStr="Leacock and Chodorow, 2003" startWordPosition="1108" endWordPosition="1111">xperiments and results. 2 Approaches to short answer scoring In short answer scoring (SAS) the task is to automatically assign labels to individual learner answers. Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is availabl</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>Claudia Leacock and Martin Chodorow. 2003. C-rater: Automated scoring of short-answer questions. Computers and the Humanities, 37(4):389–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detmar Meurers</author>
<author>Ramon Ziai</author>
<author>Niels Ott</author>
<author>Stacey Bailey</author>
</authors>
<title>Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions. Special Issue on Free-text Automatic Evaluation.</title>
<date>2011</date>
<journal>International Journal of Continuing Engineering Education and Life-Long Learning (IJCEELL),</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="7609" citStr="Meurers et al., 2011" startWordPosition="1226" endWordPosition="1229">or example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target science concepts, are </context>
<context position="9153" citStr="Meurers et al., 2011" startWordPosition="1480" endWordPosition="1483">cepts in essay writing using textual entailment techniques. They align students’ writings to extracted science concepts in order to identify misconceptions, using a similar approach to identify the correct underlying concept. An excellent and more detailed overview of related work can be found in Ziai et al. (2012). To our knowledge, there is no previous work that uses reading texts as evidence for short answer scoring in the context of foreign language learning. 3 Answer-based models In order to compare to previous work, we first implement an alignment-based model following that proposed in (Meurers et al., 2011b). We refer to this class of models as answer-based because they function by aligning learner answers to instructorsupplied target answers along several different dimensions, discussed below. Answers are then classified as correct or incorrect on the basis of features derived from these alignments. Wherever possible/practical, we directly reimplement the Meurers model for German data. In this section we describe relevant aspects of the Meurers model, along with modifications and extensions in our implementation of that model.1 Preprocessing We preprocess all material (learner answers, target </context>
<context position="18518" citStr="Meurers et al., 2011" startWordPosition="3000" endWordPosition="3003">32 learner answers from the balanced subset of the CREG corpus, in addition to 294 target answers. Each answer 4In future work we will also consider semantic relationships between the question and the text. was labeled separately by two (of three) annotators, who were given the reading text and the question and asked to identify the single best source sentence from the text. Annotators were not told whether any given instance was a target or learner answer, nor whether learner answers were correct or incorrect. Although we expected most answers to correspond directly to a single text passage (Meurers et al., 2011b), annotators were asked to look for (and annotate appropriately) two different conditions in which more than one source sentence may be relevant. We refer to these as the repeated content condition and the distributed content condition. In the repeated content condition, the same semantic content may be fully represented in more than one sentence from the original text. In such cases, we would expect the text to contain sentences that are paraphrases or near-paraphrases of one another. The distributed content condition occurs when the relevant semantic content spans multiple sentences, and s</context>
<context position="27691" citStr="Meurers et al., 2011" startWordPosition="4473" endWordPosition="4476">rman, in the context of reading comprehension for language learning. Specifically, we investigate the text-based models described in Section 4.2. In all cases, features and parameter settings were tuned on a development set which was extracted from the larger CREG corpus. In other words, there is no overlap between test and development data. For testing, we perform leave-one-out cross-validation on the slightly-smaller subset of the corpus which was used for annotation. 5.1 Answer-based baseline As a baseline for our text-based models we take our implementation of the answer-based model from (Meurers et al., 2011b). As previously mentioned, our implementation diverges from theirs at some points, and we do not quite reach the performance reported for their model (accuracy of 84.6% on the balanced CREG corpus) and are far from reaching the current state of the art accuracy of 86.3%, as reported in Hahn and Meurers (2012). Our answer-based model appears as baseline in Table 2. During development, the one extension to the baseline which helped most was the use of extended synonyms. This variant of the model appears in the results table with the annotation +syn. 292 model k=5 k=15 k=30 baseline 0.817 0.820</context>
</contexts>
<marker>Meurers, Ziai, Ott, Bailey, 2011</marker>
<rawString>Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bailey. 2011a. Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions. Special Issue on Free-text Automatic Evaluation. International Journal of Continuing Engineering Education and Life-Long Learning (IJCEELL), 21(4):355–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detmar Meurers</author>
<author>Ramon Ziai</author>
<author>Niels Ott</author>
<author>Janina Kopp</author>
</authors>
<title>Evaluating answers to reading comprehension questions in context: Results for German and the role of information structure.</title>
<date>2011</date>
<booktitle>In Proceedings of the TextInfer 2011 Workshop on Textual Entailment,</booktitle>
<pages>1--9</pages>
<location>Edinburgh, Scottland, UK.</location>
<contexts>
<context position="7609" citStr="Meurers et al., 2011" startWordPosition="1226" endWordPosition="1229">or example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target science concepts, are </context>
<context position="9153" citStr="Meurers et al., 2011" startWordPosition="1480" endWordPosition="1483">cepts in essay writing using textual entailment techniques. They align students’ writings to extracted science concepts in order to identify misconceptions, using a similar approach to identify the correct underlying concept. An excellent and more detailed overview of related work can be found in Ziai et al. (2012). To our knowledge, there is no previous work that uses reading texts as evidence for short answer scoring in the context of foreign language learning. 3 Answer-based models In order to compare to previous work, we first implement an alignment-based model following that proposed in (Meurers et al., 2011b). We refer to this class of models as answer-based because they function by aligning learner answers to instructorsupplied target answers along several different dimensions, discussed below. Answers are then classified as correct or incorrect on the basis of features derived from these alignments. Wherever possible/practical, we directly reimplement the Meurers model for German data. In this section we describe relevant aspects of the Meurers model, along with modifications and extensions in our implementation of that model.1 Preprocessing We preprocess all material (learner answers, target </context>
<context position="18518" citStr="Meurers et al., 2011" startWordPosition="3000" endWordPosition="3003">32 learner answers from the balanced subset of the CREG corpus, in addition to 294 target answers. Each answer 4In future work we will also consider semantic relationships between the question and the text. was labeled separately by two (of three) annotators, who were given the reading text and the question and asked to identify the single best source sentence from the text. Annotators were not told whether any given instance was a target or learner answer, nor whether learner answers were correct or incorrect. Although we expected most answers to correspond directly to a single text passage (Meurers et al., 2011b), annotators were asked to look for (and annotate appropriately) two different conditions in which more than one source sentence may be relevant. We refer to these as the repeated content condition and the distributed content condition. In the repeated content condition, the same semantic content may be fully represented in more than one sentence from the original text. In such cases, we would expect the text to contain sentences that are paraphrases or near-paraphrases of one another. The distributed content condition occurs when the relevant semantic content spans multiple sentences, and s</context>
<context position="27691" citStr="Meurers et al., 2011" startWordPosition="4473" endWordPosition="4476">rman, in the context of reading comprehension for language learning. Specifically, we investigate the text-based models described in Section 4.2. In all cases, features and parameter settings were tuned on a development set which was extracted from the larger CREG corpus. In other words, there is no overlap between test and development data. For testing, we perform leave-one-out cross-validation on the slightly-smaller subset of the corpus which was used for annotation. 5.1 Answer-based baseline As a baseline for our text-based models we take our implementation of the answer-based model from (Meurers et al., 2011b). As previously mentioned, our implementation diverges from theirs at some points, and we do not quite reach the performance reported for their model (accuracy of 84.6% on the balanced CREG corpus) and are far from reaching the current state of the art accuracy of 86.3%, as reported in Hahn and Meurers (2012). Our answer-based model appears as baseline in Table 2. During development, the one extension to the baseline which helped most was the use of extended synonyms. This variant of the model appears in the results table with the annotation +syn. 292 model k=5 k=15 k=30 baseline 0.817 0.820</context>
</contexts>
<marker>Meurers, Ziai, Ott, Kopp, 2011</marker>
<rawString>Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp. 2011b. Evaluating answers to reading comprehension questions in context: Results for German and the role of information structure. In Proceedings of the TextInfer 2011 Workshop on Textual Entailment, pages 1–9, Edinburgh, Scottland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Assessing the role of discourse references in entailment inference.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<marker>Mirkin, Dagan, Pad´o, 2010</marker>
<rawString>Shachar Mirkin, Ido Dagan, and Sebastian Pad´o. 2010. Assessing the role of discourse references in entailment inference. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Text-to-text semantic similarity for automatic short answer grading.</title>
<date>2009</date>
<pages>567--575</pages>
<editor>In Alex Lascarides, Claire Gardent, and Joakim Nivre, editors, EACL,</editor>
<contexts>
<context position="7052" citStr="Mohler and Mihalcea (2009)" startWordPosition="1140" endWordPosition="1143"> be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012</context>
</contexts>
<marker>Mohler, Mihalcea, 2009</marker>
<rawString>Michael Mohler and Rada Mihalcea. 2009. Text-to-text semantic similarity for automatic short answer grading. In Alex Lascarides, Claire Gardent, and Joakim Nivre, editors, EACL, pages 567–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Razvan C Bunescu</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning to grade short answer questions using semantic similarity measures and dependency graph alignments.</title>
<date>2011</date>
<pages>752--762</pages>
<editor>In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, ACL,</editor>
<contexts>
<context position="7021" citStr="Mohler et al. (2011)" startWordPosition="1135" endWordPosition="1138">. Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers e</context>
</contexts>
<marker>Mohler, Bunescu, Mihalcea, 2011</marker>
<rawString>Michael Mohler, Razvan C. Bunescu, and Rada Mihalcea. 2011. Learning to grade short answer questions using semantic similarity measures and dependency graph alignments. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, ACL, pages 752–762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Learning to assess low-level conceptual understanding.</title>
<date>2008</date>
<booktitle>FLAIRS Conference,</booktitle>
<pages>427--432</pages>
<editor>In David Wilson and H. Chad Lane, editors,</editor>
<contexts>
<context position="7921" citStr="Nielsen et al. (2008)" startWordPosition="1278" endWordPosition="1281">texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target science concepts, are then assessed by determining whether the relevant facets are addressed by the learner answers. Evaluating short answers on the basis of associated reading texts, as we do here, is a task related to textual entailment. In the context of tutoring systems, Bethard et al. (2012) identify students’ misconceptions of</context>
</contexts>
<marker>Nielsen, Ward, Martin, 2008</marker>
<rawString>Rodney D. Nielsen, Wayne Ward, and James H. Martin. 2008. Learning to assess low-level conceptual understanding. In David Wilson and H. Chad Lane, editors, FLAIRS Conference, pages 427–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niels Ott</author>
<author>Ramon Ziai</author>
<author>Detmar Meurers</author>
</authors>
<title>Creation and analysis of a reading comprehension exercise corpus: Towards evaluating meaning in context.</title>
<date>2012</date>
<booktitle>In Thomas Schmidt</booktitle>
<pages>47--69</pages>
<editor>and Kai W¨orner, editors,</editor>
<publisher>Benjamins,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="14737" citStr="Ott et al., 2012" startWordPosition="2363" endWordPosition="2366">ment (number of different token alignment types). The n-gram features are the only new features in our re-implementation of the Meurers model, hoping to capture the influence of linear ordering of aligned tokens. These features did not in the end improve the model’s performance. For classification, we use the timbl toolkit (Daelemans et al., 2009) for k-nearest neighbors classification. We treat all features as numeric values and evaluate performance via leave-one-out crossvalidation. Further details appear in Section 5. Data For all work reported in this paper, we use the German CREG corpus (Ott et al., 2012) of short answers to questions for reading comprehension tasks. More specifically, we use a balanced subset of the CREG corpus containing a total of 1032 learner answers. This corpus consists of 30 reading texts with an average of 5.9 questions per text. Each question is associated with one or more target answers, specified by a teacher. For each question in turn there are an average of 5.8 learner answers, each manually annotated according to both binary and fine-grained labeling schemes. When there are several target answers for a question, the best target answer for each learner answer is i</context>
</contexts>
<marker>Ott, Ziai, Meurers, 2012</marker>
<rawString>Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Creation and analysis of a reading comprehension exercise corpus: Towards evaluating meaning in context. In Thomas Schmidt and Kai W¨orner, editors, Multilingual Corpora and Multilingual Corpus Analysis, Hamburg Studies in Multilingualism (HSM), pages 47–69. Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen G Pulman</author>
<author>Jana Z Sukkarieh</author>
</authors>
<title>Automatic short answer marking.</title>
<date>2005</date>
<booktitle>In Proceedings of the second workshop on Building Educational Applications Using NLP, EdAppsNLP 05,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="6892" citStr="Pulman and Sukkarieh, 2005" startWordPosition="1112" endWordPosition="1115">proaches to short answer scoring In short answer scoring (SAS) the task is to automatically assign labels to individual learner answers. Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here </context>
</contexts>
<marker>Pulman, Sukkarieh, 2005</marker>
<rawString>Stephen G. Pulman and Jana Z. Sukkarieh. 2005. Automatic short answer marking. In Proceedings of the second workshop on Building Educational Applications Using NLP, EdAppsNLP 05, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="9929" citStr="Schmid, 1994" startWordPosition="1597" endWordPosition="1598"> dimensions, discussed below. Answers are then classified as correct or incorrect on the basis of features derived from these alignments. Wherever possible/practical, we directly reimplement the Meurers model for German data. In this section we describe relevant aspects of the Meurers model, along with modifications and extensions in our implementation of that model.1 Preprocessing We preprocess all material (learner answers, target answers, questions and reading texts) using standard NLP tools for sentence splitting and tokenization (both OpenNLP2), POS tagging and stemming (both Treetagger (Schmid, 1994)), NP chunking (OpenNLP), and dependency parsing (Zurich Parser (Sennrich et al., 2009)). We use an NE Tagger (Faruqui and Pad´o, 2010) to annotate named entities. Synonyms and semantic types are extracted from GermaNet (Hamp and Feldweg, 1997). For keywords, which serve to give more emphasis to content words in the target answer, we extract all nouns from the target answer. Given that we are dealing with learner language, but do not want to penalize answers for typical learner errors, spellchecking (and subsequent correction of spelling errors) is especially important for this task. Our appro</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Gerold Schneider</author>
<author>Martin Volk</author>
<author>Martin Warin</author>
</authors>
<title>A new hybrid dependency parser for German.</title>
<date>2009</date>
<booktitle>Von der Form zur Bedeutung: Texte automatisch verarbeiten ? From Form to Meaning: Processing Texts Automatically. Proceedings of the Biennial GSCL Conference 2009,</booktitle>
<pages>115--124</pages>
<editor>In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors,</editor>
<location>Narr, T¨ubingen.</location>
<contexts>
<context position="10016" citStr="Sennrich et al., 2009" startWordPosition="1607" endWordPosition="1610">ect on the basis of features derived from these alignments. Wherever possible/practical, we directly reimplement the Meurers model for German data. In this section we describe relevant aspects of the Meurers model, along with modifications and extensions in our implementation of that model.1 Preprocessing We preprocess all material (learner answers, target answers, questions and reading texts) using standard NLP tools for sentence splitting and tokenization (both OpenNLP2), POS tagging and stemming (both Treetagger (Schmid, 1994)), NP chunking (OpenNLP), and dependency parsing (Zurich Parser (Sennrich et al., 2009)). We use an NE Tagger (Faruqui and Pad´o, 2010) to annotate named entities. Synonyms and semantic types are extracted from GermaNet (Hamp and Feldweg, 1997). For keywords, which serve to give more emphasis to content words in the target answer, we extract all nouns from the target answer. Given that we are dealing with learner language, but do not want to penalize answers for typical learner errors, spellchecking (and subsequent correction of spelling errors) is especially important for this task. Our approach is as follows: we first identify all words from the learner answers that are not ac</context>
</contexts>
<marker>Sennrich, Schneider, Volk, Warin, 2009</marker>
<rawString>Rico Sennrich, Gerold Schneider, Martin Volk, and Martin Warin. 2009. A new hybrid dependency parser for German. In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors, Von der Form zur Bedeutung: Texte automatisch verarbeiten ? From Form to Meaning: Processing Texts Automatically. Proceedings of the Biennial GSCL Conference 2009, pages 115–124. Narr, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jana Z Sukkarieh</author>
<author>Stephen G Pulman</author>
</authors>
<title>Information extraction and machine learning: Automarking short free text responses to science questions.</title>
<date>2005</date>
<booktitle>of Frontiers in Artificial Intelligence and Applications,</booktitle>
<volume>125</volume>
<pages>629--637</pages>
<editor>In Chee-Kit Looi, Gordon I. McCalla, Bert Bredeweg, and Joost Breuker, editors, AIED,</editor>
<contexts>
<context position="6921" citStr="Sukkarieh and Pulman, 2005" startWordPosition="1116" endWordPosition="1119">ring In short answer scoring (SAS) the task is to automatically assign labels to individual learner answers. Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here is strongly based on approach</context>
</contexts>
<marker>Sukkarieh, Pulman, 2005</marker>
<rawString>Jana Z. Sukkarieh and Stephen G. Pulman. 2005. Information extraction and machine learning: Automarking short free text responses to science questions. In Chee-Kit Looi, Gordon I. McCalla, Bert Bredeweg, and Joost Breuker, editors, AIED, volume 125 of Frontiers in Artificial Intelligence and Applications, pages 629–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ziai</author>
<author>Niels Ott</author>
<author>Detmar Meurers</author>
</authors>
<title>Short answer assessment: Establishing links between research strands.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications (BEA7),</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7653" citStr="Ziai et al., 2012" startWordPosition="1234" endWordPosition="1237">d Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort 287 of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target science concepts, are then assessed by determining whether the rel</context>
</contexts>
<marker>Ziai, Ott, Meurers, 2012</marker>
<rawString>Ramon Ziai, Niels Ott, and Detmar Meurers. 2012. Short answer assessment: Establishing links between research strands. In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications (BEA7), Montreal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>