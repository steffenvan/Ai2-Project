<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.964803">
A Data-Driven, Factorization Parser for CCG Dependency Structures
</title>
<author confidence="0.632372">
YantaoDun, Weiwei Sun∗ and Xiaojun Wan
</author>
<affiliation confidence="0.647178">
Institute of Computer Science and Technology, Peking University
</affiliation>
<note confidence="0.693267">
The MOE Key Laboratory of Computational Linguistics, Peking University
</note>
<email confidence="0.916894">
{ws,duyantao,wanxiaojun}@pku.edu.cn
</email>
<sectionHeader confidence="0.992824" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999289">
This paper is concerned with building
CCG-grounded, semantics-oriented deep
dependency structures with a data-driven,
factorization model. Three types of fac-
torization together with different higher-
order features are designed to capture
different syntacto-semantic properties of
functor-argument dependencies. Integrat-
ing heterogeneous factorizations results
in intractability in decoding. We pro-
pose a principled method to obtain opti-
mal graphs based on dual decomposition.
Our parser obtains an unlabeled f-score of
93.23 on the CCGBank data, resulting in
an error reduction of 6.5% over the best
published result. which yields a signifi-
cant improvement over the best published
result in the literature. Our implementa-
tion is available at http://www.icst.
pku.edu.cn/lcwm/grass.
</bodyText>
<sectionHeader confidence="0.998765" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994166017241379">
Combinatory Categorial Grammar (CCG; Steed-
man, 2000) is a linguistically expressive gram-
mar formalism which has a transparent yet el-
egant interface between syntax and semantics.
By assigning each lexical category a dependency
interpretation, we can derive typed dependency
structures from CCG derivations (Clark et al.,
2002), providing a useful approximation to the
underlying meaning representations. To date,
CCG parsers are among the most competitive sys-
tems for generating such deep bi-lexical depen-
dencies that appropriately encode a wide range
of local and non-local syntacto-semantic infor-
mation (Clark and Curran, 2007a; Bender et al.,
2011). Such semantic-oriented dependency struc-
tures have been shown very helpful for NLP ap-
∗Email correspondence.
plications e.g. Question Answering (Reddy et al.,
2014).
Traditionally, CCG graphs are generated as a
by-product by grammar-guided parsers (Clark and
Curran, 2007b; Fowler and Penn, 2010). The main
challenge is that a deep-grammar-guided model
usually can only produce limited coverage and
corresponding parsing algorithms is of relatively
high complexity. Robustness and efficiency, thus,
are two major problems for handling practical
tasks. To increase the applicability of such parsers,
lexical or syntactic pruning has been shown nec-
essary (Clark and Curran, 2004; Matsuzaki et al.,
2007; Sagae et al., 2007; Zhang and Clark, 2011).
In the past decade, the techniques for data-
driven dependency parsing has made a great
progress (McDonald et al., 2005a,b; Nivre et al.,
2004; Torres Martins et al., 2009; Koo et al.,
2010). The major advantage of the data-driven
architecture is complementary to the grammar-
driven one. On one hand, data-driven approaches
make essential uses of machine learning from lin-
guistic annotations and are flexible to produce
analysis for arbitrary sentences. On the other
hand, without hard constraints, parsing algorithms
for spanning specific types of graphs, e.g. projec-
tive (Eisner, 1996) and 1-endpoint-crossing trees
(Pitler et al., 2013), can be of low complexity.
This paper proposes a new data-driven depen-
dency parser that efficiently produces globally op-
timal CCG dependency graphs according to a dis-
criminative, factorization model. The design of
the factorization is motivated by three essential
properties of the CCG dependencies. First, all ar-
guments associated with the same predicate are
highly correlated due to the nature that they ap-
proximates type-logical semantics. Second, all
predicates govern the same argument exhibit the
hybrid syntactic/semantic, i.e. head-complement-
adjunct, relationships. Finally, the CCG depen-
dency graphs are not but look very much like
</bodyText>
<page confidence="0.942468">
1545
</page>
<note confidence="0.976495666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1545–1555,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999959212121212">
trees, which have many good computational prop-
erties. Simultaneously modeling the three prop-
erties yields intrinsically heterogeneous factoriza-
tions over the same graph, and hence results in in-
tractability in decoding. Inspired by (Koo et al.,
2010; Rush et al., 2010), we employ dual decom-
position to perform principled decoding. Though
not always, we can obtain the optimal solution
most of time. The time complexity of our parser
is O(n3) when various 1st- and 2nd-order features
are incorporated.
We conduct experiments on English CCGBank
(Hockenmaier and Steedman, 2007). Though
our parser does not use any grammar informa-
tion, including both lexical categories and syntac-
tic derivations, it produces very accurate CCG de-
pendency graphs with respect to both token and
complete matching. Our parser obtains an unla-
beled f-score of 93.23, resulting in, perhaps sur-
prisingly, an error reduction of up to 6.5% over
the best published performance reported in (Auli
and Lopez, 2011). Our work indicates that high-
quality data-driven parsers can be built for produc-
ing more general dependency graphs, rather than
trees. Nevertheless, empirical evaluation indicates
that explicitly or implicitly using tree-structured
information plays an essential role. The result also
suggests that a wider range of complicated linguis-
tic phenomena beyond surface syntax can be well
modeled even without explicitly using grammars.
Our algorithm is also applicable to other graph-
structured representations, e.g. HPSG predicate-
argument analysis (Miyao et al., 2004).
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999961081967214">
Hockenmaier and Steedman (2007) developed lin-
guistic resources, namely CCGBank, from the
Penn Treebank (PTB; Marcus et al., 1993). In
CCGBank, PTB phrase-structure trees have been
transformed into normal-form CCG derivations,
and deep bi-lexical dependency graphs that encode
functor-argument strcutures have been extracted
from these derivations using coindexation infor-
mation. The typed dependency analysis provides
a useful approximation to the underlying meaning
representations, and has been shown very help-
ful for NLP applications e.g. Question Answering
(Reddy et al., 2014).
Traditionally, CCG graphs are generated as a
by-product by deep parsers with a core gram-
mar (Clark et al., 2002; Clark and Curran, 2007b;
Fowler and Penn, 2010). On the other hand, mod-
eling these dependencies within a CCG parser has
been shown very effective to improve the pars-
ing accuracy (Clark and Curran, 2007b; Xu et al.,
2014). Besides CCG, similar deep dependency
structures can be also extracted from parsers under
other deep grammar formalisms, e.g. LFG (King
et al., 2003) and HPSG (Miyao et al., 2004).
In recent years, data-driven dependency pars-
ing has been well studied and widely applied to
many NLP tasks. Research on data-driven ap-
proach to producing dependency graphs that are
not limited to tree or forest structures has also been
initialized. Sagae and Tsujii (2008) introduced
a transition-based parser that is able to handle
projective directed dependency graphs for HPSG-
style predicate-argument analysis. McDonald and
Pereira (2006) presented a graph-based parser that
can generate graphs in which a word may depend
on multiple heads, and evaluated it on the Danish
Treebank. Encouraged by their work, we study
factorization models as well as principled decod-
ing for CCG-grounded, graph-structured represen-
tations.
Dual decomposition, and more generally La-
grangian relaxation, is a classical method for solv-
ing combinatorial optimization problems. It has
been successfully applied to several NLP tasks,
including parsing (Koo et al., 2010; Rush et al.,
2010) and machine translation (Rush and Collins,
2011). To provide principled decoding for our fac-
torization parser, we employ the dual decomposi-
tion technique. Our work directly follows (Koo
et al., 2010). The two basic factorizations are
similar to the model introduced in (Martins and
Almeida, 2014). Lluis et al. (2013) introduced
a dual decomposition based joint model for joint
syntactic and semantic parsing. They are con-
cerned with shallow semantic representation, i.e.
Semantic Role Labeling, whose graphs are sparse.
Different from their concern on integrating syntac-
tic parsing and semantic role labeling under 1st-
order factorization, we are interested in designing
higher-order factorization models for more dense
and general linguistic graphs.
</bodyText>
<sectionHeader confidence="0.97698" genericHeader="method">
3 Graph Factorization
</sectionHeader>
<subsectionHeader confidence="0.995625">
3.1 Background Notations
</subsectionHeader>
<footnote confidence="0.796288666666667">
Consider a sentence s = (w, p) with words w =
w1w2 · · · wn and POS-tags p = p1p2 · · · pn. First
we add one more virtual word w0 = # Wroot#
</footnote>
<page confidence="0.991879">
1546
</page>
<figure confidence="0.9785425">
changes would exempt ... executives from
(S\NP)/(S\NP) ((S\NP)/PP)NP
</figure>
<figureCaption confidence="0.995646">
Figure 1: Examples to illustrate the predicate-
centric view.
</figureCaption>
<bodyText confidence="0.993765166666667">
with POS-tag p0 = #Proot# which is convention-
ally considered as the root node of trees or graphs
on the sentence. Then we denote the index set
of all possible dependencies as Z = {(i, j) i E
{0, · · · , n}, j E {1, · · · , n}, i =� j}. A depen-
dency parse then can be represented as a vector
</bodyText>
<equation confidence="0.951666">
y = {y(i, j) : (i, j) E Z},
</equation>
<bodyText confidence="0.999126222222222">
where y(i, j) = 1 if a dependency with predicate i
and argument j is in the graph, 0 otherwise. Note
that y is not a matrix but a long vector though we
use two indexes to index it. In this paper, we only
consider the unlabeled parsing task. Nevertheless,
it is quite straightforward to extend our models to
labeled parsing. Let Y denote the set of all possi-
ble y. Given a function f : Y —* R that assigns
scores to parse graphs, the optimal parse is
</bodyText>
<equation confidence="0.9937405">
y∗ = arg max f(y).
yEY
</equation>
<bodyText confidence="0.9995276">
Following recent advances in discriminative de-
pendency parsing, we build disambiguation mod-
els based on global linear models, as in (McDon-
ald et al., 2005a). In this framework, we score a
dependency graph using a linear model:
</bodyText>
<equation confidence="0.976541">
fθ(y) = θTΦ(s, y),
</equation>
<bodyText confidence="0.999928666666667">
where Φ(s, y) produces a d-dimensional vector
representation of the event that a CCG graph y is
assigned to sentence s. In order to perform the
decoding efficiently, we assume that the depen-
dency graphs can be factored into smaller pieces.
The main goal of this paper is to design ap-
propriate factorization models, namely different
types of fθ’s, to reflect essential properties of the
semantics-oriented CCG dependency graphs.
</bodyText>
<subsectionHeader confidence="0.998593">
3.2 Predicate-Centric Factorization
</subsectionHeader>
<bodyText confidence="0.99973062962963">
The very fundamental view of the CCG de-
pendency graphs is based on their lexicalized,
predicate-centric nature. Every word is assigned
a lexical category, which directly encodes its sub-
categorization information. Due to the type-
transparency nature of the formalism, this lexi-
cal category provides sufficient information for
not only syntactic derivation but also semantic
composition. It is important to capture functor-
argument relations by putting all arguments of
one particular predicate together. Figure 1 gives
an example. The predicate “exempt” is of type
“((S\NP)/PP)/NP,” indicating that it takes three
semantic dependents. This part of information is
very similar to Semantic Role Labeling (SRL),
whose goal is to find semantic roles for ver-
bal predicates as well as their normalization.
However, functor-argument analysis grounded in
CCG is approximation of underlying logic forms
and thus provides bi-lexical relations for almost all
words. For instance, the second word in focus-
66 would&amp;quot;-captures
ocus—&amp;quot;would&amp;quot;-captures structural information to orga-
nize other predicates yet entities.
In order to perform maximization efficiently
in this view, we treat each predicate separately.
Given a vector yp, we define
</bodyText>
<equation confidence="0.869164">
yil = {y(i, j) : j E {1, · · · , n}, j =� i}
p
</equation>
<bodyText confidence="0.762653">
and assume that f(yp) takes the form
</bodyText>
<equation confidence="0.969545666666667">
n
fp(yp) = fpi (ypil)
i=0
</equation>
<bodyText confidence="0.999620857142857">
To capture the relationships of all arguments to
one particular predicate as a whole, we employ a
Markov model. Let a1, · · · , am be the sequence of
the arguments of the word wi under ypil. To keep
the arguments in order, we constrain 1 &lt; aj1 &lt;
aj2 &lt; n if j1 &lt; j2. In a k-th order predicate-
centric model, we define
</bodyText>
<equation confidence="0.973769">
T
θp Φp(aj−(k−1), ..., aj, i, W, P)
</equation>
<bodyText confidence="0.9979175">
where aj (j &lt; 0 or j &gt; m + 1) are treated as
specific initial or end state.
Higher-order rather than arc-factored features
can be conveniently extracted from adjacent argu-
ments. This is similar to the sibling factorization
defined by a number of syntactic tree parsers, e.g.
(McDonald and Pereira, 2006), (Koo and Collins,
2010) and (Ma and Zhao, 2012).
</bodyText>
<figure confidence="0.92486975">
arg1
arg2
arg1
arg2
arg3
fpi (ypil) = m+k−1
�
j=1
</figure>
<page confidence="0.876575">
1547
</page>
<bodyText confidence="0.857874">
Similarly, we define the initial and end states for
pi (i &lt; 0 or i &gt; m + 1).
</bodyText>
<figure confidence="0.794808166666667">
arg2
arg1
arg1 arg1
arg1
In an Oct. 19 review of ...
(S/S)/N NP/N N/N N/N N (NP\NP)/NP
</figure>
<figureCaption confidence="0.994562">
Figure 2: An example to illustrate the argument-
centric view.
</figureCaption>
<subsectionHeader confidence="0.99744">
3.3 Argument-Centric Factorization
</subsectionHeader>
<bodyText confidence="0.99999124137931">
The syntactic principle for tree annotation treats
the dependency relations between two words as
syntactic projection. In another word, the head de-
termines the syntactic category of the whole struc-
ture. The (type-logical) semantic principle deter-
mines a dependency according the types of the two
words. The two kinds of dependency are coherent
but not necessarily the same. In particular, an ad-
junct is a syntactic dependent but usually a seman-
tic predicate of its syntactic head. Figure 2 gives
an example to illustrate the idea. The argument
in focus is “review” that is the complement of the
preposition “in.” The direction of this semantic de-
pendency is the same to its corresponding syntac-
tic dependency. Other predicates that semantically
govern “review” are actually its modifiers, so the
direction of these semantic dependencies are the
opposite of their syntactic counterparts. It is im-
portant to capture head-complement-adjunct rela-
tions by putting all predicates of one particular ar-
gument together.
Similar to the predicate-centric model, we treat
the graph fragment involved by each argument as
independent, and capture the relationships among
all predicates that governs the same argument us-
ing a Markov model. In the definition of predicate-
centric model, if we exchange predicates and ar-
guments, then we get our argument-centric model.
Formally, we define
</bodyText>
<equation confidence="0.892774">
ya�j = {y(i,j) : i E {0,···,n},j =�i}.
</equation>
<bodyText confidence="0.98928525">
Let p1, · · · , pm be the sequence of the predicates
(in linear word order) that semantically governs
the word j under ya�j. A k-th order argument-
centric model scores the dependency graph as
</bodyText>
<subsectionHeader confidence="0.93526">
3.4 Tree Approximation Model
</subsectionHeader>
<bodyText confidence="0.999993096774194">
Tree structures exhibit many computationally-
good properties, and have been widely applied to
model linguistic, especially syntactic, structures.
Tree-structured representation is an essential pre-
requisite for both the parsing algorithms and the
machine learning methods in state-of-the-art syn-
tactic dependency parsers. The CCG dependency
graphs are not but look very much like trees. We
thus argue that a tree-centric model can on one
hand capture some topologically essential charac-
teristics and on the other hand benefit from mature
tree parsing techniques.
To this end, we propose tree approximation to
obtain CCG sub-graphs under the factorization us-
ing tree parsing algorithms. In particular, we
introduce an algorithm to associate every graph
with a projective dependency tree, which we call
weighted conversion. The tree reflects partial in-
formation about the corresponding graph. In this
algorithm, we assign heuristic weights to all pos-
sible edges, and then find the tree with maxi-
mum weights. The key idea behind is to find a
tree frame of a given graph. Given an arbitrary
CCG graph, the conversion is perhaps imperfect in
the sense that information about a small portion of
edges is “lost.” As a result, our tree approximation
model can only generate partial graphs. Neverthe-
less, we will show (in Section 3.5 and 4.2) that
such a model can be combined with predicate- and
argument-centric factorization models in an ele-
gant way.
</bodyText>
<subsectionHeader confidence="0.50997">
3.4.1 Weighted Conversion
</subsectionHeader>
<bodyText confidence="0.999762714285714">
We assign weights to all the possible edges, i.e. all
pairs of words, and then determine which edges
to be kept by finding the maximum spanning tree.
More formally, given a graph y = {y(i, j)}, each
possible edge (i, j) is assigned a heuristic weight
ω(i, j). The maximum spanning tree t = {t(i, j)}
contains the maximum sum of values of edges:
</bodyText>
<equation confidence="0.972592833333333">
n
fa(y) = faj (yarsj)
j=1
= n m+k−1 θ�a Φa(pi−(k−1), ...,pi,j, W, P)
j=1 �
i=1
</equation>
<bodyText confidence="0.998157666666667">
We separate the ω(i, j) into three parts
(ω(i, j) = A(i, j) + B(i, j) + C(i, j)) that are
defined as below.
</bodyText>
<equation confidence="0.990746">
�tmax = arg max t(i, j)ω(i, j)
t (i,j)
</equation>
<page confidence="0.936482">
1548
</page>
<listItem confidence="0.815645833333333">
• A(i, j) = a · max{y(i, j), y(j, i)}: a is the
weight for the existing edges on graph ignor-
ing direction.
• B(i, j) = b · y(i, j): b is the weight for the
forward edges on the graph.
• C(i, j) = n — i — j : This term estimates the
</listItem>
<bodyText confidence="0.982758157894737">
importance of an edge where n is the length
of the given sentence. For dependency pars-
ing, we consider edges with short distance to
be more important because those edges can
be predicted more accurately in future pars-
ing process.
• a » b » n or a &gt; bn &gt; n2: The converted
tree should contain arcs in original graph as
many as possible, and the direction of the arcs
should not be changed if possible. The rela-
tionship of a, b, and c guarantees this.
After all edges are weighted, we can use max-
imum spanning tree (MST) algorithms to get the
converted tree. To get the projective tree, we
choose Eisner’s algorithm. However, the obtained
tree must be labeled in order to encode the origi-
nal graph. Here we introduce a label vector l =
{l(i, j)}. For each (i, j) E Z, we assign a label
l(i, j) to edge (i, j) as follows.
</bodyText>
<equation confidence="0.986262">
Case y(i, j) = 1: label “X”;
Case y(i, j) = 0 n y(j, i) = 1: label “X—R”;
Case y(i, j) = 0 n y(j, i) = 0: label “None”.
</equation>
<bodyText confidence="0.9977098">
We can convert the labeled tree back to graph and
obtain yt. Tough some edges are lost during the
conversion, a lot more are kept. In fact, according
to our evaluation, 92.74% of edges in the training
set are retained after conversion.
</bodyText>
<subsectionHeader confidence="0.515326">
3.4.2 Factorizing Trees
</subsectionHeader>
<bodyText confidence="0.9999865">
We use the tree parsing model proposed in
(Bohnet, 2010) to score the converted trees. The
model factorizes a tree into 1st-order and 2nd-
order factors. When decoding, the model searches
for a tree with the best score. The score defined
for graphs as well as trees is
</bodyText>
<equation confidence="0.99878275">
ft(yt) = gt(t, l) = 0T t 4bt(s, t, l)
= 01T
t 4b1 t (s, t, l) + 02T
t 4b2 t (s, t, l)
= (i,j)EZ t(i,j)01T
t 4b1 t (l(i,j),w,p))
+02T
t 4b2 t (s, t, l),
</equation>
<bodyText confidence="0.998583">
where 4b1 t is the 1st-order features and 4b2t is the
2nd-order features.
</bodyText>
<subsectionHeader confidence="0.993014">
3.5 Parsing as Optimization
</subsectionHeader>
<bodyText confidence="0.9996047">
Motivated by linguistic properties of the
semantics-oriented CCG dependencies, we
have designed three single factorization models
from heterogeneous views. Our single models
exhibit different predictive strengths considering
that they are designed to capture different prop-
erties separately. Integrating them can generate
better graphs, but is provably hard. To this end,
we formulate the parsing problem as the following
constrained optimization problem.
</bodyText>
<equation confidence="0.88688625">
maximize fp(yp) + fa(ya) + ft(yt)
subject to yp(i, j) = ya(i, j),
yp(i, j) &gt; yt(i, j),
ya(i, j) &gt; yt(i, j) for all (i, j)
</equation>
<bodyText confidence="0.999919666666667">
The equality constraint says that the graph given
by the predicate- and the argument-centric model
must be identical, while the inequality constraints
say that the frame of graph given by the tree ap-
proximation model must be a subgraph of what is
given by the first two models.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="method">
4 Decoding
</sectionHeader>
<subsectionHeader confidence="0.997629">
4.1 Easiness and Hardness of Decoding
</subsectionHeader>
<bodyText confidence="0.999661526315789">
The three factorization models are all solvable in
polynomial time. The predicate-centric model and
the argument-centric model can be decoded us-
ing dynamic programming. We provide the de-
tailed description of such an algorithm in our sup-
plementary note. The decoding method for k-th
(k &gt; 2) order model costs time of O(nk+1) where
n is the length of the sentence. The tree approxi-
mation model can re-use existing dependency tree
parsing algorithms.
Unfortunately, the exact joint decoding of 2nd-
order predicate- and argument-centric models is
already NP-hard, not to mention other model com-
binations. The following gives a brief proof for
the problem of combining the 2nd-order predicate-
and argument-centric models.
Proof. Formally, we want to find a graph y which
maximizes F(y) = fp(y)+fa(y). We can design
the feature function 4ba and the parameter 0a, such
</bodyText>
<page confidence="0.930454">
1549
</page>
<bodyText confidence="0.373492">
that for all 1 G i1 G i2 G n,
</bodyText>
<construct confidence="0.5909974">
θTa Φa(0, i1, j, w, p) = 0
θTa Φa(i1, n + 1,j, w, p) = 0
T
θa Φa(i1, i2, j, w, p) = −oc
θTa Φa(0, n + 1, j, w, p) = −oc
</construct>
<bodyText confidence="0.999991769230769">
where n is the length of the sentence. Note that
those 4 equations make the nodes except the root
node in the optimal graph each have exactly one
incoming edge. So the problem of finding a tree
t maximizing fp(t) is reduced to this problem.
Moreover, the NP-hard problem 3DM can be re-
duced to the problem of finding a tree t maxi-
mizing fp(t) (see McDonald and Pereira (2006)),
leading to the NP-hardness of both of the prob-
lems.
Let L* be the maximized value of L(yp, ya, yt; u)
subjected to the constraints, then L* =
minu L(u), according to the duality principle.
</bodyText>
<subsubsectionHeader confidence="0.493149">
4.2.2 Decoding Algorithm
</subsubsectionHeader>
<bodyText confidence="0.99740875">
There are two challenges in solving the dual prob-
lem. One challenge is to find the minimum value
of the dual objective. For this, we can use subgra-
dient method, as is demonstrated in Algorithm 1.
The other is the evaluation of L(u). For this, we
decompose the dual objective into three optimiza-
tion problems. Let Bp = uTAp, Ba = uTAa,
Bt = uTAt, and
</bodyText>
<equation confidence="0.7854492">
Ctl(i,j) = ( Bt(i,j), if l(i,j) = X;
l Bt(j, i), if l(i, j) = X ~ R;
⎧
⎨⎪⎪
⎪⎪⎩
</equation>
<subsectionHeader confidence="0.954751">
4.2 Decoding via Dual Decomposition
</subsectionHeader>
<bodyText confidence="0.999932">
To solve the joint decoding problem, optimization
techniques based on decomposition with coupling
variables are applicable. In this paper, we propose
to solve it via dual decomposition. The experiment
results show that though not always, we can obtain
the optimal solution most of time. To simplify the
description, we only consider the 2nd-order case
for all three models.
</bodyText>
<equation confidence="0.960282166666667">
we can just redefine
m+1
fpi (yi,) = (θTp Φp(aj−1, aj, i, w, p)
j=1
+Bp(i,j))
m+1
faj (y�j) = (θTa Φa(pi−1, pi, j, w, p)
i=1
4.2.1 Lagrangian Relaxation
Notice that yp(i, j) ~ yt(i, j) can be written as
yp(i,j) = S ( 1, if yt(i,j) = 1;
l arbitrary, if yt(i, j) = 0.
So the constraint can be written as Apyp+Aaya+
Atyt =
⎤ ⎡ ⎤ ⎡ ⎤
−I 0
⎦ Aa = ⎣0 ⎦At = ⎣−Dyt ⎦
Dyt −Dyt
</equation>
<bodyText confidence="0.988914333333333">
I is the identity matrix and Dyt is a diagonal ma-
trix whose main diagonal is the vector yt.
The Lagrangian of the optimization problem is
</bodyText>
<equation confidence="0.943012">
L(yp, ya, yt; u) = fp(yp) + fa(ya) + ft(yt)
+uT(Apyp + Aaya + Atyt),
</equation>
<bodyText confidence="0.9687645">
where u is the Lagrangian multiplier.
Omitting the constraints, the dual objective is
</bodyText>
<equation confidence="0.682678">
+Ba(i,j))
</equation>
<bodyText confidence="0.913450625">
and decode according to the new scores. In fact,
this equals to attach some new weights to 1st-order
factors, without changing the decoding algorithms
for the subproblems. This nice property also al-
lows using higher-order models for subproblems.
Algorithm 1: Joint decoding algorithm
Initialization: set u(0) to 0
fork = 1 to K do
</bodyText>
<equation confidence="0.998297285714286">
yp(k) arg maxy fp(y) + u(k)Apy
ya(k) arg maxy fa(y) + u(k)Aay
yt(k) arg maxy ft(y) + u(k)Aty
if Apyp(k) + Aaya (k) + Atyt(k) = 0 then
return ya
u(k) u(k−1)
−αk(Apyp(k) + Aaya(k) + Atyt(k))
</equation>
<bodyText confidence="0.841859">
Algorithm 1 is our decoding algorithm. In ev-
ery iteration, we first compute the optimal y’s of
</bodyText>
<figure confidence="0.836191272727273">
0, where
⎡
I
Ap = ⎣Dyt
0
L(u) = max L(yp, ya, yt; u)
yp,ya,yt
= max (fp(yp) + uTApyp)
yp
+ max
ya
</figure>
<equation confidence="0.963085666666667">
+
Yt (ft(yt) + uT Atyt)
(fa (ya)
+ uT Aaya)
� � t(i,j)�θ1T
ft(y) = t Φ1t(l(i,j), w, p)
(i,j)EZ
+Ctl(i,j))) + θ2T
t Φ2 t(s,t,l),
</equation>
<page confidence="0.92304">
1550
</page>
<bodyText confidence="0.9978135">
the three subproblems. If the y’s satisfies the con-
straints, then we’ve find the optimal solution for
the original problem. If not, we update the La-
grangian multiplier u, towards the negative sub-
gradient. We initialized u to be a zero vector,
and use αk to be the step length of each iteration.
When we decode the subproblems, the D,C in Ap
and Aa is derived from the yc obtained in the cur-
rent iteration.
We can also assign weights to different factor-
ization models. If we choose to do so, the La-
grangian becomes
</bodyText>
<equation confidence="0.998492">
L(yp, ya, yc; u) = wpfp(yp) + wafa(ya)
+wcft(yt) + u�(Apyp + Aaya + Atyt).
</equation>
<bodyText confidence="0.9144695">
And the decoding of subproblems in our algorithm
becomes
</bodyText>
<equation confidence="0.999662333333333">
yp(k) +— arg max
Y
y(k) +— arg max
() Y
ytk +— arg max
() Y
</equation>
<bodyText confidence="0.999748">
The algorithm we give here is the joint decod-
ing for all the three models. We can also decode
using any two of them, and it is trivial to adapt the
algorithm to the decoding.
</bodyText>
<subsectionHeader confidence="0.998058">
4.3 Pruning
</subsectionHeader>
<bodyText confidence="0.999963666666667">
In order to improve the efficiency of the algorithm,
we also do some pruning. One idea is that, in
predicate-centric model, different type of predi-
cates has different number of arguments. For ex-
ample, the predicates POS-tagged “DT” each has
only one argument in most cases. Therefore, we
assign each POS-tag a max number of arguments.
When decoding, we search at most those number
of arguments instead of all the words in the sen-
tence. This pruning method can also be applied
to the argument-centric model. The other idea is
that, some pairs of types never form a predicate-
argument relation. So we can skip extracting fea-
ture of those POS-tag pairs, just take −oo to be
their scores.
</bodyText>
<sectionHeader confidence="0.987249" genericHeader="evaluation">
5 Evaluation and Analysis
</sectionHeader>
<subsectionHeader confidence="0.798586">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.713359">
CCGbank is a translation of the Penn Treebank
into a corpus of CCG derivations (Hockenmaier
</bodyText>
<table confidence="0.9983265">
Devel. Test
HMM Tagger 96.74% 97.23%
Transition-based Parser 93.48% 93.09%
Graph-based Parser 93.47% 93.19%
</table>
<tableCaption confidence="0.8158305">
Table 1: The accuracy of the POS tagger and the
UAS of the syntax tree parsers.
</tableCaption>
<bodyText confidence="0.99991912195122">
and Steedman, 2007). CCGbank pairs syntac-
tic derivations with sets of word-word dependen-
cies which approximate the underlying functor-
argument structure. Our experiments were per-
formed using CCGBank which was split into three
subsets for training (Sections 02-21), development
testing (Section 00) and the final test (Section 23).
We also use the syntactic dependency trees pro-
vided by the CCGBank to obtain necessary in-
formation for graph parsing. However, different
from experiments in the CCG parsing literature, we
use no grammar information. Neither lexical cate-
gories nor CCG derivations are utilized.
All experiments were performed using automat-
ically assigned POS-tags that are generated by a
symbol-refined generative HMM tagger1 (Huang
et al., 2010), and automatically parsed dependency
trees that are generated by our in-house implemen-
tation of the transition-based model presented in
(Zhang and Nivre, 2011) as well as a 2nd-order
graph-based parser2 (Bohnet, 2010). The accu-
racy of these preprocessors is shown in Table 1.
We ran 5-fold jack-knifing on the gold-standard
training data to obtain imperfect dependency trees,
splitting off 4 of 5 sentences for training and the
other 1/5 for testing, 5 times. For each split, we
re-trained the tree parsers on the training portion
and applied the resulting model to the test portion.
Previous research on dependency parsing shows
that structured perceptron (Collins, 2002) is one of
the strongest discriminative learning algorithms.
To estimate B’s of different models, we utilize the
averaged perceptron algorithm. We implement our
own the predicate- and argument-centric models.
To perform tree parsing, we re-use the open-source
implementation provided by the mate-tool. See
the source code attached for details. We set it-
eration 5 to train predicate- and argument-centric
models and 10 for the tree approximation model.
To perform dual decomposition, we set the maxi-
mum iteration 200.
</bodyText>
<footnote confidence="0.997040333333333">
1www.code.google.com/p/
umd-featured-parser/
2www.code.google.com/p/mate-tools/
</footnote>
<figure confidence="0.470068333333333">
1 u(k)Apy;
fp(y) +
wp
fa(y) + w1 u(k)Aay;
1 u(k)Aty.
ft(y) + wc
</figure>
<page confidence="0.790686">
1551
</page>
<table confidence="0.999793818181818">
Tree Model UP UR UF UEM
PC 91.85 87.26 89.50 18.77
AC 91.94 87.06 89.43 16.47
No TA 92.85 86.39 89.51 14.48
PC+AC 93.84 88.18 90.93 23.05
PC+TA 91.80 91.69 91.74 27.29
AC+TA 90.19 92.88 91.52 25.51
PC+AC+TA 93.01 92.08 92.54 32.83
PC 94.01 90.76 92.36 30.16
Gr AC 94.14 90.44 92.25 27.71
TA 93.07 86.59 89.71 15.16
PC+AC 94.66 91.09 92.84 33.19
PC+TA 92.98 92.68 92.83 35.02
AC+TA 92.47 93.13 92.80 33.93
PC+AC+TA 93.66 92.73 93.19 37.64
PC 93.93 90.85 92.37 30.21
Tr AC 93.94 90.66 92.27 28.23
TA 93.19 86.68 89.82 14.79
PC+AC 94.58 91.14 92.83 31.94
PC+TA 92.93 92.71 92.82 35.34
AC+TA 92.33 93.16 92.74 33.61
PC+AC+TA 93.50 92.73 93.11 37.69
</table>
<tableCaption confidence="0.998042">
Table 2: Parsing performance on the development
</tableCaption>
<bodyText confidence="0.998134333333333">
data. The column “Tree” denotes the parsers that
give dependency tree of development set: no tree
(No), transition-based (Tr) or graph-based (Gr).
“PC,” “AC” and “TA” in the second column de-
notes the predicate-centric, the argument-centric
and the tree approximation models, respectively.
</bodyText>
<subsectionHeader confidence="0.99995">
5.2 Effectiveness of Data-driven Models
</subsectionHeader>
<bodyText confidence="0.999852666666667">
Table 2 summarizes parsing performance on de-
velopment set with different configurations. We
report unlabeled precision (UP), recall (UR), f-
score (UF) as well as complete match (UEM). It
can be clearly seen that the data-driven models
obtains high-quality graphs with respect to token
match. Even without any syntactic information
(see the top block associated with “No Tree”), our
parser with all three factorization models obtains
an f-score of 92.5. when assisted by a syntac-
tic parser, this figure goes up to over 93.1. If the
predicate- or argument-centric model is applied by
itself, either one can achieve a competitive accu-
racy, especially when syntactic features are uti-
lized.
</bodyText>
<subsectionHeader confidence="0.999886">
5.3 Effectiveness of Multiple Factorization
</subsectionHeader>
<bodyText confidence="0.9991512">
We use dual decomposition to perform joint de-
coding. First we combine the predicate-centric
model and the argument-centric model. Compared
to each single model, an error reduction of about
7% on f-score (UF) on average is achieved. Fur-
</bodyText>
<figure confidence="0.522139">
Iteration
</figure>
<figureCaption confidence="0.999142">
Figure 3: The exact decoding rate.
</figureCaption>
<bodyText confidence="0.999950304347826">
thermore, we ensemble all the three models. If no
syntactic features are extracted, the “TA” model
brings in a remarkable further absolute gain of
1.02 with respect to token match. If syntactic
features are used, the “PC” and “AC” models al-
ready achieves relatively good performance, and
the “TA” model does not contribute much consid-
ering token match. The join of the tree approxi-
mation model lowers the precision, it increases the
recall further, resulting in a modest improvement
of the f-score. Nonetheless, the “TA” model still
significantly improve the complete match metric.
It is noticeable that in all setting, the “TA” model
result in very significant boost in complete match.
The dual decomposition does not guarantee to
find an exact solution (in a limited number of it-
erations) in theory but usually works very well in
practice. We calculate the percentage of finding
exact decoding below k iterations, and the result is
show in Figure 3. The transition-based tree parser
is utilized here. We can see that for most sen-
tences, dual decomposition practically gives the
exact solutions.
</bodyText>
<subsectionHeader confidence="0.999271">
5.4 Importance of Tree Structures
</subsectionHeader>
<bodyText confidence="0.999860333333333">
Our factorization parser (with best experimental
setting) does not utilize a grammar but do use
syntactic information in the dependency formal-
ism. In particular, the parser extracts the so-called
path features from dependency trees. The syntac-
tic trees is very importance to our parser, which
provide a critical set of features for the predicate-
centric and the argument-centric models. With-
out the syntactic trees, their performances de-
crease significantly. We try two different parsers
to obtain the syntax tree parses. One is of the
transition-based architecture, and the other graph-
</bodyText>
<figure confidence="0.990731">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0 20 40 60 80 100 120 140 160 180 200
PC+AC
PC+AC+TA
Percentage of exact decoding
</figure>
<page confidence="0.922688">
1552
</page>
<table confidence="0.998561">
Tree Model UP UR UF UEM
No PC+AC+TA 93.03 92.03 92.53 32.61
Gr PC+AC+TA 93.71 92.72 93.21 38.14
Tr PC+AC+TA 93.63 92.83 93.23 37.47
Auli and Lopez 93.08 92.44 92.76 -
Xu et al. 93.15 91.06 92.09 37.56
</table>
<tableCaption confidence="0.9992485">
Table 3: Comparing the state-of-art with our mod-
els on test set.
</tableCaption>
<bodyText confidence="0.999725545454545">
based. The architecture of the syntactic tree parser
does not affect the results much. The two tree
parsers give identical attachment scores, and lead
to similar graph parsing accuracy. This result is
somehow non-obvious given that the combination
of a graph-based and transition-based parser usu-
ally gives significantly better parsing performance
(Nivre and McDonald, 2008; Torres Martins et al.,
2008).
Although the target representation of our parser
is general graphs rather trees, implicitly or explic-
itly using tree-structured information plays an es-
sential role. Syntactic features are able to im-
prove the f-score achieved by the “PC+AC” model
from 90.9 to 92.8, while the “TA” model can bring
in an absolute gain of 1.6. Note that the “TA”
model does not utilize any syntactic tree infor-
mation. The converted trees are automatically in-
duced from the CCG graphs. Even when syntactic
trees are available, the automatically induced trees
can still significantly improve the complete match
with respect to the whole sentence.
</bodyText>
<subsectionHeader confidence="0.973864">
5.5 Comparison to the State-of-the-art
</subsectionHeader>
<bodyText confidence="0.999985857142857">
We compare our results with the best published
CCG parsing performance obtained by the models
presented in (Auli and Lopez, 2011) and (Xu et al.,
2014)3. Auli and Lopez (2011) reported best nu-
meric performance. The performance is evaluated
on sentences that can be parsed by their model.
Xu et al. (2014) reported the best published results
for sentences with full coverage. All results on the
test set is shown in Table 3. Even without any syn-
tactic features, our parser achieves accuracies that
are superior to Xu et al.’s parser and comparable
to Auli and Lopez’s system. When unlabeled syn-
tactic trees are provided, our parser outperform the
state-of-the-art.
</bodyText>
<footnote confidence="0.922546333333333">
3The unlabeled parsing results are not reported in the orig-
inal paper. The figures presented in are provided by Wenduan
Xu.
</footnote>
<sectionHeader confidence="0.99206" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9996165">
In this paper, we have presented a factoriza-
tion parser for building CCG-grounded depen-
dency graphs. It achieves substantial improvement
over the state-of-the-art. Perhaps surprisingly, our
data-driven, grammar-free parser yields a supe-
rior accuracy to all CCG parsers in the literature.
Our work indicates that high-quality data-driven
parsers can be built for producing more general
dependency graphs, rather than trees. Our method
is also applicable to other deep dependency struc-
tures, e.g. HPSG predicate-argument analysis
(Miyao et al., 2004), as well as other graph-
structured semantic representations, e.g. Abstract
Meaning Representations (Banarescu et al., 2013).
</bodyText>
<sectionHeader confidence="0.971522" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9941285">
The work was supported by NSFC
(61300064), National High-Tech R&amp;D Pro-
gram (2015AA015403), NSFC (61331011) and
NSFC (61170166).
</bodyText>
<sectionHeader confidence="0.995371" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981617615384615">
Michael Auli and Adam Lopez. 2011. Train-
ing a log-linear parser with loss functions via
softmax-margin. In Proceedings of EMNLP,
pages 333–343. Association for Computational
Linguistics, Edinburgh, Scotland, UK.
Laura Banarescu, Claire Bonial, Shu Cai,
Madalina Georgescu, Kira Griffitt, Ulf Herm-
jakob, Kevin Knight, Philipp Koehn, Martha
Palmer, and Nathan Schneider. 2013. Abstract
meaning representation for sembanking. In Pro-
ceedings of the 7th Linguistic Annotation Work-
shop and Interoperability with Discourse, pages
178–186. Association for Computational Lin-
guistics, Sofia, Bulgaria.
Emily M. Bender, Dan Flickinger, Stephan Oepen,
and Yi Zhang. 2011. Parser evaluation over lo-
cal and non-local deep dependencies in a large
corpus. In Proceedings of EMNLP, pages 397–
408. Association for Computational Linguis-
tics, Edinburgh, Scotland, UK.
Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In Pro-
ceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010),
pages 89–97. Coling 2010 Organizing Commit-
tee, Beijing, China.
</reference>
<page confidence="0.994044">
1553
</page>
<bodyText confidence="0.975935066666667">
Stephen Clark and James Curran. 2007a.
Formalism-independent parser evaluation with
ccg and depbank. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 248–255. Association
for Computational Linguistics, Prague, Czech
Republic.
Stephen Clark and James R. Curran. 2004. The
importance of supertagging for wide-coverage
CCG parsing. In Proceedings of Coling 2004,
pages 282–288. COLING, Geneva, Switzer-
land.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Comput. Linguist.,
33(4):493–552.
Stephen Clark, Julia Hockenmaier, and Mark
Steedman. 2002. Building deep dependency
structures using a wide-coverage CCG parser.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
July 6-12, 2002, Philadelphia, PA, USA., pages
327–334.
Michael Collins. 2002. Discriminative training
methods for hidden markov models: Theory
and experiments with perceptron algorithms. In
Proceedings ofEMNLP, pages 1–8. Association
for Computational Linguistics.
Jason M. Eisner. 1996. Three new probabilis-
tic models for dependency parsing: an explo-
</bodyText>
<reference confidence="0.989414632352941">
ration. In Proceedings of the 16th conference
on Computational linguistics - Volume 1, pages
340–345. Association for Computational Lin-
guistics, Stroudsburg, PA, USA.
Timothy A. D. Fowler and Gerald Penn. 2010.
Accurate context-free parsing with combinatory
categorial grammar. In Proceedings of ACL,
pages 335–344. Association for Computational
Linguistics, Uppsala, Sweden.
Julia Hockenmaier and Mark Steedman. 2007.
CCGbank: A corpus of CCG derivations
and dependency structures extracted from the
penn treebank. Computational Linguistics,
33(3):355–396.
Zhongqiang Huang, Mary Harper, and Slav
Petrov. 2010. Self-training with products of
latent variable grammars. In Proceedings of
EMNLP, pages 12–22. Association for Compu-
tational Linguistics, Cambridge, MA.
Tracy Holloway King, Richard Crouch, Stefan
Riezler, Mary Dalrymple, and Ronald M. Ka-
plan. 2003. The PARC 700 dependency bank.
In In Proceedings of the 4th International
Workshop on Linguistically Interpreted Cor-
pora (LINC-03), pages 1–8.
Terry Koo and Michael Collins. 2010. Efficient
third-order dependency parsers. In Proceedings
of ACL, pages 1–11. Association for Computa-
tional Linguistics, Uppsala, Sweden.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective
head automata. In Proceedings of EMNLP,
pages 1288–1298. Association for Computa-
tional Linguistics, Cambridge, MA.
Xavier Lluis, Xavier Carreras, and Lluis M`arquez.
2013. Joint arc-factored parsing of syntactic and
semantic dependencies. TACL, 1:219–230.
Xuezhe Ma and Hai Zhao. 2012. Fourth-order de-
pendency parsing. In Proceedings of COLING
2012: Posters, pages 785–796. The COLING
2012 Organizing Committee, Mumbai, India.
Mitchell P. Marcus, Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a large
annotated corpus of english: the penn treebank.
Comput. Linguist., 19(2):313–330.
Andr´e F. T. Martins and Mariana S. C. Almeida.
2014. Priberam: A turbo semantic parser with
second order features. In Proceedings of Se-
mEval 2014, pages 471–476.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi
Tsujii. 2007. Efficient hpsg parsing with su-
pertagging and cfg-filtering. In Proceedings of
the 20th international joint conference on Ar-
tifical intelligence, pages 1671–1676. Morgan
Kaufmann publishers Inc., San Francisco, CA,
USA.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL’05), pages 91–98. As-
sociation for Computational Linguistics, Ann
Arbor, Michigan.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency pars-
ing algorithms. In Proceedings of 11th Con-
ference of the European Chapter of the Asso-
</reference>
<page confidence="0.887974">
1554
</page>
<reference confidence="0.999758581632652">
ciation for Computational Linguistics (EACL-
2006)), volume 6, pages 81–88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov,
and Jan Hajic. 2005b. Non-projective depen-
dency parsing using spanning tree algorithms.
In Proceedings of EMNLP, pages 523–530. As-
sociation for Computational Linguistics, Van-
couver, British Columbia, Canada.
Yusuke Miyao, Takashi Ninomiya, and Jun ichi
Tsujii. 2004. Corpus-oriented grammar de-
velopment for acquiring a head-driven phrase
structure grammar from the penn treebank. In
IJCNLP, pages 684–693.
Joakim Nivre, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing. In
Hwee Tou Ng and Ellen Riloff, editors, HLT-
NAACL 2004 Workshop: Eighth Conference
on Computational Natural Language Learn-
ing (CoNLL-2004), pages 49–56. Association
for Computational Linguistics, Boston, Mas-
sachusetts, USA.
Joakim Nivre and Ryan McDonald. 2008. In-
tegrating graph-based and transition-based de-
pendency parsers. In Proceedings of ACL-08:
HLT, pages 950–958. Association for Compu-
tational Linguistics, Columbus, Ohio.
Emily Pitler, Sampath Kannan, and Mitchell Mar-
cus. 2013. Finding optimal 1-endpoint-crossing
trees. TACL, 1:13–24.
Siva Reddy, Mirella Lapata, and Mark Steed-
man. 2014. Large-scale semantic parsing with-
out question-answer pairs. Transactions of
the Association for Computational Linguistics
(TACL).
Alexander M. Rush and Michael Collins. 2011.
Exact decoding of syntactic translation mod-
els through lagrangian relaxation. In The 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Tech-
nologies, Proceedings of the Conference, 19-24
June, 2011, Portland, Oregon, USA, pages 72–
82.
Alexander M Rush, David Sontag, Michael
Collins, and Tommi Jaakkola. 2010. On dual
decomposition and linear programming relax-
ations for natural language processing. In
Proceedings of EMNLP, pages 1–11. Associa-
tion for Computational Linguistics, Cambridge,
MA.
Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2007. Hpsg parsing with shallow depen-
dency constraints. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 624–631. Association
for Computational Linguistics, Prague, Czech
Republic.
Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-
reduce dependency DAG parsing. In Pro-
ceedings of the 22nd International Conference
on Computational Linguistics, pages 753–760.
Coling 2008 Organizing Committee, Manch-
ester, UK.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Andre Torres Martins, Noah Smith, and Eric Xing.
2009. Concise integer linear programming for-
mulations for dependency parsing. In Proceed-
ings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language
Processing of the AFNLP, pages 342–350. As-
sociation for Computational Linguistics, Sun-
tec, Singapore.
Andr´e Filipe Torres Martins, Dipanjan Das,
Noah A. Smith, and Eric P. Xing. 2008. Stack-
ing dependency parsers. In Proceedings of
EMNLP, pages 157–166. Association for Com-
putational Linguistics, Honolulu, Hawaii.
Wenduan Xu, Stephen Clark, and Yue Zhang.
2014. Shift-reduce ccg parsing with a depen-
dency model. In Proceedings of the 52nd An-
nual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 218–227. Association for Computational
Linguistics, Baltimore, Maryland.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, pages 683–692. Association for Computa-
tional Linguistics, Portland, Oregon, USA.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local
features. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 188–193. Association for Computational
Linguistics, Portland, Oregon, USA.
</reference>
<page confidence="0.993949">
1555
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762129">
<title confidence="0.999425">A Data-Driven, Factorization Parser for CCG Dependency Structures</title>
<author confidence="0.953956">Weiwei</author>
<affiliation confidence="0.991555">Institute of Computer Science and Technology, Peking</affiliation>
<address confidence="0.980638">The MOE Key Laboratory of Computational Linguistics, Peking</address>
<email confidence="0.976739">ws@pku.edu.cn</email>
<email confidence="0.976739">duyantao@pku.edu.cn</email>
<email confidence="0.976739">wanxiaojun@pku.edu.cn</email>
<abstract confidence="0.99107925">This paper is concerned with building semantics-oriented deep dependency structures with a data-driven, factorization model. Three types of factorization together with different higherorder features are designed to capture different syntacto-semantic properties of functor-argument dependencies. Integrating heterogeneous factorizations results in intractability in decoding. We propose a principled method to obtain optimal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a significant improvement over the best published result in the literature. Our implementais available athttp://www.icst.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>Training a log-linear parser with loss functions via softmax-margin.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>333--343</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="5004" citStr="Auli and Lopez, 2011" startWordPosition="724" endWordPosition="727">ain the optimal solution most of time. The time complexity of our parser is O(n3) when various 1st- and 2nd-order features are incorporated. We conduct experiments on English CCGBank (Hockenmaier and Steedman, 2007). Though our parser does not use any grammar information, including both lexical categories and syntactic derivations, it produces very accurate CCG dependency graphs with respect to both token and complete matching. Our parser obtains an unlabeled f-score of 93.23, resulting in, perhaps surprisingly, an error reduction of up to 6.5% over the best published performance reported in (Auli and Lopez, 2011). Our work indicates that highquality data-driven parsers can be built for producing more general dependency graphs, rather than trees. Nevertheless, empirical evaluation indicates that explicitly or implicitly using tree-structured information plays an essential role. The result also suggests that a wider range of complicated linguistic phenomena beyond surface syntax can be well modeled even without explicitly using grammars. Our algorithm is also applicable to other graphstructured representations, e.g. HPSG predicateargument analysis (Miyao et al., 2004). 2 Related Work Hockenmaier and Ste</context>
<context position="32407" citStr="Auli and Lopez, 2011" startWordPosition="5396" endWordPosition="5399">ic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with respect to the whole sentence. 5.5 Comparison to the State-of-the-art We compare our results with the best published CCG parsing performance obtained by the models presented in (Auli and Lopez, 2011) and (Xu et al., 2014)3. Auli and Lopez (2011) reported best numeric performance. The performance is evaluated on sentences that can be parsed by their model. Xu et al. (2014) reported the best published results for sentences with full coverage. All results on the test set is shown in Table 3. Even without any syntactic features, our parser achieves accuracies that are superior to Xu et al.’s parser and comparable to Auli and Lopez’s system. When unlabeled syntactic trees are provided, our parser outperform the state-of-the-art. 3The unlabeled parsing results are not reported in the original p</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011. Training a log-linear parser with loss functions via softmax-margin. In Proceedings of EMNLP, pages 333–343. Association for Computational Linguistics, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>178--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Sofia, Bulgaria.</location>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186. Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
<author>Yi Zhang</author>
</authors>
<title>Parser evaluation over local and non-local deep dependencies in a large corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>397--408</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="1731" citStr="Bender et al., 2011" startWordPosition="233" endWordPosition="236"> (CCG; Steedman, 2000) is a linguistically expressive grammar formalism which has a transparent yet elegant interface between syntax and semantics. By assigning each lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, l</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Zhang, 2011</marker>
<rawString>Emily M. Bender, Dan Flickinger, Stephan Oepen, and Yi Zhang. 2011. Parser evaluation over local and non-local deep dependencies in a large corpus. In Proceedings of EMNLP, pages 397– 408. Association for Computational Linguistics, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China.</location>
<contexts>
<context position="17740" citStr="Bohnet, 2010" startWordPosition="2893" endWordPosition="2894">must be labeled in order to encode the original graph. Here we introduce a label vector l = {l(i, j)}. For each (i, j) E Z, we assign a label l(i, j) to edge (i, j) as follows. Case y(i, j) = 1: label “X”; Case y(i, j) = 0 n y(j, i) = 1: label “X—R”; Case y(i, j) = 0 n y(j, i) = 0: label “None”. We can convert the labeled tree back to graph and obtain yt. Tough some edges are lost during the conversion, a lot more are kept. In fact, according to our evaluation, 92.74% of edges in the training set are retained after conversion. 3.4.2 Factorizing Trees We use the tree parsing model proposed in (Bohnet, 2010) to score the converted trees. The model factorizes a tree into 1st-order and 2ndorder factors. When decoding, the model searches for a tree with the best score. The score defined for graphs as well as trees is ft(yt) = gt(t, l) = 0T t 4bt(s, t, l) = 01T t 4b1 t (s, t, l) + 02T t 4b2 t (s, t, l) = (i,j)EZ t(i,j)01T t 4b1 t (l(i,j),w,p)) +02T t 4b2 t (s, t, l), where 4b1 t is the 1st-order features and 4b2t is the 2nd-order features. 3.5 Parsing as Optimization Motivated by linguistic properties of the semantics-oriented CCG dependencies, we have designed three single factorization models from </context>
<context position="25954" citStr="Bohnet, 2010" startWordPosition="4364" endWordPosition="4365">ncy trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous research on dependency parsing shows that structured perceptron (Collins, 2002) is one of the strongest discriminative learning algorithms. To estimate B’s of different models, we utilize the averaged perceptron algorithm. W</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97. Coling 2010 Organizing Committee, Beijing, China.</rawString>
</citation>
<citation valid="false">
<authors>
<author>ration</author>
</authors>
<booktitle>In Proceedings of the 16th conference on Computational linguistics -</booktitle>
<volume>1</volume>
<pages>340--345</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>ration, </marker>
<rawString>ration. In Proceedings of the 16th conference on Computational linguistics - Volume 1, pages 340–345. Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy A D Fowler</author>
<author>Gerald Penn</author>
</authors>
<title>Accurate context-free parsing with combinatory categorial grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>335--344</pages>
<location>Uppsala,</location>
<contexts>
<context position="2027" citStr="Fowler and Penn, 2010" startWordPosition="275" endWordPosition="278">), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres M</context>
<context position="6328" citStr="Fowler and Penn, 2010" startWordPosition="915" endWordPosition="918">., 1993). In CCGBank, PTB phrase-structure trees have been transformed into normal-form CCG derivations, and deep bi-lexical dependency graphs that encode functor-argument strcutures have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initializ</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>Timothy A. D. Fowler and Gerald Penn. 2010. Accurate context-free parsing with combinatory categorial grammar. In Proceedings of ACL, pages 335–344. Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the penn treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="4598" citStr="Hockenmaier and Steedman, 2007" startWordPosition="658" endWordPosition="661"> 2015. c�2015 Association for Computational Linguistics trees, which have many good computational properties. Simultaneously modeling the three properties yields intrinsically heterogeneous factorizations over the same graph, and hence results in intractability in decoding. Inspired by (Koo et al., 2010; Rush et al., 2010), we employ dual decomposition to perform principled decoding. Though not always, we can obtain the optimal solution most of time. The time complexity of our parser is O(n3) when various 1st- and 2nd-order features are incorporated. We conduct experiments on English CCGBank (Hockenmaier and Steedman, 2007). Though our parser does not use any grammar information, including both lexical categories and syntactic derivations, it produces very accurate CCG dependency graphs with respect to both token and complete matching. Our parser obtains an unlabeled f-score of 93.23, resulting in, perhaps surprisingly, an error reduction of up to 6.5% over the best published performance reported in (Auli and Lopez, 2011). Our work indicates that highquality data-driven parsers can be built for producing more general dependency graphs, rather than trees. Nevertheless, empirical evaluation indicates that explicit</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the penn treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Slav Petrov</author>
</authors>
<title>Self-training with products of latent variable grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>12--22</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="25736" citStr="Huang et al., 2010" startWordPosition="4330" endWordPosition="4333">nt structure. Our experiments were performed using CCGBank which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23). We also use the syntactic dependency trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous resear</context>
</contexts>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with products of latent variable grammars. In Proceedings of EMNLP, pages 12–22. Association for Computational Linguistics, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy Holloway King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The PARC 700 dependency bank. In</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6651" citStr="King et al., 2003" startWordPosition="969" endWordPosition="972">underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the </context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700 dependency bank. In In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Uppsala,</location>
<contexts>
<context position="12237" citStr="Koo and Collins, 2010" startWordPosition="1915" endWordPosition="1918">cular predicate as a whole, we employ a Markov model. Let a1, · · · , am be the sequence of the arguments of the word wi under ypil. To keep the arguments in order, we constrain 1 &lt; aj1 &lt; aj2 &lt; n if j1 &lt; j2. In a k-th order predicatecentric model, we define T θp Φp(aj−(k−1), ..., aj, i, W, P) where aj (j &lt; 0 or j &gt; m + 1) are treated as specific initial or end state. Higher-order rather than arc-factored features can be conveniently extracted from adjacent arguments. This is similar to the sibling factorization defined by a number of syntactic tree parsers, e.g. (McDonald and Pereira, 2006), (Koo and Collins, 2010) and (Ma and Zhao, 2012). arg1 arg2 arg1 arg2 arg3 fpi (ypil) = m+k−1 � j=1 1547 Similarly, we define the initial and end states for pi (i &lt; 0 or i &gt; m + 1). arg2 arg1 arg1 arg1 arg1 In an Oct. 19 review of ... (S/S)/N NP/N N/N N/N N (NP\NP)/NP Figure 2: An example to illustrate the argumentcentric view. 3.3 Argument-Centric Factorization The syntactic principle for tree annotation treats the dependency relations between two words as syntactic projection. In another word, the head determines the syntactic category of the whole structure. The (type-logical) semantic principle determines a depen</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of ACL, pages 1–11. Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1288--1298</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="2665" citStr="Koo et al., 2010" startWordPosition="375" endWordPosition="378">is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency parser that efficiently produces globally optimal CCG dependency graphs acc</context>
<context position="4271" citStr="Koo et al., 2010" startWordPosition="607" endWordPosition="610">omplementadjunct, relationships. Finally, the CCG dependency graphs are not but look very much like 1545 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1545–1555, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics trees, which have many good computational properties. Simultaneously modeling the three properties yields intrinsically heterogeneous factorizations over the same graph, and hence results in intractability in decoding. Inspired by (Koo et al., 2010; Rush et al., 2010), we employ dual decomposition to perform principled decoding. Though not always, we can obtain the optimal solution most of time. The time complexity of our parser is O(n3) when various 1st- and 2nd-order features are incorporated. We conduct experiments on English CCGBank (Hockenmaier and Steedman, 2007). Though our parser does not use any grammar information, including both lexical categories and syntactic derivations, it produces very accurate CCG dependency graphs with respect to both token and complete matching. Our parser obtains an unlabeled f-score of 93.23, result</context>
<context position="7630" citStr="Koo et al., 2010" startWordPosition="1117" endWordPosition="1120">e projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled decoding for our factorization parser, we employ the dual decomposition technique. Our work directly follows (Koo et al., 2010). The two basic factorizations are similar to the model introduced in (Martins and Almeida, 2014). Lluis et al. (2013) introduced a dual decomposition based joint model for joint syntactic and semantic parsing. They are concerned with shallow semantic representation, i.e. Semantic Role Labeling, whose graphs are sparse. Different from their concern on integrating syntactic pars</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of EMNLP, pages 1288–1298. Association for Computational Linguistics, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Lluis</author>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Joint arc-factored parsing of syntactic and semantic dependencies.</title>
<date>2013</date>
<journal>TACL,</journal>
<pages>1--219</pages>
<marker>Lluis, Carreras, M`arquez, 2013</marker>
<rawString>Xavier Lluis, Xavier Carreras, and Lluis M`arquez. 2013. Joint arc-factored parsing of syntactic and semantic dependencies. TACL, 1:219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuezhe Ma</author>
<author>Hai Zhao</author>
</authors>
<title>Fourth-order dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>785--796</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="12261" citStr="Ma and Zhao, 2012" startWordPosition="1920" endWordPosition="1923">we employ a Markov model. Let a1, · · · , am be the sequence of the arguments of the word wi under ypil. To keep the arguments in order, we constrain 1 &lt; aj1 &lt; aj2 &lt; n if j1 &lt; j2. In a k-th order predicatecentric model, we define T θp Φp(aj−(k−1), ..., aj, i, W, P) where aj (j &lt; 0 or j &gt; m + 1) are treated as specific initial or end state. Higher-order rather than arc-factored features can be conveniently extracted from adjacent arguments. This is similar to the sibling factorization defined by a number of syntactic tree parsers, e.g. (McDonald and Pereira, 2006), (Koo and Collins, 2010) and (Ma and Zhao, 2012). arg1 arg2 arg1 arg2 arg3 fpi (ypil) = m+k−1 � j=1 1547 Similarly, we define the initial and end states for pi (i &lt; 0 or i &gt; m + 1). arg2 arg1 arg1 arg1 arg1 In an Oct. 19 review of ... (S/S)/N NP/N N/N N/N N (NP\NP)/NP Figure 2: An example to illustrate the argumentcentric view. 3.3 Argument-Centric Factorization The syntactic principle for tree annotation treats the dependency relations between two words as syntactic projection. In another word, the head determines the syntactic category of the whole structure. The (type-logical) semantic principle determines a dependency according the type</context>
</contexts>
<marker>Ma, Zhao, 2012</marker>
<rawString>Xuezhe Ma and Hai Zhao. 2012. Fourth-order dependency parsing. In Proceedings of COLING 2012: Posters, pages 785–796. The COLING 2012 Organizing Committee, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5714" citStr="Marcus et al., 1993" startWordPosition="826" endWordPosition="829">general dependency graphs, rather than trees. Nevertheless, empirical evaluation indicates that explicitly or implicitly using tree-structured information plays an essential role. The result also suggests that a wider range of complicated linguistic phenomena beyond surface syntax can be well modeled even without explicitly using grammars. Our algorithm is also applicable to other graphstructured representations, e.g. HPSG predicateargument analysis (Miyao et al., 2004). 2 Related Work Hockenmaier and Steedman (2007) developed linguistic resources, namely CCGBank, from the Penn Treebank (PTB; Marcus et al., 1993). In CCGBank, PTB phrase-structure trees have been transformed into normal-form CCG derivations, and deep bi-lexical dependency graphs that encode functor-argument strcutures have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler a</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Comput. Linguist., 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Mariana S C Almeida</author>
</authors>
<title>Priberam: A turbo semantic parser with second order features.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval</booktitle>
<pages>471--476</pages>
<contexts>
<context position="7947" citStr="Martins and Almeida, 2014" startWordPosition="1167" endWordPosition="1170">ls as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled decoding for our factorization parser, we employ the dual decomposition technique. Our work directly follows (Koo et al., 2010). The two basic factorizations are similar to the model introduced in (Martins and Almeida, 2014). Lluis et al. (2013) introduced a dual decomposition based joint model for joint syntactic and semantic parsing. They are concerned with shallow semantic representation, i.e. Semantic Role Labeling, whose graphs are sparse. Different from their concern on integrating syntactic parsing and semantic role labeling under 1storder factorization, we are interested in designing higher-order factorization models for more dense and general linguistic graphs. 3 Graph Factorization 3.1 Background Notations Consider a sentence s = (w, p) with words w = w1w2 · · · wn and POS-tags p = p1p2 · · · pn. First </context>
</contexts>
<marker>Martins, Almeida, 2014</marker>
<rawString>Andr´e F. T. Martins and Mariana S. C. Almeida. 2014. Priberam: A turbo semantic parser with second order features. In Proceedings of SemEval 2014, pages 471–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficient hpsg parsing with supertagging and cfg-filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence,</booktitle>
<pages>1671--1676</pages>
<publisher>Morgan Kaufmann publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2431" citStr="Matsuzaki et al., 2007" startWordPosition="334" endWordPosition="337">r NLP ap∗Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Efficient hpsg parsing with supertagging and cfg-filtering. In Proceedings of the 20th international joint conference on Artifical intelligence, pages 1671–1676. Morgan Kaufmann publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2594" citStr="McDonald et al., 2005" startWordPosition="362" endWordPosition="365">rsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency pars</context>
<context position="9670" citStr="McDonald et al., 2005" startWordPosition="1486" endWordPosition="1490">y(i, j) = 1 if a dependency with predicate i and argument j is in the graph, 0 otherwise. Note that y is not a matrix but a long vector though we use two indexes to index it. In this paper, we only consider the unlabeled parsing task. Nevertheless, it is quite straightforward to extend our models to labeled parsing. Let Y denote the set of all possible y. Given a function f : Y —* R that assigns scores to parse graphs, the optimal parse is y∗ = arg max f(y). yEY Following recent advances in discriminative dependency parsing, we build disambiguation models based on global linear models, as in (McDonald et al., 2005a). In this framework, we score a dependency graph using a linear model: fθ(y) = θTΦ(s, y), where Φ(s, y) produces a d-dimensional vector representation of the event that a CCG graph y is assigned to sentence s. In order to perform the decoding efficiently, we assume that the dependency graphs can be factored into smaller pieces. The main goal of this paper is to design appropriate factorization models, namely different types of fθ’s, to reflect essential properties of the semantics-oriented CCG dependency graphs. 3.2 Predicate-Centric Factorization The very fundamental view of the CCG depende</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98. Association for Computational Linguistics, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL2006)),</booktitle>
<volume>6</volume>
<pages>81--88</pages>
<contexts>
<context position="7124" citStr="McDonald and Pereira (2006)" startWordPosition="1041" endWordPosition="1044">014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled d</context>
<context position="12212" citStr="McDonald and Pereira, 2006" startWordPosition="1911" endWordPosition="1914"> of all arguments to one particular predicate as a whole, we employ a Markov model. Let a1, · · · , am be the sequence of the arguments of the word wi under ypil. To keep the arguments in order, we constrain 1 &lt; aj1 &lt; aj2 &lt; n if j1 &lt; j2. In a k-th order predicatecentric model, we define T θp Φp(aj−(k−1), ..., aj, i, W, P) where aj (j &lt; 0 or j &gt; m + 1) are treated as specific initial or end state. Higher-order rather than arc-factored features can be conveniently extracted from adjacent arguments. This is similar to the sibling factorization defined by a number of syntactic tree parsers, e.g. (McDonald and Pereira, 2006), (Koo and Collins, 2010) and (Ma and Zhao, 2012). arg1 arg2 arg1 arg2 arg3 fpi (ypil) = m+k−1 � j=1 1547 Similarly, we define the initial and end states for pi (i &lt; 0 or i &gt; m + 1). arg2 arg1 arg1 arg1 arg1 In an Oct. 19 review of ... (S/S)/N NP/N N/N N/N N (NP\NP)/NP Figure 2: An example to illustrate the argumentcentric view. 3.3 Argument-Centric Factorization The syntactic principle for tree annotation treats the dependency relations between two words as syntactic projection. In another word, the head determines the syntactic category of the whole structure. The (type-logical) semantic pri</context>
<context position="20499" citStr="McDonald and Pereira (2006)" startWordPosition="3385" endWordPosition="3388">h maximizes F(y) = fp(y)+fa(y). We can design the feature function 4ba and the parameter 0a, such 1549 that for all 1 G i1 G i2 G n, θTa Φa(0, i1, j, w, p) = 0 θTa Φa(i1, n + 1,j, w, p) = 0 T θa Φa(i1, i2, j, w, p) = −oc θTa Φa(0, n + 1, j, w, p) = −oc where n is the length of the sentence. Note that those 4 equations make the nodes except the root node in the optimal graph each have exactly one incoming edge. So the problem of finding a tree t maximizing fp(t) is reduced to this problem. Moreover, the NP-hard problem 3DM can be reduced to the problem of finding a tree t maximizing fp(t) (see McDonald and Pereira (2006)), leading to the NP-hardness of both of the problems. Let L* be the maximized value of L(yp, ya, yt; u) subjected to the constraints, then L* = minu L(u), according to the duality principle. 4.2.2 Decoding Algorithm There are two challenges in solving the dual problem. One challenge is to find the minimum value of the dual objective. For this, we can use subgradient method, as is demonstrated in Algorithm 1. The other is the evaluation of L(u). For this, we decompose the dual objective into three optimization problems. Let Bp = uTAp, Ba = uTAa, Bt = uTAt, and Ctl(i,j) = ( Bt(i,j), if l(i,j) =</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL2006)), volume 6, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>523--530</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="2594" citStr="McDonald et al., 2005" startWordPosition="362" endWordPosition="365">rsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency pars</context>
<context position="9670" citStr="McDonald et al., 2005" startWordPosition="1486" endWordPosition="1490">y(i, j) = 1 if a dependency with predicate i and argument j is in the graph, 0 otherwise. Note that y is not a matrix but a long vector though we use two indexes to index it. In this paper, we only consider the unlabeled parsing task. Nevertheless, it is quite straightforward to extend our models to labeled parsing. Let Y denote the set of all possible y. Given a function f : Y —* R that assigns scores to parse graphs, the optimal parse is y∗ = arg max f(y). yEY Following recent advances in discriminative dependency parsing, we build disambiguation models based on global linear models, as in (McDonald et al., 2005a). In this framework, we score a dependency graph using a linear model: fθ(y) = θTΦ(s, y), where Φ(s, y) produces a d-dimensional vector representation of the event that a CCG graph y is assigned to sentence s. In order to perform the decoding efficiently, we assume that the dependency graphs can be factored into smaller pieces. The main goal of this paper is to design appropriate factorization models, namely different types of fθ’s, to reflect essential properties of the semantics-oriented CCG dependency graphs. 3.2 Predicate-Centric Factorization The very fundamental view of the CCG depende</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of EMNLP, pages 523–530. Association for Computational Linguistics, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank.</title>
<date>2004</date>
<booktitle>In IJCNLP,</booktitle>
<pages>684--693</pages>
<contexts>
<context position="5568" citStr="Miyao et al., 2004" startWordPosition="804" endWordPosition="807">ublished performance reported in (Auli and Lopez, 2011). Our work indicates that highquality data-driven parsers can be built for producing more general dependency graphs, rather than trees. Nevertheless, empirical evaluation indicates that explicitly or implicitly using tree-structured information plays an essential role. The result also suggests that a wider range of complicated linguistic phenomena beyond surface syntax can be well modeled even without explicitly using grammars. Our algorithm is also applicable to other graphstructured representations, e.g. HPSG predicateargument analysis (Miyao et al., 2004). 2 Related Work Hockenmaier and Steedman (2007) developed linguistic resources, namely CCGBank, from the Penn Treebank (PTB; Marcus et al., 1993). In CCGBank, PTB phrase-structure trees have been transformed into normal-form CCG derivations, and deep bi-lexical dependency graphs that encode functor-argument strcutures have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). </context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun ichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank. In IJCNLP, pages 684–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng</booktitle>
<pages>49--56</pages>
<editor>and Ellen Riloff, editors,</editor>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="2617" citStr="Nivre et al., 2004" startWordPosition="366" endWordPosition="369">007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency parser that efficiently pro</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Hwee Tou Ng and Ellen Riloff, editors, HLTNAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 49–56. Association for Computational Linguistics, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="31582" citStr="Nivre and McDonald, 2008" startWordPosition="5264" endWordPosition="5267">R UF UEM No PC+AC+TA 93.03 92.03 92.53 32.61 Gr PC+AC+TA 93.71 92.72 93.21 38.14 Tr PC+AC+TA 93.63 92.83 93.23 37.47 Auli and Lopez 93.08 92.44 92.76 - Xu et al. 93.15 91.06 92.09 37.56 Table 3: Comparing the state-of-art with our models on test set. based. The architecture of the syntactic tree parser does not affect the results much. The two tree parsers give identical attachment scores, and lead to similar graph parsing accuracy. This result is somehow non-obvious given that the combination of a graph-based and transition-based parser usually gives significantly better parsing performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Although the target representation of our parser is general graphs rather trees, implicitly or explicitly using tree-structured information plays an essential role. Syntactic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improv</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958. Association for Computational Linguistics, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Sampath Kannan</author>
<author>Mitchell Marcus</author>
</authors>
<title>Finding optimal 1-endpoint-crossing trees.</title>
<date>2013</date>
<tech>TACL,</tech>
<pages>1--13</pages>
<contexts>
<context position="3113" citStr="Pitler et al., 2013" startWordPosition="440" endWordPosition="443">de, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency parser that efficiently produces globally optimal CCG dependency graphs according to a discriminative, factorization model. The design of the factorization is motivated by three essential properties of the CCG dependencies. First, all arguments associated with the same predicate are highly correlated due to the nature that they approximates type-logical semantics. Second, all predicates govern the same argument exhibit the hybrid syntactic/semantic, i.e. head-complementadjunct, relationships. Finally, the CCG dependen</context>
</contexts>
<marker>Pitler, Kannan, Marcus, 2013</marker>
<rawString>Emily Pitler, Sampath Kannan, and Mitchell Marcus. 2013. Finding optimal 1-endpoint-crossing trees. TACL, 1:13–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Mirella Lapata</author>
<author>Mark Steedman</author>
</authors>
<title>Large-scale semantic parsing without question-answer pairs.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL).</journal>
<contexts>
<context position="1895" citStr="Reddy et al., 2014" startWordPosition="256" endWordPosition="259"> lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade</context>
<context position="6166" citStr="Reddy et al., 2014" startWordPosition="887" endWordPosition="890"> (Miyao et al., 2004). 2 Related Work Hockenmaier and Steedman (2007) developed linguistic resources, namely CCGBank, from the Penn Treebank (PTB; Marcus et al., 1993). In CCGBank, PTB phrase-structure trees have been transformed into normal-form CCG derivations, and deep bi-lexical dependency graphs that encode functor-argument strcutures have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely ap</context>
</contexts>
<marker>Reddy, Lapata, Steedman, 2014</marker>
<rawString>Siva Reddy, Mirella Lapata, and Mark Steedman. 2014. Large-scale semantic parsing without question-answer pairs. Transactions of the Association for Computational Linguistics (TACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference,</booktitle>
<pages>72--82</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="7699" citStr="Rush and Collins, 2011" startWordPosition="1128" endWordPosition="1131">e-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled decoding for our factorization parser, we employ the dual decomposition technique. Our work directly follows (Koo et al., 2010). The two basic factorizations are similar to the model introduced in (Martins and Almeida, 2014). Lluis et al. (2013) introduced a dual decomposition based joint model for joint syntactic and semantic parsing. They are concerned with shallow semantic representation, i.e. Semantic Role Labeling, whose graphs are sparse. Different from their concern on integrating syntactic parsing and semantic role labeling under 1storder factorization, we are i</context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>Alexander M. Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 72– 82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4291" citStr="Rush et al., 2010" startWordPosition="611" endWordPosition="614">relationships. Finally, the CCG dependency graphs are not but look very much like 1545 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1545–1555, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics trees, which have many good computational properties. Simultaneously modeling the three properties yields intrinsically heterogeneous factorizations over the same graph, and hence results in intractability in decoding. Inspired by (Koo et al., 2010; Rush et al., 2010), we employ dual decomposition to perform principled decoding. Though not always, we can obtain the optimal solution most of time. The time complexity of our parser is O(n3) when various 1st- and 2nd-order features are incorporated. We conduct experiments on English CCGBank (Hockenmaier and Steedman, 2007). Though our parser does not use any grammar information, including both lexical categories and syntactic derivations, it produces very accurate CCG dependency graphs with respect to both token and complete matching. Our parser obtains an unlabeled f-score of 93.23, resulting in, perhaps surp</context>
<context position="7650" citStr="Rush et al., 2010" startWordPosition="1121" endWordPosition="1124">ted dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled decoding for our factorization parser, we employ the dual decomposition technique. Our work directly follows (Koo et al., 2010). The two basic factorizations are similar to the model introduced in (Martins and Almeida, 2014). Lluis et al. (2013) introduced a dual decomposition based joint model for joint syntactic and semantic parsing. They are concerned with shallow semantic representation, i.e. Semantic Role Labeling, whose graphs are sparse. Different from their concern on integrating syntactic parsing and semantic rol</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of EMNLP, pages 1–11. Association for Computational Linguistics, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Hpsg parsing with shallow dependency constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>624--631</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2451" citStr="Sagae et al., 2007" startWordPosition="338" endWordPosition="341">dence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eis</context>
</contexts>
<marker>Sagae, Miyao, Tsujii, 2007</marker>
<rawString>Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Hpsg parsing with shallow dependency constraints. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624–631. Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Shiftreduce dependency DAG parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>753--760</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="6955" citStr="Sagae and Tsujii (2008)" startWordPosition="1019" endWordPosition="1022"> the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been suc</context>
</contexts>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2008. Shiftreduce dependency DAG parsing. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 753–760. Coling 2008 Organizing Committee, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1133" citStr="Steedman, 2000" startWordPosition="146" endWordPosition="148">ure different syntacto-semantic properties of functor-argument dependencies. Integrating heterogeneous factorizations results in intractability in decoding. We propose a principled method to obtain optimal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a significant improvement over the best published result in the literature. Our implementation is available at http://www.icst. pku.edu.cn/lcwm/grass. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a linguistically expressive grammar formalism which has a transparent yet elegant interface between syntax and semantics. By assigning each lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Torres Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>342--350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics, Suntec, Singapore.</institution>
<contexts>
<context position="2646" citStr="Martins et al., 2009" startWordPosition="371" endWordPosition="374">). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency parser that efficiently produces globally optimal CCG de</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andre Torres Martins, Noah Smith, and Eric Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 342–350. Association for Computational Linguistics, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Filipe Torres Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>157--166</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="31612" citStr="Martins et al., 2008" startWordPosition="5269" endWordPosition="5272">92.53 32.61 Gr PC+AC+TA 93.71 92.72 93.21 38.14 Tr PC+AC+TA 93.63 92.83 93.23 37.47 Auli and Lopez 93.08 92.44 92.76 - Xu et al. 93.15 91.06 92.09 37.56 Table 3: Comparing the state-of-art with our models on test set. based. The architecture of the syntactic tree parser does not affect the results much. The two tree parsers give identical attachment scores, and lead to similar graph parsing accuracy. This result is somehow non-obvious given that the combination of a graph-based and transition-based parser usually gives significantly better parsing performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Although the target representation of our parser is general graphs rather trees, implicitly or explicitly using tree-structured information plays an essential role. Syntactic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with resp</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e Filipe Torres Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings of EMNLP, pages 157–166. Association for Computational Linguistics, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenduan Xu</author>
<author>Stephen Clark</author>
<author>Yue Zhang</author>
</authors>
<title>Shift-reduce ccg parsing with a dependency model.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>218--227</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="6501" citStr="Xu et al., 2014" startWordPosition="946" endWordPosition="949">res have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDo</context>
<context position="32429" citStr="Xu et al., 2014" startWordPosition="5401" endWordPosition="5404">rove the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with respect to the whole sentence. 5.5 Comparison to the State-of-the-art We compare our results with the best published CCG parsing performance obtained by the models presented in (Auli and Lopez, 2011) and (Xu et al., 2014)3. Auli and Lopez (2011) reported best numeric performance. The performance is evaluated on sentences that can be parsed by their model. Xu et al. (2014) reported the best published results for sentences with full coverage. All results on the test set is shown in Table 3. Even without any syntactic features, our parser achieves accuracies that are superior to Xu et al.’s parser and comparable to Auli and Lopez’s system. When unlabeled syntactic trees are provided, our parser outperform the state-of-the-art. 3The unlabeled parsing results are not reported in the original paper. The figures pres</context>
</contexts>
<marker>Xu, Clark, Zhang, 2014</marker>
<rawString>Wenduan Xu, Stephen Clark, and Yue Zhang. 2014. Shift-reduce ccg parsing with a dependency model. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 218–227. Association for Computational Linguistics, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-reduce CCG parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>683--692</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2475" citStr="Zhang and Clark, 2011" startWordPosition="342" endWordPosition="345">g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoin</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Shift-reduce CCG parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 683–692. Association for Computational Linguistics, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transitionbased dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="25896" citStr="Zhang and Nivre, 2011" startWordPosition="4353" endWordPosition="4356"> and the final test (Section 23). We also use the syntactic dependency trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous research on dependency parsing shows that structured perceptron (Collins, 2002) is one of the strongest discriminative learning algorithms. To estimate B’s of differe</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transitionbased dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193. Association for Computational Linguistics, Portland, Oregon, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>