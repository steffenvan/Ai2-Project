<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.993581">
LTAG Dependency Parsing with Bidirectional Incremental Construction
</title>
<author confidence="0.989605">
Libin Shen Aravind K. Joshi
</author>
<affiliation confidence="0.984846">
BBN Technologies University of Pennsylvania
</affiliation>
<email confidence="0.998976">
lshen@bbn.com joshi@cis.upenn.edu
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999698333333333">
In this paper, we first introduce a new archi-
tecture for parsing, bidirectional incremental
parsing. We propose a novel algorithm for in-
cremental construction, which can be applied
to many structure learning problems in NLP.
We apply this algorithm to LTAG dependency
parsing, and achieve significant improvement
on accuracy over the previous best result on
the same data set.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99981912">
The phrase “Bidirectional Incremental” may appear
self-contradictory at first sight, since incremental
parsing usually means left-to-right parsing in the
context of conventional parsing. In this paper, we
will extend the meaning of incremental parsing.
The idea of bidirectional parsing is related to
the bidirectional sequential classification method de-
scribed in (Shen et al., 2007). In that paper, a tagger
assigns labels to words of highest confidence first,
and then these labels in turn serve as the context of
later labelling operations. The bidirectional tagger
obtained the best results in literature on POS tagging
on the standard PTB dataset.
We extend this method from labelling to structure
learning, The search space of structure learning is
much larger, so that it is appropriate to exploit con-
fidence scores in search.
In this paper, we are interested in LTAG depen-
dency parsing because TAG parsing is a well known
problem of high computational complexity in reg-
ular parsing. In order to get a focus for the learn-
ing algorithm, we work on a variant of LTAG based
parsing in which we learn the word dependency re-
lations encoded in LTAG derivations instead of the
full-fledged trees.
</bodyText>
<subsectionHeader confidence="0.989082">
1.1 Parsing
</subsectionHeader>
<bodyText confidence="0.999942612903226">
Two types of parsing strategies are popular in nat-
ural language parsing, which are chart parsing and
incremental parsing.
Suppose the input sentence is w1w2...wn. Let cell
[i, j] represent wiwi+1...wj, a substring of the sen-
tence. As far as CFG parsing is concerned, a chart
parser computes the possible structures over all pos-
sible cells [i, j], where 1 ≤ i ≤ j ≤ n. The order
of computing on these n(n + 1)/2 cells is based on
some partial order �, such that [p1, p2] -� [q1, q2] if
q1 ≤ p1 ≤ p2 ≤ q2. In order to employ dynamic
programming, one can only use a fragment of a hy-
pothesis to represent the whole hypothesis, which
is assumed to satisfy conditional independence as-
sumption. It is well known that richer context rep-
resentation gives rise to better parsing performance
(Johnson, 1998). However, the need for tractability
does not allow much internal information to be used
to represent a hypothesis. The designs of hypothe-
ses in (Collins, 1999; Charniak, 2000) show a del-
icate balance between expressiveness and tractabil-
ity, which play an important role in natural language
parsing.
Some recent work on incremental parsing
(Collins and Roark, 2004; Shen and Joshi, 2005)
showed another way to handle this problem. In
these incremental parsers, tree structures are used
to represent the left context. In this way, one can
access the whole tree to collect rich context in-
formation at the expense of being limited to beam
search, which only maintains k-best results at each
</bodyText>
<page confidence="0.988053">
495
</page>
<note confidence="0.962142">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999255416666667">
step. Compared to chart parsing, incremental pars-
ing searches for the analyses for only 2n − 1 cells,
[1, 1], [2, 2], [1, 2], .., [i, i], [1, i], .., [1, n],incremen-
tally, while complex structures are used for the anal-
yses for each cell, which satisfy conditional inde-
pendence under a much weaker assumption.
In this paper, we call this particular approach
left-to-right incremental parsing, since one can also
search from right to left incrementally in a similar
way. A major problem of the left-to-right approach
is that one can only utilize the structural information
on the left side but not the right side.
</bodyText>
<subsectionHeader confidence="0.993745">
1.2 Parsing as Bidirectional Construction
</subsectionHeader>
<bodyText confidence="0.998746333333333">
A natural way to handle this problem is to employ
bidirectional search, which means we can dynami-
cally search the space in two directions. So we ex-
pand the idea of incremental parsing by introducing
greedy search. Specifically, we look for the hypothe-
ses over the cell [1, n] by building analyses over
2n −1 cells [ai,1, ai,2], i = 1,.., 2n −1 step by step,
where [a2n−1,1, a2n−1,2] = [1, n]. Furthermore, for
any [ai,1, ai,2]
</bodyText>
<listItem confidence="0.9977495">
• ai,1 = ai,2, or
•
</listItem>
<bodyText confidence="0.992436274509804">
∃j, k, such that [ai,1, ai,2] = [aj,1, ak,2], where
j &lt; i, k &lt; i and aj,2 + 1 = ak,1.
It is easy to show that the set {[ai,1, ai,2]  |1 &lt;
i &lt; 2n − 1} forms a tree relation, which means that
each cell except the last one will be used to build an-
other cell just once. In this framework, we can begin
with several starting points in a sentence and search
in any direction. So left-to-right parsing is only a
special case of incremental parsing defined in this
way. We still use complex structures to represent
the partial analyses, so as to employ both top-down
and bottom-up information as in (Collins and Roark,
2004; Shen and Joshi, 2005). Furthermore, we can
utilize the rich context on both sides of the partial
results.
Similar to bidirectional labelling in (Shen et al.,
2007), there are two learning tasking in this model.
First, we need to learn which cell we should choose.
At each step, we can select only one path. Sec-
ondly, we need to learn which operation we should
take for a given cell. We maintain k-best candidates
for each cell instead of only one, which differenti-
ates this model from normal greedy search. So our
model is more robust. Furthermore, we need to find
an effective way to iterate between these two tasks.
Instead of giving an algorithm specially designed
for parsing, we generalize the problem for graphs. A
sentence can be viewed as a graph in which words
are viewed as vertices and neighboring words are
connected with an arc. In Sections 2 and 3, we
will propose decoding and training algorithms re-
spectively for graph-based incremental construction,
which can be applied to many structure learning
problems in NLP.
We will apply this algorithm to dependency pars-
ing of Lexicalized Tree Adjoining Grammar (Joshi
and Schabes, 1997). Specifically, we will train and
evaluate an LTAG dependency parser over the LTAG
treebank described in Shen et al. (2008). We report
the experimental results on PTB section 23 of the
LTAG treebank. The accuracy on LTAG dependency
is 90.5%, which is 1.2 points over 89.3%, the previ-
ous best result (Shen and Joshi, 2005) on the same
data set.
It should be noted that PTB-based bracketed la-
belling is not an appropriate evaluation metric here,
since the experiments are on an LTAG treebank.
The derived trees in the LTAG treebank are different
from the CFG trees in PTB. Hence, we do not use
metrics such as labeled precision and labeled recall
for evaluation.
</bodyText>
<sectionHeader confidence="0.945359" genericHeader="method">
2 Graph-based Incremental Construction
</sectionHeader>
<subsectionHeader confidence="0.975892">
2.1 Idea and Data Structures
</subsectionHeader>
<bodyText confidence="0.997774642857143">
Now we define the problem formally. We will use
dependency parsing as an example to illustrate the
idea.
We are given a connected graph G(V, E) whose
hidden structure is U, where V = {vi}, E C
V x V is a symmetric relation, and U = {uk} is
composed of a set of elements that vary with ap-
plications. As far as dependency parsing is con-
cerned, the input graph is simply a chain of ver-
tices, where E(vi−1, vi), and its hidden structure is
{uk = (vsk, vek, bk)}, where vertex vek depends on
vertex vsk with label bk.
A graph-based incremental construction algo-
rithm looks for the hidden structure in a bottom-up
</bodyText>
<page confidence="0.998559">
496
</page>
<bodyText confidence="0.999690019230769">
style.
Let xi and xj be two sets of connected vertexes
in V , where xi n xj = 0 and they are directly con-
nected via an edge in E. Let yxi be a hypothesized
hidden structure of xi, and yxj a hypothesized hid-
den structure of xj.
Suppose we choose to combine yxi and yxj with
an operation r to build a hypothesized hidden struc-
ture for xk = xi U xj. We say the process of con-
struction is incremental if the output of the opera-
tion, yxk = r(xi, xj, yxi, yxj) _Q yxi U yxj for all
the possible xi, xj, yxi, yxj and operation r. As far
as dependency parsing is concerned, incrementality
means that we cannot remove any links coming from
the substructures.
Once yxk is built, we can no longer use yxi or
yxj as a building block. It is easy to see that left
to right incremental construction is a special case of
our approach. So the question is how we decide the
order of construction as well as the type of operation
r. For example, in the very first step of dependency
parsing, we need to decide which two words are to
be combined as well as the dependency label to be
used.
This problem is solved statistically, based on the
features defined on the substructures involved in the
operation and their context. Suppose we are given
the weights of these features, we will show in the
next section how these parameters guide us to build
a set of hypothesized hidden structures with beam
search. In Section 3, we will present a Perceptron
like algorithm (Collins, 2002; Daum´e III and Marcu,
2005) to obtain the parameters.
Now we introduce the data structure to be used in
our algorithms.
A fragment is a connected sub-graph of G(V, E).
Each fragment x is associated with a set of hypothe-
sized hidden structures, or fragment hypotheses for
short: Y x = {yx1, ..., yxk}. Each yx is a possible frag-
ment hypothesis of x.
It is easy to see that an operation to combine two
fragments may depend on the fragments in the con-
text, i.e. fragments directly connected to one of the
operands. So we introduce the dependency relation
over fragments. Suppose there is a dependency re-
lation D C F x F, where F C 2V is the set of all
fragments in graph G. D(xi, xj) means that any op-
eration on a fragment hypothesis of xi depends on
the features in the fragment hypothesis of xj, and
vice versa.
We are especially interested in the following two
dependency relations.
</bodyText>
<listItem confidence="0.818102333333333">
• level-0 dependency: D0(xi, xj) i = j.
• level-1 dependency: D1(xi, xj) xi and
xj are directly connected in G.
</listItem>
<bodyText confidence="0.999521052631579">
Level-0 dependency means that the features of
a hypothesis for a vertex xi do not depend on the
hypotheses for other vertices. Level-1 dependency
means that the features depend on the hypotheses of
nearby vertices only.
The learning algorithm for level-0 dependency is
similar to the guided learning algorithm for labelling
as described in (Shen et al., 2007). Level-1 depen-
dency requires more data structures to maintain the
hypotheses with dependency relations among them.
However, we do not get into the details of level-1
formalism in this papers for two reasons. One is the
limit of page space and depth of a conference pa-
per. On the other hand, our experiments show that
the parsing performance with level-1 dependency is
close to what level-0 dependency could provides.
Interested readers could refer to (Shen, 2006) for
detailed description of the learning algorithms for
level-1 dependency.
</bodyText>
<subsectionHeader confidence="0.99618">
2.2 Algorithms
</subsectionHeader>
<bodyText confidence="0.999258833333333">
Algorithm 1 shows the procedure of building hy-
potheses incrementally on a given graph G(V, E).
Parameter k is used to set the beam width of search.
Weight vector w is used to compute score of an op-
eration.
We have two sets, H and Q, to maintain hypothe-
ses. Hypotheses in H are selected in beam search,
and hypotheses in Q are candidate hypotheses for
the next step of search in various directions.
We first initiate the hypotheses for each vertex,
and put them into set H. For example, in depen-
dency parsing, the initial value is a set of possible
POS tags for each single word. Then we use a queue
Q to collect all the possible hypotheses over the ini-
tial hypotheses H.
Whenever Q is not empty, we search for the hy-
pothesis with the highest score according to a given
weight vector w. Suppose we find (x, y). We select
</bodyText>
<page confidence="0.988275">
497
</page>
<figure confidence="0.970392071428571">
Algorithm 1 Incremental Construction
Require: graph G(V, E);
Require: beam width k;
Require: weight vector w;
1: H ← initH();
2: Q ← initQ(H);
3: repeat
4: (x′, y′) ← arg max(x�y)∈Q score(y);
5: H ← updateH(H, x′);
6: Q ← updateQ(Q, H, x′);
7: until (Q = 0)
NN NN
DT NN MD VB CD NN
the student will take four courses
</figure>
<figureCaption confidence="0.998908">
Figure 1: After initialization
</figureCaption>
<figure confidence="0.900625428571429">
NN NN CD NN
DT NN MD VB CD NN
the student will take four courses
Figure 2: Step 1
DT NN NN NN CD NN
DT NN MD VB CD NN
the student will take four courses
</figure>
<figureCaption confidence="0.998417">
Figure 3: Step 2
</figureCaption>
<bodyText confidence="0.999725">
top k-best hypotheses for segment x from Q and use
them to update H. Then we remove from Q all the
hypotheses for segments that have overlap with seg-
ment x. In the end, we build new candidate hypothe-
ses with the updated selected hypothesis set H, and
add them to Q.
</bodyText>
<subsectionHeader confidence="0.999724">
2.3 An Example
</subsectionHeader>
<bodyText confidence="0.999972875">
We use an example of dependency parsing to illus-
trate the incremental construction algorithm first.
Suppose the input sentence is the student will take
four courses. We are also given the candidate POS
tags for each word. So the graph is just a linear struc-
ture in this case. We use level-0 dependency and set
beam width to two.
We use boxes to represent fragments. The depen-
dency links are from the parent to the child.
Figure 1 shows the result after initialization. Fig-
ure 2 shows the result after the first step, combining
the fragments of four and courses. Figure 3 shows
the result after the second step, combining the and
student, and figure 4 shows the result after the third
step, combining take and four courses. Due to lim-
ited space, we skip the rest operations.
</bodyText>
<subsectionHeader confidence="0.896378">
2.4 Description
</subsectionHeader>
<bodyText confidence="0.78222">
Now we will explain the functions in Algorithm 1
one by one.
• initH() initiates hypotheses for each vertex.
Here we set the initial fragment hypotheses,
</bodyText>
<equation confidence="0.547782666666667">
Y xi = {yxi
1 , ..., yxi
k }, where xi = {vi} con-
</equation>
<bodyText confidence="0.691312">
tains only one vertex.
</bodyText>
<listItem confidence="0.5810865">
• initQ(H) initiates the queue of candidate op-
erations over the current hypotheses H. Sup-
posed there exist segments xi and xj which are
directly connected in G. We apply all possi-
ble operations to all fragment hypotheses for xj
and xj, and add the result hypotheses in Q. For
example, we generate (x, y) with some opera-
tion r, where segment x is xi U xj.
All the candidate operations are organized with
respect to the segments. For each segment, we
maintain top k candidates according to their
scores.
• updateH(H, x) is used to update hypotheses in
H. First, we remove from H all the hypotheses
</listItem>
<bodyText confidence="0.785872">
whose corresponding segment is a sub-set of x.
Then, we add into H the top k hypotheses for
segment x.
</bodyText>
<listItem confidence="0.7600788">
• updateQ(Q, H, x) is also designed to complete
two tasks. First, we remove from Q all the
hypotheses whose corresponding segment has
overlap with segment x. Then, we add new
candidate hypotheses depending on x in a way
</listItem>
<page confidence="0.990624">
498
</page>
<table confidence="0.6030442">
DT NN MD NN CD NN
DT NN MD VB CD NN
the student will take four courses
Figure 4: Step 3
Algorithm 2 Parameter Optimization
</table>
<listItem confidence="0.9697845625">
1: w ← 0;
2: for (round r = 0; r &lt; R; r++) do
3: load graph Gr(V, E), gold standard Hr;
4: initiate H and Q;
5: repeat
6: (x′, y′) ← arg max(x�y)EQ score(y);
7: if (y′ is compatible with Hr) then
8: update H and Q;
9: else
10: y� ← positive(Q, x′);
11: promote(w, y);
12: demote(w, y′);
13: update Q with w;
14: end if
15: until (Q = 0)
16: end for
</listItem>
<bodyText confidence="0.997158">
similar to the initQ(H) function. For each seg-
ment, we maintain the top k candidates for each
segment.
</bodyText>
<sectionHeader confidence="0.996032" genericHeader="method">
3 Parameter Optimization
</sectionHeader>
<bodyText confidence="0.999896870967742">
In the previous section, we described an algorithm
for graph-based incremental construction for a given
weight vector w. In Algorithm 2, we present a Per-
ceptron like algorithm to obtain the weight vector
for the training data.
For each given training sample (Gr, Hr), where
Hr is the gold standard hidden structure of graph
Gr, we first initiate cut T, hypotheses HT and can-
didate queue Q by calling initH and initQ as in Al-
gorithm 1.
Then we use the gold standard Hr to guide the
search. We select candidate (x′, y′) which has the
highest operation score in Q. If y′ is compatible with
Hr, we update H and Q by calling updateH and
updateQ as in Algorithm 1. If y′ is incompatible
with Hr, we treat y′ as a negative sample, and search
for a positive sample y� in Q with positive(Q, x′).
If there exists a hypothesis yx′ for fragment x′
which is compatible with Hr, then positive(Q, x′)
returns yx′. Otherwise positive(Q, x′) returns the
candidate hypothesis which is compatible with Hr
and has the highest operation score in Q.
Then we update the weight vector w with y� and
y′. At the end, we update the candidate Q by using
the new weights w.
In order to improve the performance, we use Per-
ceptron with margin in the training (Krauth and
M´ezard, 1987). The margin is proportional to the
loss of the hypothesis. Furthermore, we use aver-
aged weights (Collins, 2002; Freund and Schapire,
1999) in Algorithm 1.
</bodyText>
<sectionHeader confidence="0.997309" genericHeader="method">
4 LTAG Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999963615384615">
We apply the new algorithm to LTAG dependency
parsing on an LTAG Treebank (Shen et al., 2008)
extracted from Penn Treebank (Marcus et al., 1994)
and Proposition Bank (Palmer et al., 2005). Penn
Treebank was previously used to train and evalu-
ate various dependency parsers (Yamada and Mat-
sumoto, 2003; McDonald et al., 2005). In these
works, Magerman’s rules are used to pick the head
at each level according to the syntactic labels in a
local context.
The dependency relation encoded in the LTAG
Treebank reveals deeper information for the follow-
ing two reasons. First, the LTAG architecture itself
reveals deeper dependency. Furthermore, the PTB
was reconciled with the Propbank in the LTAG Tree-
bank extraction (Shen et al., 2008).
We are especially interested in the two types of
structures in the LTAG Treebank, predicate adjunc-
tion and predicate coordination. They are used to
encode dependency relations which are unavailable
in other approaches. On the other hand, these struc-
tures turn out to be a big problem for the general rep-
resentation of dependency relations, including ad-
junction and coordination. We will show that the
algorithm proposed here provides a nice solution for
this problem.
</bodyText>
<page confidence="0.999254">
499
</page>
<figureCaption confidence="0.999679">
Figure 5: Predicate Adjunction
</figureCaption>
<subsectionHeader confidence="0.995628">
4.1 Representation of the LTAG Treebank
</subsectionHeader>
<bodyText confidence="0.971622695652174">
In the LTAG Treebank (Shen et al., 2008), each word
is associated with a spinal template, which repre-
sents the projection from the lexical item to the root.
Templates are linked together to form a derivation
tree. The topology of the derivation tree shows a
type of dependency relation, which we call LTAG
dependency here.
There are three types of operations in the LTAG
Treebank, which are attachment, adjunction, and co-
ordination. Attachment is used to represent both
substitution and sister adjunction in the traditional
LTAG. So it is similar to the dependency relation in
other approaches.
The LTAG dependency can be a non-projective
relation thanks to the operation of adjunction. In
the LTAG Treebank, raising verbs and passive ECM
verbs are represented as auxiliary trees to be ad-
joined. In addition, adjunction is used to handle
many cases of discontinuous arguments in Prop-
bank. For example, in the following sentence,
ARG1 of says in Propbank is discontinuous, which
is First Union now has packages for seven customer
groups.
</bodyText>
<listItem confidence="0.995738">
• First Union, he says, now has packages for
seven customer groups.
</listItem>
<bodyText confidence="0.970733666666667">
In the LTAG Treebank, the subtree for he says ad-
joins onto the node of has, which is the root of the
derivation tree, as shown in Figure 5.
Another special aspect of the LTAG Treebank is
the representation of predicate coordination. Figure
6 is the representation of the following sentence.
</bodyText>
<listItem confidence="0.974893">
• I couldn’t resist rearing up on my soggy loafers
and saluting.
</listItem>
<bodyText confidence="0.998876">
The coordination between rearing and saluting is
represented explicitly with a coord-structure, and
</bodyText>
<figureCaption confidence="0.99912">
Figure 6: Predicate Coordination
</figureCaption>
<figure confidence="0.681296">
amid
</figure>
<figureCaption confidence="0.99991">
Figure 7: Non-projective Adjunction
</figureCaption>
<bodyText confidence="0.99078775">
this coord-structure attaches to resist. It is shown
in (Shen et al., 2008) that coord-structures could en-
code the ambiguity of argument sharing, which can
be non-projective also.
</bodyText>
<subsectionHeader confidence="0.901859">
4.2 Incremental Construction
</subsectionHeader>
<bodyText confidence="0.99967775">
We build LTAG derivation trees incrementally. A
hypothesis of a fragment is represented with a par-
tial derivation tree. When the fragment hypotheses
of two nearby fragments combine, the partial deriva-
tion trees are combined into one.
It is trivial to combine two partial derivation trees
with attachment. We simply attach the root of one
tree to some node on the other tree which is visible to
this root node. Adjunction is similar to attachment,
except that an adjoined subtree may be visible from
the other side of the derivation tree. For example, in
sentence
</bodyText>
<listItem confidence="0.754361">
• The stock of UAL Corp. continued to be
pounded amid signs that British Airways ...
</listItem>
<bodyText confidence="0.999754333333333">
continued adjoins onto pounded, and amid attaches
to continued from the other side of the derivation
tree (pounded is between continued and amid), as
shown in Figure 7.
The predicate coordination is decomposed into a
set of operations to meet the need for incremen-
tal processing. Suppose a coordinated structure at-
taches to the parent node on the left side. We build
this structure incrementally by attaching the first
</bodyText>
<figure confidence="0.9983054">
attach
has
attach
union
adjoin
packages
attach
attach
says
now
he
resist
attach attach
I
coordination
rearing
saluting
attach
and
attach
pounded
stock
continued
adjoin
attach
</figure>
<page confidence="0.641743">
500
</page>
<figureCaption confidence="0.998064">
Figure 8: Conjunction
</figureCaption>
<figure confidence="0.990440909090909">
mr
attach
�
✑ ◗◗s
✰✑
m1
✢.....
✠ � ❅❘ ❅❘
m1.2 m1.1 s1 s2
❅❘
m1.1.1
</figure>
<figureCaption confidence="0.999986">
Figure 9: Representation of nodes
</figureCaption>
<bodyText confidence="0.999579">
conjunct to the parent and conjoining other con-
juncts to first one. In this way, we do not need to
force the coordination to be built before the attach-
ment. Either can be executed first. A sample is
shown in Figure 8.
</bodyText>
<subsectionHeader confidence="0.985097">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.999598666666667">
In this section, we will describe the features used in
LTAG dependency parsing. An operation is repre-
sented by a 4-tuple
</bodyText>
<listItem confidence="0.93369">
• op = (type, dir, posleft, posright),
</listItem>
<bodyText confidence="0.999853717391304">
where type E {attach, adjoin, conjoin} and dir
is used to represent the direction of the operation.
posleft and posright are the POS tags of the two
operands.
Features are defined on POS tags and lexical items
of the nodes in the context. In order to represent the
features, we use m for the main-node of the oper-
ation, s for the sub-node, mr for the parent of the
main-node, m1..mi for the children of m, and s1..sj
for the children of s, as shown in Figure 9. The in-
dex always starts from the side where the operation
takes place. We use the Gorn addresses to represent
the nodes in the subtrees rooted on m and s.
Furthermore, we use lk and rk to represent the
nodes in the left and right context of the flat sen-
tence. We use hl and hr to represent the head of the
hypothesis trees on the left and right context respec-
tively. Let x be a node. We use x.p to represent the
POS tag of node x, and x.w to represent the lexical
item of node x.
Table 1 show the features used in LTAG depen-
dency parsing. There are seven classes of features.
The first three classes of features are those defined
on only one operand, on both operands, and on the
siblings respectively. If gold standard POS tags are
used as input, we define features on the POS tags in
the context. If level-1 dependency is used, we define
features on the root node of the hypothesis partial
derivation trees in the neighborhood.
Half check and full check features are designed
for grammatical check. For example, in Figure 9,
node s attaches onto node m from left. Then nothing
can attach onto s from the right side. The children of
the right side of s are fixed, so we use the half check
features to check the completeness of the children
of the right half for s. Furthermore, we notice that
all the rightmost descendants of s and the leftmost
descendants of m at each level become unavailable
for any further operation. So their children are fixed
after this operation. All these nodes are in the form
of m1.1...1 or s1.1...1. We use full check features to
check the children from both sides for these nodes.
In the discussion above, we ignored adjunction
and conjunction. We need to slightly refine the con-
ditions of checking. Due to the limit of space, we
skip these cases.
</bodyText>
<sectionHeader confidence="0.998691" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998749933333333">
We use the same data set as in (Shen and Joshi,
2005). We use Sec. 2-21 of the LTAG Treebank for
training, Sec. 22 for feature selection, and Sec. 23
for test. Table 2 shows the comparison of different
models. Beam size is set to five in our experiments.
With level-0 dependency, our system achieves an ac-
curacy of 90.3% at the speed of 4.25 sentences a sec-
ond on a Xeon 3G Hz processor with JDK 1.5. With
level-1 dependency, the parser achieves 90.5% at
3.59 sentences a second. Level-1 dependency does
not provide much improvement due to the fact that
level-0 features provide most of the useful informa-
tion for this specific application.
It is interesting to compare our system with other
dependency parsers. The accuracy on LTAG depen-
</bodyText>
<figure confidence="0.998259133333333">
attach
I
attach
resist
rearing
conjoin
and
saluting
attach
m2
❄
m
�
❄
s
</figure>
<page confidence="0.979489">
501
</page>
<bodyText confidence="0.782738740740741">
category description templates
one operand Features defined on only one operand. For each (m.p), (m.w), (m.p, m.w), (s.p),
template tp, [type, dir, tp] is used as a feature. (s.w), (s.p, s.w)
two operands Features defined on both operands. For each tem- (m.w), (s.w), (m.w, s.w)
plate tp, [op, tp] is used as a feature. In addition,
[op] is also used as a feature.
siblings Features defined on the children of the (m1.p), (m1.p, m2.p), ..,
main nodes. For each template tp, (m1.p, m2.p, .., mZ.p)
[op, tp], [op, m.w, tp], [op, mr.p, tp] and
[op, mr.p, m.w, tp] are used as features.
POS context In the case that gold standard POS tags are used (l2.p), (l1.p), (r1.p), (r2.p),
as input, features are defined on the POS tags of (l2.p, l1.p), (l1.p, r1.p),
the context. For each template tp, [op, tp] is used (r1.p, r2.p)
as a feature.
tree context In the case that level-1 dependency is employed, (hl.p), (hr.p)
features are defined on the trees in the context.
For each template tp, [op, tp] is used as a feature.
half check Suppose s1, ..., sk are all the children of s which (s.p, s1.p, s2.p, .., sk.p),
are between s and m in the flat sentence. For (m.p, s.p, s1.p, s2.p, .., sk.p)
each template tp, [tp] is used as a feature. and (s.w, s.p, s1.p, s2.p, .., sk.p),
(s.w, m.p, s.p, s1.p, s2.p, .., sk.p)
if s.w is a verb
full check Let x1, x2, .., xk be the children of x, and xr (x.p, x1.p, x2.p, .., xk.p),
the parent of x. For any x = m1.1...1 or s1.1...1, (xr.p, x.p, x1.p, x2.p, .., xk.p) and
template tp, [tp(x)] is used as a feature. (x.w, x.p, x1.p, x2.p, .., xk.p),
(x.w, xr.p, x.p, x1.p, x2.p, .., xk.p)
if x.w is a verb
</bodyText>
<tableCaption confidence="0.997612">
Table 1: Features defined on the context of operation
</tableCaption>
<table confidence="0.99680475">
model accuracy%
Shen and Joshi, 2005 89.3
level-0 dependency 90.3
level-1 dependency 90.5
</table>
<tableCaption confidence="0.999702">
Table 2: Experiments on Sec. 23 of the LTAG Treebank
</tableCaption>
<bodyText confidence="0.99992525">
dency is comparable to the numbers of the previ-
ous best systems on dependency extracted from PTB
with Magerman’s rules, for example, 90.3% in (Ya-
mada and Matsumoto, 2003) and 90.9% in (McDon-
ald et al., 2005). However, their experiments are on
the PTB, while ours is on the LTAG corpus.
It should be noted that it is more difficult to learn
LTAG dependencies. Theoretically, the LTAG de-
pendencies reveal deeper relations. Adjunction can
lead to non-projective dependencies, and the depen-
dencies defined on predicate adjunction are linguis-
tically more motivated, as shown in the examples in
Figure 5 and 7. The explicit representation of predi-
cate coordination also provides deeper relations. For
example, in Figure 6, the LTAG dependency con-
tains resist → rearing and resist → saluting,
while the Magerman’s dependency only contains
resist → rearing. The explicit representation of
predicate coordination will help to solve for the de-
pendencies for shared arguments.
</bodyText>
<sectionHeader confidence="0.999834" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99605375">
In our approach, each fragment in the graph is asso-
ciated with a hidden structure, which means that we
cannot reduce it to a labelling task. Therefore, the
problem of interest to us is different from previous
</bodyText>
<page confidence="0.990351">
502
</page>
<bodyText confidence="0.999950733333333">
work on graphical models, such as CRF (Lafferty et
al., 2001) and MMMN (Taskar et al., 2003).
McAllester et al. (2004) introduced Case-Factor
Diagram (CFD) to transform a graph based con-
struction problem to a labeling problem. However,
adjunction, prediction coordination, and long dis-
tance dependencies in LTAG dependency parsing
make it difficult to implement. Our approach pro-
vides a novel alternative to CFD.
Our learning algorithm stems from Perceptron
training in (Collins, 2002). Variants of this method
have been successfully used in many NLP tasks, like
shallow processing (Daum´e III and Marcu, 2005),
parsing (Collins and Roark, 2004; Shen and Joshi,
2005) and word alignment (Moore, 2005). Theoret-
ical justification for those algorithms can be applied
to our training algorithm in a similar way.
In our algorithm, dependency is defined on com-
plicated hidden structures instead of on a graph.
Thus long distance dependency in a graph becomes
local in hidden structures, which is desirable from
linguistic considerations.
The search strategy of our bidirectional depen-
dency parser is similar to that of the bidirectional
CFG parser in (Satta and Stock, 1994; Ageno and
Rodrguez, 2001; Kay, 1989). A unique contribu-
tion of this paper is that selection of path and deci-
sions about action are trained simultaneously with
discriminative learning. In this way, we can employ
context information more effectively.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999925538461538">
In this paper, we introduced bidirectional incremen-
tal parsing, a new architecture of parsing. We pro-
posed a novel algorithm for graph-based incremen-
tal construction, and applied this algorithm to LTAG
dependency parsing, revealing deep relations, which
are unavailable in other approaches and difficult to
learn. We evaluated the parser on an LTAG Tree-
bank. Experimental results showed significant im-
provement over the previous best system. Incre-
mental construction can be applied to other structure
learning problems of high computational complex-
ity, for example, such as machine translation and se-
mantic parsing.
</bodyText>
<sectionHeader confidence="0.996438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999772754716981">
A. Ageno and H. Rodrguez. 2001. Probabilistic mod-
elling of island-driven parsing. In International Work-
shop on Parsing Technologies.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL).
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the 2002
Conference of Empirical Methods in Natural Lan-
guage Processing.
H. Daum´e III and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proceedings of the 22nd In-
ternational Conference on Machine Learning.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277–296.
M. Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
A. K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69
– 124. Springer-Verlag.
M. Kay. 1989. Head-driven parsing. In Proceedings of
Workshop on Parsing Technologies.
W. Krauth and M. M´ezard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Physics A, 20:745–752.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for segmen-
tation and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learn-
ing.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
D. McAllester, M. Collins, and F. Pereira. 2004. Case-
factor diagrams for structured probabilistic modeling.
In UAI 2004.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings ofthe 43th Annual Meeting ofthe Association
for Computational Linguistics (ACL).
</reference>
<page confidence="0.985655">
503
</page>
<reference confidence="0.9999001">
R. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
G. Satta and O. Stock. 1994. Bi-Directional Context-
Free Grammar Parsing for Natural Language Process-
ing. Artificial Intelligence, 69(1-2).
L. Shen and A. K. Joshi. 2005. Incremental LTAG Pars-
ing. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing.
L. Shen, G. Satta, and A. K. Joshi. 2007. Guided Learn-
ing for Bidirectional Sequence Classification. In Pro-
ceedings ofthe 45th Annual Meeting ofthe Association
for Computational Linguistics (ACL).
L. Shen, L. Champollion, and A. K. Joshi. 2008. LTAG-
spinal and the Treebank: a new resource for incremen-
tal, dependency and semantic parsing. Language Re-
sources and Evaluation, 42(1):1–19.
L. Shen. 2006. Statistical LTAG Parsing. Ph.D. thesis,
University of Pennsylvania.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In Proceedings of the 17th Annual
Conference Neural Information Processing Systems.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with Support Vector Machines. In
IWPT 2003.
</reference>
<page confidence="0.998345">
504
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966235">
<title confidence="0.998763">LTAG Dependency Parsing with Bidirectional Incremental Construction</title>
<author confidence="0.999976">Libin Shen Aravind K Joshi</author>
<affiliation confidence="0.989198">BBN Technologies University of Pennsylvania</affiliation>
<email confidence="0.999738">lshen@bbn.comjoshi@cis.upenn.edu</email>
<abstract confidence="0.997763">In this paper, we first introduce a new architecture for parsing, bidirectional incremental parsing. We propose a novel algorithm for incremental construction, which can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Ageno</author>
<author>H Rodrguez</author>
</authors>
<title>Probabilistic modelling of island-driven parsing.</title>
<date>2001</date>
<booktitle>In International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="28783" citStr="Ageno and Rodrguez, 2001" startWordPosition="5051" endWordPosition="5054">w processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper is that selection of path and decisions about action are trained simultaneously with discriminative learning. In this way, we can employ context information more effectively. 7 Conclusion In this paper, we introduced bidirectional incremental parsing, a new architecture of parsing. We proposed a novel algorithm for graph-based incremental construction, and applied this algorithm to LTAG dependency parsing, revealing deep relations, which are unavailable in other approaches and difficult to learn. We evaluated the parser on an LTAG Treebank. Exp</context>
</contexts>
<marker>Ageno, Rodrguez, 2001</marker>
<rawString>A. Ageno and H. Rodrguez. 2001. Probabilistic modelling of island-driven parsing. In International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2757" citStr="Charniak, 2000" startWordPosition="447" endWordPosition="448">i ≤ j ≤ n. The order of computing on these n(n + 1)/2 cells is based on some partial order �, such that [p1, p2] -� [q1, q2] if q1 ≤ p1 ≤ p2 ≤ q2. In order to employ dynamic programming, one can only use a fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). However, the need for tractability does not allow much internal information to be used to represent a hypothesis. The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing. Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem. In these incremental parsers, tree structures are used to represent the left context. In this way, one can access the whole tree to collect rich context information at the expense of being limited to beam search, which only maintains k-best results at each 495 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processi</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2945" citStr="Collins and Roark, 2004" startWordPosition="474" endWordPosition="477">mming, one can only use a fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). However, the need for tractability does not allow much internal information to be used to represent a hypothesis. The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing. Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem. In these incremental parsers, tree structures are used to represent the left context. In this way, one can access the whole tree to collect rich context information at the expense of being limited to beam search, which only maintains k-best results at each 495 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504, Honolulu, October 2008.c�2008 Association for Computational Linguistics step. Compared to chart parsing, incremental parsing searches for the analyses for only 2n − 1 ce</context>
<context position="5163" citStr="Collins and Roark, 2004" startWordPosition="864" endWordPosition="867">,2, or • ∃j, k, such that [ai,1, ai,2] = [aj,1, ak,2], where j &lt; i, k &lt; i and aj,2 + 1 = ak,1. It is easy to show that the set {[ai,1, ai,2] |1 &lt; i &lt; 2n − 1} forms a tree relation, which means that each cell except the last one will be used to build another cell just once. In this framework, we can begin with several starting points in a sentence and search in any direction. So left-to-right parsing is only a special case of incremental parsing defined in this way. We still use complex structures to represent the partial analyses, so as to employ both top-down and bottom-up information as in (Collins and Roark, 2004; Shen and Joshi, 2005). Furthermore, we can utilize the rich context on both sides of the partial results. Similar to bidirectional labelling in (Shen et al., 2007), there are two learning tasking in this model. First, we need to learn which cell we should choose. At each step, we can select only one path. Secondly, we need to learn which operation we should take for a given cell. We maintain k-best candidates for each cell instead of only one, which differentiates this model from normal greedy search. So our model is more robust. Furthermore, we need to find an effective way to iterate betwe</context>
<context position="28234" citStr="Collins and Roark, 2004" startWordPosition="4964" endWordPosition="4967">els, such as CRF (Lafferty et al., 2001) and MMMN (Taskar et al., 2003). McAllester et al. (2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem. However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement. Our approach provides a novel alternative to CFD. Our learning algorithm stems from Perceptron training in (Collins, 2002). Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper i</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>M. Collins and B. Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2740" citStr="Collins, 1999" startWordPosition="445" endWordPosition="446"> j], where 1 ≤ i ≤ j ≤ n. The order of computing on these n(n + 1)/2 cells is based on some partial order �, such that [p1, p2] -� [q1, q2] if q1 ≤ p1 ≤ p2 ≤ q2. In order to employ dynamic programming, one can only use a fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). However, the need for tractability does not allow much internal information to be used to represent a hypothesis. The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing. Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem. In these incremental parsers, tree structures are used to represent the left context. In this way, one can access the whole tree to collect rich context information at the expense of being limited to beam search, which only maintains k-best results at each 495 Proceedings of the 2008 Conference on Empirical Methods in Natural </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9110" citStr="Collins, 2002" startWordPosition="1576" endWordPosition="1577">decide the order of construction as well as the type of operation r. For example, in the very first step of dependency parsing, we need to decide which two words are to be combined as well as the dependency label to be used. This problem is solved statistically, based on the features defined on the substructures involved in the operation and their context. Suppose we are given the weights of these features, we will show in the next section how these parameters guide us to build a set of hypothesized hidden structures with beam search. In Section 3, we will present a Perceptron like algorithm (Collins, 2002; Daum´e III and Marcu, 2005) to obtain the parameters. Now we introduce the data structure to be used in our algorithms. A fragment is a connected sub-graph of G(V, E). Each fragment x is associated with a set of hypothesized hidden structures, or fragment hypotheses for short: Y x = {yx1, ..., yxk}. Each yx is a possible fragment hypothesis of x. It is easy to see that an operation to combine two fragments may depend on the fragments in the context, i.e. fragments directly connected to one of the operands. So we introduce the dependency relation over fragments. Suppose there is a dependency </context>
<context position="16549" citStr="Collins, 2002" startWordPosition="2952" endWordPosition="2953">mple y� in Q with positive(Q, x′). If there exists a hypothesis yx′ for fragment x′ which is compatible with Hr, then positive(Q, x′) returns yx′. Otherwise positive(Q, x′) returns the candidate hypothesis which is compatible with Hr and has the highest operation score in Q. Then we update the weight vector w with y� and y′. At the end, we update the candidate Q by using the new weights w. In order to improve the performance, we use Perceptron with margin in the training (Krauth and M´ezard, 1987). The margin is proportional to the loss of the hypothesis. Furthermore, we use averaged weights (Collins, 2002; Freund and Schapire, 1999) in Algorithm 1. 4 LTAG Dependency Parsing We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper informati</context>
<context position="28075" citStr="Collins, 2002" startWordPosition="4941" endWordPosition="4942">hich means that we cannot reduce it to a labelling task. Therefore, the problem of interest to us is different from previous 502 work on graphical models, such as CRF (Lafferty et al., 2001) and MMMN (Taskar et al., 2003). McAllester et al. (2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem. However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement. Our approach provides a novel alternative to CFD. Our learning algorithm stems from Perceptron training in (Collins, 2002). Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency p</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>Learning as search optimization: Approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning.</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>H. Daum´e III and D. Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In Proceedings of the 22nd International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="16577" citStr="Freund and Schapire, 1999" startWordPosition="2954" endWordPosition="2957">th positive(Q, x′). If there exists a hypothesis yx′ for fragment x′ which is compatible with Hr, then positive(Q, x′) returns yx′. Otherwise positive(Q, x′) returns the candidate hypothesis which is compatible with Hr and has the highest operation score in Q. Then we update the weight vector w with y� and y′. At the end, we update the candidate Q by using the new weights w. In order to improve the performance, we use Perceptron with margin in the training (Krauth and M´ezard, 1987). The margin is proportional to the loss of the hypothesis. Furthermore, we use averaged weights (Collins, 2002; Freund and Schapire, 1999) in Algorithm 1. 4 LTAG Dependency Parsing We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two rea</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG Models of Linguistic Tree Representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2581" citStr="Johnson, 1998" startWordPosition="419" endWordPosition="420">resent wiwi+1...wj, a substring of the sentence. As far as CFG parsing is concerned, a chart parser computes the possible structures over all possible cells [i, j], where 1 ≤ i ≤ j ≤ n. The order of computing on these n(n + 1)/2 cells is based on some partial order �, such that [p1, p2] -� [q1, q2] if q1 ≤ p1 ≤ p2 ≤ q2. In order to employ dynamic programming, one can only use a fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). However, the need for tractability does not allow much internal information to be used to represent a hypothesis. The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing. Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem. In these incremental parsers, tree structures are used to represent the left context. In this way, one can access the whole tree to collect rich context information at t</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="6311" citStr="Joshi and Schabes, 1997" startWordPosition="1058" endWordPosition="1061"> more robust. Furthermore, we need to find an effective way to iterate between these two tasks. Instead of giving an algorithm specially designed for parsing, we generalize the problem for graphs. A sentence can be viewed as a graph in which words are viewed as vertices and neighboring words are connected with an arc. In Sections 2 and 3, we will propose decoding and training algorithms respectively for graph-based incremental construction, which can be applied to many structure learning problems in NLP. We will apply this algorithm to dependency parsing of Lexicalized Tree Adjoining Grammar (Joshi and Schabes, 1997). Specifically, we will train and evaluate an LTAG dependency parser over the LTAG treebank described in Shen et al. (2008). We report the experimental results on PTB section 23 of the LTAG treebank. The accuracy on LTAG dependency is 90.5%, which is 1.2 points over 89.3%, the previous best result (Shen and Joshi, 2005) on the same data set. It should be noted that PTB-based bracketed labelling is not an appropriate evaluation metric here, since the experiments are on an LTAG treebank. The derived trees in the LTAG treebank are different from the CFG trees in PTB. Hence, we do not use metrics </context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A. K. Joshi and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 3, pages 69 – 124. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Head-driven parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="28795" citStr="Kay, 1989" startWordPosition="5055" endWordPosition="5056">nd Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper is that selection of path and decisions about action are trained simultaneously with discriminative learning. In this way, we can employ context information more effectively. 7 Conclusion In this paper, we introduced bidirectional incremental parsing, a new architecture of parsing. We proposed a novel algorithm for graph-based incremental construction, and applied this algorithm to LTAG dependency parsing, revealing deep relations, which are unavailable in other approaches and difficult to learn. We evaluated the parser on an LTAG Treebank. Experimental re</context>
</contexts>
<marker>Kay, 1989</marker>
<rawString>M. Kay. 1989. Head-driven parsing. In Proceedings of Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Krauth</author>
<author>M M´ezard</author>
</authors>
<title>Learning algorithms with optimal stability in neural networks.</title>
<date>1987</date>
<journal>Journal of Physics A,</journal>
<pages>20--745</pages>
<marker>Krauth, M´ezard, 1987</marker>
<rawString>W. Krauth and M. M´ezard. 1987. Learning algorithms with optimal stability in neural networks. Journal of Physics A, 20:745–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmentation and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="27651" citStr="Lafferty et al., 2001" startWordPosition="4876" endWordPosition="4879">n of predicate coordination also provides deeper relations. For example, in Figure 6, the LTAG dependency contains resist → rearing and resist → saluting, while the Magerman’s dependency only contains resist → rearing. The explicit representation of predicate coordination will help to solve for the dependencies for shared arguments. 6 Discussion In our approach, each fragment in the graph is associated with a hidden structure, which means that we cannot reduce it to a labelling task. Therefore, the problem of interest to us is different from previous 502 work on graphical models, such as CRF (Lafferty et al., 2001) and MMMN (Taskar et al., 2003). McAllester et al. (2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem. However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement. Our approach provides a novel alternative to CFD. Our learning algorithm stems from Perceptron training in (Collins, 2002). Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi,</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmentation and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="16764" citStr="Marcus et al., 1994" startWordPosition="2986" endWordPosition="2989">ich is compatible with Hr and has the highest operation score in Q. Then we update the weight vector w with y� and y′. At the end, we update the candidate Q by using the new weights w. In order to improve the performance, we use Perceptron with margin in the training (Krauth and M´ezard, 1987). The margin is proportional to the loss of the hypothesis. Furthermore, we use averaged weights (Collins, 2002; Freund and Schapire, 1999) in Algorithm 1. 4 LTAG Dependency Parsing We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons. First, the LTAG architecture itself reveals deeper dependency. Furthermore, the PTB was reconciled with the Propbank in the LTAG Treebank extraction (Shen et al., 2008). We are espe</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
<author>M Collins</author>
<author>F Pereira</author>
</authors>
<title>Casefactor diagrams for structured probabilistic modeling.</title>
<date>2004</date>
<booktitle>In UAI</booktitle>
<contexts>
<context position="27708" citStr="McAllester et al. (2004)" startWordPosition="4886" endWordPosition="4889">ons. For example, in Figure 6, the LTAG dependency contains resist → rearing and resist → saluting, while the Magerman’s dependency only contains resist → rearing. The explicit representation of predicate coordination will help to solve for the dependencies for shared arguments. 6 Discussion In our approach, each fragment in the graph is associated with a hidden structure, which means that we cannot reduce it to a labelling task. Therefore, the problem of interest to us is different from previous 502 work on graphical models, such as CRF (Lafferty et al., 2001) and MMMN (Taskar et al., 2003). McAllester et al. (2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem. However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement. Our approach provides a novel alternative to CFD. Our learning algorithm stems from Perceptron training in (Collins, 2002). Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical just</context>
</contexts>
<marker>McAllester, Collins, Pereira, 2004</marker>
<rawString>D. McAllester, M. Collins, and F. Pereira. 2004. Casefactor diagrams for structured probabilistic modeling. In UAI 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofthe 43th Annual Meeting ofthe Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="16943" citStr="McDonald et al., 2005" startWordPosition="3015" endWordPosition="3018">ghts w. In order to improve the performance, we use Perceptron with margin in the training (Krauth and M´ezard, 1987). The margin is proportional to the loss of the hypothesis. Furthermore, we use averaged weights (Collins, 2002; Freund and Schapire, 1999) in Algorithm 1. 4 LTAG Dependency Parsing We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons. First, the LTAG architecture itself reveals deeper dependency. Furthermore, the PTB was reconciled with the Propbank in the LTAG Treebank extraction (Shen et al., 2008). We are especially interested in the two types of structures in the LTAG Treebank, predicate adjunction and predicate coordination. They are used to encode dependency relations which are unav</context>
<context position="26605" citStr="McDonald et al., 2005" startWordPosition="4706" endWordPosition="4710">parent of x. For any x = m1.1...1 or s1.1...1, (xr.p, x.p, x1.p, x2.p, .., xk.p) and template tp, [tp(x)] is used as a feature. (x.w, x.p, x1.p, x2.p, .., xk.p), (x.w, xr.p, x.p, x1.p, x2.p, .., xk.p) if x.w is a verb Table 1: Features defined on the context of operation model accuracy% Shen and Joshi, 2005 89.3 level-0 dependency 90.3 level-1 dependency 90.5 Table 2: Experiments on Sec. 23 of the LTAG Treebank dency is comparable to the numbers of the previous best systems on dependency extracted from PTB with Magerman’s rules, for example, 90.3% in (Yamada and Matsumoto, 2003) and 90.9% in (McDonald et al., 2005). However, their experiments are on the PTB, while ours is on the LTAG corpus. It should be noted that it is more difficult to learn LTAG dependencies. Theoretically, the LTAG dependencies reveal deeper relations. Adjunction can lead to non-projective dependencies, and the dependencies defined on predicate adjunction are linguistically more motivated, as shown in the examples in Figure 5 and 7. The explicit representation of predicate coordination also provides deeper relations. For example, in Figure 6, the LTAG dependency contains resist → rearing and resist → saluting, while the Magerman’s </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofthe 43th Annual Meeting ofthe Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="28290" citStr="Moore, 2005" startWordPosition="4975" endWordPosition="4976">003). McAllester et al. (2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem. However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement. Our approach provides a novel alternative to CFD. Our learning algorithm stems from Perceptron training in (Collins, 2002). Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper is that selection of path and decisions about action are </context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>R. Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="16807" citStr="Palmer et al., 2005" startWordPosition="2993" endWordPosition="2996">st operation score in Q. Then we update the weight vector w with y� and y′. At the end, we update the candidate Q by using the new weights w. In order to improve the performance, we use Perceptron with margin in the training (Krauth and M´ezard, 1987). The margin is proportional to the loss of the hypothesis. Furthermore, we use averaged weights (Collins, 2002; Freund and Schapire, 1999) in Algorithm 1. 4 LTAG Dependency Parsing We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons. First, the LTAG architecture itself reveals deeper dependency. Furthermore, the PTB was reconciled with the Propbank in the LTAG Treebank extraction (Shen et al., 2008). We are especially interested in the two types of struc</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Satta</author>
<author>O Stock</author>
</authors>
<title>Bi-Directional ContextFree Grammar Parsing for Natural Language Processing.</title>
<date>1994</date>
<journal>Artificial Intelligence,</journal>
<pages>69--1</pages>
<contexts>
<context position="28757" citStr="Satta and Stock, 1994" startWordPosition="5047" endWordPosition="5050"> NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper is that selection of path and decisions about action are trained simultaneously with discriminative learning. In this way, we can employ context information more effectively. 7 Conclusion In this paper, we introduced bidirectional incremental parsing, a new architecture of parsing. We proposed a novel algorithm for graph-based incremental construction, and applied this algorithm to LTAG dependency parsing, revealing deep relations, which are unavailable in other approaches and difficult to learn. We evaluated the parse</context>
</contexts>
<marker>Satta, Stock, 1994</marker>
<rawString>G. Satta and O. Stock. 1994. Bi-Directional ContextFree Grammar Parsing for Natural Language Processing. Artificial Intelligence, 69(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Joshi</author>
</authors>
<title>Incremental LTAG Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2968" citStr="Shen and Joshi, 2005" startWordPosition="478" endWordPosition="481"> fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). However, the need for tractability does not allow much internal information to be used to represent a hypothesis. The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing. Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem. In these incremental parsers, tree structures are used to represent the left context. In this way, one can access the whole tree to collect rich context information at the expense of being limited to beam search, which only maintains k-best results at each 495 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504, Honolulu, October 2008.c�2008 Association for Computational Linguistics step. Compared to chart parsing, incremental parsing searches for the analyses for only 2n − 1 cells, [1, 1], [2, 2], [1</context>
<context position="5186" citStr="Shen and Joshi, 2005" startWordPosition="868" endWordPosition="871"> [ai,1, ai,2] = [aj,1, ak,2], where j &lt; i, k &lt; i and aj,2 + 1 = ak,1. It is easy to show that the set {[ai,1, ai,2] |1 &lt; i &lt; 2n − 1} forms a tree relation, which means that each cell except the last one will be used to build another cell just once. In this framework, we can begin with several starting points in a sentence and search in any direction. So left-to-right parsing is only a special case of incremental parsing defined in this way. We still use complex structures to represent the partial analyses, so as to employ both top-down and bottom-up information as in (Collins and Roark, 2004; Shen and Joshi, 2005). Furthermore, we can utilize the rich context on both sides of the partial results. Similar to bidirectional labelling in (Shen et al., 2007), there are two learning tasking in this model. First, we need to learn which cell we should choose. At each step, we can select only one path. Secondly, we need to learn which operation we should take for a given cell. We maintain k-best candidates for each cell instead of only one, which differentiates this model from normal greedy search. So our model is more robust. Furthermore, we need to find an effective way to iterate between these two tasks. Ins</context>
<context position="6632" citStr="Shen and Joshi, 2005" startWordPosition="1113" endWordPosition="1116">ections 2 and 3, we will propose decoding and training algorithms respectively for graph-based incremental construction, which can be applied to many structure learning problems in NLP. We will apply this algorithm to dependency parsing of Lexicalized Tree Adjoining Grammar (Joshi and Schabes, 1997). Specifically, we will train and evaluate an LTAG dependency parser over the LTAG treebank described in Shen et al. (2008). We report the experimental results on PTB section 23 of the LTAG treebank. The accuracy on LTAG dependency is 90.5%, which is 1.2 points over 89.3%, the previous best result (Shen and Joshi, 2005) on the same data set. It should be noted that PTB-based bracketed labelling is not an appropriate evaluation metric here, since the experiments are on an LTAG treebank. The derived trees in the LTAG treebank are different from the CFG trees in PTB. Hence, we do not use metrics such as labeled precision and labeled recall for evaluation. 2 Graph-based Incremental Construction 2.1 Idea and Data Structures Now we define the problem formally. We will use dependency parsing as an example to illustrate the idea. We are given a connected graph G(V, E) whose hidden structure is U, where V = {vi}, E C</context>
<context position="23808" citStr="Shen and Joshi, 2005" startWordPosition="4199" endWordPosition="4202">of the children of the right half for s. Furthermore, we notice that all the rightmost descendants of s and the leftmost descendants of m at each level become unavailable for any further operation. So their children are fixed after this operation. All these nodes are in the form of m1.1...1 or s1.1...1. We use full check features to check the children from both sides for these nodes. In the discussion above, we ignored adjunction and conjunction. We need to slightly refine the conditions of checking. Due to the limit of space, we skip these cases. 5 Experiments We use the same data set as in (Shen and Joshi, 2005). We use Sec. 2-21 of the LTAG Treebank for training, Sec. 22 for feature selection, and Sec. 23 for test. Table 2 shows the comparison of different models. Beam size is set to five in our experiments. With level-0 dependency, our system achieves an accuracy of 90.3% at the speed of 4.25 sentences a second on a Xeon 3G Hz processor with JDK 1.5. With level-1 dependency, the parser achieves 90.5% at 3.59 sentences a second. Level-1 dependency does not provide much improvement due to the fact that level-0 features provide most of the useful information for this specific application. It is intere</context>
<context position="26291" citStr="Shen and Joshi, 2005" startWordPosition="4653" endWordPosition="4656">), are between s and m in the flat sentence. For (m.p, s.p, s1.p, s2.p, .., sk.p) each template tp, [tp] is used as a feature. and (s.w, s.p, s1.p, s2.p, .., sk.p), (s.w, m.p, s.p, s1.p, s2.p, .., sk.p) if s.w is a verb full check Let x1, x2, .., xk be the children of x, and xr (x.p, x1.p, x2.p, .., xk.p), the parent of x. For any x = m1.1...1 or s1.1...1, (xr.p, x.p, x1.p, x2.p, .., xk.p) and template tp, [tp(x)] is used as a feature. (x.w, x.p, x1.p, x2.p, .., xk.p), (x.w, xr.p, x.p, x1.p, x2.p, .., xk.p) if x.w is a verb Table 1: Features defined on the context of operation model accuracy% Shen and Joshi, 2005 89.3 level-0 dependency 90.3 level-1 dependency 90.5 Table 2: Experiments on Sec. 23 of the LTAG Treebank dency is comparable to the numbers of the previous best systems on dependency extracted from PTB with Magerman’s rules, for example, 90.3% in (Yamada and Matsumoto, 2003) and 90.9% in (McDonald et al., 2005). However, their experiments are on the PTB, while ours is on the LTAG corpus. It should be noted that it is more difficult to learn LTAG dependencies. Theoretically, the LTAG dependencies reveal deeper relations. Adjunction can lead to non-projective dependencies, and the dependencies</context>
<context position="28257" citStr="Shen and Joshi, 2005" startWordPosition="4968" endWordPosition="4971">y et al., 2001) and MMMN (Taskar et al., 2003). McAllester et al. (2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem. However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement. Our approach provides a novel alternative to CFD. Our learning algorithm stems from Perceptron training in (Collins, 2002). Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper is that selection of pat</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>L. Shen and A. K. Joshi. 2005. Incremental LTAG Parsing. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>G Satta</author>
<author>A K Joshi</author>
</authors>
<title>Guided Learning for Bidirectional Sequence Classification.</title>
<date>2007</date>
<booktitle>In Proceedings ofthe 45th Annual Meeting ofthe Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="959" citStr="Shen et al., 2007" startWordPosition="131" endWordPosition="134">, which can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set. 1 Introduction The phrase “Bidirectional Incremental” may appear self-contradictory at first sight, since incremental parsing usually means left-to-right parsing in the context of conventional parsing. In this paper, we will extend the meaning of incremental parsing. The idea of bidirectional parsing is related to the bidirectional sequential classification method described in (Shen et al., 2007). In that paper, a tagger assigns labels to words of highest confidence first, and then these labels in turn serve as the context of later labelling operations. The bidirectional tagger obtained the best results in literature on POS tagging on the standard PTB dataset. We extend this method from labelling to structure learning, The search space of structure learning is much larger, so that it is appropriate to exploit confidence scores in search. In this paper, we are interested in LTAG dependency parsing because TAG parsing is a well known problem of high computational complexity in regular p</context>
<context position="5328" citStr="Shen et al., 2007" startWordPosition="891" endWordPosition="894"> relation, which means that each cell except the last one will be used to build another cell just once. In this framework, we can begin with several starting points in a sentence and search in any direction. So left-to-right parsing is only a special case of incremental parsing defined in this way. We still use complex structures to represent the partial analyses, so as to employ both top-down and bottom-up information as in (Collins and Roark, 2004; Shen and Joshi, 2005). Furthermore, we can utilize the rich context on both sides of the partial results. Similar to bidirectional labelling in (Shen et al., 2007), there are two learning tasking in this model. First, we need to learn which cell we should choose. At each step, we can select only one path. Secondly, we need to learn which operation we should take for a given cell. We maintain k-best candidates for each cell instead of only one, which differentiates this model from normal greedy search. So our model is more robust. Furthermore, we need to find an effective way to iterate between these two tasks. Instead of giving an algorithm specially designed for parsing, we generalize the problem for graphs. A sentence can be viewed as a graph in which</context>
<context position="10466" citStr="Shen et al., 2007" startWordPosition="1816" endWordPosition="1819"> depends on the features in the fragment hypothesis of xj, and vice versa. We are especially interested in the following two dependency relations. • level-0 dependency: D0(xi, xj) i = j. • level-1 dependency: D1(xi, xj) xi and xj are directly connected in G. Level-0 dependency means that the features of a hypothesis for a vertex xi do not depend on the hypotheses for other vertices. Level-1 dependency means that the features depend on the hypotheses of nearby vertices only. The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described in (Shen et al., 2007). Level-1 dependency requires more data structures to maintain the hypotheses with dependency relations among them. However, we do not get into the details of level-1 formalism in this papers for two reasons. One is the limit of page space and depth of a conference paper. On the other hand, our experiments show that the parsing performance with level-1 dependency is close to what level-0 dependency could provides. Interested readers could refer to (Shen, 2006) for detailed description of the learning algorithms for level-1 dependency. 2.2 Algorithms Algorithm 1 shows the procedure of building </context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>L. Shen, G. Satta, and A. K. Joshi. 2007. Guided Learning for Bidirectional Sequence Classification. In Proceedings ofthe 45th Annual Meeting ofthe Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>L Champollion</author>
<author>A K Joshi</author>
</authors>
<title>LTAGspinal and the Treebank: a new resource for incremental, dependency and semantic parsing.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="6434" citStr="Shen et al. (2008)" startWordPosition="1078" endWordPosition="1081">ecially designed for parsing, we generalize the problem for graphs. A sentence can be viewed as a graph in which words are viewed as vertices and neighboring words are connected with an arc. In Sections 2 and 3, we will propose decoding and training algorithms respectively for graph-based incremental construction, which can be applied to many structure learning problems in NLP. We will apply this algorithm to dependency parsing of Lexicalized Tree Adjoining Grammar (Joshi and Schabes, 1997). Specifically, we will train and evaluate an LTAG dependency parser over the LTAG treebank described in Shen et al. (2008). We report the experimental results on PTB section 23 of the LTAG treebank. The accuracy on LTAG dependency is 90.5%, which is 1.2 points over 89.3%, the previous best result (Shen and Joshi, 2005) on the same data set. It should be noted that PTB-based bracketed labelling is not an appropriate evaluation metric here, since the experiments are on an LTAG treebank. The derived trees in the LTAG treebank are different from the CFG trees in PTB. Hence, we do not use metrics such as labeled precision and labeled recall for evaluation. 2 Graph-based Incremental Construction 2.1 Idea and Data Struc</context>
<context position="16713" citStr="Shen et al., 2008" startWordPosition="2978" endWordPosition="2981">sitive(Q, x′) returns the candidate hypothesis which is compatible with Hr and has the highest operation score in Q. Then we update the weight vector w with y� and y′. At the end, we update the candidate Q by using the new weights w. In order to improve the performance, we use Perceptron with margin in the training (Krauth and M´ezard, 1987). The margin is proportional to the loss of the hypothesis. Furthermore, we use averaged weights (Collins, 2002; Freund and Schapire, 1999) in Algorithm 1. 4 LTAG Dependency Parsing We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons. First, the LTAG architecture itself reveals deeper dependency. Furthermore, the PTB was reconciled with the Propbank in the LTAG T</context>
<context position="17936" citStr="Shen et al., 2008" startWordPosition="3175" endWordPosition="3178">nk extraction (Shen et al., 2008). We are especially interested in the two types of structures in the LTAG Treebank, predicate adjunction and predicate coordination. They are used to encode dependency relations which are unavailable in other approaches. On the other hand, these structures turn out to be a big problem for the general representation of dependency relations, including adjunction and coordination. We will show that the algorithm proposed here provides a nice solution for this problem. 499 Figure 5: Predicate Adjunction 4.1 Representation of the LTAG Treebank In the LTAG Treebank (Shen et al., 2008), each word is associated with a spinal template, which represents the projection from the lexical item to the root. Templates are linked together to form a derivation tree. The topology of the derivation tree shows a type of dependency relation, which we call LTAG dependency here. There are three types of operations in the LTAG Treebank, which are attachment, adjunction, and coordination. Attachment is used to represent both substitution and sister adjunction in the traditional LTAG. So it is similar to the dependency relation in other approaches. The LTAG dependency can be a non-projective r</context>
<context position="19606" citStr="Shen et al., 2008" startWordPosition="3446" endWordPosition="3449">omer groups. In the LTAG Treebank, the subtree for he says adjoins onto the node of has, which is the root of the derivation tree, as shown in Figure 5. Another special aspect of the LTAG Treebank is the representation of predicate coordination. Figure 6 is the representation of the following sentence. • I couldn’t resist rearing up on my soggy loafers and saluting. The coordination between rearing and saluting is represented explicitly with a coord-structure, and Figure 6: Predicate Coordination amid Figure 7: Non-projective Adjunction this coord-structure attaches to resist. It is shown in (Shen et al., 2008) that coord-structures could encode the ambiguity of argument sharing, which can be non-projective also. 4.2 Incremental Construction We build LTAG derivation trees incrementally. A hypothesis of a fragment is represented with a partial derivation tree. When the fragment hypotheses of two nearby fragments combine, the partial derivation trees are combined into one. It is trivial to combine two partial derivation trees with attachment. We simply attach the root of one tree to some node on the other tree which is visible to this root node. Adjunction is similar to attachment, except that an adjo</context>
</contexts>
<marker>Shen, Champollion, Joshi, 2008</marker>
<rawString>L. Shen, L. Champollion, and A. K. Joshi. 2008. LTAGspinal and the Treebank: a new resource for incremental, dependency and semantic parsing. Language Resources and Evaluation, 42(1):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
</authors>
<date>2006</date>
<tech>Statistical LTAG Parsing. Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10930" citStr="Shen, 2006" startWordPosition="1894" endWordPosition="1895">s only. The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described in (Shen et al., 2007). Level-1 dependency requires more data structures to maintain the hypotheses with dependency relations among them. However, we do not get into the details of level-1 formalism in this papers for two reasons. One is the limit of page space and depth of a conference paper. On the other hand, our experiments show that the parsing performance with level-1 dependency is close to what level-0 dependency could provides. Interested readers could refer to (Shen, 2006) for detailed description of the learning algorithms for level-1 dependency. 2.2 Algorithms Algorithm 1 shows the procedure of building hypotheses incrementally on a given graph G(V, E). Parameter k is used to set the beam width of search. Weight vector w is used to compute score of an operation. We have two sets, H and Q, to maintain hypotheses. Hypotheses in H are selected in beam search, and hypotheses in Q are candidate hypotheses for the next step of search in various directions. We first initiate the hypotheses for each vertex, and put them into set H. For example, in dependency parsing,</context>
</contexts>
<marker>Shen, 2006</marker>
<rawString>L. Shen. 2006. Statistical LTAG Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin markov networks.</title>
<date>2003</date>
<booktitle>In Proceedings of the 17th Annual Conference Neural Information Processing Systems.</booktitle>
<contexts>
<context position="27682" citStr="Taskar et al., 2003" startWordPosition="4882" endWordPosition="4885">provides deeper relations. For example, in Figure 6, the LTAG dependency contains resist → rearing and resist → saluting, while the Magerman’s dependency only contains resist → rearing. The explicit representation of predicate coordination will help to solve for the dependencies for shared arguments. 6 Discussion In our approach, each fragment in the graph is associated with a hidden structure, which means that we cannot reduce it to a labelling task. Therefore, the problem of interest to us is different from previous 502 work on graphical models, such as CRF (Lafferty et al., 2001) and MMMN (Taskar et al., 2003). McAllester et al. (2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem. However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement. Our approach provides a novel alternative to CFD. Our learning algorithm stems from Perceptron training in (Collins, 2002). Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moor</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin markov networks. In Proceedings of the 17th Annual Conference Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In IWPT</booktitle>
<contexts>
<context position="16919" citStr="Yamada and Matsumoto, 2003" startWordPosition="3010" endWordPosition="3014">idate Q by using the new weights w. In order to improve the performance, we use Perceptron with margin in the training (Krauth and M´ezard, 1987). The margin is proportional to the loss of the hypothesis. Furthermore, we use averaged weights (Collins, 2002; Freund and Schapire, 1999) in Algorithm 1. 4 LTAG Dependency Parsing We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons. First, the LTAG architecture itself reveals deeper dependency. Furthermore, the PTB was reconciled with the Propbank in the LTAG Treebank extraction (Shen et al., 2008). We are especially interested in the two types of structures in the LTAG Treebank, predicate adjunction and predicate coordination. They are used to encode dependency </context>
<context position="26568" citStr="Yamada and Matsumoto, 2003" startWordPosition="4698" endWordPosition="4702">, and xr (x.p, x1.p, x2.p, .., xk.p), the parent of x. For any x = m1.1...1 or s1.1...1, (xr.p, x.p, x1.p, x2.p, .., xk.p) and template tp, [tp(x)] is used as a feature. (x.w, x.p, x1.p, x2.p, .., xk.p), (x.w, xr.p, x.p, x1.p, x2.p, .., xk.p) if x.w is a verb Table 1: Features defined on the context of operation model accuracy% Shen and Joshi, 2005 89.3 level-0 dependency 90.3 level-1 dependency 90.5 Table 2: Experiments on Sec. 23 of the LTAG Treebank dency is comparable to the numbers of the previous best systems on dependency extracted from PTB with Magerman’s rules, for example, 90.3% in (Yamada and Matsumoto, 2003) and 90.9% in (McDonald et al., 2005). However, their experiments are on the PTB, while ours is on the LTAG corpus. It should be noted that it is more difficult to learn LTAG dependencies. Theoretically, the LTAG dependencies reveal deeper relations. Adjunction can lead to non-projective dependencies, and the dependencies defined on predicate adjunction are linguistically more motivated, as shown in the examples in Figure 5 and 7. The explicit representation of predicate coordination also provides deeper relations. For example, in Figure 6, the LTAG dependency contains resist → rearing and res</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with Support Vector Machines. In IWPT 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>