<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014350">
<title confidence="0.929347">
Simple English Wikipedia: A New Text Simplification Task
</title>
<author confidence="0.996795">
William Coster
</author>
<affiliation confidence="0.983417">
Computer Science Department
Pomona College
</affiliation>
<address confidence="0.675933">
Claremont, CA 91711
</address>
<email confidence="0.997408">
wpc02009@pomona.edu
</email>
<author confidence="0.997391">
David Kauchak
</author>
<affiliation confidence="0.983664">
Computer Science Department
Pomona College
</affiliation>
<address confidence="0.676396">
Claremont, CA 91711
</address>
<email confidence="0.998755">
dkauchak@cs.pomona.edu
</email>
<sectionHeader confidence="0.99585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995064071428571">
In this paper we examine the task of sentence
simplification which aims to reduce the read-
ing complexity of a sentence by incorporat-
ing more accessible vocabulary and sentence
structure. We introduce a new data set that
pairs English Wikipedia with Simple English
Wikipedia and is orders of magnitude larger
than any previously examined for sentence
simplification. The data contains the full range
of simplification operations including reword-
ing, reordering, insertion and deletion. We
provide an analysis of this corpus as well as
preliminary results using a phrase-based trans-
lation approach for simplification.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98969818">
The task of text simplification aims to reduce the
complexity of text while maintaining the content
(Chandrasekar and Srinivas, 1997; Carroll et al.,
1998; Feng, 2008). In this paper, we explore the
sentence simplification problem: given a sentence,
the goal is to produce an equivalent sentence where
the vocabulary and sentence structure are simpler.
Text simplification has a number of important ap-
plications. Simplification techniques can be used to
make text resources available to a broader range of
readers, including children, language learners, the
elderly, the hearing impaired and people with apha-
sia or cognitive disabilities (Carroll et al., 1998;
Feng, 2008). As a preprocessing step, simplification
can improve the performance of NLP tasks, includ-
ing parsing, semantic role labeling, machine transla-
tion and summarization (Miwa et al., 2010; Jonnala-
665
gadda et al., 2009; Vickrey and Koller, 2008; Chan-
drasekar and Srinivas, 1997). Finally, models for
text simplification are similar to models for sentence
compression; advances in simplification can bene-
fit compression, which has applications in mobile
devices, summarization and captioning (Knight and
Marcu, 2002; McDonald, 2006; Galley and McKe-
own, 2007; Nomoto, 2009; Cohn and Lapata, 2009).
One of the key challenges for text simplification
is data availability. The small amount of simplifi-
cation data currently available has prevented the ap-
plication of data-driven techniques like those used
in other text-to-text translation areas (Och and Ney,
2004; Chiang, 2010). Most prior techniques for
text simplification have involved either hand-crafted
rules (Vickrey and Koller, 2008; Feng, 2008) or
learned within a very restricted rule space (Chan-
drasekar and Srinivas, 1997).
We have generated a data set consisting of 137K
aligned simplified/unsimplified sentence pairs by
pairing documents, then sentences from English
Wikipedia1 with corresponding documents and sen-
tences from Simple English Wikipedia2. Simple En-
glish Wikipedia contains articles aimed at children
and English language learners and contains similar
content to English Wikipedia but with simpler vo-
cabulary and grammar.
Figure 1 shows example sentence simplifications
from the data set. Like machine translation and other
text-to-text domains, text simplification involves the
full range of transformation operations including
deletion, rewording, reordering and insertion.
</bodyText>
<footnote confidence="0.999991">
1http://en.wikipedia.org/
2http://simple.wikipedia.org
</footnote>
<note confidence="0.885516">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 665–669,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.448716846153846">
Normal: As Isolde arrives at his side, Tristan dies with her name on his lips.
Simple: As Isolde arrives at his side, Tristan dies while speaking her name.
Normal: Alfonso Perez Munoz, usually referred to as Alfonso, is a
Simple: former Spanish footballer, in the striker position.
Alfonso Perez is a former Spanish football player.
Normal: Endemic types or species are especially likely to develop on islands
Simple: because of their geographical isolation.
Endemic types are most likely to develop on islands because
they are isolated.
Normal: The reverse process, producing electrical energy from mechanical,
Simple: energy, is accomplished by a generator or dynamo.
A dynamo or an electric generator does the reverse: it changes
mechanical movement into electric energy.
</table>
<figureCaption confidence="0.948883">
Figure 1: Example sentence simplifications extracted from Wikipedia. Normal refers to a sentence in an English
Wikipedia article and Simple to a corresponding sentence in Simple English Wikipedia.
</figureCaption>
<sectionHeader confidence="0.941261" genericHeader="method">
2 Previous Data
</sectionHeader>
<bodyText confidence="0.998868">
Wikipedia and Simple English Wikipedia have both
received some recent attention as a useful resource
for text simplification and the related task of text
compression. Yamangil and Nelken (2008) examine
the history logs of English Wikipedia to learn sen-
tence compression rules. Yatskar et al. (2010) learn
a set of candidate phrase simplification rules based
on edits identified in the revision histories of both
Simple English Wikipedia and English Wikipedia.
However, they only provide a list of the top phrasal
simplifications and do not utilize them in an end-
to-end simplification system. Finally, Napoles and
Dredze (2010) provide an analysis of the differences
between documents in English Wikipedia and Sim-
ple English Wikipedia, though they do not view the
data set as a parallel corpus.
Although the simplification problem shares some
characteristics with the text compression problem,
existing text compression data sets are small and
contain a restricted set of possible transformations
(often only deletion). Knight and Marcu (2002) in-
troduced the Zipf-Davis corpus which contains 1K
sentence pairs. Cohn and Lapata (2009) manually
generated two parallel corpora from news stories to-
taling 3K sentence pairs. Finally, Nomoto (2009)
generated a data set based on RSS feeds containing
2K sentence pairs.
</bodyText>
<sectionHeader confidence="0.992365" genericHeader="method">
3 Simplification Corpus Generation
</sectionHeader>
<bodyText confidence="0.999905517241379">
We generated a parallel simplification corpus by
aligning sentences between English Wikipedia and
Simple English Wikipedia. We obtained complete
copies of English Wikipedia and Simple English
Wikipedia in May 2010. We first paired the articles
by title, then removed all article pairs where either
article: contained only a single line, was flagged as a
stub, was flagged as a disambiguation page or was a
meta-page about Wikipedia. After pairing and filter-
ing, 10,588 aligned, content article pairs remained
(a 90% reduction from the original 110K Simple En-
glish Wikipedia articles). Throughout the rest of this
paper we will refer to unsimplified text from English
Wikipedia as normal and to the simplified version
from Simple English Wikipedia as simple.
To generate aligned sentence pairs from the
aligned document pairs we followed an approach
similar to those utilized in previous monolingual
alignment problems (Barzilay and Elhadad, 2003;
Nelken and Shieber, 2006). Paragraphs were iden-
tified based on formatting information available in
the articles. Each simple paragraph was then aligned
to every normal paragraph where the TF-IDF, co-
sine similarity was over a threshold or 0.5. We ini-
tially investigated the paragraph clustering prepro-
cessing step in (Barzilay and Elhadad, 2003), but
did not find a qualitative difference and opted for the
simpler similarity-based alignment approach, which
does not require manual annotation.
</bodyText>
<page confidence="0.993935">
666
</page>
<bodyText confidence="0.999279285714286">
For each aligned paragraph pair (i.e. a simple
paragraph and one or more normal paragraphs), we
then used a dynamic programming approach to find
that best global sentence alignment following Barzi-
lay and Elhadad (2003). Specifically, given n nor-
mal sentences to align to m simple sentences, we
find a(n, m) using the following recurrence:
</bodyText>
<equation confidence="0.929886571428572">
a(i, j) _
a(i, j − 1) − skip penalty
a(i − 1, j) − skip penalty
a(i − 1, j − 1) + sim(i, j)
a(i − 1, j − 2) + sim(i, j) + sim(i, j − 1)
a(i − 2, j − 1) + sim(i, j) + sim(i − 1, j)
a(i − 2, j − 2) + sim(i, j − 1) + sim(i − 1, j)
</equation>
<bodyText confidence="0.999976454545455">
where each line above corresponds to a sentence
alignment operation: skip the simple sentence, skip
the normal sentence, align one normal to one sim-
ple, align one normal to two simple, align two nor-
mal to one simple and align two normal to two sim-
ple. sim(i, j) is the similarity between the ith nor-
mal sentence and the jth simple sentence and was
calculated using TF-IDF, cosine similarity. We set
skip penalty = 0.0001 manually.
Barzilay and Elhadad (2003) further discourage
aligning dissimilar sentences by including a “mis-
match penalty” in the similarity measure. Instead,
we included a filtering step removing all sentence
pairs with a normalized similarity below a threshold
of 0.5. We found this approach to be more intuitive
and allowed us to compare the effects of differing
levels of similarity in the training set. Our choice of
threshold is high enough to ensure that most align-
ments are correct, but low enough to allow for vari-
ation in the paired sentences. In the future, we hope
to explore other similarity techniques that will pair
sentences with even larger variation.
</bodyText>
<sectionHeader confidence="0.989166" genericHeader="method">
4 Corpus Analysis
</sectionHeader>
<bodyText confidence="0.998671434782609">
From the 10K article pairs, we extracted 75K
aligned paragraphs. From these, we extracted the
final set of 137K aligned sentence pairs. To evaluate
the quality of the aligned sentences, we asked two
human evaluators to independently judge whether or
not the aligned sentences were correctly aligned on
a random sample of 100 sentence pairs. They then
were asked to reach a consensus about correctness.
91/100 were identified as correct, though many of
the remaining 9 also had some partial content over-
lap. We also repeated the experiment using only
those sentences with a similarity above 0.75 (rather
than 0.50 in the original data). This reduced the
number of pairs from 137K to 90K, but the eval-
uators identified 98/100 as correct. The analysis
throughout the rest of the section is for threshold
of 0.5, though similar results were also seen for the
threshold of 0.75.
Although the average simple article contained ap-
proximately 40 sentences, we extracted an average
of 14 aligned sentence pairs per article. Qualita-
tively, it is rare to find a simple article that is a direct
translation of the normal article, that is, a simple ar-
ticle that was generated by only making sentence-
level changes to the normal document. However,
there is a strong relationship between the two data
sets: 27% of our aligned sentences were identical
between simple and normal. We left these identical
sentence pairs in our data set since not all sentences
need to be simplified and it is important for any sim-
plification algorithm to be able to handle this case.
Much of the content without direct correspon-
dence is removed during paragraph alignment. 65%
of the simple paragraphs do not align to a normal
paragraphs and are ignored. On top of this, within
aligned paragraphs, there are a large number of sen-
tences that do not align. Table 1 shows the propor-
tion of the different sentence level alignment opera-
tions in our data set. On both the simple and normal
sides there are many sentences that do not align.
Operation %
skip simple 27%
skip normal 23%
one normal to one simple 37%
one normal to two simple 8%
two normal to one simple 5%
</bodyText>
<tableCaption confidence="0.985924">
Table 1: Frequency of sentence-level alignment opera-
tions based on our learned sentence alignment. No 2-to-2
alignments were found in the data.
</tableCaption>
<bodyText confidence="0.9985154">
To better understand how sentences are trans-
formed from normal to simple sentences we learned
a word alignment using GIZA++ (Och and Ney,
2003). Based on this word alignment, we calcu-
lated the percentage of sentences that included: re-
</bodyText>
<figure confidence="0.733998">
I
max
</figure>
<page confidence="0.983454">
667
</page>
<bodyText confidence="0.9995945">
wordings – a normal word is changed to a different
simple word, deletions – a normal word is deleted,
reorderings – non-monotonic alignment, splits – a
normal words is split into multiple simple words,
and merges – multiple normal words are condensed
to a single simple word.
</bodyText>
<table confidence="0.9920005">
Transformation %
rewordings 65%
deletions 47%
reorders 34%
merges 31%
splits 27%
</table>
<tableCaption confidence="0.9413208">
Table 2: Percentage of sentence pairs that contained
word-level operations based on the induced word align-
ment. Splits and merges are from the perspective of
words in the normal sentence. These are not mutually
exclusive events.
</tableCaption>
<bodyText confidence="0.9960975">
Table 2 shows the percentage of each of these phe-
nomena occurring in the sentence pairs. All of the
different operations occur frequently in the data set
with rewordings being particularly prevalent.
</bodyText>
<sectionHeader confidence="0.951839" genericHeader="method">
5 Sentence-level Text Simplification
</sectionHeader>
<bodyText confidence="0.999896095238095">
To understand the usefulness of this data we ran
preliminary experiments to learn a sentence-level
simplification system. We view the problem of
text simplification as an English-to-English transla-
tion problem. Motivated by the importance of lex-
ical changes, we used Moses, a phrase-based ma-
chine translation system (Och and Ney, 2004).3 We
trained Moses on 124K pairs from the data set and
the n-gram language model on the simple side of this
data. We trained the hyper-parameters of the log-
linear model on a 500 sentence pair development set.
We compared the trained system to a baseline of
not doing any simplification (NONE). We evaluated
the two approaches on a test set of 1300 sentence
pairs. Since there is currently no standard for au-
tomatically evaluating sentence simplification, we
used three different automatic measures that have
been used in related domains: BLEU, which has
been used extensively in machine translation (Pap-
ineni et al., 2002), and word-level F1 and simple
string accuracy (SSA) which have been suggested
</bodyText>
<footnote confidence="0.891597">
3We also experimented with T3 (Cohn and Lapata, 2009)
but the results were poor and are not presented here.
</footnote>
<table confidence="0.9997425">
System BLEU word-F1 SSA
NONE 0.5937 0.5967 0.6179
Moses 0.5987 0.6076 0.6224
Moses-Oracle 0.6317 0.6661 0.6550
</table>
<tableCaption confidence="0.9816885">
Table 3: Test scores for the baseline (NONE), Moses and
Moses-Oracle.
</tableCaption>
<bodyText confidence="0.999912476190476">
for text compression (Clarke and Lapata, 2006). All
three of these measures have been shown to correlate
with human judgements in their respective domains.
Table 3 shows the results of our initial test. All
differences are statistically significant at p = 0.01,
measured using bootstrap resampling with 100 sam-
ples (Koehn, 2004). Although the baseline does well
(recall that over a quarter of the sentence pairs in
the data set are identical) the phrase-based approach
does obtain a statistically significant improvement.
To understand the the limits of the phrase-based
model for text simplification, we generated an n-
best list of the 1000 most-likely simplifications for
each test sentence. We then greedily picked the sim-
plification from this n-best list that had the highest
sentence-level BLEU score based on the test exam-
ples, labeled Moses-Oracle in Table 3. The large
difference between Moses and Moses-Oracle indi-
cates possible room for improvement utilizing better
parameter estimation or n-best list reranking tech-
niques (Och et al., 2004; Ge and Mooney, 2006).
</bodyText>
<sectionHeader confidence="0.996156" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999914666666667">
We have described a new text simplification data set
generated from aligning sentences in Simple English
Wikipedia with sentences in English Wikipedia. The
data set is orders of magnitude larger than any cur-
rently available for text simplification or for the re-
lated field of text compression and is publicly avail-
able.4 We provided preliminary text simplification
results using Moses, a phrase-based translation sys-
tem, and saw a statistically significant improvement
of 0.005 BLEU over the baseline of no simplifica-
tion and showed that further improvement of up to
0.034 BLEU may be possible based on the oracle
results. In the future, we hope to explore alignment
techniques more tailored to simplification as well as
applications of this data to text simplification.
</bodyText>
<footnote confidence="0.9779">
4http://www.cs.pomona.edu/—dkauchak/simplification/
</footnote>
<page confidence="0.99368">
668
</page>
<sectionHeader confidence="0.989718" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776897959184">
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
John Carroll, Gido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of English newspaper text to assist aphasic readers. In
Proceedings of AAAI Workshop on Integrating AI and
Assistive Technology.
Raman Chandrasekar and Bangalore Srinivas. 1997. Au-
tomatic induction of rules for text simplification. In
Knowledge Based Systems.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of ACL.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research.
Lijun Feng. 2008. Text simplification: A survey. CUNY
Technical Report.
Michel Galley and Kathleen McKeown. 2007. Lexical-
ized Markov grammars for sentence compression. In
Proceedings of HLT/NAACL.
Ruifang Ge and Raymond Mooney. 2006. Discrimina-
tive reranking for semantic parsing. In Proceedings of
COLING.
Siddhartha Jonnalagadda, Luis Tari, Jorg Hakenberg,
Chitta Baral, and Graciela Gonzalez. 2009. To-
wards effective sentence simplification for automatic
processing of biomedical text. In Proceedings of
HLT/NAACL.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Kevin Knight and Daniel Marcu. 2002. Summarization
beyond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun’ichi
Tsujii. 2010. Entity-focused sentence simplication for
relation extraction. In Proceedings of COLING.
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple Wikipedia: A cogitation in ascertaining
abecedarian language. In Proceedings of HLT/NAACL
Workshop on Computation Linguistics and Writing.
Rani Nelken and Stuart Shieber. 2006. Towards robust
context-sensitive sentence alignment for monolingual
corpora. In Proceedings of AMTA.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. In Informa-
tion Processing and Management.
Tadashi Nomoto. 2008. A generic sentence trimmer with
CRFs. In Proceedings of HLT/NAACL.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
putational Linguistics.
Franz Josef Och, Kenji Yamada, Stanford U, Alex Fraser,
Daniel Gildea, and Viren Jain. 2004. A smorgasbord
of features for statistical machine translation. In Pro-
ceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL.
Emily Pitler. 2010. Methods for sentence compression.
Technical Report MS-CIS-10-20, University of Penn-
sylvania.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
David Vickrey and Daphne Koller. 2008. Sentence sim-
plification for semantic role labeling. In Proceedings
of ACL.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia revision histories for improving sentence
compression. In ACL.
Mark Yatskar, Bo Pang, Critian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In HLT/NAACL Short Papers.
</reference>
<page confidence="0.998525">
669
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.276327">
<title confidence="0.99589">Simple English Wikipedia: A New Text Simplification Task</title>
<author confidence="0.981966">William</author>
<affiliation confidence="0.8282975">Computer Science Pomona</affiliation>
<address confidence="0.915796">Claremont, CA</address>
<email confidence="0.997083">wpc02009@pomona.edu</email>
<author confidence="0.832886">David</author>
<affiliation confidence="0.8194375">Computer Science Pomona</affiliation>
<address confidence="0.984598">Claremont, CA</address>
<email confidence="0.999325">dkauchak@cs.pomona.edu</email>
<abstract confidence="0.995526733333333">In this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based translation approach for simplification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6848" citStr="Barzilay and Elhadad, 2003" startWordPosition="1000" endWordPosition="1003">ly a single line, was flagged as a stub, was flagged as a disambiguation page or was a meta-page about Wikipedia. After pairing and filtering, 10,588 aligned, content article pairs remained (a 90% reduction from the original 110K Simple English Wikipedia articles). Throughout the rest of this paper we will refer to unsimplified text from English Wikipedia as normal and to the simplified version from Simple English Wikipedia as simple. To generate aligned sentence pairs from the aligned document pairs we followed an approach similar to those utilized in previous monolingual alignment problems (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006). Paragraphs were identified based on formatting information available in the articles. Each simple paragraph was then aligned to every normal paragraph where the TF-IDF, cosine similarity was over a threshold or 0.5. We initially investigated the paragraph clustering preprocessing step in (Barzilay and Elhadad, 2003), but did not find a qualitative difference and opted for the simpler similarity-based alignment approach, which does not require manual annotation. 666 For each aligned paragraph pair (i.e. a simple paragraph and one or more normal paragraphs), we then </context>
<context position="8372" citStr="Barzilay and Elhadad (2003)" startWordPosition="1280" endWordPosition="1283">alty a(i − 1, j − 1) + sim(i, j) a(i − 1, j − 2) + sim(i, j) + sim(i, j − 1) a(i − 2, j − 1) + sim(i, j) + sim(i − 1, j) a(i − 2, j − 2) + sim(i, j − 1) + sim(i − 1, j) where each line above corresponds to a sentence alignment operation: skip the simple sentence, skip the normal sentence, align one normal to one simple, align one normal to two simple, align two normal to one simple and align two normal to two simple. sim(i, j) is the similarity between the ith normal sentence and the jth simple sentence and was calculated using TF-IDF, cosine similarity. We set skip penalty = 0.0001 manually. Barzilay and Elhadad (2003) further discourage aligning dissimilar sentences by including a “mismatch penalty” in the similarity measure. Instead, we included a filtering step removing all sentence pairs with a normalized similarity below a threshold of 0.5. We found this approach to be more intuitive and allowed us to compare the effects of differing levels of similarity in the training set. Our choice of threshold is high enough to ensure that most alignments are correct, but low enough to allow for variation in the paired sentences. In the future, we hope to explore other similarity techniques that will pair sentence</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Gido Minnen</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Practical simplification of English newspaper text to assist aphasic readers.</title>
<date>1998</date>
<booktitle>In Proceedings of AAAI Workshop on Integrating AI and Assistive Technology.</booktitle>
<contexts>
<context position="1049" citStr="Carroll et al., 1998" startWordPosition="146" endWordPosition="149">d sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based translation approach for simplification. 1 Introduction The task of text simplification aims to reduce the complexity of text while maintaining the content (Chandrasekar and Srinivas, 1997; Carroll et al., 1998; Feng, 2008). In this paper, we explore the sentence simplification problem: given a sentence, the goal is to produce an equivalent sentence where the vocabulary and sentence structure are simpler. Text simplification has a number of important applications. Simplification techniques can be used to make text resources available to a broader range of readers, including children, language learners, the elderly, the hearing impaired and people with aphasia or cognitive disabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks</context>
</contexts>
<marker>Carroll, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>John Carroll, Gido Minnen, Yvonne Canning, Siobhan Devlin, and John Tait. 1998. Practical simplification of English newspaper text to assist aphasic readers. In Proceedings of AAAI Workshop on Integrating AI and Assistive Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raman Chandrasekar</author>
<author>Bangalore Srinivas</author>
</authors>
<title>Automatic induction of rules for text simplification. In Knowledge Based Systems.</title>
<date>1997</date>
<contexts>
<context position="1027" citStr="Chandrasekar and Srinivas, 1997" startWordPosition="142" endWordPosition="145">ing more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based translation approach for simplification. 1 Introduction The task of text simplification aims to reduce the complexity of text while maintaining the content (Chandrasekar and Srinivas, 1997; Carroll et al., 1998; Feng, 2008). In this paper, we explore the sentence simplification problem: given a sentence, the goal is to produce an equivalent sentence where the vocabulary and sentence structure are simpler. Text simplification has a number of important applications. Simplification techniques can be used to make text resources available to a broader range of readers, including children, language learners, the elderly, the hearing impaired and people with aphasia or cognitive disabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the pe</context>
<context position="2643" citStr="Chandrasekar and Srinivas, 1997" startWordPosition="382" endWordPosition="386">bile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set consisting of 137K aligned simplified/unsimplified sentence pairs by pairing documents, then sentences from English Wikipedia1 with corresponding documents and sentences from Simple English Wikipedia2. Simple English Wikipedia contains articles aimed at children and English language learners and contains similar content to English Wikipedia but with simpler vocabulary and grammar. Figure 1 shows example sentence simplifications from the data set. Like machine translation and other text-to-text domains, text simplification involves the full range of transformation</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>Raman Chandrasekar and Bangalore Srinivas. 1997. Automatic induction of rules for text simplification. In Knowledge Based Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2436" citStr="Chiang, 2010" startWordPosition="355" endWordPosition="356">nd Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set consisting of 137K aligned simplified/unsimplified sentence pairs by pairing documents, then sentences from English Wikipedia1 with corresponding documents and sentences from Simple English Wikipedia2. Simple English Wikipedia contains articles aimed at children and English language learners and contains similar content to English Wikipedia but with simpler vo</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13729" citStr="Clarke and Lapata, 2006" startWordPosition="2168" endWordPosition="2171">ically evaluating sentence simplification, we used three different automatic measures that have been used in related domains: BLEU, which has been used extensively in machine translation (Papineni et al., 2002), and word-level F1 and simple string accuracy (SSA) which have been suggested 3We also experimented with T3 (Cohn and Lapata, 2009) but the results were poor and are not presented here. System BLEU word-F1 SSA NONE 0.5937 0.5967 0.6179 Moses 0.5987 0.6076 0.6224 Moses-Oracle 0.6317 0.6661 0.6550 Table 3: Test scores for the baseline (NONE), Moses and Moses-Oracle. for text compression (Clarke and Lapata, 2006). All three of these measures have been shown to correlate with human judgements in their respective domains. Table 3 shows the results of our initial test. All differences are statistically significant at p = 0.01, measured using bootstrap resampling with 100 samples (Koehn, 2004). Although the baseline does well (recall that over a quarter of the sentence pairs in the data set are identical) the phrase-based approach does obtain a statistically significant improvement. To understand the the limits of the phrase-based model for text simplification, we generated an nbest list of the 1000 most-</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="2157" citStr="Cohn and Lapata, 2009" startWordPosition="311" endWordPosition="314">arroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks, including parsing, semantic role labeling, machine translation and summarization (Miwa et al., 2010; Jonnala665 gadda et al., 2009; Vickrey and Koller, 2008; Chandrasekar and Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set consisting of 137K aligned simplified/unsimplified sentence pairs by pairing docume</context>
<context position="5688" citStr="Cohn and Lapata (2009)" startWordPosition="823" endWordPosition="826">al simplifications and do not utilize them in an endto-end simplification system. Finally, Napoles and Dredze (2010) provide an analysis of the differences between documents in English Wikipedia and Simple English Wikipedia, though they do not view the data set as a parallel corpus. Although the simplification problem shares some characteristics with the text compression problem, existing text compression data sets are small and contain a restricted set of possible transformations (often only deletion). Knight and Marcu (2002) introduced the Zipf-Davis corpus which contains 1K sentence pairs. Cohn and Lapata (2009) manually generated two parallel corpora from news stories totaling 3K sentence pairs. Finally, Nomoto (2009) generated a data set based on RSS feeds containing 2K sentence pairs. 3 Simplification Corpus Generation We generated a parallel simplification corpus by aligning sentences between English Wikipedia and Simple English Wikipedia. We obtained complete copies of English Wikipedia and Simple English Wikipedia in May 2010. We first paired the articles by title, then removed all article pairs where either article: contained only a single line, was flagged as a stub, was flagged as a disambig</context>
<context position="13447" citStr="Cohn and Lapata, 2009" startWordPosition="2124" endWordPosition="2127">er-parameters of the loglinear model on a 500 sentence pair development set. We compared the trained system to a baseline of not doing any simplification (NONE). We evaluated the two approaches on a test set of 1300 sentence pairs. Since there is currently no standard for automatically evaluating sentence simplification, we used three different automatic measures that have been used in related domains: BLEU, which has been used extensively in machine translation (Papineni et al., 2002), and word-level F1 and simple string accuracy (SSA) which have been suggested 3We also experimented with T3 (Cohn and Lapata, 2009) but the results were poor and are not presented here. System BLEU word-F1 SSA NONE 0.5937 0.5967 0.6179 Moses 0.5987 0.6076 0.6224 Moses-Oracle 0.6317 0.6661 0.6550 Table 3: Test scores for the baseline (NONE), Moses and Moses-Oracle. for text compression (Clarke and Lapata, 2006). All three of these measures have been shown to correlate with human judgements in their respective domains. Table 3 shows the results of our initial test. All differences are statistically significant at p = 0.01, measured using bootstrap resampling with 100 samples (Koehn, 2004). Although the baseline does well (r</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijun Feng</author>
</authors>
<title>Text simplification: A survey. CUNY</title>
<date>2008</date>
<tech>Technical Report.</tech>
<contexts>
<context position="1062" citStr="Feng, 2008" startWordPosition="150" endWordPosition="151">We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based translation approach for simplification. 1 Introduction The task of text simplification aims to reduce the complexity of text while maintaining the content (Chandrasekar and Srinivas, 1997; Carroll et al., 1998; Feng, 2008). In this paper, we explore the sentence simplification problem: given a sentence, the goal is to produce an equivalent sentence where the vocabulary and sentence structure are simpler. Text simplification has a number of important applications. Simplification techniques can be used to make text resources available to a broader range of readers, including children, language learners, the elderly, the hearing impaired and people with aphasia or cognitive disabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks, including p</context>
<context position="2562" citStr="Feng, 2008" startWordPosition="372" endWordPosition="373">cation can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set consisting of 137K aligned simplified/unsimplified sentence pairs by pairing documents, then sentences from English Wikipedia1 with corresponding documents and sentences from Simple English Wikipedia2. Simple English Wikipedia contains articles aimed at children and English language learners and contains similar content to English Wikipedia but with simpler vocabulary and grammar. Figure 1 shows example sentence simplifications from the data set. Like machine translation and other te</context>
</contexts>
<marker>Feng, 2008</marker>
<rawString>Lijun Feng. 2008. Text simplification: A survey. CUNY Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="2119" citStr="Galley and McKeown, 2007" startWordPosition="304" endWordPosition="308">ith aphasia or cognitive disabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks, including parsing, semantic role labeling, machine translation and summarization (Miwa et al., 2010; Jonnala665 gadda et al., 2009; Vickrey and Koller, 2008; Chandrasekar and Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set consisting of 137K aligned simplified/unsimpl</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond Mooney</author>
</authors>
<title>Discriminative reranking for semantic parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="14754" citStr="Ge and Mooney, 2006" startWordPosition="2329" endWordPosition="2332">se-based approach does obtain a statistically significant improvement. To understand the the limits of the phrase-based model for text simplification, we generated an nbest list of the 1000 most-likely simplifications for each test sentence. We then greedily picked the simplification from this n-best list that had the highest sentence-level BLEU score based on the test examples, labeled Moses-Oracle in Table 3. The large difference between Moses and Moses-Oracle indicates possible room for improvement utilizing better parameter estimation or n-best list reranking techniques (Och et al., 2004; Ge and Mooney, 2006). 6 Conclusion We have described a new text simplification data set generated from aligning sentences in Simple English Wikipedia with sentences in English Wikipedia. The data set is orders of magnitude larger than any currently available for text simplification or for the related field of text compression and is publicly available.4 We provided preliminary text simplification results using Moses, a phrase-based translation system, and saw a statistically significant improvement of 0.005 BLEU over the baseline of no simplification and showed that further improvement of up to 0.034 BLEU may be </context>
</contexts>
<marker>Ge, Mooney, 2006</marker>
<rawString>Ruifang Ge and Raymond Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddhartha Jonnalagadda</author>
<author>Luis Tari</author>
<author>Jorg Hakenberg</author>
<author>Chitta Baral</author>
<author>Graciela Gonzalez</author>
</authors>
<title>Towards effective sentence simplification for automatic processing of biomedical text.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<marker>Jonnalagadda, Tari, Hakenberg, Baral, Gonzalez, 2009</marker>
<rawString>Siddhartha Jonnalagadda, Luis Tari, Jorg Hakenberg, Chitta Baral, and Graciela Gonzalez. 2009. Towards effective sentence simplification for automatic processing of biomedical text. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence.</journal>
<contexts>
<context position="2077" citStr="Knight and Marcu, 2002" startWordPosition="298" endWordPosition="301">derly, the hearing impaired and people with aphasia or cognitive disabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks, including parsing, semantic role labeling, machine translation and summarization (Miwa et al., 2010; Jonnala665 gadda et al., 2009; Vickrey and Koller, 2008; Chandrasekar and Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set con</context>
<context position="5598" citStr="Knight and Marcu (2002)" startWordPosition="809" endWordPosition="812">English Wikipedia and English Wikipedia. However, they only provide a list of the top phrasal simplifications and do not utilize them in an endto-end simplification system. Finally, Napoles and Dredze (2010) provide an analysis of the differences between documents in English Wikipedia and Simple English Wikipedia, though they do not view the data set as a parallel corpus. Although the simplification problem shares some characteristics with the text compression problem, existing text compression data sets are small and contain a restricted set of possible transformations (often only deletion). Knight and Marcu (2002) introduced the Zipf-Davis corpus which contains 1K sentence pairs. Cohn and Lapata (2009) manually generated two parallel corpora from news stories totaling 3K sentence pairs. Finally, Nomoto (2009) generated a data set based on RSS feeds containing 2K sentence pairs. 3 Simplification Corpus Generation We generated a parallel simplification corpus by aligning sentences between English Wikipedia and Simple English Wikipedia. We obtained complete copies of English Wikipedia and Simple English Wikipedia in May 2010. We first paired the articles by title, then removed all article pairs where eith</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="14011" citStr="Koehn, 2004" startWordPosition="2215" endWordPosition="2216">lso experimented with T3 (Cohn and Lapata, 2009) but the results were poor and are not presented here. System BLEU word-F1 SSA NONE 0.5937 0.5967 0.6179 Moses 0.5987 0.6076 0.6224 Moses-Oracle 0.6317 0.6661 0.6550 Table 3: Test scores for the baseline (NONE), Moses and Moses-Oracle. for text compression (Clarke and Lapata, 2006). All three of these measures have been shown to correlate with human judgements in their respective domains. Table 3 shows the results of our initial test. All differences are statistically significant at p = 0.01, measured using bootstrap resampling with 100 samples (Koehn, 2004). Although the baseline does well (recall that over a quarter of the sentence pairs in the data set are identical) the phrase-based approach does obtain a statistically significant improvement. To understand the the limits of the phrase-based model for text simplification, we generated an nbest list of the 1000 most-likely simplifications for each test sentence. We then greedily picked the simplification from this n-best list that had the highest sentence-level BLEU score based on the test examples, labeled Moses-Oracle in Table 3. The large difference between Moses and Moses-Oracle indicates </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="2093" citStr="McDonald, 2006" startWordPosition="302" endWordPosition="303">red and people with aphasia or cognitive disabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks, including parsing, semantic role labeling, machine translation and summarization (Miwa et al., 2010; Jonnala665 gadda et al., 2009; Vickrey and Koller, 2008; Chandrasekar and Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set consisting of 137K </context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Saetre</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Entity-focused sentence simplication for relation extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1750" citStr="Miwa et al., 2010" startWordPosition="251" endWordPosition="254">a sentence, the goal is to produce an equivalent sentence where the vocabulary and sentence structure are simpler. Text simplification has a number of important applications. Simplification techniques can be used to make text resources available to a broader range of readers, including children, language learners, the elderly, the hearing impaired and people with aphasia or cognitive disabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks, including parsing, semantic role labeling, machine translation and summarization (Miwa et al., 2010; Jonnala665 gadda et al., 2009; Vickrey and Koller, 2008; Chandrasekar and Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques lik</context>
</contexts>
<marker>Miwa, Saetre, Miyao, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun’ichi Tsujii. 2010. Entity-focused sentence simplication for relation extraction. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Mark Dredze</author>
</authors>
<title>Learning simple Wikipedia: A cogitation in ascertaining abecedarian language.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT/NAACL Workshop on Computation Linguistics and Writing.</booktitle>
<contexts>
<context position="5182" citStr="Napoles and Dredze (2010)" startWordPosition="747" endWordPosition="750">ikipedia and Simple English Wikipedia have both received some recent attention as a useful resource for text simplification and the related task of text compression. Yamangil and Nelken (2008) examine the history logs of English Wikipedia to learn sentence compression rules. Yatskar et al. (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. However, they only provide a list of the top phrasal simplifications and do not utilize them in an endto-end simplification system. Finally, Napoles and Dredze (2010) provide an analysis of the differences between documents in English Wikipedia and Simple English Wikipedia, though they do not view the data set as a parallel corpus. Although the simplification problem shares some characteristics with the text compression problem, existing text compression data sets are small and contain a restricted set of possible transformations (often only deletion). Knight and Marcu (2002) introduced the Zipf-Davis corpus which contains 1K sentence pairs. Cohn and Lapata (2009) manually generated two parallel corpora from news stories totaling 3K sentence pairs. Finally</context>
</contexts>
<marker>Napoles, Dredze, 2010</marker>
<rawString>Courtney Napoles and Mark Dredze. 2010. Learning simple Wikipedia: A cogitation in ascertaining abecedarian language. In Proceedings of HLT/NAACL Workshop on Computation Linguistics and Writing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart Shieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingual corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="6875" citStr="Nelken and Shieber, 2006" startWordPosition="1004" endWordPosition="1007">d as a stub, was flagged as a disambiguation page or was a meta-page about Wikipedia. After pairing and filtering, 10,588 aligned, content article pairs remained (a 90% reduction from the original 110K Simple English Wikipedia articles). Throughout the rest of this paper we will refer to unsimplified text from English Wikipedia as normal and to the simplified version from Simple English Wikipedia as simple. To generate aligned sentence pairs from the aligned document pairs we followed an approach similar to those utilized in previous monolingual alignment problems (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006). Paragraphs were identified based on formatting information available in the articles. Each simple paragraph was then aligned to every normal paragraph where the TF-IDF, cosine similarity was over a threshold or 0.5. We initially investigated the paragraph clustering preprocessing step in (Barzilay and Elhadad, 2003), but did not find a qualitative difference and opted for the simpler similarity-based alignment approach, which does not require manual annotation. 666 For each aligned paragraph pair (i.e. a simple paragraph and one or more normal paragraphs), we then used a dynamic programming </context>
</contexts>
<marker>Nelken, Shieber, 2006</marker>
<rawString>Rani Nelken and Stuart Shieber. 2006. Towards robust context-sensitive sentence alignment for monolingual corpora. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>Discriminative sentence compression with conditional random fields.</title>
<date>2007</date>
<booktitle>In Information Processing and Management.</booktitle>
<marker>Nomoto, 2007</marker>
<rawString>Tadashi Nomoto. 2007. Discriminative sentence compression with conditional random fields. In Information Processing and Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>A generic sentence trimmer with CRFs.</title>
<date>2008</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<marker>Nomoto, 2008</marker>
<rawString>Tadashi Nomoto. 2008. A generic sentence trimmer with CRFs. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>A comparison of model free versus model intensive approaches to sentence compression.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2133" citStr="Nomoto, 2009" startWordPosition="309" endWordPosition="310">isabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks, including parsing, semantic role labeling, machine translation and summarization (Miwa et al., 2010; Jonnala665 gadda et al., 2009; Vickrey and Koller, 2008; Chandrasekar and Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set consisting of 137K aligned simplified/unsimplified sentence</context>
<context position="5797" citStr="Nomoto (2009)" startWordPosition="841" endWordPosition="842">rovide an analysis of the differences between documents in English Wikipedia and Simple English Wikipedia, though they do not view the data set as a parallel corpus. Although the simplification problem shares some characteristics with the text compression problem, existing text compression data sets are small and contain a restricted set of possible transformations (often only deletion). Knight and Marcu (2002) introduced the Zipf-Davis corpus which contains 1K sentence pairs. Cohn and Lapata (2009) manually generated two parallel corpora from news stories totaling 3K sentence pairs. Finally, Nomoto (2009) generated a data set based on RSS feeds containing 2K sentence pairs. 3 Simplification Corpus Generation We generated a parallel simplification corpus by aligning sentences between English Wikipedia and Simple English Wikipedia. We obtained complete copies of English Wikipedia and Simple English Wikipedia in May 2010. We first paired the articles by title, then removed all article pairs where either article: contained only a single line, was flagged as a stub, was flagged as a disambiguation page or was a meta-page about Wikipedia. After pairing and filtering, 10,588 aligned, content article </context>
</contexts>
<marker>Nomoto, 2009</marker>
<rawString>Tadashi Nomoto. 2009. A comparison of model free versus model intensive approaches to sentence compression. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11433" citStr="Och and Ney, 2003" startWordPosition="1799" endWordPosition="1802">ces that do not align. Table 1 shows the proportion of the different sentence level alignment operations in our data set. On both the simple and normal sides there are many sentences that do not align. Operation % skip simple 27% skip normal 23% one normal to one simple 37% one normal to two simple 8% two normal to one simple 5% Table 1: Frequency of sentence-level alignment operations based on our learned sentence alignment. No 2-to-2 alignments were found in the data. To better understand how sentences are transformed from normal to simple sentences we learned a word alignment using GIZA++ (Och and Ney, 2003). Based on this word alignment, we calculated the percentage of sentences that included: reI max 667 wordings – a normal word is changed to a different simple word, deletions – a normal word is deleted, reorderings – non-monotonic alignment, splits – a normal words is split into multiple simple words, and merges – multiple normal words are condensed to a single simple word. Transformation % rewordings 65% deletions 47% reorders 34% merges 31% splits 27% Table 2: Percentage of sentence pairs that contained word-level operations based on the induced word alignment. Splits and merges are from the</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation. Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="2421" citStr="Och and Ney, 2004" startWordPosition="351" endWordPosition="354">008; Chandrasekar and Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och and Ney, 2004; Chiang, 2010). Most prior techniques for text simplification have involved either hand-crafted rules (Vickrey and Koller, 2008; Feng, 2008) or learned within a very restricted rule space (Chandrasekar and Srinivas, 1997). We have generated a data set consisting of 137K aligned simplified/unsimplified sentence pairs by pairing documents, then sentences from English Wikipedia1 with corresponding documents and sentences from Simple English Wikipedia2. Simple English Wikipedia contains articles aimed at children and English language learners and contains similar content to English Wikipedia but </context>
<context position="12692" citStr="Och and Ney, 2004" startWordPosition="1999" endWordPosition="2002">nce. These are not mutually exclusive events. Table 2 shows the percentage of each of these phenomena occurring in the sentence pairs. All of the different operations occur frequently in the data set with rewordings being particularly prevalent. 5 Sentence-level Text Simplification To understand the usefulness of this data we ran preliminary experiments to learn a sentence-level simplification system. We view the problem of text simplification as an English-to-English translation problem. Motivated by the importance of lexical changes, we used Moses, a phrase-based machine translation system (Och and Ney, 2004).3 We trained Moses on 124K pairs from the data set and the n-gram language model on the simple side of this data. We trained the hyper-parameters of the loglinear model on a 500 sentence pair development set. We compared the trained system to a baseline of not doing any simplification (NONE). We evaluated the two approaches on a test set of 1300 sentence pairs. Since there is currently no standard for automatically evaluating sentence simplification, we used three different automatic measures that have been used in related domains: BLEU, which has been used extensively in machine translation </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Kenji Yamada</author>
<author>U Stanford</author>
<author>Alex Fraser</author>
<author>Daniel Gildea</author>
<author>Viren Jain</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="14732" citStr="Och et al., 2004" startWordPosition="2325" endWordPosition="2328">dentical) the phrase-based approach does obtain a statistically significant improvement. To understand the the limits of the phrase-based model for text simplification, we generated an nbest list of the 1000 most-likely simplifications for each test sentence. We then greedily picked the simplification from this n-best list that had the highest sentence-level BLEU score based on the test examples, labeled Moses-Oracle in Table 3. The large difference between Moses and Moses-Oracle indicates possible room for improvement utilizing better parameter estimation or n-best list reranking techniques (Och et al., 2004; Ge and Mooney, 2006). 6 Conclusion We have described a new text simplification data set generated from aligning sentences in Simple English Wikipedia with sentences in English Wikipedia. The data set is orders of magnitude larger than any currently available for text simplification or for the related field of text compression and is publicly available.4 We provided preliminary text simplification results using Moses, a phrase-based translation system, and saw a statistically significant improvement of 0.005 BLEU over the baseline of no simplification and showed that further improvement of up</context>
</contexts>
<marker>Och, Yamada, Stanford, Fraser, Gildea, Jain, 2004</marker>
<rawString>Franz Josef Och, Kenji Yamada, Stanford U, Alex Fraser, Daniel Gildea, and Viren Jain. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13315" citStr="Papineni et al., 2002" startWordPosition="2102" endWordPosition="2106">3 We trained Moses on 124K pairs from the data set and the n-gram language model on the simple side of this data. We trained the hyper-parameters of the loglinear model on a 500 sentence pair development set. We compared the trained system to a baseline of not doing any simplification (NONE). We evaluated the two approaches on a test set of 1300 sentence pairs. Since there is currently no standard for automatically evaluating sentence simplification, we used three different automatic measures that have been used in related domains: BLEU, which has been used extensively in machine translation (Papineni et al., 2002), and word-level F1 and simple string accuracy (SSA) which have been suggested 3We also experimented with T3 (Cohn and Lapata, 2009) but the results were poor and are not presented here. System BLEU word-F1 SSA NONE 0.5937 0.5967 0.6179 Moses 0.5987 0.6076 0.6224 Moses-Oracle 0.6317 0.6661 0.6550 Table 3: Test scores for the baseline (NONE), Moses and Moses-Oracle. for text compression (Clarke and Lapata, 2006). All three of these measures have been shown to correlate with human judgements in their respective domains. Table 3 shows the results of our initial test. All differences are statistic</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
</authors>
<title>Methods for sentence compression.</title>
<date>2010</date>
<tech>Technical Report MS-CIS-10-20,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Pitler, 2010</marker>
<rawString>Emily Pitler. 2010. Methods for sentence compression. Technical Report MS-CIS-10-20, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Daphne Koller</author>
</authors>
<title>Sentence simplification for semantic role labeling.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1807" citStr="Vickrey and Koller, 2008" startWordPosition="261" endWordPosition="264">entence where the vocabulary and sentence structure are simpler. Text simplification has a number of important applications. Simplification techniques can be used to make text resources available to a broader range of readers, including children, language learners, the elderly, the hearing impaired and people with aphasia or cognitive disabilities (Carroll et al., 1998; Feng, 2008). As a preprocessing step, simplification can improve the performance of NLP tasks, including parsing, semantic role labeling, machine translation and summarization (Miwa et al., 2010; Jonnala665 gadda et al., 2009; Vickrey and Koller, 2008; Chandrasekar and Srinivas, 1997). Finally, models for text simplification are similar to models for sentence compression; advances in simplification can benefit compression, which has applications in mobile devices, summarization and captioning (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Nomoto, 2009; Cohn and Lapata, 2009). One of the key challenges for text simplification is data availability. The small amount of simplification data currently available has prevented the application of data-driven techniques like those used in other text-to-text translation areas (Och</context>
</contexts>
<marker>Vickrey, Koller, 2008</marker>
<rawString>David Vickrey and Daphne Koller. 2008. Sentence simplification for semantic role labeling. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Rani Nelken</author>
</authors>
<title>Mining Wikipedia revision histories for improving sentence compression.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4749" citStr="Yamangil and Nelken (2008)" startWordPosition="679" endWordPosition="682">se process, producing electrical energy from mechanical, Simple: energy, is accomplished by a generator or dynamo. A dynamo or an electric generator does the reverse: it changes mechanical movement into electric energy. Figure 1: Example sentence simplifications extracted from Wikipedia. Normal refers to a sentence in an English Wikipedia article and Simple to a corresponding sentence in Simple English Wikipedia. 2 Previous Data Wikipedia and Simple English Wikipedia have both received some recent attention as a useful resource for text simplification and the related task of text compression. Yamangil and Nelken (2008) examine the history logs of English Wikipedia to learn sentence compression rules. Yatskar et al. (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. However, they only provide a list of the top phrasal simplifications and do not utilize them in an endto-end simplification system. Finally, Napoles and Dredze (2010) provide an analysis of the differences between documents in English Wikipedia and Simple English Wikipedia, though they do not view the data set as a parallel corpus.</context>
</contexts>
<marker>Yamangil, Nelken, 2008</marker>
<rawString>Elif Yamangil and Rani Nelken. 2008. Mining Wikipedia revision histories for improving sentence compression. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Critian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In HLT/NAACL Short Papers.</booktitle>
<contexts>
<context position="4854" citStr="Yatskar et al. (2010)" startWordPosition="696" endWordPosition="699">amo. A dynamo or an electric generator does the reverse: it changes mechanical movement into electric energy. Figure 1: Example sentence simplifications extracted from Wikipedia. Normal refers to a sentence in an English Wikipedia article and Simple to a corresponding sentence in Simple English Wikipedia. 2 Previous Data Wikipedia and Simple English Wikipedia have both received some recent attention as a useful resource for text simplification and the related task of text compression. Yamangil and Nelken (2008) examine the history logs of English Wikipedia to learn sentence compression rules. Yatskar et al. (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. However, they only provide a list of the top phrasal simplifications and do not utilize them in an endto-end simplification system. Finally, Napoles and Dredze (2010) provide an analysis of the differences between documents in English Wikipedia and Simple English Wikipedia, though they do not view the data set as a parallel corpus. Although the simplification problem shares some characteristics with the text compression problem, exist</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Critian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia. In HLT/NAACL Short Papers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>