<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.996561">
Unsupervised Semantic Parsing
</title>
<author confidence="0.999173">
Hoifung Poon Pedro Domingos
</author>
<affiliation confidence="0.998808">
Department of Computer Science and Engineering
University of Washington
</affiliation>
<address confidence="0.686726">
Seattle, WA 98195-2350, U.S.A.
</address>
<email confidence="0.99971">
{hoifung,pedrod}@cs.washington.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999708588235294">
We present the first unsupervised approach
to the problem of learning a semantic
parser, using Markov logic. Our USP
system transforms dependency trees into
quasi-logical forms, recursively induces
lambda forms from these, and clusters
them to abstract away syntactic variations
of the same meaning. The MAP semantic
parse of a sentence is obtained by recur-
sively assigning its parts to lambda-form
clusters and composing them. We evalu-
ate our approach by using it to extract a
knowledge base from biomedical abstracts
and answer questions. USP substantially
outperforms TextRunner, DIRT and an in-
formed baseline on both precision and re-
call on this task.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999879289473684">
Semantic parsing maps text to formal meaning
representations. This contrasts with semantic role
labeling (Carreras and Marquez, 2004) and other
forms of shallow semantic processing, which do
not aim to produce complete formal meanings.
Traditionally, semantic parsers were constructed
manually, but this is too costly and brittle. Re-
cently, a number of machine learning approaches
have been proposed (Zettlemoyer and Collins,
2005; Mooney, 2007). However, they are super-
vised, and providing the target logical form for
each sentence is costly and difficult to do consis-
tently and with high quality. Unsupervised ap-
proaches have been applied to shallow semantic
tasks (e.g., paraphrasing (Lin and Pantel, 2001),
information extraction (Banko et al., 2007)), but
not to semantic parsing.
In this paper we develop the first unsupervised
approach to semantic parsing, using Markov logic
(Richardson and Domingos, 2006). Our USP sys-
tem starts by clustering tokens of the same type,
and then recursively clusters expressions whose
subexpressions belong to the same clusters. Ex-
periments on a biomedical corpus show that this
approach is able to successfully translate syntac-
tic variations into a logical representation of their
common meaning (e.g., USP learns to map active
and passive voice to the same logical form, etc.).
This in turn allows it to correctly answer many
more questions than systems based on TextRun-
ner (Banko et al., 2007) and DIRT (Lin and Pantel,
2001).
We begin by reviewing the necessary back-
ground on semantic parsing and Markov logic. We
then describe our Markov logic network for un-
supervised semantic parsing, and the learning and
inference algorithms we used. Finally, we present
our experiments and results.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.997155">
2.1 Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.999959153846154">
The standard language for formal meaning repre-
sentation is first-order logic. A term is any ex-
pression representing an object in the domain. An
atomic formula or atom is a predicate symbol ap-
plied to a tuple of terms. Formulas are recursively
constructed from atomic formulas using logical
connectives and quantifiers. A lexical entry de-
fines the logical form for a lexical item (e.g., a
word). The semantic parse of a sentence is de-
rived by starting with logical forms in the lexi-
cal entries and recursively composing the mean-
ing of larger fragments from their parts. In tradi-
tional approaches, the lexical entries and meaning-
</bodyText>
<page confidence="0.822323">
1
</page>
<note confidence="0.9965645">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1–10,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99951925">
composition rules are both manually constructed.
Below are sample rules in a definite clause gram-
mar (DCG) for parsing the sentence: “Utah bor-
ders Idaho”.
</bodyText>
<equation confidence="0.9992032">
V erb[AyAx.borders(x, y)] → borders
NP[Utah] → Utah
NP[Idaho] → Idaho
VP[rel(obj)] → Verb[rel] NP[obj]
5[rel(obj)] → NP[obj] VP[rel]
</equation>
<bodyText confidence="0.996047722222222">
The first three lines are lexical entries. They are
fired upon seeing the individual words. For exam-
ple, the first rule applies to the word “borders” and
generates syntactic category Verb with the mean-
ing AyAx.borders(x, y) that represents the next-
to relation. Here, we use the standard lambda-
calculus notation, where AyAx.borders(x, y)
represents a function that is true for any (x, y)-
pair such that borders(x, y) holds. The last two
rules compose the meanings of sub-parts into that
of the larger part. For example, after the first
and third rules are fired, the fourth rule fires and
generates VP[AyAx.borders(x, y)(Idaho)]; this
meaning simplifies to Ax.borders(x, Idaho) by
the A-reduction rule, which substitutes the argu-
ment for a variable in a functional application.
A major challenge to semantic parsing is syn-
tactic variations of the same meaning, which
abound in natural languages. For example, the
aforementioned sentence can be rephrased as
“Utah is next to Idaho,”“Utah shares a border with
Idaho,” etc. Manually encoding all these varia-
tions into the grammar is tedious and error-prone.
Supervised semantic parsing addresses this issue
by learning to construct the grammar automati-
cally from sample meaning annotations (Mooney,
2007). Existing approaches differ in the meaning
representation languages they use and the amount
of annotation required. In the approach of Zettle-
moyer and Collins (2005), the training data con-
sists of sentences paired with their meanings in
lambda form. A probabilistic combinatory cate-
gorial grammar (PCCG) is learned using a log-
linear model, where the probability of the final
logical form L and meaning-derivation tree T
conditioned on the sentence 5 is P(L, T |5) =
</bodyText>
<equation confidence="0.973333">
Z exp (�
1 i wifi(L, T, 5)). Here Z is the normal-
</equation>
<bodyText confidence="0.999974809523809">
ization constant and fi are the feature functions
with weights wi. Candidate lexical entries are gen-
erated by a domain-specific procedure based on
the target logical forms.
The major limitation of supervised approaches
is that they require meaning annotations for ex-
ample sentences. Even in a restricted domain,
doing this consistently and with high quality re-
quires nontrivial effort. For unrestricted text, the
complexity and subjectivity of annotation render it
essentially infeasible; even pre-specifying the tar-
get predicates and objects is very difficult. There-
fore, to apply semantic parsing beyond limited do-
mains, it is crucial to develop unsupervised meth-
ods that do not rely on labeled meanings.
In the past, unsupervised approaches have been
applied to some semantic tasks, but not to seman-
tic parsing. For example, DIRT (Lin and Pan-
tel, 2001) learns paraphrases of binary relations
based on distributional similarity of their argu-
ments; TextRunner (Banko et al., 2007) automati-
cally extracts relational triples in open domains us-
ing a self-trained extractor; SNE applies relational
clustering to generate a semantic network from
TextRunner triples (Kok and Domingos, 2008).
While these systems illustrate the promise of un-
supervised methods, the semantic content they ex-
tract is nonetheless shallow and does not constitute
the complete formal meaning that can be obtained
by a semantic parser.
Another issue is that existing approaches to se-
mantic parsing learn to parse syntax and semantics
together.1 The drawback is that the complexity
in syntactic processing is coupled with semantic
parsing and makes the latter even harder. For ex-
ample, when applying their approach to a different
domain with somewhat less rigid syntax, Zettle-
moyer and Collins (2007) need to introduce new
combinators and new forms of candidate lexical
entries. Ideally, we should leverage the enormous
progress made in syntactic parsing and generate
semantic parses directly from syntactic analysis.
</bodyText>
<subsectionHeader confidence="0.999571">
2.2 Markov Logic
</subsectionHeader>
<bodyText confidence="0.999635555555555">
In many NLP applications, there exist rich rela-
tions among objects, and recent work in statisti-
cal relational learning (Getoor and Taskar, 2007)
and structured prediction (Bakir et al., 2007) has
shown that leveraging these can greatly improve
accuracy. One of the most powerful representa-
tions for this is Markov logic, which is a proba-
bilistic extension of first-order logic (Richardson
and Domingos, 2006). Markov logic makes it
</bodyText>
<footnote confidence="0.9959925">
1The only exception that we are aware of is Ge and
Mooney (2009).
</footnote>
<page confidence="0.995928">
2
</page>
<bodyText confidence="0.999431666666667">
possible to compactly specify probability distri-
butions over complex relational domains, and has
been successfully applied to unsupervised corefer-
ence resolution (Poon and Domingos, 2008) and
other tasks. A Markov logic network (MLN) is
a set of weighted first-order clauses. Together
with a set of constants, it defines a Markov net-
work with one node per ground atom and one fea-
ture per ground clause. The weight of a feature
is the weight of the first-order clause that origi-
nated it. The probability of a state x in such a
network is given by the log-linear model P(x) =
</bodyText>
<equation confidence="0.782835">
Z exp (�
1 i wini(x)), where Z is a normalization
</equation>
<bodyText confidence="0.8587435">
constant, wi is the weight of the ith formula, and
ni is the number of satisfied groundings.
</bodyText>
<sectionHeader confidence="0.9313445" genericHeader="method">
3 Unsupervised Semantic Parsing with
Markov Logic
</sectionHeader>
<bodyText confidence="0.999907213114755">
Unsupervised semantic parsing (USP) rests on
three key ideas. First, the target predicate and ob-
ject constants, which are pre-specified in super-
vised semantic parsing, can be viewed as clusters
of syntactic variations of the same meaning, and
can be learned from data. For example, borders
represents the next-to relation, and can be viewed
as the cluster of different forms for expressing this
relation, such as “borders”, “is next to”, “share the
border with”; Utah represents the state of Utah,
and can be viewed as the cluster of “Utah”, “the
beehive state”, etc.
Second, the identification and clustering of can-
didate forms are integrated with the learning for
meaning composition, where forms that are used
in composition with the same forms are encour-
aged to cluster together, and so are forms that are
composed of the same sub-forms. This amounts to
a novel form of relational clustering, where clus-
tering is done not just on fixed elements in rela-
tional tuples, but on arbitrary forms that are built
up recursively.
Third, while most existing approaches (manual
or supervised learning) learn to parse both syn-
tax and semantics, unsupervised semantic pars-
ing starts directly from syntactic analyses and fo-
cuses solely on translating them to semantic con-
tent. This enables us to leverage advanced syn-
tactic parsers and (indirectly) the available rich re-
sources for them. More importantly, it separates
the complexity in syntactic analysis from the se-
mantic one, and makes the latter much easier to
perform. In particular, meaning composition does
not require domain-specific procedures for gener-
ating candidate lexicons, as is often needed by su-
pervised methods.
The input to our USP system consists of de-
pendency trees of training sentences. Compared
to phrase-structure syntax, dependency trees are
the more appropriate starting point for semantic
processing, as they already exhibit much of the
relation-argument structure at the lexical level.
USP first uses a deterministic procedure to con-
vert dependency trees into quasi-logical forms
(QLFs). The QLFs and their sub-formulas have
natural lambda forms, as will be described later.
Starting with clusters of lambda forms at the atom
level, USP recursively builds up clusters of larger
lambda forms. The final output is a probability
distribution over lambda-form clusters and their
compositions, as well as the MAP semantic parses
of training sentences.
In the remainder of the section, we describe
the details of USP. We first present the procedure
for generating QLFs from dependency trees. We
then introduce their lambda forms and clusters,
and show how semantic parsing works in this set-
ting. Finally, we present the Markov logic net-
work (MLN) used by USP. In the next sections, we
present efficient algorithms for learning and infer-
ence with this MLN.
</bodyText>
<subsectionHeader confidence="0.999863">
3.1 Derivation of Quasi-Logical Forms
</subsectionHeader>
<bodyText confidence="0.999520071428572">
A dependency tree is a tree where nodes are words
and edges are dependency labels. To derive the
QLF, we convert each node to an unary atom with
the predicate being the lemma plus POS tag (be-
low, we still use the word for simplicity), and each
edge to a binary atom with the predicate being
the dependency label. For example, the node for
Utah becomes Utah(n1) and the subject depen-
dency becomes nsubj(n1, n2). Here, the ni are
Skolem constants indexed by the nodes. The QLF
for a sentence is the conjunction of the atoms for
the nodes and edges, e.g., the sentence above will
become borders(n1) ∧ Utah(n2) ∧ Idaho(n3) ∧
nsubj(n1, n2) ∧ dobj(n1, n3).
</bodyText>
<subsectionHeader confidence="0.9977175">
3.2 Lambda-Form Clusters and Semantic
Parsing in USP
</subsectionHeader>
<bodyText confidence="0.9999452">
Given a QLF, a relation or an object is repre-
sented by the conjunction of a subset of the atoms.
For example, the next-to relation is represented
by borders(n1) ∧ nsubj(n1, n2) ∧ dobj(n1, n3),
and the states of Utah and Idaho are represented
</bodyText>
<page confidence="0.994961">
3
</page>
<bodyText confidence="0.999028072727273">
by Utah(n2) and Idaho(n3). The meaning com-
position of two sub-formulas is simply their con-
junction. This allows the maximum flexibility in
learning. In particular, lexical entries are no longer
limited to be adjacent words as in Zettlemoyer and
Collins (2005), but can be arbitrary fragments in a
dependency tree.
For every sub-formula F, we define a corre-
sponding lambda form that can be derived by re-
placing every Skolem constant ni that does not
appear in any unary atom in F with a unique
lambda variable xi. Intuitively, such constants
represent objects introduced somewhere else (by
the unary atoms containing them), and corre-
spond to the arguments of the relation repre-
sented by F. For example, the lambda form
for borders(n1) ∧ nsubj(n1, n2) ∧ dobj(n1, n3)
is Ax2Ax3. borders(n1) ∧ nsubj(n1, x2) ∧
dobj(n1, x3).
Conceptually, a lambda-form cluster is a set of
semantically interchangeable lambda forms. For
example, to express the meaning that Utah bor-
ders Idaho, we can use any form in the cluster
representing the next-to relation (e.g., “borders”,
“shares a border with”), any form in the cluster
representing the state of Utah (e.g., “the beehive
state”), and any form in the cluster representing
the state of Idaho (e.g., “Idaho”). Conditioned
on the clusters, the choices of individual lambda
forms are independent of each other.
To handle variable number of arguments, we
follow Davidsonian semantics and further de-
compose a lambda form into the core form,
which does not contain any lambda variable
(e.g., borders(n1)), and the argument forms,
which contain a single lambda variable (e.g.,
Ax2.nsubj(n1, x2) and Ax3.dobj(n1, x3)). Each
lambda-form cluster may contain some number of
argument types, which cluster distinct forms of the
same argument in a relation. For example, in Stan-
ford dependencies, the object of a verb uses the de-
pendency dobj in the active voice, but nsubjpass
in passive.
Lambda-form clusters abstract away syntactic
variations of the same meaning. Given an in-
stance of cluster T with arguments of argument
types A1, · · · , Ak, its abstract lambda form is given
by Ax1 ··· Axk.T(n) ∧ MI=1 Ai(n, xi).
Given a sentence and its QLF, semantic pars-
ing amounts to partitioning the atoms in the QLF,
dividing each part into core form and argument
forms, and then assigning each form to a cluster
or an argument type. The final logical form is de-
rived by composing the abstract lambda forms of
the parts using the A-reduction rule.2
</bodyText>
<subsectionHeader confidence="0.997112">
3.3 The USP MLN
</subsectionHeader>
<bodyText confidence="0.99970365625">
Formally, for a QLF Q, a semantic parse L par-
titions Q into parts p1, p2, · · · , pn; each part p is
assigned to some lambda-form cluster c, and is
further partitioned into core form f and argument
forms f1, · · · , fk; each argument form is assigned
to an argument type a in c. The USP MLN de-
fines a joint probability distribution over Q and L
by modeling the distributions over forms and ar-
guments given the cluster or argument type.
Before presenting the predicates and formu-
las in our MLN, we should emphasize that they
should not be confused with the atoms and formu-
las in the QLFs, which are represented by reified
constants and variables.
To model distributions over lambda forms,
we introduce the predicates Form(p, f!) and
ArgForm(p, i, f!), where p is a part, i is the in-
dex of an argument, and f is a QLF subformula.
Form(p, f) is true iff part p has core form f, and
ArgForm(p, i, f) is true iff the ith argument in p
has form f.3 The “f!” notation signifies that each
part or argument can have only one form.
To model distributions over arguments, we in-
troduce three more predicates: ArgType(p, i, a!)
signifies that the ith argument of p is assigned to
argument type a; Arg(p, i, p′) signifies that the
ith argument of p is p′; Number(p, a, n) signifies
that there are n arguments of p that are assigned
to type a. The truth value of Number(p, a, n) is
determined by the ArgType atoms.
Unsupervised semantic parsing can be captured
by four formulas:
</bodyText>
<equation confidence="0.98642325">
p ∈ +c ∧ Form(p, +f)
ArgType(p, i, +a) ∧ ArgForm(p, i, +f)
Arg(p, i, p′) ∧ ArgType(p, i, +a) ∧ p′ ∈ +c′
Number(p, +a, +n)
</equation>
<bodyText confidence="0.886658714285714">
All free variables are implicitly universally quan-
tified. The “+” notation signifies that the MLN
contains an instance of the formula, with a sep-
arate weight, for each value combination of the
2Currently, we do not handle quantifier scoping or se-
mantics for specific closed-class words such as determiners.
These will be pursued in future work.
</bodyText>
<footnote confidence="0.9700415">
3There are hard constraints to guarantee that these assign-
ments form a legal partition. We omit them for simplicity.
</footnote>
<page confidence="0.993912">
4
</page>
<bodyText confidence="0.999933310344827">
variables with a plus sign. The first formula mod-
els the mixture of core forms given the cluster, and
the others model the mixtures of argument forms,
argument types, and argument numbers, respec-
tively, given the argument type.
To encourage clustering and avoid overfitting,
we impose an exponential prior with weight α on
the number of parameters.4
The MLN above has one problem: it often
clusters expressions that are semantically oppo-
site. For example, it clusters antonyms like “el-
derly/young”, “mature/immature”. This issue also
occurs in other semantic-processing systems (e.g.,
DIRT). In general, this is a difficult open problem
that only recently has started to receive some at-
tention (Mohammad et al., 2008). Resolving this
is not the focus of this paper, but we describe a
general heuristic for fixing this problem. We ob-
serve that the problem stems from the lack of nega-
tive features for discovering meanings in contrast.
In natural languages, parallel structures like con-
junctions are one such feature.5 We thus introduce
an exponential prior with weight β on the number
of conjunctions where the two conjunctive parts
are assigned to the same cluster. To detect con-
junction, we simply used the Stanford dependen-
cies that begin with “conj”. This proves very ef-
fective, fixing the majority of the errors in our ex-
periments.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999853428571429">
Given a sentence and the quasi-logical form Q
derived from its dependency tree, the conditional
probability for a semantic parse L is given by
Pr(L|Q) a exp (Ei wini(L, Q)). The MAP se-
mantic parse is simply arg maxL &amp; wini(L, Q).
Enumerating all L’s is intractable. It is also un-
necessary, since most partitions will result in parts
whose lambda forms have no cluster they can be
assigned to. Instead, USP uses a greedy algorithm
to search for the MAP parse. First we introduce
some definitions: a partition is called A-reducible
from p if it can be obtained from the current parti-
tion by recursively A-reducing the part containing
p with one of its arguments; such a partition is
</bodyText>
<footnote confidence="0.749255">
4Excluding weights of ∞ or −∞, which signify hard con-
straints.
5For example, in the sentence “IL-2 inhibits X in A and
induces Y in B”, the conjunction between “inhibits” and “in-
duces” suggests that they are different. If “inhibits” and “in-
duces” are indeed synonyms, such a sentence will sound awk-
ward and would probably be rephrased as “IL-2 inhibits X in
A and Y in B”.
</footnote>
<bodyText confidence="0.898435">
Algorithm 1 USP-Parse(MLN, QLF)
Form parts for individual atoms in QLF and as-
sign each to its most probable cluster
repeat
for all parts p in the current partition do
for all partitions that are A-reducible from
p and feasible do
Find the most probable cluster and argu-
ment type assignments for the new part
</bodyText>
<listItem confidence="0.444406666666667">
and its arguments
end for
end for
</listItem>
<bodyText confidence="0.997958621621622">
Change to the new partition and assignments
with the highest gain in probability
until none of these improve the probability
return current partition and assignments
called feasible if the core form of the new part is
contained in some cluster. For example, consider
the QLF of “Utah borders Idaho” and assume
that the current partition is Ax2x3.borders(n1) ∧
nsubj(n1, x2) ∧ dobj(n1, x3), Utah(n2),
Idaho(n3). Then the following partition is
A-reducible from the first part in the above
partition: Ax3.borders(n1) ∧ nsubj(n1, n2) ∧
Utah(n2) ∧ dobj(n1, x3), Idaho(n3). Whether
this new partition is feasible depends on whether
the core form of the new part Ax3.borders(n1) ∧
nsubj(n1, n2) ∧ Utah(n2) ∧ dobj(n1, x3) (i.e.
borders(n1) ∧ nsubj(n1, n2) ∧ Utah(n2)) is
contained in some lambda-form cluster.
Algorithm 1 gives pseudo-code for our algo-
rithm. Given part p, finding partitions that are A-
reducible from p and feasible can be done in time
O(5T), where 5 is the size of the clustering in
the number of core forms and T is the maximum
number of atoms in a core form. We omit the proof
here but point out that it is related to the unordered
subtree matching problem which can be solved in
linear time (Kilpelainen, 1992). Inverted indexes
(e.g., from p to eligible core forms) are used to fur-
ther improve the efficiency. For a new part p and
a cluster that contains p’s core form, there are km
ways of assigning p’s m arguments to the k argu-
ment types of the cluster. For larger k and m, this
is very expensive. We therefore approximate it by
assigning each argument to the best type, indepen-
dent of other arguments.
This algorithm is very efficient, and is used re-
peatedly in learning.
</bodyText>
<page confidence="0.989476">
5
</page>
<sectionHeader confidence="0.997623" genericHeader="method">
5 Learning
</sectionHeader>
<bodyText confidence="0.999964">
The learning problem in USP is to maximize the
log-likelihood of observing the QLFs obtained
from the dependency trees, denoted by Q, sum-
ming out the unobserved semantic parses:
</bodyText>
<equation confidence="0.9969025">
LB(Q) = log PB(Q)
= log EL PB(Q, L)
</equation>
<bodyText confidence="0.994376317073171">
Here, L are the semantic parses, 0 are the MLN pa-
rameters, and PB(Q, L) are the completion likeli-
hoods. A serious challenge in unsupervised learn-
ing is the identifiability problem (i.e., the opti-
mal parameters are not unique) (Liang and Klein,
2008). This problem is particularly severe for
log-linear models with hard constraints, which are
common in MLNs. For example, in our USP
MLN, conditioned on the fact that p E c, there is
exactly one value of f that can satisfy the formula
p E c n Form(p, f), and if we add some constant
number to the weights of p E c n Form(p, f) for
all f, the probability distribution stays the same.6
The learner can be easily confused by the infinitely
many optima, especially in the early stages. To
address this problem, we impose local normaliza-
tion constraints on specific groups of formulas that
are mutually exclusive and exhaustive, i.e., in each
group, we require that Eki=1 ewi = 1, where wi
are the weights of formulas in the group. Group-
ing is done in such a way as to encourage the
intended mixture behaviors. Specifically, for the
rule p E +c n Form(p,+f), all instances given
a fixed c form a group; for each of the remain-
ing three rules, all instances given a fixed a form a
group. Notice that with these constraints the com-
pletion likelihood P(Q, L) can be computed in
closed form for any L. In particular, each formula
group contributes a term equal to the weight of the
currently satisfied formula. In addition, the opti-
mal weights that maximize the completion likeli-
hood P(Q, L) can be derived in closed form us-
ing empirical relative frequencies. E.g., the opti-
mal weight of p E c n Form(p, f) is log(nc,f/nc),
where nc,f is the number of parts p that satisfy
both p E c and Form(p, f), and nc is the number
of parts p that satisfy p E c.7 We leverage this fact
for efficient learning in USP.
6Regularizations, e.g., Gaussian priors on weights, allevi-
ate this problem by penalizing large weights, but it remains
true that weights within a short range are roughly equivalent.
</bodyText>
<footnote confidence="0.991579">
7To see this, notice that for a given c, the total contribu-
tion to the completion likelihood from all groundings in its
formula group is Ef wc,fnc,f. In addition, Ef nc,f = nc
</footnote>
<table confidence="0.366063357142857">
Algorithm 2 USP-Learn(MLN, QLFs)
Create initial clusters and semantic parses
Merge clusters with the same core form
Agenda +— 0
repeat
for all candidate operations O do
Score O by log-likelihood improvement
if score is above a threshold then
Add O to agenda
end if
end for
Execute the highest scoring operation O∗ in
the agenda
Regenerate MAP parses for affected QLFs
</table>
<bodyText confidence="0.977313594594594">
and update agenda and candidate operations
until agenda is empty
return the MLN with learned weights and the
semantic parses
Another major challenge in USP learning is the
summation in the likelihood, which is over all pos-
sible semantic parses for a given dependency tree.
Even an efficient sampler like MC-SAT (Poon and
Domingos, 2006), as used in Poon &amp; Domingos
(2008), would have a hard time generating accu-
rate estimates within a reasonable amount of time.
On the other hand, as already noted in the previous
section, the lambda-form distribution is generally
sparse. Large lambda-forms are rare, as they cor-
respond to complex expressions that are often de-
composable into smaller ones. Moreover, while
ambiguities are present at the lexical level, they
quickly diminish when more words are present.
Therefore, a lambda form can usually only belong
to a small number of clusters, if not a unique one.
We thus simplify the problem by approximating
the sum with the mode, and search instead for the
L and 0 that maximize log PB(Q, L). Since the op-
timal weights and log-likelihood can be derived in
closed form given the semantic parses L, we sim-
ply search over semantic parses, evaluating them
using log-likelihood.
Algorithm 2 gives pseudo-code for our algo-
rithm. The input consists of an MLN without
weights and the QLFs for the training sentences.
Two operators are used for updating semantic
parses. The first is to merge two clusters, denoted
by MERGE(C1, C2) for clusters C1, C2, which does
the following:
and there is the local normalization constraint Ef ewc,f = 1.
The optimal weights wc,f are easily derived by solving this
constrained optimization problem.
</bodyText>
<page confidence="0.995542">
6
</page>
<listItem confidence="0.999519833333333">
1. Create a new cluster C and add all core forms
in C1, C2 to C;
2. Create new argument types for C by merg-
ing those in C1, C2 so as to maximize the log-
likelihood;
3. Remove C1, C2.
</listItem>
<bodyText confidence="0.9993196875">
Here, merging two argument types refers to pool-
ing their argument forms to create a new argument
type. Enumerating all possible ways of creating
new argument types is intractable. USP approxi-
mates it by considering one type at a time and ei-
ther creating a new type for it or merging it to types
already considered, whichever maximizes the log-
likelihood. The types are considered in decreasing
order of their numbers of occurrences so that more
information is available for each decision. MERGE
clusters syntactically different expressions whose
meanings appear to be the same according to the
model.
The second operator is to create a new clus-
ter by composing two existing ones, denoted by
COMPOSE(CR, CA), which does the following:
</bodyText>
<listItem confidence="0.983988714285714">
1. Create a new cluster C;
2. Find all parts r ∈ CR, a ∈ CA such that a is
an argument of r, compose them to r(a) by
A-reduction and add the new part to C;
3. Create new argument types for C from the ar-
gument forms of r(a) so as to maximize the
log-likelihood.
</listItem>
<bodyText confidence="0.997939857142857">
COMPOSE creates clusters of large lambda-forms
if they tend to be composed of the same sub-
forms (e.g., the lambda form for “is next to”).
These lambda-forms may later be merged with
other clusters (e.g., borders).
At learning time, USP maintains an agenda that
contains operations that have been evaluated and
are pending execution. During initialization, USP
forms a part and creates a new cluster for each
unary atom u(n). It also assigns binary atoms of
the form b(n, n′) to the part as argument forms
and creates a new argument type for each. This
forms the initial clustering and semantic parses.
USP then merges clusters with the same core form
(i.e., the same unary predicate) using MERGE.8 At
each step, USP evaluates the candidate operations
and adds them to the agenda if the improvement is
8Word-sense disambiguation can be handled by including
a new kind of operator that splits a cluster into subclusters.
We leave this to future work.
above a threshold.9 The operation with the highest
score is executed, and the parameters are updated
with the new optimal values. The QLFs which
contain an affected part are reparsed, and opera-
tions in the agenda whose score might be affected
are re-evaluated. These changes are done very ef-
ficiently using inverted indexes. We omit the de-
tails here due to space limitations. USP terminates
when the agenda is empty, and outputs the current
MLN parameters and semantic parses.
USP learning uses the same optimization objec-
tive as hard EM, and is also guaranteed to find a
local optimum since at each step it improves the
log-likelihood. It differs from EM in directly opti-
mizing the likelihood instead of a lower bound.
</bodyText>
<sectionHeader confidence="0.999143" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.981726">
6.1 Task
</subsectionHeader>
<bodyText confidence="0.999757392857143">
Evaluating unsupervised semantic parsers is dif-
ficult, because there is no predefined formal lan-
guage or gold logical forms for the input sen-
tences. Thus the best way to test them is by using
them for the ultimate goal: answering questions
based on the input corpus. In this paper, we ap-
plied USP to extracting knowledge from biomedi-
cal abstracts and evaluated its performance in an-
swering a set of questions that simulate the in-
formation needs of biomedical researchers. We
used the GENIA dataset (Kim et al., 2003) as
the source for knowledge extraction. It contains
1999 PubMed abstracts and marks all mentions
of biomedical entities according to the GENIA
ontology, such as cell, protein, and DNA. As a
first approximation to the questions a biomedi-
cal researcher might ask, we generated a set of
two thousand questions on relations between enti-
ties. Sample questions are: “What regulates MIP-
1alpha?”, “What does anti-STAT 1 inhibit?”. To
simulate the real information need, we sample the
relations from the 100 most frequently used verbs
(excluding the auxiliary verbs be, have, and do),
and sample the entities from those annotated in
GENIA, both according to their numbers of occur-
rences. We evaluated USP by the number of an-
swers it provided and the accuracy as determined
by manual labeling.10
</bodyText>
<footnote confidence="0.9997375">
9We currently set it to 10 to favor precision and guard
against errors due to inexact estimates.
10The labels and questions are available at
http://alchemy.cs.washington.edu/papers/poon09.
</footnote>
<page confidence="0.9987">
7
</page>
<subsectionHeader confidence="0.995947">
6.2 Systems
</subsectionHeader>
<bodyText confidence="0.999718767676768">
Since USP is the first unsupervised semantic
parser, conducting a meaningful comparison of it
with other systems is not straightforward. Stan-
dard question-answering (QA) benchmarks do not
provide the most appropriate comparison, be-
cause they tend to simultaneously emphasize other
aspects not directly related to semantic pars-
ing. Moreover, most state-of-the-art QA sys-
tems use supervised learning in their key compo-
nents and/or require domain-specific engineering
efforts. The closest available system to USP in
aims and capabilities is TextRunner (Banko et al.,
2007), and we compare with it. TextRunner is the
state-of-the-art system for open-domain informa-
tion extraction; its goal is to extract knowledge
from text without using supervised labels. Given
that a central challenge to semantic parsing is re-
solving syntactic variations of the same meaning,
we also compare with RESOLVER (Yates and Et-
zioni, 2009), a state-of-the-art unsupervised sys-
tem based on TextRunner for jointly resolving en-
tities and relations, and DIRT (Lin and Pantel,
2001), which resolves paraphrases of binary rela-
tions. Finally, we also compared to an informed
baseline based on keyword matching.
Keyword: We consider a baseline system based
on keyword matching. The question substring
containing the verb and the available argument is
directly matched with the input text, ignoring case
and morphology. We consider two ways to derive
the answer given a match. The first one (KW) sim-
ply returns the rest of sentence on the other side of
the verb. The second one (KW-SYN) is informed
by syntax: the answer is extracted from the subject
or object of the verb, depending on the question. If
the verb does not contain the expected argument,
the sentence is ignored.
TextRunner: TextRunner inputs text and outputs
relational triples in the form (R, A1, A2), where
R is the relation string, and A1, A2 the argument
strings. Given a triple and a question, we first
match their relation strings, and then match the
strings for the argument that is present in the ques-
tion. If both match, we return the other argument
string in the triple as an answer. We report results
when exact match is used (TR-EXACT), or when
the triple string can contain the question one as a
substring (TR-SUB).
RESOLVER: RESOLVER (Yates and Etzioni,
2009) inputs TextRunner triples and collectively
resolves coreferent relation and argument strings.
On the GENIA data, using the default parameters,
RESOLVER produces only a few trivial relation
clusters and no argument clusters. This is not sur-
prising, since RESOLVER assumes high redun-
dancy in the data, and will discard any strings with
fewer than 25 extractions. For a fair compari-
son, we also ran RESOLVER using all extractions,
and manually tuned the parameters based on eye-
balling of clustering quality. The best result was
obtained with 25 rounds of execution and with the
entity multiple set to 200 (the default is 30). To an-
swer questions, the only difference from TextRun-
ner is that a question string can match any string
in its cluster. As in TextRunner, we report results
for both exact match (RS-EXACT) and substring
(RS-SUB).
DIRT: The DIRT system inputs a path and returns
a set of similar paths. To use DIRT in question
answering, we queried it to obtain similar paths
for the relation of the question, and used these
paths while matching sentences. We first used
MINIPAR (Lin, 1998) to parse input text using
the same dependencies as DIRT. To determine a
match, we first check if the sentence contains the
question path or one of its DIRT paths. If so, and if
the available argument slot in the question is con-
tained in the one in the sentence, it is a match, and
we return the other argument slot from the sen-
tence if it is present. Ideally, a fair comparison will
require running DIRT on the GENIA text, but we
were not able to obtain the source code. We thus
resorted to using the latest DIRT database released
by the author, which contains paths extracted from
a large corpus with more than 1GB of text. This
puts DIRT in a very advantageous position com-
pared with other systems. In our experiments, we
used the top three similar paths, as including more
results in very low precision.
USP: We built a system for knowledge extrac-
tion and question answering on top of USP. It
generated Stanford dependencies (de Marneffe et
al., 2006) from the input text using the Stan-
ford parser, and then fed these to USP-Learn11,
which produced an MLN with learned weights
and the MAP semantic parses of the input sen-
tences. These MAP parses formed our knowledge
base (KB). To answer questions, the system first
parses the questions12 using USP-Parse with the
</bodyText>
<footnote confidence="0.5300435">
11α and β are set to −5 and −10.
12The question slot is replaced by a dummy word.
</footnote>
<page confidence="0.998699">
8
</page>
<tableCaption confidence="0.9905455">
Table 1: Comparison of question answering re-
sults on the GENIA dataset.
</tableCaption>
<table confidence="0.999768555555555">
# Total # Correct Accuracy
KW 150 67 45%
KW-SYN 87 67 77%
TR-EXACT 29 23 79%
TR-SUB 152 81 53%
RS-EXACT 53 24 45%
RS-SUB 196 81 41%
DIRT 159 94 59%
USP 334 295 88%
</table>
<bodyText confidence="0.999793636363636">
learned MLN, and then matches the question parse
to parses in the KB by testing subsumption (i.e., a
question parse matches a KB one iff the former is
subsumed by the latter). When a match occurs, our
system then looks for arguments of type in accor-
dance with the question. For example, if the ques-
tion is “What regulates MIP-1alpha?”, it searches
for the argument type of the relation that contains
the argument form “nsubj” for subject. If such an
argument exists for the relation part, it will be re-
turned as the answer.
</bodyText>
<subsectionHeader confidence="0.54132">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999991523809524">
Table 1 shows the results for all systems. USP
extracted the highest number of answers, almost
doubling that of the second highest (RS-SUB).
It obtained the highest accuracy at 88%, and
the number of correct answers it extracted is
three times that of the second highest system.
The informed baseline (KW-SYN) did surpris-
ingly well compared to systems other than USP, in
terms of accuracy and number of correct answers.
TextRunner achieved good accuracy when exact
match is used (TR-EXACT), but only obtained a
fraction of the answers compared to USP. With
substring match, its recall substantially improved,
but precision dropped more than 20 points. RE-
SOLVER improved the number of extracted an-
swers by sanctioning more matches based on the
clusters it generated. However, most of those ad-
ditional answers are incorrect due to wrong clus-
tering. DIRT obtained the second highest number
of correct answers, but its precision is quite low
because the similar paths contain many errors.
</bodyText>
<subsectionHeader confidence="0.989933">
6.4 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.99083732">
Manual inspection shows that USP is able to re-
solve many nontrivial syntactic variations with-
out user supervision. It consistently resolves the
syntactic difference between active and passive
voices. It successfully identifies many distinct ar-
gument forms that mean the same (e.g., “X stimu-
lates Y” ≈ “Y is stimulated with X”, “expression
of X” ≈ “X expression”). It also resolves many
nouns correctly and forms meaningful groups of
relations. Here are some sample clusters in core
forms:
{investigate, examine, evaluate, analyze, study,
assay}
{diminish, reduce, decrease, attenuate}
{synthesis, production, secretion, release}
{dramatically, substantially, significantly}
An example question-answer pair, together with
the source sentence, is shown below:
Q: What does IL-13 enhance?
A: The 12-lipoxygenase activity of murine
macrophages.
Sentence: The data presented here indicate
that (1) the 12-lipoxygenase activity of murine
macrophages is upregulated in vitro and in vivo
by IL-4 and/or IL-13, . . .
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999752">
This paper introduces the first unsupervised ap-
proach to learning semantic parsers. Our USP
system is based on Markov logic, and recursively
clusters expressions to abstract away syntactic
variations of the same meaning. We have suc-
cessfully applied USP to extracting a knowledge
base from biomedical text and answering ques-
tions based on it.
Directions for future work include: better han-
dling of antonyms, subsumption relations among
expressions, quantifier scoping, more complex
lambda forms, etc.; use of context and discourse
to aid expression clustering and semantic parsing;
more efficient learning and inference; application
to larger corpora; etc.
</bodyText>
<sectionHeader confidence="0.999" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997806">
We thank the anonymous reviewers for their comments. This
research was partly funded by ARO grant W911NF-08-1-
0242, DARPA contracts FA8750-05-2-0283, FA8750-07-D-
0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-
D030010, NSF grants IIS-0534881 and IIS-0803481, and
ONR grant N00014-08-1-0670. The views and conclusions
contained in this document are those of the authors and
should not be interpreted as necessarily representing the offi-
cial policies, either expressed or implied, of ARO, DARPA,
NSF, ONR, or the United States Government.
</bodyText>
<page confidence="0.997396">
9
</page>
<sectionHeader confidence="0.998336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923617021277">
G. Bakir, T. Hofmann, B. B. Sch¨olkopf, A. Smola,
B. Taskar, S. Vishwanathan, and (eds.). 2007. Pre-
dicting Structured Data. MIT Press, Cambridge,
MA.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the Twentieth International Joint Con-
ference on Artificial Intelligence, pages 2670–2676,
Hyderabad, India. AAAI Press.
Xavier Carreras and Luis Marquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role la-
beling. In Proceedings of the Eighth Conference on
Computational Natural Language Learning, pages
89–97, Boston, MA. ACL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation, pages 449–
454, Genoa, Italy. ELRA.
Ruifang Ge and Raymond J. Mooney. 2009. Learning
a compositional semantic parser using an existing
syntactic parser. In Proceedings of the Forty Sev-
enth Annual Meeting of the Association for Compu-
tational Linguistics, Singapore. ACL.
Lise Getoor and Ben Taskar, editors. 2007. Introduc-
tion to Statistical Relational Learning. MIT Press,
Cambridge, MA.
Pekka Kilpelainen. 1992. Tree Matching Prob-
lems with Applications to Structured Text databases.
Ph.D. Thesis, Department of Computer Science,
University of Helsinki.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun’ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:180–82.
Stanley Kok and Pedro Domingos. 2008. Extract-
ing semantic networks from text via relational clus-
tering. In Proceedings of the Nineteenth European
Conference on Machine Learning, pages 624–639,
Antwerp, Belgium. Springer.
Percy Liang and Dan Klein. 2008. Analyzing the er-
rors of unsupervised learning. In Proceedings of the
Forty Sixth Annual Meeting of the Association for
Computational Linguistics, pages 879–887, Colum-
bus, OH. ACL.
Dekang Lin and Patrick Pantel. 2001. DIRT - dis-
covery of inference rules from text. In Proceedings
of the Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 323–328, San Francisco, CA. ACM Press.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, Granada, Spain.
ELRA.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 982–991,
Honolulu, HI. ACL.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Proceedings of the Eighth International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 311–324, Mexico City,
Mexico. Springer.
Hoifung Poon and Pedro Domingos. 2006. Sound and
efficient inference with probabilistic and determin-
istic dependencies. In Proceedings of the Twenty
First National Conference on Artificial Intelligence,
pages 458–463, Boston, MA. AAAI Press.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
649–658, Honolulu, HI. ACL.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62:107–136.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34:255–296.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammers. In Proceedings of the Twenty First
Conference on Uncertainty in Artificial Intelligence,
pages 658–666, Edinburgh, Scotland. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 878–887, Prague, Czech. ACL.
</reference>
<page confidence="0.997764">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968917">
<title confidence="0.999746">Unsupervised Semantic Parsing</title>
<author confidence="0.995751">Hoifung Poon Pedro Domingos</author>
<affiliation confidence="0.9998015">Department of Computer Science and University of</affiliation>
<address confidence="0.999088">Seattle, WA 98195-2350,</address>
<abstract confidence="0.998487722222222">We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Predicting Structured Data.</title>
<date>2007</date>
<editor>G. Bakir, T. Hofmann, B. B. Sch¨olkopf, A. Smola, B. Taskar, S. Vishwanathan, and (eds.).</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7245" citStr="(2007)" startWordPosition="1133" endWordPosition="1133">k and Domingos, 2008). While these systems illustrate the promise of unsupervised methods, the semantic content they extract is nonetheless shallow and does not constitute the complete formal meaning that can be obtained by a semantic parser. Another issue is that existing approaches to semantic parsing learn to parse syntax and semantics together.1 The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder. For example, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introduce new combinators and new forms of candidate lexical entries. Ideally, we should leverage the enormous progress made in syntactic parsing and generate semantic parses directly from syntactic analysis. 2.2 Markov Logic In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-</context>
</contexts>
<marker>2007</marker>
<rawString>G. Bakir, T. Hofmann, B. B. Sch¨olkopf, A. Smola, B. Taskar, S. Vishwanathan, and (eds.). 2007. Predicting Structured Data. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2670--2676</pages>
<publisher>AAAI Press.</publisher>
<location>Hyderabad,</location>
<contexts>
<context position="1630" citStr="Banko et al., 2007" startWordPosition="235" endWordPosition="238">nd other forms of shallow semantic processing, which do not aim to produce complete formal meanings. Traditionally, semantic parsers were constructed manually, but this is too costly and brittle. Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007). However, they are supervised, and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality. Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006). Our USP system starts by clustering tokens of the same type, and then recursively clusters expressions whose subexpressions belong to the same clusters. Experiments on a biomedical corpus show that this approach is able to successfully translate syntactic variations into a logical representation of their common meaning (e.g., USP learns to map active and passive voice to the same logical form, etc.). This in turn allows it to correctl</context>
<context position="6456" citStr="Banko et al., 2007" startWordPosition="1008" endWordPosition="1011">ires nontrivial effort. For unrestricted text, the complexity and subjectivity of annotation render it essentially infeasible; even pre-specifying the target predicates and objects is very difficult. Therefore, to apply semantic parsing beyond limited domains, it is crucial to develop unsupervised methods that do not rely on labeled meanings. In the past, unsupervised approaches have been applied to some semantic tasks, but not to semantic parsing. For example, DIRT (Lin and Pantel, 2001) learns paraphrases of binary relations based on distributional similarity of their arguments; TextRunner (Banko et al., 2007) automatically extracts relational triples in open domains using a self-trained extractor; SNE applies relational clustering to generate a semantic network from TextRunner triples (Kok and Domingos, 2008). While these systems illustrate the promise of unsupervised methods, the semantic content they extract is nonetheless shallow and does not constitute the complete formal meaning that can be obtained by a semantic parser. Another issue is that existing approaches to semantic parsing learn to parse syntax and semantics together.1 The drawback is that the complexity in syntactic processing is co</context>
<context position="30931" citStr="Banko et al., 2007" startWordPosition="5179" endWordPosition="5182">my.cs.washington.edu/papers/poon09. 7 6.2 Systems Since USP is the first unsupervised semantic parser, conducting a meaningful comparison of it with other systems is not straightforward. Standard question-answering (QA) benchmarks do not provide the most appropriate comparison, because they tend to simultaneously emphasize other aspects not directly related to semantic parsing. Moreover, most state-of-the-art QA systems use supervised learning in their key components and/or require domain-specific engineering efforts. The closest available system to USP in aims and capabilities is TextRunner (Banko et al., 2007), and we compare with it. TextRunner is the state-of-the-art system for open-domain information extraction; its goal is to extract knowledge from text without using supervised labels. Given that a central challenge to semantic parsing is resolving syntactic variations of the same meaning, we also compare with RESOLVER (Yates and Etzioni, 2009), a state-of-the-art unsupervised system based on TextRunner for jointly resolving entities and relations, and DIRT (Lin and Pantel, 2001), which resolves paraphrases of binary relations. Finally, we also compared to an informed baseline based on keyword </context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence, pages 2670–2676, Hyderabad, India. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Luis Marquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth Conference on Computational Natural Language Learning,</booktitle>
<pages>89--97</pages>
<publisher>ACL.</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="1009" citStr="Carreras and Marquez, 2004" startWordPosition="141" endWordPosition="144"> recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task. 1 Introduction Semantic parsing maps text to formal meaning representations. This contrasts with semantic role labeling (Carreras and Marquez, 2004) and other forms of shallow semantic processing, which do not aim to produce complete formal meanings. Traditionally, semantic parsers were constructed manually, but this is too costly and brittle. Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007). However, they are supervised, and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality. Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction</context>
</contexts>
<marker>Carreras, Marquez, 2004</marker>
<rawString>Xavier Carreras and Luis Marquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of the Eighth Conference on Computational Natural Language Learning, pages 89–97, Boston, MA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy. ELRA.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, pages 449– 454, Genoa, Italy. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning a compositional semantic parser using an existing syntactic parser.</title>
<date>2009</date>
<booktitle>In Proceedings of the Forty Seventh Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="7976" citStr="Ge and Mooney (2009)" startWordPosition="1245" endWordPosition="1248">enormous progress made in syntactic parsing and generate semantic parses directly from syntactic analysis. 2.2 Markov Logic In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-order logic (Richardson and Domingos, 2006). Markov logic makes it 1The only exception that we are aware of is Ge and Mooney (2009). 2 possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. A Markov logic network (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the first-order clause that originated it. The probability of a state x in such a network is given by the log-linear model P(x) = Z exp (� 1 i wini(x)),</context>
</contexts>
<marker>Ge, Mooney, 2009</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2009. Learning a compositional semantic parser using an existing syntactic parser. In Proceedings of the Forty Seventh Annual Meeting of the Association for Computational Linguistics, Singapore. ACL.</rawString>
</citation>
<citation valid="true">
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<editor>Lise Getoor and Ben Taskar, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7245" citStr="(2007)" startWordPosition="1133" endWordPosition="1133">k and Domingos, 2008). While these systems illustrate the promise of unsupervised methods, the semantic content they extract is nonetheless shallow and does not constitute the complete formal meaning that can be obtained by a semantic parser. Another issue is that existing approaches to semantic parsing learn to parse syntax and semantics together.1 The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder. For example, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introduce new combinators and new forms of candidate lexical entries. Ideally, we should leverage the enormous progress made in syntactic parsing and generate semantic parses directly from syntactic analysis. 2.2 Markov Logic In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-</context>
</contexts>
<marker>2007</marker>
<rawString>Lise Getoor and Ben Taskar, editors. 2007. Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pekka Kilpelainen</author>
</authors>
<title>Tree Matching Problems with Applications to Structured Text databases.</title>
<date>1992</date>
<tech>Ph.D. Thesis,</tech>
<institution>Department of Computer Science, University of Helsinki.</institution>
<contexts>
<context position="21051" citStr="Kilpelainen, 1992" startWordPosition="3481" endWordPosition="3482">le depends on whether the core form of the new part Ax3.borders(n1) ∧ nsubj(n1, n2) ∧ Utah(n2) ∧ dobj(n1, x3) (i.e. borders(n1) ∧ nsubj(n1, n2) ∧ Utah(n2)) is contained in some lambda-form cluster. Algorithm 1 gives pseudo-code for our algorithm. Given part p, finding partitions that are Areducible from p and feasible can be done in time O(5T), where 5 is the size of the clustering in the number of core forms and T is the maximum number of atoms in a core form. We omit the proof here but point out that it is related to the unordered subtree matching problem which can be solved in linear time (Kilpelainen, 1992). Inverted indexes (e.g., from p to eligible core forms) are used to further improve the efficiency. For a new part p and a cluster that contains p’s core form, there are km ways of assigning p’s m arguments to the k argument types of the cluster. For larger k and m, this is very expensive. We therefore approximate it by assigning each argument to the best type, independent of other arguments. This algorithm is very efficient, and is used repeatedly in learning. 5 5 Learning The learning problem in USP is to maximize the log-likelihood of observing the QLFs obtained from the dependency trees, </context>
</contexts>
<marker>Kilpelainen, 1992</marker>
<rawString>Pekka Kilpelainen. 1992. Tree Matching Problems with Applications to Structured Text databases. Ph.D. Thesis, Department of Computer Science, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA corpus - a semantically annotated corpus for bio-textmining.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<pages>19--180</pages>
<contexts>
<context position="29372" citStr="Kim et al., 2003" startWordPosition="4940" endWordPosition="4943">ffers from EM in directly optimizing the likelihood instead of a lower bound. 6 Experiments 6.1 Task Evaluating unsupervised semantic parsers is difficult, because there is no predefined formal language or gold logical forms for the input sentences. Thus the best way to test them is by using them for the ultimate goal: answering questions based on the input corpus. In this paper, we applied USP to extracting knowledge from biomedical abstracts and evaluated its performance in answering a set of questions that simulate the information needs of biomedical researchers. We used the GENIA dataset (Kim et al., 2003) as the source for knowledge extraction. It contains 1999 PubMed abstracts and marks all mentions of biomedical entities according to the GENIA ontology, such as cell, protein, and DNA. As a first approximation to the questions a biomedical researcher might ask, we generated a set of two thousand questions on relations between entities. Sample questions are: “What regulates MIP1alpha?”, “What does anti-STAT 1 inhibit?”. To simulate the real information need, we sample the relations from the 100 most frequently used verbs (excluding the auxiliary verbs be, have, and do), and sample the entities</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii. 2003. GENIA corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19:180–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Pedro Domingos</author>
</authors>
<title>Extracting semantic networks from text via relational clustering.</title>
<date>2008</date>
<booktitle>In Proceedings of the Nineteenth European Conference on Machine Learning,</booktitle>
<pages>624--639</pages>
<publisher>Springer.</publisher>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="6660" citStr="Kok and Domingos, 2008" startWordPosition="1037" endWordPosition="1040">Therefore, to apply semantic parsing beyond limited domains, it is crucial to develop unsupervised methods that do not rely on labeled meanings. In the past, unsupervised approaches have been applied to some semantic tasks, but not to semantic parsing. For example, DIRT (Lin and Pantel, 2001) learns paraphrases of binary relations based on distributional similarity of their arguments; TextRunner (Banko et al., 2007) automatically extracts relational triples in open domains using a self-trained extractor; SNE applies relational clustering to generate a semantic network from TextRunner triples (Kok and Domingos, 2008). While these systems illustrate the promise of unsupervised methods, the semantic content they extract is nonetheless shallow and does not constitute the complete formal meaning that can be obtained by a semantic parser. Another issue is that existing approaches to semantic parsing learn to parse syntax and semantics together.1 The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder. For example, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introd</context>
</contexts>
<marker>Kok, Domingos, 2008</marker>
<rawString>Stanley Kok and Pedro Domingos. 2008. Extracting semantic networks from text via relational clustering. In Proceedings of the Nineteenth European Conference on Machine Learning, pages 624–639, Antwerp, Belgium. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>Analyzing the errors of unsupervised learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the Forty Sixth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>879--887</pages>
<publisher>ACL.</publisher>
<location>Columbus, OH.</location>
<contexts>
<context position="21994" citStr="Liang and Klein, 2008" startWordPosition="3649" endWordPosition="3652">y assigning each argument to the best type, independent of other arguments. This algorithm is very efficient, and is used repeatedly in learning. 5 5 Learning The learning problem in USP is to maximize the log-likelihood of observing the QLFs obtained from the dependency trees, denoted by Q, summing out the unobserved semantic parses: LB(Q) = log PB(Q) = log EL PB(Q, L) Here, L are the semantic parses, 0 are the MLN parameters, and PB(Q, L) are the completion likelihoods. A serious challenge in unsupervised learning is the identifiability problem (i.e., the optimal parameters are not unique) (Liang and Klein, 2008). This problem is particularly severe for log-linear models with hard constraints, which are common in MLNs. For example, in our USP MLN, conditioned on the fact that p E c, there is exactly one value of f that can satisfy the formula p E c n Form(p, f), and if we add some constant number to the weights of p E c n Form(p, f) for all f, the probability distribution stays the same.6 The learner can be easily confused by the infinitely many optima, especially in the early stages. To address this problem, we impose local normalization constraints on specific groups of formulas that are mutually ex</context>
</contexts>
<marker>Liang, Klein, 2008</marker>
<rawString>Percy Liang and Dan Klein. 2008. Analyzing the errors of unsupervised learning. In Proceedings of the Forty Sixth Annual Meeting of the Association for Computational Linguistics, pages 879–887, Columbus, OH. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<publisher>ACM Press.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="1585" citStr="Lin and Pantel, 2001" startWordPosition="229" endWordPosition="232">ic role labeling (Carreras and Marquez, 2004) and other forms of shallow semantic processing, which do not aim to produce complete formal meanings. Traditionally, semantic parsers were constructed manually, but this is too costly and brittle. Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007). However, they are supervised, and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality. Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006). Our USP system starts by clustering tokens of the same type, and then recursively clusters expressions whose subexpressions belong to the same clusters. Experiments on a biomedical corpus show that this approach is able to successfully translate syntactic variations into a logical representation of their common meaning (e.g., USP learns to map active and passive voice to the same logical fo</context>
<context position="6330" citStr="Lin and Pantel, 2001" startWordPosition="989" endWordPosition="993">quire meaning annotations for example sentences. Even in a restricted domain, doing this consistently and with high quality requires nontrivial effort. For unrestricted text, the complexity and subjectivity of annotation render it essentially infeasible; even pre-specifying the target predicates and objects is very difficult. Therefore, to apply semantic parsing beyond limited domains, it is crucial to develop unsupervised methods that do not rely on labeled meanings. In the past, unsupervised approaches have been applied to some semantic tasks, but not to semantic parsing. For example, DIRT (Lin and Pantel, 2001) learns paraphrases of binary relations based on distributional similarity of their arguments; TextRunner (Banko et al., 2007) automatically extracts relational triples in open domains using a self-trained extractor; SNE applies relational clustering to generate a semantic network from TextRunner triples (Kok and Domingos, 2008). While these systems illustrate the promise of unsupervised methods, the semantic content they extract is nonetheless shallow and does not constitute the complete formal meaning that can be obtained by a semantic parser. Another issue is that existing approaches to sem</context>
<context position="31414" citStr="Lin and Pantel, 2001" startWordPosition="5254" endWordPosition="5257">quire domain-specific engineering efforts. The closest available system to USP in aims and capabilities is TextRunner (Banko et al., 2007), and we compare with it. TextRunner is the state-of-the-art system for open-domain information extraction; its goal is to extract knowledge from text without using supervised labels. Given that a central challenge to semantic parsing is resolving syntactic variations of the same meaning, we also compare with RESOLVER (Yates and Etzioni, 2009), a state-of-the-art unsupervised system based on TextRunner for jointly resolving entities and relations, and DIRT (Lin and Pantel, 2001), which resolves paraphrases of binary relations. Finally, we also compared to an informed baseline based on keyword matching. Keyword: We consider a baseline system based on keyword matching. The question substring containing the verb and the available argument is directly matched with the input text, ignoring case and morphology. We consider two ways to derive the answer given a match. The first one (KW) simply returns the rest of sentence on the other side of the verb. The second one (KW-SYN) is informed by syntax: the answer is extracted from the subject or object of the verb, depending on</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of inference rules from text. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 323–328, San Francisco, CA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation of Parsing Systems,</booktitle>
<publisher>ELRA.</publisher>
<location>Granada,</location>
<contexts>
<context position="33762" citStr="Lin, 1998" startWordPosition="5652" endWordPosition="5653"> clustering quality. The best result was obtained with 25 rounds of execution and with the entity multiple set to 200 (the default is 30). To answer questions, the only difference from TextRunner is that a question string can match any string in its cluster. As in TextRunner, we report results for both exact match (RS-EXACT) and substring (RS-SUB). DIRT: The DIRT system inputs a path and returns a set of similar paths. To use DIRT in question answering, we queried it to obtain similar paths for the relation of the question, and used these paths while matching sentences. We first used MINIPAR (Lin, 1998) to parse input text using the same dependencies as DIRT. To determine a match, we first check if the sentence contains the question path or one of its DIRT paths. If so, and if the available argument slot in the question is contained in the one in the sentence, it is a match, and we return the other argument slot from the sentence if it is present. Ideally, a fair comparison will require running DIRT on the GENIA text, but we were not able to obtain the source code. We thus resorted to using the latest DIRT database released by the author, which contains paths extracted from a large corpus wi</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In Proceedings of the Workshop on the Evaluation of Parsing Systems, Granada, Spain. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word-pair antonymy.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>982--991</pages>
<publisher>ACL.</publisher>
<location>Honolulu, HI.</location>
<contexts>
<context position="17801" citStr="Mohammad et al., 2008" startWordPosition="2915" endWordPosition="2918">r, and the others model the mixtures of argument forms, argument types, and argument numbers, respectively, given the argument type. To encourage clustering and avoid overfitting, we impose an exponential prior with weight α on the number of parameters.4 The MLN above has one problem: it often clusters expressions that are semantically opposite. For example, it clusters antonyms like “elderly/young”, “mature/immature”. This issue also occurs in other semantic-processing systems (e.g., DIRT). In general, this is a difficult open problem that only recently has started to receive some attention (Mohammad et al., 2008). Resolving this is not the focus of this paper, but we describe a general heuristic for fixing this problem. We observe that the problem stems from the lack of negative features for discovering meanings in contrast. In natural languages, parallel structures like conjunctions are one such feature.5 We thus introduce an exponential prior with weight β on the number of conjunctions where the two conjunctive parts are assigned to the same cluster. To detect conjunction, we simply used the Stanford dependencies that begin with “conj”. This proves very effective, fixing the majority of the errors i</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, 2008</marker>
<rawString>Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008. Computing word-pair antonymy. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 982–991, Honolulu, HI. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eighth International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>311--324</pages>
<publisher>Springer.</publisher>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="1321" citStr="Mooney, 2007" startWordPosition="189" endWordPosition="190">dical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task. 1 Introduction Semantic parsing maps text to formal meaning representations. This contrasts with semantic role labeling (Carreras and Marquez, 2004) and other forms of shallow semantic processing, which do not aim to produce complete formal meanings. Traditionally, semantic parsers were constructed manually, but this is too costly and brittle. Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007). However, they are supervised, and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality. Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006). Our USP system starts by clustering tokens of the same type, and then recursively clusters expressions whose subexpressions belon</context>
<context position="4959" citStr="Mooney, 2007" startWordPosition="769" endWordPosition="770">ning simplifies to Ax.borders(x, Idaho) by the A-reduction rule, which substitutes the argument for a variable in a functional application. A major challenge to semantic parsing is syntactic variations of the same meaning, which abound in natural languages. For example, the aforementioned sentence can be rephrased as “Utah is next to Idaho,”“Utah shares a border with Idaho,” etc. Manually encoding all these variations into the grammar is tedious and error-prone. Supervised semantic parsing addresses this issue by learning to construct the grammar automatically from sample meaning annotations (Mooney, 2007). Existing approaches differ in the meaning representation languages they use and the amount of annotation required. In the approach of Zettlemoyer and Collins (2005), the training data consists of sentences paired with their meanings in lambda form. A probabilistic combinatory categorial grammar (PCCG) is learned using a loglinear model, where the probability of the final logical form L and meaning-derivation tree T conditioned on the sentence 5 is P(L, T |5) = Z exp (� 1 i wifi(L, T, 5)). Here Z is the normalization constant and fi are the feature functions with weights wi. Candidate lexical</context>
</contexts>
<marker>Mooney, 2007</marker>
<rawString>Raymond J. Mooney. 2007. Learning for semantic parsing. In Proceedings of the Eighth International Conference on Computational Linguistics and Intelligent Text Processing, pages 311–324, Mexico City, Mexico. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Sound and efficient inference with probabilistic and deterministic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of the Twenty First National Conference on Artificial Intelligence,</booktitle>
<pages>458--463</pages>
<publisher>AAAI Press.</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="24652" citStr="Poon and Domingos, 2006" startWordPosition="4123" endWordPosition="4126">ge clusters with the same core form Agenda +— 0 repeat for all candidate operations O do Score O by log-likelihood improvement if score is above a threshold then Add O to agenda end if end for Execute the highest scoring operation O∗ in the agenda Regenerate MAP parses for affected QLFs and update agenda and candidate operations until agenda is empty return the MLN with learned weights and the semantic parses Another major challenge in USP learning is the summation in the likelihood, which is over all possible semantic parses for a given dependency tree. Even an efficient sampler like MC-SAT (Poon and Domingos, 2006), as used in Poon &amp; Domingos (2008), would have a hard time generating accurate estimates within a reasonable amount of time. On the other hand, as already noted in the previous section, the lambda-form distribution is generally sparse. Large lambda-forms are rare, as they correspond to complex expressions that are often decomposable into smaller ones. Moreover, while ambiguities are present at the lexical level, they quickly diminish when more words are present. Therefore, a lambda form can usually only belong to a small number of clusters, if not a unique one. We thus simplify the problem by</context>
</contexts>
<marker>Poon, Domingos, 2006</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2006. Sound and efficient inference with probabilistic and deterministic dependencies. In Proceedings of the Twenty First National Conference on Artificial Intelligence, pages 458–463, Boston, MA. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>649--658</pages>
<publisher>ACL.</publisher>
<location>Honolulu, HI.</location>
<contexts>
<context position="8167" citStr="Poon and Domingos, 2008" startWordPosition="1271" endWordPosition="1274">cts, and recent work in statistical relational learning (Getoor and Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-order logic (Richardson and Domingos, 2006). Markov logic makes it 1The only exception that we are aware of is Ge and Mooney (2009). 2 possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. A Markov logic network (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the first-order clause that originated it. The probability of a state x in such a network is given by the log-linear model P(x) = Z exp (� 1 i wini(x)), where Z is a normalization constant, wi is the weight of the ith formula, and ni is the number of satisfied groundings. 3 Unsupervised Semantic Parsing with Markov Logic Unsupervised semanti</context>
<context position="24687" citStr="Poon &amp; Domingos (2008)" startWordPosition="4130" endWordPosition="4133">genda +— 0 repeat for all candidate operations O do Score O by log-likelihood improvement if score is above a threshold then Add O to agenda end if end for Execute the highest scoring operation O∗ in the agenda Regenerate MAP parses for affected QLFs and update agenda and candidate operations until agenda is empty return the MLN with learned weights and the semantic parses Another major challenge in USP learning is the summation in the likelihood, which is over all possible semantic parses for a given dependency tree. Even an efficient sampler like MC-SAT (Poon and Domingos, 2006), as used in Poon &amp; Domingos (2008), would have a hard time generating accurate estimates within a reasonable amount of time. On the other hand, as already noted in the previous section, the lambda-form distribution is generally sparse. Large lambda-forms are rare, as they correspond to complex expressions that are often decomposable into smaller ones. Moreover, while ambiguities are present at the lexical level, they quickly diminish when more words are present. Therefore, a lambda form can usually only belong to a small number of clusters, if not a unique one. We thus simplify the problem by approximating the sum with the mod</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov logic. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 649–658, Honolulu, HI. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="1790" citStr="Richardson and Domingos, 2006" startWordPosition="259" endWordPosition="262"> manually, but this is too costly and brittle. Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007). However, they are supervised, and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality. Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006). Our USP system starts by clustering tokens of the same type, and then recursively clusters expressions whose subexpressions belong to the same clusters. Experiments on a biomedical corpus show that this approach is able to successfully translate syntactic variations into a logical representation of their common meaning (e.g., USP learns to map active and passive voice to the same logical form, etc.). This in turn allows it to correctly answer many more questions than systems based on TextRunner (Banko et al., 2007) and DIRT (Lin and Pantel, 2001). We begin by reviewing the necessary backgrou</context>
<context position="7888" citStr="Richardson and Domingos, 2006" startWordPosition="1228" endWordPosition="1231">oduce new combinators and new forms of candidate lexical entries. Ideally, we should leverage the enormous progress made in syntactic parsing and generate semantic parses directly from syntactic analysis. 2.2 Markov Logic In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-order logic (Richardson and Domingos, 2006). Markov logic makes it 1The only exception that we are aware of is Ge and Mooney (2009). 2 possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. A Markov logic network (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the first-order clause that originated it. The probability of a </context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov logic networks. Machine Learning, 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>34--255</pages>
<contexts>
<context position="31276" citStr="Yates and Etzioni, 2009" startWordPosition="5232" endWordPosition="5236">ot directly related to semantic parsing. Moreover, most state-of-the-art QA systems use supervised learning in their key components and/or require domain-specific engineering efforts. The closest available system to USP in aims and capabilities is TextRunner (Banko et al., 2007), and we compare with it. TextRunner is the state-of-the-art system for open-domain information extraction; its goal is to extract knowledge from text without using supervised labels. Given that a central challenge to semantic parsing is resolving syntactic variations of the same meaning, we also compare with RESOLVER (Yates and Etzioni, 2009), a state-of-the-art unsupervised system based on TextRunner for jointly resolving entities and relations, and DIRT (Lin and Pantel, 2001), which resolves paraphrases of binary relations. Finally, we also compared to an informed baseline based on keyword matching. Keyword: We consider a baseline system based on keyword matching. The question substring containing the verb and the available argument is directly matched with the input text, ignoring case and morphology. We consider two ways to derive the answer given a match. The first one (KW) simply returns the rest of sentence on the other sid</context>
<context position="32667" citStr="Yates and Etzioni, 2009" startWordPosition="5467" endWordPosition="5470">es not contain the expected argument, the sentence is ignored. TextRunner: TextRunner inputs text and outputs relational triples in the form (R, A1, A2), where R is the relation string, and A1, A2 the argument strings. Given a triple and a question, we first match their relation strings, and then match the strings for the argument that is present in the question. If both match, we return the other argument string in the triple as an answer. We report results when exact match is used (TR-EXACT), or when the triple string can contain the question one as a substring (TR-SUB). RESOLVER: RESOLVER (Yates and Etzioni, 2009) inputs TextRunner triples and collectively resolves coreferent relation and argument strings. On the GENIA data, using the default parameters, RESOLVER produces only a few trivial relation clusters and no argument clusters. This is not surprising, since RESOLVER assumes high redundancy in the data, and will discard any strings with fewer than 25 extractions. For a fair comparison, we also ran RESOLVER using all extractions, and manually tuned the parameters based on eyeballing of clustering quality. The best result was obtained with 25 rounds of execution and with the entity multiple set to 2</context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Journal of Artificial Intelligence Research, 34:255–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammers.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twenty First Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>658--666</pages>
<publisher>AUAI Press.</publisher>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1306" citStr="Zettlemoyer and Collins, 2005" startWordPosition="185" endWordPosition="188">act a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task. 1 Introduction Semantic parsing maps text to formal meaning representations. This contrasts with semantic role labeling (Carreras and Marquez, 2004) and other forms of shallow semantic processing, which do not aim to produce complete formal meanings. Traditionally, semantic parsers were constructed manually, but this is too costly and brittle. Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007). However, they are supervised, and providing the target logical form for each sentence is costly and difficult to do consistently and with high quality. Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006). Our USP system starts by clustering tokens of the same type, and then recursively clusters expressions whose subex</context>
<context position="5125" citStr="Zettlemoyer and Collins (2005)" startWordPosition="791" endWordPosition="795">hallenge to semantic parsing is syntactic variations of the same meaning, which abound in natural languages. For example, the aforementioned sentence can be rephrased as “Utah is next to Idaho,”“Utah shares a border with Idaho,” etc. Manually encoding all these variations into the grammar is tedious and error-prone. Supervised semantic parsing addresses this issue by learning to construct the grammar automatically from sample meaning annotations (Mooney, 2007). Existing approaches differ in the meaning representation languages they use and the amount of annotation required. In the approach of Zettlemoyer and Collins (2005), the training data consists of sentences paired with their meanings in lambda form. A probabilistic combinatory categorial grammar (PCCG) is learned using a loglinear model, where the probability of the final logical form L and meaning-derivation tree T conditioned on the sentence 5 is P(L, T |5) = Z exp (� 1 i wifi(L, T, 5)). Here Z is the normalization constant and fi are the feature functions with weights wi. Candidate lexical entries are generated by a domain-specific procedure based on the target logical forms. The major limitation of supervised approaches is that they require meaning an</context>
<context position="12811" citStr="Zettlemoyer and Collins (2005)" startWordPosition="2048" endWordPosition="2051">become borders(n1) ∧ Utah(n2) ∧ Idaho(n3) ∧ nsubj(n1, n2) ∧ dobj(n1, n3). 3.2 Lambda-Form Clusters and Semantic Parsing in USP Given a QLF, a relation or an object is represented by the conjunction of a subset of the atoms. For example, the next-to relation is represented by borders(n1) ∧ nsubj(n1, n2) ∧ dobj(n1, n3), and the states of Utah and Idaho are represented 3 by Utah(n2) and Idaho(n3). The meaning composition of two sub-formulas is simply their conjunction. This allows the maximum flexibility in learning. In particular, lexical entries are no longer limited to be adjacent words as in Zettlemoyer and Collins (2005), but can be arbitrary fragments in a dependency tree. For every sub-formula F, we define a corresponding lambda form that can be derived by replacing every Skolem constant ni that does not appear in any unary atom in F with a unique lambda variable xi. Intuitively, such constants represent objects introduced somewhere else (by the unary atoms containing them), and correspond to the arguments of the relation represented by F. For example, the lambda form for borders(n1) ∧ nsubj(n1, n2) ∧ dobj(n1, n3) is Ax2Ax3. borders(n1) ∧ nsubj(n1, x2) ∧ dobj(n1, x3). Conceptually, a lambda-form cluster is </context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammers. In Proceedings of the Twenty First Conference on Uncertainty in Artificial Intelligence, pages 658–666, Edinburgh, Scotland. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>878--887</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech.</location>
<contexts>
<context position="7245" citStr="Zettlemoyer and Collins (2007)" startWordPosition="1129" endWordPosition="1133">m TextRunner triples (Kok and Domingos, 2008). While these systems illustrate the promise of unsupervised methods, the semantic content they extract is nonetheless shallow and does not constitute the complete formal meaning that can be obtained by a semantic parser. Another issue is that existing approaches to semantic parsing learn to parse syntax and semantics together.1 The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder. For example, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introduce new combinators and new forms of candidate lexical entries. Ideally, we should leverage the enormous progress made in syntactic parsing and generate semantic parses directly from syntactic analysis. 2.2 Markov Logic In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 878–887, Prague, Czech. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>