<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016713">
<title confidence="0.9980665">
Natural Language Generation as Planning Under Uncertainty for Spoken
Dialogue Systems
</title>
<author confidence="0.998751">
Verena Rieser Oliver Lemon
</author>
<affiliation confidence="0.9997075">
School of Informatics School of Informatics
University of Edinburgh University of Edinburgh
</affiliation>
<email confidence="0.995874">
vrieser@inf.ed.ac.uk olemon@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999802">
We present and evaluate a new model for
Natural Language Generation (NLG) in
Spoken Dialogue Systems, based on statis-
tical planning, given noisy feedback from
the current generation context (e.g. a user
and a surface realiser). We study its use in
a standard NLG problem: how to present
information (in this case a set of search re-
sults) to users, given the complex trade-
offs between utterance length, amount of
information conveyed, and cognitive load.
We set these trade-offs by analysing exist-
ing MATCH data. We then train a NLG pol-
icy using Reinforcement Learning (RL),
which adapts its behaviour to noisy feed-
back from the current generation context.
This policy is compared to several base-
lines derived from previous work in this
area. The learned policy significantly out-
performs all the prior approaches.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999877220338983">
Natural language allows us to achieve the same
communicative goal (“what to say”) using many
different expressions (“how to say it”). In a Spo-
ken Dialogue System (SDS), an abstract commu-
nicative goal (CG) can be generated in many dif-
ferent ways. For example, the CG to present
database results to the user can be realized as a
summary (Polifroni and Walker, 2008; Demberg
and Moore, 2006), or by comparing items (Walker
et al., 2004), or by picking one item and recom-
mending it to the user (Young et al., 2007).
Previous work has shown that it is useful to
adapt the generated output to certain features of
the dialogue context, for example user prefer-
ences, e.g. (Walker et al., 2004; Demberg and
Moore, 2006), user knowledge, e.g. (Janarthanam
and Lemon, 2008), or predicted TTS quality, e.g.
(Nakatsu and White, 2006).
In extending this previous work we treat NLG
as a statistical sequential planning problem, anal-
ogously to current statistical approaches to Dia-
logue Management (DM), e.g. (Singh et al., 2002;
Henderson et al., 2008; Rieser and Lemon, 2008a)
and “conversation as action under uncertainty”
(Paek and Horvitz, 2000). In NLG we have
similar trade-offs and unpredictability as in DM,
and in some systems the content planning and DM
tasks are overlapping. Clearly, very long system
utterances with many actions in them are to be
avoided, because users may become confused or
impatient, but each individual NLG action will
convey some (potentially) useful information to
the user. There is therefore an optimization prob-
lem to be solved. Moreover, the user judgements
or next (most likely) action after each NLG action
are unpredictable, and the behaviour of the surface
realizer may also be variable (see Section 6.2).
NLG could therefore fruitfully be approached
as a sequential statistical planning task, where
there are trade-offs and decisions to make, such as
whether to choose another NLG action (and which
one to choose) or to instead stop generating. Re-
inforcement Learning (RL) allows us to optimize
such trade-offs in the presence of uncertainty, i.e.
the chances of achieving a better state, while en-
gaging in the risk of choosing another action.
In this paper we present and evaluate a new
model for NLG in Spoken Dialogue Systems as
planning under uncertainty. In Section 2 we argue
for applying RL to NLG problems and explain the
overall framework. In Section 3 we discuss chal-
lenges for NLG for Information Presentation. In
Section 4 we present results from our analysis of
the MATCH corpus (Walker et al., 2004). In Sec-
tion 5 we present a detailed example of our pro-
posed NLG method. In Section 6 we report on
experimental results using this framework for ex-
ploring Information Presentation policies. In Sec-
tion 7 we conclude and discuss future directions.
</bodyText>
<note confidence="0.9230075">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999299">
683
</page>
<bodyText confidence="0.989519421052632">
2 NLG as planning under uncertainty
We adopt the general framework of NLG as plan-
ning under uncertainty (see (Lemon, 2008) for the
initial version of this approach). Some aspects of
NLG have been treated as planning, e.g. (Koller
and Stone, 2007; Koller and Petrick, 2008), but
never before as statistical planning.
NLG actions take place in a stochastic environ-
ment, for example consisting of a user, a realizer,
and a TTS system, where the individual NLG ac-
tions have uncertain effects on the environment.
For example, presenting differing numbers of at-
tributes to the user, and making the user more or
less likely to choose an item, as shown by (Rieser
and Lemon, 2008b) for multimodal interaction.
Most SDS employ fixed template-based gener-
ation. Our goal, however, is to employ a stochas-
tic realizer for SDS, see for example (Stent et al.,
2004). This will introduce additional noise, which
higher level NLG decisions will need to react
to. In our framework, the NLG component must
achieve a high-level Communicative Goal from
the Dialogue Manager (e.g. to present a number
of items) through planning a sequence of lower-
level generation steps or actions, for example first
to summarize all the items and then to recommend
the highest ranking one. Each such action has un-
predictable effects due to the stochastic realizer.
For example the realizer might employ 6 attributes
when recommending item i4, but it might use only
2 (e.g. price and cuisine for restaurants), depend-
ing on its own processing constraints (see e.g. the
realizer used to collect the MATCH project data).
Likewise, the user may be likely to choose an item
after hearing a summary, or they may wish to hear
more. Generating appropriate language in context
(e.g. attributes presented so far) thus has the fol-
lowing important features in general:
</bodyText>
<listItem confidence="0.9996226">
• NLG is goal driven behaviour
• NLG must plan a sequence of actions
• each action changes the environment state or
context
• the effect of each action is uncertain.
</listItem>
<bodyText confidence="0.999716">
These facts make it clear that the problem of
planning how to generate an utterance falls nat-
urally into the class of statistical planning prob-
lems, rather than rule-based approaches such as
(Moore et al., 2004; Walker et al., 2004), or super-
vised learning as explored in previous work, such
as classifier learning and re-ranking, e.g. (Stent et
al., 2004; Oh and Rudnicky, 2002). Supervised
approaches involve the ranking of a set of com-
pleted plans/utterances and as such cannot adapt
online to the context or the user. Reinforcement
Learning (RL) provides a principled, data-driven
optimisation framework for our type of planning
problem (Sutton and Barto, 1998).
</bodyText>
<sectionHeader confidence="0.98903" genericHeader="method">
3 The Information Presentation Problem
</sectionHeader>
<bodyText confidence="0.999987058823529">
We will tackle the well-studied problem of Infor-
mation Presentation in NLG to show the benefits
of this approach. The task here is to find the best
way to present a set of search results to a user
(e.g. some restaurants meeting a certain set of con-
straints). This is a task common to much prior
work in NLG, e.g. (Walker et al., 2004; Demberg
and Moore, 2006; Polifroni and Walker, 2008).
In this problem, there there are many decisions
available for exploration. For instance, which pre-
sentation strategy to apply (NLG strategy selec-
tion), how many attributes of each item to present
(attribute selection), how to rank the items and at-
tributes according to different models of user pref-
erences (attribute ordering), how many (specific)
items to tell them about (conciseness), how many
sentences to use when doing so (syntactic plan-
ning), and which words to use (lexical choice) etc.
All these parameters (and potentially many more)
can be varied, and ideally, jointly optimised based
on user judgements.
We had two corpora available to study some of
the regions of this decision space. We utilise the
MATCH corpus (Walker et al., 2004) to extract an
evaluation function (also known as ”reward func-
tion”) for RL. Furthermore, we utilise the SPaRKy
corpus (Stent et al., 2004) to build a high quality
stochastic realizer. Both corpora contain data from
“overhearer” experiments targeted to Information
Presentation in dialogues in the restaurant domain.
While we are ultimately interested in how hearers
engaged in dialogues judge different Information
Presentations, results from overhearers are still di-
rectly relevant to the task.
</bodyText>
<sectionHeader confidence="0.992303" genericHeader="method">
4 MATCH corpus analysis
</sectionHeader>
<bodyText confidence="0.9998395">
The MATCH project made two data sets available,
see (Stent et al., 2002) and (Whittaker et al., 2003),
which we combine to define an evaluation function
for different Information Presentation strategies.
</bodyText>
<page confidence="0.99676">
684
</page>
<table confidence="0.998664307692308">
strategy example av.#attr av.#sentence
SUMMARY “The 4 restaurants differ in food quality, and cost.” 2.07±.63 1.56±.5
(#attr = 2, #sentence = 1)
COMPARE “Among the selected restaurants, the following offer 3.2±1.5 5.5±3.11
exceptional overall value. Aureole’s price is 71 dol-
lars. It has superb food quality, superb service and
superb decor. Daniel’s price is 82 dollars. It has su-
perb food quality, superb service and superb decor.”
(#attr = 4, #sentence = 5)
RECOMMEND “Le Madeleine has the best overall value among the 2.4±.7 3.5±.53
selected restaurants. Le Madeleine’s price is 40 dol-
lars and It has very good food quality. It’s in Mid-
town West. ” (#attr = 3, #sentence = 3)
</table>
<tableCaption confidence="0.9983">
Table 1: NLG strategies present in the MATCH corpus with average no. attributes and sentences as found
</tableCaption>
<bodyText confidence="0.988070828571428">
in the data.
The first data set, see (Stent et al., 2002), com-
prises 1024 ratings by 16 subjects (where we only
use the speech-based half, n = 512) on the follow-
ing presentation strategies: RECOMMEND, COM-
PARE, SUMMARY. These strategies are realized
using templates as in Table 2, and varying num-
bers of attributes. In this study the users rate the
individual presentation strategies as significantly
different (F(2) = 1361,p &lt; .001). We find that
SUMMARY is rated significantly worse (p = .05
with Bonferroni correction) than RECOMMEND
and COMPARE, which are rated as equally good.
This suggests that one should never generate
a SUMMARY. However, SUMMARY has different
qualities from COMPARE and RECOMMEND, as
it gives users a general overview of the domain,
and probably helps the user to feel more confi-
dent when choosing an item, especially when they
are unfamiliar with the domain, as shown by (Po-
lifroni and Walker, 2008).
In order to further describe the strategies, we ex-
tracted different surface features as present in the
data (e.g. number of attributes realised, number of
sentences, number of words, number of database
items talked about, etc.) and performed a step-
wise linear regression to find the features which
were important to the overhearers (following the
PARADISE framework (Walker et al., 2000)). We
discovered a trade-off between the length of the ut-
terance (#sentence) and the number of attributes
realised (#attr), i.e. its informativeness, where
overhearers like to hear as many attributes as pos-
sible in the most concise way, as indicated by
the regression model shown in Equation 1 (R2 =
</bodyText>
<equation confidence="0.847853">
.34). 1
score = .775 x #attr + (−.301) x #sentence;
(1)
</equation>
<bodyText confidence="0.998320428571429">
The second MATCH data set, see (Whittaker et
al., 2003), comprises 1224 ratings by 17 subjects
on the NLG strategies RECOMMEND and COM-
PARE. The strategies realise varying numbers of
attributes according to different “conciseness” val-
ues: concise (1 or 2 attributes), average (3
or 4), and verbose (4,5, or 6). Overhearers
rate all conciseness levels as significantly different
(F(2) = 198.3,p &lt; .001), with verbose rated
highest and concise rated lowest, supporting
our findings in the first data set. However, the rela-
tion between number of attributes and user ratings
is not strictly linear: ratings drop for #attr = 6.
This suggests that there is an upper limit on how
many attributes users like to hear. We expect this
to be especially true for real users engaged in ac-
tual dialogue interaction, see (Winterboer et al.,
2007). We therefore include “cognitive load” as a
variable when training the policy (see Section 6).
In addition to the trade-off between length and
informativeness for single NLG strategies, we are
interested whether this trade-off will also hold for
generating sequences of NLG actions. (Whittaker
et al., 2002), for example, generate a combined
strategy where first a SUMMARY is used to de-
scribe the retrieved subset and then they RECOM-
MEND one specific item/restaurant. For example
“The 4 restaurants are all French, but differ in
</bodyText>
<footnote confidence="0.9764445">
1For comparison: (Walker et al., 2000) report on R2 be-
tween .4 and .5 on a slightly lager data set.
</footnote>
<page confidence="0.993406">
685
</page>
<figureCaption confidence="0.999795">
Figure 1: Possible NLG policies (X=stop generation)
</figureCaption>
<bodyText confidence="0.987611">
food quality, and cost. Le Madeleine has the best
overall value among the selected restaurants. Le
Madeleine’s price is 40 dollars and It has very
good food quality. It’s in Midtown West.”
We therefore extend the set of possible strate-
gies present in the data for exploration: we allow
ordered combinations of the strategies, assuming
that only COMPARE or RECOMMEND can follow a
SUMMARY, and that only RECOMMEND can fol-
low COMPARE, resulting in 7 possible actions:
</bodyText>
<listItem confidence="0.997475714285714">
1. RECOMMEND
2. COMPARE
3. SUMMARY
4. COMPARE+RECOMMEND
5. SUMMARY+RECOMMEND
6. SUMMARY+COMPARE
7. SUMMARY+COMPARE+RECOMMEND
</listItem>
<bodyText confidence="0.999643888888889">
We then analytically solved the regression
model in Equation 1 for the 7 possible strategies
using average values from the MATCH data. This is
solved by a system of linear inequalities. Accord-
ing to this model, the best ranking strategy is to
do all the presentation strategies in one sequence,
i.e. SUMMARY+COMPARE+RECOMMEND. How-
ever, this analytic solution assumes a “one-shot”
generation strategy where there is no intermediate
feedback from the environment: users are simply
static overhearers (they cannot “barge-in” for ex-
ample), there is no variation in the behaviour of the
surface realizer, i.e. one would use fixed templates
as in MATCH, and the user has unlimited cogni-
tive capabilities. These assumptions are not real-
istic, and must be relaxed. In the next Section we
describe a worked through example of the overall
framework.
</bodyText>
<sectionHeader confidence="0.99324" genericHeader="method">
5 Method: the RL-NLG model
</sectionHeader>
<bodyText confidence="0.999950515151515">
For the reasons discussed above, we treat the
NLG module as a statistical planner, operat-
ing in a stochastic environment, and optimise
it using Reinforcement Learning. The in-
put to the module is a Communicative Goal
supplied by the Dialogue Manager. The CG
consists of a Dialogue Act to be generated,
for example present items(i1, i2, i5, i8),
and a System Goal (SysGoal) which is
the desired user reaction, e.g. to make the
user choose one of the presented items
(user choose one of(i1, i2, i5, i8)). The
RL-NLG module must plan a sequence of lower-
level NLG actions that achieve the goal (at lowest
cost) in the current context. The context consists
of a user (who may remain silent, supply more
constraints, choose an item, or quit), and variation
from the sentence realizer described above.
Now let us walk-through one simple ut-
terance plan as carried out by this model,
as shown in Table 2. Here, we start
with the CG present items(i1, i2, i5, i8)&amp;
user choose one of(i1, i2, i5, i8) from the
system’s DM. This initialises the NLG state (init).
The policy chooses the action SUMMARY and this
transitions us to state s1, where we observe that
4 attributes and 1 sentence have been generated,
and the user is predicted to remain silent. In this
state, the current NLG policy is to RECOMMEND
the top ranked item (i5, for this user), which takes
us to state s2, where 8 attributes have been gener-
ated in a total of 4 sentences, and the user chooses
an item. The policy holds that in states like s2 the
</bodyText>
<page confidence="0.996587">
686
</page>
<figureCaption confidence="0.987792">
Figure 2: Example RL-NLG action sequence for Table 4
</figureCaption>
<table confidence="0.9997426">
State Action State change/effect
init SysGoal: present items(i1, i2, i5, is)&amp; user choose one of(i1, i2, i5, is) initialise state
s1 RL-NLG: SUMMARY(i1, i2, i5, is) att=4, sent=1, user=silent
s2 RL-NLG: RECOMMEND(i5) att=8, sent=4, user=choose(i5)
end RL-NLG: stop calculate Reward
</table>
<tableCaption confidence="0.998936">
Table 2: Example utterance planning sequence for Figure 2
</tableCaption>
<figure confidence="0.990268692307693">
stop
summarise recommend
ACTIONS:
atts=4
user=silent
atts=8
user=choose
Reward
ENVIRONMENT:
GOAL
s1 s2
init
end
</figure>
<bodyText confidence="0.9977545">
best thing to do is “stop” and pass the turn to the
user. This takes us to the state end, where the total
reward of this action sequence is computed (see
Section 6.3), and used to update the NLG policy
in each of the visited state-action pairs via back-
propagation.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999844818181818">
We now report on a proof-of-concept study where
we train our policy in a simulated learning envi-
ronment based on the results from the MATCH cor-
pus analysis in Section 4. Simulation-based RL
allows to explore unseen actions which are not in
the data, and thus less initial data is needed (Rieser
and Lemon, 2008b). Note, that we cannot directly
learn from the MATCH data, as therefore we would
need data from an interactive dialogue. We are
currently collecting such data in a Wizard-of-Oz
experiment.
</bodyText>
<subsectionHeader confidence="0.997118">
6.1 User simulation
</subsectionHeader>
<bodyText confidence="0.999902971428571">
User simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al., 2007). A user simulation for
NLG is very similar, in that it is a predictive model
of the most likely next user act. However, this user
act does not actually change the overall dialogue
state (e.g. by filling slots) but it only changes the
generator state. In other words, the NLG user sim-
ulation tells us what the user is most likely to do
next, if we were to stop generating now. It also
tells us the probability whether the user chooses
to “barge-in” after a system NLG action (by either
choosing an item or providing more information).
The user simulation for this study is a simple
bi-gram model, which relates the number of at-
tributes presented to the next likely user actions,
see Table 3. The user can either follow the goal
provided by the DM (SysGoal), for example
choosing an item. The user can also do some-
thing else (userElse), e.g. providing another
constraint, or the user can quit (userQuit).
For simplification, we discretise the number of
attributes into concise-average-verbose,
reflecting the conciseness values from the MATCH
data, as described in Section 4. In addition, we
assume that the user’s cognitive abilities are lim-
ited (“cognitive load”), based on the results from
the second MATCH data set in Section 4. Once the
number of attributes is more than the “magic num-
ber 7” (reflecting psychological results on short-
term memory) (Baddeley, 2001)) the user is more
likely to become confused and quit.
The probabilities in Table 3 are currently man-
ually set heuristics. We are currently conducting a
Wizard-of-Oz study in order to learn these proba-
</bodyText>
<page confidence="0.997064">
687
</page>
<table confidence="0.9318144">
bilities (and other user parameters) from real data.
SysGoal userElse userQuit
concise 20.0 60.0 20.0
average 60.0 20.0 20.0
verbose 20.0 20.0 60.0
</table>
<tableCaption confidence="0.998878">
Table 3: NLG bi-gram user simulation
</tableCaption>
<subsectionHeader confidence="0.978673">
6.2 Realizer model
</subsectionHeader>
<bodyText confidence="0.999396454545455">
The sequential NLG model assumes a realizer,
which updates the context after each generation
step (i.e. after each single action). We estimate
the realiser’s parameters from the mean values we
found in the MATCH data (see Table 1). For this
study we first (randomly) vary the number of at-
tributes, whereas the number of sentences is fixed
(see Table 4). In current work we replace the re-
alizer model with an implemented generator that
replicates the variation found in the SPaRKy real-
izer (Stent et al., 2004).
</bodyText>
<table confidence="0.93623225">
#attr #sentence
SUMMARY 1 or 2 2
COMPARE 3 or 4 6
RECOMMEND 2 or 3 3
</table>
<tableCaption confidence="0.997501">
Table 4: Realizer parameters
</tableCaption>
<subsectionHeader confidence="0.985048">
6.3 Reward function
</subsectionHeader>
<bodyText confidence="0.999980923076923">
The reward function defines the final goal of the
utterance generation sequence. In this experiment
the reward is a function of the various data-driven
trade-offs as identified in the data analysis in Sec-
tion 4: utterance length and number of provided
attributes, as weighted by the regression model
in Equation 1, as well as the next predicted user
action. Since we currently only have overhearer
data, we manually estimate the reward for the
next most likely user act, to supplement the data-
driven model. If in the end state the next most
likely user act is userQuit, the learner gets a
penalty of −100, userElse receives 0 reward,
and SysGoal gains +100 reward. Again, these
hand coded scores need to be refined by a more
targeted data collection, but the other components
of the reward function are data-driven.
Note that RL learns to “make compromises”
with respect to the different trade-offs. For ex-
ample, the user is less likely to choose an item
if there are more than 7 attributes, but the real-
izer can generate 9 attributes. However, in some
contexts it might be desirable to generate all 9 at-
tributes, e.g. if the generated utterance is short.
Threshold-based approaches, in contrast, cannot
(easily) reason with respect to the current content.
</bodyText>
<subsectionHeader confidence="0.996659">
6.4 State and Action Space
</subsectionHeader>
<bodyText confidence="0.999997857142857">
We now formulate the problem as a Markov De-
cision Process (MDP), relating states to actions.
Each state-action pair is associated with a transi-
tion probability, which is the probability of mov-
ing from state s at time t to state s&apos; at time t+1 af-
ter having performed action a when in state s. This
transition probability is computed by the environ-
ment model (i.e. user and realizer), and explic-
itly captures noise/uncertainty in the environment.
This is a major difference to other non-statistical
planning approaches. Each transition is also as-
sociated with a reinforcement signal (or reward)
rt+1 describing how good the result of action a
was when performed in state s.
The state space comprises 9 binary features rep-
resenting the number of attributes, 2 binary fea-
tures representing the predicted user’s next ac-
tion to follow the system goal or quit, as well as
a discrete feature reflecting the number of sen-
tences generated so far, as shown in Figure 3.
This results in 211 x 6 = 12, 288 distinct genera-
tion states. We trained the policy using the well
known SARSA algorithm, using linear function ap-
proximation (Sutton and Barto, 1998). The policy
was trained for 3600 simulated NLG sequences.
In future work we plan to learn lower level deci-
sions, such as lexical adaptation based on the vo-
cabulary used by the user.
</bodyText>
<subsectionHeader confidence="0.995827">
6.5 Baselines
</subsectionHeader>
<bodyText confidence="0.9994812">
We derive the baseline policies from Informa-
tion Presentation strategies as deployed by cur-
rent dialogue systems. In total we utilise 7 differ-
ent baselines (B1-B7), which correspond to single
branches in our policy space (see Figure 1):
</bodyText>
<listItem confidence="0.967843875">
B1: RECOMMEND only, e.g. (Young et al., 2007)
B2: COMPARE only, e.g. (Henderson et al., 2008)
B3: SUMMARY only, e.g. (Polifroni and Walker,
2008)
B4: SUMMARY followed by RECOMMEND, e.g.
(Whittaker et al., 2002)
B5: Randomly choosing between COMPARE and
RECOMMEND, e.g. (Walker et al., 2007)
</listItem>
<page confidence="0.992051">
688
</page>
<table confidence="0.963720166666667">
⎡ ⎡SUMMARY ⎤ state: ⎡ attributesr |1 |-119  |: 0,1
⎢ ⎢ ⎢ ⎢ ⎢ ⎢action: ⎢ ⎢COMPARE ⎦⎥⎥⎥ sentence: { 1-11 } l o
⎢ ⎢ ⎣ ⎢ ⎣RECOMMEND userGoal: 0,1oJ ⎥ ⎥
end ⎥ ⎥
⎣n o ⎦ ⎦
userQuit: 0,1
</table>
<figureCaption confidence="0.982744">
Figure 3: State-Action space for RL-NLG
</figureCaption>
<bodyText confidence="0.7861632">
B6: Randomly choosing between all 7 outputs
B7: Always generating whole sequence, i.e.
SUMMARY+COMPARE+RECOMMEND, as
suggested by the analytic solution (see
Section 4).
</bodyText>
<subsectionHeader confidence="0.531426">
6.6 Results
</subsectionHeader>
<bodyText confidence="0.99996375">
We analyse the test runs (n=200) using an ANOVA
with a PostHoc T-Test (with Bonferroni correc-
tion). RL significantly (p &lt; .001) outperforms all
baselines in terms of final reward, see Table 5. RL
is the only policy which significantly improves the
next most likely user action by adapting to features
in the current context. In contrast to conventional
approaches, RL learns to ‘control’ its environment
according to the estimated transition probabilities
and the associated rewards.
The learnt policy can be described as follows:
It either starts with SUMMARY or COMPARE af-
ter the init state, i.e. it learnt to never start with a
RECOMMEND. It stops generating after COMPARE
if the userGoal is (probably) reached (e.g. the
user is most likely to choose an item in the next
turn, which depends on the number of attributes
generated), otherwise it goes on and generates a
RECOMMEND. If it starts with SUMMARY, it al-
ways generates a COMPARE afterwards. Again, it
stops if the userGoal is (probably) reached, oth-
erwise it generates the full sequence (which corre-
sponds to the analytic solution B7).
The analytic solution B7 performs second best,
and significantly outperforms all the other base-
lines (p &lt; .01). Still, it is significantly worse
(p &lt; .001) than the learnt policy as this ‘one-shot-
strategy’ cannot robustly and dynamically adopt to
noise or changes in the environment.
In general, generating sequences of NLG ac-
tions rates higher than generating single actions
only: B4 and B6 rate directly after RL and B7,
while B1, B2, B3, B5 are all equally bad given
our data-driven definition of reward and environ-
ment. Furthermore, the simulated environment
allows us to replicate the results in the MATCH
corpus (see Section 4) when only comparing sin-
gle strategies: SUMMARY performs significantly
worse, while RECOMMEND and COMPARE per-
form equally well.
</bodyText>
<table confidence="0.954176">
policy reward (±std)
B1 99.1 (±129.6)
B2 90.9 (±142.2)
B3 65.5 (±137.3)
B4 176.0 (±154.1)
B5 95.9 (±144.9)
B6 168.8 (±165.3)
B7 229.3 (±157.1)
RL 310.8 (±136.1)
</table>
<tableCaption confidence="0.997586">
Table 5: Evaluation Results (p &lt; .001 )
</tableCaption>
<sectionHeader confidence="0.995449" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999837">
We presented and evaluated a new model for Nat-
ural Language Generation (NLG) in Spoken Dia-
logue Systems, based on statistical planning. After
motivating and presenting the model, we studied
its use in Information Presentation.
We derived a data-driven model predicting
users’ judgements to different information presen-
tation actions (reward function), via a regression
analysis on MATCH data. We used this regression
model to set weights in a reward function for Re-
inforcement Learning, and so optimize a context-
adaptive presentation policy. The learnt policy was
compared to several baselines derived from previ-
ous work in this area, where the learnt policy sig-
nificantly outperforms all the baselines.
There are many possible extensions to this
model, e.g. using the same techniques to jointly
optimise choosing the number of attributes, aggre-
gation, word choice, referring expressions, and so
on, in a hierarchical manner.
</bodyText>
<page confidence="0.997326">
689
</page>
<bodyText confidence="0.99997525">
We are currently collecting data in targeted
Wizard-of-Oz experiments, to derive a fully data-
driven training environment and test the learnt
policy with real users, following (Rieser and
Lemon, 2008b). The trained NLG strategy
will also be integrated in an end-to-end statis-
tical system within the CLASSiC project (www.
classic-project.org).
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999952666666667">
The research leading to these results has received
funding from the European Community’s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no. 216594 (CLASSiC project
project: www.classic-project.org) and
from the EPSRC project no. EP/E019501/1.
</bodyText>
<sectionHeader confidence="0.99884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999601535714286">
A. Baddeley. 2001. Working memory and language:
an overview. Journal of Communication Disorder,
36(3):189–208.
Vera Demberg and Johanna D. Moore. 2006. Infor-
mation presentation in spoken dialogue systems. In
Proceedings of EACL.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement / supervised
learning of dialogue policies from fixed datasets.
Computational Linguistics (to appear).
Srinivasan Janarthanam and Oliver Lemon. 2008. User
simulations for online adaptation and knowledge-
alignment in Troubleshooting dialogue systems. In
Proc. of SEMdial.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of ACL.
Oliver Lemon. 2008. Adaptive Natural Language
Generation in Dialogue using Reinforcement Learn-
ing. In Proceedings of SEMdial.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, com-
parative descriptions in spoken dialogue. In Proc.
FLAIRS.
Crystal Nakatsu and Michael White. 2006. Learning
to say it well: Reranking realizations by predicted
synthesis quality. In Proceedings of ACL.
Alice Oh and Alexander Rudnicky. 2002. Stochastic
natural language generation for spoken dialog sys-
tems. Computer, Speech &amp; Language, 16(3/4):387–
407.
Tim Paek and Eric Horvitz. 2000. Conversation as
action under uncertainty. In Proc. of the 16th Con-
ference on Uncertainty in Artificial Intelligence.
Joseph Polifroni and Marilyn Walker. 2008. Inten-
sional Summaries as Cooperative Responses in Di-
alogue Automation and Evaluation. In Proceedings
ofACL.
Verena Rieser and Oliver Lemon. 2008a. Does this
list contain what you were searching for? Learn-
ing adaptive dialogue strategies for Interactive Ques-
tion Answering. J. Natural Language Engineering,
15(1):55–72.
Verena Rieser and Oliver Lemon. 2008b. Learn-
ing Effective Multimodal Dialogue Strategies from
Wizard-of-Oz data: Bootstrapping and Evaluation.
In Proceedings ofACL.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with Reinforce-
ment Learning: Experiments with the NJFun sys-
tem. JAIR, 16:105–133.
Amanda Stent, Marilyn Walker, Steve Whittaker, and
Preetam Maloor. 2002. User-tailored generation for
spoken dialogue: an experiment. In In Proc. of IC-
SLP.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards developing general mod-
els of usability with PARADISE. Natural Language
Engineering, 6(3).
Marilyn Walker, S. Whittaker, A. Stent, P. Maloor,
J. Moore, M. Johnston, and G. Vasireddy. 2004.
User tailored generation in the match multimodal di-
alogue system. Cognitive Science, 28:811–840.
Marilyn Walker, Amanda Stent, Franc¸ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413–456.
Steve Whittaker, Marilyn Walker, and Johanna Moore.
2002. Fish or Fowl: A Wizard of Oz evaluation
of dialogue strategies in the restaurant domain. In
Proc. of the International Conference on Language
Resources and Evaluation (LREC).
Stephen Whittaker, Marilyn Walker, and Preetam Mal-
oor. 2003. Should i tell all? an experiment on
conciseness in spoken dialogue. In Proc. European
Conference on Speech Processing (EUROSPEECH).
</reference>
<page confidence="0.970404">
690
</page>
<reference confidence="0.999254777777778">
Andi Winterboer, Jiang Hu, Johanna D. Moore, and
Clifford Nass. 2007. The influence of user tailoring
and cognitive load on user performance in spoken
dialogue systems. In Proc. of the 10th International
Conference of Spoken Language Processing (Inter-
speech/ICSLP).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
</reference>
<page confidence="0.998259">
691
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.914082">
<title confidence="0.9991085">Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems</title>
<author confidence="0.999878">Verena Rieser Oliver Lemon</author>
<affiliation confidence="0.9998755">School of Informatics School of Informatics University of Edinburgh University of Edinburgh</affiliation>
<email confidence="0.96861">vrieser@inf.ed.ac.ukolemon@inf.ed.ac.uk</email>
<abstract confidence="0.997316047619048">We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex tradeoffs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing exist- We then train a NLG policy using Reinforcement Learning (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Baddeley</author>
</authors>
<title>Working memory and language: an overview.</title>
<date>2001</date>
<journal>Journal of Communication Disorder,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="18327" citStr="Baddeley, 2001" startWordPosition="3027" endWordPosition="3028">oal), for example choosing an item. The user can also do something else (userElse), e.g. providing another constraint, or the user can quit (userQuit). For simplification, we discretise the number of attributes into concise-average-verbose, reflecting the conciseness values from the MATCH data, as described in Section 4. In addition, we assume that the user’s cognitive abilities are limited (“cognitive load”), based on the results from the second MATCH data set in Section 4. Once the number of attributes is more than the “magic number 7” (reflecting psychological results on shortterm memory) (Baddeley, 2001)) the user is more likely to become confused and quit. The probabilities in Table 3 are currently manually set heuristics. We are currently conducting a Wizard-of-Oz study in order to learn these proba687 bilities (and other user parameters) from real data. SysGoal userElse userQuit concise 20.0 60.0 20.0 average 60.0 20.0 20.0 verbose 20.0 20.0 60.0 Table 3: NLG bi-gram user simulation 6.2 Realizer model The sequential NLG model assumes a realizer, which updates the context after each generation step (i.e. after each single action). We estimate the realiser’s parameters from the mean values w</context>
</contexts>
<marker>Baddeley, 2001</marker>
<rawString>A. Baddeley. 2001. Working memory and language: an overview. Journal of Communication Disorder, 36(3):189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Johanna D Moore</author>
</authors>
<title>Information presentation in spoken dialogue systems.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="1471" citStr="Demberg and Moore, 2006" startWordPosition="229" endWordPosition="232">ehaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches. 1 Introduction Natural language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), </context>
<context position="7100" citStr="Demberg and Moore, 2006" startWordPosition="1168" endWordPosition="1171">ted plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). 3 The Information Presentation Problem We will tackle the well-studied problem of Information Presentation in NLG to show the benefits of this approach. The task here is to find the best way to present a set of search results to a user (e.g. some restaurants meeting a certain set of constraints). This is a task common to much prior work in NLG, e.g. (Walker et al., 2004; Demberg and Moore, 2006; Polifroni and Walker, 2008). In this problem, there there are many decisions available for exploration. For instance, which presentation strategy to apply (NLG strategy selection), how many attributes of each item to present (attribute selection), how to rank the items and attributes according to different models of user preferences (attribute ordering), how many (specific) items to tell them about (conciseness), how many sentences to use when doing so (syntactic planning), and which words to use (lexical choice) etc. All these parameters (and potentially many more) can be varied, and ideall</context>
</contexts>
<marker>Demberg, Moore, 2006</marker>
<rawString>Vera Demberg and Johanna D. Moore. 2006. Information presentation in spoken dialogue systems. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
</authors>
<title>Hybrid reinforcement / supervised learning of dialogue policies from fixed datasets.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<note>(to appear).</note>
<contexts>
<context position="2119" citStr="Henderson et al., 2008" startWordPosition="337" endWordPosition="340">alker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual NLG action will convey some (potentially) useful information to the user. There is therefore an optimization problem to be solved. Moreover, the user judgements or next (most likely) action after each NLG action ar</context>
<context position="22317" citStr="Henderson et al., 2008" startWordPosition="3706" endWordPosition="3709">ing the well known SARSA algorithm, using linear function approximation (Sutton and Barto, 1998). The policy was trained for 3600 simulated NLG sequences. In future work we plan to learn lower level decisions, such as lexical adaptation based on the vocabulary used by the user. 6.5 Baselines We derive the baseline policies from Information Presentation strategies as deployed by current dialogue systems. In total we utilise 7 different baselines (B1-B7), which correspond to single branches in our policy space (see Figure 1): B1: RECOMMEND only, e.g. (Young et al., 2007) B2: COMPARE only, e.g. (Henderson et al., 2008) B3: SUMMARY only, e.g. (Polifroni and Walker, 2008) B4: SUMMARY followed by RECOMMEND, e.g. (Whittaker et al., 2002) B5: Randomly choosing between COMPARE and RECOMMEND, e.g. (Walker et al., 2007) 688 ⎡ ⎡SUMMARY ⎤ state: ⎡ attributesr |1 |-119 |: 0,1 ⎢ ⎢ ⎢ ⎢ ⎢ ⎢action: ⎢ ⎢COMPARE ⎦⎥⎥⎥ sentence: { 1-11 } l o ⎢ ⎢ ⎣ ⎢ ⎣RECOMMEND userGoal: 0,1oJ ⎥ ⎥ end ⎥ ⎥ ⎣n o ⎦ ⎦ userQuit: 0,1 Figure 3: State-Action space for RL-NLG B6: Randomly choosing between all 7 outputs B7: Always generating whole sequence, i.e. SUMMARY+COMPARE+RECOMMEND, as suggested by the analytic solution (see Section 4). 6.6 Results</context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2008</marker>
<rawString>James Henderson, Oliver Lemon, and Kallirroi Georgila. 2008. Hybrid reinforcement / supervised learning of dialogue policies from fixed datasets. Computational Linguistics (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>User simulations for online adaptation and knowledgealignment in Troubleshooting dialogue systems.</title>
<date>2008</date>
<booktitle>In Proc. of SEMdial.</booktitle>
<contexts>
<context position="1845" citStr="Janarthanam and Lemon, 2008" startWordPosition="294" endWordPosition="297">In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because </context>
</contexts>
<marker>Janarthanam, Lemon, 2008</marker>
<rawString>Srinivasan Janarthanam and Oliver Lemon. 2008. User simulations for online adaptation and knowledgealignment in Troubleshooting dialogue systems. In Proc. of SEMdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Ronald Petrick</author>
</authors>
<title>Experiences with planning for natural language generation.</title>
<date>2008</date>
<booktitle>In ICAPS.</booktitle>
<contexts>
<context position="4324" citStr="Koller and Petrick, 2008" startWordPosition="700" endWordPosition="703">In Section 6 we report on experimental results using this framework for exploring Information Presentation policies. In Section 7 we conclude and discuss future directions. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 683 2 NLG as planning under uncertainty We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004). This will introduce </context>
</contexts>
<marker>Koller, Petrick, 2008</marker>
<rawString>Alexander Koller and Ronald Petrick. 2008. Experiences with planning for natural language generation. In ICAPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Matthew Stone</author>
</authors>
<title>Sentence generation as planning.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4297" citStr="Koller and Stone, 2007" startWordPosition="696" endWordPosition="699">ur proposed NLG method. In Section 6 we report on experimental results using this framework for exploring Information Presentation policies. In Section 7 we conclude and discuss future directions. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 683 2 NLG as planning under uncertainty We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., </context>
</contexts>
<marker>Koller, Stone, 2007</marker>
<rawString>Alexander Koller and Matthew Stone. 2007. Sentence generation as planning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
</authors>
<title>Adaptive Natural Language Generation in Dialogue using Reinforcement Learning.</title>
<date>2008</date>
<booktitle>In Proceedings of SEMdial.</booktitle>
<contexts>
<context position="1845" citStr="Lemon, 2008" startWordPosition="296" endWordPosition="297">ogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because </context>
<context position="4174" citStr="Lemon, 2008" startWordPosition="677" endWordPosition="678">ults from our analysis of the MATCH corpus (Walker et al., 2004). In Section 5 we present a detailed example of our proposed NLG method. In Section 6 we report on experimental results using this framework for exploring Information Presentation policies. In Section 7 we conclude and discuss future directions. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 683 2 NLG as planning under uncertainty We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixe</context>
<context position="16633" citStr="Lemon, 2008" startWordPosition="2742" endWordPosition="2743">: GOAL s1 s2 init end best thing to do is “stop” and pass the turn to the user. This takes us to the state end, where the total reward of this action sequence is computed (see Section 6.3), and used to update the NLG policy in each of the visited state-action pairs via backpropagation. 6 Experiments We now report on a proof-of-concept study where we train our policy in a simulated learning environment based on the results from the MATCH corpus analysis in Section 4. Simulation-based RL allows to explore unseen actions which are not in the data, and thus less initial data is needed (Rieser and Lemon, 2008b). Note, that we cannot directly learn from the MATCH data, as therefore we would need data from an interactive dialogue. We are currently collecting such data in a Wizard-of-Oz experiment. 6.1 User simulation User simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007). A user simulation for NLG is very similar, in that it is a predictive model of the most likely next user act. However, this user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, the N</context>
</contexts>
<marker>Lemon, 2008</marker>
<rawString>Oliver Lemon. 2008. Adaptive Natural Language Generation in Dialogue using Reinforcement Learning. In Proceedings of SEMdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
<author>Mary Ellen Foster</author>
<author>Oliver Lemon</author>
<author>Michael White</author>
</authors>
<title>Generating tailored, comparative descriptions in spoken dialogue.</title>
<date>2004</date>
<booktitle>In Proc. FLAIRS.</booktitle>
<contexts>
<context position="6246" citStr="Moore et al., 2004" startWordPosition="1024" endWordPosition="1027">). Likewise, the user may be likely to choose an item after hearing a summary, or they may wish to hear more. Generating appropriate language in context (e.g. attributes presented so far) thus has the following important features in general: • NLG is goal driven behaviour • NLG must plan a sequence of actions • each action changes the environment state or context • the effect of each action is uncertain. These facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). 3 The Information Presentation Problem We will tackle the well-studied problem of Information Presentation in NLG to show the benefits of this </context>
</contexts>
<marker>Moore, Foster, Lemon, White, 2004</marker>
<rawString>Johanna Moore, Mary Ellen Foster, Oliver Lemon, and Michael White. 2004. Generating tailored, comparative descriptions in spoken dialogue. In Proc. FLAIRS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Crystal Nakatsu</author>
<author>Michael White</author>
</authors>
<title>Learning to say it well: Reranking realizations by predicted synthesis quality.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1903" citStr="Nakatsu and White, 2006" startWordPosition="303" endWordPosition="306">oal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individua</context>
</contexts>
<marker>Nakatsu, White, 2006</marker>
<rawString>Crystal Nakatsu and Michael White. 2006. Learning to say it well: Reranking realizations by predicted synthesis quality. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Oh</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Stochastic natural language generation for spoken dialog systems.</title>
<date>2002</date>
<journal>Computer, Speech &amp; Language,</journal>
<volume>16</volume>
<issue>3</issue>
<pages>407</pages>
<contexts>
<context position="6415" citStr="Oh and Rudnicky, 2002" startWordPosition="1052" endWordPosition="1055">utes presented so far) thus has the following important features in general: • NLG is goal driven behaviour • NLG must plan a sequence of actions • each action changes the environment state or context • the effect of each action is uncertain. These facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). 3 The Information Presentation Problem We will tackle the well-studied problem of Information Presentation in NLG to show the benefits of this approach. The task here is to find the best way to present a set of search results to a user (e.g. some restaurants meeting a certain set of constraints). This is a task</context>
</contexts>
<marker>Oh, Rudnicky, 2002</marker>
<rawString>Alice Oh and Alexander Rudnicky. 2002. Stochastic natural language generation for spoken dialog systems. Computer, Speech &amp; Language, 16(3/4):387– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Paek</author>
<author>Eric Horvitz</author>
</authors>
<title>Conversation as action under uncertainty.</title>
<date>2000</date>
<booktitle>In Proc. of the 16th Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="2217" citStr="Paek and Horvitz, 2000" startWordPosition="351" endWordPosition="354"> Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual NLG action will convey some (potentially) useful information to the user. There is therefore an optimization problem to be solved. Moreover, the user judgements or next (most likely) action after each NLG action are unpredictable, and the behaviour of the surface realizer may also be variable (see Section 6.2).</context>
</contexts>
<marker>Paek, Horvitz, 2000</marker>
<rawString>Tim Paek and Eric Horvitz. 2000. Conversation as action under uncertainty. In Proc. of the 16th Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Polifroni</author>
<author>Marilyn Walker</author>
</authors>
<title>Intensional Summaries as Cooperative Responses in Dialogue Automation and Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1445" citStr="Polifroni and Walker, 2008" startWordPosition="225" endWordPosition="228">ing (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches. 1 Introduction Natural language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to </context>
<context position="7129" citStr="Polifroni and Walker, 2008" startWordPosition="1172" endWordPosition="1175">as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). 3 The Information Presentation Problem We will tackle the well-studied problem of Information Presentation in NLG to show the benefits of this approach. The task here is to find the best way to present a set of search results to a user (e.g. some restaurants meeting a certain set of constraints). This is a task common to much prior work in NLG, e.g. (Walker et al., 2004; Demberg and Moore, 2006; Polifroni and Walker, 2008). In this problem, there there are many decisions available for exploration. For instance, which presentation strategy to apply (NLG strategy selection), how many attributes of each item to present (attribute selection), how to rank the items and attributes according to different models of user preferences (attribute ordering), how many (specific) items to tell them about (conciseness), how many sentences to use when doing so (syntactic planning), and which words to use (lexical choice) etc. All these parameters (and potentially many more) can be varied, and ideally, jointly optimised based on</context>
<context position="10316" citStr="Polifroni and Walker, 2008" startWordPosition="1686" endWordPosition="1690"> of attributes. In this study the users rate the individual presentation strategies as significantly different (F(2) = 1361,p &lt; .001). We find that SUMMARY is rated significantly worse (p = .05 with Bonferroni correction) than RECOMMEND and COMPARE, which are rated as equally good. This suggests that one should never generate a SUMMARY. However, SUMMARY has different qualities from COMPARE and RECOMMEND, as it gives users a general overview of the domain, and probably helps the user to feel more confident when choosing an item, especially when they are unfamiliar with the domain, as shown by (Polifroni and Walker, 2008). In order to further describe the strategies, we extracted different surface features as present in the data (e.g. number of attributes realised, number of sentences, number of words, number of database items talked about, etc.) and performed a stepwise linear regression to find the features which were important to the overhearers (following the PARADISE framework (Walker et al., 2000)). We discovered a trade-off between the length of the utterance (#sentence) and the number of attributes realised (#attr), i.e. its informativeness, where overhearers like to hear as many attributes as possible</context>
<context position="22369" citStr="Polifroni and Walker, 2008" startWordPosition="3714" endWordPosition="3717"> function approximation (Sutton and Barto, 1998). The policy was trained for 3600 simulated NLG sequences. In future work we plan to learn lower level decisions, such as lexical adaptation based on the vocabulary used by the user. 6.5 Baselines We derive the baseline policies from Information Presentation strategies as deployed by current dialogue systems. In total we utilise 7 different baselines (B1-B7), which correspond to single branches in our policy space (see Figure 1): B1: RECOMMEND only, e.g. (Young et al., 2007) B2: COMPARE only, e.g. (Henderson et al., 2008) B3: SUMMARY only, e.g. (Polifroni and Walker, 2008) B4: SUMMARY followed by RECOMMEND, e.g. (Whittaker et al., 2002) B5: Randomly choosing between COMPARE and RECOMMEND, e.g. (Walker et al., 2007) 688 ⎡ ⎡SUMMARY ⎤ state: ⎡ attributesr |1 |-119 |: 0,1 ⎢ ⎢ ⎢ ⎢ ⎢ ⎢action: ⎢ ⎢COMPARE ⎦⎥⎥⎥ sentence: { 1-11 } l o ⎢ ⎢ ⎣ ⎢ ⎣RECOMMEND userGoal: 0,1oJ ⎥ ⎥ end ⎥ ⎥ ⎣n o ⎦ ⎦ userQuit: 0,1 Figure 3: State-Action space for RL-NLG B6: Randomly choosing between all 7 outputs B7: Always generating whole sequence, i.e. SUMMARY+COMPARE+RECOMMEND, as suggested by the analytic solution (see Section 4). 6.6 Results We analyse the test runs (n=200) using an ANOVA wit</context>
</contexts>
<marker>Polifroni, Walker, 2008</marker>
<rawString>Joseph Polifroni and Marilyn Walker. 2008. Intensional Summaries as Cooperative Responses in Dialogue Automation and Evaluation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Does this list contain what you were searching for? Learning adaptive dialogue strategies for Interactive Question Answering.</title>
<date>2008</date>
<journal>J. Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="2143" citStr="Rieser and Lemon, 2008" startWordPosition="341" endWordPosition="344">by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual NLG action will convey some (potentially) useful information to the user. There is therefore an optimization problem to be solved. Moreover, the user judgements or next (most likely) action after each NLG action are unpredictable, and the</context>
<context position="4723" citStr="Rieser and Lemon, 2008" startWordPosition="769" endWordPosition="772">general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004). This will introduce additional noise, which higher level NLG decisions will need to react to. In our framework, the NLG component must achieve a high-level Communicative Goal from the Dialogue Manager (e.g. to present a number of items) through planning a sequence of lowerlevel generation steps or actions, for example first to summarize all the items and then to recommend the highest ranking one. Each such action ha</context>
<context position="16633" citStr="Rieser and Lemon, 2008" startWordPosition="2740" endWordPosition="2743">ENVIRONMENT: GOAL s1 s2 init end best thing to do is “stop” and pass the turn to the user. This takes us to the state end, where the total reward of this action sequence is computed (see Section 6.3), and used to update the NLG policy in each of the visited state-action pairs via backpropagation. 6 Experiments We now report on a proof-of-concept study where we train our policy in a simulated learning environment based on the results from the MATCH corpus analysis in Section 4. Simulation-based RL allows to explore unseen actions which are not in the data, and thus less initial data is needed (Rieser and Lemon, 2008b). Note, that we cannot directly learn from the MATCH data, as therefore we would need data from an interactive dialogue. We are currently collecting such data in a Wizard-of-Oz experiment. 6.1 User simulation User simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007). A user simulation for NLG is very similar, in that it is a predictive model of the most likely next user act. However, this user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, the N</context>
</contexts>
<marker>Rieser, Lemon, 2008</marker>
<rawString>Verena Rieser and Oliver Lemon. 2008a. Does this list contain what you were searching for? Learning adaptive dialogue strategies for Interactive Question Answering. J. Natural Language Engineering, 15(1):55–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz data: Bootstrapping and Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2143" citStr="Rieser and Lemon, 2008" startWordPosition="341" endWordPosition="344">by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual NLG action will convey some (potentially) useful information to the user. There is therefore an optimization problem to be solved. Moreover, the user judgements or next (most likely) action after each NLG action are unpredictable, and the</context>
<context position="4723" citStr="Rieser and Lemon, 2008" startWordPosition="769" endWordPosition="772">general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004). This will introduce additional noise, which higher level NLG decisions will need to react to. In our framework, the NLG component must achieve a high-level Communicative Goal from the Dialogue Manager (e.g. to present a number of items) through planning a sequence of lowerlevel generation steps or actions, for example first to summarize all the items and then to recommend the highest ranking one. Each such action ha</context>
<context position="16633" citStr="Rieser and Lemon, 2008" startWordPosition="2740" endWordPosition="2743">ENVIRONMENT: GOAL s1 s2 init end best thing to do is “stop” and pass the turn to the user. This takes us to the state end, where the total reward of this action sequence is computed (see Section 6.3), and used to update the NLG policy in each of the visited state-action pairs via backpropagation. 6 Experiments We now report on a proof-of-concept study where we train our policy in a simulated learning environment based on the results from the MATCH corpus analysis in Section 4. Simulation-based RL allows to explore unseen actions which are not in the data, and thus less initial data is needed (Rieser and Lemon, 2008b). Note, that we cannot directly learn from the MATCH data, as therefore we would need data from an interactive dialogue. We are currently collecting such data in a Wizard-of-Oz experiment. 6.1 User simulation User simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007). A user simulation for NLG is very similar, in that it is a predictive model of the most likely next user act. However, this user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, the N</context>
</contexts>
<marker>Rieser, Lemon, 2008</marker>
<rawString>Verena Rieser and Oliver Lemon. 2008b. Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz data: Bootstrapping and Evaluation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>D Litman</author>
<author>M Kearns</author>
<author>M Walker</author>
</authors>
<title>Optimizing dialogue management with Reinforcement Learning: Experiments with the NJFun system.</title>
<date>2002</date>
<journal>JAIR,</journal>
<pages>16--105</pages>
<contexts>
<context position="2095" citStr="Singh et al., 2002" startWordPosition="333" endWordPosition="336">y comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual NLG action will convey some (potentially) useful information to the user. There is therefore an optimization problem to be solved. Moreover, the user judgements or next (most likely) action </context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>S. Singh, D. Litman, M. Kearns, and M. Walker. 2002. Optimizing dialogue management with Reinforcement Learning: Experiments with the NJFun system. JAIR, 16:105–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
<author>Preetam Maloor</author>
</authors>
<title>User-tailored generation for spoken dialogue: an experiment. In</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="8471" citStr="Stent et al., 2002" startWordPosition="1383" endWordPosition="1386">rpus (Walker et al., 2004) to extract an evaluation function (also known as ”reward function”) for RL. Furthermore, we utilise the SPaRKy corpus (Stent et al., 2004) to build a high quality stochastic realizer. Both corpora contain data from “overhearer” experiments targeted to Information Presentation in dialogues in the restaurant domain. While we are ultimately interested in how hearers engaged in dialogues judge different Information Presentations, results from overhearers are still directly relevant to the task. 4 MATCH corpus analysis The MATCH project made two data sets available, see (Stent et al., 2002) and (Whittaker et al., 2003), which we combine to define an evaluation function for different Information Presentation strategies. 684 strategy example av.#attr av.#sentence SUMMARY “The 4 restaurants differ in food quality, and cost.” 2.07±.63 1.56±.5 (#attr = 2, #sentence = 1) COMPARE “Among the selected restaurants, the following offer 3.2±1.5 5.5±3.11 exceptional overall value. Aureole’s price is 71 dollars. It has superb food quality, superb service and superb decor. Daniel’s price is 82 dollars. It has superb food quality, superb service and superb decor.” (#attr = 4, #sentence = 5) REC</context>
</contexts>
<marker>Stent, Walker, Whittaker, Maloor, 2002</marker>
<rawString>Amanda Stent, Marilyn Walker, Steve Whittaker, and Preetam Maloor. 2002. User-tailored generation for spoken dialogue: an experiment. In In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>Rashmi Prasad</author>
<author>Marilyn Walker</author>
</authors>
<title>Trainable sentence planning for complex information presentation in spoken dialog systems.</title>
<date>2004</date>
<journal>In Association for Computational</journal>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4902" citStr="Stent et al., 2004" startWordPosition="798" endWordPosition="801">nd Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004). This will introduce additional noise, which higher level NLG decisions will need to react to. In our framework, the NLG component must achieve a high-level Communicative Goal from the Dialogue Manager (e.g. to present a number of items) through planning a sequence of lowerlevel generation steps or actions, for example first to summarize all the items and then to recommend the highest ranking one. Each such action has unpredictable effects due to the stochastic realizer. For example the realizer might employ 6 attributes when recommending item i4, but it might use only 2 (e.g. price and cuisi</context>
<context position="6391" citStr="Stent et al., 2004" startWordPosition="1048" endWordPosition="1051">context (e.g. attributes presented so far) thus has the following important features in general: • NLG is goal driven behaviour • NLG must plan a sequence of actions • each action changes the environment state or context • the effect of each action is uncertain. These facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). 3 The Information Presentation Problem We will tackle the well-studied problem of Information Presentation in NLG to show the benefits of this approach. The task here is to find the best way to present a set of search results to a user (e.g. some restaurants meeting a certain set of cons</context>
<context position="8017" citStr="Stent et al., 2004" startWordPosition="1316" endWordPosition="1319">o different models of user preferences (attribute ordering), how many (specific) items to tell them about (conciseness), how many sentences to use when doing so (syntactic planning), and which words to use (lexical choice) etc. All these parameters (and potentially many more) can be varied, and ideally, jointly optimised based on user judgements. We had two corpora available to study some of the regions of this decision space. We utilise the MATCH corpus (Walker et al., 2004) to extract an evaluation function (also known as ”reward function”) for RL. Furthermore, we utilise the SPaRKy corpus (Stent et al., 2004) to build a high quality stochastic realizer. Both corpora contain data from “overhearer” experiments targeted to Information Presentation in dialogues in the restaurant domain. While we are ultimately interested in how hearers engaged in dialogues judge different Information Presentations, results from overhearers are still directly relevant to the task. 4 MATCH corpus analysis The MATCH project made two data sets available, see (Stent et al., 2002) and (Whittaker et al., 2003), which we combine to define an evaluation function for different Information Presentation strategies. 684 strategy e</context>
<context position="19245" citStr="Stent et al., 2004" startWordPosition="3179" endWordPosition="3182">.0 20.0 average 60.0 20.0 20.0 verbose 20.0 20.0 60.0 Table 3: NLG bi-gram user simulation 6.2 Realizer model The sequential NLG model assumes a realizer, which updates the context after each generation step (i.e. after each single action). We estimate the realiser’s parameters from the mean values we found in the MATCH data (see Table 1). For this study we first (randomly) vary the number of attributes, whereas the number of sentences is fixed (see Table 4). In current work we replace the realizer model with an implemented generator that replicates the variation found in the SPaRKy realizer (Stent et al., 2004). #attr #sentence SUMMARY 1 or 2 2 COMPARE 3 or 4 6 RECOMMEND 2 or 3 3 Table 4: Realizer parameters 6.3 Reward function The reward function defines the final goal of the utterance generation sequence. In this experiment the reward is a function of the various data-driven trade-offs as identified in the data analysis in Section 4: utterance length and number of provided attributes, as weighted by the regression model in Equation 1, as well as the next predicted user action. Since we currently only have overhearer data, we manually estimate the reward for the next most likely user act, to supple</context>
</contexts>
<marker>Stent, Prasad, Walker, 2004</marker>
<rawString>Amanda Stent, Rashmi Prasad, and Marilyn Walker. 2004. Trainable sentence planning for complex information presentation in spoken dialog systems. In Association for Computational Linguistics. R. Sutton and A. Barto. 1998. Reinforcement Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards developing general models of usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="10705" citStr="Walker et al., 2000" startWordPosition="1748" endWordPosition="1751"> RECOMMEND, as it gives users a general overview of the domain, and probably helps the user to feel more confident when choosing an item, especially when they are unfamiliar with the domain, as shown by (Polifroni and Walker, 2008). In order to further describe the strategies, we extracted different surface features as present in the data (e.g. number of attributes realised, number of sentences, number of words, number of database items talked about, etc.) and performed a stepwise linear regression to find the features which were important to the overhearers (following the PARADISE framework (Walker et al., 2000)). We discovered a trade-off between the length of the utterance (#sentence) and the number of attributes realised (#attr), i.e. its informativeness, where overhearers like to hear as many attributes as possible in the most concise way, as indicated by the regression model shown in Equation 1 (R2 = .34). 1 score = .775 x #attr + (−.301) x #sentence; (1) The second MATCH data set, see (Whittaker et al., 2003), comprises 1224 ratings by 17 subjects on the NLG strategies RECOMMEND and COMPARE. The strategies realise varying numbers of attributes according to different “conciseness” values: concis</context>
<context position="12458" citStr="Walker et al., 2000" startWordPosition="2038" endWordPosition="2041"> dialogue interaction, see (Winterboer et al., 2007). We therefore include “cognitive load” as a variable when training the policy (see Section 6). In addition to the trade-off between length and informativeness for single NLG strategies, we are interested whether this trade-off will also hold for generating sequences of NLG actions. (Whittaker et al., 2002), for example, generate a combined strategy where first a SUMMARY is used to describe the retrieved subset and then they RECOMMEND one specific item/restaurant. For example “The 4 restaurants are all French, but differ in 1For comparison: (Walker et al., 2000) report on R2 between .4 and .5 on a slightly lager data set. 685 Figure 1: Possible NLG policies (X=stop generation) food quality, and cost. Le Madeleine has the best overall value among the selected restaurants. Le Madeleine’s price is 40 dollars and It has very good food quality. It’s in Midtown West.” We therefore extend the set of possible strategies present in the data for exploration: we allow ordered combinations of the strategies, assuming that only COMPARE or RECOMMEND can follow a SUMMARY, and that only RECOMMEND can follow COMPARE, resulting in 7 possible actions: 1. RECOMMEND 2. C</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn A. Walker, Candace A. Kamm, and Diane J. Litman. 2000. Towards developing general models of usability with PARADISE. Natural Language Engineering, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>S Whittaker</author>
<author>A Stent</author>
<author>P Maloor</author>
<author>J Moore</author>
<author>M Johnston</author>
<author>G Vasireddy</author>
</authors>
<title>User tailored generation in the match multimodal dialogue system.</title>
<date>2004</date>
<journal>Cognitive Science,</journal>
<pages>28--811</pages>
<contexts>
<context position="1516" citStr="Walker et al., 2004" startWordPosition="237" endWordPosition="240">ation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches. 1 Introduction Natural language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2</context>
<context position="3626" citStr="Walker et al., 2004" startWordPosition="584" endWordPosition="587">which one to choose) or to instead stop generating. Reinforcement Learning (RL) allows us to optimize such trade-offs in the presence of uncertainty, i.e. the chances of achieving a better state, while engaging in the risk of choosing another action. In this paper we present and evaluate a new model for NLG in Spoken Dialogue Systems as planning under uncertainty. In Section 2 we argue for applying RL to NLG problems and explain the overall framework. In Section 3 we discuss challenges for NLG for Information Presentation. In Section 4 we present results from our analysis of the MATCH corpus (Walker et al., 2004). In Section 5 we present a detailed example of our proposed NLG method. In Section 6 we report on experimental results using this framework for exploring Information Presentation policies. In Section 7 we conclude and discuss future directions. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 683 2 NLG as planning under uncertainty We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some asp</context>
<context position="6268" citStr="Walker et al., 2004" startWordPosition="1028" endWordPosition="1031">r may be likely to choose an item after hearing a summary, or they may wish to hear more. Generating appropriate language in context (e.g. attributes presented so far) thus has the following important features in general: • NLG is goal driven behaviour • NLG must plan a sequence of actions • each action changes the environment state or context • the effect of each action is uncertain. These facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). 3 The Information Presentation Problem We will tackle the well-studied problem of Information Presentation in NLG to show the benefits of this approach. The task her</context>
<context position="7878" citStr="Walker et al., 2004" startWordPosition="1293" endWordPosition="1296">NLG strategy selection), how many attributes of each item to present (attribute selection), how to rank the items and attributes according to different models of user preferences (attribute ordering), how many (specific) items to tell them about (conciseness), how many sentences to use when doing so (syntactic planning), and which words to use (lexical choice) etc. All these parameters (and potentially many more) can be varied, and ideally, jointly optimised based on user judgements. We had two corpora available to study some of the regions of this decision space. We utilise the MATCH corpus (Walker et al., 2004) to extract an evaluation function (also known as ”reward function”) for RL. Furthermore, we utilise the SPaRKy corpus (Stent et al., 2004) to build a high quality stochastic realizer. Both corpora contain data from “overhearer” experiments targeted to Information Presentation in dialogues in the restaurant domain. While we are ultimately interested in how hearers engaged in dialogues judge different Information Presentations, results from overhearers are still directly relevant to the task. 4 MATCH corpus analysis The MATCH project made two data sets available, see (Stent et al., 2002) and (W</context>
</contexts>
<marker>Walker, Whittaker, Stent, Maloor, Moore, Johnston, Vasireddy, 2004</marker>
<rawString>Marilyn Walker, S. Whittaker, A. Stent, P. Maloor, J. Moore, M. Johnston, and G. Vasireddy. 2004. User tailored generation in the match multimodal dialogue system. Cognitive Science, 28:811–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Amanda Stent</author>
<author>Franc¸ois Mairesse</author>
<author>Rashmi Prasad</author>
</authors>
<title>Individual and domain adaptation in sentence planning for dialogue.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>30--413</pages>
<contexts>
<context position="22514" citStr="Walker et al., 2007" startWordPosition="3736" endWordPosition="3739">ecisions, such as lexical adaptation based on the vocabulary used by the user. 6.5 Baselines We derive the baseline policies from Information Presentation strategies as deployed by current dialogue systems. In total we utilise 7 different baselines (B1-B7), which correspond to single branches in our policy space (see Figure 1): B1: RECOMMEND only, e.g. (Young et al., 2007) B2: COMPARE only, e.g. (Henderson et al., 2008) B3: SUMMARY only, e.g. (Polifroni and Walker, 2008) B4: SUMMARY followed by RECOMMEND, e.g. (Whittaker et al., 2002) B5: Randomly choosing between COMPARE and RECOMMEND, e.g. (Walker et al., 2007) 688 ⎡ ⎡SUMMARY ⎤ state: ⎡ attributesr |1 |-119 |: 0,1 ⎢ ⎢ ⎢ ⎢ ⎢ ⎢action: ⎢ ⎢COMPARE ⎦⎥⎥⎥ sentence: { 1-11 } l o ⎢ ⎢ ⎣ ⎢ ⎣RECOMMEND userGoal: 0,1oJ ⎥ ⎥ end ⎥ ⎥ ⎣n o ⎦ ⎦ userQuit: 0,1 Figure 3: State-Action space for RL-NLG B6: Randomly choosing between all 7 outputs B7: Always generating whole sequence, i.e. SUMMARY+COMPARE+RECOMMEND, as suggested by the analytic solution (see Section 4). 6.6 Results We analyse the test runs (n=200) using an ANOVA with a PostHoc T-Test (with Bonferroni correction). RL significantly (p &lt; .001) outperforms all baselines in terms of final reward, see Table 5. RL </context>
</contexts>
<marker>Walker, Stent, Mairesse, Prasad, 2007</marker>
<rawString>Marilyn Walker, Amanda Stent, Franc¸ois Mairesse, and Rashmi Prasad. 2007. Individual and domain adaptation in sentence planning for dialogue. Journal of Artificial Intelligence Research (JAIR), 30:413–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Whittaker</author>
<author>Marilyn Walker</author>
<author>Johanna Moore</author>
</authors>
<title>Fish or Fowl: A Wizard of Oz evaluation of dialogue strategies in the restaurant domain.</title>
<date>2002</date>
<booktitle>In Proc. of the International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="12198" citStr="Whittaker et al., 2002" startWordPosition="1995" endWordPosition="1998"> relation between number of attributes and user ratings is not strictly linear: ratings drop for #attr = 6. This suggests that there is an upper limit on how many attributes users like to hear. We expect this to be especially true for real users engaged in actual dialogue interaction, see (Winterboer et al., 2007). We therefore include “cognitive load” as a variable when training the policy (see Section 6). In addition to the trade-off between length and informativeness for single NLG strategies, we are interested whether this trade-off will also hold for generating sequences of NLG actions. (Whittaker et al., 2002), for example, generate a combined strategy where first a SUMMARY is used to describe the retrieved subset and then they RECOMMEND one specific item/restaurant. For example “The 4 restaurants are all French, but differ in 1For comparison: (Walker et al., 2000) report on R2 between .4 and .5 on a slightly lager data set. 685 Figure 1: Possible NLG policies (X=stop generation) food quality, and cost. Le Madeleine has the best overall value among the selected restaurants. Le Madeleine’s price is 40 dollars and It has very good food quality. It’s in Midtown West.” We therefore extend the set of po</context>
<context position="22434" citStr="Whittaker et al., 2002" startWordPosition="3724" endWordPosition="3727">ned for 3600 simulated NLG sequences. In future work we plan to learn lower level decisions, such as lexical adaptation based on the vocabulary used by the user. 6.5 Baselines We derive the baseline policies from Information Presentation strategies as deployed by current dialogue systems. In total we utilise 7 different baselines (B1-B7), which correspond to single branches in our policy space (see Figure 1): B1: RECOMMEND only, e.g. (Young et al., 2007) B2: COMPARE only, e.g. (Henderson et al., 2008) B3: SUMMARY only, e.g. (Polifroni and Walker, 2008) B4: SUMMARY followed by RECOMMEND, e.g. (Whittaker et al., 2002) B5: Randomly choosing between COMPARE and RECOMMEND, e.g. (Walker et al., 2007) 688 ⎡ ⎡SUMMARY ⎤ state: ⎡ attributesr |1 |-119 |: 0,1 ⎢ ⎢ ⎢ ⎢ ⎢ ⎢action: ⎢ ⎢COMPARE ⎦⎥⎥⎥ sentence: { 1-11 } l o ⎢ ⎢ ⎣ ⎢ ⎣RECOMMEND userGoal: 0,1oJ ⎥ ⎥ end ⎥ ⎥ ⎣n o ⎦ ⎦ userQuit: 0,1 Figure 3: State-Action space for RL-NLG B6: Randomly choosing between all 7 outputs B7: Always generating whole sequence, i.e. SUMMARY+COMPARE+RECOMMEND, as suggested by the analytic solution (see Section 4). 6.6 Results We analyse the test runs (n=200) using an ANOVA with a PostHoc T-Test (with Bonferroni correction). RL significantly</context>
</contexts>
<marker>Whittaker, Walker, Moore, 2002</marker>
<rawString>Steve Whittaker, Marilyn Walker, and Johanna Moore. 2002. Fish or Fowl: A Wizard of Oz evaluation of dialogue strategies in the restaurant domain. In Proc. of the International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Whittaker</author>
<author>Marilyn Walker</author>
<author>Preetam Maloor</author>
</authors>
<title>Should i tell all? an experiment on conciseness in spoken dialogue. In</title>
<date>2003</date>
<booktitle>Proc. European Conference on Speech Processing (EUROSPEECH).</booktitle>
<contexts>
<context position="8500" citStr="Whittaker et al., 2003" startWordPosition="1388" endWordPosition="1391">) to extract an evaluation function (also known as ”reward function”) for RL. Furthermore, we utilise the SPaRKy corpus (Stent et al., 2004) to build a high quality stochastic realizer. Both corpora contain data from “overhearer” experiments targeted to Information Presentation in dialogues in the restaurant domain. While we are ultimately interested in how hearers engaged in dialogues judge different Information Presentations, results from overhearers are still directly relevant to the task. 4 MATCH corpus analysis The MATCH project made two data sets available, see (Stent et al., 2002) and (Whittaker et al., 2003), which we combine to define an evaluation function for different Information Presentation strategies. 684 strategy example av.#attr av.#sentence SUMMARY “The 4 restaurants differ in food quality, and cost.” 2.07±.63 1.56±.5 (#attr = 2, #sentence = 1) COMPARE “Among the selected restaurants, the following offer 3.2±1.5 5.5±3.11 exceptional overall value. Aureole’s price is 71 dollars. It has superb food quality, superb service and superb decor. Daniel’s price is 82 dollars. It has superb food quality, superb service and superb decor.” (#attr = 4, #sentence = 5) RECOMMEND “Le Madeleine has the </context>
<context position="11116" citStr="Whittaker et al., 2003" startWordPosition="1820" endWordPosition="1823"> words, number of database items talked about, etc.) and performed a stepwise linear regression to find the features which were important to the overhearers (following the PARADISE framework (Walker et al., 2000)). We discovered a trade-off between the length of the utterance (#sentence) and the number of attributes realised (#attr), i.e. its informativeness, where overhearers like to hear as many attributes as possible in the most concise way, as indicated by the regression model shown in Equation 1 (R2 = .34). 1 score = .775 x #attr + (−.301) x #sentence; (1) The second MATCH data set, see (Whittaker et al., 2003), comprises 1224 ratings by 17 subjects on the NLG strategies RECOMMEND and COMPARE. The strategies realise varying numbers of attributes according to different “conciseness” values: concise (1 or 2 attributes), average (3 or 4), and verbose (4,5, or 6). Overhearers rate all conciseness levels as significantly different (F(2) = 198.3,p &lt; .001), with verbose rated highest and concise rated lowest, supporting our findings in the first data set. However, the relation between number of attributes and user ratings is not strictly linear: ratings drop for #attr = 6. This suggests that there is an up</context>
</contexts>
<marker>Whittaker, Walker, Maloor, 2003</marker>
<rawString>Stephen Whittaker, Marilyn Walker, and Preetam Maloor. 2003. Should i tell all? an experiment on conciseness in spoken dialogue. In Proc. European Conference on Speech Processing (EUROSPEECH).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Winterboer</author>
<author>Jiang Hu</author>
<author>Johanna D Moore</author>
<author>Clifford Nass</author>
</authors>
<title>The influence of user tailoring and cognitive load on user performance in spoken dialogue systems.</title>
<date>2007</date>
<booktitle>In Proc. of the 10th International Conference of Spoken Language Processing (Interspeech/ICSLP).</booktitle>
<contexts>
<context position="11890" citStr="Winterboer et al., 2007" startWordPosition="1948" endWordPosition="1951"> different “conciseness” values: concise (1 or 2 attributes), average (3 or 4), and verbose (4,5, or 6). Overhearers rate all conciseness levels as significantly different (F(2) = 198.3,p &lt; .001), with verbose rated highest and concise rated lowest, supporting our findings in the first data set. However, the relation between number of attributes and user ratings is not strictly linear: ratings drop for #attr = 6. This suggests that there is an upper limit on how many attributes users like to hear. We expect this to be especially true for real users engaged in actual dialogue interaction, see (Winterboer et al., 2007). We therefore include “cognitive load” as a variable when training the policy (see Section 6). In addition to the trade-off between length and informativeness for single NLG strategies, we are interested whether this trade-off will also hold for generating sequences of NLG actions. (Whittaker et al., 2002), for example, generate a combined strategy where first a SUMMARY is used to describe the retrieved subset and then they RECOMMEND one specific item/restaurant. For example “The 4 restaurants are all French, but differ in 1For comparison: (Walker et al., 2000) report on R2 between .4 and .5 </context>
</contexts>
<marker>Winterboer, Hu, Moore, Nass, 2007</marker>
<rawString>Andi Winterboer, Jiang Hu, Johanna D. Moore, and Clifford Nass. 2007. The influence of user tailoring and cognitive load on user performance in spoken dialogue systems. In Proc. of the 10th International Conference of Spoken Language Processing (Interspeech/ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>H Ye</author>
</authors>
<title>The Hidden Information State Approach to Dialog Management.</title>
<date>2007</date>
<booktitle>In ICASSP</booktitle>
<contexts>
<context position="1593" citStr="Young et al., 2007" startWordPosition="253" endWordPosition="256">ous work in this area. The learned policy significantly outperforms all the prior approaches. 1 Introduction Natural language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and “conversation as action under uncertainty” </context>
<context position="16960" citStr="Young et al., 2007" startWordPosition="2792" endWordPosition="2795"> proof-of-concept study where we train our policy in a simulated learning environment based on the results from the MATCH corpus analysis in Section 4. Simulation-based RL allows to explore unseen actions which are not in the data, and thus less initial data is needed (Rieser and Lemon, 2008b). Note, that we cannot directly learn from the MATCH data, as therefore we would need data from an interactive dialogue. We are currently collecting such data in a Wizard-of-Oz experiment. 6.1 User simulation User simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007). A user simulation for NLG is very similar, in that it is a predictive model of the most likely next user act. However, this user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, the NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. It also tells us the probability whether the user chooses to “barge-in” after a system NLG action (by either choosing an item or providing more information). The user simulation for this study is a simple bi-gram model, whi</context>
<context position="22269" citStr="Young et al., 2007" startWordPosition="3698" endWordPosition="3701"> generation states. We trained the policy using the well known SARSA algorithm, using linear function approximation (Sutton and Barto, 1998). The policy was trained for 3600 simulated NLG sequences. In future work we plan to learn lower level decisions, such as lexical adaptation based on the vocabulary used by the user. 6.5 Baselines We derive the baseline policies from Information Presentation strategies as deployed by current dialogue systems. In total we utilise 7 different baselines (B1-B7), which correspond to single branches in our policy space (see Figure 1): B1: RECOMMEND only, e.g. (Young et al., 2007) B2: COMPARE only, e.g. (Henderson et al., 2008) B3: SUMMARY only, e.g. (Polifroni and Walker, 2008) B4: SUMMARY followed by RECOMMEND, e.g. (Whittaker et al., 2002) B5: Randomly choosing between COMPARE and RECOMMEND, e.g. (Walker et al., 2007) 688 ⎡ ⎡SUMMARY ⎤ state: ⎡ attributesr |1 |-119 |: 0,1 ⎢ ⎢ ⎢ ⎢ ⎢ ⎢action: ⎢ ⎢COMPARE ⎦⎥⎥⎥ sentence: { 1-11 } l o ⎢ ⎢ ⎣ ⎢ ⎣RECOMMEND userGoal: 0,1oJ ⎥ ⎥ end ⎥ ⎥ ⎣n o ⎦ ⎦ userQuit: 0,1 Figure 3: State-Action space for RL-NLG B6: Randomly choosing between all 7 outputs B7: Always generating whole sequence, i.e. SUMMARY+COMPARE+RECOMMEND, as suggested by th</context>
</contexts>
<marker>Young, Schatzmann, Weilhammer, Ye, 2007</marker>
<rawString>SJ Young, J Schatzmann, K Weilhammer, and H Ye. 2007. The Hidden Information State Approach to Dialog Management. In ICASSP 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>