<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.996228">
Applying Machine Learning to Chinese Temporal Relation Resolution
</title>
<author confidence="0.996882">
Wenjie Li
</author>
<affiliation confidence="0.9379955">
Department of Computing
The Hong Kong Polytechnic University, Hong Kong
</affiliation>
<email confidence="0.923602">
cswjli@comp.polyu.edu.hk
</email>
<author confidence="0.997307">
Guihong Cao
</author>
<affiliation confidence="0.9668725">
Department of Computing
The Hong Kong Polytechnic University, Hong Kong
</affiliation>
<email confidence="0.971404">
csghcao@comp.polyu.edu.hk
</email>
<author confidence="0.989067">
Kam-Fai Wong
</author>
<affiliation confidence="0.981262333333333">
Department of Systems Engineering and Engineering
Management
The Chinese University of Hong Kong, Hong Kong
</affiliation>
<email confidence="0.972788">
kfwong@se.cuhk.edu.hk
</email>
<author confidence="0.989636">
Chunfa Yuan
</author>
<affiliation confidence="0.996912">
Department of Computer Science and Technology
Tsinghua University, Beijing, China.
</affiliation>
<email confidence="0.986422">
cfyuan@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.995534" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930315789474">
Temporal relation resolution involves extraction
of temporal information explicitly or implicitly
embedded in a language. This information is of-
ten inferred from a variety of interactive gram-
matical and lexical cues, especially in Chinese.
For this purpose, inter-clause relations (tempo-
ral or otherwise) in a multiple-clause sentence
play an important role. In this paper, a computa-
tional model based on machine learning and
heterogeneous collaborative bootstrapping is
proposed for analyzing temporal relations in a
Chinese multiple-clause sentence. The model
makes use of the fact that events are represented
in different temporal structures. It takes into ac-
count the effects of linguistic features such as
tense/aspect, temporal connectives, and dis-
course structures. A set of experiments has been
conducted to investigate how linguistic features
could affect temporal relation resolution.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979246753247">
In language studies, temporal information de-
scribes changes and time of changes expressed in a
language. Such information is critical in many typi-
cal natural language processing (NLP) applications,
e.g. language generation and machine translation, etc.
Modeling temporal aspects of an event in a written
text is more complex than capturing time in a physi-
cal time-stamped system. Event time may be speci-
fied explicitly in a sentence, e.g. “他们在 1997 年解
决了该市的交通问题 (They solved the traffic prob-
lem of the city in 1997)”; or it may be left implicit, to
be recovered by readers from context. For example,
one may know that “修成立交桥以后,他们解决了该
市的交通问题 (after the street bridge had been built,
they solved the traffic problem of the city)”, yet
without knowing the exact time when the street
bridge was built. As reported by Partee (Partee,
1984), the expression of relative temporal relations
in which precise times are not stated is common in
natural language. The objective of relative temporal
relation resolution is to determine the type of rela-
tive relation embedded in a sentence.
In English, temporal expressions have been
widely studied. Lascarides and Asher (Lascarides,
Asher and Oberlander, 1992) suggested that tempo-
ral relations between two events followed from dis-
course structures. They investigated various
contextual effects on five discourse relations
(namely narration, elaboration, explanation, back-
ground and result) and then corresponded each of
them to a kind of temporal relations. Hitzeman et al.
(Hitzeman, Moens and Grover, 1995) described a
method for analyzing temporal structure of a dis-
course by taking into account the effects of tense,
aspect, temporal adverbials and rhetorical relations
(e.g. causation and elaboration) on temporal order-
ing. They argued that rhetorical relations could be
further constrained by event temporal classification.
Later, Dorr and Gaasterland (Dorr and Gaasterland,
2002) developed a constraint-based approach to
generate sentences, which reflect temporal relations,
by making appropriate selections of tense, aspect
and connecting words (e.g. before, after and when).
Their works, however, are theoretical in nature and
have not investigated computational aspects.
The pioneer work on Chinese temporal relation
extraction was first reported by Li and Wong (Li and
Wong, 2002). To discover temporal relations em-
bedded in a sentence, they devised a set of simple
rules to map the combined effects of temporal indi-
cators, which are gathered from different grammati-
cal categories, to their corresponding relations.
However, their work did not focus on relative tem-
poral relations. Given a sentence describing two
temporally related events, Li and Wong only took
the temporal position words (including before, after
and when, which serve as temporal connectives) and
the tense/aspect markers of the second event into
consideration. The proposed rule-based approach
was simple; but it suffered from low coverage and
was particularly ineffective when the interaction be-
tween the linguistic elements was unclear.
This paper studies how linguistic features in Chi-
nese interact to influence relative relation resolution.
For this purpose, statistics-based machine learning
approaches are applied. The remainder of the paper
is structured as follows: Section 2 summarizes the
linguistic features, which must be taken into account
in temporal relation resolution, and introduces how
these features are expressed in Chinese. In Section 3,
the proposed machine learning algorithms to identify
temporal relations are outlined; furthermore, a het-
erogeneous collaborative bootstrapping technique
for smoothing is presented. Experiments designed
for studying the impact of different approaches and
linguistic features are described in Section 4. Finally,
Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.977852" genericHeader="method">
2 Modeling Temporal Relations
</sectionHeader>
<subsectionHeader confidence="0.994635">
2.1 Temporal Relation Representations
</subsectionHeader>
<bodyText confidence="0.999819058823529">
As the importance of temporal information proc-
essing has become apparent, a variety of temporal
systems have been introduced, attempting to ac-
commodate the characteristics of relative temporal
information. Among those who worked on temporal
relation representations, many took the work of Rei-
chenbach (Reichenbach, 1947) as a starting point,
while some others based their works on Allen’s (Al-
len, 1981).
Reichenbach proposed a point-based temporal
theory. This was later enhanced by Bruce who de-
fined seven relative temporal relations (Bruce. 1972).
Given two durative events, the interval relations be-
tween them were modeled by the order between the
greatest lower bounding points and least upper
bounding points of the two events. In the other camp,
instead of adopting time points, Allen took intervals
as temporal primitives and introduced thirteen basic
binary relations. In this interval-based theory, points
are relegated to a subsidiary status as ‘meeting
places’ of intervals. An extension to Allen’s theory,
which treated both points and intervals as primitives
on an equal footing, was later investigated by Ma
and Knight (Ma and Knight, 1994).
In natural language, events can either be punctual
(e.g. 爆炸 (explore)) or durative (e.g. 盖楼 (built a
house)) in nature. Thus Ma and Knight’s model is
adopted in our work (see Figure 1). Taking the sen-
tence “修成立交桥以后,他们解决了该市的交通问题
(after the street bridge had been built, they solved
the traffic problem of the city)” as an example, the
relation held between building the bridge (i.e. an
interval) and solving the problem (i.e. a point) is
BEFORE.
</bodyText>
<figure confidence="0.682575444444445">
BEFORE/AFTER
MEETS/MET-BY
OVERLAPS/OVERLAPPED-BY
STARTS/STARTED-BY
DURING/CONTAINS
FINISHES/FINISHED-BY
SAME-AS
A punctual event (i.e. represented in time point)
A durative event (i.e. represented in time interval)
</figure>
<figureCaption confidence="0.993177">
Figure 1. Thirteen temporal relations between points and
</figureCaption>
<bodyText confidence="0.509697">
intervals
</bodyText>
<subsectionHeader confidence="0.987154">
2.2 Linguistic Features for Determining Relative
Relations
</subsectionHeader>
<bodyText confidence="0.999923108108108">
Relative relations are generally determined by
tense/aspect, connecting words (temporal or other-
wise) and event classes.
Tense/Aspect in English is manifested by verb in-
flections. But such morphological variations are in-
applicable to Chinese verbs; instead, they are
conveyed lexically (Li and Wong, 2002). In other
words, tense and aspect in Chinese are expressed
using a combination of time words, auxiliaries, tem-
poral position words, adverbs and prepositions, and
particular verbs.
Temporal Connectives in English primarily in-
volve conjunctions, e.g. after, before and when (Dorr
and Gaasterland, 2002). They are key components in
discourse structures. In Chinese, however, conjunc-
tions, conjunctive adverbs, prepositions and position
words are required to represent connectives. A few
verbs which express cause and effect also imply a
forward movement of event time. The words, which
contribute to the tense/aspect and temporal connec-
tive expressions, are explicit in a sentence and gen-
erally known as Temporal Indicators.
Event Class is implicit in a sentence. Events can
be classified according to their inherent temporal
characteristics, such as the degree of telicity and/or
atomicity (Li and Wong, 2002). The four widespread
accepted temporal classes1 are state, process, punc-
tual event and developing event. Based on their
classes, events interact with the tense/aspect of verbs
to define the temporal relations between two events.
Temporal indicators and event classes are together
referred to as Linguistic Features (see Table 1). For
example, linguistic features are underlined in the
sentence “(因为)修成立交桥(以后),他们解决了该市
的交通问题 after/because the street bridge had been
built (i.e. a developing event), they solved the traffic
problem of the city (i.e. a punctual event)”.
</bodyText>
<table confidence="0.992889142857143">
1 Temporal classification refers to aspectual classification.
Linguistic Feature Symbol POS Tag Effect Example
With/Without PT Not Applica- Not Applicable Not Applicable
punctuations ble
Speech verbs VS TI_vs Tense � �T, �
Trend verbs TR TI_tr Aspect k�, _F1
Preposition words P TI_p Discourse Structure/Aspect n, X A
Position words PS TI_f Discourse Structure A, r, ffAh,
Verbs with verb VV TI_vv Tense/Aspect A, ATf, �
objects
Verbs expressing VA TI_va Tense ��, �k, 7
wish/hope
Verbs related to VC TI_vc Discourse Structure �(, (�, �jt�
causality
Conjunctive words C TI_c Discourse Structure 4, Ali, T31
Auxiliary words U TI_u Aspect *, -T, 31
Time words T TI_t Tense 311, fir, fi�
Adverbs D TI_d Tense/Aspect/Discourse Structure IR, 4, A*, �
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event,
Developing Event,
Process
</table>
<tableCaption confidence="0.999891">
Table 1. Linguistic features: eleven temporal indicators and one event class
</tableCaption>
<bodyText confidence="0.998025">
Table 1 shows the mapping between a temporal
indicator and its effects. Notice that the mapping is
not one-to-one. For example, adverbs affect
tense/aspect as well as discourse structure. For an-
other example, tense/aspect can be affected by auxil-
iary words, trend verbs, etc. This shows that
classification of temporal indicators based on part-
of-speech (POS) information alone cannot determine
relative temporal relations.
</bodyText>
<sectionHeader confidence="0.976673" genericHeader="method">
3 Machine Learning Approaches for Relative
Relation Resolution
</sectionHeader>
<bodyText confidence="0.999864789473684">
Previous efforts in corpus-based natural language
processing have incorporated machine learning
methods to coordinate multiple linguistic features
for example in accent restoration (Yarowsky, 1994)
and event classification (Siegel and McKeown,
1998), etc.
Relative relation resolution can be modeled as a
relation classification task. We model the thirteen
relative temporal relations (see Figure 1) as the
classes to be decided by a classifier. The resolution
process is to assign an event pair (i.e. the two events
under concern)2 to one class according to their lin-
guistic features. For this purpose, we train two clas-
sifiers, a Probabilistic Decision Tree Classifier
(PDT) and a Naïve Bayesian Classifier (NBC). We
then combine the results by the Collaborative Boot-
strapping (CB) technique which is used to mediate
the sparse data problem arose due to the limited
number of training cases.
</bodyText>
<footnote confidence="0.552739">
2 It is an object in machine learning algorithms.
</footnote>
<subsectionHeader confidence="0.994476">
3.1 Probabilistic Decision Tree (PDT)
</subsectionHeader>
<bodyText confidence="0.999930294117647">
Due to two domain-specific characteristics, we
encounter some difficulties in classification. (a) Un-
known values are common, for many events are
modified by less than three linguistic features. (b)
Both training and testing data are noisy. For this rea-
son, it is impossible to obtain a tree which can com-
pletely classify all training examples. To overcome
this predicament, we aim to obtain more adjusted
probability distributions of event pairs over their
possible classes. Therefore, a probabilistic decision
tree approach is preferred over conventional deci-
sion tree approaches (e.g. C4.5, ID3). We adopt a
non-incremental supervised learning algorithm in
TDIDT (Top Down Induction of Decision Trees)
family. It constructs a tree top-down and the process
is guided by distributional information learned from
examples (Quinlan, 1993).
</bodyText>
<subsectionHeader confidence="0.498197">
3.1.1 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999987833333333">
Based on probabilities, each object in the PDT ap-
proach can belong to a number of classes. These
probabilities could be estimated from training cases
with Maximum Likelihood Estimation (MLE). Let l
be the decision sequence, z the object and c the class.
The probability of z belonging to c is:
</bodyText>
<equation confidence="0.994946833333333">
p(c|z)=E Al, c |z) - Ep c l p l z
(  |) (  |)
l l
let l= B1 B2 ... Bn , by MLE we have:
p(c |l) - p(c  |BJ = f (c, Bn) (2)
f(Bn)
</equation>
<bodyText confidence="0.81684">
f (c, Bn) is the count of the items whose leaf nodes
are Bn and belonging to class c. And
</bodyText>
<equation confidence="0.973174">
(1)
(3)
</equation>
<bodyText confidence="0.97294075">
An object might traverse more than one decision
path if it has unknown attribute values.
f(BmBm−1Bm−2 ... B1  |z) is the count of the item z,
which owns the decision paths from B1 to Bm.
</bodyText>
<subsectionHeader confidence="0.550548">
3.1.2 Classification Attributes
</subsectionHeader>
<bodyText confidence="0.999778190476191">
Objects are classified into classes based on their
attributes. In the context of temporal relation resolu-
tion, how to categorize linguistic features into classi-
fication attributes is a major design issue. We extract
all temporal indicators surrounding an event. As-
sume m and n are the anterior and posterior window
size. They represent the numbers of the indicators
BEFORE and AFTER respectively. Consider the
most extreme case where an event consists of at
most 4 temporal indicators before and 2 after. We
set m and n to 4 and 2 initially. Experiments show
that learning performance drops when m&gt;4 and n&gt;2
and there is only very little difference otherwise (i.e.
when m≤4 and n≤2).
In addition to temporal indicators alone, the posi-
tion of the punctuation mark separating the two
clauses describing the events and the classes of the
events are also useful classification attributes. We
will outline why this is so in Section 4.1. Altogether,
the following 15 attributes are used to train the PDT
and NBC classifiers:
</bodyText>
<table confidence="0.989896833333333">
TI TI TI TI class e TI TI r2 ,
l , , , , ( ), , e1
l l l r
4 3 2 1 1
e e e e 1 e
1 1 1 1 1
</table>
<equation confidence="0.978296285714286">
l 2
l l l r r
wi / , , , , , ( ), ,
wo punc TI TI TI TI class e TI TI
4 3 2 1 1
e e e e 2 e e
2 2 2 2 2 2
</equation>
<bodyText confidence="0.9979645">
li (i=1,2,3,4) and rj (j=1,2) are the ith indictor before
and the jth indicator after the event ek (k=1,2). Given
a sentence, for example, 5&amp;quot;c/TI_d �/E0 _T/TI_u �T-
/n ,/w A-/TI_d T,0/E2 _T/TI_u IRiI/n o/w, the at-
tribute vector could be represented as: [0, 0, 0, 5&amp;quot;c,
E0, _T, 0, 1, 0, 0, 0, A-, E2, _T, 0].
</bodyText>
<subsectionHeader confidence="0.811483">
3.1.3 Attribute Selection Function
</subsectionHeader>
<bodyText confidence="0.965965">
Many similar attribute selection functions were
used to construct a decision tree (Marquez, 2000).
These included information gain and information
gain ratio (Quinlan, 1993), 2
χ Test and Symmetrical
Tau (Zhou and Dillon, 1991). We adopt the one pro-
posed by Lopez de Mantaraz (Mantaras, 1991) for it
shows more stable performance than Quinlan’s
information gain ratio in our experiments. Compared
with Quinlan’s information gain ratio, Lopez’s dis-
tance-based measurement is unbiased towards the
attributes with a large number of values and is capa-
ble of generating smaller trees with no loss of accu-
racy (Marquez, Padro and Rodriguez, 2000). This
characteristic makes it an ideal choice for our work,
where most attributes have more than 200 values.
</bodyText>
<subsectionHeader confidence="0.997271">
3.2 Naïve Bayesian Classifier (NBC)
</subsectionHeader>
<bodyText confidence="0.999723375">
NBC assumes independence among features.
Given the class label c, NBC learns from training
data the conditional probability of each attribute Ai
(see Section 3.1.2). Classification is then performed
by applying Bayes rule to compute the probability of
c given the particular instance of A1,...,An, and then
predicting the class with the highest posterior prob-
ability ratio.
</bodyText>
<equation confidence="0.898556375">
c
p(Ai  |c) and p(Ai  |c) are estimated by MLE from
training data with Dirichlet Smoothing method:
p(Ai  |c)= n c (Ai,c)+u
(8)
n
c
c(Ai,c)+u
</equation>
<subsectionHeader confidence="0.99297">
3.3 Collaborative Bootstrapping (CB)
</subsectionHeader>
<bodyText confidence="0.998772176470588">
PDT and NB are both supervised learning ap-
proach. Thus, the training processes require many
labeled cases. Recent results (Blum and Mitchell,
1998; Collins, 1999) have suggested that unlabeled
data could also be used effectively to reduce the
amount of labeled data by taking advantage of col-
laborative bootstrapping (CB) techniques. In previ-
ous works, CB trained two homogeneous classifiers
based on different independent feature spaces. How-
ever, this approach is not applicable to our work
since only a few temporal indicators occur in each
case. Therefore, we develop an alternative CB algo-
rithm, i.e. to train two different classifiers based on
the same feature spaces. PDT (a non-linear classifier)
and NBC (a linear classifier) are under consideration.
This is inspired by Blum and Mitchell’s theory that
two collaborative classifiers should be conditionally
</bodyText>
<figure confidence="0.938093827586207">
*
arg max (  |1, 2, 3
A A A
c = score c ,..., n
A
)
(  |, ,
c A A A
1 2 3
p
(  |, , ,...,
c A A A A
1 2 3 n
p
,
A
..
,.
)
n
score(c  |A1, A2, A3 ,..., An) =
p c A A A A
(  |, , ,...,
1 2 3
Apply Bayesian rule to (5), we have:
)
|
n
∏
</figure>
<equation confidence="0.885924507936508">
c)p ( c)
p(Ai|c)p(c)
p(A1, A2 , A3 ,..., An
score(c  |A1, A2, A3 ,..., An) =
p(c  |A1, A2,A3,...,A
i
n
)
1
=
n
∏
c )p ( c)
p(Ai |
)p(
c
|
=
1
p(A1 , A2, A3,..., An
i
=
) ≈
(6)
(5)
n
)
p (l  |z) = p(B1  |) ( 2  |B1 ,z)p(B3  |B1 , B2, z)
z p B
...p(Bn  |Bn− 1 ...B1 , z )
where
p(Bm  |Bm
−1Bm−
( B B B ... B
m m − 1 m − 2 1
f
( B B ... B
m − 1 m − 2 1
p B B B ... B
( m m − 1 m − 2 1
p B B ... B
( m−1 m−2 1
z) , (m = 2,3, ... , n).
|
=
f
2 ...B1, z) =
|
z)
|
z
)
|
z
)
(4)
p(Ai  |c) = n (7)
∑ c A c u n
( , )+ ×
jj=1
∑ c(Aj,c)+u
×
j=1
</equation>
<bodyText confidence="0.993508285714286">
independent so that each classifier can make its own
contribution (Blum and Mitchell, 1998). The learn-
ing steps are outlined in Figure 2.
Inputs: A collection of the labeled cases and unla-
beled cases is prepared. The labeled cases
are separated into three parts, training
cases, test cases and held-out cases.
</bodyText>
<footnote confidence="0.730783533333333">
Loop: While the breaking criteria is not satisfied
1 Build the PDT and NBC classifiers us-
ing training cases
2 Use PDT and NBC to classify the unla-
beled cases, and exchange with the se-
lected cases which have higher
Classification Confidence (i.e. the un-
certainty is less than a threshold).
3 Evaluate the PDT and NBC classifiers
with the held-out cases. If the error rate
increases or its reduction is below a
threshold break the loop; else go to step
1.
Output: Use the optimal classifier to label the test
cases
</footnote>
<figureCaption confidence="0.999062">
Figure 2. Collaborative bootstrapping algorithm
</figureCaption>
<subsectionHeader confidence="0.964228">
3.4 Classification Confidence Measurement
</subsectionHeader>
<bodyText confidence="0.99928125">
Classification confidence is the metric used to
measure the correctness of each labeled case auto-
matically (see Step 2 in Figure 2). The desirable
metric should satisfy two principles:
</bodyText>
<listItem confidence="0.999036">
• It should be able to measure the uncertainty/ cer-
tainty of the output of the classifiers; and
• It should be easy to calculate.
</listItem>
<bodyText confidence="0.998878">
We adopt entropy, i.e. an information theory
based criterion, for this purpose. Let x be the classi-
fied object, and C = {c1, c2, c3,..., cn } the set of output.
</bodyText>
<equation confidence="0.982029222222222">
x is classified as ci with the probability
p(ci  |x) i =1,2,3, .. , n . The entropy of the output is
then calculated as:
n
e C x
(  |) = − ∑ p ci x p ci x
(  |) log (  |) (9)
i = 1
Once p(ci  |x) is known, the entropy can be deter-
</equation>
<bodyText confidence="0.991091833333333">
mined. These parameters can be easily determined in
PDT, as each incoming case is classified into each
class with a probability. However, the incoming
cases in NBC are grouped into one class which is
assigned the highest score. We then have to estimate
p(ci  |x) from those scores. Without loss of general-
</bodyText>
<equation confidence="0.95104825">
ity, the probability is estimated as:
p(ci  |x) = nscore(ci  |x)
(10)
∑ score(cj  |x)
</equation>
<bodyText confidence="0.999413">
where score(ci  |x) is the ranking score of x be-
longing to ci.
</bodyText>
<sectionHeader confidence="0.876114" genericHeader="evaluation">
4 Experiment Setup and Evaluation
</sectionHeader>
<bodyText confidence="0.999877555555556">
Several experiments have been designed to evalu-
ate the proposed learning approaches and to reveal
the impact of linguistic features on learning per-
formance. 700 sentences are extracted from Ta Kong
Pao (a local Hong Kong Chinese newspaper) finan-
cial version. 600 cases are labeled manually and 100
left unlabeled. Among those labeled, 400 are used as
training data, 100 as test data and the rest as held-out
data.
</bodyText>
<subsectionHeader confidence="0.9987055">
4.1 Use of Linguistic Features As Classification
Attributes
</subsectionHeader>
<bodyText confidence="0.98878785106383">
The impact of a temporal indicator is determined
by its position in a sentence. In PDT and NBC, we
consider an indicator located in four positions: (1)
BEFORE the first event; (2) AFTER the first event
and BEFORE the second and it modifies the first
event; (3) the same as (2) but it modifies the second
event; and (4) AFTER the second event. Cases (2)
and (3) are ambiguous. The positions of the temporal
indicators are the same. But it is uncertain whether
these indicators modify the first or the second event
if there is no punctuation separating their roles. We
introduce two methods, namely NA and SAP to
check if the ambiguity affects the two learning ap-
proaches.
N(atural) O(rder): the temporal indicators between
the two events are extracted and compared accord-
ing to their occurrence in the sentences regardless
which event they modify.
S(eparate) A(uxiliary) and P(osition) words: we
try to resolve the above ambiguity with the gram-
matical features of the indicators. In this method,
we assume that an indicator modifies the first
event if it is an auxiliary word (e.g. T), a trend
verb (e.g. 起来) or a position word (e.g. u); oth-
erwise it modifies the second event.
Temporal indicators are either tense/aspect or con-
nectives (see Section 2.2). Intuitively, it seems that
classification could be better achieved if connective
features are isolated from tense/ aspect features,
allowing like to be compared with like. Methods
SC1 and SC2 are designed based on this assumption.
Table 2 shows the effect the different classification
methods.
SC1 (Separate Connecting words 1): it separates
conjunctions and verbs relating to causality from
others. They are assumed to contribute to dis-
course structure (intra- or inter-sentence structure),
and the others contribute to the tense/aspect ex-
pressions for each individual event. They are built
into 2 separate attributes, one for each event.
j =1
SC2 (Separate Connecting words 2): it is the same
as SC1 except that it combines the connecting
word pairs (i.e. as a single pattern) into one attrib-
ute.
EC (Event Class): it takes event classes into con-
sideration.
</bodyText>
<table confidence="0.998644714285714">
Method Accuracy
PDT NBC
NO 82.00% 81.00%
SAP 82.20% 81.50%
SAP +SC1 80.20% 78.00%
SAP +SC2 81.70% 79.20%
SAP +EC 85.70% 82.25%
</table>
<tableCaption confidence="0.9956655">
Table 2. Effect of encoding linguistic features in the dif-
ferent ways
</tableCaption>
<subsectionHeader confidence="0.991722">
4.2 Impact of Individual Features
</subsectionHeader>
<bodyText confidence="0.999549111111111">
From linguistic perspectives, 13 features (see Ta-
ble 1) are useful for relative relation resolution. To
examine the impact of each individual feature, we
feed a single linguistic feature to the PDT learning
algorithm one at a time and study the accuracy of the
resultant classifier. The experimental results are
given in Table 3. It shows that event classes have
greatest accuracy, followed by conjunctions in the
second place, and adverbs in the third.
</bodyText>
<table confidence="0.999787375">
Feature Accuracy Feature Accuracy
PT 50.5% VA 56.5%
VS 54% C 62%
VC 54% U 51.5%
TR 50.5% T 57.2%
P 52.2 % D 61.7%
PS 58.7% EC 68.2%
VS 51.2% None 50.5%
</table>
<tableCaption confidence="0.999405">
Table 3. Impact of individual linguistic features
</tableCaption>
<subsectionHeader confidence="0.983788">
4.3 Discussions
</subsectionHeader>
<bodyText confidence="0.99996675">
Analysis of the results in Tables 2 and 3 reveals
some linguistic insights:
1.In a situation where temporal indicators appear
between two events and there is no punctuation
mark separating them, POS information help re-
duce the ambiguity. Compared with NO, SAP
shows a slight improvement from 82% to 82.2%.
But the improvement seems trivial and is not as
good as our prediction. This might due to the
small percent of such cases in the corpus.
2.Separating conjunctions and verbs relating to
causality from others is ineffective. This reveals
the complexity of Chinese in connecting expres-
sions. It is because other words (such as adverbs,
proposition and position words) also serve such
a function. Meanwhile, experiments based on
SC1 and SC2 suggest that the connecting ex-
pressions generally involve more than one word
or phrase. Although the words in a connecting
expression are separated in a sentence, the action
is indeed interactive. It would be more useful to
regard them as one attribute.
3.The effect of event classification is striking.
Taking this feature into account, the accuracies
of both PDT and NB improved significantly. As
a matter of fact, different event classes may in-
troduce different relations even if they are con-
strained by the same temporal indicators.
</bodyText>
<subsectionHeader confidence="0.995976">
4.4 Collaborative Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999976777777778">
Table 4 presents the evaluation results of the four
different classification approaches. DM is the default
model, which classifies all incoming cases as the
most likely class. It is used as evaluation baseline.
Compare with DM, PDT and NBC show improve-
ment in accuracy (i.e. above 60% improvement).
And CB in turn outperforms PDT and NBC. This
proves that using unlabeled data to boost the per-
formance of the two classifiers is effective.
</bodyText>
<table confidence="0.998390166666667">
Approach Accuracy
Close test Open test
DM 50.50% 55.00%
NBC 82.25% 72.00%
PDT 85.70% 74.00%
CB 88.70% 78.00%
</table>
<tableCaption confidence="0.999927">
Table 4. Evaluation of NBC, PDT and CB approaches
</tableCaption>
<sectionHeader confidence="0.998666" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99996478125">
Relative temporal relation resolution received
growing attentions in recent years. It is important for
many natural language processing applications, such
as information extraction and machine translation.
This topic, however, has not been well studied, es-
pecially in Chinese. In this paper, we propose a
model for relative temporal relation resolution in
Chinese. Our model combines linguistic knowledge
and machine learning approaches. Two learning ap-
proaches, namely probabilistic decision tree (PDT)
and naive Bayesian classifier (NBC) and 13 linguis-
tic features are employed. Due to the limited labeled
cases, we also propose a collaborative bootstrapping
technique to improve learning performance. The
experimental results show that our approaches are
encouraging. To our knowledge, this is the first at-
tempt of collaborative bootstrapping, which involves
two heterogeneous classifiers, in NLP application.
This lays down the main contribution of our research.
In this pilot work, temporal indicators are selected
based on linguistic knowledge. It is time-consuming
and could be error-prone. This suggests two direc-
tions for future studies. We will try to automate or at
least semi-automate feature selection process. An-
other future work worth investigating is temporal
indicator clustering. There are two methods we
could investigate, i.e. clustering the recognized indi-
cators which occur in training corpus according to
co-occurrence information or grouping them into
two semantic roles, one related to tense/aspect ex-
pressions and the other to connecting expressions
between two events.
</bodyText>
<sectionHeader confidence="0.996478" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.936565">
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong
(RGC reference number PolyU5085/02E) and
CUHK Strategic Grant (account number 4410001).
</bodyText>
<sectionHeader confidence="0.995984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891216216216">
Allen J., 1981. An Interval-based Represent Action
of Temporal Knowledge. In Proceedings of 7th In-
ternational Joint Conference on Artificial Intelli-
gence, pages 221-226. Los Altos, CA.
Blum, A. and Mitchell T., 1998. Combining Labeled
and Unlabeled Data with Co-Training. In Proceed-
ings of the Eleventh Annual Conference on Com-
putational Learning Theory, Madison, Wisconsin,
pages 92-100
Bruce B., 1972. A Model for Temporal References
and its Application in Question-Answering Pro-
gram. Artificial Intelligence, 3(1):1-25.
Collins M. and Singer Y, 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceed-
ings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 189-196. Uni-
versity of Maryland.
Dorr B. and Gaasterland T., 2002. Constraints on the
Generation of Tense, Aspect, and Connecting
Words from Temporal Expressions. (submitted to
JAIR)
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of
Discourse. In Proceedings of the 7th European
Meeting of the Association for Computational
Linguistics, pages 253-260. Dublin, Ireland.
Lascarides A., Asher N. and Oberlander J., 1992.
Inferring Discourse Relations in Context. In
Proceedings of the 30th Meeting of the
Association for Computational Linguistics, pages
1-8, Newark, Del.
Li W.J. and Wong K.F., 2002. A Word-based Ap-
proach for Modeling and Discovering Temporal
Relations Embedded in Chinese Sentences, ACM
Transaction on Asian Language Processing,
1(3):173-206.
Ma J. and Knight B., 1994. A General Temporal
Theory. The Computer Journal, 37(2):114- 123.
Màntaras L., 1991. A Distance-based Attribute Se-
lection Measure for Decision Tree Induction. Ma-
chine Learning, 6(1): 81–92.
Màrquez L., Padró L. and Rodríguez H., 2000. A
Machine Learning Approach to POS Tagging.
Machine Learning, 39(1):59-91. Kluwer Aca-
demic Publishers.
Partee, B., 1984. Nominal and Temporal Anaphora.
Linguistics and Philosophy, 7(3):287-324.
Quinlan J., 1993. C4.5 Programs for Machine
Learning. Morgan Kauman Press.
Reichenbach H., 1947. Elements of Symbolic Logic.
Berkeley CA, University of California Press.
Siegel E. and McKeown K., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving
Aspectual Classification and Revealing Linguistic
Insights. Computational Linguistics, 26(4): 595-
627.
Wiebe, J.M., O&apos;Hara, T.P., Ohrstrom-Sandgren, T.
and McKeever, K.J, 1998. An Empirical Approach
to Temporal Reference Resolution. Journal of Ar-
tificial Intelligence Research, 9:247-293.
Wong F., Li W., Yuan C., etc., 2002. Temporal Rep-
resentation and Classification in Chinese. Interna-
tional Journal of Computer Processing of Oriental
Languages, 15(2):211-230.
Yarowsky D., 1994. Decision Lists for Lexical Am-
biguity Resolution: Application to the Accent Res-
toration in Spanish and French. In Proceeding of
the 32rd Annual Meeting of ACL, San Francisco,
CA.
Zhou X., Dillon T., 1991. A Statistical-heuristic Fea-
ture Selection Criterion for Decision Tree Induc-
tion. IEEE Transaction on Pattern Analysis and
Machine Intelligence, 13(8): 834-841.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.100947">
<title confidence="0.999832">Applying Machine Learning to Chinese Temporal Relation Resolution</title>
<author confidence="0.997726">Wenjie Li</author>
<affiliation confidence="0.998041">Department of Computing</affiliation>
<address confidence="0.724848">The Hong Kong Polytechnic University, Hong Kong</address>
<email confidence="0.984964">cswjli@comp.polyu.edu.hk</email>
<author confidence="0.993955">Guihong Cao</author>
<affiliation confidence="0.996396">Department of Computing</affiliation>
<address confidence="0.723062">The Hong Kong Polytechnic University, Hong Kong</address>
<email confidence="0.984595">csghcao@comp.polyu.edu.hk</email>
<author confidence="0.997853">Kam-Fai Wong</author>
<affiliation confidence="0.577690666666667">Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong, Hong Kong</affiliation>
<email confidence="0.857567">kfwong@se.cuhk.edu.hk</email>
<author confidence="0.998838">Chunfa Yuan</author>
<affiliation confidence="0.8659665">Department of Computer Science and Technology Tsinghua University, Beijing, China.</affiliation>
<email confidence="0.982212">cfyuan@tsinghua.edu.cn</email>
<abstract confidence="0.99891015">Temporal relation resolution involves extraction of temporal information explicitly or implicitly embedded in a language. This information is often inferred from a variety of interactive grammatical and lexical cues, especially in Chinese. For this purpose, inter-clause relations (temporal or otherwise) in a multiple-clause sentence play an important role. In this paper, a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relations in a Chinese multiple-clause sentence. The model makes use of the fact that events are represented in different temporal structures. It takes into account the effects of linguistic features such as tense/aspect, temporal connectives, and discourse structures. A set of experiments has been conducted to investigate how linguistic features could affect temporal relation resolution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
</authors>
<title>An Interval-based Represent Action of Temporal Knowledge.</title>
<date>1981</date>
<booktitle>In Proceedings of 7th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>221--226</pages>
<location>Los Altos, CA.</location>
<contexts>
<context position="5754" citStr="Allen, 1981" startWordPosition="833" endWordPosition="835">d for studying the impact of different approaches and linguistic features are described in Section 4. Finally, Section 5 concludes the paper. 2 Modeling Temporal Relations 2.1 Temporal Relation Representations As the importance of temporal information processing has become apparent, a variety of temporal systems have been introduced, attempting to accommodate the characteristics of relative temporal information. Among those who worked on temporal relation representations, many took the work of Reichenbach (Reichenbach, 1947) as a starting point, while some others based their works on Allen’s (Allen, 1981). Reichenbach proposed a point-based temporal theory. This was later enhanced by Bruce who defined seven relative temporal relations (Bruce. 1972). Given two durative events, the interval relations between them were modeled by the order between the greatest lower bounding points and least upper bounding points of the two events. In the other camp, instead of adopting time points, Allen took intervals as temporal primitives and introduced thirteen basic binary relations. In this interval-based theory, points are relegated to a subsidiary status as ‘meeting places’ of intervals. An extension to </context>
</contexts>
<marker>Allen, 1981</marker>
<rawString>Allen J., 1981. An Interval-based Represent Action of Temporal Knowledge. In Proceedings of 7th International Joint Conference on Artificial Intelligence, pages 221-226. Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<location>Madison, Wisconsin,</location>
<contexts>
<context position="16100" citStr="Blum and Mitchell, 1998" startWordPosition="2516" endWordPosition="2519">arns from training data the conditional probability of each attribute Ai (see Section 3.1.2). Classification is then performed by applying Bayes rule to compute the probability of c given the particular instance of A1,...,An, and then predicting the class with the highest posterior probability ratio. c p(Ai |c) and p(Ai |c) are estimated by MLE from training data with Dirichlet Smoothing method: p(Ai |c)= n c (Ai,c)+u (8) n c c(Ai,c)+u 3.3 Collaborative Bootstrapping (CB) PDT and NB are both supervised learning approach. Thus, the training processes require many labeled cases. Recent results (Blum and Mitchell, 1998; Collins, 1999) have suggested that unlabeled data could also be used effectively to reduce the amount of labeled data by taking advantage of collaborative bootstrapping (CB) techniques. In previous works, CB trained two homogeneous classifiers based on different independent feature spaces. However, this approach is not applicable to our work since only a few temporal indicators occur in each case. Therefore, we develop an alternative CB algorithm, i.e. to train two different classifiers based on the same feature spaces. PDT (a non-linear classifier) and NBC (a linear classifier) are under co</context>
<context position="17664" citStr="Blum and Mitchell, 1998" startWordPosition="2894" endWordPosition="2897">) | n ∏ c)p ( c) p(Ai|c)p(c) p(A1, A2 , A3 ,..., An score(c |A1, A2, A3 ,..., An) = p(c |A1, A2,A3,...,A i n ) 1 = n ∏ c )p ( c) p(Ai | )p( c | = 1 p(A1 , A2, A3,..., An i = ) ≈ (6) (5) n ) p (l |z) = p(B1 |) ( 2 |B1 ,z)p(B3 |B1 , B2, z) z p B ...p(Bn |Bn− 1 ...B1 , z ) where p(Bm |Bm −1Bm− ( B B B ... B m m − 1 m − 2 1 f ( B B ... B m − 1 m − 2 1 p B B B ... B ( m m − 1 m − 2 1 p B B ... B ( m−1 m−2 1 z) , (m = 2,3, ... , n). | = f 2 ...B1, z) = | z) | z ) | z ) (4) p(Ai |c) = n (7) ∑ c A c u n ( , )+ × jj=1 ∑ c(Aj,c)+u × j=1 independent so that each classifier can make its own contribution (Blum and Mitchell, 1998). The learning steps are outlined in Figure 2. Inputs: A collection of the labeled cases and unlabeled cases is prepared. The labeled cases are separated into three parts, training cases, test cases and held-out cases. Loop: While the breaking criteria is not satisfied 1 Build the PDT and NBC classifiers using training cases 2 Use PDT and NBC to classify the unlabeled cases, and exchange with the selected cases which have higher Classification Confidence (i.e. the uncertainty is less than a threshold). 3 Evaluate the PDT and NBC classifiers with the held-out cases. If the error rate increases </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Blum, A. and Mitchell T., 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, Madison, Wisconsin, pages 92-100</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bruce</author>
</authors>
<title>A Model for Temporal References and its Application in Question-Answering Program.</title>
<date>1972</date>
<journal>Artificial Intelligence,</journal>
<pages>3--1</pages>
<marker>Bruce, 1972</marker>
<rawString>Bruce B., 1972. A Model for Temporal References and its Application in Question-Answering Program. Artificial Intelligence, 3(1):1-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>189--196</pages>
<institution>University of Maryland.</institution>
<marker>Collins, Singer, 1999</marker>
<rawString>Collins M. and Singer Y, 1999. Unsupervised Models for Named Entity Classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 189-196. University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
<author>T Gaasterland</author>
</authors>
<title>Constraints on the Generation of Tense, Aspect, and Connecting Words from Temporal Expressions.</title>
<date>2002</date>
<note>(submitted to JAIR)</note>
<contexts>
<context position="3386" citStr="Dorr and Gaasterland, 2002" startWordPosition="488" endWordPosition="491">tigated various contextual effects on five discourse relations (namely narration, elaboration, explanation, background and result) and then corresponded each of them to a kind of temporal relations. Hitzeman et al. (Hitzeman, Moens and Grover, 1995) described a method for analyzing temporal structure of a discourse by taking into account the effects of tense, aspect, temporal adverbials and rhetorical relations (e.g. causation and elaboration) on temporal ordering. They argued that rhetorical relations could be further constrained by event temporal classification. Later, Dorr and Gaasterland (Dorr and Gaasterland, 2002) developed a constraint-based approach to generate sentences, which reflect temporal relations, by making appropriate selections of tense, aspect and connecting words (e.g. before, after and when). Their works, however, are theoretical in nature and have not investigated computational aspects. The pioneer work on Chinese temporal relation extraction was first reported by Li and Wong (Li and Wong, 2002). To discover temporal relations embedded in a sentence, they devised a set of simple rules to map the combined effects of temporal indicators, which are gathered from different grammatical categ</context>
<context position="7900" citStr="Dorr and Gaasterland, 2002" startWordPosition="1150" endWordPosition="1153">g Relative Relations Relative relations are generally determined by tense/aspect, connecting words (temporal or otherwise) and event classes. Tense/Aspect in English is manifested by verb inflections. But such morphological variations are inapplicable to Chinese verbs; instead, they are conveyed lexically (Li and Wong, 2002). In other words, tense and aspect in Chinese are expressed using a combination of time words, auxiliaries, temporal position words, adverbs and prepositions, and particular verbs. Temporal Connectives in English primarily involve conjunctions, e.g. after, before and when (Dorr and Gaasterland, 2002). They are key components in discourse structures. In Chinese, however, conjunctions, conjunctive adverbs, prepositions and position words are required to represent connectives. A few verbs which express cause and effect also imply a forward movement of event time. The words, which contribute to the tense/aspect and temporal connective expressions, are explicit in a sentence and generally known as Temporal Indicators. Event Class is implicit in a sentence. Events can be classified according to their inherent temporal characteristics, such as the degree of telicity and/or atomicity (Li and Wong</context>
</contexts>
<marker>Dorr, Gaasterland, 2002</marker>
<rawString>Dorr B. and Gaasterland T., 2002. Constraints on the Generation of Tense, Aspect, and Connecting Words from Temporal Expressions. (submitted to JAIR)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hitzeman</author>
<author>M Moens</author>
<author>C Grover</author>
</authors>
<title>Algorithms for Analyzing the Temporal Structure of Discourse.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th European Meeting of the Association for Computational Linguistics,</booktitle>
<pages>253--260</pages>
<location>Dublin, Ireland.</location>
<marker>Hitzeman, Moens, Grover, 1995</marker>
<rawString>Hitzeman J., Moens M. and Grover C., 1995. Algorithms for Analyzing the Temporal Structure of Discourse. In Proceedings of the 7th European Meeting of the Association for Computational Linguistics, pages 253-260. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lascarides</author>
<author>N Asher</author>
<author>J Oberlander</author>
</authors>
<title>Inferring Discourse Relations in Context.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Newark, Del.</location>
<marker>Lascarides, Asher, Oberlander, 1992</marker>
<rawString>Lascarides A., Asher N. and Oberlander J., 1992. Inferring Discourse Relations in Context. In Proceedings of the 30th Meeting of the Association for Computational Linguistics, pages 1-8, Newark, Del.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Li</author>
<author>K F Wong</author>
</authors>
<title>A Word-based Approach for Modeling and Discovering Temporal Relations Embedded in Chinese Sentences,</title>
<date>2002</date>
<journal>ACM Transaction on Asian Language Processing,</journal>
<pages>1--3</pages>
<contexts>
<context position="3791" citStr="Li and Wong, 2002" startWordPosition="546" endWordPosition="549"> (e.g. causation and elaboration) on temporal ordering. They argued that rhetorical relations could be further constrained by event temporal classification. Later, Dorr and Gaasterland (Dorr and Gaasterland, 2002) developed a constraint-based approach to generate sentences, which reflect temporal relations, by making appropriate selections of tense, aspect and connecting words (e.g. before, after and when). Their works, however, are theoretical in nature and have not investigated computational aspects. The pioneer work on Chinese temporal relation extraction was first reported by Li and Wong (Li and Wong, 2002). To discover temporal relations embedded in a sentence, they devised a set of simple rules to map the combined effects of temporal indicators, which are gathered from different grammatical categories, to their corresponding relations. However, their work did not focus on relative temporal relations. Given a sentence describing two temporally related events, Li and Wong only took the temporal position words (including before, after and when, which serve as temporal connectives) and the tense/aspect markers of the second event into consideration. The proposed rule-based approach was simple; but</context>
<context position="7599" citStr="Li and Wong, 2002" startWordPosition="1106" endWordPosition="1109">ERLAPS/OVERLAPPED-BY STARTS/STARTED-BY DURING/CONTAINS FINISHES/FINISHED-BY SAME-AS A punctual event (i.e. represented in time point) A durative event (i.e. represented in time interval) Figure 1. Thirteen temporal relations between points and intervals 2.2 Linguistic Features for Determining Relative Relations Relative relations are generally determined by tense/aspect, connecting words (temporal or otherwise) and event classes. Tense/Aspect in English is manifested by verb inflections. But such morphological variations are inapplicable to Chinese verbs; instead, they are conveyed lexically (Li and Wong, 2002). In other words, tense and aspect in Chinese are expressed using a combination of time words, auxiliaries, temporal position words, adverbs and prepositions, and particular verbs. Temporal Connectives in English primarily involve conjunctions, e.g. after, before and when (Dorr and Gaasterland, 2002). They are key components in discourse structures. In Chinese, however, conjunctions, conjunctive adverbs, prepositions and position words are required to represent connectives. A few verbs which express cause and effect also imply a forward movement of event time. The words, which contribute to th</context>
</contexts>
<marker>Li, Wong, 2002</marker>
<rawString>Li W.J. and Wong K.F., 2002. A Word-based Approach for Modeling and Discovering Temporal Relations Embedded in Chinese Sentences, ACM Transaction on Asian Language Processing, 1(3):173-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ma</author>
<author>B Knight</author>
</authors>
<title>A General Temporal Theory.</title>
<date>1994</date>
<journal>The Computer Journal,</journal>
<volume>37</volume>
<issue>2</issue>
<pages>123</pages>
<contexts>
<context position="6506" citStr="Ma and Knight, 1994" startWordPosition="948" endWordPosition="951"> (Bruce. 1972). Given two durative events, the interval relations between them were modeled by the order between the greatest lower bounding points and least upper bounding points of the two events. In the other camp, instead of adopting time points, Allen took intervals as temporal primitives and introduced thirteen basic binary relations. In this interval-based theory, points are relegated to a subsidiary status as ‘meeting places’ of intervals. An extension to Allen’s theory, which treated both points and intervals as primitives on an equal footing, was later investigated by Ma and Knight (Ma and Knight, 1994). In natural language, events can either be punctual (e.g. 爆炸 (explore)) or durative (e.g. 盖楼 (built a house)) in nature. Thus Ma and Knight’s model is adopted in our work (see Figure 1). Taking the sentence “修成立交桥以后,他们解决了该市的交通问题 (after the street bridge had been built, they solved the traffic problem of the city)” as an example, the relation held between building the bridge (i.e. an interval) and solving the problem (i.e. a point) is BEFORE. BEFORE/AFTER MEETS/MET-BY OVERLAPS/OVERLAPPED-BY STARTS/STARTED-BY DURING/CONTAINS FINISHES/FINISHED-BY SAME-AS A punctual event (i.e. represented in tim</context>
</contexts>
<marker>Ma, Knight, 1994</marker>
<rawString>Ma J. and Knight B., 1994. A General Temporal Theory. The Computer Journal, 37(2):114- 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Màntaras</author>
</authors>
<title>A Distance-based Attribute Selection Measure for Decision Tree Induction.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>81--92</pages>
<marker>Màntaras, 1991</marker>
<rawString>Màntaras L., 1991. A Distance-based Attribute Selection Measure for Decision Tree Induction. Machine Learning, 6(1): 81–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Màrquez</author>
<author>L Padró</author>
<author>H Rodríguez</author>
</authors>
<title>A Machine Learning Approach to POS Tagging.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--1</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Màrquez, Padró, Rodríguez, 2000</marker>
<rawString>Màrquez L., Padró L. and Rodríguez H., 2000. A Machine Learning Approach to POS Tagging. Machine Learning, 39(1):59-91. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Partee</author>
</authors>
<date>1984</date>
<booktitle>Nominal and Temporal Anaphora. Linguistics and Philosophy,</booktitle>
<pages>7--3</pages>
<contexts>
<context position="2297" citStr="Partee, 1984" startWordPosition="332" endWordPosition="333">n and machine translation, etc. Modeling temporal aspects of an event in a written text is more complex than capturing time in a physical time-stamped system. Event time may be specified explicitly in a sentence, e.g. “他们在 1997 年解 决了该市的交通问题 (They solved the traffic problem of the city in 1997)”; or it may be left implicit, to be recovered by readers from context. For example, one may know that “修成立交桥以后,他们解决了该 市的交通问题 (after the street bridge had been built, they solved the traffic problem of the city)”, yet without knowing the exact time when the street bridge was built. As reported by Partee (Partee, 1984), the expression of relative temporal relations in which precise times are not stated is common in natural language. The objective of relative temporal relation resolution is to determine the type of relative relation embedded in a sentence. In English, temporal expressions have been widely studied. Lascarides and Asher (Lascarides, Asher and Oberlander, 1992) suggested that temporal relations between two events followed from discourse structures. They investigated various contextual effects on five discourse relations (namely narration, elaboration, explanation, background and result) and the</context>
</contexts>
<marker>Partee, 1984</marker>
<rawString>Partee, B., 1984. Nominal and Temporal Anaphora. Linguistics and Philosophy, 7(3):287-324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<date>1993</date>
<booktitle>C4.5 Programs for Machine Learning.</booktitle>
<publisher>Morgan Kauman Press.</publisher>
<contexts>
<context position="12298" citStr="Quinlan, 1993" startWordPosition="1817" endWordPosition="1818">ng data are noisy. For this reason, it is impossible to obtain a tree which can completely classify all training examples. To overcome this predicament, we aim to obtain more adjusted probability distributions of event pairs over their possible classes. Therefore, a probabilistic decision tree approach is preferred over conventional decision tree approaches (e.g. C4.5, ID3). We adopt a non-incremental supervised learning algorithm in TDIDT (Top Down Induction of Decision Trees) family. It constructs a tree top-down and the process is guided by distributional information learned from examples (Quinlan, 1993). 3.1.1 Parameter Estimation Based on probabilities, each object in the PDT approach can belong to a number of classes. These probabilities could be estimated from training cases with Maximum Likelihood Estimation (MLE). Let l be the decision sequence, z the object and c the class. The probability of z belonging to c is: p(c|z)=E Al, c |z) - Ep c l p l z ( |) ( |) l l let l= B1 B2 ... Bn , by MLE we have: p(c |l) - p(c |BJ = f (c, Bn) (2) f(Bn) f (c, Bn) is the count of the items whose leaf nodes are Bn and belonging to class c. And (1) (3) An object might traverse more than one decision path </context>
<context position="14792" citStr="Quinlan, 1993" startWordPosition="2309" endWordPosition="2310"> l l r r wi / , , , , , ( ), , wo punc TI TI TI TI class e TI TI 4 3 2 1 1 e e e e 2 e e 2 2 2 2 2 2 li (i=1,2,3,4) and rj (j=1,2) are the ith indictor before and the jth indicator after the event ek (k=1,2). Given a sentence, for example, 5&amp;quot;c/TI_d �/E0 _T/TI_u �T/n ,/w A-/TI_d T,0/E2 _T/TI_u IRiI/n o/w, the attribute vector could be represented as: [0, 0, 0, 5&amp;quot;c, E0, _T, 0, 1, 0, 0, 0, A-, E2, _T, 0]. 3.1.3 Attribute Selection Function Many similar attribute selection functions were used to construct a decision tree (Marquez, 2000). These included information gain and information gain ratio (Quinlan, 1993), 2 χ Test and Symmetrical Tau (Zhou and Dillon, 1991). We adopt the one proposed by Lopez de Mantaraz (Mantaras, 1991) for it shows more stable performance than Quinlan’s information gain ratio in our experiments. Compared with Quinlan’s information gain ratio, Lopez’s distance-based measurement is unbiased towards the attributes with a large number of values and is capable of generating smaller trees with no loss of accuracy (Marquez, Padro and Rodriguez, 2000). This characteristic makes it an ideal choice for our work, where most attributes have more than 200 values. 3.2 Naïve Bayesian Clas</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan J., 1993. C4.5 Programs for Machine Learning. Morgan Kauman Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Reichenbach</author>
</authors>
<title>Elements of Symbolic Logic.</title>
<date>1947</date>
<institution>Berkeley CA, University of California Press.</institution>
<contexts>
<context position="5672" citStr="Reichenbach, 1947" startWordPosition="819" endWordPosition="820">us collaborative bootstrapping technique for smoothing is presented. Experiments designed for studying the impact of different approaches and linguistic features are described in Section 4. Finally, Section 5 concludes the paper. 2 Modeling Temporal Relations 2.1 Temporal Relation Representations As the importance of temporal information processing has become apparent, a variety of temporal systems have been introduced, attempting to accommodate the characteristics of relative temporal information. Among those who worked on temporal relation representations, many took the work of Reichenbach (Reichenbach, 1947) as a starting point, while some others based their works on Allen’s (Allen, 1981). Reichenbach proposed a point-based temporal theory. This was later enhanced by Bruce who defined seven relative temporal relations (Bruce. 1972). Given two durative events, the interval relations between them were modeled by the order between the greatest lower bounding points and least upper bounding points of the two events. In the other camp, instead of adopting time points, Allen took intervals as temporal primitives and introduced thirteen basic binary relations. In this interval-based theory, points are r</context>
</contexts>
<marker>Reichenbach, 1947</marker>
<rawString>Reichenbach H., 1947. Elements of Symbolic Logic. Berkeley CA, University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Siegel</author>
<author>K McKeown</author>
</authors>
<title>Learning Methods to Combine Linguistic Indicators: Improving Aspectual Classification and Revealing Linguistic Insights.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<pages>595--627</pages>
<marker>Siegel, McKeown, 2000</marker>
<rawString>Siegel E. and McKeown K., 2000. Learning Methods to Combine Linguistic Indicators: Improving Aspectual Classification and Revealing Linguistic Insights. Computational Linguistics, 26(4): 595-627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Wiebe</author>
<author>T P O&apos;Hara</author>
<author>T Ohrstrom-Sandgren</author>
<author>K J McKeever</author>
</authors>
<title>An Empirical Approach to Temporal Reference Resolution.</title>
<date>1998</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>9--247</pages>
<marker>Wiebe, O&apos;Hara, Ohrstrom-Sandgren, McKeever, 1998</marker>
<rawString>Wiebe, J.M., O&apos;Hara, T.P., Ohrstrom-Sandgren, T. and McKeever, K.J, 1998. An Empirical Approach to Temporal Reference Resolution. Journal of Artificial Intelligence Research, 9:247-293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wong</author>
<author>W Li</author>
<author>C Yuan</author>
<author>etc</author>
</authors>
<title>Temporal Representation and Classification</title>
<date>2002</date>
<booktitle>in Chinese. International Journal of Computer Processing of Oriental Languages,</booktitle>
<pages>15--2</pages>
<marker>Wong, Li, Yuan, etc, 2002</marker>
<rawString>Wong F., Li W., Yuan C., etc., 2002. Temporal Representation and Classification in Chinese. International Journal of Computer Processing of Oriental Languages, 15(2):211-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision Lists for Lexical Ambiguity Resolution: Application to the Accent Restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceeding of the 32rd Annual Meeting of ACL,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="10678" citStr="Yarowsky, 1994" startWordPosition="1566" endWordPosition="1567">ts. Notice that the mapping is not one-to-one. For example, adverbs affect tense/aspect as well as discourse structure. For another example, tense/aspect can be affected by auxiliary words, trend verbs, etc. This shows that classification of temporal indicators based on partof-speech (POS) information alone cannot determine relative temporal relations. 3 Machine Learning Approaches for Relative Relation Resolution Previous efforts in corpus-based natural language processing have incorporated machine learning methods to coordinate multiple linguistic features for example in accent restoration (Yarowsky, 1994) and event classification (Siegel and McKeown, 1998), etc. Relative relation resolution can be modeled as a relation classification task. We model the thirteen relative temporal relations (see Figure 1) as the classes to be decided by a classifier. The resolution process is to assign an event pair (i.e. the two events under concern)2 to one class according to their linguistic features. For this purpose, we train two classifiers, a Probabilistic Decision Tree Classifier (PDT) and a Naïve Bayesian Classifier (NBC). We then combine the results by the Collaborative Bootstrapping (CB) technique whi</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>Yarowsky D., 1994. Decision Lists for Lexical Ambiguity Resolution: Application to the Accent Restoration in Spanish and French. In Proceeding of the 32rd Annual Meeting of ACL, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhou</author>
<author>T Dillon</author>
</authors>
<title>A Statistical-heuristic Feature Selection Criterion for Decision Tree Induction.</title>
<date>1991</date>
<journal>IEEE Transaction on Pattern Analysis and Machine Intelligence,</journal>
<volume>13</volume>
<issue>8</issue>
<pages>834--841</pages>
<contexts>
<context position="14846" citStr="Zhou and Dillon, 1991" startWordPosition="2317" endWordPosition="2320">I TI class e TI TI 4 3 2 1 1 e e e e 2 e e 2 2 2 2 2 2 li (i=1,2,3,4) and rj (j=1,2) are the ith indictor before and the jth indicator after the event ek (k=1,2). Given a sentence, for example, 5&amp;quot;c/TI_d �/E0 _T/TI_u �T/n ,/w A-/TI_d T,0/E2 _T/TI_u IRiI/n o/w, the attribute vector could be represented as: [0, 0, 0, 5&amp;quot;c, E0, _T, 0, 1, 0, 0, 0, A-, E2, _T, 0]. 3.1.3 Attribute Selection Function Many similar attribute selection functions were used to construct a decision tree (Marquez, 2000). These included information gain and information gain ratio (Quinlan, 1993), 2 χ Test and Symmetrical Tau (Zhou and Dillon, 1991). We adopt the one proposed by Lopez de Mantaraz (Mantaras, 1991) for it shows more stable performance than Quinlan’s information gain ratio in our experiments. Compared with Quinlan’s information gain ratio, Lopez’s distance-based measurement is unbiased towards the attributes with a large number of values and is capable of generating smaller trees with no loss of accuracy (Marquez, Padro and Rodriguez, 2000). This characteristic makes it an ideal choice for our work, where most attributes have more than 200 values. 3.2 Naïve Bayesian Classifier (NBC) NBC assumes independence among features. </context>
</contexts>
<marker>Zhou, Dillon, 1991</marker>
<rawString>Zhou X., Dillon T., 1991. A Statistical-heuristic Feature Selection Criterion for Decision Tree Induction. IEEE Transaction on Pattern Analysis and Machine Intelligence, 13(8): 834-841.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>