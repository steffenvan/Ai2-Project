<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000146">
<title confidence="0.987468">
Joint Unsupervised Coreference Resolution with Markov Logic
</title>
<author confidence="0.999259">
Hoifung Poon Pedro Domingos
</author>
<affiliation confidence="0.9988825">
Department of Computer Science and Engineering
University of Washington
</affiliation>
<address confidence="0.534558">
Seattle, WA 98195-2350, U.S.A.
</address>
<email confidence="0.999201">
{hoifung,pedrod}@cs.washington.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999658736842105">
Machine learning approaches to coreference
resolution are typically supervised, and re-
quire expensive labeled data. Some unsuper-
vised approaches have been proposed (e.g.,
Haghighi and Klein (2007)), but they are less
accurate. In this paper, we present the first un-
supervised approach that is competitive with
supervised ones. This is made possible by
performing joint inference across mentions,
in contrast to the pairwise classification typ-
ically used in supervised methods, and by us-
ing Markov logic as a representation language,
which enables us to easily express relations
like apposition and predicate nominals. On
MUC and ACE datasets, our model outper-
forms Haghigi and Klein’s one using only a
fraction of the training data, and often matches
or exceeds the accuracy of state-of-the-art su-
pervised models.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966869565218">
The goal of coreference resolution is to identify
mentions (typically noun phrases) that refer to the
same entities. This is a key subtask in many NLP
applications, including information extraction, ques-
tion answering, machine translation, and others. Su-
pervised learning approaches treat the problem as
one of classification: for each pair of mentions,
predict whether they corefer or not (e.g., McCal-
lum &amp; Wellner (2005)). While successful, these
approaches require labeled training data, consisting
of mention pairs and the correct decisions for them.
This limits their applicability.
Unsupervised approaches are attractive due to the
availability of large quantities of unlabeled text.
However, unsupervised coreference resolution is
much more difficult. Haghighi and Klein’s (2007)
model, the most sophisticated to date, still lags su-
pervised ones by a substantial margin. Extending it
appears difficult, due to the limitations of its Dirich-
let process-based representation.
The lack of label information in unsupervised
coreference resolution can potentially be overcome
by performing joint inference, which leverages the
“easy” decisions to help make related “hard” ones.
Relations that have been exploited in supervised
coreference resolution include transitivity (McCal-
lum &amp; Wellner, 2005) and anaphoricity (Denis &amp;
Baldridge, 2007). However, there is little work to
date on joint inference for unsupervised resolution.
We address this problem using Markov logic,
a powerful and flexible language that combines
probabilistic graphical models and first-order logic
(Richardson &amp; Domingos, 2006). Markov logic
allows us to easily build models involving rela-
tions among mentions, like apposition and predi-
cate nominals. By extending the state-of-the-art al-
gorithms for inference and learning, we developed
the first general-purpose unsupervised learning al-
gorithm for Markov logic, and applied it to unsuper-
vised coreference resolution.
We test our approach on standard MUC and ACE
datasets. Our basic model, trained on a minimum
of data, suffices to outperform Haghighi and Klein’s
(2007) one. Our full model, using apposition and
other relations for joint inference, is often as accu-
rate as the best supervised models, or more.
</bodyText>
<page confidence="0.972962">
650
</page>
<note confidence="0.9619925">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650–659,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999841333333333">
We begin by reviewing the necessary background
on Markov logic. We then describe our Markov
logic network for joint unsupervised coreference
resolution, and the learning and inference algorithms
we used. Finally, we present our experiments and re-
sults.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999870321839081">
Most existing supervised learning approaches for
coreference resolution are suboptimal since they re-
solve each mention pair independently, only impos-
ing transitivity in postprocessing (Ng, 2005). More-
over, many of them break up the resolution step into
subtasks (e.g., first determine whether a mention is
anaphoric, then classify whether it is coreferent with
an antecedent), which further forsakes opportunities
for joint inference that have been shown to be help-
ful (Poon &amp; Domingos, 2007). Using graph parti-
tioning, McCallum &amp; Wellner (2005) incorporated
transitivity into pairwise classification and achieved
the state-of-the-art result on the MUC-6 dataset, but
their approach can only leverage one binary relation
at a time, not arbitrary relations among mentions.
Denis &amp; Baldridge (2007) determined anaphoricity
and pairwise classification jointly using integer pro-
gramming, but they did not incorporate transitivity
or other relations.
While potentially more appealing, unsupervised
learning is very challenging, and unsupervised
coreference resolution systems are still rare to this
date. Prior to our work, the best performance in
unsupervised coreference resolution was achieved
by Haghighi &amp; Klein (2007), using a nonparamet-
ric Bayesian model based on hierarchical Dirichlet
processes. At the heart of their system is a mixture
model with a few linguistically motivated features
such as head words, entity properties and salience.
Their approach is a major step forward in unsuper-
vised coreference resolution, but extending it is chal-
lenging. The main advantage of Dirichlet processes
is that they are exchangeable, allowing parameters
to be integrated out, but Haghighi and Klein forgo
this when they introduce salience. Their model thus
requires Gibbs sampling over both assignments and
parameters, which can be very expensive. Haghighi
and Klein circumvent this by making approxima-
tions that potentially hurt accuracy. At the same
time, the Dirichlet process prior favors skewed clus-
ter sizes and a number of clusters that grows loga-
rithmically with the number of data points, neither of
which seems generally appropriate for coreference
resolution.
Further, deterministic or strong non-deterministic
dependencies cause Gibbs sampling to break down
(Poon &amp; Domingos, 2006), making it difficult to
leverage many linguistic regularities. For exam-
ple, apposition (as in “Bill Gates, the chairman of
Microsoft”) suggests coreference, and thus the two
mentions it relates should always be placed in the
same cluster. However, Gibbs sampling can only
move one mention at a time from one cluster to
another, and this is unlikely to happen, because it
would require breaking the apposition rule. Blocked
sampling can alleviate this problem by sampling
multiple mentions together, but it requires that the
block size be predetermined to a small fixed number.
When we incorporate apposition and other regular-
ities the blocks can become arbitrarily large, mak-
ing this infeasible. For example, suppose we also
want to leverage predicate nominals (i.e., the sub-
ject and the predicating noun of a copular verb are
likely coreferent). Then a sentence like “He is Bill
Gates, the chairman of Microsoft” requires a block
of four mentions: “He”, “Bill Gates”, “the chair-
man of Microsoft”, and “Bill Gates, the chairman
of Microsoft”. Similar difficulties occur with other
inference methods. Thus, extending Haghighi and
Klein’s model to include richer linguistic features is
a challenging problem.
Our approach is instead based on Markov logic,
a powerful representation for joint inference with
uncertainty (Richardson &amp; Domingos, 2006). Like
Haghighi and Klein’s, our model is cluster-based
rather than pairwise, and implicitly imposes tran-
sitivity. We do not predetermine anaphoricity of a
mention, but rather fuse it into the integrated reso-
lution process. As a result, our model is inherently
joint among mentions and subtasks. It shares sev-
eral features with Haghighi &amp; Klein’s model, but re-
moves or refines features where we believe it is ap-
propriate to. Most importantly, our model leverages
apposition and predicate nominals, which Haghighi
&amp; Klein did not use. We show that this can be done
very easily in our framework, and yet results in very
substantial accuracy gains.
</bodyText>
<page confidence="0.99804">
651
</page>
<bodyText confidence="0.999217833333333">
It is worth noticing that Markov logic is also well
suited for joint inference in supervised systems (e.g.,
transitivity, which took McCallum &amp; Wellner (2005)
nontrivial effort to incorporate, can be handled in
Markov logic with the addition of a single formula
(Poon &amp; Domingos, 2008)).
</bodyText>
<sectionHeader confidence="0.974627" genericHeader="method">
3 Markov Logic
</sectionHeader>
<bodyText confidence="0.986737869565217">
In many NLP applications, there exist rich relations
among objects, and recent work in statistical rela-
tional learning (Getoor &amp; Taskar, 2007) and struc-
tured prediction (Bakir et al., 2007) has shown that
leveraging these can greatly improve accuracy. One
of the most powerful representations for joint infer-
ence is Markov logic, a probabilistic extension of
first-order logic (Richardson &amp; Domingos, 2006). A
Markov logic network (MLN) is a set of weighted
first-order clauses. Together with a set of con-
stants, it defines a Markov network with one node
per ground atom and one feature per ground clause.
The weight of a feature is the weight of the first-
order clause that originated it. The probability of
a state x in such a network is given by P(x) =
(1/Z) exp (Ez wzfz(x)), where Z is a normaliza-
tion constant, wz is the weight of the ith clause,
fz = 1 if the ith clause is true, and fz = 0 other-
wise.
Markov logic makes it possible to compactly
specify probability distributions over complex re-
lational domains. Efficient inference can be per-
formed using MC-SAT (Poon &amp; Domingos, 2006).
MC-SAT is a “slice sampling” Markov chain Monte
Carlo algorithm. Slice sampling introduces auxil-
iary variables u that decouple the original ones x,
and alternately samples u conditioned on x and vice-
versa. To sample from the slice (the set of states x
consistent with the current u), MC-SAT calls Sam-
pleSAT (Wei et al., 2004), which uses a combina-
tion of satisfiability testing and simulated annealing.
The advantage of using a satisfiability solver (Walk-
SAT) is that it efficiently finds isolated modes in the
distribution, and as a result the Markov chain mixes
very rapidly. The slice sampling scheme ensures
that detailed balance is (approximately) preserved.
MC-SAT is orders of magnitude faster than previous
MCMC algorithms like Gibbs sampling, making ef-
ficient sampling possible on a scale that was previ-
Algorithm 1 MC-SAT(clauses, weights,
num samples)
x(c) Satisfy(hard clauses)
for i 1 to num samples do
M 0
for all ck E clauses satisfied by x(z−1) do
With probability 1 — e−lk add ck to M
</bodyText>
<listItem confidence="0.358969333333333">
end for
Sample x(z) — USAT(M)
end for
</listItem>
<bodyText confidence="0.993262657142857">
ously out of reach.
Algorithm 1 gives pseudo-code for MC-SAT. At
iteration i — 1, the factor 0k for clause ck is ei-
ther elk if ck is satisfied in x(z−1), or 1 otherwise.
MC-SAT first samples the auxiliary variable uk uni-
formly from (0, 0k), then samples a new state uni-
formly from the set of states that satisfy 0k &gt; uk
for all k (the slice). Equivalently, for each k, with
probability 1 — e−lk the next state must satisfy ck.
In general, we can factorize the probability distribu-
tion in any way that facilitates inference, sample the
uk’s, and make sure that the next state is drawn uni-
formly from solutions that satisfy 0k &gt; uk for all
factors.
MC-SAT, like most existing relational inference
algorithms, grounds all predicates and clauses, thus
requiring memory and time exponential in the pred-
icate and clause arities. We developed a general
method for producing a “lazy” version of relational
inference algorithms (Poon &amp; Domingos, 2008),
which carries exactly the same inference steps as the
original algorithm, but only maintains a small sub-
set of “active” predicates/clauses, grounding more
as needed. We showed that Lazy-MC-SAT, the lazy
version of MC-SAT, reduced memory and time by
orders of magnitude in several domains. We use
Lazy-MC-SAT in this paper.
Supervised learning for Markov logic maximizes
the conditional log-likelihood L(x, y) = log P(Y =
yJX = x), where Y represents the non-evidence
predicates, X the evidence predicates, and x, y their
values in the training data. For simplicity, from now
on we omit X, whose values are fixed and always
conditioned on. The optimization problem is convex
and a global optimum can be found using gradient
</bodyText>
<page confidence="0.989906">
652
</page>
<bodyText confidence="0.938351">
descent, with the gradient being
</bodyText>
<equation confidence="0.987673333333333">
∂ L(y) = ni(y) − �y, P(Y = y&apos;)ni(y&apos;)
∂w
= ni(y) − EY[ni].
</equation>
<bodyText confidence="0.998559">
where ni is the number of true groundings of clause
i. The expected count can be approximated as
</bodyText>
<equation confidence="0.990195">
�N
1
EY [ni] Pz� N
k=1
</equation>
<bodyText confidence="0.999967">
where yk are samples generated by MC-SAT. To
combat overfitting, a Gaussian prior is imposed on
all weights.
In practice, it is difficult to tune the learning rate
for gradient descent, especially when the number
of groundings varies widely among clauses. Lowd
&amp; Domingos (2007) used a preconditioned scaled
conjugate gradient algorithm (PSCG) to address this
problem. This estimates the optimal step size in each
step as
</bodyText>
<equation confidence="0.98408">
−dT g
dT Hd + AdT d.
</equation>
<bodyText confidence="0.9985258">
where g is the gradient, d the conjugate update direc-
tion, and A a parameter that is automatically tuned
to trade off second-order information with gradient
descent. H is the Hessian matrix, with the (i, j)th
entry being
</bodyText>
<equation confidence="0.830338666666667">
∂2
∂w∂w� L(y) = EY [ni] · EY [nj] − EY [ni · nj]
= −CovY [ni,nj].
</equation>
<bodyText confidence="0.990401125">
The Hessian can be approximated with the same
samples used for the gradient. Its negative inverse
diagonal is used as the preconditioner.1
The open-source Alchemy package (Kok et al.,
2007) provides implementations of existing algo-
rithms for Markov logic. In Section 5, we develop
the first general-purpose unsupervised learning al-
gorithm for Markov logic by extending the existing
algorithms to handle hidden predicates.2
1Lowd &amp; Domingos showed that α can be computed more
efficiently, without explicitly approximating or storing the Hes-
sian. Readers are referred to their paper for details.
2Alchemy includes a discriminative EM algorithm, but it as-
sumes that only a few values are missing, and cannot handle
completely hidden predicates. Kok &amp; Domingos (2007) applied
Markov logic to relational clustering, but they used hard EM.
</bodyText>
<sectionHeader confidence="0.8138855" genericHeader="method">
4 An MLN for Joint Unsupervised
Coreference Resolution
</sectionHeader>
<bodyText confidence="0.999995827586207">
In this section, we present our MLN for joint unsu-
pervised coreference resolution. Our model deviates
from Haghighi &amp; Klein’s (2007) in several impor-
tant ways. First, our MLN does not model saliences
for proper nouns or nominals, as their influence is
marginal compared to other features; for pronoun
salience, it uses a more intuitive and simpler def-
inition based on distance, and incorporated it as a
prior. Another difference is in identifying heads. For
the ACE datasets, Haghighi and Klein used the gold
heads; for the MUC-6 dataset, where labels are not
available, they crudely picked the rightmost token in
a mention. We show that a better way is to determine
the heads using head rules in a parser. This improves
resolution accuracy and is always applicable. Cru-
cially, our MLN leverages syntactic relations such
as apposition and predicate nominals, which are not
used by Haghighi and Klein. In our approach, what
it takes is just adding two formulas to the MLN.
As common in previous work, we assume that
true mention boundaries are given. We do not as-
sume any other labeled information. In particu-
lar, we do not assume gold name entity recogni-
tion (NER) labels, and unlike Haghighi &amp; Klein
(2007), we do not assume gold mention types (for
ACE datasets, they also used gold head words). We
determined the head of a mention either by taking
its rightmost token, or by using the head rules in a
parser. We detected pronouns using a list.
</bodyText>
<subsectionHeader confidence="0.99662">
4.1 Base MLN
</subsectionHeader>
<bodyText confidence="0.99814775">
The main query predicate is InClust(m, c!), which
is true iff mention m is in cluster c. The “t!” notation
signifies that for each m, this predicate is true for a
unique value of c. The main evidence predicate is
Head(m, t!), where m is a mention and t a token, and
which is true iff t is the head of m. A key component
in our MLN is a simple head mixture model, where
the mixture component priors are represented by the
unit clause
InClust(+m, +c)
and the head distribution is represented by the head
prediction rule
</bodyText>
<equation confidence="0.988676666666667">
InClust(m, +c) ∧ Head(m, +t).
ni(yk)
α =
</equation>
<page confidence="0.981548">
653
</page>
<bodyText confidence="0.997795">
All free variables are implicitly universally quanti-
fied. The “+” notation signifies that the MLN con-
tains an instance of the rule, with a separate weight,
for each value combination of the variables with a
plus sign.
By convention, at each inference step we name
each non-empty cluster after the earliest mention it
contains. This helps break the symmetry among
mentions, which otherwise produces multiple op-
tima and makes learning unnecessarily harder. To
encourage clustering, we impose an exponential
prior on the number of non-empty clusters with
weight −1.
The above model only clusters mentions with the
same head, and does not work well for pronouns. To
address this, we introduce the predicate IsPrn(m),
which is true iff the mention m is a pronoun, and
adapt the head prediction rule as follows:
</bodyText>
<equation confidence="0.853879">
--iIsPrn(m) ∧ InClust(m, +c) ∧ Head(m, +t)
</equation>
<bodyText confidence="0.919339571428572">
This is always false when m is a pronoun, and thus
applies only to non-pronouns.
Pronouns tend to resolve with men-
tions that are semantically compatible with
them. Thus we introduce predicates that
represent entity type, number, and gender:
Type(x, e!), Number(x, n!), Gender(x, g!),
where x can be either a cluster or mention,
e E {Person,Organization,Location,Other},
n E {Singular,Plural} and g E
{Male, Female, Neuter}. Many of these are
known for pronouns, and some can be inferred
from simple linguistic cues (e.g., “Ms. Galen”
is a singular female person, while “XYZ Corp.”
is an organization).3 Entity type assignment is
represented by the unit clause
Type(+x, +e)
and similarly for number and gender. A mention
should agree with its cluster in entity type. This is
ensured by the hard rule (which has infinite weight
and must be satisfied)
InClust(m, c) ==&gt; (Type(m, e) &lt;---&gt; Type(c, e))
3We used the following cues: Mr., Ms., Jr., Inc., Corp., cor-
poration, company. The proportions of known properties range
from 14% to 26%.
There are similar hard rules for number and gender.
Different pronouns prefer different entity types,
as represented by
</bodyText>
<equation confidence="0.989558">
IsPrn(m) ∧ InClust(m, c)
∧Head(m, +t) ∧ Type(c, +e)
</equation>
<bodyText confidence="0.9999378">
which only applies to pronouns, and whose weight is
positive if pronoun t is likely to assume entity type
e and negative otherwise. There are similar rules for
number and gender.
Aside from semantic compatibility, pronouns tend
to resolve with nearby mentions. To model this, we
impose an exponential prior on the distance (number
of mentions) between a pronoun and its antecedent,
with weight −1.4 This is similar to Haghighi and
Klein’s treatment of salience, but simpler.
</bodyText>
<subsectionHeader confidence="0.835135">
4.2 Full MLN
</subsectionHeader>
<bodyText confidence="0.977404666666667">
Syntactic relations among mentions often suggest
coreference. Incorporating such relations into our
MLN is straightforward. We illustrate this with
two examples: apposition and predicate nominals.
We introduce a predicate for apposition, Appo(x, y),
where x, y are mentions, and which is true iff y is an
appositive of x. We then add the rule
Appo(x, y) ==&gt; (InClust(x, c) &lt;---&gt; InClust(y, c))
which ensures that x, y are in the same cluster if y is
an appositive of x. Similarly, we introduce a predi-
cate for predicate nominals, PredNom(x, y), and the
corresponding rule.5 The weights of both rules can
be learned from data with a positive prior mean. For
simplicity, in this paper we treat them as hard con-
straints.
</bodyText>
<subsectionHeader confidence="0.971854">
4.3 Rule-Based MLN
</subsectionHeader>
<bodyText confidence="0.999934">
We also consider a rule-based system that clusters
non-pronouns by their heads, and attaches a pro-
noun to the cluster which has no known conflicting
</bodyText>
<footnote confidence="0.998905888888889">
4For simplicity, if a pronoun has no antecedent, we define
the distance to be oo. So a pronoun must have an antecedent in
our model, unless it is the first mention in the document or it can
not resolve with previous mentions without violating hard con-
straints. It is straightforward to soften this with a finite penalty.
5We detected apposition and predicate nominatives using
simple heuristics based on parses, e.g., if (NP, comma, NP) are
the first three children of an NP, then any two of the three noun
phrases are apposition.
</footnote>
<page confidence="0.998839">
654
</page>
<bodyText confidence="0.999961166666667">
type, number, or gender, and contains the closest an-
tecedent for the pronoun. This system can be en-
coded in an MLN with just four rules. Three of them
are the ones for enforcing agreement in type, num-
ber, and gender between a cluster and its members,
as defined in the base MLN. The fourth rule is
</bodyText>
<equation confidence="0.9995515">
¬IsPrn(m1) ∧ ¬IsPrn(m2)
∧Head(m1, h1) ∧ Head(m2, h2)
∧InClust(m1, c1) ∧ InClust(m2, c2)
⇒ (c1 = c2 ⇔ h1 = h2).
</equation>
<bodyText confidence="0.9999519">
With a large but not infinite weight (e.g., 100),
this rule has the effect of clustering non-pronouns
by their heads, except when it violates the hard
rules. The MLN can also include the apposition and
predicate-nominal rules. As in the base MLN, we
impose the same exponential prior on the number of
non-empty clusters and that on the distance between
a pronoun and its antecedent. This simple MLN is
remarkably competitive, as we will see in the exper-
iment section.
</bodyText>
<sectionHeader confidence="0.893834" genericHeader="method">
5 Learning and Inference
</sectionHeader>
<bodyText confidence="0.8793685">
Unsupervised learning in Markov logic maximizes
the conditional log-likelihood
</bodyText>
<equation confidence="0.853096">
L(x, y) = logP(Y = y|X = x)
= log &amp;; P(Y = y,Z = z|X = x)
</equation>
<bodyText confidence="0.9999884">
where Z are unknown predicates. In our coref-
erence resolution MLN, Y includes Head and
known groundings of Type, Number and Gender,
Z includes InClust and unknown groundings of
Type, Number, Gender, and X includes IsPrn,
Appo and PredNom. (For simplicity, from now on
we drop X from the formula.) With Z, the opti-
mization problem is no longer convex. However, we
can still find a local optimum using gradient descent,
with the gradient being
where ni is the number of true groundings of the ith
clause. We extended PSCG for unsupervised learn-
ing. The gradient is the difference of two expec-
tations, each of which can be approximated using
samples generated by MC-SAT. The (i, j)th entry of
</bodyText>
<equation confidence="0.927046">
the Hessian is now
a2
awiawj
L(y) = CovZ|y[ni, nj] − CovY,Z[ni, nj]
</equation>
<bodyText confidence="0.999980340909091">
and the step size can be computed accordingly.
Since our problem is no longer convex, the nega-
tive diagonal Hessian may contain zero or negative
entries, so we first took the absolute values of the
diagonal and added 1, then used the inverse as the
preconditioner. We also adjusted A more conserva-
tively than Lowd &amp; Domingos (2007).
Notice that when the objects form independent
subsets (in our cases, mentions in each document),
we can process them in parallel and then gather suf-
ficient statistics for learning. We developed an ef-
ficient parallelized implementation of our unsuper-
vised learning algorithm using the message-passing
interface (MPI). Learning in MUC-6 took only one
hour, and in ACE-2004 two and a half.
To reduce burn-in time, we initialized MC-SAT
with the state returned by MaxWalkSAT (Kautz et
al., 1997), rather than a random solution to the hard
clauses. In the existing implementation in Alchemy
(Kok et al., 2007), SampleSAT flips only one atom
in each step, which is inefficient for predicates with
unique-value constraints (e.g., Head(m, c!)). Such
predicates can be viewed as multi-valued predi-
cates (e.g., Head(m) with value ranging over all
c’s) and are prevalent in NLP applications. We
adapted SampleSAT to flip two or more atoms in
each step so that the unique-value constraints are
automatically satisfied. By default, MC-SAT treats
each ground clause as a separate factor while de-
termining the slice. This can be very inefficient
for highly correlated clauses. For example, given
a non-pronoun mention m currently in cluster c and
with head t, among the mixture prior rules involv-
ing m InClust(m, c) is the only one that is satisfied,
and among those head-prediction rules involving m,
¬IsPrn(m)∧InClust(m, c)∧Head(m, t) is the only
one that is satisfied; the factors for these rules mul-
tiply to = exp(wm, + wm,�,t), where wm,r is the
weight for InClust(m, c), and wm,�,t is the weight
for ¬IsPrn(m) ∧ InClust(m, c) ∧ Head(m, t), since
an unsatisfied rule contributes a factor of e0 = 1. We
extended MC-SAT to treat each set of mutually ex-
clusive and exhaustive rules as a single factor. E.g.,
for the above m, MC-SAT now samples u uniformly
</bodyText>
<figure confidence="0.8903535">
a L(y) = EZ|y[ni] − EY,Z[ni]
awi
</figure>
<page confidence="0.996488">
655
</page>
<bodyText confidence="0.999968857142857">
from (0, 0), and requires that in the next state 0&apos; be
no less than u. Equivalently, the new cluster and
head for m should satisfy wm,c, + wm,c,,t, &gt; log(u).
We extended SampleSAT so that when it consid-
ers flipping any variable involved in such constraints
(e.g., c or t above), it ensures that their new values
still satisfy these constraints.
The final clustering is found using the MaxWalk-
SAT weighted satisfiability solver (Kautz et al.,
1997), with the appropriate extensions. We first ran
a MaxWalkSAT pass with only finite-weight formu-
las, then ran another pass with all formulas. We
found that this significantly improved the quality of
the results that MaxWalkSAT returned.
</bodyText>
<sectionHeader confidence="0.998915" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.983216">
6.1 System
</subsectionHeader>
<bodyText confidence="0.999943111111111">
We implemented our method as an extension to the
Alchemy system (Kok et al., 2007). Since our learn-
ing uses sampling, all results are the average of five
runs using different random seeds. Our optimiza-
tion problem is not convex, so initialization is im-
portant. The core of our model (head mixture) tends
to cluster non-pronouns with the same head. There-
fore, we initialized by setting all weights to zero,
and running the same learning algorithm on the base
MLN, while assuming that in the ground truth, non-
pronouns are clustered by their heads. (Effectively,
the corresponding InClust atoms are assigned to
appropriate values and are included in Y rather than
Z during learning.) We used 30 iterations of PSCG
for learning. (In preliminary experiments, additional
iterations had little effect on coreference accuracy.)
We generated 100 samples using MC-SAT for each
expectation approximation.6
</bodyText>
<subsectionHeader confidence="0.99816">
6.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999978285714286">
We conducted experiments on MUC-6, ACE-2004,
and ACE Phrase-2 (ACE-2). We evaluated our sys-
tems using two commonly-used scoring programs:
MUC (Vilain et al., 1995) and B3 (Amit &amp; Bald-
win, 1998). To gain more insight, we also report
pairwise resolution scores and mean absolute error
in the number of clusters.
</bodyText>
<footnote confidence="0.966789">
6Each sample actually contains a large number of ground-
ings, so 100 samples yield sufficiently accurate statistics for
learning.
</footnote>
<bodyText confidence="0.998471409090909">
The MUC-6 dataset consists of 30 documents for
testing and 221 for training. To evaluate the contri-
bution of the major components in our model, we
conducted five experiments, each differing from the
previous one in a single aspect. We emphasize that
our approach is unsupervised, and thus the data only
contains raw text plus true mention boundaries.
MLN-1 In this experiment, the base MLN was
used, and the head was chosen crudely as the
rightmost token in a mention. Our system was
run on each test document separately, using a
minimum of training data (the document itself).
MLN-30 Our system was trained on all 30 test doc-
uments together. This tests how much can be
gained by pooling information.
MLN-H The heads were determined using the head
rules in the Stanford parser (Klein &amp; Manning,
2003), plus simple heuristics to handle suffixes
such as “Corp.” and “Inc.”
MLN-HA The apposition rule was added.
MLN-HAN The predicate-nominal rule was added.
This is our full model.
We also compared with two rule-based MLNs:
RULE chose the head crudely as the rightmost token
in a mention, and did not include the apposition rule
and predicate-nominal rule; RULE-HAN chose the
head using the head rules in the Stanford parser, and
included the apposition rule and predicate-nominal
rule.
Past results on ACE were obtained on different
releases of the datasets, e.g., Haghighi and Klein
(2007) used the ACE-2004 training corpus, Ng
(2005) and Denis and Baldridge (2007) used ACE
Phrase-2, and Culotta et al. (2007) used the ACE-
2004 formal test set. In this paper, we used the
ACE-2004 training corpus and ACE Phrase-2 (ACE-
2) to enable direct comparisons with Haghighi &amp;
Klein (2007), Ng (2005), and Denis and Baldridge
(2007). Due to license restrictions, we were not able
to obtain the ACE-2004 formal test set and so cannot
compare directly to Culotta et al. (2007). The En-
glish version of the ACE-2004 training corpus con-
tains two sections, BNEWS and NWIRE, with 220
and 128 documents, respectively. ACE-2 contains a
</bodyText>
<page confidence="0.9993">
656
</page>
<tableCaption confidence="0.96941075">
Table 1: Comparison of coreference results in MUC
scores on the MUC-6 dataset.
Table 3: Comparison of coreference results in MUC
scores on the ACE-2 datasets.
</tableCaption>
<table confidence="0.999952545454546">
# Doc. Prec. Rec. F1
H&amp;K 60 80.8 52.8 63.9
H&amp;K 381 80.4 62.4 70.3
M&amp;W 221 - - 73.4
RULE - 76.0 65.9 70.5
RULE-HAN - 81.3 72.7 76.7
MLN-1 1 76.5 66.4 71.1
MLN-30 30 77.5 67.3 72.0
MLN-H 30 81.8 70.1 75.5
MLN-HA 30 82.7 75.1 78.7
MLN-HAN 30 83.0 75.8 79.2
</table>
<tableCaption confidence="0.998409">
Table 2: Comparison of coreference results in MUC
scores on the ACE-2004 (English) datasets.
</tableCaption>
<table confidence="0.999838166666667">
EN-BNEWS Prec. Rec. F1
H&amp;K 63.2 61.3 62.3
MLN-HAN 66.8 67.8 67.3
EN-NWIRE Prec. Rec. F1
H&amp;K 66.7 62.3 64.2
MLN-HAN 71.3 70.5 70.9
</table>
<bodyText confidence="0.996143">
training set and a test set. In our experiments, we
only used the test set, which contains three sections,
BNEWS, NWIRE, and NPAPER, with 51, 29, and
17 documents, respectively.
</bodyText>
<sectionHeader confidence="0.508783" genericHeader="evaluation">
6.3 Results
</sectionHeader>
<bodyText confidence="0.999893">
Table 1 compares our system with previous ap-
proaches on the MUC-6 dataset, in MUC scores.
Our approach greatly outperformed Haghighi &amp;
Klein (2007), the state-of-the-art unsupervised sys-
tem. Our system, trained on individual documents,
achieved an F1 score more than 7% higher than
theirs trained on 60 documents, and still outper-
formed it trained on 381 documents. Training on
the 30 test documents together resulted in a signif-
icant gain. (We also ran experiments using more
documents, and the results were similar.) Better
head identification (MLN-H) led to a large improve-
ment in accuracy, which is expected since for men-
tions with a right modifier, the rightmost tokens con-
fuse rather than help coreference (e.g., “the chair-
man of Microsoft”). Notice that with this improve-
ment our system already outperforms a state-of-the-
</bodyText>
<table confidence="0.999912916666667">
BNEWS Prec. Rec. F1
Ng 67.9 62.2 64.9
D&amp;B 78.0 62.1 69.2
MLN-HAN 68.3 66.6 67.4
NWIRE Prec. Rec. F1
Ng 60.3 50.1 54.7
D&amp;B 75.8 60.8 67.5
MLN-HAN 67.7 67.3 67.4
NPAPER Prec. Rec. F1
Ng 71.4 67.4 69.3
D&amp;B 77.6 68.0 72.5
MLN-HAN 69.2 71.7 70.4
</table>
<tableCaption confidence="0.987922">
Table 4: Comparison of coreference results in B3 scores
on the ACE-2 datasets.
</tableCaption>
<table confidence="0.999810666666667">
BNEWS Prec. Rec. F1
Ng 77.1 57.0 65.6
MLN-HAN 70.3 65.3 67.7
NWIRE Prec. Rec. F1
Ng 75.4 59.3 66.4
MLN-HAN 74.7 68.8 71.6
NPAPER Prec. Rec. F1
Ng 75.4 59.3 66.4
MLN-HAN 70.0 66.5 68.2
</table>
<bodyText confidence="0.9999144375">
art supervised system (McCallum &amp; Wellner, 2005).
Leveraging apposition resulted in another large im-
provement, and predicate nominals also helped. Our
full model scores about 9% higher than Haghighi &amp;
Klein (2007), and about 6% higher than McCallum
&amp; Wellner (2005). To our knowledge, this is the best
coreference accuracy reported on MUC-6 to date.7
The B3 scores of MLN-HAN on the MUC-6 dataset
are 77.4 (precision), 67.6 (recall) and 72.2 (F1).
(The other systems did not report B3.) Interest-
ingly, the rule-based MLN (RULE) sufficed to out-
perform Haghighi &amp; Klein (2007), and by using bet-
ter heads and the apposition and predicate-nominal
rules (RULE-HAN), it outperformed McCallum &amp;
Wellner (2005), the supervised system. The MLNs
with learning (MLN-30 and MLN-HAN), on the
</bodyText>
<footnote confidence="0.993343666666667">
7As pointed out by Haghighi &amp; Klein (2007), Luo et al.
(2004) obtained a very high accuracy on MUC-6, but their sys-
tem used gold NER features and is not directly comparable.
</footnote>
<page confidence="0.996424">
657
</page>
<tableCaption confidence="0.982574">
Table 5: Our coreference results in precision, recall, and
F1 for pairwise resolution.
</tableCaption>
<table confidence="0.999959571428572">
Pairwise Prec. Rec. F1
MUC-6 63.0 57.0 59.9
EN-BNEWS 51.2 36.4 42.5
EN-NWIRE 62.6 38.9 48.0
BNEWS 44.6 32.3 37.5
NWIRE 59.7 42.1 49.4
NPAPER 64.3 43.6 52.0
</table>
<tableCaption confidence="0.9898395">
Table 6: Average gold number of clusters per document
vs. the mean absolute error of our system.
</tableCaption>
<table confidence="0.999476">
# Clusters MUC-6 EN-BN EN-NW
Gold 15.4 22.3 37.2
Mean Error 4.7 3.0 4.8
# Clusters BNEWS NWIRE NPAPER
Gold 20.4 39.2 55.2
Mean Error 2.5 5.6 6.6
</table>
<bodyText confidence="0.998005580645161">
other hand, substantially outperformed the corre-
sponding rule-based ones.
Table 2 compares our system to Haghighi &amp; Klein
(2007) on the ACE-2004 training set in MUC scores.
Again, our system outperformed theirs by a large
margin. The B3 scores of MLN-HAN on the ACE-
2004 dataset are 71.6 (precision), 68.4 (recall) and
70.0 (F1) for BNEWS, and 75.7 (precision), 69.2
(recall) and 72.3 (F1) for NWIRE. (Haghighi &amp;
Klein (2007) did not report B3.) Due to license re-
strictions, we could not compare directly to Culotta
et al. (2007), who reported overall B3-F1 of 79.3 on
the formal test set.
Tables 3 and 4 compare our system to two re-
cent supervised systems, Ng (2005) and Denis
&amp; Baldridge (2007). Our approach significantly
outperformed Ng (2005). It tied with Denis &amp;
Baldridge (2007) on NWIRE, and was somewhat
less accurate on BNEWS and NPAPER.
Luo et al. (2004) pointed out that one can ob-
tain a very high MUC score simply by lumping all
mentions together. B3 suffers less from this prob-
lem but is not perfect. Thus we also report pairwise
resolution scores (Table 5), the gold number of clus-
ters, and our mean absolute error in the number of
clusters (Table 6). Systems that simply merge all
mentions will have exceedingly low pairwise preci-
sion (far below 50%), and very large errors in the
number of clusters. Our system has fairly good pair-
wise precisions and small mean error in the number
of clusters, which verifies that our results are sound.
</bodyText>
<subsectionHeader confidence="0.775879">
6.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9999639375">
Many of our system’s remaining errors involve nom-
inals. Additional features should be considered to
distinguish mentions that have the same head but are
different entities. For pronouns, many remaining er-
rors can be corrected using linguistic knowledge like
binding theory and salience hierarchy. Our heuris-
tics for identifying appositives and predicate nomi-
nals also make many errors, which often can be fixed
with additional name entity recognition capabilities
(e.g., given “Mike Sullivan, VOA News”, it helps to
know that the former is a person and the latter an
organization). The most challenging case involves
phrases with different heads that are both proper
nouns (e.g., “Mr. Bush” and “the White House”).
Handling these cases requires domain knowledge
and/or more powerful joint inference.
</bodyText>
<sectionHeader confidence="0.998827" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999983909090909">
This paper introduces the first unsupervised coref-
erence resolution system that is as accurate as su-
pervised systems. It performs joint inference among
mentions, using relations like apposition and predi-
cate nominals. It uses Markov logic as a representa-
tion language, which allows it to be easily extended
to incorporate additional linguistic and world knowl-
edge. Future directions include incorporating addi-
tional knowledge, conducting joint entity detection
and coreference resolution, and combining corefer-
ence resolution with other NLP tasks.
</bodyText>
<sectionHeader confidence="0.998541" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997628">
We thank the anonymous reviewers for their comments.
This research was funded by DARPA contracts NBCH-
D030010/02-000225, FA8750-07-D-0185, and HR0011-
07-C-0060, DARPA grant FA8750-05-2-0283, NSF grant
IIS-0534881, and ONR grant N-00014-05-1-0313 and
N00014-08-1-0670. The views and conclusions con-
tained in this document are those of the authors and
should not be interpreted as necessarily representing the
official policies, either expressed or implied, of DARPA,
NSF, ONR, or the United States Government.
</bodyText>
<page confidence="0.998248">
658
</page>
<sectionHeader confidence="0.995877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999960703703704">
Amit, B. &amp; Baldwin, B. 1998. Algorithms for scoring
coreference chains. In Proc. MUC-7.
Bakir, G.; Hofmann, T.; Sch¨olkopf, B.; Smola, A.;
Taskar, B. and Vishwanathan, S. (eds.) 2007. Pre-
dicting Structured Data. MIT Press.
Culotta, A.; Wick, M.; Hall, R. and McCallum, A. 2007.
First-order probabilistic models for coreference reso-
lution. In Proc. NAACL-07.
Denis, P. &amp; Baldridge, J. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proc. NAACL-07.
Getoor, L. &amp; Taskar, B. (eds.) 2007. Introduction to
Statistical Relational Learning. MIT Press.
Haghighi, A. &amp; Klein, D. 2007. Unsupervised corefer-
ence resolution in a nonparametric Bayesian model. In
Proc. ACL-07.
Kautz, H.; Selman, B.; and Jiang, Y. 1997. A general
stochastic approach to solving problems with hard and
soft constraints. In The Satisfiability Problem: Theory
and Applications. AMS.
Klein, D. &amp; Manning, C. 2003. Accurate unlexicalized
parsing. In Proc. ACL-03.
Kok, S.; Singla, P.; Richardson, M.; Domingos,
P.; Sumner, M.; Poon, H. &amp; Lowd, D. 2007.
The Alchemy system for statistical relational AI.
http://alchemy.cs.washington.edu/.
Lowd, D. &amp; Domingos, D. 2007. Efficient weight learn-
ing for Markov logic networks. In Proc. PKDD-07.
Luo, X.; Ittycheriah, A.; Jing, H.; Kambhatla, N. and
Roukos, S. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proc. ACL-04.
McCallum, A. &amp; Wellner, B. 2005. Conditional models
of identity uncertainty with application to noun coref-
erence. In Proc. NIPS-04.
Ng, V. 2005. Machine Learning for Coreference Resolu-
tion: From Local Classification to Global Ranking. In
Proc. ACL-05.
Poon, H. &amp; Domingos, P. 2006. Sound and efficient
inference with probabilistic and deterministic depen-
dencies. In Proc. AAAI-06.
Poon, H. &amp; Domingos, P. 2007. Joint inference in infor-
mation extraction. In Proc. AAAI-07.
Poon, H. &amp; Domingos, P. 2008. A general method for
reducing the complexity of relational inference and its
application to MCMC. In Proc. AAAI-08.
Richardson, M. &amp; Domingos, P. 2006. Markov logic
networks. Machine Learning 62:107–136.
Vilain, M.; Burger, J.; Aberdeen, J.; Connolly, D. &amp;
Hirschman, L. 1995. A model-theoretic coreference
scoring scheme. In Proc. MUC-6.
Wei, W.; Erenrich, J. and Selman, B. 2004. Towards
efficient sampling: Exploiting random walk strategies.
In Proc. AAAI-04.
</reference>
<page confidence="0.998962">
659
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923810">
<title confidence="0.992853">Joint Unsupervised Coreference Resolution with Markov Logic</title>
<author confidence="0.996191">Hoifung Poon Pedro Domingos</author>
<affiliation confidence="0.9998015">Department of Computer Science and University of</affiliation>
<address confidence="0.999088">Seattle, WA 98195-2350,</address>
<abstract confidence="0.9967443">Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Amit</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proc. MUC-7.</booktitle>
<contexts>
<context position="25779" citStr="Amit &amp; Baldwin, 1998" startWordPosition="4202" endWordPosition="4206">in the ground truth, nonpronouns are clustered by their heads. (Effectively, the corresponding InClust atoms are assigned to appropriate values and are included in Y rather than Z during learning.) We used 30 iterations of PSCG for learning. (In preliminary experiments, additional iterations had little effect on coreference accuracy.) We generated 100 samples using MC-SAT for each expectation approximation.6 6.2 Methodology We conducted experiments on MUC-6, ACE-2004, and ACE Phrase-2 (ACE-2). We evaluated our systems using two commonly-used scoring programs: MUC (Vilain et al., 1995) and B3 (Amit &amp; Baldwin, 1998). To gain more insight, we also report pairwise resolution scores and mean absolute error in the number of clusters. 6Each sample actually contains a large number of groundings, so 100 samples yield sufficiently accurate statistics for learning. The MUC-6 dataset consists of 30 documents for testing and 221 for training. To evaluate the contribution of the major components in our model, we conducted five experiments, each differing from the previous one in a single aspect. We emphasize that our approach is unsupervised, and thus the data only contains raw text plus true mention boundaries. MLN</context>
</contexts>
<marker>Amit, Baldwin, 1998</marker>
<rawString>Amit, B. &amp; Baldwin, B. 1998. Algorithms for scoring coreference chains. In Proc. MUC-7.</rawString>
</citation>
<citation valid="true">
<title>Predicting Structured Data.</title>
<date>2007</date>
<editor>Bakir, G.; Hofmann, T.; Sch¨olkopf, B.; Smola, A.; Taskar, B. and Vishwanathan, S. (eds.)</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1850" citStr="(2007)" startWordPosition="265" endWordPosition="265">mation extraction, question answering, machine translation, and others. Supervised learning approaches treat the problem as one of classification: for each pair of mentions, predict whether they corefer or not (e.g., McCallum &amp; Wellner (2005)). While successful, these approaches require labeled training data, consisting of mention pairs and the correct decisions for them. This limits their applicability. Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text. However, unsupervised coreference resolution is much more difficult. Haghighi and Klein’s (2007) model, the most sophisticated to date, still lags supervised ones by a substantial margin. Extending it appears difficult, due to the limitations of its Dirichlet process-based representation. The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the “easy” decisions to help make related “hard” ones. Relations that have been exploited in supervised coreference resolution include transitivity (McCallum &amp; Wellner, 2005) and anaphoricity (Denis &amp; Baldridge, 2007). However, there is little work to date on jo</context>
<context position="3159" citStr="(2007)" startWordPosition="456" endWordPosition="456">lexible language that combines probabilistic graphical models and first-order logic (Richardson &amp; Domingos, 2006). Markov logic allows us to easily build models involving relations among mentions, like apposition and predicate nominals. By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution. We test our approach on standard MUC and ACE datasets. Our basic model, trained on a minimum of data, suffices to outperform Haghighi and Klein’s (2007) one. Our full model, using apposition and other relations for joint inference, is often as accurate as the best supervised models, or more. 650 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650–659, Honolulu, October 2008.c�2008 Association for Computational Linguistics We begin by reviewing the necessary background on Markov logic. We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used. Finally, we present our experiments and results. 2 Related Work Most existin</context>
<context position="4543" citStr="(2007)" startWordPosition="660" endWordPosition="660">005). Moreover, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be helpful (Poon &amp; Domingos, 2007). Using graph partitioning, McCallum &amp; Wellner (2005) incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions. Denis &amp; Baldridge (2007) determined anaphoricity and pairwise classification jointly using integer programming, but they did not incorporate transitivity or other relations. While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date. Prior to our work, the best performance in unsupervised coreference resolution was achieved by Haghighi &amp; Klein (2007), using a nonparametric Bayesian model based on hierarchical Dirichlet processes. At the heart of their system is a mixture model with a few linguistically motivated features suc</context>
<context position="12629" citStr="(2007)" startWordPosition="1983" endWordPosition="1983">d and always conditioned on. The optimization problem is convex and a global optimum can be found using gradient 652 descent, with the gradient being ∂ L(y) = ni(y) − �y, P(Y = y&apos;)ni(y&apos;) ∂w = ni(y) − EY[ni]. where ni is the number of true groundings of clause i. The expected count can be approximated as �N 1 EY [ni] Pz� N k=1 where yk are samples generated by MC-SAT. To combat overfitting, a Gaussian prior is imposed on all weights. In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses. Lowd &amp; Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem. This estimates the optimal step size in each step as −dT g dT Hd + AdT d. where g is the gradient, d the conjugate update direction, and A a parameter that is automatically tuned to trade off second-order information with gradient descent. H is the Hessian matrix, with the (i, j)th entry being ∂2 ∂w∂w� L(y) = EY [ni] · EY [nj] − EY [ni · nj] = −CovY [ni,nj]. The Hessian can be approximated with the same samples used for the gradient. Its negative inverse diagonal is used as the preconditioner.1 The open-</context>
<context position="13844" citStr="(2007)" startWordPosition="2184" endWordPosition="2184">chemy package (Kok et al., 2007) provides implementations of existing algorithms for Markov logic. In Section 5, we develop the first general-purpose unsupervised learning algorithm for Markov logic by extending the existing algorithms to handle hidden predicates.2 1Lowd &amp; Domingos showed that α can be computed more efficiently, without explicitly approximating or storing the Hessian. Readers are referred to their paper for details. 2Alchemy includes a discriminative EM algorithm, but it assumes that only a few values are missing, and cannot handle completely hidden predicates. Kok &amp; Domingos (2007) applied Markov logic to relational clustering, but they used hard EM. 4 An MLN for Joint Unsupervised Coreference Resolution In this section, we present our MLN for joint unsupervised coreference resolution. Our model deviates from Haghighi &amp; Klein’s (2007) in several important ways. First, our MLN does not model saliences for proper nouns or nominals, as their influence is marginal compared to other features; for pronoun salience, it uses a more intuitive and simpler definition based on distance, and incorporated it as a prior. Another difference is in identifying heads. For the ACE datasets</context>
<context position="15177" citStr="(2007)" startWordPosition="2408" endWordPosition="2408">ost token in a mention. We show that a better way is to determine the heads using head rules in a parser. This improves resolution accuracy and is always applicable. Crucially, our MLN leverages syntactic relations such as apposition and predicate nominals, which are not used by Haghighi and Klein. In our approach, what it takes is just adding two formulas to the MLN. As common in previous work, we assume that true mention boundaries are given. We do not assume any other labeled information. In particular, we do not assume gold name entity recognition (NER) labels, and unlike Haghighi &amp; Klein (2007), we do not assume gold mention types (for ACE datasets, they also used gold head words). We determined the head of a mention either by taking its rightmost token, or by using the head rules in a parser. We detected pronouns using a list. 4.1 Base MLN The main query predicate is InClust(m, c!), which is true iff mention m is in cluster c. The “t!” notation signifies that for each m, this predicate is true for a unique value of c. The main evidence predicate is Head(m, t!), where m is a mention and t a token, and which is true iff t is the head of m. A key component in our MLN is a simple head </context>
<context position="22085" citStr="(2007)" startWordPosition="3598" endWordPosition="3598"> of the ith clause. We extended PSCG for unsupervised learning. The gradient is the difference of two expectations, each of which can be approximated using samples generated by MC-SAT. The (i, j)th entry of the Hessian is now a2 awiawj L(y) = CovZ|y[ni, nj] − CovY,Z[ni, nj] and the step size can be computed accordingly. Since our problem is no longer convex, the negative diagonal Hessian may contain zero or negative entries, so we first took the absolute values of the diagonal and added 1, then used the inverse as the preconditioner. We also adjusted A more conservatively than Lowd &amp; Domingos (2007). Notice that when the objects form independent subsets (in our cases, mentions in each document), we can process them in parallel and then gather sufficient statistics for learning. We developed an efficient parallelized implementation of our unsupervised learning algorithm using the message-passing interface (MPI). Learning in MUC-6 took only one hour, and in ACE-2004 two and a half. To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al.,</context>
<context position="27414" citStr="(2007)" startWordPosition="4475" endWordPosition="4475"> &amp; Manning, 2003), plus simple heuristics to handle suffixes such as “Corp.” and “Inc.” MLN-HA The apposition rule was added. MLN-HAN The predicate-nominal rule was added. This is our full model. We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule. Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set. In this paper, we used the ACE-2004 training corpus and ACE Phrase-2 (ACE2) to enable direct comparisons with Haghighi &amp; Klein (2007), Ng (2005), and Denis and Baldridge (2007). Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007). The English version of the ACE-2004 training corpus contains two sections, BNEWS and NWIRE, with 220 and 128 documents, resp</context>
<context position="29018" citStr="(2007)" startWordPosition="4758" endWordPosition="4758">A 30 82.7 75.1 78.7 MLN-HAN 30 83.0 75.8 79.2 Table 2: Comparison of coreference results in MUC scores on the ACE-2004 (English) datasets. EN-BNEWS Prec. Rec. F1 H&amp;K 63.2 61.3 62.3 MLN-HAN 66.8 67.8 67.3 EN-NWIRE Prec. Rec. F1 H&amp;K 66.7 62.3 64.2 MLN-HAN 71.3 70.5 70.9 training set and a test set. In our experiments, we only used the test set, which contains three sections, BNEWS, NWIRE, and NPAPER, with 51, 29, and 17 documents, respectively. 6.3 Results Table 1 compares our system with previous approaches on the MUC-6 dataset, in MUC scores. Our approach greatly outperformed Haghighi &amp; Klein (2007), the state-of-the-art unsupervised system. Our system, trained on individual documents, achieved an F1 score more than 7% higher than theirs trained on 60 documents, and still outperformed it trained on 381 documents. Training on the 30 test documents together resulted in a significant gain. (We also ran experiments using more documents, and the results were similar.) Better head identification (MLN-H) led to a large improvement in accuracy, which is expected since for mentions with a right modifier, the rightmost tokens confuse rather than help coreference (e.g., “the chairman of Microsoft”)</context>
<context position="30416" citStr="(2007)" startWordPosition="4996" endWordPosition="4996">1 54.7 D&amp;B 75.8 60.8 67.5 MLN-HAN 67.7 67.3 67.4 NPAPER Prec. Rec. F1 Ng 71.4 67.4 69.3 D&amp;B 77.6 68.0 72.5 MLN-HAN 69.2 71.7 70.4 Table 4: Comparison of coreference results in B3 scores on the ACE-2 datasets. BNEWS Prec. Rec. F1 Ng 77.1 57.0 65.6 MLN-HAN 70.3 65.3 67.7 NWIRE Prec. Rec. F1 Ng 75.4 59.3 66.4 MLN-HAN 74.7 68.8 71.6 NPAPER Prec. Rec. F1 Ng 75.4 59.3 66.4 MLN-HAN 70.0 66.5 68.2 art supervised system (McCallum &amp; Wellner, 2005). Leveraging apposition resulted in another large improvement, and predicate nominals also helped. Our full model scores about 9% higher than Haghighi &amp; Klein (2007), and about 6% higher than McCallum &amp; Wellner (2005). To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1). (The other systems did not report B3.) Interestingly, the rule-based MLN (RULE) sufficed to outperform Haghighi &amp; Klein (2007), and by using better heads and the apposition and predicate-nominal rules (RULE-HAN), it outperformed McCallum &amp; Wellner (2005), the supervised system. The MLNs with learning (MLN-30 and MLN-HAN), on the 7As pointed out by Haghighi &amp; Klei</context>
</contexts>
<marker>2007</marker>
<rawString>Bakir, G.; Hofmann, T.; Sch¨olkopf, B.; Smola, A.; Taskar, B. and Vishwanathan, S. (eds.) 2007. Predicting Structured Data. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>M Wick</author>
<author>R Hall</author>
<author>A McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution. In</title>
<date>2007</date>
<booktitle>Proc. NAACL-07.</booktitle>
<contexts>
<context position="27535" citStr="Culotta et al. (2007)" startWordPosition="4492" endWordPosition="4495">ion rule was added. MLN-HAN The predicate-nominal rule was added. This is our full model. We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule. Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set. In this paper, we used the ACE-2004 training corpus and ACE Phrase-2 (ACE2) to enable direct comparisons with Haghighi &amp; Klein (2007), Ng (2005), and Denis and Baldridge (2007). Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007). The English version of the ACE-2004 training corpus contains two sections, BNEWS and NWIRE, with 220 and 128 documents, respectively. ACE-2 contains a 656 Table 1: Comparison of coreference results in MUC scores on the MUC-6 dataset. Table 3: Co</context>
<context position="32173" citStr="Culotta et al. (2007)" startWordPosition="5293" endWordPosition="5296">n Error 4.7 3.0 4.8 # Clusters BNEWS NWIRE NPAPER Gold 20.4 39.2 55.2 Mean Error 2.5 5.6 6.6 other hand, substantially outperformed the corresponding rule-based ones. Table 2 compares our system to Haghighi &amp; Klein (2007) on the ACE-2004 training set in MUC scores. Again, our system outperformed theirs by a large margin. The B3 scores of MLN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE. (Haghighi &amp; Klein (2007) did not report B3.) Due to license restrictions, we could not compare directly to Culotta et al. (2007), who reported overall B3-F1 of 79.3 on the formal test set. Tables 3 and 4 compare our system to two recent supervised systems, Ng (2005) and Denis &amp; Baldridge (2007). Our approach significantly outperformed Ng (2005). It tied with Denis &amp; Baldridge (2007) on NWIRE, and was somewhat less accurate on BNEWS and NPAPER. Luo et al. (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. B3 suffers less from this problem but is not perfect. Thus we also report pairwise resolution scores (Table 5), the gold number of clusters, and our mean absolute erro</context>
</contexts>
<marker>Culotta, Wick, Hall, McCallum, 2007</marker>
<rawString>Culotta, A.; Wick, M.; Hall, R. and McCallum, A. 2007. First-order probabilistic models for coreference resolution. In Proc. NAACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proc. NAACL-07.</booktitle>
<contexts>
<context position="27490" citStr="Denis and Baldridge (2007)" startWordPosition="4484" endWordPosition="4487">ixes such as “Corp.” and “Inc.” MLN-HA The apposition rule was added. MLN-HAN The predicate-nominal rule was added. This is our full model. We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule. Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set. In this paper, we used the ACE-2004 training corpus and ACE Phrase-2 (ACE2) to enable direct comparisons with Haghighi &amp; Klein (2007), Ng (2005), and Denis and Baldridge (2007). Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007). The English version of the ACE-2004 training corpus contains two sections, BNEWS and NWIRE, with 220 and 128 documents, respectively. ACE-2 contains a 656 Table 1: Comparison of coreference results in</context>
<context position="2405" citStr="Denis &amp; Baldridge, 2007" startWordPosition="341" endWordPosition="344">erence resolution is much more difficult. Haghighi and Klein’s (2007) model, the most sophisticated to date, still lags supervised ones by a substantial margin. Extending it appears difficult, due to the limitations of its Dirichlet process-based representation. The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the “easy” decisions to help make related “hard” ones. Relations that have been exploited in supervised coreference resolution include transitivity (McCallum &amp; Wellner, 2005) and anaphoricity (Denis &amp; Baldridge, 2007). However, there is little work to date on joint inference for unsupervised resolution. We address this problem using Markov logic, a powerful and flexible language that combines probabilistic graphical models and first-order logic (Richardson &amp; Domingos, 2006). Markov logic allows us to easily build models involving relations among mentions, like apposition and predicate nominals. By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution</context>
<context position="4543" citStr="Denis &amp; Baldridge (2007)" startWordPosition="657" endWordPosition="660">tprocessing (Ng, 2005). Moreover, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be helpful (Poon &amp; Domingos, 2007). Using graph partitioning, McCallum &amp; Wellner (2005) incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions. Denis &amp; Baldridge (2007) determined anaphoricity and pairwise classification jointly using integer programming, but they did not incorporate transitivity or other relations. While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date. Prior to our work, the best performance in unsupervised coreference resolution was achieved by Haghighi &amp; Klein (2007), using a nonparametric Bayesian model based on hierarchical Dirichlet processes. At the heart of their system is a mixture model with a few linguistically motivated features suc</context>
<context position="32340" citStr="Denis &amp; Baldridge (2007)" startWordPosition="5324" endWordPosition="5327">s. Table 2 compares our system to Haghighi &amp; Klein (2007) on the ACE-2004 training set in MUC scores. Again, our system outperformed theirs by a large margin. The B3 scores of MLN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE. (Haghighi &amp; Klein (2007) did not report B3.) Due to license restrictions, we could not compare directly to Culotta et al. (2007), who reported overall B3-F1 of 79.3 on the formal test set. Tables 3 and 4 compare our system to two recent supervised systems, Ng (2005) and Denis &amp; Baldridge (2007). Our approach significantly outperformed Ng (2005). It tied with Denis &amp; Baldridge (2007) on NWIRE, and was somewhat less accurate on BNEWS and NPAPER. Luo et al. (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. B3 suffers less from this problem but is not perfect. Thus we also report pairwise resolution scores (Table 5), the gold number of clusters, and our mean absolute error in the number of clusters (Table 6). Systems that simply merge all mentions will have exceedingly low pairwise precision (far below 50%), and very large errors in th</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Denis, P. &amp; Baldridge, J. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Proc. NAACL-07.</rawString>
</citation>
<citation valid="true">
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<editor>Getoor, L. &amp; Taskar, B. (eds.)</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1850" citStr="(2007)" startWordPosition="265" endWordPosition="265">mation extraction, question answering, machine translation, and others. Supervised learning approaches treat the problem as one of classification: for each pair of mentions, predict whether they corefer or not (e.g., McCallum &amp; Wellner (2005)). While successful, these approaches require labeled training data, consisting of mention pairs and the correct decisions for them. This limits their applicability. Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text. However, unsupervised coreference resolution is much more difficult. Haghighi and Klein’s (2007) model, the most sophisticated to date, still lags supervised ones by a substantial margin. Extending it appears difficult, due to the limitations of its Dirichlet process-based representation. The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the “easy” decisions to help make related “hard” ones. Relations that have been exploited in supervised coreference resolution include transitivity (McCallum &amp; Wellner, 2005) and anaphoricity (Denis &amp; Baldridge, 2007). However, there is little work to date on jo</context>
<context position="3159" citStr="(2007)" startWordPosition="456" endWordPosition="456">lexible language that combines probabilistic graphical models and first-order logic (Richardson &amp; Domingos, 2006). Markov logic allows us to easily build models involving relations among mentions, like apposition and predicate nominals. By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution. We test our approach on standard MUC and ACE datasets. Our basic model, trained on a minimum of data, suffices to outperform Haghighi and Klein’s (2007) one. Our full model, using apposition and other relations for joint inference, is often as accurate as the best supervised models, or more. 650 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650–659, Honolulu, October 2008.c�2008 Association for Computational Linguistics We begin by reviewing the necessary background on Markov logic. We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used. Finally, we present our experiments and results. 2 Related Work Most existin</context>
<context position="4543" citStr="(2007)" startWordPosition="660" endWordPosition="660">005). Moreover, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be helpful (Poon &amp; Domingos, 2007). Using graph partitioning, McCallum &amp; Wellner (2005) incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions. Denis &amp; Baldridge (2007) determined anaphoricity and pairwise classification jointly using integer programming, but they did not incorporate transitivity or other relations. While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date. Prior to our work, the best performance in unsupervised coreference resolution was achieved by Haghighi &amp; Klein (2007), using a nonparametric Bayesian model based on hierarchical Dirichlet processes. At the heart of their system is a mixture model with a few linguistically motivated features suc</context>
<context position="12629" citStr="(2007)" startWordPosition="1983" endWordPosition="1983">d and always conditioned on. The optimization problem is convex and a global optimum can be found using gradient 652 descent, with the gradient being ∂ L(y) = ni(y) − �y, P(Y = y&apos;)ni(y&apos;) ∂w = ni(y) − EY[ni]. where ni is the number of true groundings of clause i. The expected count can be approximated as �N 1 EY [ni] Pz� N k=1 where yk are samples generated by MC-SAT. To combat overfitting, a Gaussian prior is imposed on all weights. In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses. Lowd &amp; Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem. This estimates the optimal step size in each step as −dT g dT Hd + AdT d. where g is the gradient, d the conjugate update direction, and A a parameter that is automatically tuned to trade off second-order information with gradient descent. H is the Hessian matrix, with the (i, j)th entry being ∂2 ∂w∂w� L(y) = EY [ni] · EY [nj] − EY [ni · nj] = −CovY [ni,nj]. The Hessian can be approximated with the same samples used for the gradient. Its negative inverse diagonal is used as the preconditioner.1 The open-</context>
<context position="13844" citStr="(2007)" startWordPosition="2184" endWordPosition="2184">chemy package (Kok et al., 2007) provides implementations of existing algorithms for Markov logic. In Section 5, we develop the first general-purpose unsupervised learning algorithm for Markov logic by extending the existing algorithms to handle hidden predicates.2 1Lowd &amp; Domingos showed that α can be computed more efficiently, without explicitly approximating or storing the Hessian. Readers are referred to their paper for details. 2Alchemy includes a discriminative EM algorithm, but it assumes that only a few values are missing, and cannot handle completely hidden predicates. Kok &amp; Domingos (2007) applied Markov logic to relational clustering, but they used hard EM. 4 An MLN for Joint Unsupervised Coreference Resolution In this section, we present our MLN for joint unsupervised coreference resolution. Our model deviates from Haghighi &amp; Klein’s (2007) in several important ways. First, our MLN does not model saliences for proper nouns or nominals, as their influence is marginal compared to other features; for pronoun salience, it uses a more intuitive and simpler definition based on distance, and incorporated it as a prior. Another difference is in identifying heads. For the ACE datasets</context>
<context position="15177" citStr="(2007)" startWordPosition="2408" endWordPosition="2408">ost token in a mention. We show that a better way is to determine the heads using head rules in a parser. This improves resolution accuracy and is always applicable. Crucially, our MLN leverages syntactic relations such as apposition and predicate nominals, which are not used by Haghighi and Klein. In our approach, what it takes is just adding two formulas to the MLN. As common in previous work, we assume that true mention boundaries are given. We do not assume any other labeled information. In particular, we do not assume gold name entity recognition (NER) labels, and unlike Haghighi &amp; Klein (2007), we do not assume gold mention types (for ACE datasets, they also used gold head words). We determined the head of a mention either by taking its rightmost token, or by using the head rules in a parser. We detected pronouns using a list. 4.1 Base MLN The main query predicate is InClust(m, c!), which is true iff mention m is in cluster c. The “t!” notation signifies that for each m, this predicate is true for a unique value of c. The main evidence predicate is Head(m, t!), where m is a mention and t a token, and which is true iff t is the head of m. A key component in our MLN is a simple head </context>
<context position="22085" citStr="(2007)" startWordPosition="3598" endWordPosition="3598"> of the ith clause. We extended PSCG for unsupervised learning. The gradient is the difference of two expectations, each of which can be approximated using samples generated by MC-SAT. The (i, j)th entry of the Hessian is now a2 awiawj L(y) = CovZ|y[ni, nj] − CovY,Z[ni, nj] and the step size can be computed accordingly. Since our problem is no longer convex, the negative diagonal Hessian may contain zero or negative entries, so we first took the absolute values of the diagonal and added 1, then used the inverse as the preconditioner. We also adjusted A more conservatively than Lowd &amp; Domingos (2007). Notice that when the objects form independent subsets (in our cases, mentions in each document), we can process them in parallel and then gather sufficient statistics for learning. We developed an efficient parallelized implementation of our unsupervised learning algorithm using the message-passing interface (MPI). Learning in MUC-6 took only one hour, and in ACE-2004 two and a half. To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al.,</context>
<context position="27414" citStr="(2007)" startWordPosition="4475" endWordPosition="4475"> &amp; Manning, 2003), plus simple heuristics to handle suffixes such as “Corp.” and “Inc.” MLN-HA The apposition rule was added. MLN-HAN The predicate-nominal rule was added. This is our full model. We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule. Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set. In this paper, we used the ACE-2004 training corpus and ACE Phrase-2 (ACE2) to enable direct comparisons with Haghighi &amp; Klein (2007), Ng (2005), and Denis and Baldridge (2007). Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007). The English version of the ACE-2004 training corpus contains two sections, BNEWS and NWIRE, with 220 and 128 documents, resp</context>
<context position="29018" citStr="(2007)" startWordPosition="4758" endWordPosition="4758">A 30 82.7 75.1 78.7 MLN-HAN 30 83.0 75.8 79.2 Table 2: Comparison of coreference results in MUC scores on the ACE-2004 (English) datasets. EN-BNEWS Prec. Rec. F1 H&amp;K 63.2 61.3 62.3 MLN-HAN 66.8 67.8 67.3 EN-NWIRE Prec. Rec. F1 H&amp;K 66.7 62.3 64.2 MLN-HAN 71.3 70.5 70.9 training set and a test set. In our experiments, we only used the test set, which contains three sections, BNEWS, NWIRE, and NPAPER, with 51, 29, and 17 documents, respectively. 6.3 Results Table 1 compares our system with previous approaches on the MUC-6 dataset, in MUC scores. Our approach greatly outperformed Haghighi &amp; Klein (2007), the state-of-the-art unsupervised system. Our system, trained on individual documents, achieved an F1 score more than 7% higher than theirs trained on 60 documents, and still outperformed it trained on 381 documents. Training on the 30 test documents together resulted in a significant gain. (We also ran experiments using more documents, and the results were similar.) Better head identification (MLN-H) led to a large improvement in accuracy, which is expected since for mentions with a right modifier, the rightmost tokens confuse rather than help coreference (e.g., “the chairman of Microsoft”)</context>
<context position="30416" citStr="(2007)" startWordPosition="4996" endWordPosition="4996">1 54.7 D&amp;B 75.8 60.8 67.5 MLN-HAN 67.7 67.3 67.4 NPAPER Prec. Rec. F1 Ng 71.4 67.4 69.3 D&amp;B 77.6 68.0 72.5 MLN-HAN 69.2 71.7 70.4 Table 4: Comparison of coreference results in B3 scores on the ACE-2 datasets. BNEWS Prec. Rec. F1 Ng 77.1 57.0 65.6 MLN-HAN 70.3 65.3 67.7 NWIRE Prec. Rec. F1 Ng 75.4 59.3 66.4 MLN-HAN 74.7 68.8 71.6 NPAPER Prec. Rec. F1 Ng 75.4 59.3 66.4 MLN-HAN 70.0 66.5 68.2 art supervised system (McCallum &amp; Wellner, 2005). Leveraging apposition resulted in another large improvement, and predicate nominals also helped. Our full model scores about 9% higher than Haghighi &amp; Klein (2007), and about 6% higher than McCallum &amp; Wellner (2005). To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1). (The other systems did not report B3.) Interestingly, the rule-based MLN (RULE) sufficed to outperform Haghighi &amp; Klein (2007), and by using better heads and the apposition and predicate-nominal rules (RULE-HAN), it outperformed McCallum &amp; Wellner (2005), the supervised system. The MLNs with learning (MLN-30 and MLN-HAN), on the 7As pointed out by Haghighi &amp; Klei</context>
</contexts>
<marker>2007</marker>
<rawString>Getoor, L. &amp; Taskar, B. (eds.) 2007. Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric Bayesian model.</title>
<date>2007</date>
<booktitle>In Proc. ACL-07.</booktitle>
<contexts>
<context position="4965" citStr="Haghighi &amp; Klein (2007)" startWordPosition="714" endWordPosition="717">ation and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions. Denis &amp; Baldridge (2007) determined anaphoricity and pairwise classification jointly using integer programming, but they did not incorporate transitivity or other relations. While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date. Prior to our work, the best performance in unsupervised coreference resolution was achieved by Haghighi &amp; Klein (2007), using a nonparametric Bayesian model based on hierarchical Dirichlet processes. At the heart of their system is a mixture model with a few linguistically motivated features such as head words, entity properties and salience. Their approach is a major step forward in unsupervised coreference resolution, but extending it is challenging. The main advantage of Dirichlet processes is that they are exchangeable, allowing parameters to be integrated out, but Haghighi and Klein forgo this when they introduce salience. Their model thus requires Gibbs sampling over both assignments and parameters, whi</context>
<context position="15177" citStr="Haghighi &amp; Klein (2007)" startWordPosition="2405" endWordPosition="2408">picked the rightmost token in a mention. We show that a better way is to determine the heads using head rules in a parser. This improves resolution accuracy and is always applicable. Crucially, our MLN leverages syntactic relations such as apposition and predicate nominals, which are not used by Haghighi and Klein. In our approach, what it takes is just adding two formulas to the MLN. As common in previous work, we assume that true mention boundaries are given. We do not assume any other labeled information. In particular, we do not assume gold name entity recognition (NER) labels, and unlike Haghighi &amp; Klein (2007), we do not assume gold mention types (for ACE datasets, they also used gold head words). We determined the head of a mention either by taking its rightmost token, or by using the head rules in a parser. We detected pronouns using a list. 4.1 Base MLN The main query predicate is InClust(m, c!), which is true iff mention m is in cluster c. The “t!” notation signifies that for each m, this predicate is true for a unique value of c. The main evidence predicate is Head(m, t!), where m is a mention and t a token, and which is true iff t is the head of m. A key component in our MLN is a simple head </context>
<context position="27703" citStr="Haghighi &amp; Klein (2007)" startWordPosition="4522" endWordPosition="4525">e rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule. Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set. In this paper, we used the ACE-2004 training corpus and ACE Phrase-2 (ACE2) to enable direct comparisons with Haghighi &amp; Klein (2007), Ng (2005), and Denis and Baldridge (2007). Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007). The English version of the ACE-2004 training corpus contains two sections, BNEWS and NWIRE, with 220 and 128 documents, respectively. ACE-2 contains a 656 Table 1: Comparison of coreference results in MUC scores on the MUC-6 dataset. Table 3: Comparison of coreference results in MUC scores on the ACE-2 datasets. # Doc. Prec. Rec. F1 H&amp;K 60 80.8 52.8 63.9 H&amp;K 381 80.4 62.4 70.3 M&amp;W 221 - - 73.4 RULE - 76.0 65.9</context>
<context position="29018" citStr="Haghighi &amp; Klein (2007)" startWordPosition="4755" endWordPosition="4758">8 70.1 75.5 MLN-HA 30 82.7 75.1 78.7 MLN-HAN 30 83.0 75.8 79.2 Table 2: Comparison of coreference results in MUC scores on the ACE-2004 (English) datasets. EN-BNEWS Prec. Rec. F1 H&amp;K 63.2 61.3 62.3 MLN-HAN 66.8 67.8 67.3 EN-NWIRE Prec. Rec. F1 H&amp;K 66.7 62.3 64.2 MLN-HAN 71.3 70.5 70.9 training set and a test set. In our experiments, we only used the test set, which contains three sections, BNEWS, NWIRE, and NPAPER, with 51, 29, and 17 documents, respectively. 6.3 Results Table 1 compares our system with previous approaches on the MUC-6 dataset, in MUC scores. Our approach greatly outperformed Haghighi &amp; Klein (2007), the state-of-the-art unsupervised system. Our system, trained on individual documents, achieved an F1 score more than 7% higher than theirs trained on 60 documents, and still outperformed it trained on 381 documents. Training on the 30 test documents together resulted in a significant gain. (We also ran experiments using more documents, and the results were similar.) Better head identification (MLN-H) led to a large improvement in accuracy, which is expected since for mentions with a right modifier, the rightmost tokens confuse rather than help coreference (e.g., “the chairman of Microsoft”)</context>
<context position="30416" citStr="Haghighi &amp; Klein (2007)" startWordPosition="4993" endWordPosition="4996">c. F1 Ng 60.3 50.1 54.7 D&amp;B 75.8 60.8 67.5 MLN-HAN 67.7 67.3 67.4 NPAPER Prec. Rec. F1 Ng 71.4 67.4 69.3 D&amp;B 77.6 68.0 72.5 MLN-HAN 69.2 71.7 70.4 Table 4: Comparison of coreference results in B3 scores on the ACE-2 datasets. BNEWS Prec. Rec. F1 Ng 77.1 57.0 65.6 MLN-HAN 70.3 65.3 67.7 NWIRE Prec. Rec. F1 Ng 75.4 59.3 66.4 MLN-HAN 74.7 68.8 71.6 NPAPER Prec. Rec. F1 Ng 75.4 59.3 66.4 MLN-HAN 70.0 66.5 68.2 art supervised system (McCallum &amp; Wellner, 2005). Leveraging apposition resulted in another large improvement, and predicate nominals also helped. Our full model scores about 9% higher than Haghighi &amp; Klein (2007), and about 6% higher than McCallum &amp; Wellner (2005). To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1). (The other systems did not report B3.) Interestingly, the rule-based MLN (RULE) sufficed to outperform Haghighi &amp; Klein (2007), and by using better heads and the apposition and predicate-nominal rules (RULE-HAN), it outperformed McCallum &amp; Wellner (2005), the supervised system. The MLNs with learning (MLN-30 and MLN-HAN), on the 7As pointed out by Haghighi &amp; Klei</context>
<context position="31773" citStr="Haghighi &amp; Klein (2007)" startWordPosition="5223" endWordPosition="5226">able. 657 Table 5: Our coreference results in precision, recall, and F1 for pairwise resolution. Pairwise Prec. Rec. F1 MUC-6 63.0 57.0 59.9 EN-BNEWS 51.2 36.4 42.5 EN-NWIRE 62.6 38.9 48.0 BNEWS 44.6 32.3 37.5 NWIRE 59.7 42.1 49.4 NPAPER 64.3 43.6 52.0 Table 6: Average gold number of clusters per document vs. the mean absolute error of our system. # Clusters MUC-6 EN-BN EN-NW Gold 15.4 22.3 37.2 Mean Error 4.7 3.0 4.8 # Clusters BNEWS NWIRE NPAPER Gold 20.4 39.2 55.2 Mean Error 2.5 5.6 6.6 other hand, substantially outperformed the corresponding rule-based ones. Table 2 compares our system to Haghighi &amp; Klein (2007) on the ACE-2004 training set in MUC scores. Again, our system outperformed theirs by a large margin. The B3 scores of MLN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE. (Haghighi &amp; Klein (2007) did not report B3.) Due to license restrictions, we could not compare directly to Culotta et al. (2007), who reported overall B3-F1 of 79.3 on the formal test set. Tables 3 and 4 compare our system to two recent supervised systems, Ng (2005) and Denis &amp; Baldridge (2007). Our approach significantly outp</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Haghighi, A. &amp; Klein, D. 2007. Unsupervised coreference resolution in a nonparametric Bayesian model. In Proc. ACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kautz</author>
<author>B Selman</author>
<author>Y Jiang</author>
</authors>
<title>A general stochastic approach to solving problems with hard and soft constraints. In The Satisfiability Problem: Theory and Applications.</title>
<date>1997</date>
<publisher>AMS.</publisher>
<contexts>
<context position="22578" citStr="Kautz et al., 1997" startWordPosition="3673" endWordPosition="3676">l and added 1, then used the inverse as the preconditioner. We also adjusted A more conservatively than Lowd &amp; Domingos (2007). Notice that when the objects form independent subsets (in our cases, mentions in each document), we can process them in parallel and then gather sufficient statistics for learning. We developed an efficient parallelized implementation of our unsupervised learning algorithm using the message-passing interface (MPI). Learning in MUC-6 took only one hour, and in ACE-2004 two and a half. To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al., 2007), SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m, c!)). Such predicates can be viewed as multi-valued predicates (e.g., Head(m) with value ranging over all c’s) and are prevalent in NLP applications. We adapted SampleSAT to flip two or more atoms in each step so that the unique-value constraints are automatically satisfied. By default, MC-SAT treats each ground clause as a separate factor while determining </context>
<context position="24414" citStr="Kautz et al., 1997" startWordPosition="3986" endWordPosition="3989">to treat each set of mutually exclusive and exhaustive rules as a single factor. E.g., for the above m, MC-SAT now samples u uniformly a L(y) = EZ|y[ni] − EY,Z[ni] awi 655 from (0, 0), and requires that in the next state 0&apos; be no less than u. Equivalently, the new cluster and head for m should satisfy wm,c, + wm,c,,t, &gt; log(u). We extended SampleSAT so that when it considers flipping any variable involved in such constraints (e.g., c or t above), it ensures that their new values still satisfy these constraints. The final clustering is found using the MaxWalkSAT weighted satisfiability solver (Kautz et al., 1997), with the appropriate extensions. We first ran a MaxWalkSAT pass with only finite-weight formulas, then ran another pass with all formulas. We found that this significantly improved the quality of the results that MaxWalkSAT returned. 6 Experiments 6.1 System We implemented our method as an extension to the Alchemy system (Kok et al., 2007). Since our learning uses sampling, all results are the average of five runs using different random seeds. Our optimization problem is not convex, so initialization is important. The core of our model (head mixture) tends to cluster non-pronouns with the sa</context>
</contexts>
<marker>Kautz, Selman, Jiang, 1997</marker>
<rawString>Kautz, H.; Selman, B.; and Jiang, Y. 1997. A general stochastic approach to solving problems with hard and soft constraints. In The Satisfiability Problem: Theory and Applications. AMS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ACL-03.</booktitle>
<contexts>
<context position="26825" citStr="Klein &amp; Manning, 2003" startWordPosition="4378" endWordPosition="4381">h differing from the previous one in a single aspect. We emphasize that our approach is unsupervised, and thus the data only contains raw text plus true mention boundaries. MLN-1 In this experiment, the base MLN was used, and the head was chosen crudely as the rightmost token in a mention. Our system was run on each test document separately, using a minimum of training data (the document itself). MLN-30 Our system was trained on all 30 test documents together. This tests how much can be gained by pooling information. MLN-H The heads were determined using the head rules in the Stanford parser (Klein &amp; Manning, 2003), plus simple heuristics to handle suffixes such as “Corp.” and “Inc.” MLN-HA The apposition rule was added. MLN-HAN The predicate-nominal rule was added. This is our full model. We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule. Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the A</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. &amp; Manning, C. 2003. Accurate unlexicalized parsing. In Proc. ACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kok</author>
<author>P Singla</author>
<author>M Richardson</author>
<author>P Domingos</author>
<author>M Sumner</author>
<author>H Poon</author>
<author>D Lowd</author>
</authors>
<title>The Alchemy system for statistical relational AI.</title>
<date>2007</date>
<note>http://alchemy.cs.washington.edu/.</note>
<contexts>
<context position="13270" citStr="Kok et al., 2007" startWordPosition="2094" endWordPosition="2097">caled conjugate gradient algorithm (PSCG) to address this problem. This estimates the optimal step size in each step as −dT g dT Hd + AdT d. where g is the gradient, d the conjugate update direction, and A a parameter that is automatically tuned to trade off second-order information with gradient descent. H is the Hessian matrix, with the (i, j)th entry being ∂2 ∂w∂w� L(y) = EY [ni] · EY [nj] − EY [ni · nj] = −CovY [ni,nj]. The Hessian can be approximated with the same samples used for the gradient. Its negative inverse diagonal is used as the preconditioner.1 The open-source Alchemy package (Kok et al., 2007) provides implementations of existing algorithms for Markov logic. In Section 5, we develop the first general-purpose unsupervised learning algorithm for Markov logic by extending the existing algorithms to handle hidden predicates.2 1Lowd &amp; Domingos showed that α can be computed more efficiently, without explicitly approximating or storing the Hessian. Readers are referred to their paper for details. 2Alchemy includes a discriminative EM algorithm, but it assumes that only a few values are missing, and cannot handle completely hidden predicates. Kok &amp; Domingos (2007) applied Markov logic to r</context>
<context position="22691" citStr="Kok et al., 2007" startWordPosition="3692" endWordPosition="3695">ngos (2007). Notice that when the objects form independent subsets (in our cases, mentions in each document), we can process them in parallel and then gather sufficient statistics for learning. We developed an efficient parallelized implementation of our unsupervised learning algorithm using the message-passing interface (MPI). Learning in MUC-6 took only one hour, and in ACE-2004 two and a half. To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al., 2007), SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m, c!)). Such predicates can be viewed as multi-valued predicates (e.g., Head(m) with value ranging over all c’s) and are prevalent in NLP applications. We adapted SampleSAT to flip two or more atoms in each step so that the unique-value constraints are automatically satisfied. By default, MC-SAT treats each ground clause as a separate factor while determining the slice. This can be very inefficient for highly correlated clauses. For example, given a non-pronoun mention m</context>
<context position="24757" citStr="Kok et al., 2007" startWordPosition="4041" endWordPosition="4044">pleSAT so that when it considers flipping any variable involved in such constraints (e.g., c or t above), it ensures that their new values still satisfy these constraints. The final clustering is found using the MaxWalkSAT weighted satisfiability solver (Kautz et al., 1997), with the appropriate extensions. We first ran a MaxWalkSAT pass with only finite-weight formulas, then ran another pass with all formulas. We found that this significantly improved the quality of the results that MaxWalkSAT returned. 6 Experiments 6.1 System We implemented our method as an extension to the Alchemy system (Kok et al., 2007). Since our learning uses sampling, all results are the average of five runs using different random seeds. Our optimization problem is not convex, so initialization is important. The core of our model (head mixture) tends to cluster non-pronouns with the same head. Therefore, we initialized by setting all weights to zero, and running the same learning algorithm on the base MLN, while assuming that in the ground truth, nonpronouns are clustered by their heads. (Effectively, the corresponding InClust atoms are assigned to appropriate values and are included in Y rather than Z during learning.) W</context>
</contexts>
<marker>Kok, Singla, Richardson, Domingos, Sumner, Poon, Lowd, 2007</marker>
<rawString>Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. &amp; Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lowd</author>
<author>D Domingos</author>
</authors>
<title>Efficient weight learning for Markov logic networks.</title>
<date>2007</date>
<booktitle>In Proc. PKDD-07.</booktitle>
<contexts>
<context position="12629" citStr="Lowd &amp; Domingos (2007)" startWordPosition="1980" endWordPosition="1983"> values are fixed and always conditioned on. The optimization problem is convex and a global optimum can be found using gradient 652 descent, with the gradient being ∂ L(y) = ni(y) − �y, P(Y = y&apos;)ni(y&apos;) ∂w = ni(y) − EY[ni]. where ni is the number of true groundings of clause i. The expected count can be approximated as �N 1 EY [ni] Pz� N k=1 where yk are samples generated by MC-SAT. To combat overfitting, a Gaussian prior is imposed on all weights. In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses. Lowd &amp; Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem. This estimates the optimal step size in each step as −dT g dT Hd + AdT d. where g is the gradient, d the conjugate update direction, and A a parameter that is automatically tuned to trade off second-order information with gradient descent. H is the Hessian matrix, with the (i, j)th entry being ∂2 ∂w∂w� L(y) = EY [ni] · EY [nj] − EY [ni · nj] = −CovY [ni,nj]. The Hessian can be approximated with the same samples used for the gradient. Its negative inverse diagonal is used as the preconditioner.1 The open-</context>
<context position="22085" citStr="Lowd &amp; Domingos (2007)" startWordPosition="3595" endWordPosition="3598"> true groundings of the ith clause. We extended PSCG for unsupervised learning. The gradient is the difference of two expectations, each of which can be approximated using samples generated by MC-SAT. The (i, j)th entry of the Hessian is now a2 awiawj L(y) = CovZ|y[ni, nj] − CovY,Z[ni, nj] and the step size can be computed accordingly. Since our problem is no longer convex, the negative diagonal Hessian may contain zero or negative entries, so we first took the absolute values of the diagonal and added 1, then used the inverse as the preconditioner. We also adjusted A more conservatively than Lowd &amp; Domingos (2007). Notice that when the objects form independent subsets (in our cases, mentions in each document), we can process them in parallel and then gather sufficient statistics for learning. We developed an efficient parallelized implementation of our unsupervised learning algorithm using the message-passing interface (MPI). Learning in MUC-6 took only one hour, and in ACE-2004 two and a half. To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al.,</context>
</contexts>
<marker>Lowd, Domingos, 2007</marker>
<rawString>Lowd, D. &amp; Domingos, D. 2007. Efficient weight learning for Markov logic networks. In Proc. PKDD-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proc. ACL-04.</booktitle>
<contexts>
<context position="31043" citStr="Luo et al. (2004)" startWordPosition="5097" endWordPosition="5100">t 6% higher than McCallum &amp; Wellner (2005). To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1). (The other systems did not report B3.) Interestingly, the rule-based MLN (RULE) sufficed to outperform Haghighi &amp; Klein (2007), and by using better heads and the apposition and predicate-nominal rules (RULE-HAN), it outperformed McCallum &amp; Wellner (2005), the supervised system. The MLNs with learning (MLN-30 and MLN-HAN), on the 7As pointed out by Haghighi &amp; Klein (2007), Luo et al. (2004) obtained a very high accuracy on MUC-6, but their system used gold NER features and is not directly comparable. 657 Table 5: Our coreference results in precision, recall, and F1 for pairwise resolution. Pairwise Prec. Rec. F1 MUC-6 63.0 57.0 59.9 EN-BNEWS 51.2 36.4 42.5 EN-NWIRE 62.6 38.9 48.0 BNEWS 44.6 32.3 37.5 NWIRE 59.7 42.1 49.4 NPAPER 64.3 43.6 52.0 Table 6: Average gold number of clusters per document vs. the mean absolute error of our system. # Clusters MUC-6 EN-BN EN-NW Gold 15.4 22.3 37.2 Mean Error 4.7 3.0 4.8 # Clusters BNEWS NWIRE NPAPER Gold 20.4 39.2 55.2 Mean Error 2.5 5.6 6.</context>
<context position="32510" citStr="Luo et al. (2004)" startWordPosition="5352" endWordPosition="5355">LN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE. (Haghighi &amp; Klein (2007) did not report B3.) Due to license restrictions, we could not compare directly to Culotta et al. (2007), who reported overall B3-F1 of 79.3 on the formal test set. Tables 3 and 4 compare our system to two recent supervised systems, Ng (2005) and Denis &amp; Baldridge (2007). Our approach significantly outperformed Ng (2005). It tied with Denis &amp; Baldridge (2007) on NWIRE, and was somewhat less accurate on BNEWS and NPAPER. Luo et al. (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. B3 suffers less from this problem but is not perfect. Thus we also report pairwise resolution scores (Table 5), the gold number of clusters, and our mean absolute error in the number of clusters (Table 6). Systems that simply merge all mentions will have exceedingly low pairwise precision (far below 50%), and very large errors in the number of clusters. Our system has fairly good pairwise precisions and small mean error in the number of clusters, which verifies that our results are sound. 6.4 Error </context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Luo, X.; Ittycheriah, A.; Jing, H.; Kambhatla, N. and Roukos, S. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In Proc. ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2005</date>
<booktitle>In Proc. NIPS-04.</booktitle>
<contexts>
<context position="1486" citStr="McCallum &amp; Wellner (2005)" startWordPosition="213" endWordPosition="217">nd ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models. 1 Introduction The goal of coreference resolution is to identify mentions (typically noun phrases) that refer to the same entities. This is a key subtask in many NLP applications, including information extraction, question answering, machine translation, and others. Supervised learning approaches treat the problem as one of classification: for each pair of mentions, predict whether they corefer or not (e.g., McCallum &amp; Wellner (2005)). While successful, these approaches require labeled training data, consisting of mention pairs and the correct decisions for them. This limits their applicability. Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text. However, unsupervised coreference resolution is much more difficult. Haghighi and Klein’s (2007) model, the most sophisticated to date, still lags supervised ones by a substantial margin. Extending it appears difficult, due to the limitations of its Dirichlet process-based representation. The lack of label information in unsupervi</context>
<context position="4292" citStr="McCallum &amp; Wellner (2005)" startWordPosition="622" endWordPosition="625">rithms we used. Finally, we present our experiments and results. 2 Related Work Most existing supervised learning approaches for coreference resolution are suboptimal since they resolve each mention pair independently, only imposing transitivity in postprocessing (Ng, 2005). Moreover, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be helpful (Poon &amp; Domingos, 2007). Using graph partitioning, McCallum &amp; Wellner (2005) incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions. Denis &amp; Baldridge (2007) determined anaphoricity and pairwise classification jointly using integer programming, but they did not incorporate transitivity or other relations. While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date. Prior to our work, the best performance in un</context>
<context position="8191" citStr="McCallum &amp; Wellner (2005)" startWordPosition="1218" endWordPosition="1221"> into the integrated resolution process. As a result, our model is inherently joint among mentions and subtasks. It shares several features with Haghighi &amp; Klein’s model, but removes or refines features where we believe it is appropriate to. Most importantly, our model leverages apposition and predicate nominals, which Haghighi &amp; Klein did not use. We show that this can be done very easily in our framework, and yet results in very substantial accuracy gains. 651 It is worth noticing that Markov logic is also well suited for joint inference in supervised systems (e.g., transitivity, which took McCallum &amp; Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon &amp; Domingos, 2008)). 3 Markov Logic In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor &amp; Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for joint inference is Markov logic, a probabilistic extension of first-order logic (Richardson &amp; Domingos, 2006). A Markov logic network (MLN) is a set of weight</context>
<context position="30251" citStr="McCallum &amp; Wellner, 2005" startWordPosition="4968" endWordPosition="4971"> that with this improvement our system already outperforms a state-of-theBNEWS Prec. Rec. F1 Ng 67.9 62.2 64.9 D&amp;B 78.0 62.1 69.2 MLN-HAN 68.3 66.6 67.4 NWIRE Prec. Rec. F1 Ng 60.3 50.1 54.7 D&amp;B 75.8 60.8 67.5 MLN-HAN 67.7 67.3 67.4 NPAPER Prec. Rec. F1 Ng 71.4 67.4 69.3 D&amp;B 77.6 68.0 72.5 MLN-HAN 69.2 71.7 70.4 Table 4: Comparison of coreference results in B3 scores on the ACE-2 datasets. BNEWS Prec. Rec. F1 Ng 77.1 57.0 65.6 MLN-HAN 70.3 65.3 67.7 NWIRE Prec. Rec. F1 Ng 75.4 59.3 66.4 MLN-HAN 74.7 68.8 71.6 NPAPER Prec. Rec. F1 Ng 75.4 59.3 66.4 MLN-HAN 70.0 66.5 68.2 art supervised system (McCallum &amp; Wellner, 2005). Leveraging apposition resulted in another large improvement, and predicate nominals also helped. Our full model scores about 9% higher than Haghighi &amp; Klein (2007), and about 6% higher than McCallum &amp; Wellner (2005). To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1). (The other systems did not report B3.) Interestingly, the rule-based MLN (RULE) sufficed to outperform Haghighi &amp; Klein (2007), and by using better heads and the apposition and predicate-nominal rules</context>
</contexts>
<marker>McCallum, Wellner, 2005</marker>
<rawString>McCallum, A. &amp; Wellner, B. 2005. Conditional models of identity uncertainty with application to noun coreference. In Proc. NIPS-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Machine Learning for Coreference Resolution: From Local Classification to Global Ranking. In</title>
<date>2005</date>
<booktitle>Proc. ACL-05.</booktitle>
<contexts>
<context position="3941" citStr="Ng, 2005" startWordPosition="568" endWordPosition="569">erence on Empirical Methods in Natural Language Processing, pages 650–659, Honolulu, October 2008.c�2008 Association for Computational Linguistics We begin by reviewing the necessary background on Markov logic. We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used. Finally, we present our experiments and results. 2 Related Work Most existing supervised learning approaches for coreference resolution are suboptimal since they resolve each mention pair independently, only imposing transitivity in postprocessing (Ng, 2005). Moreover, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be helpful (Poon &amp; Domingos, 2007). Using graph partitioning, McCallum &amp; Wellner (2005) incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions. Denis &amp; Baldridge (200</context>
<context position="27459" citStr="Ng (2005)" startWordPosition="4481" endWordPosition="4482">to handle suffixes such as “Corp.” and “Inc.” MLN-HA The apposition rule was added. MLN-HAN The predicate-nominal rule was added. This is our full model. We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule. Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set. In this paper, we used the ACE-2004 training corpus and ACE Phrase-2 (ACE2) to enable direct comparisons with Haghighi &amp; Klein (2007), Ng (2005), and Denis and Baldridge (2007). Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007). The English version of the ACE-2004 training corpus contains two sections, BNEWS and NWIRE, with 220 and 128 documents, respectively. ACE-2 contains a 656 Table 1: Compa</context>
<context position="32311" citStr="Ng (2005)" startWordPosition="5321" endWordPosition="5322">rule-based ones. Table 2 compares our system to Haghighi &amp; Klein (2007) on the ACE-2004 training set in MUC scores. Again, our system outperformed theirs by a large margin. The B3 scores of MLN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE. (Haghighi &amp; Klein (2007) did not report B3.) Due to license restrictions, we could not compare directly to Culotta et al. (2007), who reported overall B3-F1 of 79.3 on the formal test set. Tables 3 and 4 compare our system to two recent supervised systems, Ng (2005) and Denis &amp; Baldridge (2007). Our approach significantly outperformed Ng (2005). It tied with Denis &amp; Baldridge (2007) on NWIRE, and was somewhat less accurate on BNEWS and NPAPER. Luo et al. (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. B3 suffers less from this problem but is not perfect. Thus we also report pairwise resolution scores (Table 5), the gold number of clusters, and our mean absolute error in the number of clusters (Table 6). Systems that simply merge all mentions will have exceedingly low pairwise precision (far below 50%)</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>Ng, V. 2005. Machine Learning for Coreference Resolution: From Local Classification to Global Ranking. In Proc. ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Sound and efficient inference with probabilistic and deterministic dependencies.</title>
<date>2006</date>
<booktitle>In Proc. AAAI-06.</booktitle>
<contexts>
<context position="6036" citStr="Poon &amp; Domingos, 2006" startWordPosition="876" endWordPosition="879">out, but Haghighi and Klein forgo this when they introduce salience. Their model thus requires Gibbs sampling over both assignments and parameters, which can be very expensive. Haghighi and Klein circumvent this by making approximations that potentially hurt accuracy. At the same time, the Dirichlet process prior favors skewed cluster sizes and a number of clusters that grows logarithmically with the number of data points, neither of which seems generally appropriate for coreference resolution. Further, deterministic or strong non-deterministic dependencies cause Gibbs sampling to break down (Poon &amp; Domingos, 2006), making it difficult to leverage many linguistic regularities. For example, apposition (as in “Bill Gates, the chairman of Microsoft”) suggests coreference, and thus the two mentions it relates should always be placed in the same cluster. However, Gibbs sampling can only move one mention at a time from one cluster to another, and this is unlikely to happen, because it would require breaking the apposition rule. Blocked sampling can alleviate this problem by sampling multiple mentions together, but it requires that the block size be predetermined to a small fixed number. When we incorporate ap</context>
<context position="9428" citStr="Poon &amp; Domingos, 2006" startWordPosition="1432" endWordPosition="1435">clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the firstorder clause that originated it. The probability of a state x in such a network is given by P(x) = (1/Z) exp (Ez wzfz(x)), where Z is a normalization constant, wz is the weight of the ith clause, fz = 1 if the ith clause is true, and fz = 0 otherwise. Markov logic makes it possible to compactly specify probability distributions over complex relational domains. Efficient inference can be performed using MC-SAT (Poon &amp; Domingos, 2006). MC-SAT is a “slice sampling” Markov chain Monte Carlo algorithm. Slice sampling introduces auxiliary variables u that decouple the original ones x, and alternately samples u conditioned on x and viceversa. To sample from the slice (the set of states x consistent with the current u), MC-SAT calls SampleSAT (Wei et al., 2004), which uses a combination of satisfiability testing and simulated annealing. The advantage of using a satisfiability solver (WalkSAT) is that it efficiently finds isolated modes in the distribution, and as a result the Markov chain mixes very rapidly. The slice sampling s</context>
</contexts>
<marker>Poon, Domingos, 2006</marker>
<rawString>Poon, H. &amp; Domingos, P. 2006. Sound and efficient inference with probabilistic and deterministic dependencies. In Proc. AAAI-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Joint inference in information extraction.</title>
<date>2007</date>
<booktitle>In Proc. AAAI-07.</booktitle>
<contexts>
<context position="4239" citStr="Poon &amp; Domingos, 2007" startWordPosition="614" endWordPosition="617">ce resolution, and the learning and inference algorithms we used. Finally, we present our experiments and results. 2 Related Work Most existing supervised learning approaches for coreference resolution are suboptimal since they resolve each mention pair independently, only imposing transitivity in postprocessing (Ng, 2005). Moreover, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be helpful (Poon &amp; Domingos, 2007). Using graph partitioning, McCallum &amp; Wellner (2005) incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions. Denis &amp; Baldridge (2007) determined anaphoricity and pairwise classification jointly using integer programming, but they did not incorporate transitivity or other relations. While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to thi</context>
</contexts>
<marker>Poon, Domingos, 2007</marker>
<rawString>Poon, H. &amp; Domingos, P. 2007. Joint inference in information extraction. In Proc. AAAI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>A general method for reducing the complexity of relational inference and its application to MCMC. In</title>
<date>2008</date>
<booktitle>Proc. AAAI-08.</booktitle>
<contexts>
<context position="8318" citStr="Poon &amp; Domingos, 2008" startWordPosition="1239" endWordPosition="1242"> features with Haghighi &amp; Klein’s model, but removes or refines features where we believe it is appropriate to. Most importantly, our model leverages apposition and predicate nominals, which Haghighi &amp; Klein did not use. We show that this can be done very easily in our framework, and yet results in very substantial accuracy gains. 651 It is worth noticing that Markov logic is also well suited for joint inference in supervised systems (e.g., transitivity, which took McCallum &amp; Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon &amp; Domingos, 2008)). 3 Markov Logic In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor &amp; Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for joint inference is Markov logic, a probabilistic extension of first-order logic (Richardson &amp; Domingos, 2006). A Markov logic network (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one fea</context>
<context position="11411" citStr="Poon &amp; Domingos, 2008" startWordPosition="1772" endWordPosition="1775">for all k (the slice). Equivalently, for each k, with probability 1 — e−lk the next state must satisfy ck. In general, we can factorize the probability distribution in any way that facilitates inference, sample the uk’s, and make sure that the next state is drawn uniformly from solutions that satisfy 0k &gt; uk for all factors. MC-SAT, like most existing relational inference algorithms, grounds all predicates and clauses, thus requiring memory and time exponential in the predicate and clause arities. We developed a general method for producing a “lazy” version of relational inference algorithms (Poon &amp; Domingos, 2008), which carries exactly the same inference steps as the original algorithm, but only maintains a small subset of “active” predicates/clauses, grounding more as needed. We showed that Lazy-MC-SAT, the lazy version of MC-SAT, reduced memory and time by orders of magnitude in several domains. We use Lazy-MC-SAT in this paper. Supervised learning for Markov logic maximizes the conditional log-likelihood L(x, y) = log P(Y = yJX = x), where Y represents the non-evidence predicates, X the evidence predicates, and x, y their values in the training data. For simplicity, from now on we omit X, whose val</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Poon, H. &amp; Domingos, P. 2008. A general method for reducing the complexity of relational inference and its application to MCMC. In Proc. AAAI-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<journal>Machine Learning</journal>
<pages>62--107</pages>
<contexts>
<context position="2666" citStr="Richardson &amp; Domingos, 2006" startWordPosition="378" endWordPosition="381">tion. The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the “easy” decisions to help make related “hard” ones. Relations that have been exploited in supervised coreference resolution include transitivity (McCallum &amp; Wellner, 2005) and anaphoricity (Denis &amp; Baldridge, 2007). However, there is little work to date on joint inference for unsupervised resolution. We address this problem using Markov logic, a powerful and flexible language that combines probabilistic graphical models and first-order logic (Richardson &amp; Domingos, 2006). Markov logic allows us to easily build models involving relations among mentions, like apposition and predicate nominals. By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution. We test our approach on standard MUC and ACE datasets. Our basic model, trained on a minimum of data, suffices to outperform Haghighi and Klein’s (2007) one. Our full model, using apposition and other relations for joint inference, is often as accurate as the</context>
<context position="7383" citStr="Richardson &amp; Domingos, 2006" startWordPosition="1086" endWordPosition="1089">e also want to leverage predicate nominals (i.e., the subject and the predicating noun of a copular verb are likely coreferent). Then a sentence like “He is Bill Gates, the chairman of Microsoft” requires a block of four mentions: “He”, “Bill Gates”, “the chairman of Microsoft”, and “Bill Gates, the chairman of Microsoft”. Similar difficulties occur with other inference methods. Thus, extending Haghighi and Klein’s model to include richer linguistic features is a challenging problem. Our approach is instead based on Markov logic, a powerful representation for joint inference with uncertainty (Richardson &amp; Domingos, 2006). Like Haghighi and Klein’s, our model is cluster-based rather than pairwise, and implicitly imposes transitivity. We do not predetermine anaphoricity of a mention, but rather fuse it into the integrated resolution process. As a result, our model is inherently joint among mentions and subtasks. It shares several features with Haghighi &amp; Klein’s model, but removes or refines features where we believe it is appropriate to. Most importantly, our model leverages apposition and predicate nominals, which Haghighi &amp; Klein did not use. We show that this can be done very easily in our framework, and ye</context>
<context position="8742" citStr="Richardson &amp; Domingos, 2006" startWordPosition="1304" endWordPosition="1307"> supervised systems (e.g., transitivity, which took McCallum &amp; Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon &amp; Domingos, 2008)). 3 Markov Logic In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor &amp; Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the most powerful representations for joint inference is Markov logic, a probabilistic extension of first-order logic (Richardson &amp; Domingos, 2006). A Markov logic network (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the firstorder clause that originated it. The probability of a state x in such a network is given by P(x) = (1/Z) exp (Ez wzfz(x)), where Z is a normalization constant, wz is the weight of the ith clause, fz = 1 if the ith clause is true, and fz = 0 otherwise. Markov logic makes it possible to compactly specify probability distributions over complex relatio</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Richardson, M. &amp; Domingos, P. 2006. Markov logic networks. Machine Learning 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proc. MUC-6.</booktitle>
<contexts>
<context position="25749" citStr="Vilain et al., 1995" startWordPosition="4196" endWordPosition="4199">ase MLN, while assuming that in the ground truth, nonpronouns are clustered by their heads. (Effectively, the corresponding InClust atoms are assigned to appropriate values and are included in Y rather than Z during learning.) We used 30 iterations of PSCG for learning. (In preliminary experiments, additional iterations had little effect on coreference accuracy.) We generated 100 samples using MC-SAT for each expectation approximation.6 6.2 Methodology We conducted experiments on MUC-6, ACE-2004, and ACE Phrase-2 (ACE-2). We evaluated our systems using two commonly-used scoring programs: MUC (Vilain et al., 1995) and B3 (Amit &amp; Baldwin, 1998). To gain more insight, we also report pairwise resolution scores and mean absolute error in the number of clusters. 6Each sample actually contains a large number of groundings, so 100 samples yield sufficiently accurate statistics for learning. The MUC-6 dataset consists of 30 documents for testing and 221 for training. To evaluate the contribution of the major components in our model, we conducted five experiments, each differing from the previous one in a single aspect. We emphasize that our approach is unsupervised, and thus the data only contains raw text plu</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Vilain, M.; Burger, J.; Aberdeen, J.; Connolly, D. &amp; Hirschman, L. 1995. A model-theoretic coreference scoring scheme. In Proc. MUC-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wei</author>
<author>J Erenrich</author>
<author>B Selman</author>
</authors>
<title>Towards efficient sampling: Exploiting random walk strategies.</title>
<date>2004</date>
<booktitle>In Proc. AAAI-04.</booktitle>
<contexts>
<context position="9755" citStr="Wei et al., 2004" startWordPosition="1488" endWordPosition="1491">ion constant, wz is the weight of the ith clause, fz = 1 if the ith clause is true, and fz = 0 otherwise. Markov logic makes it possible to compactly specify probability distributions over complex relational domains. Efficient inference can be performed using MC-SAT (Poon &amp; Domingos, 2006). MC-SAT is a “slice sampling” Markov chain Monte Carlo algorithm. Slice sampling introduces auxiliary variables u that decouple the original ones x, and alternately samples u conditioned on x and viceversa. To sample from the slice (the set of states x consistent with the current u), MC-SAT calls SampleSAT (Wei et al., 2004), which uses a combination of satisfiability testing and simulated annealing. The advantage of using a satisfiability solver (WalkSAT) is that it efficiently finds isolated modes in the distribution, and as a result the Markov chain mixes very rapidly. The slice sampling scheme ensures that detailed balance is (approximately) preserved. MC-SAT is orders of magnitude faster than previous MCMC algorithms like Gibbs sampling, making efficient sampling possible on a scale that was previAlgorithm 1 MC-SAT(clauses, weights, num samples) x(c) Satisfy(hard clauses) for i 1 to num samples do M 0 for al</context>
</contexts>
<marker>Wei, Erenrich, Selman, 2004</marker>
<rawString>Wei, W.; Erenrich, J. and Selman, B. 2004. Towards efficient sampling: Exploiting random walk strategies. In Proc. AAAI-04.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>