<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000192">
<title confidence="0.9976275">
Answer Sentence Retrieval by Matching Dependency Paths
Acquired from Question/Answer Sentence Pairs
</title>
<author confidence="0.914428">
Michael Kaisser
</author>
<affiliation confidence="0.756602">
AGT Group (R&amp;D) GmbH
</affiliation>
<address confidence="0.847164">
J¨agerstr. 41, 10117 Berlin, Germany
</address>
<email confidence="0.997345">
mkaisser@agtgermany.com
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999540826086957">
In Information Retrieval (IR) in general
and Question Answering (QA) in particu-
lar, queries and relevant textual content of-
ten significantly differ in their properties
and are therefore difficult to relate with tra-
ditional IR methods, e.g. key-word match-
ing. In this paper we describe an algorithm
that addresses this problem, but rather than
looking at it on a term matching/term re-
formulation level, we focus on the syntac-
tic differences between questions and rele-
vant text passages. To this end we propose
a novel algorithm that analyzes dependency
structures of queries and known relevant
text passages and acquires transformational
patterns that can be used to retrieve rele-
vant textual content. We evaluate our algo-
rithm in a QA setting, and show that it out-
performs a baseline that uses only depen-
dency information contained in the ques-
tions by 300% and that it also improves per-
formance of a state of the art QA system
significantly.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938142857143">
It is a well known problem in Information Re-
trieval (IR) and Question Answering (QA) that
queries and relevant textual content often signif-
icantly differ in their properties, and are therefore
difficult to match with traditional IR methods. A
common example is a user entering words to de-
scribe their information need that do not match
the words used in the most relevant indexed doc-
uments. This work addresses this problem, but
shifts focus from words to syntactic structures of
questions and relevant pieces of text. To this end,
we present a novel algorithm that analyses the de-
pendency structures of known valid answer sen-
tence and from these acquires patterns that can be
used to more precisely retrieve relevant text pas-
sages from the underlying document collection.
To achieve this, the position of key phrases in the
answer sentence relative to the answer itself is an-
alyzed and linked to a certain syntactic question
type. Unlike most previous work that uses depen-
dency paths for QA (see Section 2), our approach
does not require a candidate sentence to be similar
to the question in any respect. We learn valid de-
pendency structures from the known answer sen-
tences alone, and therefore are able to link a much
wider spectrum of answer sentences to the ques-
tion.
The work in this paper is presented and eval-
uated in a classical factoid Question Answering
(QA) setting. The main reason for this is that
in QA suitable training and test data is available
in the public domain, e.g. via the Text REtrieval
Conference (TREC), see for example (Voorhees,
1999). The methods described in this paper how-
ever can also be applied to other IR scenarios, e.g.
web search. The necessary condition for our ap-
proach to work is that the user query is somewhat
grammatically well formed; this kind of queries
are commonly referred to as Natural Language
Queries or NLQs.
Table 1 provides evidence that users indeed
search the web with NLQs. The data is based on
two query sets sampled from three months of user
logs from a popular search engine, using two dif-
ferent sampling techniques. The “head” set sam-
ples queries taking query frequency into account,
so that more common queries have a proportion-
ally higher chance of being selected. The “tail”
query set samples only queries that have been is-
</bodyText>
<page confidence="0.989058">
88
</page>
<note confidence="0.9938375">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 88–98,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<table confidence="0.969356785714286">
Set Head Tail
Query # 15,665 12,500
how 1.33% 2.42%
what 0.77% 1.89%
define 0.34% 0.18%
is/are 0.25% 0.42%
where 0.18% 0.45%
do/does 0.14% 0.30%
can 0.14% 0.25%
why 0.13% 0.30%
who 0.12% 0.38%
when 0.09% 0.21%
which 0.03% 0.08%
Total 3.55% 6.86%
</table>
<tableCaption confidence="0.971589">
Table 1: Percentages of Natural Language queries in
head and tail search engine query logs. See text for
details.
</tableCaption>
<bodyText confidence="0.999886181818182">
sued less that 500 times during a three months pe-
riod and it disregards query frequency. As a result,
rare and frequent queries have the same chance of
being selected. Doubles are excluded from both
sets. Table 1 lists the percentage of queries in
the query sets that start with the specified word.
In most contexts this indicates that the query is a
question, which in turn means that we are dealing
with an NLQ. Of course there are many NLQs that
start with words other than the ones listed, so we
can expect their real percentage to be even higher.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999906457142857">
In IR the problem that queries and relevant tex-
tual content often do not exhibit the same terms is
commonly encountered. Latent Semantic Index-
ing (Deerwester et al., 1900) was an early, highly
influential approach to solve this problem. More
recently, a significant amount of research is ded-
icated to query alteration approaches. (Cui et al.,
2002), for example, assume that if queries con-
taining one term often result in the selection of
documents containing another term, then a strong
relationship between the two terms exist. In their
approach, query terms and document terms are
linked via sessions in which users click on doc-
uments that are presented as results for the query.
(Riezler and Liu, 2010) apply a Statistical Ma-
chine Translation model to parallel data consist-
ing of user queries and snippets from clicked web
documents and in such a way extract contextual
expansion terms from the query rewrites.
We see our work as addressing the same fun-
damental problem, but shifting focus from query
term/document term mismatch to mismatches ob-
served between the grammatical structure of Nat-
ural Language Queries and relevant text pieces. In
order to achieve this we analyze the queries’ and
the relevant contents’ syntactic structure by using
dependency paths.
Especially in QA there is a strong tradition
of using dependency structures: (Lin and Pan-
tel, 2001) present an unsupervised algorithm to
automatically discover inference rules (essentially
paraphrases) from text. These inference rules are
based on dependency paths, each of which con-
nects two nouns. Their paths have the following
form:
</bodyText>
<equation confidence="0.718637">
N:subj:V+—find—*V:obj:N—*solution—*N:to:N
</equation>
<bodyText confidence="0.999505235294117">
This path represents the relation “X finds a solu-
tion to Y” and can be mapped to another path rep-
resenting e.g. “X solves Y.” As such the approach
is suitable to detect paraphrases that describe the
relation between two entities in documents. How-
ever, the paper does not describe how the mined
paraphrases can be linked to questions, and which
paraphrase is suitable to answer which question
type.
(Attardi et al., 2001) describes a QA system
that, after a set of candidate answer sentences
have been identified, matches their dependency
relations against the question. Questions and
answer sentences are parsed with MiniPar (Lin,
1998) and the dependency output is analyzed in
order to determine whether relations present in a
question also appear in a candidate sentence. For
the question “Who killed John F. Kennedy”, for
example an answer sentence is expected to con-
tain the answer as subject of the verb “kill”, to
which “John F. Kennedy” should be in object re-
lation.
(Cui et al., 2005) describe a fuzzy depen-
dency relation matching approach to passage re-
trieval in QA. Here, the authors present a statis-
tical technique to measure the degree of overlap
between dependency relations in candidate sen-
tences with their corresponding relations in the
question. Question/answer passage pairs from
TREC-8 and TREC-9 evaluations are used as
training data. As in some of the papers mentioned
earlier, a statistical translation model is used, but
this time to learn relatedness between paths. (Cui
et al., 2004) apply the same idea to answer ex-
</bodyText>
<page confidence="0.999587">
89
</page>
<bodyText confidence="0.994199111111111">
traction. In each sentences returned by the IR
module, all named entities of the expected answer
types are treated as answer candidates. For ques-
tions with an unknown answer type, all NPs in
the candidate sentence are considered. Then those
paths in the answer sentence that are connected
to an answer candidate are compared against the
corresponding paths in the question, in a similar
fashion as in (Cui et al., 2005). The candidate
whose paths show the highest matching score is
selected. (Shen and Klakow, 2006) also describe
a method that is primarily based on similarity
scores between dependency relation pairs. How-
ever, their algorithm computes the similarity of
paths between key phrases, not between words.
Furthermore, it takes relations in a path not as in-
dependent from each other, but acknowledges that
they form a sequence, by comparing two paths
with the help of an adaptation of the Dynamic
Time Warping algorithm (Rabiner et al., 1991).
(Molla, 2006) presents an approach for the ac-
quisition of question answering rules by apply-
ing graph manipulation methods. Questions are
represented as dependency graphs, which are ex-
tended with information from answer sentences.
These combined graphs can then be used to iden-
tify answers. Finally, in (Wang et al., 2007), a
quasi-synchronous grammar (Smith and Eisner,
2006) is used to model relations between ques-
tions and answer sentences.
In this paper we describe an algorithm that
learns possible syntactic answer sentence formu-
lations for syntactic question classes from a set of
example question/answer sentence pairs. Unlike
the related work described above, it acknowledges
that a) a valid answer sentence’s syntax might
be very different for the question’s syntax and b)
several valid answer sentence structures, which
might be completely independent from each other,
can exist for one and the same question.
To illustrate this consider the question “When
was Alaska purchased?” The following four sen-
tences all answer the given question, but only the
first sentence is a straightforward reformulation of
the question:
</bodyText>
<listItem confidence="0.993265625">
1. The United States purchased Alaska in 1867
from Russia.
2. Alaska was bought from Russia in 1867.
3. In 1867, the Russian Empire sold the Alaska
territory to the USA.
4. The acquisition of Alaska by the United
States of America from Russia in 1867 is
known as “Seward’s Folly”.
</listItem>
<bodyText confidence="0.999787545454546">
The remaining three sentences introduce vari-
ous forms of syntactic and semantic transforma-
tions. In order to capture a wide range of possible
ways on how answer sentences can be formulated,
in our model a candidate sentence is not evalu-
ated according to its similarity with the question.
Instead, its similarity to known answer sentences
(which were presented to the system during train-
ing) is evaluated. This allows to us to capture a
much wider range of syntactic and semantic trans-
formations.
</bodyText>
<sectionHeader confidence="0.912117" genericHeader="method">
3 Overview of the Algorithm
</sectionHeader>
<bodyText confidence="0.9997431">
Our algorithm uses input data containing pairs of
the following:
NLQs/Questions NLQs that describe the users’
information need. For the experiments car-
ried out in this paper we use questions from
the TREC QA track 2002-2006.
Relevant textual content This is a piece of text
that is relevant to the user query in that it
contains the information the user is search-
ing for. In this paper, we use sentences ex-
tracted from the AQUAINT corpus (Graff,
2002) that contain the answer to the given
TREC question.
In total, the data available to us for our experi-
ments consists of 8,830 question/answer sentence
pairs. This data is publicly available, see (Kaisser
and Lowe, 2008). The algorithm described in this
paper has three main steps:
Phrase alignment Key phrases from the ques-
tion are paired with phrases from the answer
sentences.
Pattern creation The dependency structures of
queries and answer sentences are analyzed
and patterns are extracted.
Pattern evaluation The patterns discovered in
the last step are evaluated and a confidence
score is assigned to each.
The acquired patterns can then be used during
retrieval, where a question is matched against the
antecedents describing the syntax of the question.
</bodyText>
<page confidence="0.971696">
90
</page>
<figure confidence="0.931318777777778">
Input: (a) Query: “When was Alaska purchased?”
(b) Answer sentence: “The acquisition of Alaska happened in 1867.”
Step 1: Question is segmented into key phrases and stop words:
When[1]+was[2]+NP[3]+VERB[4]
Step 2: Key question phrases are aligned with key answer sentence phrases:
[3]Alaska → Alaska
[4]purchased → acquisition
ANSWER → 1867
Step 3: A pre-computed parse tree of the answer sentence is loaded:
</figure>
<listItem confidence="0.933462714285714">
1: The (the, DT, 2) [det]
2: acquisition (acquisition, NN, 5) [nsubj]
3: of (of, IN, 2) [prep]
4: Alaska (Alaska, IN, 2) [pobj]
5: happened (happen, VBD, null) [ROOT]
6: in (in, IN, 5) [prep]
7: 1867 (1867, CD, 6) [pobj]
</listItem>
<figure confidence="0.567496666666667">
Step 4: Dependency paths from key question phrases to the answer are computed:
Alaska⇒1867: ⇑pobj⇑prep⇑nsubj⇓prep⇓pobj
acquisition⇒1867: ⇑nsubj⇓prep⇓pobj
</figure>
<figureCaption confidence="0.885258">
Step 5: The resulting pattern is stored:
Query: When[1]+was[2]+NP[3]+VERB[4]
Path 3: ⇑pobj⇑prep⇑nsubj⇓prep⇓pobj
Path 4: ⇑nsubj⇓prep⇓pobj
Figure 1: The pattern creation algorithm exemplified in five key steps for the query “When was Alaska pur-
chased?” and the answer sentence “The acquisition of Alaska happened in 1867.”
</figureCaption>
<bodyText confidence="0.9997386">
Note that one question can potentially match sev-
eral patterns. The consequents contain descrip-
tions of grammatical structures of potential an-
swer sentences that can be used to identify and
evaluate candidate sentences.
</bodyText>
<sectionHeader confidence="0.991229" genericHeader="method">
4 Phrase Alignment
</sectionHeader>
<bodyText confidence="0.997029477272727">
The goal of this processing step is to align phrases
from the question with corresponding phrases
from the answer sentences in the training data.
Consider the following example:
Query: “When was the Alaska territory pur-
chased?”
Answer sentence: “The acquisition of what
would become the territory of Alaska took place
in 1867.”
The mapping that has to be achieved is:
Query Answer Sentence
phrase phrase
“Alaska territory” “territory of Alaska”
“purchased” “acquisition”
ANSWER “1867”
In our approach, this is a two step process.
First we align on a word level, then the output
of the word alignment process is used to iden-
tify and align phrases. Word Alignment is im-
portant in many fields of NLP, e.g. Machine
Translation (MT) where words in parallel, bilin-
gual corpora need to be aligned, see (Och and
Ney, 2003) for a comparison of various statisti-
cal alignment models. In our case however we
are dealing with a monolingual alignment prob-
lem which enables us to exploit clues not available
for bilingual alignment: First of all, we can expect
many query words to be present in the answer sen-
tence, either with the exact same surface appear-
ance or in some morphological variant. Secondly,
there are tools available that tell us how semanti-
cally related two words are, most notably Word-
Net (Miller et al., 1993). For these reasons we im-
plemented a bespoke alignment strategy, tailored
towards our problem description.
This method is described in detail in (Kaisser,
2009). The processing steps described in the
next sections build on its output. For reasons of
brevity, we skip a detailed explanations in this pa-
per and focus only on its key part: the alignment
of words with very different surface structures.
For more details we would like to point the reader
to the aforementioned work.
In the above example, the alignment of “pur-
</bodyText>
<page confidence="0.993748">
91
</page>
<bodyText confidence="0.999991857142857">
chased” and “acquisition” is the most problem-
atic, because the surface structures of the two
words clearly are very different. For such cases
we experimented with a number of alignment
strategies based on WordNet. These approaches
are similar in that each picks one word that has to
be aligned from the question at a time and com-
pares it to all of the non-stop words in the answer
sentence. Each of the answer sentence words is
assigned a value between zero and one express-
ing its relatedness to the question word. The
highest scoring word, if above a certain thresh-
old, is selected as the closest semantic match.
Most of these approaches make use of Word-
Net::Similarity, a Perl software package that mea-
sures semantic similarity (or relatedness) between
a pair of word senses by returning a numeric value
that represents the degree to which they are sim-
ilar or related (Pedersen et al., 2004). Addition-
ally, we developed a custom-built method that as-
sumes that two words are semantically related if
any kind of pointer exists between any occurrence
of the words root form in WordNet. For details of
these experiments, please refer to (Kaisser, 2009).
In our experiments the custom-built method per-
formed best, and was therefore used for the exper-
iments described in this paper. The main reasons
for this are:
</bodyText>
<listItem confidence="0.788721">
1. Many of the measures in the Word-
Net::Similarity package take only hyponym/
</listItem>
<bodyText confidence="0.70213475">
hypernym relations into account. This makes
aligning word of different parts of speech
difficult or even impossible. However, such
alignments are important for our needs.
</bodyText>
<listItem confidence="0.97262575">
2. Many of the measures return results, even if
only a weak semantic relationship exists. For
our purposes however, it is beneficial to only
take strong semantic relations into account.
</listItem>
<sectionHeader confidence="0.990246" genericHeader="method">
5 Pattern Creation
</sectionHeader>
<bodyText confidence="0.999833517857143">
Figure 1 details our algorithm in its five key steps.
In step 1 and 2 key phrases from the question are
aligned to the corresponding phrases in the an-
swer sentence, see Section 4 of this paper. Step
3 is concerned with retrieving the parse tree for
the answer sentence. In our implementation all
answer sentences in the training set have for per-
formance reasons been parsed beforehand with
the Stanford Parser (Klein and Manning, 2003b;
Klein and Manning, 2003a), so at this point they
are simply loaded from file. Step 4 is the key step
in our algorithm. From the previous steps, we
know where the key constituents from the ques-
tion as well as the answer are located in the an-
swer sentence. This enables us to compute the
dependency paths in the answer sentences’ parse
tree that connect the answer with the key con-
stituents. In our example the answer is “1867”
and the key constituents are “acquisition” and
“Alaska.” Knowing the syntactic relationships
(captured by their dependency paths) between the
answer and the key phrases enables us to capture
one syntactic possibility of how answer sentences
to queries of the form When+was+NP+VERB can
be formulated.
As can be seen in Step 5 a flat syntactic ques-
tion representation is stored, together with num-
bers assigned to each constituent. The num-
bers for those constituents for which alignments
in the answer sentence were sought and found
are listed together with the resulting dependency
paths. Path 3 for example denotes the path from
constituent 3 (the NP “Alaska”) to the answer. If
no alignment could be found for a constituent,
null is stored instead of a path. Should two or
more alternative constituents be identified for one
question constituent, additional patterns are cre-
ated, so that each contains one of the possibilities.
The described procedure is repeated for all ques-
tion/answer sentence pairs in the training set and
for each, one or more patterns are created.
It is worth to note that many TREC ques-
tions are fairly short and grammatically sim-
ple. In our training data we for exam-
ple find 102 questions matching the pattern
When[1]+was[2]+NP[3]+VERB[4], which
together list 382 answer sentences, and thus 382
potentially different answer sentence structures
from which patterns can be gained. As a result,
the amount of training examples we have avail-
able, is sufficient to achieve the performance de-
scribed in Section 7. The algorithm described in
this paper can of course also be used for more
complicated NLQs, although in such a scenario a
significantly larger amount of training data would
have to be used.
</bodyText>
<sectionHeader confidence="0.998105" genericHeader="method">
6 Pattern Evaluation
</sectionHeader>
<bodyText confidence="0.996202">
For each created pattern, at least one match-
ing example must exists: the sentence that was
</bodyText>
<page confidence="0.980521">
92
</page>
<bodyText confidence="0.9996023125">
used to create it in the first place. However, we
do not know how precise each pattern is. To
this end, an additional processing step between
pattern creation and application is needed: pat-
tern evaluation. Similar approaches to ours have
been described in the relevant literature, many
of them concerned with bootstrapping, starting
with (Ravichandran and Hovy, 2002). The gen-
eral purpose of this step is to use the available
data about questions and their correct answers to
evaluate how often each created pattern returns a
correct or an incorrect result. This data is stored
with each pattern and the result of the equation,
often called pattern precision, can be used during
retrieval stage. Pattern precision in our case is de-
fined as:
</bodyText>
<equation confidence="0.9927395">
p= #correct + 1 (1)
#correct + #incorrect + 2
</equation>
<bodyText confidence="0.999883772727273">
We use Lucene to retrieve the top 100 para-
graphs from the AQUAINT corpus by issuing a
query that consists of the query’s key words and
all non-stop words in the answer. Then, all pat-
terns are loaded whose antecedent matches the
query that is currently being processed. After that,
constituents from all sentences in the retrieved
100 paragraphs are aligned to the query’s con-
stituents in the same way as for the sentences dur-
ing pattern creation, see Section 5. Now, the paths
specified in these patterns are searched for in the
paragraphs’ parse trees. If they are all found,
it is checked whether they all point to the same
node and whether this node’s surface structure is
in some morphological form present in the answer
strings associated with the question in our train-
ing data. If this is the case a variable in the pat-
tern named correct is increased by 1, otherwise
the variable incorrect is increased by 1. After the
evaluation process is finished the final version of
the pattern given as an example in Figure 1 now
is:
</bodyText>
<equation confidence="0.6150984">
Query: When[1]+was[2]+NP[3]+VERB[4]
Path 3: ⇑pobj⇑prep⇑nsubj⇓prep⇓pobj
Path 4: ⇑nsubj⇓prep⇓pobj
Correct: 15
Incorrect: 4
</equation>
<bodyText confidence="0.95816375">
The variables correct and incorrect are used
during retrieval, where the score of an answer can-
didate ac is the sum of all scores of all matching
patterns p:
</bodyText>
<equation confidence="0.995348">
n
score(ac) = score(pi) (2)
i=1
</equation>
<bodyText confidence="0.678659">
where
</bodyText>
<equation confidence="0.991717">
� correcti+1
correcti+incorrecti+2 if match
score(pi) = (3)
0 no match
</equation>
<bodyText confidence="0.999825">
The highest scoring candidate is selected.
We would like to explicitly call out one prop-
erty of our algorithm: It effectively returns two
entities: a) a sentence that constitutes a valid
response to the query, b) the head node of a
phrase in that sentence that constitutes the answer.
Therefore the algorithm can be used for sentence
retrieval or for answer retrieval. It depends on
the application which of the two behaviors is de-
sired. In the next section, we evaluate its answer
retrieval performance.
</bodyText>
<sectionHeader confidence="0.996166" genericHeader="evaluation">
7 Experiments &amp; Results
</sectionHeader>
<bodyText confidence="0.99365">
This section provides an evaluation of the algo-
rithm described in this paper. The key questions
we seek to answer are the following:
</bodyText>
<listItem confidence="0.9774188">
1. How does our method perform when com-
pared to a baseline that extracts dependency
paths from the question?
2. How much does the described algorithm im-
prove performance of a state-of-the-art QA
system?
3. What is the effect of training data size on per-
formance? Can we expect that more training
data would further improve the algorithm’s
performance?
</listItem>
<subsectionHeader confidence="0.994859">
7.1 Evaluation Setup
</subsectionHeader>
<bodyText confidence="0.999701692307692">
We use all factoid questions in TREC’s QA test
sets from 2002 to 2006 for evaluation for which
a known answer exists in the AQUAINT corpus.
Additionally, the data in (Lin and Katz, 2005) is
used. In this paper the authors attempt to identify
a much more complete set of relevant documents
for a subset of TREC 2002 questions than TREC
itself. We adopt a cross validation approach for
our evaluation. Table 4 shows how the data is split
into five folds.
In order to evaluate the algorithm’s patterns we
need a set of sentences to which they can be ap-
plied. In a traditional QA system architecture,
</bodyText>
<page confidence="0.996852">
93
</page>
<table confidence="0.99234675">
Test Number of Correct Answer Sentences Mean Med
set
= 0 &lt;= 1 &lt;= 3 &lt;= 5 &lt;= 10 &lt;= 25 &lt;= 50 &gt;= 75 &gt;= 90 &gt;= 100
2002 0.203 0.396 0.580 0.671 0.809 0.935 0.984 0.0 0.0 0.0 6.86 2.0
2003 0.249 0.429 0.627 0.732 0.828 0.955 0.997 0.003 0.003 0.0 5.67 2.0
2004 0.221 0.368 0.539 0.637 0.799 0.936 0.985 0.0 0.0 0.0 6.51 3.0
2005 0.245 0.404 0.574 0.665 0.777 0.912 0.987 0.0 0.0 0.0 7.56 2.0
2006 0.241 0.389 0.568 0.665 0.807 0.920 0.966 0.006 0.0 0.0 8.04 3.0
</table>
<tableCaption confidence="0.974957">
Table 2: Fraction of sentences that contain correct answers in Evaluation Set 1 (approximation).
</tableCaption>
<table confidence="0.998162625">
Test Number of Correct Answer Sentences Mean Med
set
= 0 &lt;= 1 &lt;= 3 &lt;= 5 &lt;= 10 &lt;= 25 &lt;= 50 &gt;= 75 &gt;= 90 &gt;= 100
2002 0.0 0.074 0.158 0.235 0.342 0.561 0.748 0.172 0.116 0.060 33.46 21.0
2003 0.0 0.099 0.203 0.254 0.356 0.573 0.720 0.161 0.090 0.031 32.88 19.0
2004 0.0 0.073 0.137 0.211 0.328 0.598 0.779 0.142 0.069 0.034 30.82 20.0
2005 0.0 0.163 0.238 0.279 0.410 0.589 0.759 0.141 0.097 0.069 30.87 17.0
2006 0.0 0.125 0.207 0.281 0.415 0.596 0.727 0.173 0.122 0.088 32.93 17.5
</table>
<tableCaption confidence="0.996893">
Table 3: Fraction of sentences that contain correct answers in Evaluation Set 2 (approximation).
</tableCaption>
<table confidence="0.998868857142857">
Fold Training Data Test Data
sets used # set #
1 T03, T04, T05, T06 4565 T02 1159
2 T02, T04, T05, T06, Lin02 6174 T03 1352
3 T02, T03, T05, T06, Lin02 6700 T04 826
4 T02, T03, T04, T06, Lin02 6298 T05 1228
5 T02, T03, T04, T05, Lin02 6367 T06 1159
</table>
<tableCaption confidence="0.9972322">
Table 4: Splits into training and tests sets of the data
used for evaluation. T02 stands for TREC 2002 data
etc. Lin02 is based on (Lin and Katz, 2005). The #
rows show how many question/answer sentence pairs
are used for training and for testing.
</tableCaption>
<bodyText confidence="0.999853083333333">
see e.g. (Prager, 2006; Voorhees, 2003), the docu-
ment or passage retrieval step performs this func-
tion. This step is crucial to a QA system’s per-
formance, because it is impossible to locate an-
swers in the subsequent answer extraction step if
the passages returned during passage retrieval do
not contain the answer in the first place. This also
holds true in our case: the patterns cannot be ex-
pected to identify a correct answer if none of the
sentences used as input contains the correct an-
swer. We therefore use two different evaluation
sets to evaluate our algorithm:
</bodyText>
<listItem confidence="0.929146272727273">
1. The first set contains for each question all
sentences in the top 100 paragraphs returned
by Lucene when using simple queries made
up from the question’s key words. It cannot
be guaranteed that answers to every question
are present in this test set.
2. For the second set, the query additionally list
all known correct answers to the question as
parts of one OR operator. This increases the
chance that the evaluation set actually con-
tains valid answer sentences significantly.
</listItem>
<bodyText confidence="0.999991214285714">
In order to provide a quantitative characteriza-
tion of the two evaluation sets we estimated the
number of correct answer sentences they contain.
For each paragraph it was determined whether it
contained one of the known answer strings and
at least of one of the question key words. Ta-
bles 2 and 3 show for each evaluation set how
many answers on average it contains per ques-
tion. The column “= 0” for example shows the
fraction of questions for which no valid answer
sentence is contained in the evaluation set, while
column “&gt;= 90” gives the fraction of questions
with 90 or more valid answer sentences. The last
two columns show mean and median values.
</bodyText>
<subsectionHeader confidence="0.999883">
7.2 Comparison with Baseline
</subsectionHeader>
<bodyText confidence="0.999960947368421">
As pointed out in Section 2 there is a strong tra-
dition of using dependency paths in QA. Many
relevant papers describe algorithms that analyze
a question’s grammatical structure and expect
to find a similar structure in valid answer sen-
tences, e.g. (Attardi et al., 2001), (Cui et al., 2005)
or (Bouma et al., 2005) to name just a few. As
already pointed out, a major contribution of our
work is that we do not assume this similarity. In
our approach valid answer sentences are allowed
to have grammatical structures that are very dif-
ferent from the question and also very different
from each other. Thus it is natural to compare our
approach against a baseline that compares can-
didate sentences not against patterns that were
gained from question/answer sentence pairs, but
from questions alone. In order to create these pat-
terns, we use a small trick: During the Pattern
Creation step, see Section 5 and Figure 1, we re-
</bodyText>
<page confidence="0.998521">
94
</page>
<bodyText confidence="0.9873715">
place the answer sentences in the input file with
the questions, and assume that the question word
indicates the position where the answer should be
located.
</bodyText>
<table confidence="0.9963023125">
Test Q Qs with Min one Overall Accuracy Acc. if
set number patterns correct correct overall pattern
2002 429 321 77 37 0.086 0.115
2003 354 237 39 26 0.073 0.120
2004 204 142 25 15 0.074 0.073
2005 319 214 38 18 0.056 0.084
2006 352 208 34 16 0.045 0.077
Sum 1658 1122 213 112 0.068 0.100
Test Q Qs with &gt; 1 Overall Accuracy Acc. if
set number patterns correct correct overall pattern
2002 429 321 147 50 0.117 0.156
2003 354 237 76 22 0.062 0.093
2004 204 142 74 26 0.127 0.183
2005 319 214 97 46 0.144 0.215
2006 352 208 85 31 0.088 0.149
Sum 1658 1122 452 176 0.106 0.156
</table>
<tableCaption confidence="0.998106">
Table 5: Performance based on evaluation set 1.
</tableCaption>
<table confidence="0.994248625">
Test Q Qs with &gt; 1 Overall Accuracy Acc. if
set number patterns correct correct overall pattern
2002 429 321 239 133 0.310 0.414
2003 354 237 149 88 0.248 0.371
2004 204 142 119 65 0.319 0.458
2005 319 214 161 92 0.288 0.429
2006 352 208 139 84 0.238 0.403
Sum 1658 1122 807 462 0.278 0.411
</table>
<tableCaption confidence="0.999917">
Table 6: Performance based on evaluation set 2.
</tableCaption>
<bodyText confidence="0.99719425">
Tables 5 and 6 show how our algorithm per-
forms on evaluation sets 1 and 2, respectively. Ta-
bles 7 and 8 show how the baseline performs on
evaluation sets 1 and 2, respectively. The tables’
columns list the year of the TREC test set used,
the number of questions in the set (we only use
questions for which we know that there is an an-
swer in the corpus), the number of questions for
which one or more patterns exist, how often at
least one pattern returned the correct answer, how
often we get an overall correct result by taking
all patterns and their confidence values into ac-
count, accuracy@1 of the overall system, and ac-
curacy@1 computed only for those questions for
which we have at least one pattern available (for
all other questions the system returns no result.)
As can be seen, on evaluation set 1 our method
outperforms the baseline by 300%, on evaluation
set 2 by 311%, taking accuracy if a pattern exists
as a basis.
</bodyText>
<table confidence="0.993024375">
Test Q Qs with Min one Overall Accuracy Acc. if
set number patterns correct correct overall pattern
2002 429 321 43 14 0.033 0.044
2003 354 237 28 10 0.028 0.042
2004 204 142 19 6 0.029 0.042
2005 319 214 21 7 0.022 0.033
2006 352 208 20 7 0.020 0.034
Sum 1658 1122 131 44 0.027 0.039
</table>
<tableCaption confidence="0.9744085">
Table 7: Baseline performance based on evaluation set
1.
</tableCaption>
<bodyText confidence="0.9984105">
Many of the papers cited earlier that use an ap-
proach similar to our baseline approach of course
report much better results than Tables 7 and 8.
This however is not too surprising as the approach
</bodyText>
<tableCaption confidence="0.980917">
Table 8: Baseline performance based on evaluation set
2.
</tableCaption>
<bodyText confidence="0.998672142857143">
described in this paper and the baseline approach
do not make use of many techniques commonly
used to increase performance of a QA system, e.g.
TF-IDF fallback strategies, fuzzy matching, man-
ual reformulation patterns etc. It was a deliberate
decision from our side not to use any of these ap-
proaches. After all, this would result in an ex-
perimental setup where the performance of our
answer extraction strategy could not have been
observed in isolation. The QA system used as a
baseline in the next section makes use of many of
these techniques and we will see that our method,
as described here, is suitable to increase its per-
formance significantly.
</bodyText>
<subsectionHeader confidence="0.98398">
7.3 Impact on an existing QA System
</subsectionHeader>
<bodyText confidence="0.999928925925926">
Tables 9 and 10 show how our algorithm in-
creases performance of our QuALiM system, see
e.g. (Kaisser et al., 2006). Section 6 in this pa-
per describes via formulas 2 and 3 how answer
candidates are ranked. This ranking is combined
with the existing QA system’s candidate ranking
by simply using it as an additional feature that
boosts candidates proportionally to their confi-
dence score. The difference between both tables
is that the first uses all 1658 questions in our test
sets for the evaluation, whereas the second con-
siders only those 1122 questions for which our
system was able to learn a pattern. Thus for Table
10 questions which the system had no chance of
answering due to limited training data are omitted.
As can be seen, accuracy@1 increases by 4.9% on
the complete test set and by 11.5% on the partial
set.
Note that the QA system used as a baseline is
at an advantage in at least two respects: a) It has
important web-based components and as such has
access to a much larger body of textual informa-
tion. b) The algorithm described in this paper is an
answer extraction approach only. For paragraph
retrieval we use the same approach as for evalu-
ation set 1, see Section 7.1. However, in more
than 20% of the cases, this method returns not
</bodyText>
<page confidence="0.996131">
95
</page>
<bodyText confidence="0.9987576">
a single paragraph that contains both the answer
and at least one question keyword. In such cases,
the simple paragraph retrieval makes it close to
impossible for our algorithm to return the correct
answer.
</bodyText>
<table confidence="0.987465">
Test Set QuALiM QASP combined increase
2002 0.503 0.117 0.524 4.2%
2003 0.367 0.062 0.390 6.2%
2004 0.426 0.127 0.451 5.7%
2005 0.373 0.144 0.389 4.2%
2006 0.341 0.088 0.358 5.0%
02-06 0.405 0.106 0.425 4.9%
</table>
<tableCaption confidence="0.996502">
Table 9: Top-1 accuracy of the QuALiM system on its
own and when combined with the algorithm described
in this paper. All increases are statistically significant
using a sign test (p &lt; 0.05).
</tableCaption>
<figureCaption confidence="0.9193245">
Figure 2: Effect of the amount of training data on sys-
tem performance
</figureCaption>
<sectionHeader confidence="0.970665" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<table confidence="0.998127">
Test Set QuALiM QASP combined increase
2002 0.530 0.156 0.595 12.3%
2003 0.380 0.093 0.430 13.3%
2004 0.465 0.183 0.514 10.6%
2005 0.388 0.214 0.421 8.4%
2006 0.385 0.149 0.428 11.3%
02-06 0.436 0.157 0.486 11.5%
</table>
<tableCaption confidence="0.959702">
Table 10: Top-1 accuracy of the QuALiM system on
</tableCaption>
<bodyText confidence="0.8755888">
its own and when combined with the algorithm de-
scribed in this paper, when only considering questions
for which a pattern could be acquired from the training
data. All increases are statistically significant using a
sign test (p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.995174">
7.4 Effect of Training Data Size
</subsectionHeader>
<bodyText confidence="0.999990961538462">
We now assess the effect of training data size on
performance. Tables 5 and 6 presented earlier
show that an average of 32.2% of the questions
have no matching patterns. This is because the
data used for training contained no examples for a
significant subset of question classes. It can be ex-
pected that, if more training data would be avail-
able, this percentage would decrease and perfor-
mance would increase. In order to test this as-
sumption, we repeated the evaluation procedure
detailed in this section several times, initially us-
ing data from only one TREC test set for train-
ing and then gradually adding more sets until all
available training data had been used. The results
for evaluation set 2 are presented in Figure 2. As
can be seen, every time more data is added, per-
formance increases. This strongly suggests that
the point of diminishing returns, when adding ad-
ditional training data no longer improves perfor-
mance is not yet reached.
In this paper we present an algorithm that acquires
syntactic information about how relevant textual
content to a question can be formulated from a
collection of paired questions and answer sen-
tences. Other than previous work employing de-
pendency paths for QA, our approach does not as-
sume that a valid answer sentence is similar to the
question and it allows many potentially very dif-
ferent syntactic answer sentence structures. The
algorithm is evaluated using TREC data, and it
is shown that it outperforms an algorithm that
merely uses the syntactic information contained
in the question itself by 300%. It is also shown
that the algorithm improves the performance of a
state-of-the-art QA system significantly.
As always, there are many ways how we could
imagine our algorithm to be improved. Combin-
ing it with fuzzy matching techniques as in (Cui et
al., 2004) or (Cui et al., 2005) is an obvious direc-
tion for future work. We are also aware that in or-
der to apply our algorithm on a larger scale and in
a real world setting with real users, we would need
a much larger set of training data. These could
be acquired semi-manually, for example by using
crowd-sourcing techniques. We are also thinking
about fully automated approaches, or about us-
ing indirect human evidence, e.g. user clicks in
search engine logs. Typically users only see the
title and a short abstract of the document when
clicking on a result, so it is possible to imagine a
scenario where a subset of these abstracts, paired
with user queries, could serve as training data.
</bodyText>
<page confidence="0.989198">
96
</page>
<sectionHeader confidence="0.995865" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999638633928571">
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, and Alessandro Tommasi.
2001. PIQASso: Pisa Question Answering System.
In Proceedings of the 2001 Edition of the Text RE-
trieval Conference (TREC-01).
Gosse Bouma, Jori Mur, and Gertjan van Noord. 2005.
Reasoning over Dependency Relations for QA. In
Proceedings of the IJCAI workshop on Knowledge
and Reasoning for Answering Questions (KRAQ-
05).
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In 11th International World Wide Web
Conference (WWW-02).
Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, and
Min-Yen Kan. 2004. National University of Sin-
gapore at the TREC-13 Question Answering Main
Task. In Proceedings of the 2004 Edition of the Text
REtrieval Conference (TREC-04).
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. 2005. Question Answering Pas-
sage Retrieval Using Dependency Relations. In
Proceedings of the 28th ACM-SIGIR International
Conference on Research and Development in Infor-
mation Retrieval (SIGIR-05).
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1900.
Indexing by Latent Semantic Analysis. Journal of
the American society for information science, 41(6).
David Graff. 2002. The AQUAINT Corpus of English
News Text.
Michael Kaisser and John Lowe. 2008. Creating a
Research Collection of Question Answer Sentence
Pairs with Amazon’s Mechanical Turk. In Proceed-
ings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC-08).
Michael Kaisser, Silke Scheible, and Bonnie Webber.
2006. Experiments at the University of Edinburgh
for the TREC 2006 QA track. In Proceedings of
the 2006 Edition of the Text REtrieval Conference
(TREC-06).
Michael Kaisser. 2009. Acquiring Syntactic and
Semantic Transformations in Question Answering.
Ph.D. thesis, University of Edinburgh.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics (ACL-03).
Dan Klein and Christopher D. Manning. 2003b. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Informa-
tion Processing Systems 15.
Jimmy Lin and Boris Katz. 2005. Building a Reusable
Test Collection for Question Answering. Journal of
the American Society for Information Science and
Technology (JASIST).
Dekang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question-Answering. Natural
Language Engineering, 7(4):343–360.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Pars-
ing Systems.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993.
Introduction to WordNet: An On-Line Lexical
Database. Journal of Lexicography, 3(4):235–244.
Diego Molla. 2006. Learning of Graph-based
Question Answering Rules. In Proceedings of
HLT/NAACL 2006 Workshop on Graph Algorithms
for Natural Language Processing.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–52.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. 2004. WordNet::Similarity - Measur-
ing the Relatedness of Concepts. In Proceedings
of the Nineteenth National Conference on Artificial
Intelligence (AAAI-04).
John Prager. 2006. Open-Domain Question-
Answering. Foundations and Trends in Information
Retrieval, 1(2).
L. R. Rabiner, A. E. Rosenberg, and S. E. Levin-
son. 1991. Considerations in Dynamic Time Warp-
ing Algorithms for Discrete Word Recognition. In
Proceedings of IEEE Transactions on Acoustics,
Speech and Signal Processing.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning Surface Text Patterns for a Question An-
swering System. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-02).
Stefan Riezler and Yi Liu. 2010. Query Rewriting
using Monolingual Statistical Machine Translation.
Computational Linguistics, 36(3).
Dan Shen and Dietrich Klakow. 2006. Exploring Cor-
relation of Dependency Relation Paths for Answer
Extraction. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL (COLING/ACL-06).
David A. Smith and Jason Eisner. 2006. Quasisyn-
chronous grammars: Alignment by Soft Projec-
tion of Syntactic Dependencies. In Proceedings of
the HLTNAACL Workshop on Statistical Machine
Translation.
Ellen M. Voorhees. 1999. Overview of the Eighth
Text REtrieval Conference (TREC-8). In Pro-
ceedings of the Eighth Text REtrieval Conference
(TREC-8).
Ellen M. Voorhees. 2003. Overview of the TREC
2003 Question Answering Track. In Proceedings of
the 2003 Edition of the Text REtrieval Conference
(TREC-03).
</reference>
<page confidence="0.993056">
97
</page>
<reference confidence="0.9987125">
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A Qua-
sisynchronous Grammar for QA. In Proceedings of
EMNLP-CoNLL 2007.
</reference>
<page confidence="0.996244">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.701476">
<title confidence="0.989334">Answer Sentence Retrieval by Matching Dependency Acquired from Question/Answer Sentence Pairs</title>
<author confidence="0.993648">Michael</author>
<affiliation confidence="0.880289">AGT Group (R&amp;D) J¨agerstr. 41, 10117 Berlin,</affiliation>
<email confidence="0.99972">mkaisser@agtgermany.com</email>
<abstract confidence="0.99709075">In Information Retrieval (IR) in general and Question Answering (QA) in particular, queries and relevant textual content often significantly differ in their properties and are therefore difficult to relate with traditional IR methods, e.g. key-word matching. In this paper we describe an algorithm that addresses this problem, but rather than looking at it on a term matching/term reformulation level, we focus on the syntactic differences between questions and relevant text passages. To this end we propose a novel algorithm that analyzes dependency structures of queries and known relevant text passages and acquires transformational patterns that can be used to retrieve relevant textual content. We evaluate our algorithm in a QA setting, and show that it outperforms a baseline that uses only dependency information contained in the questions by 300% and that it also improves performance of a state of the art QA system significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
<author>Antonio Cisternino</author>
<author>Francesco Formica</author>
<author>Maria Simi</author>
<author>Alessandro Tommasi</author>
</authors>
<title>PIQASso: Pisa Question Answering System.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Edition of the Text REtrieval Conference (TREC-01).</booktitle>
<contexts>
<context position="6657" citStr="Attardi et al., 2001" startWordPosition="1100" endWordPosition="1103">ssentially paraphrases) from text. These inference rules are based on dependency paths, each of which connects two nouns. Their paths have the following form: N:subj:V+—find—*V:obj:N—*solution—*N:to:N This path represents the relation “X finds a solution to Y” and can be mapped to another path representing e.g. “X solves Y.” As such the approach is suitable to detect paraphrases that describe the relation between two entities in documents. However, the paper does not describe how the mined paraphrases can be linked to questions, and which paraphrase is suitable to answer which question type. (Attardi et al., 2001) describes a QA system that, after a set of candidate answer sentences have been identified, matches their dependency relations against the question. Questions and answer sentences are parsed with MiniPar (Lin, 1998) and the dependency output is analyzed in order to determine whether relations present in a question also appear in a candidate sentence. For the question “Who killed John F. Kennedy”, for example an answer sentence is expected to contain the answer as subject of the verb “kill”, to which “John F. Kennedy” should be in object relation. (Cui et al., 2005) describe a fuzzy dependency</context>
<context position="27003" citStr="Attardi et al., 2001" startWordPosition="4534" endWordPosition="4537"> answers on average it contains per question. The column “= 0” for example shows the fraction of questions for which no valid answer sentence is contained in the evaluation set, while column “&gt;= 90” gives the fraction of questions with 90 or more valid answer sentences. The last two columns show mean and median values. 7.2 Comparison with Baseline As pointed out in Section 2 there is a strong tradition of using dependency paths in QA. Many relevant papers describe algorithms that analyze a question’s grammatical structure and expect to find a similar structure in valid answer sentences, e.g. (Attardi et al., 2001), (Cui et al., 2005) or (Bouma et al., 2005) to name just a few. As already pointed out, a major contribution of our work is that we do not assume this similarity. In our approach valid answer sentences are allowed to have grammatical structures that are very different from the question and also very different from each other. Thus it is natural to compare our approach against a baseline that compares candidate sentences not against patterns that were gained from question/answer sentence pairs, but from questions alone. In order to create these patterns, we use a small trick: During the Patter</context>
</contexts>
<marker>Attardi, Cisternino, Formica, Simi, Tommasi, 2001</marker>
<rawString>Giuseppe Attardi, Antonio Cisternino, Francesco Formica, Maria Simi, and Alessandro Tommasi. 2001. PIQASso: Pisa Question Answering System. In Proceedings of the 2001 Edition of the Text REtrieval Conference (TREC-01).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Jori Mur</author>
<author>Gertjan van Noord</author>
</authors>
<title>Reasoning over Dependency Relations for QA.</title>
<date>2005</date>
<booktitle>In Proceedings of the IJCAI workshop on Knowledge and Reasoning for Answering Questions (KRAQ05).</booktitle>
<marker>Bouma, Mur, van Noord, 2005</marker>
<rawString>Gosse Bouma, Jori Mur, and Gertjan van Noord. 2005. Reasoning over Dependency Relations for QA. In Proceedings of the IJCAI workshop on Knowledge and Reasoning for Answering Questions (KRAQ05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Ji-Rong Wen</author>
<author>Jian-Yun Nie</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Probabilistic query expansion using query logs.</title>
<date>2002</date>
<booktitle>In 11th International World Wide Web Conference (WWW-02).</booktitle>
<contexts>
<context position="4939" citStr="Cui et al., 2002" startWordPosition="826" endWordPosition="829"> In most contexts this indicates that the query is a question, which in turn means that we are dealing with an NLQ. Of course there are many NLQs that start with words other than the ones listed, so we can expect their real percentage to be even higher. 2 Related Work In IR the problem that queries and relevant textual content often do not exhibit the same terms is commonly encountered. Latent Semantic Indexing (Deerwester et al., 1900) was an early, highly influential approach to solve this problem. More recently, a significant amount of research is dedicated to query alteration approaches. (Cui et al., 2002), for example, assume that if queries containing one term often result in the selection of documents containing another term, then a strong relationship between the two terms exist. In their approach, query terms and document terms are linked via sessions in which users click on documents that are presented as results for the query. (Riezler and Liu, 2010) apply a Statistical Machine Translation model to parallel data consisting of user queries and snippets from clicked web documents and in such a way extract contextual expansion terms from the query rewrites. We see our work as addressing the</context>
</contexts>
<marker>Cui, Wen, Nie, Ma, 2002</marker>
<rawString>Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying Ma. 2002. Probabilistic query expansion using query logs. In 11th International World Wide Web Conference (WWW-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Keya Li</author>
<author>Renxu Sun</author>
<author>Tat-Seng Chua</author>
<author>Min-Yen Kan</author>
</authors>
<title>at the TREC-13 Question Answering Main Task.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Edition of the Text REtrieval Conference (TREC-04).</booktitle>
<institution>National University of Singapore</institution>
<contexts>
<context position="7745" citStr="Cui et al., 2004" startWordPosition="1276" endWordPosition="1279">bject of the verb “kill”, to which “John F. Kennedy” should be in object relation. (Cui et al., 2005) describe a fuzzy dependency relation matching approach to passage retrieval in QA. Here, the authors present a statistical technique to measure the degree of overlap between dependency relations in candidate sentences with their corresponding relations in the question. Question/answer passage pairs from TREC-8 and TREC-9 evaluations are used as training data. As in some of the papers mentioned earlier, a statistical translation model is used, but this time to learn relatedness between paths. (Cui et al., 2004) apply the same idea to answer ex89 traction. In each sentences returned by the IR module, all named entities of the expected answer types are treated as answer candidates. For questions with an unknown answer type, all NPs in the candidate sentence are considered. Then those paths in the answer sentence that are connected to an answer candidate are compared against the corresponding paths in the question, in a similar fashion as in (Cui et al., 2005). The candidate whose paths show the highest matching score is selected. (Shen and Klakow, 2006) also describe a method that is primarily based o</context>
<context position="35288" citStr="Cui et al., 2004" startWordPosition="6013" endWordPosition="6016">ths for QA, our approach does not assume that a valid answer sentence is similar to the question and it allows many potentially very different syntactic answer sentence structures. The algorithm is evaluated using TREC data, and it is shown that it outperforms an algorithm that merely uses the syntactic information contained in the question itself by 300%. It is also shown that the algorithm improves the performance of a state-of-the-art QA system significantly. As always, there are many ways how we could imagine our algorithm to be improved. Combining it with fuzzy matching techniques as in (Cui et al., 2004) or (Cui et al., 2005) is an obvious direction for future work. We are also aware that in order to apply our algorithm on a larger scale and in a real world setting with real users, we would need a much larger set of training data. These could be acquired semi-manually, for example by using crowd-sourcing techniques. We are also thinking about fully automated approaches, or about using indirect human evidence, e.g. user clicks in search engine logs. Typically users only see the title and a short abstract of the document when clicking on a result, so it is possible to imagine a scenario where a</context>
</contexts>
<marker>Cui, Li, Sun, Chua, Kan, 2004</marker>
<rawString>Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, and Min-Yen Kan. 2004. National University of Singapore at the TREC-13 Question Answering Main Task. In Proceedings of the 2004 Edition of the Text REtrieval Conference (TREC-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Question Answering Passage Retrieval Using Dependency Relations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th ACM-SIGIR International Conference on Research and Development in Information Retrieval (SIGIR-05).</booktitle>
<contexts>
<context position="7229" citStr="Cui et al., 2005" startWordPosition="1195" endWordPosition="1198">r which question type. (Attardi et al., 2001) describes a QA system that, after a set of candidate answer sentences have been identified, matches their dependency relations against the question. Questions and answer sentences are parsed with MiniPar (Lin, 1998) and the dependency output is analyzed in order to determine whether relations present in a question also appear in a candidate sentence. For the question “Who killed John F. Kennedy”, for example an answer sentence is expected to contain the answer as subject of the verb “kill”, to which “John F. Kennedy” should be in object relation. (Cui et al., 2005) describe a fuzzy dependency relation matching approach to passage retrieval in QA. Here, the authors present a statistical technique to measure the degree of overlap between dependency relations in candidate sentences with their corresponding relations in the question. Question/answer passage pairs from TREC-8 and TREC-9 evaluations are used as training data. As in some of the papers mentioned earlier, a statistical translation model is used, but this time to learn relatedness between paths. (Cui et al., 2004) apply the same idea to answer ex89 traction. In each sentences returned by the IR m</context>
<context position="27023" citStr="Cui et al., 2005" startWordPosition="4538" endWordPosition="4541">ontains per question. The column “= 0” for example shows the fraction of questions for which no valid answer sentence is contained in the evaluation set, while column “&gt;= 90” gives the fraction of questions with 90 or more valid answer sentences. The last two columns show mean and median values. 7.2 Comparison with Baseline As pointed out in Section 2 there is a strong tradition of using dependency paths in QA. Many relevant papers describe algorithms that analyze a question’s grammatical structure and expect to find a similar structure in valid answer sentences, e.g. (Attardi et al., 2001), (Cui et al., 2005) or (Bouma et al., 2005) to name just a few. As already pointed out, a major contribution of our work is that we do not assume this similarity. In our approach valid answer sentences are allowed to have grammatical structures that are very different from the question and also very different from each other. Thus it is natural to compare our approach against a baseline that compares candidate sentences not against patterns that were gained from question/answer sentence pairs, but from questions alone. In order to create these patterns, we use a small trick: During the Pattern Creation step, see</context>
<context position="35310" citStr="Cui et al., 2005" startWordPosition="6018" endWordPosition="6021">ch does not assume that a valid answer sentence is similar to the question and it allows many potentially very different syntactic answer sentence structures. The algorithm is evaluated using TREC data, and it is shown that it outperforms an algorithm that merely uses the syntactic information contained in the question itself by 300%. It is also shown that the algorithm improves the performance of a state-of-the-art QA system significantly. As always, there are many ways how we could imagine our algorithm to be improved. Combining it with fuzzy matching techniques as in (Cui et al., 2004) or (Cui et al., 2005) is an obvious direction for future work. We are also aware that in order to apply our algorithm on a larger scale and in a real world setting with real users, we would need a much larger set of training data. These could be acquired semi-manually, for example by using crowd-sourcing techniques. We are also thinking about fully automated approaches, or about using indirect human evidence, e.g. user clicks in search engine logs. Typically users only see the title and a short abstract of the document when clicking on a result, so it is possible to imagine a scenario where a subset of these abstr</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua. 2005. Question Answering Passage Retrieval Using Dependency Relations. In Proceedings of the 28th ACM-SIGIR International Conference on Research and Development in Information Retrieval (SIGIR-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1900</date>
<journal>Journal of the American society for information science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="4762" citStr="Deerwester et al., 1900" startWordPosition="798" endWordPosition="801">quent queries have the same chance of being selected. Doubles are excluded from both sets. Table 1 lists the percentage of queries in the query sets that start with the specified word. In most contexts this indicates that the query is a question, which in turn means that we are dealing with an NLQ. Of course there are many NLQs that start with words other than the ones listed, so we can expect their real percentage to be even higher. 2 Related Work In IR the problem that queries and relevant textual content often do not exhibit the same terms is commonly encountered. Latent Semantic Indexing (Deerwester et al., 1900) was an early, highly influential approach to solve this problem. More recently, a significant amount of research is dedicated to query alteration approaches. (Cui et al., 2002), for example, assume that if queries containing one term often result in the selection of documents containing another term, then a strong relationship between the two terms exist. In their approach, query terms and document terms are linked via sessions in which users click on documents that are presented as results for the query. (Riezler and Liu, 2010) apply a Statistical Machine Translation model to parallel data c</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1900</marker>
<rawString>Scott Deerwester, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. 1900. Indexing by Latent Semantic Analysis. Journal of the American society for information science, 41(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<date>2002</date>
<journal>The AQUAINT Corpus of English News Text.</journal>
<contexts>
<context position="11122" citStr="Graff, 2002" startWordPosition="1833" endWordPosition="1834">the system during training) is evaluated. This allows to us to capture a much wider range of syntactic and semantic transformations. 3 Overview of the Algorithm Our algorithm uses input data containing pairs of the following: NLQs/Questions NLQs that describe the users’ information need. For the experiments carried out in this paper we use questions from the TREC QA track 2002-2006. Relevant textual content This is a piece of text that is relevant to the user query in that it contains the information the user is searching for. In this paper, we use sentences extracted from the AQUAINT corpus (Graff, 2002) that contain the answer to the given TREC question. In total, the data available to us for our experiments consists of 8,830 question/answer sentence pairs. This data is publicly available, see (Kaisser and Lowe, 2008). The algorithm described in this paper has three main steps: Phrase alignment Key phrases from the question are paired with phrases from the answer sentences. Pattern creation The dependency structures of queries and answer sentences are analyzed and patterns are extracted. Pattern evaluation The patterns discovered in the last step are evaluated and a confidence score is assig</context>
</contexts>
<marker>Graff, 2002</marker>
<rawString>David Graff. 2002. The AQUAINT Corpus of English News Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>John Lowe</author>
</authors>
<title>Creating a Research Collection of Question Answer Sentence Pairs with Amazon’s Mechanical Turk.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC-08).</booktitle>
<contexts>
<context position="11341" citStr="Kaisser and Lowe, 2008" startWordPosition="1867" endWordPosition="1870">of the following: NLQs/Questions NLQs that describe the users’ information need. For the experiments carried out in this paper we use questions from the TREC QA track 2002-2006. Relevant textual content This is a piece of text that is relevant to the user query in that it contains the information the user is searching for. In this paper, we use sentences extracted from the AQUAINT corpus (Graff, 2002) that contain the answer to the given TREC question. In total, the data available to us for our experiments consists of 8,830 question/answer sentence pairs. This data is publicly available, see (Kaisser and Lowe, 2008). The algorithm described in this paper has three main steps: Phrase alignment Key phrases from the question are paired with phrases from the answer sentences. Pattern creation The dependency structures of queries and answer sentences are analyzed and patterns are extracted. Pattern evaluation The patterns discovered in the last step are evaluated and a confidence score is assigned to each. The acquired patterns can then be used during retrieval, where a question is matched against the antecedents describing the syntax of the question. 90 Input: (a) Query: “When was Alaska purchased?” (b) Answ</context>
</contexts>
<marker>Kaisser, Lowe, 2008</marker>
<rawString>Michael Kaisser and John Lowe. 2008. Creating a Research Collection of Question Answer Sentence Pairs with Amazon’s Mechanical Turk. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>Silke Scheible</author>
<author>Bonnie Webber</author>
</authors>
<title>Experiments at the University of Edinburgh for the TREC</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Edition of the Text REtrieval Conference (TREC-06).</booktitle>
<contexts>
<context position="31104" citStr="Kaisser et al., 2006" startWordPosition="5292" endWordPosition="5295">hing, manual reformulation patterns etc. It was a deliberate decision from our side not to use any of these approaches. After all, this would result in an experimental setup where the performance of our answer extraction strategy could not have been observed in isolation. The QA system used as a baseline in the next section makes use of many of these techniques and we will see that our method, as described here, is suitable to increase its performance significantly. 7.3 Impact on an existing QA System Tables 9 and 10 show how our algorithm increases performance of our QuALiM system, see e.g. (Kaisser et al., 2006). Section 6 in this paper describes via formulas 2 and 3 how answer candidates are ranked. This ranking is combined with the existing QA system’s candidate ranking by simply using it as an additional feature that boosts candidates proportionally to their confidence score. The difference between both tables is that the first uses all 1658 questions in our test sets for the evaluation, whereas the second considers only those 1122 questions for which our system was able to learn a pattern. Thus for Table 10 questions which the system had no chance of answering due to limited training data are omi</context>
</contexts>
<marker>Kaisser, Scheible, Webber, 2006</marker>
<rawString>Michael Kaisser, Silke Scheible, and Bonnie Webber. 2006. Experiments at the University of Edinburgh for the TREC 2006 QA track. In Proceedings of the 2006 Edition of the Text REtrieval Conference (TREC-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
</authors>
<title>Acquiring Syntactic and Semantic Transformations in Question Answering.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="14701" citStr="Kaisser, 2009" startWordPosition="2402" endWordPosition="2403">gnment models. In our case however we are dealing with a monolingual alignment problem which enables us to exploit clues not available for bilingual alignment: First of all, we can expect many query words to be present in the answer sentence, either with the exact same surface appearance or in some morphological variant. Secondly, there are tools available that tell us how semantically related two words are, most notably WordNet (Miller et al., 1993). For these reasons we implemented a bespoke alignment strategy, tailored towards our problem description. This method is described in detail in (Kaisser, 2009). The processing steps described in the next sections build on its output. For reasons of brevity, we skip a detailed explanations in this paper and focus only on its key part: the alignment of words with very different surface structures. For more details we would like to point the reader to the aforementioned work. In the above example, the alignment of “pur91 chased” and “acquisition” is the most problematic, because the surface structures of the two words clearly are very different. For such cases we experimented with a number of alignment strategies based on WordNet. These approaches are </context>
<context position="16216" citStr="Kaisser, 2009" startWordPosition="2663" endWordPosition="2664">ord, if above a certain threshold, is selected as the closest semantic match. Most of these approaches make use of WordNet::Similarity, a Perl software package that measures semantic similarity (or relatedness) between a pair of word senses by returning a numeric value that represents the degree to which they are similar or related (Pedersen et al., 2004). Additionally, we developed a custom-built method that assumes that two words are semantically related if any kind of pointer exists between any occurrence of the words root form in WordNet. For details of these experiments, please refer to (Kaisser, 2009). In our experiments the custom-built method performed best, and was therefore used for the experiments described in this paper. The main reasons for this are: 1. Many of the measures in the WordNet::Similarity package take only hyponym/ hypernym relations into account. This makes aligning word of different parts of speech difficult or even impossible. However, such alignments are important for our needs. 2. Many of the measures return results, even if only a weak semantic relationship exists. For our purposes however, it is beneficial to only take strong semantic relations into account. 5 Pat</context>
</contexts>
<marker>Kaisser, 2009</marker>
<rawString>Michael Kaisser. 2009. Acquiring Syntactic and Semantic Transformations in Question Answering. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics (ACL-03).</booktitle>
<contexts>
<context position="17264" citStr="Klein and Manning, 2003" startWordPosition="2835" endWordPosition="2838">easures return results, even if only a weak semantic relationship exists. For our purposes however, it is beneficial to only take strong semantic relations into account. 5 Pattern Creation Figure 1 details our algorithm in its five key steps. In step 1 and 2 key phrases from the question are aligned to the corresponding phrases in the answer sentence, see Section 4 of this paper. Step 3 is concerned with retrieving the parse tree for the answer sentence. In our implementation all answer sentences in the training set have for performance reasons been parsed beforehand with the Stanford Parser (Klein and Manning, 2003b; Klein and Manning, 2003a), so at this point they are simply loaded from file. Step 4 is the key step in our algorithm. From the previous steps, we know where the key constituents from the question as well as the answer are located in the answer sentence. This enables us to compute the dependency paths in the answer sentences’ parse tree that connect the answer with the key constituents. In our example the answer is “1867” and the key constituents are “acquisition” and “Alaska.” Knowing the syntactic relationships (captured by their dependency paths) between the answer and the key phrases en</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. Accurate Unlexicalized Parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics (ACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15.</booktitle>
<contexts>
<context position="17264" citStr="Klein and Manning, 2003" startWordPosition="2835" endWordPosition="2838">easures return results, even if only a weak semantic relationship exists. For our purposes however, it is beneficial to only take strong semantic relations into account. 5 Pattern Creation Figure 1 details our algorithm in its five key steps. In step 1 and 2 key phrases from the question are aligned to the corresponding phrases in the answer sentence, see Section 4 of this paper. Step 3 is concerned with retrieving the parse tree for the answer sentence. In our implementation all answer sentences in the training set have for performance reasons been parsed beforehand with the Stanford Parser (Klein and Manning, 2003b; Klein and Manning, 2003a), so at this point they are simply loaded from file. Step 4 is the key step in our algorithm. From the previous steps, we know where the key constituents from the question as well as the answer are located in the answer sentence. This enables us to compute the dependency paths in the answer sentences’ parse tree that connect the answer with the key constituents. In our example the answer is “1867” and the key constituents are “acquisition” and “Alaska.” Knowing the syntactic relationships (captured by their dependency paths) between the answer and the key phrases en</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Advances in Neural Information Processing Systems 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Boris Katz</author>
</authors>
<title>Building a Reusable Test Collection for Question Answering.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science</journal>
<contexts>
<context position="22956" citStr="Lin and Katz, 2005" startWordPosition="3798" endWordPosition="3801">per. The key questions we seek to answer are the following: 1. How does our method perform when compared to a baseline that extracts dependency paths from the question? 2. How much does the described algorithm improve performance of a state-of-the-art QA system? 3. What is the effect of training data size on performance? Can we expect that more training data would further improve the algorithm’s performance? 7.1 Evaluation Setup We use all factoid questions in TREC’s QA test sets from 2002 to 2006 for evaluation for which a known answer exists in the AQUAINT corpus. Additionally, the data in (Lin and Katz, 2005) is used. In this paper the authors attempt to identify a much more complete set of relevant documents for a subset of TREC 2002 questions than TREC itself. We adopt a cross validation approach for our evaluation. Table 4 shows how the data is split into five folds. In order to evaluate the algorithm’s patterns we need a set of sentences to which they can be applied. In a traditional QA system architecture, 93 Test Number of Correct Answer Sentences Mean Med set = 0 &lt;= 1 &lt;= 3 &lt;= 5 &lt;= 10 &lt;= 25 &lt;= 50 &gt;= 75 &gt;= 90 &gt;= 100 2002 0.203 0.396 0.580 0.671 0.809 0.935 0.984 0.0 0.0 0.0 6.86 2.0 2003 0.24</context>
<context position="24898" citStr="Lin and Katz, 2005" startWordPosition="4172" endWordPosition="4175"> 0.410 0.589 0.759 0.141 0.097 0.069 30.87 17.0 2006 0.0 0.125 0.207 0.281 0.415 0.596 0.727 0.173 0.122 0.088 32.93 17.5 Table 3: Fraction of sentences that contain correct answers in Evaluation Set 2 (approximation). Fold Training Data Test Data sets used # set # 1 T03, T04, T05, T06 4565 T02 1159 2 T02, T04, T05, T06, Lin02 6174 T03 1352 3 T02, T03, T05, T06, Lin02 6700 T04 826 4 T02, T03, T04, T06, Lin02 6298 T05 1228 5 T02, T03, T04, T05, Lin02 6367 T06 1159 Table 4: Splits into training and tests sets of the data used for evaluation. T02 stands for TREC 2002 data etc. Lin02 is based on (Lin and Katz, 2005). The # rows show how many question/answer sentence pairs are used for training and for testing. see e.g. (Prager, 2006; Voorhees, 2003), the document or passage retrieval step performs this function. This step is crucial to a QA system’s performance, because it is impossible to locate answers in the subsequent answer extraction step if the passages returned during passage retrieval do not contain the answer in the first place. This also holds true in our case: the patterns cannot be expected to identify a correct answer if none of the sentences used as input contains the correct answer. We th</context>
</contexts>
<marker>Lin, Katz, 2005</marker>
<rawString>Jimmy Lin and Boris Katz. 2005. Building a Reusable Test Collection for Question Answering. Journal of the American Society for Information Science and Technology (JASIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of Inference Rules for Question-Answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="5957" citStr="Lin and Pantel, 2001" startWordPosition="991" endWordPosition="995">n model to parallel data consisting of user queries and snippets from clicked web documents and in such a way extract contextual expansion terms from the query rewrites. We see our work as addressing the same fundamental problem, but shifting focus from query term/document term mismatch to mismatches observed between the grammatical structure of Natural Language Queries and relevant text pieces. In order to achieve this we analyze the queries’ and the relevant contents’ syntactic structure by using dependency paths. Especially in QA there is a strong tradition of using dependency structures: (Lin and Pantel, 2001) present an unsupervised algorithm to automatically discover inference rules (essentially paraphrases) from text. These inference rules are based on dependency paths, each of which connects two nouns. Their paths have the following form: N:subj:V+—find—*V:obj:N—*solution—*N:to:N This path represents the relation “X finds a solution to Y” and can be mapped to another path representing e.g. “X solves Y.” As such the approach is suitable to detect paraphrases that describe the relation between two entities in documents. However, the paper does not describe how the mined paraphrases can be linked </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of Inference Rules for Question-Answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="6873" citStr="Lin, 1998" startWordPosition="1134" endWordPosition="1135">elation “X finds a solution to Y” and can be mapped to another path representing e.g. “X solves Y.” As such the approach is suitable to detect paraphrases that describe the relation between two entities in documents. However, the paper does not describe how the mined paraphrases can be linked to questions, and which paraphrase is suitable to answer which question type. (Attardi et al., 2001) describes a QA system that, after a set of candidate answer sentences have been identified, matches their dependency relations against the question. Questions and answer sentences are parsed with MiniPar (Lin, 1998) and the dependency output is analyzed in order to determine whether relations present in a question also appear in a candidate sentence. For the question “Who killed John F. Kennedy”, for example an answer sentence is expected to contain the answer as subject of the verb “kill”, to which “John F. Kennedy” should be in object relation. (Cui et al., 2005) describe a fuzzy dependency relation matching approach to passage retrieval in QA. Here, the authors present a statistical technique to measure the degree of overlap between dependency relations in candidate sentences with their corresponding </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An On-Line Lexical Database.</title>
<date>1993</date>
<journal>Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="14541" citStr="Miller et al., 1993" startWordPosition="2376" endWordPosition="2379"> NLP, e.g. Machine Translation (MT) where words in parallel, bilingual corpora need to be aligned, see (Och and Ney, 2003) for a comparison of various statistical alignment models. In our case however we are dealing with a monolingual alignment problem which enables us to exploit clues not available for bilingual alignment: First of all, we can expect many query words to be present in the answer sentence, either with the exact same surface appearance or in some morphological variant. Secondly, there are tools available that tell us how semantically related two words are, most notably WordNet (Miller et al., 1993). For these reasons we implemented a bespoke alignment strategy, tailored towards our problem description. This method is described in detail in (Kaisser, 2009). The processing steps described in the next sections build on its output. For reasons of brevity, we skip a detailed explanations in this paper and focus only on its key part: the alignment of words with very different surface structures. For more details we would like to point the reader to the aforementioned work. In the above example, the alignment of “pur91 chased” and “acquisition” is the most problematic, because the surface stru</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1993. Introduction to WordNet: An On-Line Lexical Database. Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego Molla</author>
</authors>
<title>Learning of Graph-based Question Answering Rules.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL 2006 Workshop on Graph Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="8749" citStr="Molla, 2006" startWordPosition="1445" endWordPosition="1446"> in the question, in a similar fashion as in (Cui et al., 2005). The candidate whose paths show the highest matching score is selected. (Shen and Klakow, 2006) also describe a method that is primarily based on similarity scores between dependency relation pairs. However, their algorithm computes the similarity of paths between key phrases, not between words. Furthermore, it takes relations in a path not as independent from each other, but acknowledges that they form a sequence, by comparing two paths with the help of an adaptation of the Dynamic Time Warping algorithm (Rabiner et al., 1991). (Molla, 2006) presents an approach for the acquisition of question answering rules by applying graph manipulation methods. Questions are represented as dependency graphs, which are extended with information from answer sentences. These combined graphs can then be used to identify answers. Finally, in (Wang et al., 2007), a quasi-synchronous grammar (Smith and Eisner, 2006) is used to model relations between questions and answer sentences. In this paper we describe an algorithm that learns possible syntactic answer sentence formulations for syntactic question classes from a set of example question/answer se</context>
</contexts>
<marker>Molla, 2006</marker>
<rawString>Diego Molla. 2006. Learning of Graph-based Question Answering Rules. In Proceedings of HLT/NAACL 2006 Workshop on Graph Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="14043" citStr="Och and Ney, 2003" startWordPosition="2290" endWordPosition="2293">he Alaska territory purchased?” Answer sentence: “The acquisition of what would become the territory of Alaska took place in 1867.” The mapping that has to be achieved is: Query Answer Sentence phrase phrase “Alaska territory” “territory of Alaska” “purchased” “acquisition” ANSWER “1867” In our approach, this is a two step process. First we align on a word level, then the output of the word alignment process is used to identify and align phrases. Word Alignment is important in many fields of NLP, e.g. Machine Translation (MT) where words in parallel, bilingual corpora need to be aligned, see (Och and Ney, 2003) for a comparison of various statistical alignment models. In our case however we are dealing with a monolingual alignment problem which enables us to exploit clues not available for bilingual alignment: First of all, we can expect many query words to be present in the answer sentence, either with the exact same surface appearance or in some morphological variant. Secondly, there are tools available that tell us how semantically related two words are, most notably WordNet (Miller et al., 1993). For these reasons we implemented a bespoke alignment strategy, tailored towards our problem descript</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04).</booktitle>
<contexts>
<context position="15959" citStr="Pedersen et al., 2004" startWordPosition="2619" endWordPosition="2622">that has to be aligned from the question at a time and compares it to all of the non-stop words in the answer sentence. Each of the answer sentence words is assigned a value between zero and one expressing its relatedness to the question word. The highest scoring word, if above a certain threshold, is selected as the closest semantic match. Most of these approaches make use of WordNet::Similarity, a Perl software package that measures semantic similarity (or relatedness) between a pair of word senses by returning a numeric value that represents the degree to which they are similar or related (Pedersen et al., 2004). Additionally, we developed a custom-built method that assumes that two words are semantically related if any kind of pointer exists between any occurrence of the words root form in WordNet. For details of these experiments, please refer to (Kaisser, 2009). In our experiments the custom-built method performed best, and was therefore used for the experiments described in this paper. The main reasons for this are: 1. Many of the measures in the WordNet::Similarity package take only hyponym/ hypernym relations into account. This makes aligning word of different parts of speech difficult or even </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
</authors>
<date>2006</date>
<booktitle>Open-Domain QuestionAnswering. Foundations and Trends in Information Retrieval,</booktitle>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="25017" citStr="Prager, 2006" startWordPosition="4194" endWordPosition="4195">e 3: Fraction of sentences that contain correct answers in Evaluation Set 2 (approximation). Fold Training Data Test Data sets used # set # 1 T03, T04, T05, T06 4565 T02 1159 2 T02, T04, T05, T06, Lin02 6174 T03 1352 3 T02, T03, T05, T06, Lin02 6700 T04 826 4 T02, T03, T04, T06, Lin02 6298 T05 1228 5 T02, T03, T04, T05, Lin02 6367 T06 1159 Table 4: Splits into training and tests sets of the data used for evaluation. T02 stands for TREC 2002 data etc. Lin02 is based on (Lin and Katz, 2005). The # rows show how many question/answer sentence pairs are used for training and for testing. see e.g. (Prager, 2006; Voorhees, 2003), the document or passage retrieval step performs this function. This step is crucial to a QA system’s performance, because it is impossible to locate answers in the subsequent answer extraction step if the passages returned during passage retrieval do not contain the answer in the first place. This also holds true in our case: the patterns cannot be expected to identify a correct answer if none of the sentences used as input contains the correct answer. We therefore use two different evaluation sets to evaluate our algorithm: 1. The first set contains for each question all se</context>
</contexts>
<marker>Prager, 2006</marker>
<rawString>John Prager. 2006. Open-Domain QuestionAnswering. Foundations and Trends in Information Retrieval, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>A E Rosenberg</author>
<author>S E Levinson</author>
</authors>
<title>Considerations in Dynamic Time Warping Algorithms for Discrete Word Recognition.</title>
<date>1991</date>
<booktitle>In Proceedings of IEEE Transactions on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="8734" citStr="Rabiner et al., 1991" startWordPosition="1441" endWordPosition="1444"> the corresponding paths in the question, in a similar fashion as in (Cui et al., 2005). The candidate whose paths show the highest matching score is selected. (Shen and Klakow, 2006) also describe a method that is primarily based on similarity scores between dependency relation pairs. However, their algorithm computes the similarity of paths between key phrases, not between words. Furthermore, it takes relations in a path not as independent from each other, but acknowledges that they form a sequence, by comparing two paths with the help of an adaptation of the Dynamic Time Warping algorithm (Rabiner et al., 1991). (Molla, 2006) presents an approach for the acquisition of question answering rules by applying graph manipulation methods. Questions are represented as dependency graphs, which are extended with information from answer sentences. These combined graphs can then be used to identify answers. Finally, in (Wang et al., 2007), a quasi-synchronous grammar (Smith and Eisner, 2006) is used to model relations between questions and answer sentences. In this paper we describe an algorithm that learns possible syntactic answer sentence formulations for syntactic question classes from a set of example que</context>
</contexts>
<marker>Rabiner, Rosenberg, Levinson, 1991</marker>
<rawString>L. R. Rabiner, A. E. Rosenberg, and S. E. Levinson. 1991. Considerations in Dynamic Time Warping Algorithms for Discrete Word Recognition. In Proceedings of IEEE Transactions on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning Surface Text Patterns for a Question Answering System.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02).</booktitle>
<contexts>
<context position="19898" citStr="Ravichandran and Hovy, 2002" startWordPosition="3273" endWordPosition="3276"> course also be used for more complicated NLQs, although in such a scenario a significantly larger amount of training data would have to be used. 6 Pattern Evaluation For each created pattern, at least one matching example must exists: the sentence that was 92 used to create it in the first place. However, we do not know how precise each pattern is. To this end, an additional processing step between pattern creation and application is needed: pattern evaluation. Similar approaches to ours have been described in the relevant literature, many of them concerned with bootstrapping, starting with (Ravichandran and Hovy, 2002). The general purpose of this step is to use the available data about questions and their correct answers to evaluate how often each created pattern returns a correct or an incorrect result. This data is stored with each pattern and the result of the equation, often called pattern precision, can be used during retrieval stage. Pattern precision in our case is defined as: p= #correct + 1 (1) #correct + #incorrect + 2 We use Lucene to retrieve the top 100 paragraphs from the AQUAINT corpus by issuing a query that consists of the query’s key words and all non-stop words in the answer. Then, all p</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning Surface Text Patterns for a Question Answering System. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Yi Liu</author>
</authors>
<title>Query Rewriting using Monolingual Statistical Machine Translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="5297" citStr="Riezler and Liu, 2010" startWordPosition="886" endWordPosition="889"> same terms is commonly encountered. Latent Semantic Indexing (Deerwester et al., 1900) was an early, highly influential approach to solve this problem. More recently, a significant amount of research is dedicated to query alteration approaches. (Cui et al., 2002), for example, assume that if queries containing one term often result in the selection of documents containing another term, then a strong relationship between the two terms exist. In their approach, query terms and document terms are linked via sessions in which users click on documents that are presented as results for the query. (Riezler and Liu, 2010) apply a Statistical Machine Translation model to parallel data consisting of user queries and snippets from clicked web documents and in such a way extract contextual expansion terms from the query rewrites. We see our work as addressing the same fundamental problem, but shifting focus from query term/document term mismatch to mismatches observed between the grammatical structure of Natural Language Queries and relevant text pieces. In order to achieve this we analyze the queries’ and the relevant contents’ syntactic structure by using dependency paths. Especially in QA there is a strong trad</context>
</contexts>
<marker>Riezler, Liu, 2010</marker>
<rawString>Stefan Riezler and Yi Liu. 2010. Query Rewriting using Monolingual Statistical Machine Translation. Computational Linguistics, 36(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Dietrich Klakow</author>
</authors>
<title>Exploring Correlation of Dependency Relation Paths for Answer Extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL (COLING/ACL-06).</booktitle>
<contexts>
<context position="8296" citStr="Shen and Klakow, 2006" startWordPosition="1370" endWordPosition="1373">d, but this time to learn relatedness between paths. (Cui et al., 2004) apply the same idea to answer ex89 traction. In each sentences returned by the IR module, all named entities of the expected answer types are treated as answer candidates. For questions with an unknown answer type, all NPs in the candidate sentence are considered. Then those paths in the answer sentence that are connected to an answer candidate are compared against the corresponding paths in the question, in a similar fashion as in (Cui et al., 2005). The candidate whose paths show the highest matching score is selected. (Shen and Klakow, 2006) also describe a method that is primarily based on similarity scores between dependency relation pairs. However, their algorithm computes the similarity of paths between key phrases, not between words. Furthermore, it takes relations in a path not as independent from each other, but acknowledges that they form a sequence, by comparing two paths with the help of an adaptation of the Dynamic Time Warping algorithm (Rabiner et al., 1991). (Molla, 2006) presents an approach for the acquisition of question answering rules by applying graph manipulation methods. Questions are represented as dependen</context>
</contexts>
<marker>Shen, Klakow, 2006</marker>
<rawString>Dan Shen and Dietrich Klakow. 2006. Exploring Correlation of Dependency Relation Paths for Answer Extraction. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL (COLING/ACL-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasisynchronous grammars: Alignment by Soft Projection of Syntactic Dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLTNAACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="9111" citStr="Smith and Eisner, 2006" startWordPosition="1499" endWordPosition="1502">en words. Furthermore, it takes relations in a path not as independent from each other, but acknowledges that they form a sequence, by comparing two paths with the help of an adaptation of the Dynamic Time Warping algorithm (Rabiner et al., 1991). (Molla, 2006) presents an approach for the acquisition of question answering rules by applying graph manipulation methods. Questions are represented as dependency graphs, which are extended with information from answer sentences. These combined graphs can then be used to identify answers. Finally, in (Wang et al., 2007), a quasi-synchronous grammar (Smith and Eisner, 2006) is used to model relations between questions and answer sentences. In this paper we describe an algorithm that learns possible syntactic answer sentence formulations for syntactic question classes from a set of example question/answer sentence pairs. Unlike the related work described above, it acknowledges that a) a valid answer sentence’s syntax might be very different for the question’s syntax and b) several valid answer sentence structures, which might be completely independent from each other, can exist for one and the same question. To illustrate this consider the question “When was Alas</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Quasisynchronous grammars: Alignment by Soft Projection of Syntactic Dependencies. In Proceedings of the HLTNAACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<date>1999</date>
<booktitle>Overview of the Eighth Text REtrieval Conference (TREC-8). In Proceedings of the Eighth Text REtrieval Conference (TREC-8).</booktitle>
<contexts>
<context position="2727" citStr="Voorhees, 1999" startWordPosition="449" endWordPosition="450">ious work that uses dependency paths for QA (see Section 2), our approach does not require a candidate sentence to be similar to the question in any respect. We learn valid dependency structures from the known answer sentences alone, and therefore are able to link a much wider spectrum of answer sentences to the question. The work in this paper is presented and evaluated in a classical factoid Question Answering (QA) setting. The main reason for this is that in QA suitable training and test data is available in the public domain, e.g. via the Text REtrieval Conference (TREC), see for example (Voorhees, 1999). The methods described in this paper however can also be applied to other IR scenarios, e.g. web search. The necessary condition for our approach to work is that the user query is somewhat grammatically well formed; this kind of queries are commonly referred to as Natural Language Queries or NLQs. Table 1 provides evidence that users indeed search the web with NLQs. The data is based on two query sets sampled from three months of user logs from a popular search engine, using two different sampling techniques. The “head” set samples queries taking query frequency into account, so that more com</context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>Ellen M. Voorhees. 1999. Overview of the Eighth Text REtrieval Conference (TREC-8). In Proceedings of the Eighth Text REtrieval Conference (TREC-8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Edition of the Text REtrieval Conference (TREC-03).</booktitle>
<contexts>
<context position="25034" citStr="Voorhees, 2003" startWordPosition="4196" endWordPosition="4197">of sentences that contain correct answers in Evaluation Set 2 (approximation). Fold Training Data Test Data sets used # set # 1 T03, T04, T05, T06 4565 T02 1159 2 T02, T04, T05, T06, Lin02 6174 T03 1352 3 T02, T03, T05, T06, Lin02 6700 T04 826 4 T02, T03, T04, T06, Lin02 6298 T05 1228 5 T02, T03, T04, T05, Lin02 6367 T06 1159 Table 4: Splits into training and tests sets of the data used for evaluation. T02 stands for TREC 2002 data etc. Lin02 is based on (Lin and Katz, 2005). The # rows show how many question/answer sentence pairs are used for training and for testing. see e.g. (Prager, 2006; Voorhees, 2003), the document or passage retrieval step performs this function. This step is crucial to a QA system’s performance, because it is impossible to locate answers in the subsequent answer extraction step if the passages returned during passage retrieval do not contain the answer in the first place. This also holds true in our case: the patterns cannot be expected to identify a correct answer if none of the sentences used as input contains the correct answer. We therefore use two different evaluation sets to evaluate our algorithm: 1. The first set contains for each question all sentences in the to</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Ellen M. Voorhees. 2003. Overview of the TREC 2003 Question Answering Track. In Proceedings of the 2003 Edition of the Text REtrieval Conference (TREC-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? A Quasisynchronous Grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<contexts>
<context position="9057" citStr="Wang et al., 2007" startWordPosition="1492" endWordPosition="1495">imilarity of paths between key phrases, not between words. Furthermore, it takes relations in a path not as independent from each other, but acknowledges that they form a sequence, by comparing two paths with the help of an adaptation of the Dynamic Time Warping algorithm (Rabiner et al., 1991). (Molla, 2006) presents an approach for the acquisition of question answering rules by applying graph manipulation methods. Questions are represented as dependency graphs, which are extended with information from answer sentences. These combined graphs can then be used to identify answers. Finally, in (Wang et al., 2007), a quasi-synchronous grammar (Smith and Eisner, 2006) is used to model relations between questions and answer sentences. In this paper we describe an algorithm that learns possible syntactic answer sentence formulations for syntactic question classes from a set of example question/answer sentence pairs. Unlike the related work described above, it acknowledges that a) a valid answer sentence’s syntax might be very different for the question’s syntax and b) several valid answer sentence structures, which might be completely independent from each other, can exist for one and the same question. T</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? A Quasisynchronous Grammar for QA. In Proceedings of EMNLP-CoNLL 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>