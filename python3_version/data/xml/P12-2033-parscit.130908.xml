<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000206">
<title confidence="0.997318">
A Two-step Approach to Sentence Compression of Spoken Utterances
</title>
<author confidence="0.973981">
Dong Wang, Xian Qian, Yang Liu
</author>
<affiliation confidence="0.973377">
The University of Texas at Dallas
</affiliation>
<email confidence="0.998274">
dongwang,qx,yangl@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999622588235294">
This paper presents a two-step approach to
compress spontaneous spoken utterances. In
the first step, we use a sequence labeling
method to determine if a word in the utterance
can be removed, and generate n-best com-
pressed sentences. In the second step, we
use a discriminative training approach to cap-
ture sentence level global information from
the candidates and rerank them. For evalua-
tion, we compare our system output with mul-
tiple human references. Our results show that
the new features we introduced in the first
compression step improve performance upon
the previous work on the same data set, and
reranking is able to yield additional gain, espe-
cially when training is performed to take into
account multiple references.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.884735063829787">
Sentence compression aims to preserve the most im-
portant information in the original sentence with
fewer words. It can be used for abstractive summa-
rization where extracted important sentences often
need to be compressed and merged. For summariza-
tion of spontaneous speech, sentence compression
is especially important, since unlike fluent and well-
structured written text, spontaneous speech contains
a lot of disfluencies and much redundancy. The fol-
lowing shows an example of a pair of source and
compressed spoken sentences1 from human annota-
tion (removed words shown in bold):
[original sentence]
1For speech domains, “sentences” are not clearly defined.
We use sentences and utterances interchangeably when there is
no ambiguity.
and then um in terms of the source the things uh the
only things that we had on there I believe were whether...
[compressed sentence]
and then in terms of the source the only things that we
had on there were whether...
In this study we investigate sentence compres-
sion of spoken utterances in order to remove re-
dundant or unnecessary words while trying to pre-
serve the information in the original sentence. Sen-
tence compression has been studied from formal
text domain to speech domain. In text domain,
(Knight and Marcu, 2000) applies noisy-channel
model and decision tree approaches on this prob-
lem. (Galley and Mckeown, 2007) proposes to use a
synchronous context-free grammars (SCFG) based
method to compress the sentence. (Cohn and La-
pata, 2008) expands the operation set by including
insertion, substitution and reordering, and incorpo-
rates grammar rules. In speech domain, (Clarke and
Lapata, 2008) investigates sentence compression in
broadcast news using an integer linear programming
approach. There is only a few existing work in spon-
taneous speech domains. (Liu and Liu, 2010) mod-
eled it as a sequence labeling problem using con-
ditional random fields model. (Liu and Liu, 2009)
compared the effect of different compression meth-
ods on a meeting summarization task, but did not
evaluate sentence compression itself.
We propose to use a two-step approach in this pa-
per for sentence compression of spontaneous speech
utterances. The contributions of our work are:
</bodyText>
<listItem confidence="0.8728862">
• Our proposed two-step approach allows us to
incorporate features from local and global lev-
els. In the first step, we adopt a similar se-
quence labeling method as used in (Liu and
Liu, 2010), but expanded the feature set, which
</listItem>
<page confidence="0.982126">
166
</page>
<note confidence="0.684203">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 166–170,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997362">
results in better performance. In the second
step, we use discriminative reranking to in-
corporate global information about the com-
pressed sentence candidates, which cannot be
accomplished by word level labeling.
</bodyText>
<listItem confidence="0.986588125">
• We evaluate our methods using different met-
rics including word-level accuracy and F1-
measure by comparing to one reference com-
pression, and BLEU scores comparing with
multiple references. We also demonstrate that
training in the reranking module can be tailed
to the evaluation metrics to optimize system
performance.
</listItem>
<sectionHeader confidence="0.922014" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999991405405405">
We use the same corpus as (Liu and Liu, 2010)
where they annotated 2,860 summary sentences in
26 meetings from the ICSI meeting corpus (Murray
et al., 2005). In their annotation procedure, filled
pauses such as “uh/um” and incomplete words are
removed before annotation. In the first step, 8 anno-
tators were asked to select words to be removed to
compress the sentences. In the second step, 6 an-
notators (different from the first step) were asked
to pick the best one from the 8 compressions from
the previous step. Therefore for each sentence, we
have 8 human compressions, as well a best one se-
lected by the majority of the 6 annotators in the sec-
ond step. The compression ratio of the best human
reference is 63.64%.
In the first step of our sentence compression ap-
proach (described below), for model training we
need the reference labels for each word, which rep-
resents whether it is preserved or deleted in the com-
pressed sentence. In (Liu and Liu, 2010), they used
the labels from the annotators directly. In this work,
we use a different way. For each sentence, we still
use the best compression as the gold standard, but
we realign the pair of the source sentence and the
compressed sentence, instead of using the labels
provided by annotators. This is because when there
are repeated words, annotators sometimes randomly
pick removed ones. However, we want to keep the
patterns consistent for model training – we always
label the last appearance of the repeated words as
‘preserved’, and the earlier ones as ‘deleted’. An-
other difference in our processing of the corpus from
the previous work is that when aligning the original
and the compressed sentence, we keep filled pauses
and incomplete words since they tend to appear to-
gether with disfluencies and thus provide useful in-
formation for compression.
</bodyText>
<sectionHeader confidence="0.97015" genericHeader="method">
3 Sentence Compression Approach
</sectionHeader>
<bodyText confidence="0.999996727272727">
Our compression approach has two steps: in the
first step, we use Conditional Random Fields (CRFs)
to model this problem as a sequence labeling task,
where the label indicates whether the word should be
removed or not. We select n-best candidates (n = 25
in our work) from this step. In the second step we
use discriminative training based on a maximum En-
tropy model to rerank the candidate compressions,
in order to select the best one based on the quality
of the whole candidate sentence, which cannot be
performed in the first step.
</bodyText>
<subsectionHeader confidence="0.998775">
3.1 Generate N-best Candidates
</subsectionHeader>
<bodyText confidence="0.999992588235294">
In the first step, we cast sentence compression as
a sequence labeling problem. Considering that in
many cases phrases instead of single words are
deleted, we adopt the ‘BIO’ labeling scheme, simi-
lar to the name entity recognition task: “B” indicates
the first word of the removed fragment, “I” repre-
sents inside the removed fragment (except the first
word), and “O” means outside the removed frag-
ment, i.e., words remaining in the compressed sen-
tence. Each sentence with n words can be viewed as
a word sequence X1, X2,..., Xn, and our task is to
find the best label sequence Y1, Y2,..., Yn where Yi
is one of the three labels. Similar to (Liu and Liu,
2010), for sequence labeling we use linear-chain
first-order CRFs. These models define the condi-
tional probability of each labeling sequence given
the word sequence as:
</bodyText>
<equation confidence="0.735103">
p(Y |X) a
exp E k_1(Ej ajfj(Yk, Yk−1, X) + Ei µi9i(xk, Yk, X))
</equation>
<bodyText confidence="0.999769411764706">
where fj are transition feature functions (here first-
order Markov independence assumption is used); gi
are observation feature functions; Aj and µi are their
corresponding weights. To train the model for this
step, we use the best reference compression to obtain
the reference labels (as described in Section 2).
In the CRF compression model, each word is rep-
resented by a feature vector. We incorporate most
of the features used in (Liu and Liu, 2010), includ-
ing unigram, position, length of utterance, part-of-
speech tag as well as syntactic parse tree tags. We
did not use the discourse parsing tree based features
because we found they are not useful in our exper-
iments. In this work, we further expand the feature
set in order to represent the characteristics of disflu-
encies in spontaneous speech as well as model the
adjacent output labels. The additional features we
</bodyText>
<page confidence="0.984278">
167
</page>
<listItem confidence="0.975724631578947">
introduced are:
• the distance to the next same word and the next
same POS tag.
• a binary feature to indicate if there is a filled
pause or incomplete word in the following 4-
word window. We add this feature since filled
pauses or incomplete words often appear after
disfluent words.
• the combination of word/POS tag and its posi-
tion in the sentence.
• language model probabilities: the bigram prob-
ability of the current word given the previous
one, and followed by the next word, and their
product. These probabilities are obtained from
the Google Web 1T 5-gram.
• transition features: a combination of the current
output label and the previous one, together with
some observation features such as the unigram
and bigrams of word or POS tag.
</listItem>
<subsectionHeader confidence="0.995944">
3.2 Discriminative Reranking
</subsectionHeader>
<bodyText confidence="0.999949285714286">
Although CRFs is able to model the dependency
of adjacent labels, it does not measure the quality
of the whole sentence. In this work, we propose
to use discriminative training to rerank the candi-
dates generated in the first step. Reranking has been
used in many tasks to find better global solutions,
such as machine translation (Wang et al., 2007),
parsing (Charniak and Johnson, 2005), and disflu-
ency detection (Zwarts and Johnson, 2011). We use
a maximum Entropy reranker to learn distributions
over a set of candidates such that the probability of
the best compression is maximized. The conditional
probability of output y given observation x in the
maximum entropy model is defined as:
</bodyText>
<equation confidence="0.896281">
p(y |x) = Z(X) exp [� 1 Ad (x, y)]
</equation>
<bodyText confidence="0.9995152">
where f(x, y) are feature functions and Ai are their
weighting parameters; Z(x) is the normalization
factor.
In this reranking model, every compression can-
didate is represented by the following features:
</bodyText>
<listItem confidence="0.998972666666667">
• All the bigrams and trigrams of words and POS
tags in the candidate sentence.
• Bigrams and trigrams of words and POS tags in
</listItem>
<bodyText confidence="0.5791972">
the original sentence in combination with their
binary labels in the candidate sentence (delete
the word or not). For example, if the origi-
nal sentence is “so I should go”, and the can-
didate compression sentence is “I should go”,
then “so I 10”, “so I should 100” are included
in the features (1 means the word is deleted).
• The log likelihood of the candidate sentence
based on the language model.
• The absolute difference of the compression ra-
tio of the candidate sentence with that of the
first ranked candidate. This is because we try
to avoid a very large or small compression ra-
tio, and the first candidate is generally a good
candidate with reasonable length.
</bodyText>
<listItem confidence="0.9543305">
• The probability of the label sequence of the
candidate sentence given by the first step CRFs.
• The rank of the candidate sentence in 25 best
list.
</listItem>
<bodyText confidence="0.999879555555556">
For discriminative training using the n-best can-
didates, we need to identify the best candidate from
the n-best list, which can be either the reference
compression (if it exists on the list), or the most
similar candidate to the reference. Since we have
8 human compressions and also want to evaluate
system performance using all of them (see exper-
iments later), we try to use multiple references in
this reranking step. In order to use the same train-
ing objective (maximize the score for the single best
among all the instances), for the 25-best list, if m
reference compressions exist, we split the list into
m groups, each of which is a new sample containing
one reference as positive and several negative can-
didates. If no reference compression appears in 25-
best list, we just keep the entire list and label the in-
stance that is most similar to the best reference com-
pression as positive.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.994919142857143">
We perform a cross-validation evaluation where one
meeting is used for testing and the rest of them are
used as the training set. When evaluating the system
performance, we do not consider filled pauses and
incomplete words since they can be easily identi-
fied and removed. We use two different performance
metrics in this study.
</bodyText>
<listItem confidence="0.90619825">
• Word-level accuracy and F1 score based on the
minor class (removed words). This was used
in (Liu and Liu, 2010). These measures are ob-
tained by comparing with the best compression.
</listItem>
<bodyText confidence="0.769353">
In evaluation we map the result using ‘BIO’ la-
bels from the first-step compression to binary
labels that indicate a word is removed or not.
</bodyText>
<page confidence="0.981609">
168
</page>
<listItem confidence="0.54834825">
• BLEU score. BLEU is a widely used metric
in evaluating machine translation systems that
often use multiple references. Since there is a
great variation in human compression results,
</listItem>
<bodyText confidence="0.997500545454546">
and we have 8 reference compressions, we ex-
plore using BLEU for our sentence compres-
sion task. BLEU is calculated based on the pre-
cision of n-grams. In our experiments we use
up to 4-grams.
Table 1 shows the averaged scores of the cross
validation evaluation using the above metrics for
several methods. Also shown in the table is the com-
pression ratio of the system output. For “reference”,
we randomly choose one compression from 8 ref-
erences, and use the rest of them as references in
calculating the BLEU score. This represents human
performance. The row “basic features” shows the
result of using all features in (Liu and Liu, 2010)
except discourse parsing tree based features, and us-
ing binary labels (removed or not). The next row
uses this same basic feature set and “BIO” labels.
Row “expanded features” shows the result of our ex-
panded feature set using “BIO” label set from the
first step of compression. The last two rows show
the results after reranking, trained using one best ref-
erence or 8 reference compressions, respectively.
</bodyText>
<table confidence="0.9988889">
accuracy F1 BLEU ratio (%)
reference 81.96 69.73 95.36 76.78
basic features (Liu 76.44 62.11 91.08 73.49
and Liu, 2010)
basic features, BIO 77.10 63.34 91.41 73.22
expanded features 79.28 67.37 92.70 72.17
reranking 79.01 67.74 91.90 70.60
train w/ 1 ref
reranking 78.78 63.76 94.21 77.15
train w/ 8 refs
</table>
<tableCaption confidence="0.999935">
Table 1: Compression results using different systems.
</tableCaption>
<bodyText confidence="0.999985">
Our result using the basic feature set is similar to
that in (Liu and Liu, 2010) (their accuracy is 76.27%
when compression ratio is 0.7), though the experi-
mental setups are different: they used 6 meetings as
the test set while we performed cross validation. Us-
ing the “BIO” label set instead of binary labels has
marginal improvement for the three scores. From
the table, we can see that our expanded feature set is
able to significantly improve the result, suggesting
the effectiveness of the new introduced features.
Regarding the two training settings in reranking,
we find that there is no gain from reranking when
using only one best compression, however, train-
ing with multiple references improves BLEU scores.
This indicates the discriminative training used in
maximum entropy reranking is consistent with the
performance metrics. Another reason for the per-
formance gain for this condition is that there is less
data imbalance in model training (since we split the
n-best list, each containing fewer negative exam-
ples). We also notice that the compression ratio af-
ter reranking is more similar to the reference. As
suggested in (Napoles et al., 2011), it is not appro-
priate to compare compression systems with differ-
ent compression ratios, especially when considering
grammars and meanings. Therefore for the com-
pression system without reranking, we generated re-
sults with the same compression ratio (77.15%), and
found that using reranking still outperforms this re-
sult, 1.19% higher in BLEU score.
For an analysis, we check how often our sys-
tem output contains reference compressions based
on the 8 references. We found that 50.8% of sys-
tem generated compressions appear in the 8 refer-
ences when using CRF output with a compression
ration of 77.15%; and after reranking this number
increases to 54.8%. This is still far from the oracle
result – for 84.7% of sentences, the 25-best list con-
tains one or more reference sentences, that is, there
is still much room for improvement in the reranking
process. The results above also show that the token
level measures by comparing to one best reference
do not always correlate well with BLEU scores ob-
tained by comparing with multiple references, which
shows the need of considering multiple metrics.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999939">
This paper presents a 2-step approach for sentence
compression: we first generate an n-best list for each
source sentence using a sequence labeling method,
then rerank the n-best candidates to select the best
one based on the quality of the whole candidate sen-
tence using discriminative training. We evaluate the
system performance using different metrics. Our re-
sults show that our expanded feature set improves
the performance across multiple metrics, and rerank-
ing is able to improve the BLEU score. In future
work, we will incorporate more syntactic informa-
tion in the model to better evaluate sentence quality.
We also plan to perform a human evaluation for the
compressed sentences, and use sentence compres-
sion in summarization.
</bodyText>
<page confidence="0.998417">
169
</page>
<sectionHeader confidence="0.99899" genericHeader="conclusions">
6 Acknowledgment
</sectionHeader>
<bodyText confidence="0.9997264">
This work is partly supported by DARPA un-
der Contract No. HR0011-12-C-0016 and NSF
No. 0845484. Any opinions expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of DARPA or NSF.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999435268292683">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
173–180, Stroudsburg, PA, USA. Proceedings of ACL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399–429, March.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Michel Galley and Kathleen R. Mckeown. 2007. Lex-
icalized Markov grammars for sentence compression.
In Proceedings of HLT-NAACL.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization-step one: Sentence compression. In
Proceedings of AAAI.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: can it be done by sentence
compression? In Proceedings of the ACL-IJCNLP.
Fei Liu and Yang Liu. 2010. Using spoken utterance
compression for meeting summarization: a pilot study.
In Proceedings of SLT.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of EUROSPEECH.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating Sentence Com-
pression: Pitfalls and Suggested Remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-Text
Generation, pages 91–97, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Wen Wang, A. Stolcke, and Jing Zheng. 2007. Rerank-
ing machine translation hypotheses with structured
and web-based language models. In Proceedings of
IEEE Workshop on Speech Recognition and Under-
standing, pages 159–164, Kyoto.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. In Proceedings of ACL.
</reference>
<page confidence="0.997383">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982793">
<title confidence="0.99962">A Two-step Approach to Sentence Compression of Spoken Utterances</title>
<author confidence="0.999438">Dong Wang</author>
<author confidence="0.999438">Xian Qian</author>
<author confidence="0.999438">Yang</author>
<affiliation confidence="0.988612">The University of Texas at</affiliation>
<email confidence="0.99818">dongwang,qx,yangl@hlt.utdallas.edu</email>
<abstract confidence="0.999804333333334">This paper presents a two-step approach to compress spontaneous spoken utterances. In the first step, we use a sequence labeling method to determine if a word in the utterance can be removed, and generate n-best compressed sentences. In the second step, we use a discriminative training approach to capture sentence level global information from the candidates and rerank them. For evaluation, we compare our system output with multiple human references. Our results show that the new features we introduced in the first compression step improve performance upon the previous work on the same data set, and reranking is able to yield additional gain, especially when training is performed to take into account multiple references.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9435" citStr="Charniak and Johnson, 2005" startWordPosition="1552" endWordPosition="1555"> from the Google Web 1T 5-gram. • transition features: a combination of the current output label and the previous one, together with some observation features such as the unigram and bigrams of word or POS tag. 3.2 Discriminative Reranking Although CRFs is able to model the dependency of adjacent labels, it does not measure the quality of the whole sentence. In this work, we propose to use discriminative training to rerank the candidates generated in the first step. Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al., 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). We use a maximum Entropy reranker to learn distributions over a set of candidates such that the probability of the best compression is maximized. The conditional probability of output y given observation x in the maximum entropy model is defined as: p(y |x) = Z(X) exp [� 1 Ad (x, y)] where f(x, y) are feature functions and Ai are their weighting parameters; Z(x) is the normalization factor. In this reranking model, every compression candidate is represented by the following features: • All the bigrams and trigrams of words and POS tags in </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173–180, Stroudsburg, PA, USA. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>31--399</pages>
<contexts>
<context position="2560" citStr="Clarke and Lapata, 2008" startWordPosition="399" endWordPosition="402">terances in order to remove redundant or unnecessary words while trying to preserve the information in the original sentence. Sentence compression has been studied from formal text domain to speech domain. In text domain, (Knight and Marcu, 2000) applies noisy-channel model and decision tree approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling problem using conditional random fields model. (Liu and Liu, 2009) compared the effect of different compression methods on a meeting summarization task, but did not evaluate sentence compression itself. We propose to use a two-step approach in this paper for sentence compression of spontaneous speech utterances. The contributions of our work are: • Our proposed two-step approach </context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression an integer linear programming approach. Journal of Artificial Intelligence Research, 31:399–429, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2405" citStr="Cohn and Lapata, 2008" startWordPosition="376" endWordPosition="380">ence] and then in terms of the source the only things that we had on there were whether... In this study we investigate sentence compression of spoken utterances in order to remove redundant or unnecessary words while trying to preserve the information in the original sentence. Sentence compression has been studied from formal text domain to speech domain. In text domain, (Knight and Marcu, 2000) applies noisy-channel model and decision tree approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling problem using conditional random fields model. (Liu and Liu, 2009) compared the effect of different compression methods on a meeting summarization task, but did not evaluate sentence compression itself. We propose to use a two-</context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen R Mckeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2283" citStr="Galley and Mckeown, 2007" startWordPosition="358" endWordPosition="361"> then um in terms of the source the things uh the only things that we had on there I believe were whether... [compressed sentence] and then in terms of the source the only things that we had on there were whether... In this study we investigate sentence compression of spoken utterances in order to remove redundant or unnecessary words while trying to preserve the information in the original sentence. Sentence compression has been studied from formal text domain to speech domain. In text domain, (Knight and Marcu, 2000) applies noisy-channel model and decision tree approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling problem using conditional random fields model. (Liu and Liu, 2009) compared the effect of different compr</context>
</contexts>
<marker>Galley, Mckeown, 2007</marker>
<rawString>Michel Galley and Kathleen R. Mckeown. 2007. Lexicalized Markov grammars for sentence compression. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistics-based summarization-step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="2182" citStr="Knight and Marcu, 2000" startWordPosition="343" endWordPosition="346">ot clearly defined. We use sentences and utterances interchangeably when there is no ambiguity. and then um in terms of the source the things uh the only things that we had on there I believe were whether... [compressed sentence] and then in terms of the source the only things that we had on there were whether... In this study we investigate sentence compression of spoken utterances in order to remove redundant or unnecessary words while trying to preserve the information in the original sentence. Sentence compression has been studied from formal text domain to speech domain. In text domain, (Knight and Marcu, 2000) applies noisy-channel model and decision tree approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling prob</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statistics-based summarization-step one: Sentence compression. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>From extractive to abstractive meeting summaries: can it be done by sentence compression?</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP.</booktitle>
<contexts>
<context position="2844" citStr="Liu and Liu, 2009" startWordPosition="445" endWordPosition="448">ee approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling problem using conditional random fields model. (Liu and Liu, 2009) compared the effect of different compression methods on a meeting summarization task, but did not evaluate sentence compression itself. We propose to use a two-step approach in this paper for sentence compression of spontaneous speech utterances. The contributions of our work are: • Our proposed two-step approach allows us to incorporate features from local and global levels. In the first step, we adopt a similar sequence labeling method as used in (Liu and Liu, 2010), but expanded the feature set, which 166 Proceedings of the 50th Annual Meeting of the Association for Computational Linguisti</context>
</contexts>
<marker>Liu, Liu, 2009</marker>
<rawString>Fei Liu and Yang Liu. 2009. From extractive to abstractive meeting summaries: can it be done by sentence compression? In Proceedings of the ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>Using spoken utterance compression for meeting summarization: a pilot study.</title>
<date>2010</date>
<booktitle>In Proceedings of SLT.</booktitle>
<contexts>
<context position="2743" citStr="Liu and Liu, 2010" startWordPosition="427" endWordPosition="430">o speech domain. In text domain, (Knight and Marcu, 2000) applies noisy-channel model and decision tree approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling problem using conditional random fields model. (Liu and Liu, 2009) compared the effect of different compression methods on a meeting summarization task, but did not evaluate sentence compression itself. We propose to use a two-step approach in this paper for sentence compression of spontaneous speech utterances. The contributions of our work are: • Our proposed two-step approach allows us to incorporate features from local and global levels. In the first step, we adopt a similar sequence labeling method as used in (Liu and Liu, 2010), but expanded the feature</context>
<context position="4140" citStr="Liu and Liu, 2010" startWordPosition="649" endWordPosition="652"> for Computational Linguistics results in better performance. In the second step, we use discriminative reranking to incorporate global information about the compressed sentence candidates, which cannot be accomplished by word level labeling. • We evaluate our methods using different metrics including word-level accuracy and F1- measure by comparing to one reference compression, and BLEU scores comparing with multiple references. We also demonstrate that training in the reranking module can be tailed to the evaluation metrics to optimize system performance. 2 Corpus We use the same corpus as (Liu and Liu, 2010) where they annotated 2,860 summary sentences in 26 meetings from the ICSI meeting corpus (Murray et al., 2005). In their annotation procedure, filled pauses such as “uh/um” and incomplete words are removed before annotation. In the first step, 8 annotators were asked to select words to be removed to compress the sentences. In the second step, 6 annotators (different from the first step) were asked to pick the best one from the 8 compressions from the previous step. Therefore for each sentence, we have 8 human compressions, as well a best one selected by the majority of the 6 annotators in the</context>
<context position="7168" citStr="Liu and Liu, 2010" startWordPosition="1169" endWordPosition="1172">ession as a sequence labeling problem. Considering that in many cases phrases instead of single words are deleted, we adopt the ‘BIO’ labeling scheme, similar to the name entity recognition task: “B” indicates the first word of the removed fragment, “I” represents inside the removed fragment (except the first word), and “O” means outside the removed fragment, i.e., words remaining in the compressed sentence. Each sentence with n words can be viewed as a word sequence X1, X2,..., Xn, and our task is to find the best label sequence Y1, Y2,..., Yn where Yi is one of the three labels. Similar to (Liu and Liu, 2010), for sequence labeling we use linear-chain first-order CRFs. These models define the conditional probability of each labeling sequence given the word sequence as: p(Y |X) a exp E k_1(Ej ajfj(Yk, Yk−1, X) + Ei µi9i(xk, Yk, X)) where fj are transition feature functions (here firstorder Markov independence assumption is used); gi are observation feature functions; Aj and µi are their corresponding weights. To train the model for this step, we use the best reference compression to obtain the reference labels (as described in Section 2). In the CRF compression model, each word is represented by a </context>
<context position="12277" citStr="Liu and Liu, 2010" startWordPosition="2048" endWordPosition="2051">nce compression appears in 25- best list, we just keep the entire list and label the instance that is most similar to the best reference compression as positive. 4 Experiments We perform a cross-validation evaluation where one meeting is used for testing and the rest of them are used as the training set. When evaluating the system performance, we do not consider filled pauses and incomplete words since they can be easily identified and removed. We use two different performance metrics in this study. • Word-level accuracy and F1 score based on the minor class (removed words). This was used in (Liu and Liu, 2010). These measures are obtained by comparing with the best compression. In evaluation we map the result using ‘BIO’ labels from the first-step compression to binary labels that indicate a word is removed or not. 168 • BLEU score. BLEU is a widely used metric in evaluating machine translation systems that often use multiple references. Since there is a great variation in human compression results, and we have 8 reference compressions, we explore using BLEU for our sentence compression task. BLEU is calculated based on the precision of n-grams. In our experiments we use up to 4-grams. Table 1 show</context>
<context position="14159" citStr="Liu and Liu, 2010" startWordPosition="2367" endWordPosition="2370">BIO” label set from the first step of compression. The last two rows show the results after reranking, trained using one best reference or 8 reference compressions, respectively. accuracy F1 BLEU ratio (%) reference 81.96 69.73 95.36 76.78 basic features (Liu 76.44 62.11 91.08 73.49 and Liu, 2010) basic features, BIO 77.10 63.34 91.41 73.22 expanded features 79.28 67.37 92.70 72.17 reranking 79.01 67.74 91.90 70.60 train w/ 1 ref reranking 78.78 63.76 94.21 77.15 train w/ 8 refs Table 1: Compression results using different systems. Our result using the basic feature set is similar to that in (Liu and Liu, 2010) (their accuracy is 76.27% when compression ratio is 0.7), though the experimental setups are different: they used 6 meetings as the test set while we performed cross validation. Using the “BIO” label set instead of binary labels has marginal improvement for the three scores. From the table, we can see that our expanded feature set is able to significantly improve the result, suggesting the effectiveness of the new introduced features. Regarding the two training settings in reranking, we find that there is no gain from reranking when using only one best compression, however, training with mult</context>
</contexts>
<marker>Liu, Liu, 2010</marker>
<rawString>Fei Liu and Yang Liu. 2010. Using spoken utterance compression for meeting summarization: a pilot study. In Proceedings of SLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Steve Renals</author>
<author>Jean Carletta</author>
</authors>
<title>Extractive summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proceedings of EUROSPEECH.</booktitle>
<contexts>
<context position="4251" citStr="Murray et al., 2005" startWordPosition="667" endWordPosition="670">king to incorporate global information about the compressed sentence candidates, which cannot be accomplished by word level labeling. • We evaluate our methods using different metrics including word-level accuracy and F1- measure by comparing to one reference compression, and BLEU scores comparing with multiple references. We also demonstrate that training in the reranking module can be tailed to the evaluation metrics to optimize system performance. 2 Corpus We use the same corpus as (Liu and Liu, 2010) where they annotated 2,860 summary sentences in 26 meetings from the ICSI meeting corpus (Murray et al., 2005). In their annotation procedure, filled pauses such as “uh/um” and incomplete words are removed before annotation. In the first step, 8 annotators were asked to select words to be removed to compress the sentences. In the second step, 6 annotators (different from the first step) were asked to pick the best one from the 8 compressions from the previous step. Therefore for each sentence, we have 8 human compressions, as well a best one selected by the majority of the 6 annotators in the second step. The compression ratio of the best human reference is 63.64%. In the first step of our sentence co</context>
</contexts>
<marker>Murray, Renals, Carletta, 2005</marker>
<rawString>Gabriel Murray, Steve Renals, and Jean Carletta. 2005. Extractive summarization of meeting recordings. In Proceedings of EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Evaluating Sentence Compression: Pitfalls and Suggested Remedies.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>91--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<marker>Napoles, Van Durme, Callison-Burch, 2011</marker>
<rawString>Courtney Napoles, Benjamin Van Durme, and Chris Callison-Burch. 2011. Evaluating Sentence Compression: Pitfalls and Suggested Remedies. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 91–97, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>A Stolcke</author>
<author>Jing Zheng</author>
</authors>
<title>Reranking machine translation hypotheses with structured and web-based language models.</title>
<date>2007</date>
<booktitle>In Proceedings of IEEE Workshop on Speech Recognition and Understanding,</booktitle>
<pages>159--164</pages>
<contexts>
<context position="9397" citStr="Wang et al., 2007" startWordPosition="1547" endWordPosition="1550">se probabilities are obtained from the Google Web 1T 5-gram. • transition features: a combination of the current output label and the previous one, together with some observation features such as the unigram and bigrams of word or POS tag. 3.2 Discriminative Reranking Although CRFs is able to model the dependency of adjacent labels, it does not measure the quality of the whole sentence. In this work, we propose to use discriminative training to rerank the candidates generated in the first step. Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al., 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). We use a maximum Entropy reranker to learn distributions over a set of candidates such that the probability of the best compression is maximized. The conditional probability of output y given observation x in the maximum entropy model is defined as: p(y |x) = Z(X) exp [� 1 Ad (x, y)] where f(x, y) are feature functions and Ai are their weighting parameters; Z(x) is the normalization factor. In this reranking model, every compression candidate is represented by the following features: • All the bigrams </context>
</contexts>
<marker>Wang, Stolcke, Zheng, 2007</marker>
<rawString>Wen Wang, A. Stolcke, and Jing Zheng. 2007. Reranking machine translation hypotheses with structured and web-based language models. In Proceedings of IEEE Workshop on Speech Recognition and Understanding, pages 159–164, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Zwarts</author>
<author>Mark Johnson</author>
</authors>
<title>The impact of language models and loss functions on repair disfluency detection.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9488" citStr="Zwarts and Johnson, 2011" startWordPosition="1560" endWordPosition="1563">a combination of the current output label and the previous one, together with some observation features such as the unigram and bigrams of word or POS tag. 3.2 Discriminative Reranking Although CRFs is able to model the dependency of adjacent labels, it does not measure the quality of the whole sentence. In this work, we propose to use discriminative training to rerank the candidates generated in the first step. Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al., 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). We use a maximum Entropy reranker to learn distributions over a set of candidates such that the probability of the best compression is maximized. The conditional probability of output y given observation x in the maximum entropy model is defined as: p(y |x) = Z(X) exp [� 1 Ad (x, y)] where f(x, y) are feature functions and Ai are their weighting parameters; Z(x) is the normalization factor. In this reranking model, every compression candidate is represented by the following features: • All the bigrams and trigrams of words and POS tags in the candidate sentence. • Bigrams and trigrams of wor</context>
</contexts>
<marker>Zwarts, Johnson, 2011</marker>
<rawString>Simon Zwarts and Mark Johnson. 2011. The impact of language models and loss functions on repair disfluency detection. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>