<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000936">
<title confidence="0.951505">
Disfluency Detection Using Multi-step Stacked Learning
</title>
<author confidence="0.994921">
Xian Qian and Yang Liu
</author>
<affiliation confidence="0.995355">
Computer Science Department
The University of Texas at Dallas
</affiliation>
<email confidence="0.998826">
{qx,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.996559" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985554238095238">
In this paper, we propose a multi-step stacked
learning model for disfluency detection. Our
method incorporates refined n-gram features
step by step from different word sequences.
First, we detect filler words. Second, edited
words are detected using n-gram features ex-
tracted from both the original text and filler fil-
tered text. In the third step, additional n-gram
features are extracted from edit removed texts
together with our newly induced in-between
features to improve edited word detection. We
use Max-Margin Markov Networks (M3Ns) as
the classifier with the weighted hamming loss
to balance precision and recall. Experiments
on the Switchboard corpus show that the re-
fined n-gram features from multiple steps and
M3Ns with weighted hamming loss can signif-
icantly improve the performance. Our method
for disfluency detection achieves the best re-
ported F-score 0.841 without the use of addi-
tional resources.)
</bodyText>
<sectionHeader confidence="0.998855" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.969048153846154">
Detecting disfluencies in spontaneous speech can
be used to clean up speech transcripts, which help-
s improve readability of the transcripts and make it
easy for downstream language processing modules.
There are two types of disfluencies: filler words in-
cluding filled pauses (e.g., ‘uh’, ‘um’) and discourse
markers (e.g., ‘I mean’, ‘you know’), and edited
words that are repeated, discarded, or corrected by
&apos;Our source code is available at
http://code.google.com/p/disfluency-detection/downloads/list
the following words. An example is shown below
that includes edited words and filler words.
I want a flight to Boston
</bodyText>
<equation confidence="0.8747485">
� Y �
edited
</equation>
<bodyText confidence="0.999938166666667">
Automatic filler word detection is much more ac-
curate than edit detection as they are often fixed
phrases (e.g., “uh”, “you know”, “I mean”), hence
our work focuses on edited word detection.
Many models have been evaluated for this task.
Liu et al. (2006) used Conditional Random Fields
(CRFs) for sentence boundary and edited word de-
tection. They showed that CRFs significantly out-
performed Maximum Entropy models and HMM-
s. Johnson and Charniak (2004) proposed a TAG-
based noisy channel model which showed great im-
provement over boosting based classifier (Charniak
and Johnson, 2001). Zwarts and Johnson (2011)
extended this model using minimal expected F-loss
oriented n-best reranking. They obtained the best re-
ported F-score of 83.8% on the Switchboard corpus.
Georgila (2009) presented a post-processing method
during testing based on Integer Linear Programming
(ILP) to incorporate local and global constraints.
From the view of features, in addition to tex-
tual information, prosodic features extracted from
speech have been incorporated to detect edited
words in some previous work (Kahn et al., 2005;
Zhang et al., 2006; Liu et al., 2006). Zwarts and
Johnson (2011) trained an extra language model on
additional corpora, and used output log probabili-
ties of language models as features in the reranking
stage. They reported that the language model gained
about absolute 3% F-score for edited word detection
on the Switchboard development dataset.
</bodyText>
<figure confidence="0.741429333333333">
uh I mean to Denver
Y
filler
</figure>
<page confidence="0.862476">
820
</page>
<subsectionHeader confidence="0.271664">
Proceedings of NAACL-HLT 2013, pages 820–825,
</subsectionHeader>
<bodyText confidence="0.978877529411765">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
In this paper, we propose a multi-step stacked
learning approach for disfluency detection. In our
method, we first perform filler word detection, then
edited word detection. In every step, we generate
new refined n-gram features based on the processed
text (remove the detected filler or edited words from
the previous step), and use these in the next step.
We also include a new type of features, called in-
between features, and incorporate them into the last
step. For edited word detection, we use Max-Margin
Markov Networks (M3Ns) with weighted hamming
loss as the classifier, as it can well balance the pre-
cision and recall to achieve high performance. On
the commonly used Switchboard corpus, we demon-
strate that our proposed method outperforms other
state-of-the-art systems for edit disfluency detection.
</bodyText>
<subsectionHeader confidence="0.4937575">
2 Balancing Precision and Recall Using
Weighted M3Ns
</subsectionHeader>
<bodyText confidence="0.999216833333333">
We use a sequence labeling model for edit detection.
Each word is assigned one of the five labels: BE (be-
ginning of the multi-word edited region), IE (in the
edited region), EE (end of the edited region), SE (s-
ingle word edited region), O (other). For example,
the previous sentence is represented as:
I/O want/O a/O flight/O to/BE Boston/EE uh/O
I/O mean/O to/O Denver/O
We use the F-score as the evaluation metrics
(Zwarts and Johnson, 2011; Johnson and Charniak,
2004), which is defined as the harmonic mean of the
precision and recall of the edited words:
</bodyText>
<equation confidence="0.984617142857143">
P = #correctly predicted edited words
#predicted edited words
R = #correctly predicted edited words
#gold standard edited words
2 × P × R
F=
P + R
</equation>
<bodyText confidence="0.999668277777778">
There are many methods to train the sequence mod-
el, such as CRFs (Lafferty et al., 2001), averaged
structured perceptrons (Collins, 2002), structured
SVM (Altun et al., 2003), online passive aggressive
learning (Crammer et al., 2006). Previous work has
shown that minimizing F-loss is more effective than
minimizing log-loss (Zwarts and Johnson, 2011),
because edited words are much fewer than normal
words.
In this paper, we use Max-margin Markov Net-
works (Taskar et al., 2004) because our preliminary
results showed that they outperform other classifier-
s, and using weighted hamming loss is simple in this
approach (whereas for perceptron or CRFs, the mod-
ification of the objective function is not straightfor-
ward).
The learning task for M3Ns can be represented as
follows:
</bodyText>
<equation confidence="0.9853164">
αx,y∆�(x, y)∥� 2 + � αx,yL(x, y)
x,y
��.�. αx,y = 1 ∀x
y
αx,y ≥ 0, ∀x, y
</equation>
<bodyText confidence="0.924779333333333">
The above shows the dual form for training M3Ns,
where x is the observation of a training sample,
y E Y is a label. α is the parameter needed
to be optimized, C &gt; 0 is the regularization pa-
rameter. ∆f(x, y) is the residual feature vector:
f(x, ˜y) − f(x, y), where y˜ is the true label of x.
L(x, y) is the loss function. Taskar et al. (2004) used
un-weighted hamming loss, which is the number
of incorrect components: L(x, y) = Et 6(yt, ˜yt),
where 6(a, b) is the binary indicator function (it is 0
if a = b). In our work, we use the weighted ham-
ming loss:
</bodyText>
<equation confidence="0.997578">
L(x, y) = � v(yt, ˜yt)6(yt, ˜yt)
t
</equation>
<bodyText confidence="0.9978769">
where v(yt, ˜yt) is the weighted loss for the error
when ˜yt is mislabeled as yt. Such a weighted loss
function allows us to balance the model’s precision
and recall rates. For example, if we assign a large
value to v(O, ·E) (·E denotes SE, BE, IE, EE), then
the classifier is more sensitive to false negative er-
rors (edited word misclassified as non-edited word),
thus we can improve the recall rate. In our work,
we tune the weight matrix v using the development
dataset.
</bodyText>
<sectionHeader confidence="0.9478805" genericHeader="method">
3 Multi-step Stacked Learning for Edit
Disfluency Detection
</sectionHeader>
<bodyText confidence="0.9992228">
Rather than just using the above M3Ns with some
features, in this paper we propose to use stacked
learning to incorporate gradually refined n-gram fea-
tures. Stacked learning is a meta-learning approach
(Cohen and de Carvalho, 2005). Its idea is to use two
</bodyText>
<figure confidence="0.8922696">
1 �
2C∥
x,y
min
α
</figure>
<page confidence="0.985759">
821
</page>
<bodyText confidence="0.999985090909091">
(or more) levels of predictors, where the outputs of
the low level predictors are incorporated as features
into the next level predictors. It has the advantage
of incorporating non-local features as well as non-
linear classifiers. In our task, we do not just use the
classifier’s output (a word is an edited word or not)
as a feature, rather we use such output to remove the
disfluencies and extract new n-gram features for the
subsequent stacked classifiers. We use 10 fold cross
validation to train the low level predictors. The fol-
lowing describes the three steps in our approach.
</bodyText>
<subsectionHeader confidence="0.997109">
3.1 Step 1: Filler Word Detection
</subsectionHeader>
<bodyText confidence="0.99998219047619">
In the first step, we automatically detect filler word-
s. Since filler words often occur immediately after
edited words (before the corrected words), we ex-
pect that removing them will make rough copy de-
tection easy. For example, in the previous example
shown in Section 1, if “uh I mean” is removed, then
the reparandum “to Boston” and repair “to Denver”
will be adjacent and we can use word/POS based n-
gram features to detect that disfluency. Otherwise,
the classifier needs to skip possible filler words to
find the rough copy of the reparandum.
For filler word detection, similar to edited word
detection, we define 5 labels: BP, IP, EP, 5P, O.
We use un-weighted hamming loss to learn M3Ns
for this task. Since for filler word detection, our per-
formance metric is not F-measure, but just the over-
all accuracy in order to generate cleaned text for sub-
sequent n-gram features, we did not use the weight-
ed hamming hoss for this. The features we used are
listed in Table 1. All n-grams are extracted from the
original text.
</bodyText>
<subsectionHeader confidence="0.997113">
3.2 Step 2: Edited Word Detection
</subsectionHeader>
<bodyText confidence="0.999992625">
In the second step, edited words are detected using
M3Ns with the weighted-hamming loss. The fea-
tures we used are listed in Table 2. All n-grams in
the first step are also used here. Besides that, word
n-grams, POS n-grams and logic n-grams extracted
from filler word removed text are included. Feature
templates I(w0, w′�) is to generate features detecting
rough copies separated by filler words.
</bodyText>
<subsectionHeader confidence="0.990121">
3.3 Step 3: Refined Edited Word Detection
</subsectionHeader>
<bodyText confidence="0.98841">
In this step, we use n-gram features extracted from
the text after removing edit disfluencies based on
</bodyText>
<table confidence="0.9992148">
unigrams w0, w−1, w1, w−2, w2
p0, p−1, p1, p−2, p2, w0p0
bigrams w−1w0, w0w1, p−1p0, p0p1
trigrams p−2p−1p0, p−1p0p1, p0p1p2
logic unigrams I(wi, w0), I(pi, p0), −4 &lt; i &lt; 4
logic bigrams I(wi−1wi, w−1, w0)
I(pi−1pi, p−1p0)
I(wiwi+1, w0w1)
I(pipi+1,p0p1), −4 &lt; i &lt; 4
transitions y−1y0
</table>
<tableCaption confidence="0.998938">
Table 1: Feature templates for filler word detection.
</tableCaption>
<bodyText confidence="0.9660686">
w0, p0 denote the current word and POS tag respective-
ly. w−i denotes the ith word to the left, wi denotes the
ith word to the right. The logic function I(a, b) indicates
whether a and b are identical (eigher unigrams or bigram-
s).
</bodyText>
<table confidence="0.992002">
All templates in Table 1
unigrams w′1, w′2, w′3, w′4
bigrams p0p′1, p0p′2, p0p′3, p0p′4
w0p′1, w0p′2, w0p′3, w0p′4
w0p1, w0p2, w0p3, w0p4
logic unigrams I(w0, w′i), 1 &lt; i &lt; 4
transitions p0y−1y0
</table>
<tableCaption confidence="0.9202476">
Table 2: Feature templates for edit detection (step 2).
w′i, p′i denote the ith word/POS tag to the right in the filler
words removed text. If current word w0 is removed in
step 1, we use its original n-gram features rather than the
refined n-gram features.
</tableCaption>
<bodyText confidence="0.9998058">
the previous step. According to our analysis of the
errors produced by step 2, we observed that many
errors occurred at the boundaries of the disfluen-
cies, and the word bigrams after removing the edited
words are unnatural. The following is an example:
</bodyText>
<listItem confidence="0.99487225">
• Ref: The new type is prettier than what
their/5E they used to look like.
• Sys: The new type is prettier than what/BE
their/EE they used to look like.
</listItem>
<bodyText confidence="0.999858142857143">
Using the system’s prediction, we would have bi-
gram than they, which is odd. Usually, the pronoun
following than is accusative case. We expect adding
n-gram features derived from the cleaned-up sen-
tences would allow the new classifier to fix such hy-
pothesis. This kind of n-gram features is similar to
the language models used in (Zwarts and Johnson,
</bodyText>
<page confidence="0.994797">
822
</page>
<bodyText confidence="0.999867833333333">
2011). They have the benefit of measuring the flu-
ency of the cleaned text.
Another common error we noticed is caused by
the ambiguities of coordinates, because the coordi-
nates have similar patterns as rough copies. For ex-
ample,
</bodyText>
<listItem confidence="0.9991555">
• Coordinates: they ca n′t decide which are the
good aspects and which are the bad aspects
• Rough Copies: it/BE ′s/IE a/IE pleasure/IE
to/EE it s good to get outside
</listItem>
<bodyText confidence="0.9637625625">
To distinguish the rough copies and the coordinate
examples shown above, we analyze the training data
statistically. We extract all the pieces lying between
identical word bigrams AB ... AB. The observation
is that coordinates are often longer than edited se-
quences. Hence we introduce the in-between fea-
tures for each word. If a word lies between identical
word bigrams, then its in-between feature is the log
length of the subsequence lying between the two bi-
grams; otherwise, it is zero (we use log length to
avoid sparsity). We also used other patterns such as
A ... A and ABC ... ABC, but they are too noisy or
infrequent and do not yield much performance gain.
Table 3 lists the feature templates used in this last
step.
All templates in Table 1, Table 2
</bodyText>
<tableCaption confidence="0.96972">
Table 3: Feature templates for refined edit detection (step
3). w′′
</tableCaption>
<bodyText confidence="0.9996932">
i denotes the ith word tag to the right in the edit-
ed word removed text. LAB denotes the log length of
the sub-sequence in the pattern AB... AB, bAB indicates
whether the current word lies between two identical bi-
grams.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998619">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9998275">
We use the Switchboard corpus in our experimen-
t, with the same train/develop/test split as the pre-
vious work (Johnson and Charniak, 2004). We al-
so remove the partial words and punctuation from
the training and test data for the reason to simulate
the situation when speech recognizers are used and
such kind of information is not available (Johnson
and Charniak, 2004).
We tuned the weight matrix for hamming loss on
the development dataset using simple grid search.
The diagonal elements are fixed at 0; for false pos-
itive errors, O —* •E (non-edited word mis-labeled
as edited word), their weights are fixed at 1; for false
negative errors, •E —* O, we tried the weight from
1 to 3, and increased the weight 0.5 each time. The
optimal weight matrix is shown in Table 4. Note
that we use five labels in the sequence labeling task;
however, for edited word detection evaluation, it is
only a binary task, that is, all of the words labeled
with •E will be mapped to the class of edited words.
</bodyText>
<table confidence="0.997888285714286">
predict BE IE EE SE O
truth
BE 0 1 1 1 2
IE 1 0 1 1 2
EE 1 1 0 1 2
SE 1 1 1 0 2
O 1 1 1 1 0
</table>
<tableCaption confidence="0.99979">
Table 4: Weighted hamming loss for M3Ns.
</tableCaption>
<subsectionHeader confidence="0.589671">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999988681818182">
We compare several sequence labeling models:
CRFs, structured averaged perceptron (AP), M3Ns
with un-weighted/weighted loss, and online passive-
aggressive (PA) learning. For each model, we tuned
the parameters on the development data: Gaussian
prior for CRFs is 1.0, iteration number for AP is 10,
iteration number and regularization penalty for PA
are 10 and 1. For M3Ns, we use Structured Sequen-
tial Minimal Optimization (Taskar, 2004) for model
training. Regularization penalty is C = 0.1 and iter-
ation number is 30.
Table 5 shows the results using different models
and features. The baseline models use only the n-
grams features extracted from the original text. We
can see that M3Ns with the weighted hamming loss
achieve the best performance, outperforming all the
other models. Regarding the features, the gradually
added n-gram features have consistent improvemen-
t for all models. Using the weighted hamming loss
in M3Ns, we observe a gain of 2.2% after deleting
filler words, and 1.8% after deleting edited words. In
our analysis, we also noticed that the in-between fea-
</bodyText>
<figure confidence="0.53316125">
word n-grams
w′′1, w0w′′1
in-between
LAB, w0bAB, bAB
</figure>
<page confidence="0.936569">
823
</page>
<table confidence="0.99941525">
CRF AP PA M3N w. M3N
Baseline 78.8 79.0 78.9 79.4 80.1
Step 2 81.0 81.1 81.1 81.5 82.3
Step 3 82.9 83.0 82.8 83.3 84.1
</table>
<tableCaption confidence="0.994084">
Table 5: Effect of training strategy and recovered features
</tableCaption>
<bodyText confidence="0.966377285714286">
for stacked learning. F scores are reported. AP = Aver-
aged Perceptron, PA = online Passive Aggresive, M3N =
un-weighted M3Ns, w. M3N = weighted M3Ns.
tures yield about 1% improvement in F-score for all
models (the gain of step 3 over step 2 is because of
the in-between features and the new n-gram features
extracted from the text after removing previously
detected edited words). We performed McNemar’s
test to evaluate the significance of the difference a-
mong various methods, and found that when using
the same features, weighted M3Ns significantly out-
performs all the other models (p value &lt; 0.001).
There are no significant differences among CRFs,
AP and PA. Using recovered n-gram features and in-
between features significantly improves all sequence
labeling models (p value &lt; 0.001).
We also list the state-of-the-art systems evaluat-
ed on the same dataset, as shown in Table 6. We
achieved the best F-score. The most competitive
system is (Zwarts and Johnson, 2011), which uses
extra resources to train language models.
</bodyText>
<table confidence="0.996064857142857">
System F score
(Johnson and Charniak, 2004) 79.7
(Kahn et al., 2005) 78.2
(Zhang et al., 2006)† 81.2
(Georgila, 2009)* 80.1
(Zwarts and Johnson, 2011)+ 83.8
This paper 84.1
</table>
<tableCaption confidence="0.976482">
Table 6: Comparison with other systems. † they used
the re-segmented Switchboard corpus, which is not ex-
actly the same as ours. * they reported the F-score of
BE tag (beginning of the edited sequences). + they used
language model learned from 3 additional corpora.
</tableCaption>
<sectionHeader confidence="0.998449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999888583333333">
In this paper, we proposed multi-step stacked learn-
ing to extract n-gram features step by step. The first
level removes the filler words providing new ngram-
s for the second level to remove edited words. The
third level uses the n-grams from the original tex-
t and the cleaned text generated by the previous t-
wo steps for accurate edit detection. To minimize
the F-loss approximately, we modified the hamming
loss in M3Ns. Experimental results show that our
method is effective, and achieved the best reported
performance on the Switchboard corpus without the
use of any additional resources.
</bodyText>
<sectionHeader confidence="0.998168" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999912333333333">
We thank three anonymous reviewers for their valu-
able comments. This work is partly supported by
DARPA under Contract No. HR0011-12-C-0016
and FA8750-13-2-0041. Any opinions expressed in
this material are those of the authors and do not nec-
essarily reflect the views of DARPA.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979659375">
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector ma-
chines. In Proc. of ICML.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proc. of
NAACL.
William W. Cohen and Vitor Rocha de Carvalho. 2005.
Stacked sequential learning. In Proc. of IJCAI.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, Yoram Singer, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. Journal of Ma-
chine Learning Research.
Kallirroi Georgila. 2009. Using integer linear program-
ming for detecting speech disfluencies. In Proc. of
NAACL.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In Proc.
of ACL.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
Proc. of HLT-EMNLP.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proc. of ICML.
Yang Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Osten-
dorf, and M. Harper. 2006. Enriching speech recog-
nition with automatic detection of sentence bound-
</reference>
<page confidence="0.987075">
824
</page>
<reference confidence="0.998860769230769">
aries and disfluencies. IEEE Transactions on Audio,
Speech, and Language Processing, 14(5).
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin markov networks. In Proc. of NIPS.
Ben Taskar. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stanford
University.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A pro-
gressive feature selection algorithm for ultra large fea-
ture spaces. In Proc. of ACL.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disfluen-
cy detection. In Proc. of ACL-HLT.
</reference>
<page confidence="0.99887">
825
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.698877">
<title confidence="0.998667">Disfluency Detection Using Multi-step Stacked Learning</title>
<author confidence="0.977933">Qian</author>
<affiliation confidence="0.9806365">Computer Science The University of Texas at</affiliation>
<abstract confidence="0.985511095238095">In this paper, we propose a multi-step stacked learning model for disfluency detection. Our method incorporates refined n-gram features step by step from different word sequences. First, we detect filler words. Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text. In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. We Max-Margin Markov Networks as the classifier with the weighted hamming loss to balance precision and recall. Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and with weighted hamming loss can significantly improve the performance. Our method for disfluency detection achieves the best re- F-score the use of addi-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasemin Altun</author>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
</authors>
<title>Hidden markov support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="5106" citStr="Altun et al., 2003" startWordPosition="796" endWordPosition="799">he previous sentence is represented as: I/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = #correctly predicted edited words #predicted edited words R = #correctly predicted edited words #gold standard edited words 2 × P × R F= P + R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words. In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward). The learning task for M3Ns can be represented as follows</context>
</contexts>
<marker>Altun, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Yasemin Altun, Ioannis Tsochantaridis, and Thomas Hofmann. 2003. Hidden markov support vector machines. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="2327" citStr="Charniak and Johnson, 2001" startWordPosition="350" endWordPosition="353">rds. I want a flight to Boston � Y � edited Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection. Many models have been evaluated for this task. Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection. They showed that CRFs significantly outperformed Maximum Entropy models and HMMs. Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001). Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking. They obtained the best reported F-score of 83.8% on the Switchboard corpus. Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints. From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006). Zwarts and Johnson (2011) trained an</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>Eugene Charniak and Mark Johnson. 2001. Edit detection and parsing for transcribed speech. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor Rocha de Carvalho</author>
</authors>
<title>Stacked sequential learning.</title>
<date>2005</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<marker>Cohen, de Carvalho, 2005</marker>
<rawString>William W. Cohen and Vitor Rocha de Carvalho. 2005. Stacked sequential learning. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5069" citStr="Collins, 2002" startWordPosition="792" endWordPosition="793">gion), O (other). For example, the previous sentence is represented as: I/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = #correctly predicted edited words #predicted edited words R = #correctly predicted edited words #gold standard edited words 2 × P × R F= P + R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words. In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward). The learning task f</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="5165" citStr="Crammer et al., 2006" startWordPosition="804" endWordPosition="807">ight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = #correctly predicted edited words #predicted edited words R = #correctly predicted edited words #gold standard edited words 2 × P × R F= P + R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words. In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward). The learning task for M3Ns can be represented as follows: αx,y∆�(x, y)∥� 2 + � αx,yL(x, y) x,y ��.�. αx,y = 1 ∀x y </context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, Yoram Singer, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
</authors>
<title>Using integer linear programming for detecting speech disfluencies.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="2523" citStr="Georgila (2009)" startWordPosition="381" endWordPosition="382"> on edited word detection. Many models have been evaluated for this task. Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection. They showed that CRFs significantly outperformed Maximum Entropy models and HMMs. Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001). Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking. They obtained the best reported F-score of 83.8% on the Switchboard corpus. Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints. From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006). Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage. They reported that the language model gained about absolute 3%</context>
<context position="16259" citStr="Georgila, 2009" startWordPosition="2757" endWordPosition="2758">hted M3Ns significantly outperforms all the other models (p value &lt; 0.001). There are no significant differences among CRFs, AP and PA. Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value &lt; 0.001). We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6. We achieved the best F-score. The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models. System F score (Johnson and Charniak, 2004) 79.7 (Kahn et al., 2005) 78.2 (Zhang et al., 2006)† 81.2 (Georgila, 2009)* 80.1 (Zwarts and Johnson, 2011)+ 83.8 This paper 84.1 Table 6: Comparison with other systems. † they used the re-segmented Switchboard corpus, which is not exactly the same as ours. * they reported the F-score of BE tag (beginning of the edited sequences). + they used language model learned from 3 additional corpora. 5 Conclusion In this paper, we proposed multi-step stacked learning to extract n-gram features step by step. The first level removes the filler words providing new ngrams for the second level to remove edited words. The third level uses the n-grams from the original text and the</context>
</contexts>
<marker>Georgila, 2009</marker>
<rawString>Kallirroi Georgila. 2009. Using integer linear programming for detecting speech disfluencies. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A TAGbased noisy-channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2196" citStr="Johnson and Charniak (2004)" startWordPosition="330" endWordPosition="333">e.com/p/disfluency-detection/downloads/list the following words. An example is shown below that includes edited words and filler words. I want a flight to Boston � Y � edited Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection. Many models have been evaluated for this task. Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection. They showed that CRFs significantly outperformed Maximum Entropy models and HMMs. Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001). Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking. They obtained the best reported F-score of 83.8% on the Switchboard corpus. Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints. From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect</context>
<context position="4696" citStr="Johnson and Charniak, 2004" startWordPosition="724" endWordPosition="727">ate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection. 2 Balancing Precision and Recall Using Weighted M3Ns We use a sequence labeling model for edit detection. Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other). For example, the previous sentence is represented as: I/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = #correctly predicted edited words #predicted edited words R = #correctly predicted edited words #gold standard edited words 2 × P × R F= P + R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edi</context>
<context position="12803" citStr="Johnson and Charniak, 2004" startWordPosition="2151" endWordPosition="2154">ey are too noisy or infrequent and do not yield much performance gain. Table 3 lists the feature templates used in this last step. All templates in Table 1, Table 2 Table 3: Feature templates for refined edit detection (step 3). w′′ i denotes the ith word tag to the right in the edited word removed text. LAB denotes the log length of the sub-sequence in the pattern AB... AB, bAB indicates whether the current word lies between two identical bigrams. 4 Experiments 4.1 Experimental Setup We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004). We also remove the partial words and punctuation from the training and test data for the reason to simulate the situation when speech recognizers are used and such kind of information is not available (Johnson and Charniak, 2004). We tuned the weight matrix for hamming loss on the development dataset using simple grid search. The diagonal elements are fixed at 0; for false positive errors, O —* •E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, •E —* O, we tried the weight from 1 to 3, and increased the weight 0.5 each time. The optimal </context>
<context position="16185" citStr="Johnson and Charniak, 2004" startWordPosition="2742" endWordPosition="2745">he difference among various methods, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value &lt; 0.001). There are no significant differences among CRFs, AP and PA. Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value &lt; 0.001). We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6. We achieved the best F-score. The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models. System F score (Johnson and Charniak, 2004) 79.7 (Kahn et al., 2005) 78.2 (Zhang et al., 2006)† 81.2 (Georgila, 2009)* 80.1 (Zwarts and Johnson, 2011)+ 83.8 This paper 84.1 Table 6: Comparison with other systems. † they used the re-segmented Switchboard corpus, which is not exactly the same as ours. * they reported the F-score of BE tag (beginning of the edited sequences). + they used language model learned from 3 additional corpora. 5 Conclusion In this paper, we proposed multi-step stacked learning to extract n-gram features step by step. The first level removes the filler words providing new ngrams for the second level to remove edi</context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A TAGbased noisy-channel model of speech repairs. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy G Kahn</author>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Mari Ostendorf</author>
</authors>
<title>Effective use of prosody in parsing conversational speech.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP.</booktitle>
<contexts>
<context position="2850" citStr="Kahn et al., 2005" startWordPosition="428" endWordPosition="431">del which showed great improvement over boosting based classifier (Charniak and Johnson, 2001). Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking. They obtained the best reported F-score of 83.8% on the Switchboard corpus. Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints. From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006). Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage. They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset. uh I mean to Denver Y filler 820 Proceedings of NAACL-HLT 2013, pages 820–825, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this paper, we propose a multi-step stacked learning approach for disfluency detection.</context>
<context position="16210" citStr="Kahn et al., 2005" startWordPosition="2747" endWordPosition="2750">s, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value &lt; 0.001). There are no significant differences among CRFs, AP and PA. Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value &lt; 0.001). We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6. We achieved the best F-score. The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models. System F score (Johnson and Charniak, 2004) 79.7 (Kahn et al., 2005) 78.2 (Zhang et al., 2006)† 81.2 (Georgila, 2009)* 80.1 (Zwarts and Johnson, 2011)+ 83.8 This paper 84.1 Table 6: Comparison with other systems. † they used the re-segmented Switchboard corpus, which is not exactly the same as ours. * they reported the F-score of BE tag (beginning of the edited sequences). + they used language model learned from 3 additional corpora. 5 Conclusion In this paper, we proposed multi-step stacked learning to extract n-gram features step by step. The first level removes the filler words providing new ngrams for the second level to remove edited words. The third leve</context>
</contexts>
<marker>Kahn, Lease, Charniak, Johnson, Ostendorf, 2005</marker>
<rawString>Jeremy G. Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf. 2005. Effective use of prosody in parsing conversational speech. In Proc. of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="5020" citStr="Lafferty et al., 2001" startWordPosition="785" endWordPosition="788"> EE (end of the edited region), SE (single word edited region), O (other). For example, the previous sentence is represented as: I/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = #correctly predicted edited words #predicted edited words R = #correctly predicted edited words #gold standard edited words 2 × P × R F= P + R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words. In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective func</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>M Harper</author>
</authors>
<title>Enriching speech recognition with automatic detection of sentence boundaries and disfluencies.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="1999" citStr="Liu et al. (2006)" startWordPosition="300" endWordPosition="303">es (e.g., ‘uh’, ‘um’) and discourse markers (e.g., ‘I mean’, ‘you know’), and edited words that are repeated, discarded, or corrected by &apos;Our source code is available at http://code.google.com/p/disfluency-detection/downloads/list the following words. An example is shown below that includes edited words and filler words. I want a flight to Boston � Y � edited Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection. Many models have been evaluated for this task. Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection. They showed that CRFs significantly outperformed Maximum Entropy models and HMMs. Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001). Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking. They obtained the best reported F-score of 83.8% on the Switchboard corpus. Georgila (2009) presented a post-processing method during testing based on Integer Linear P</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Hillard, Ostendorf, Harper, 2006</marker>
<rawString>Yang Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf, and M. Harper. 2006. Enriching speech recognition with automatic detection of sentence boundaries and disfluencies. IEEE Transactions on Audio, Speech, and Language Processing, 14(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin markov networks.</title>
<date>2004</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="5410" citStr="Taskar et al., 2004" startWordPosition="843" endWordPosition="846">correctly predicted edited words #predicted edited words R = #correctly predicted edited words #gold standard edited words 2 × P × R F= P + R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words. In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward). The learning task for M3Ns can be represented as follows: αx,y∆�(x, y)∥� 2 + � αx,yL(x, y) x,y ��.�. αx,y = 1 ∀x y αx,y ≥ 0, ∀x, y The above shows the dual form for training M3Ns, where x is the observation of a training sample, y E Y is a label. α is the parameter needed to be optimized, C &gt; 0 is the regularization parameter. ∆f(x, y) is the residual featur</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2004</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004. Max-margin markov networks. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
</authors>
<title>Learning Structured Prediction Models: A Large Margin Approach.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="14239" citStr="Taskar, 2004" startWordPosition="2419" endWordPosition="2420">d to the class of edited words. predict BE IE EE SE O truth BE 0 1 1 1 2 IE 1 0 1 1 2 EE 1 1 0 1 2 SE 1 1 1 0 2 O 1 1 1 1 0 Table 4: Weighted hamming loss for M3Ns. 4.2 Results We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passiveaggressive (PA) learning. For each model, we tuned the parameters on the development data: Gaussian prior for CRFs is 1.0, iteration number for AP is 10, iteration number and regularization penalty for PA are 10 and 1. For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training. Regularization penalty is C = 0.1 and iteration number is 30. Table 5 shows the results using different models and features. The baseline models use only the ngrams features extracted from the original text. We can see that M3Ns with the weighted hamming loss achieve the best performance, outperforming all the other models. Regarding the features, the gradually added n-gram features have consistent improvement for all models. Using the weighted hamming loss in M3Ns, we observe a gain of 2.2% after deleting filler words, and 1.8% after deleting edited words. In our analysis</context>
</contexts>
<marker>Taskar, 2004</marker>
<rawString>Ben Taskar. 2004. Learning Structured Prediction Models: A Large Margin Approach. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Fuliang Weng</author>
<author>Zhe Feng</author>
</authors>
<title>A progressive feature selection algorithm for ultra large feature spaces.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2870" citStr="Zhang et al., 2006" startWordPosition="432" endWordPosition="435">eat improvement over boosting based classifier (Charniak and Johnson, 2001). Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking. They obtained the best reported F-score of 83.8% on the Switchboard corpus. Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints. From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006). Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage. They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset. uh I mean to Denver Y filler 820 Proceedings of NAACL-HLT 2013, pages 820–825, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this paper, we propose a multi-step stacked learning approach for disfluency detection. In our method, we f</context>
<context position="16236" citStr="Zhang et al., 2006" startWordPosition="2752" endWordPosition="2755">ing the same features, weighted M3Ns significantly outperforms all the other models (p value &lt; 0.001). There are no significant differences among CRFs, AP and PA. Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value &lt; 0.001). We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6. We achieved the best F-score. The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models. System F score (Johnson and Charniak, 2004) 79.7 (Kahn et al., 2005) 78.2 (Zhang et al., 2006)† 81.2 (Georgila, 2009)* 80.1 (Zwarts and Johnson, 2011)+ 83.8 This paper 84.1 Table 6: Comparison with other systems. † they used the re-segmented Switchboard corpus, which is not exactly the same as ours. * they reported the F-score of BE tag (beginning of the edited sequences). + they used language model learned from 3 additional corpora. 5 Conclusion In this paper, we proposed multi-step stacked learning to extract n-gram features step by step. The first level removes the filler words providing new ngrams for the second level to remove edited words. The third level uses the n-grams from th</context>
</contexts>
<marker>Zhang, Weng, Feng, 2006</marker>
<rawString>Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A progressive feature selection algorithm for ultra large feature spaces. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Zwarts</author>
<author>Mark Johnson</author>
</authors>
<title>The impact of language models and loss functions on repair disfluency detection.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="2354" citStr="Zwarts and Johnson (2011)" startWordPosition="354" endWordPosition="357">n � Y � edited Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection. Many models have been evaluated for this task. Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection. They showed that CRFs significantly outperformed Maximum Entropy models and HMMs. Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001). Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking. They obtained the best reported F-score of 83.8% on the Switchboard corpus. Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints. From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006). Zwarts and Johnson (2011) trained an extra language model on ad</context>
<context position="4667" citStr="Zwarts and Johnson, 2011" startWordPosition="720" endWordPosition="723">hboard corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection. 2 Balancing Precision and Recall Using Weighted M3Ns We use a sequence labeling model for edit detection. Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other). For example, the previous sentence is represented as: I/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = #correctly predicted edited words #predicted edited words R = #correctly predicted edited words #gold standard edited words 2 × P × R F= P + R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts an</context>
<context position="16087" citStr="Zwarts and Johnson, 2011" startWordPosition="2727" endWordPosition="2730">previously detected edited words). We performed McNemar’s test to evaluate the significance of the difference among various methods, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value &lt; 0.001). There are no significant differences among CRFs, AP and PA. Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value &lt; 0.001). We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6. We achieved the best F-score. The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models. System F score (Johnson and Charniak, 2004) 79.7 (Kahn et al., 2005) 78.2 (Zhang et al., 2006)† 81.2 (Georgila, 2009)* 80.1 (Zwarts and Johnson, 2011)+ 83.8 This paper 84.1 Table 6: Comparison with other systems. † they used the re-segmented Switchboard corpus, which is not exactly the same as ours. * they reported the F-score of BE tag (beginning of the edited sequences). + they used language model learned from 3 additional corpora. 5 Conclusion In this paper, we proposed multi-step stacked learning to extract n-gram features step by step</context>
</contexts>
<marker>Zwarts, Johnson, 2011</marker>
<rawString>Simon Zwarts and Mark Johnson. 2011. The impact of language models and loss functions on repair disfluency detection. In Proc. of ACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>