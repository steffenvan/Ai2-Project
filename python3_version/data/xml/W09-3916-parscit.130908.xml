<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022356">
<title confidence="0.999629">
A Two-tier User Simulation Model for Reinforcement Learning of
Adaptive Referring Expression Generation Policies
</title>
<author confidence="0.998899">
Srinivasan Janarthanam Oliver Lemon
</author>
<affiliation confidence="0.9997115">
School of Informatics School of Informatics
University of Edinburgh University of Edinburgh
</affiliation>
<email confidence="0.996144">
s.janarthanam@ed.ac.uk olemon@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999722352941177">
We present a new two-tier user simula-
tion model for learning adaptive referring
expression generation (REG) policies for
spoken dialogue systems using reinforce-
ment learning. Current user simulation
models that are used for dialogue pol-
icy learning do not simulate users with
different levels of domain expertise and
are not responsive to referring expres-
sions used by the system. The two-
tier model displays these features, that
are crucial to learning an adaptive REG
policy. We also show that the two-tier
model simulates real user behaviour more
closely than other baseline models, using
the dialogue similarity measure
based on Kullback-Leibler divergence.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975">
We present a new user simulation model for
learning adaptive referring expression generation
(REG) policies for spoken dialogue systems us-
ing reinforcement learning methods. An adap-
tive REG policy equips a dialogue system to dy-
namically modify its utterances in order to adapt
to user’s domain knowledge level. For instance,
to refer to domain objects, the system might use
simple descriptive expressions with novices and
technical jargon with experts. Such adaptations
help grounding between the dialogue partners (Is-
sacs and Clark, 1987). Since the user’s knowl-
edge level is unknown, the system must be able to
adapt dynamically during the conversation. Hand-
coding such a policy could be extremely difficult.
(Janarthanam and Lemon, 2009b) have shown
that such policies can be learned using simula-
tion based reinforcement learning (RL) methods.
The quality of such learned policies is directly de-
pendent on the performance of the user simula-
tions used to train them. So far, only hand-coded
user simulations have been employed. In contrast,
we now present a data driven two-tier user sim-
ulation model trained on dialogue data collected
from real users. We also show that the two-tier
model simulates real users more faithfully than
other data driven baseline n-gram models (Eckert
et al., 1997).
In section 2 we briefly discuss other work re-
lated to user simulations for dialogue policy learn-
ing using RL. In section 3 we describe the data
used to build the simulation. Section 4 describes
the simulation models in detail. In section 5 and
6 we present the evaluation metrics used and the
results.
</bodyText>
<sectionHeader confidence="0.999408" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9998076">
Several user simulation models have been pro-
posed for dialogue management policy learning
(Schatzmann et al., 2006; Schatzmann et al.,
2007). However, these models cannot be directly
used for REG policy learning because they inter-
act with the dialogue system only using high-level
dialogue acts. Also, they do not simulate differ-
ent user groups like experts, novices, etc. In order
to learn adaptive REG policies, user simulations
need to respond to the system’s choice of referring
expressions and simulate user groups with differ-
ent knowledge levels. We propose a two-tier simu-
lation which simulates users with different knowl-
edge levels and is sensitive to the system’s choice
of referring expressions.
</bodyText>
<subsubsectionHeader confidence="0.642454">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 120–123,
</subsubsectionHeader>
<affiliation confidence="0.938265">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999267">
120
</page>
<sectionHeader confidence="0.995744" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.99996990625">
The “Wizard-of-Oz” (WOZ) methodology is a
widely accepted way of collecting dialogue data
for user simulation modeling (Whittaker et al.,
2002). In this setup, real users interact with a hu-
man wizard disguised as a dialogue system. The
wizard interprets the users responses and passes
them on to the dialogue system. The dialogue sys-
tem updates the dialogue state and decides the re-
sponses to user’s moves. The task of the partici-
pant is to interact with the dialogue system to get
instructions to setup a broadband Internet connec-
tion. The referring expression generation strategy
is chosen before the dialogue starts and stays the
same for the whole session. The strategies used
were “jargon”, “descriptive” and “tutorial”. In the
jargon strategy the system instructs the user us-
ing technical terms (e.g. “Plug the broadband
filter into the phone socket.”). In the de-
scriptive strategy, it uses descriptive terms (e.g.
“Plug the small white box into the square
white box on the wall.”). In the tutorial
strategy, the system uses both jargon and descrip-
tive terms together. The system provides clari-
fications on referring expressions when users re-
quest them. The participant’s domain knowledge
is also recorded during the task. Please refer to (Ja-
narthanam and Lemon, 2009a) for a more details
on our Wizard-of-Oz environment for data collec-
tion. The dialogues were collected from 17 par-
ticipants (one dialogue each) with around 24 to 35
turns per dialogue depending on the strategy and
user’s domain knowledge.
</bodyText>
<sectionHeader confidence="0.986344" genericHeader="method">
4 User Simulation models
</sectionHeader>
<bodyText confidence="0.999940875">
The dialogue data and knowledge profiles were
used to build user simulation models. These mod-
els take as input the system’s dialogue act As,t (at
turn t) and choice of referring expressions RECs,t
and output the user’s dialogue Au,t and environ-
ment EAu,t acts. User’s observation and manipu-
lation of the domain objects is represented by the
environment act.
</bodyText>
<subsectionHeader confidence="0.996306">
4.1 Advanced n-gram model
</subsectionHeader>
<bodyText confidence="0.996713333333333">
A simple approach to model real user behaviour
is to model user responses (dialogue act and
environment act) as advanced n-gram models
(Georgila et al., 2006) based on many context vari-
ables - all referring expressions used in the utter-
ance (RECs,t), the user’s knowledge of the REs
(DKu), history of clarification requests on the
REs (H), and the system’s dialogue act (As,t), as
defined below:
</bodyText>
<construct confidence="0.7022325">
P(Au,t|As,t, RECs,t, DKu, H)
P(EAu,t|As,t, RECs,t, DKu, H)
</construct>
<bodyText confidence="0.999830777777778">
Although this is an ideal model of the real user
data, it covers only a limited number of contexts
owing to the limited size of the corpus. Therefore,
it cannot be used for training as there may be a
large number of unseen contexts which the model
needs to respond to. For example, this model can-
not respond when the system uses a mix of jar-
gon and descriptive expressions in its utterance be-
cause such a context does not exist in our corpus.
</bodyText>
<subsectionHeader confidence="0.987684">
4.2 A Two-tier model
</subsectionHeader>
<bodyText confidence="0.999783571428571">
Instead of using a complex context model, we di-
vide the large context in to several sub-contexts
and model the user’s response based on them. We
propose a two-tier model, in which the simulation
of a user’s response is divided into two steps. First,
all the referring expressions used in the system’s
utterance are processed as below:
</bodyText>
<equation confidence="0.752543">
P(CRu,t|REs,t, DKRE,u, HRE, As,t)
</equation>
<bodyText confidence="0.95897636">
This step is repeated for each expression REs,t
separately. The above model returns a clarifi-
cation request based on the referring expression
REs,t used, the user’s knowledge of the expres-
sion DKRE,u, and previous clarification requests
on the expression HRE and the system dialogue
act As,t. A clarification request is highly likely in
case of the jargon strategy and less likely in other
strategies. Also, if a clarification has already been
issued, the user is less likely to issue another re-
quest for clarification. In such cases, the clarifica-
tion request model simply returns none.
In the next step, the model returns a user di-
alogue act Au,t and an environment act EAu,t
based on the system dialogue act As,t and the clar-
ification request CRu,t, as follows:
P(Au,t|As,t, CRu,t)
P(EAu,t|As,t, CRu,t)
By dividing the complex context into smaller
sub-contexts, the two-tier model simulates real
users in contexts that are not directly observed in
the dialogue data. The model will therefore re-
spond to system utterances containing a mix of
REG strategies (for example, one jargon and one
descriptive expression in the same utterance).
</bodyText>
<page confidence="0.990245">
121
</page>
<subsectionHeader confidence="0.959776">
4.3 Baseline Bigram model
</subsectionHeader>
<bodyText confidence="0.999927666666667">
A bigram model was built using the dialogue data
by conditioning the user responses only on the sys-
tem’s dialogue act (Eckert et al., 1997).
</bodyText>
<equation confidence="0.998836">
P(Au,t|As,t)
P(EAu,t|As,t)
</equation>
<bodyText confidence="0.998870333333333">
Since it ignores all the context variables except
the system dialogue act, it can be used in contexts
that are not observed in the dialogue data.
</bodyText>
<subsectionHeader confidence="0.979876">
4.4 Trigram model
</subsectionHeader>
<bodyText confidence="0.999984">
The trigram model is similar to the bigram model,
but with the previous system dialogue act As,t−1
as an additional context variable.
</bodyText>
<equation confidence="0.880937">
P(Au,t|As,t, As,t−1)
P(EAu,t|As,t, As,t−1)
</equation>
<subsectionHeader confidence="0.728504">
4.5 Equal Probability model baseline
</subsectionHeader>
<bodyText confidence="0.9999528">
The equal probability model is similar to the bi-
gram model, except that it is not trained on the
dialogue data. Instead, it assigns equal probabil-
ity to all possible responses for the given system
dialogue act.
</bodyText>
<subsectionHeader confidence="0.9905">
4.6 Smoothing
</subsectionHeader>
<bodyText confidence="0.928141727272727">
We used Witten-Bell discounting to smooth all
our models except the equal probability model,
in order to account for unobserved but possible
responses in dialogue contexts. Witten-Bell dis-
counting extracts a small percentage of probability
mass, i.e. number of distinct responses observed
for the first time (T) in a context, out of the to-
tal number of instances (N), and redistributes this
mass to unobserved responses in the given context
(V − T) (where V is the number of all possible
responses) . The discounted probabilities P* of
</bodyText>
<equation confidence="0.970971666666667">
observed responses (C(ei) &gt; 0) and unobserved
responses (C(ei) = 0) are given below.
P*(ei) = C(e�)
N+T if(C(ei) &gt; 0)
(N+T )(V −T) if(C(ei) = 0)
t
</equation>
<bodyText confidence="0.999980590909091">
On analysis, we found that the Witten-Bell
discounting assigns greater probability to unob-
served responses than to observed responses, in
cases where the number of responses per con-
text is very low. For instance, in a partic-
ular context, the possible responses, their fre-
quencies and their original probabilities were -
provide info (3, 0.75), other (1, 0.25),
request clarification (0, 0). After dis-
counting, the revised probabilities P* are 0.5,
0.167 and 0.33. request clarification
gets the whole share of extracted probability as
it is the only unobserved response in the context
and is more than the other responses actually
observed in the data. This is counter-intuitive for
our application. Therefore, we use a modified ver-
sion of Witten-Bell discounting (given below) to
smooth our models, where the extracted proba-
bility is equally divided amongst all possible re-
sponses. Using the modified version, the revised
probabilities for the illustrated example are 0.61,
0.28 and 0.11 respectively.
</bodyText>
<equation confidence="0.985773">
P*(ei) = C(e�)
N+T + T
(N+T)V
</equation>
<sectionHeader confidence="0.972628" genericHeader="method">
5 Metrics for evaluation of simulations
</sectionHeader>
<bodyText confidence="0.9999276">
While there are many proposed measures to rank
user simulation models with respect to real user
data (Schatzmann et al., 2005; Georgila et al.,
2006; Rieser and Lemon, 2006a; Williams, 2008),
we use the Dialogue Similarity measure
based on Kullback-Leibler (KL) (Cuayahuitl et
al., 2005; Cuayahuitl, 2009) divergence to mea-
sure how similar the probability distributions of
the simulation models are to the original real hu-
man data.
</bodyText>
<subsectionHeader confidence="0.996679">
5.1 Dialogue Similarity
</subsectionHeader>
<bodyText confidence="0.9998696">
Dialogue Similarity is a measure of divergence be-
tween real and simulated dialogues and can mea-
sure how similar a model is to real data. The mea-
sure is based on Kullback-Leibler (KL) divergence
and is defined as follows:
</bodyText>
<equation confidence="0.96023125">
DS(P||Q) = 1 N DKL(P||Q)+DKL(Q||P)
N �i=1 2
DKL(P||Q) = EMi=1 pi * log(p�
q� )
</equation>
<bodyText confidence="0.9997442">
The metric measures the divergence between
distributions P and Q in N different contexts
with M responses per context. Ideally, the dia-
logue similarity between two similar distributions
is close to zero.
</bodyText>
<sectionHeader confidence="0.982762" genericHeader="evaluation">
6 Evaluation results
</sectionHeader>
<bodyText confidence="0.999938166666667">
We consider the Advanced N-gram model to be
a realistic model of the real human dialogue cor-
pus, as it takes into account all context variables
and is reasonably smoothed to account for unob-
served user responses. Therefore, we compare the
probability distributions of all the other models to
</bodyText>
<equation confidence="0.911108">
P*(ei) =
</equation>
<page confidence="0.992337">
122
</page>
<table confidence="0.9992332">
Model Au,t EAu,t
Two-tier 0.078 0.018
Bigram 0.150 0.139
Trigram 0.145 0.158
Equal Probability 0.445 0.047
</table>
<tableCaption confidence="0.991735">
Table 1: Dialogue Similarity with Modified
</tableCaption>
<bodyText confidence="0.9789909375">
Witten-Bell discounting w.r.t Advanced N-gram
model
the advanced n-gram model using the dialogue
similarity measure. The results of the evalu-
ation are given in table 1.
The results show that the two-tier model is
much closer (0.078, 0.018) to the Advanced N-
gram model than the other models. This is due to
the fact that the bigram and trigram models don’t
take into account factors like the user’s knowl-
edge, the strategy used, and the dialogue history.
By effectively dividing the RE processing and the
environment interaction, the two-tier simulation
model is not only realistic in observed contexts but
also usable in unobserved contexts (unlike the Ad-
vanced N-gram model).
</bodyText>
<sectionHeader confidence="0.998376" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99997">
We have presented a data driven user simulation
model called the two-tier model for learning REG
policies using reinforcement learning. We have
also shown that the two-tier model is much closer
to real user data than the other baseline models.
We will now train REG policies using the two-tier
model and test them on real users in the future.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999913375">
The research leading to these results has re-
ceived funding from the EPSRC (project no.
EP/E019501/1) and from the European Commu-
nity’s Seventh Framework Programme (FP7/2007-
2013) under grant agreement no. 216594 (CLAS-
SiC project www.classic-project.org),
and from the British Council’s UKERI pro-
gramme.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999147361702128">
H. Cuayahuitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2005. Human-Computer Dialogue Sim-
ulation Using Hidden Markov Models. In Proc. of
ASRU 2005.
H. Cuayahuitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. the-
sis, University of Edinburgh, UK.
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
Modeling for Spoken Dialogue System Evaluation.
In Proc. of ASRU97.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
Simulation for Spoken Dialogue System: Learning
and Evaluation. In Proc of ICSLP 2006.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26–37.
S. Janarthanam and O. Lemon. 2009a. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG’09.
S. Janarthanam and O. Lemon. 2009b. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG’09.
V. Rieser and O. Lemon. 2006a. Cluster-based User
Simulations for Learning Dialogue Strategies. In
Proc. Interspeech/ICSLP.
J. Schatzmann, K. Georgila, and S. J. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. SIG-
dial workshop on Discourse and Dialogue ’05.
J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97–126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.
S. Whittaker, M. Walker, and J. Moore. 2002. Fish
or Fowl: A Wizard of Oz Evaluation of Dialogue
Strategies in the Restaurant Domain. In Language
Resources and Evaluation Conference.
J. Williams. 2008. Evaluating User Simulations with
the Cramer-von Mises Divergence. Speech Commu-
nication, 50:829–846.
</reference>
<page confidence="0.998946">
123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945215">
<title confidence="0.9995435">A Two-tier User Simulation Model for Reinforcement Learning Adaptive Referring Expression Generation Policies</title>
<author confidence="0.999658">Srinivasan Janarthanam Oliver Lemon</author>
<affiliation confidence="0.9998685">School of Informatics School of Informatics University of Edinburgh University of Edinburgh</affiliation>
<email confidence="0.981107">s.janarthanam@ed.ac.ukolemon@inf.ed.ac.uk</email>
<abstract confidence="0.998025111111111">We present a new two-tier user simulation model for learning adaptive referring expression generation (REG) policies for spoken dialogue systems using reinforcement learning. Current user simulation models that are used for dialogue policy learning do not simulate users with different levels of domain expertise and are not responsive to referring expressions used by the system. The twotier model displays these features, that are crucial to learning an adaptive REG policy. We also show that the two-tier model simulates real user behaviour more closely than other baseline models, using similarity based on Kullback-Leibler divergence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Cuayahuitl</author>
<author>S Renals</author>
<author>O Lemon</author>
<author>H Shimodaira</author>
</authors>
<title>Human-Computer Dialogue Simulation Using Hidden Markov Models.</title>
<date>2005</date>
<booktitle>In Proc. of ASRU</booktitle>
<contexts>
<context position="10718" citStr="Cuayahuitl et al., 2005" startWordPosition="1720" endWordPosition="1723"> of Witten-Bell discounting (given below) to smooth our models, where the extracted probability is equally divided amongst all possible responses. Using the modified version, the revised probabilities for the illustrated example are 0.61, 0.28 and 0.11 respectively. P*(ei) = C(e�) N+T + T (N+T)V 5 Metrics for evaluation of simulations While there are many proposed measures to rank user simulation models with respect to real user data (Schatzmann et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2006a; Williams, 2008), we use the Dialogue Similarity measure based on Kullback-Leibler (KL) (Cuayahuitl et al., 2005; Cuayahuitl, 2009) divergence to measure how similar the probability distributions of the simulation models are to the original real human data. 5.1 Dialogue Similarity Dialogue Similarity is a measure of divergence between real and simulated dialogues and can measure how similar a model is to real data. The measure is based on Kullback-Leibler (KL) divergence and is defined as follows: DS(P||Q) = 1 N DKL(P||Q)+DKL(Q||P) N �i=1 2 DKL(P||Q) = EMi=1 pi * log(p� q� ) The metric measures the divergence between distributions P and Q in N different contexts with M responses per context. Ideally, th</context>
</contexts>
<marker>Cuayahuitl, Renals, Lemon, Shimodaira, 2005</marker>
<rawString>H. Cuayahuitl, S. Renals, O. Lemon, and H. Shimodaira. 2005. Human-Computer Dialogue Simulation Using Hidden Markov Models. In Proc. of ASRU 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cuayahuitl</author>
</authors>
<title>Hierarchical Reinforcement Learning for Spoken Dialogue Systems.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh, UK.</institution>
<contexts>
<context position="10737" citStr="Cuayahuitl, 2009" startWordPosition="1724" endWordPosition="1725">ng (given below) to smooth our models, where the extracted probability is equally divided amongst all possible responses. Using the modified version, the revised probabilities for the illustrated example are 0.61, 0.28 and 0.11 respectively. P*(ei) = C(e�) N+T + T (N+T)V 5 Metrics for evaluation of simulations While there are many proposed measures to rank user simulation models with respect to real user data (Schatzmann et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2006a; Williams, 2008), we use the Dialogue Similarity measure based on Kullback-Leibler (KL) (Cuayahuitl et al., 2005; Cuayahuitl, 2009) divergence to measure how similar the probability distributions of the simulation models are to the original real human data. 5.1 Dialogue Similarity Dialogue Similarity is a measure of divergence between real and simulated dialogues and can measure how similar a model is to real data. The measure is based on Kullback-Leibler (KL) divergence and is defined as follows: DS(P||Q) = 1 N DKL(P||Q)+DKL(Q||P) N �i=1 2 DKL(P||Q) = EMi=1 pi * log(p� q� ) The metric measures the divergence between distributions P and Q in N different contexts with M responses per context. Ideally, the dialogue similari</context>
</contexts>
<marker>Cuayahuitl, 2009</marker>
<rawString>H. Cuayahuitl. 2009. Hierarchical Reinforcement Learning for Spoken Dialogue Systems. Ph.D. thesis, University of Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Eckert</author>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>User Modeling for Spoken Dialogue System Evaluation.</title>
<date>1997</date>
<booktitle>In Proc. of ASRU97.</booktitle>
<contexts>
<context position="2265" citStr="Eckert et al., 1997" startWordPosition="337" endWordPosition="340">licy could be extremely difficult. (Janarthanam and Lemon, 2009b) have shown that such policies can be learned using simulation based reinforcement learning (RL) methods. The quality of such learned policies is directly dependent on the performance of the user simulations used to train them. So far, only hand-coded user simulations have been employed. In contrast, we now present a data driven two-tier user simulation model trained on dialogue data collected from real users. We also show that the two-tier model simulates real users more faithfully than other data driven baseline n-gram models (Eckert et al., 1997). In section 2 we briefly discuss other work related to user simulations for dialogue policy learning using RL. In section 3 we describe the data used to build the simulation. Section 4 describes the simulation models in detail. In section 5 and 6 we present the evaluation metrics used and the results. 2 Related work Several user simulation models have been proposed for dialogue management policy learning (Schatzmann et al., 2006; Schatzmann et al., 2007). However, these models cannot be directly used for REG policy learning because they interact with the dialogue system only using high-level </context>
<context position="8044" citStr="Eckert et al., 1997" startWordPosition="1288" endWordPosition="1291">he system dialogue act As,t and the clarification request CRu,t, as follows: P(Au,t|As,t, CRu,t) P(EAu,t|As,t, CRu,t) By dividing the complex context into smaller sub-contexts, the two-tier model simulates real users in contexts that are not directly observed in the dialogue data. The model will therefore respond to system utterances containing a mix of REG strategies (for example, one jargon and one descriptive expression in the same utterance). 121 4.3 Baseline Bigram model A bigram model was built using the dialogue data by conditioning the user responses only on the system’s dialogue act (Eckert et al., 1997). P(Au,t|As,t) P(EAu,t|As,t) Since it ignores all the context variables except the system dialogue act, it can be used in contexts that are not observed in the dialogue data. 4.4 Trigram model The trigram model is similar to the bigram model, but with the previous system dialogue act As,t−1 as an additional context variable. P(Au,t|As,t, As,t−1) P(EAu,t|As,t, As,t−1) 4.5 Equal Probability model baseline The equal probability model is similar to the bigram model, except that it is not trained on the dialogue data. Instead, it assigns equal probability to all possible responses for the given sys</context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>W. Eckert, E. Levin, and R. Pieraccini. 1997. User Modeling for Spoken Dialogue System Evaluation. In Proc. of ASRU97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>J Henderson</author>
<author>O Lemon</author>
</authors>
<title>User Simulation for Spoken Dialogue System: Learning and Evaluation.</title>
<date>2006</date>
<booktitle>In Proc of ICSLP</booktitle>
<contexts>
<context position="5606" citStr="Georgila et al., 2006" startWordPosition="877" endWordPosition="880">depending on the strategy and user’s domain knowledge. 4 User Simulation models The dialogue data and knowledge profiles were used to build user simulation models. These models take as input the system’s dialogue act As,t (at turn t) and choice of referring expressions RECs,t and output the user’s dialogue Au,t and environment EAu,t acts. User’s observation and manipulation of the domain objects is represented by the environment act. 4.1 Advanced n-gram model A simple approach to model real user behaviour is to model user responses (dialogue act and environment act) as advanced n-gram models (Georgila et al., 2006) based on many context variables - all referring expressions used in the utterance (RECs,t), the user’s knowledge of the REs (DKu), history of clarification requests on the REs (H), and the system’s dialogue act (As,t), as defined below: P(Au,t|As,t, RECs,t, DKu, H) P(EAu,t|As,t, RECs,t, DKu, H) Although this is an ideal model of the real user data, it covers only a limited number of contexts owing to the limited size of the corpus. Therefore, it cannot be used for training as there may be a large number of unseen contexts which the model needs to respond to. For example, this model cannot res</context>
<context position="10580" citStr="Georgila et al., 2006" startWordPosition="1700" endWordPosition="1703">n the other responses actually observed in the data. This is counter-intuitive for our application. Therefore, we use a modified version of Witten-Bell discounting (given below) to smooth our models, where the extracted probability is equally divided amongst all possible responses. Using the modified version, the revised probabilities for the illustrated example are 0.61, 0.28 and 0.11 respectively. P*(ei) = C(e�) N+T + T (N+T)V 5 Metrics for evaluation of simulations While there are many proposed measures to rank user simulation models with respect to real user data (Schatzmann et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2006a; Williams, 2008), we use the Dialogue Similarity measure based on Kullback-Leibler (KL) (Cuayahuitl et al., 2005; Cuayahuitl, 2009) divergence to measure how similar the probability distributions of the simulation models are to the original real human data. 5.1 Dialogue Similarity Dialogue Similarity is a measure of divergence between real and simulated dialogues and can measure how similar a model is to real data. The measure is based on Kullback-Leibler (KL) divergence and is defined as follows: DS(P||Q) = 1 N DKL(P||Q)+DKL(Q||P) N �i=1 2 DKL(P||Q) = EMi=1 pi * log(</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2006</marker>
<rawString>K. Georgila, J. Henderson, and O. Lemon. 2006. User Simulation for Spoken Dialogue System: Learning and Evaluation. In Proc of ICSLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Issacs</author>
<author>H H Clark</author>
</authors>
<title>References in conversations between experts and novices.</title>
<date>1987</date>
<journal>Journal of Experimental Psychology: General,</journal>
<pages>116--26</pages>
<contexts>
<context position="1508" citStr="Issacs and Clark, 1987" startWordPosition="213" endWordPosition="217">he dialogue similarity measure based on Kullback-Leibler divergence. 1 Introduction We present a new user simulation model for learning adaptive referring expression generation (REG) policies for spoken dialogue systems using reinforcement learning methods. An adaptive REG policy equips a dialogue system to dynamically modify its utterances in order to adapt to user’s domain knowledge level. For instance, to refer to domain objects, the system might use simple descriptive expressions with novices and technical jargon with experts. Such adaptations help grounding between the dialogue partners (Issacs and Clark, 1987). Since the user’s knowledge level is unknown, the system must be able to adapt dynamically during the conversation. Handcoding such a policy could be extremely difficult. (Janarthanam and Lemon, 2009b) have shown that such policies can be learned using simulation based reinforcement learning (RL) methods. The quality of such learned policies is directly dependent on the performance of the user simulations used to train them. So far, only hand-coded user simulations have been employed. In contrast, we now present a data driven two-tier user simulation model trained on dialogue data collected f</context>
</contexts>
<marker>Issacs, Clark, 1987</marker>
<rawString>E. A. Issacs and H. H. Clark. 1987. References in conversations between experts and novices. Journal of Experimental Psychology: General, 116:26–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>A Wizard-ofOz environment to study Referring Expression Generation in a Situated Spoken Dialogue Task. In</title>
<date>2009</date>
<booktitle>Proc. ENLG’09.</booktitle>
<contexts>
<context position="1708" citStr="Janarthanam and Lemon, 2009" startWordPosition="246" endWordPosition="249">spoken dialogue systems using reinforcement learning methods. An adaptive REG policy equips a dialogue system to dynamically modify its utterances in order to adapt to user’s domain knowledge level. For instance, to refer to domain objects, the system might use simple descriptive expressions with novices and technical jargon with experts. Such adaptations help grounding between the dialogue partners (Issacs and Clark, 1987). Since the user’s knowledge level is unknown, the system must be able to adapt dynamically during the conversation. Handcoding such a policy could be extremely difficult. (Janarthanam and Lemon, 2009b) have shown that such policies can be learned using simulation based reinforcement learning (RL) methods. The quality of such learned policies is directly dependent on the performance of the user simulations used to train them. So far, only hand-coded user simulations have been employed. In contrast, we now present a data driven two-tier user simulation model trained on dialogue data collected from real users. We also show that the two-tier model simulates real users more faithfully than other data driven baseline n-gram models (Eckert et al., 1997). In section 2 we briefly discuss other wor</context>
<context position="4799" citStr="Janarthanam and Lemon, 2009" startWordPosition="744" endWordPosition="748">whole session. The strategies used were “jargon”, “descriptive” and “tutorial”. In the jargon strategy the system instructs the user using technical terms (e.g. “Plug the broadband filter into the phone socket.”). In the descriptive strategy, it uses descriptive terms (e.g. “Plug the small white box into the square white box on the wall.”). In the tutorial strategy, the system uses both jargon and descriptive terms together. The system provides clarifications on referring expressions when users request them. The participant’s domain knowledge is also recorded during the task. Please refer to (Janarthanam and Lemon, 2009a) for a more details on our Wizard-of-Oz environment for data collection. The dialogues were collected from 17 participants (one dialogue each) with around 24 to 35 turns per dialogue depending on the strategy and user’s domain knowledge. 4 User Simulation models The dialogue data and knowledge profiles were used to build user simulation models. These models take as input the system’s dialogue act As,t (at turn t) and choice of referring expressions RECs,t and output the user’s dialogue Au,t and environment EAu,t acts. User’s observation and manipulation of the domain objects is represented b</context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>S. Janarthanam and O. Lemon. 2009a. A Wizard-ofOz environment to study Referring Expression Generation in a Situated Spoken Dialogue Task. In Proc. ENLG’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>Learning Lexical Alignment Policies for Generating Referring Expressions for Spoken Dialogue Systems.</title>
<date>2009</date>
<booktitle>In Proc. ENLG’09.</booktitle>
<contexts>
<context position="1708" citStr="Janarthanam and Lemon, 2009" startWordPosition="246" endWordPosition="249">spoken dialogue systems using reinforcement learning methods. An adaptive REG policy equips a dialogue system to dynamically modify its utterances in order to adapt to user’s domain knowledge level. For instance, to refer to domain objects, the system might use simple descriptive expressions with novices and technical jargon with experts. Such adaptations help grounding between the dialogue partners (Issacs and Clark, 1987). Since the user’s knowledge level is unknown, the system must be able to adapt dynamically during the conversation. Handcoding such a policy could be extremely difficult. (Janarthanam and Lemon, 2009b) have shown that such policies can be learned using simulation based reinforcement learning (RL) methods. The quality of such learned policies is directly dependent on the performance of the user simulations used to train them. So far, only hand-coded user simulations have been employed. In contrast, we now present a data driven two-tier user simulation model trained on dialogue data collected from real users. We also show that the two-tier model simulates real users more faithfully than other data driven baseline n-gram models (Eckert et al., 1997). In section 2 we briefly discuss other wor</context>
<context position="4799" citStr="Janarthanam and Lemon, 2009" startWordPosition="744" endWordPosition="748">whole session. The strategies used were “jargon”, “descriptive” and “tutorial”. In the jargon strategy the system instructs the user using technical terms (e.g. “Plug the broadband filter into the phone socket.”). In the descriptive strategy, it uses descriptive terms (e.g. “Plug the small white box into the square white box on the wall.”). In the tutorial strategy, the system uses both jargon and descriptive terms together. The system provides clarifications on referring expressions when users request them. The participant’s domain knowledge is also recorded during the task. Please refer to (Janarthanam and Lemon, 2009a) for a more details on our Wizard-of-Oz environment for data collection. The dialogues were collected from 17 participants (one dialogue each) with around 24 to 35 turns per dialogue depending on the strategy and user’s domain knowledge. 4 User Simulation models The dialogue data and knowledge profiles were used to build user simulation models. These models take as input the system’s dialogue act As,t (at turn t) and choice of referring expressions RECs,t and output the user’s dialogue Au,t and environment EAu,t acts. User’s observation and manipulation of the domain objects is represented b</context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>S. Janarthanam and O. Lemon. 2009b. Learning Lexical Alignment Policies for Generating Referring Expressions for Spoken Dialogue Systems. In Proc. ENLG’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Cluster-based User Simulations for Learning Dialogue Strategies. In</title>
<date>2006</date>
<booktitle>Proc. Interspeech/ICSLP.</booktitle>
<contexts>
<context position="10604" citStr="Rieser and Lemon, 2006" startWordPosition="1704" endWordPosition="1707">ctually observed in the data. This is counter-intuitive for our application. Therefore, we use a modified version of Witten-Bell discounting (given below) to smooth our models, where the extracted probability is equally divided amongst all possible responses. Using the modified version, the revised probabilities for the illustrated example are 0.61, 0.28 and 0.11 respectively. P*(ei) = C(e�) N+T + T (N+T)V 5 Metrics for evaluation of simulations While there are many proposed measures to rank user simulation models with respect to real user data (Schatzmann et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2006a; Williams, 2008), we use the Dialogue Similarity measure based on Kullback-Leibler (KL) (Cuayahuitl et al., 2005; Cuayahuitl, 2009) divergence to measure how similar the probability distributions of the simulation models are to the original real human data. 5.1 Dialogue Similarity Dialogue Similarity is a measure of divergence between real and simulated dialogues and can measure how similar a model is to real data. The measure is based on Kullback-Leibler (KL) divergence and is defined as follows: DS(P||Q) = 1 N DKL(P||Q)+DKL(Q||P) N �i=1 2 DKL(P||Q) = EMi=1 pi * log(p� q� ) The metric measu</context>
</contexts>
<marker>Rieser, Lemon, 2006</marker>
<rawString>V. Rieser and O. Lemon. 2006a. Cluster-based User Simulations for Learning Dialogue Strategies. In Proc. Interspeech/ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Georgila</author>
<author>S J Young</author>
</authors>
<title>Quantitative Evaluation of User Simulation Techniques for Spoken Dialogue Systems.</title>
<date>2005</date>
<booktitle>In Proc. SIGdial workshop on Discourse and Dialogue ’05.</booktitle>
<contexts>
<context position="10557" citStr="Schatzmann et al., 2005" startWordPosition="1696" endWordPosition="1699">e context and is more than the other responses actually observed in the data. This is counter-intuitive for our application. Therefore, we use a modified version of Witten-Bell discounting (given below) to smooth our models, where the extracted probability is equally divided amongst all possible responses. Using the modified version, the revised probabilities for the illustrated example are 0.61, 0.28 and 0.11 respectively. P*(ei) = C(e�) N+T + T (N+T)V 5 Metrics for evaluation of simulations While there are many proposed measures to rank user simulation models with respect to real user data (Schatzmann et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2006a; Williams, 2008), we use the Dialogue Similarity measure based on Kullback-Leibler (KL) (Cuayahuitl et al., 2005; Cuayahuitl, 2009) divergence to measure how similar the probability distributions of the simulation models are to the original real human data. 5.1 Dialogue Similarity Dialogue Similarity is a measure of divergence between real and simulated dialogues and can measure how similar a model is to real data. The measure is based on Kullback-Leibler (KL) divergence and is defined as follows: DS(P||Q) = 1 N DKL(P||Q)+DKL(Q||P) N �i=1 2 DKL(</context>
</contexts>
<marker>Schatzmann, Georgila, Young, 2005</marker>
<rawString>J. Schatzmann, K. Georgila, and S. J. Young. 2005. Quantitative Evaluation of User Simulation Techniques for Spoken Dialogue Systems. In Proc. SIGdial workshop on Discourse and Dialogue ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>M N Stuttle</author>
<author>S J Young</author>
</authors>
<title>A Survey of Statistical User Simulation Techniques for Reinforcement Learning of Dialogue Management Strategies. Knowledge Engineering Review,</title>
<date>2006</date>
<pages>97--126</pages>
<contexts>
<context position="2698" citStr="Schatzmann et al., 2006" startWordPosition="411" endWordPosition="414">ined on dialogue data collected from real users. We also show that the two-tier model simulates real users more faithfully than other data driven baseline n-gram models (Eckert et al., 1997). In section 2 we briefly discuss other work related to user simulations for dialogue policy learning using RL. In section 3 we describe the data used to build the simulation. Section 4 describes the simulation models in detail. In section 5 and 6 we present the evaluation metrics used and the results. 2 Related work Several user simulation models have been proposed for dialogue management policy learning (Schatzmann et al., 2006; Schatzmann et al., 2007). However, these models cannot be directly used for REG policy learning because they interact with the dialogue system only using high-level dialogue acts. Also, they do not simulate different user groups like experts, novices, etc. In order to learn adaptive REG policies, user simulations need to respond to the system’s choice of referring expressions and simulate user groups with different knowledge levels. We propose a two-tier simulation which simulates users with different knowledge levels and is sensitive to the system’s choice of referring expressions. Proceedi</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement Learning of Dialogue Management Strategies. Knowledge Engineering Review, pages 97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>S J Young</author>
</authors>
<title>Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In</title>
<date>2007</date>
<booktitle>Proc of HLT/NAACL</booktitle>
<contexts>
<context position="2724" citStr="Schatzmann et al., 2007" startWordPosition="415" endWordPosition="418">lected from real users. We also show that the two-tier model simulates real users more faithfully than other data driven baseline n-gram models (Eckert et al., 1997). In section 2 we briefly discuss other work related to user simulations for dialogue policy learning using RL. In section 3 we describe the data used to build the simulation. Section 4 describes the simulation models in detail. In section 5 and 6 we present the evaluation metrics used and the results. 2 Related work Several user simulation models have been proposed for dialogue management policy learning (Schatzmann et al., 2006; Schatzmann et al., 2007). However, these models cannot be directly used for REG policy learning because they interact with the dialogue system only using high-level dialogue acts. Also, they do not simulate different user groups like experts, novices, etc. In order to learn adaptive REG policies, user simulations need to respond to the system’s choice of referring expressions and simulate user groups with different knowledge levels. We propose a two-tier simulation which simulates users with different knowledge levels and is sensitive to the system’s choice of referring expressions. Proceedings of SIGDIAL 2009: the 1</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. J. Young. 2007. Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In Proc of HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Whittaker</author>
<author>M Walker</author>
<author>J Moore</author>
</authors>
<title>Fish or Fowl: A Wizard of Oz Evaluation</title>
<date>2002</date>
<booktitle>of Dialogue Strategies in the Restaurant Domain. In Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="3669" citStr="Whittaker et al., 2002" startWordPosition="559" endWordPosition="562"> choice of referring expressions and simulate user groups with different knowledge levels. We propose a two-tier simulation which simulates users with different knowledge levels and is sensitive to the system’s choice of referring expressions. Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 120–123, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 120 3 Corpus The “Wizard-of-Oz” (WOZ) methodology is a widely accepted way of collecting dialogue data for user simulation modeling (Whittaker et al., 2002). In this setup, real users interact with a human wizard disguised as a dialogue system. The wizard interprets the users responses and passes them on to the dialogue system. The dialogue system updates the dialogue state and decides the responses to user’s moves. The task of the participant is to interact with the dialogue system to get instructions to setup a broadband Internet connection. The referring expression generation strategy is chosen before the dialogue starts and stays the same for the whole session. The strategies used were “jargon”, “descriptive” and “tutorial”. In the jargon str</context>
</contexts>
<marker>Whittaker, Walker, Moore, 2002</marker>
<rawString>S. Whittaker, M. Walker, and J. Moore. 2002. Fish or Fowl: A Wizard of Oz Evaluation of Dialogue Strategies in the Restaurant Domain. In Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
</authors>
<title>Evaluating User Simulations with the Cramer-von Mises Divergence. Speech Communication,</title>
<date>2008</date>
<pages>50--829</pages>
<contexts>
<context position="10622" citStr="Williams, 2008" startWordPosition="1708" endWordPosition="1709">ata. This is counter-intuitive for our application. Therefore, we use a modified version of Witten-Bell discounting (given below) to smooth our models, where the extracted probability is equally divided amongst all possible responses. Using the modified version, the revised probabilities for the illustrated example are 0.61, 0.28 and 0.11 respectively. P*(ei) = C(e�) N+T + T (N+T)V 5 Metrics for evaluation of simulations While there are many proposed measures to rank user simulation models with respect to real user data (Schatzmann et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2006a; Williams, 2008), we use the Dialogue Similarity measure based on Kullback-Leibler (KL) (Cuayahuitl et al., 2005; Cuayahuitl, 2009) divergence to measure how similar the probability distributions of the simulation models are to the original real human data. 5.1 Dialogue Similarity Dialogue Similarity is a measure of divergence between real and simulated dialogues and can measure how similar a model is to real data. The measure is based on Kullback-Leibler (KL) divergence and is defined as follows: DS(P||Q) = 1 N DKL(P||Q)+DKL(Q||P) N �i=1 2 DKL(P||Q) = EMi=1 pi * log(p� q� ) The metric measures the divergence</context>
</contexts>
<marker>Williams, 2008</marker>
<rawString>J. Williams. 2008. Evaluating User Simulations with the Cramer-von Mises Divergence. Speech Communication, 50:829–846.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>