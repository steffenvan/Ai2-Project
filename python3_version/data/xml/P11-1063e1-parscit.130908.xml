<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039629">
<title confidence="0.9613915">
Erratum to Incremental Syntactic Language Models for Phrase-based
Translation
</title>
<author confidence="0.984111">
Lane Schwartz
</author>
<affiliation confidence="0.8070565">
Air Force Research Laboratory
Wright-Patterson AFB, OH USA
</affiliation>
<email confidence="0.992569">
lane.schwartz@wpafb.af.mil
</email>
<author confidence="0.998589">
William Schuler
</author>
<affiliation confidence="0.998993">
Ohio State University
</affiliation>
<address confidence="0.50232">
Columbus, OH USA
</address>
<email confidence="0.993267">
schuler@ling.ohio-state.edu
</email>
<author confidence="0.951568">
Chris Callison-Burch
</author>
<affiliation confidence="0.940263">
Johns Hopkins University
</affiliation>
<address confidence="0.811894">
Baltimore, MD USA
</address>
<email confidence="0.99869">
ccb@cs.jhu.edu
</email>
<author confidence="0.988903">
Stephen Wu
</author>
<affiliation confidence="0.512481">
Mayo Clinic
Rochester, MN USA
</affiliation>
<email confidence="0.994711">
wu.stephen@mayo.edu
</email>
<sectionHeader confidence="0.995557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843428571429">
Schwartz et al. (2011) presented a novel
technique for incorporating syntactic knowl-
edge into phrase-based machine translation
through incremental syntactic parsing, and
presented empirical results on a constrained
Urdu-English translation task. The work con-
tained an error in the description of the ex-
perimental setup, which was discovered sub-
sequent to publication. After correcting the
error, no improvement in BLEU score is seen
over the baseline when the syntactic lan-
guage model is used on the constrained Urdu-
English translation task. The error does not af-
fect the originally reported perplexity results.
</bodyText>
<sectionHeader confidence="0.998585" genericHeader="keywords">
1 Error
</sectionHeader>
<bodyText confidence="0.999564083333333">
Schwartz et al. (2011) presented a novel technique
for incorporating syntactic knowledge into phrase-
based machine translation through incremental syn-
tactic parsing. That work contained an error in the
description of the experimental setup, which was
discovered subsequent to publication. The penulti-
mate sentence of Section 6 stated that during MERT
(Och, 2003), “we tuned the parameters using a con-
strained dev set (only sentences with 1-20 words).”
Opinions, interpretations, conclusions, and recommenda-
tions are those of the authors and are not necessarily endorsed
by the sponsors or the United States Air Force. Material pre-
sented here was cleared for public release with Case Number
88ABW-2010-6489 on 10 Dec 2010 and with Case Number
88ABW-2012-0302 on 19 Jan 2012.
While this was the intended experimental configura-
tion, subsequent to publication a re-examination of
the experiment revealed that for the condition where
the HHMM syntactic language model was used in
addition to the n-gram language model (HHMM +
n-gram), tuning was actually performed using a con-
strained dev set of sentences with 1-40 words.
As a result of this error, the BLEU scores reported
in Figure 9 do not represent directly comparable ex-
perimental conditions, since the dev set used for tun-
ing was different (sentences with 1-20 words for
n-gram only versus sentences with 1-40 words for
HHMM + n-gram).
Because the results are not comparable, the claims
of statistically significant improvements to transla-
tion quality are not justified. In order to provide
comparable results, we re-ran the n-gram only con-
figuration performing tuning with a constrained dev
set of 1-40 words, to match the actual configuration
that was used for the HHMM + n-gram configura-
tion. A list of corrections is listed below.
</bodyText>
<listItem confidence="0.8798705">
2 List of Corrections
• Abstract, final sentence:
</listItem>
<bodyText confidence="0.996293181818182">
We present empirical results on a
constrained Urdu-English translation
task that demonstrate a significant
BLEU score improvement and a
large decrease in perplexity.
should become
We present empirical results on a
constrained Urdu-English translation
task that demonstrate a large de-
crease in perplexity but no significant
improvement to BLEU score.
</bodyText>
<listItem confidence="0.986165">
• Section 1, final sentence:
</listItem>
<bodyText confidence="0.9921595">
Integration with Moses (§5) along
with empirical results for perplexity
and significant translation score im-
provement on a constrained Urdu-
English task (§6)
should become
Integration with Moses (§5) along
with empirical results for perplex-
ity and translation scores on a con-
strained Urdu-English task (§6)
</bodyText>
<listItem confidence="0.990596">
• Section 6, final two sentences:
</listItem>
<bodyText confidence="0.996032714285714">
Due to this slowdown, we tuned the
parameters using a constrained dev
set (only sentences with 1-20 words),
and tested using a constrained devtest
set (only sentences with 1-20 words).
Figure 9 shows a statistically signifi-
cant improvement to the BLEU score
when using the HHMM and the n-
gram LMs together on this reduced
test set.
should become
Due to this slowdown, we tuned the
parameters using a constrained dev
set (only sentences with 1-40 words),
and tested using a constrained devtest
set (only sentences with 1-20 words).
Figure 9 shows no statistically sig-
nificant improvement to the BLEU
score when using the HHMM and the
n-gram LMs together on this reduced
test set.
</bodyText>
<listItem confidence="0.976038">
• Figure 9:
</listItem>
<table confidence="0.988495571428571">
Moses LM(s) BLEU
n-gram only 18.78
HHMM + n-gram 19.78
should become
Moses LM(s) BLEU
n-gram only 21.43
HHMM + n-gram 21.72
</table>
<listItem confidence="0.862891">
• Section 7, sentence 5:
</listItem>
<bodyText confidence="0.996856615384615">
The translation quality significantly
improved on a constrained task, and
the perplexity improvements suggest
that interpolating between n-gram
and syntactic LMs may hold promise
on larger data sets.
should become
While translation quality did not sig-
nificantly improve on a constrained
task, the perplexity improvements
suggest that interpolating between n-
gram and syntactic LMs may hold
promise on larger data sets.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999991">
The description of the experimental setup in
Schwartz et al. (2011) contained an error that was
discovered subsequent to publication. The descrip-
tion stated that MERT was performed on a con-
strained dev set of sentences with 1-20 words. In
fact, one of the experimental conditions (HHMM +
n-gram) was instead run on a constrained dev set of
sentences with 1-40 words. This error has been cor-
rected — after correction, no statistically significant
improvement to translation quality is seen in terms
of BLEU score. The error does not affect the origi-
nally reported perplexity results.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991885">
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160–167, Sapporo, Japan, July.
Lane Schwartz, Chris Callison-Burch, William Schuler,
and Stephen Wu. 2011. Incremental syntactic lan-
guage models for phrase-based translation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 620–631, Portland, Oregon, June.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.006092">
<title confidence="0.9591965">to Syntactic Language Models for Phrase-based Translation Lane Air Force Research</title>
<author confidence="0.933836">Wright-Patterson AFB</author>
<author confidence="0.933836">OH</author>
<email confidence="0.980381">lane.schwartz@wpafb.af.mil</email>
<affiliation confidence="0.747119">William Ohio State</affiliation>
<address confidence="0.987471">Columbus, OH</address>
<email confidence="0.998274">schuler@ling.ohio-state.edu</email>
<author confidence="0.856344">Chris</author>
<affiliation confidence="0.34996">Johns Hopkins</affiliation>
<address confidence="0.667075">Baltimore, MD</address>
<email confidence="0.997978">ccb@cs.jhu.edu</email>
<author confidence="0.943642">Stephen</author>
<affiliation confidence="0.786217">Mayo</affiliation>
<address confidence="0.819421">Rochester, MN</address>
<email confidence="0.999414">wu.stephen@mayo.edu</email>
<abstract confidence="0.996591105263158">Schwartz et al. (2011) presented a novel technique for incorporating syntactic knowledge into phrase-based machine translation through incremental syntactic parsing, and presented empirical results on a constrained Urdu-English translation task. The work contained an error in the description of the experimental setup, which was discovered subsequent to publication. After correcting the error, no improvement in BLEU score is seen over the baseline when the syntactic language model is used on the constrained Urdu- English translation task. The error does not affect the originally reported perplexity results. 1 Error Schwartz et al. (2011) presented a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. That work contained an error in the description of the experimental setup, which was discovered subsequent to publication. The penultimate sentence of Section 6 stated that during MERT (Och, 2003), “we tuned the parameters using a constrained dev set (only sentences with 1-20 words).” Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the sponsors or the United States Air Force. Material presented here was cleared for public release with Case Number 88ABW-2010-6489 on 10 Dec 2010 and with Case Number 88ABW-2012-0302 on 19 Jan 2012. While this was the intended experimental configuration, subsequent to publication a re-examination of the experiment revealed that for the condition where the HHMM syntactic language model was used in to the language model (HHMM + tuning was actually performed using a constrained dev set of sentences with 1-40 words. As a result of this error, the BLEU scores reported in Figure 9 do not represent directly comparable experimental conditions, since the dev set used for tuning was different (sentences with 1-20 words for only versus sentences with 1-40 words for + Because the results are not comparable, the claims of statistically significant improvements to translation quality are not justified. In order to provide results, we re-ran the only configuration performing tuning with a constrained dev set of 1-40 words, to match the actual configuration was used for the HHMM + configuration. A list of corrections is listed below. 2 List of Corrections • Abstract, final sentence: We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity. should become We present empirical results on a constrained Urdu-English translation task that demonstrate a large decrease in perplexity but no significant improvement to BLEU score. • Section 1, final sentence: with Moses along with empirical results for perplexity and significant translation score improvement on a constrained Urdutask should become with Moses along with empirical results for perplexity and translation scores on a con- Urdu-English task • Section 6, final two sentences: Due to this slowdown, we tuned the parameters using a constrained dev set (only sentences with 1-20 words), and tested using a constrained devtest set (only sentences with 1-20 words). Figure 9 shows a statistically significant improvement to the BLEU score using the HHMM and the gram LMs together on this reduced test set. should become Due to this slowdown, we tuned the parameters using a constrained dev set (only sentences with 1-40 words), and tested using a constrained devtest set (only sentences with 1-20 words). Figure 9 shows no statistically significant improvement to the BLEU score when using the HHMM and the LMs together on this reduced test set. • Figure 9: Moses LM(s) BLEU only 18.78 + 19.78 should become Moses LM(s) BLEU only 21.43 + 21.72 • Section 7, sentence 5: The translation quality significantly improved on a constrained task, and the perplexity improvements suggest interpolating between and syntactic LMs may hold promise on larger data sets. should become While translation quality did not significantly improve on a constrained task, the perplexity improvements that interpolating between gram and syntactic LMs may hold promise on larger data sets. 3 Conclusion The description of the experimental setup in Schwartz et al. (2011) contained an error that was discovered subsequent to publication. The description stated that MERT was performed on a constrained dev set of sentences with 1-20 words. In fact, one of the experimental conditions (HHMM + was instead run on a constrained dev set of sentences with 1-40 words. This error has been corrected — after correction, no statistically significant improvement to translation quality is seen in terms of BLEU score. The error does not affect the originally reported perplexity results.</abstract>
<note confidence="0.6087564">References Franz Och. 2003. Minimum error rate training in statismachine translation. In of the 41st Annual Meeting of the Association for Computational pages 160–167, Sapporo, Japan, July.</note>
<author confidence="0.792264">Lane Schwartz</author>
<author confidence="0.792264">Chris Callison-Burch</author>
<author confidence="0.792264">William Schuler</author>
<abstract confidence="0.6411905">and Stephen Wu. 2011. Incremental syntactic lanmodels for phrase-based translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language</abstract>
<note confidence="0.702681">pages 620–631, Portland, Oregon, June.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1391" citStr="Och, 2003" startWordPosition="192" endWordPosition="193">er correcting the error, no improvement in BLEU score is seen over the baseline when the syntactic language model is used on the constrained UrduEnglish translation task. The error does not affect the originally reported perplexity results. 1 Error Schwartz et al. (2011) presented a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. That work contained an error in the description of the experimental setup, which was discovered subsequent to publication. The penultimate sentence of Section 6 stated that during MERT (Och, 2003), “we tuned the parameters using a constrained dev set (only sentences with 1-20 words).” Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the sponsors or the United States Air Force. Material presented here was cleared for public release with Case Number 88ABW-2010-6489 on 10 Dec 2010 and with Case Number 88ABW-2012-0302 on 19 Jan 2012. While this was the intended experimental configuration, subsequent to publication a re-examination of the experiment revealed that for the condition where the HHMM syntactic language model</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
<author>Chris Callison-Burch</author>
<author>William Schuler</author>
<author>Stephen Wu</author>
</authors>
<title>Incremental syntactic language models for phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>620--631</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="1052" citStr="Schwartz et al. (2011)" startWordPosition="141" endWordPosition="144">1) presented a novel technique for incorporating syntactic knowledge into phrase-based machine translation through incremental syntactic parsing, and presented empirical results on a constrained Urdu-English translation task. The work contained an error in the description of the experimental setup, which was discovered subsequent to publication. After correcting the error, no improvement in BLEU score is seen over the baseline when the syntactic language model is used on the constrained UrduEnglish translation task. The error does not affect the originally reported perplexity results. 1 Error Schwartz et al. (2011) presented a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. That work contained an error in the description of the experimental setup, which was discovered subsequent to publication. The penultimate sentence of Section 6 stated that during MERT (Och, 2003), “we tuned the parameters using a constrained dev set (only sentences with 1-20 words).” Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the sponsors or the United States Air Force. Mater</context>
</contexts>
<marker>Schwartz, Callison-Burch, Schuler, Wu, 2011</marker>
<rawString>Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language models for phrase-based translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 620–631, Portland, Oregon, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>