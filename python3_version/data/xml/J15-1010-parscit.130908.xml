<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99863725">
When the Whole Is Not Greater
Than the Combination of Its Parts:
A “Decompositional” Look at
Compositional Distributional Semantics
</title>
<author confidence="0.999076">
Fabio Massimo Zanzotto*
</author>
<affiliation confidence="0.997422">
University of Rome “Tor Vergata”
</affiliation>
<author confidence="0.986526">
Lorenzo Ferrone
</author>
<affiliation confidence="0.998292">
University of Rome “Tor Vergata”
</affiliation>
<author confidence="0.978133">
Marco Baroni
</author>
<affiliation confidence="0.992858">
University of Trento
</affiliation>
<bodyText confidence="0.9708988">
Distributional semantics has been extended to phrases and sentences by means of composition
operations. We look at how these operations affect similarity measurements, showing that simi-
larity equations of an important class of composition methods can be decomposed into operations
performed on the subparts of the input phrases. This establishes a strong link between these
models and convolution kernels.
</bodyText>
<sectionHeader confidence="0.997645" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999787916666667">
Distributional semantics approximates word meanings with vectors tracking co-
occurrence in corpora (Turney and Pantel 2010). Recent work has extended this ap-
proach to phrases and sentences through vector composition (Clark 2015). Resulting
compositional distributional semantic models (CDSMs) estimate degrees of seman-
tic similarity (or, more generally, relatedness) between two phrases: A good CDSM
might tell us that green bird is closer to parrot than to pigeon, useful for tasks such as
paraphrasing.
We take a mathematical look1 at how the composition operations postulated by
CDSMs affect similarity measurements involving the vectors they produce for phrases
or sentences. We show that, for an important class of composition methods, encom-
passing at least those based on linear transformations, the similarity equations can
be decomposed into operations performed on the subparts of the input phrases,
</bodyText>
<note confidence="0.604561">
* Department of Enterprise Engineering, University of Rome “Tor Vergata,” Viale del Politecnico, 1,
00133 Rome, Italy. E-mail: fabio.massimo.zanzotto@uniroma2.it.
</note>
<footnote confidence="0.968737333333333">
1 Ganesalingam and Herbelot (2013) also present a mathematical investigation of CDSMs. However,
except for the tensor product (a composition method we do not consider here as it is not empirically
effective), they do not look at how composition strategies affect similarity comparisons.
Original submission received: 10 December 2013; revision received: 26 May 2014; accepted for publication:
1 August 2014.
doi:10.1162/COLI a 00215
</footnote>
<note confidence="0.829184">
© 2015 Association for Computational Linguistics
Computational Linguistics Volume 41, Number 1
</note>
<tableCaption confidence="0.7852874">
Table 1
Compositional Distributional Semantic Models: it, b, and c�are distributional vectors representing
the words a, b, and c, respectively; matrices X, Y, and Z are constant across a phrase type,
corresponding to syntactic slots; the matrix A and the third-order tensor B represent the
predicate words a in the first phrase and b in the second phrase, respectively.
</tableCaption>
<table confidence="0.9794992">
2-word phrase 3-word phrase reference
Additive a�+ b a�+ 6 +c� Mitchell and Lapata (2008)
Multiplicative a� 0 b a�0 9 0 c� Mitchell and Lapata (2008)
Full Additive Xa + Y b Ya + Xb + Zc Guevara (2010), Zanzotto et al. (2010)
Lexical Function A b a�B c� Coecke, Sadrzadeh, and Clark (2010)
</table>
<bodyText confidence="0.9994488">
and typically factorized into terms that reflect the linguistic structure of the input.
This establishes a strong link between CDSMs and convolution kernels (Haussler
1999), which act in the same way. We thus refer to our claim as the “Convolution
Conjecture.”
We focus on the models in Table 1. These CDSMs all apply linear methods, and
we suspect that linearity is a sufficient (but not necessary) condition to ensure that the
Convolution Conjecture holds. We will first illustrate the conjecture for linear methods,
and then briefly consider two nonlinear approaches: the dual space model of Turney
(2012), for which it does, and a representative of the recent strand of work on neural-
network models of composition, for which it does not.
</bodyText>
<sectionHeader confidence="0.973682" genericHeader="method">
2. Mathematical Preliminaries
</sectionHeader>
<bodyText confidence="0.999892">
Vectors are represented as small letters with an arrow a� and their elements are ai,
matrices as capital letters in bold A and their elements are Aij, and third-order or
fourth-order tensors as capital letters in the form A and their elements are Aijk or Aijkh.
The symbol 0 represents the element-wise product and ® is the tensor product. The
dot product is (a,b) and the Frobenius product—that is, the generalization of the dot
product to matrices and high-order tensors—is represented as (A, B)F and (A,B)F. The
Frobenius product acts on vectors, matrices, and third-order tensors as follows:
</bodyText>
<equation confidence="0.9974175">
(a,�b)F = � aibi = (�a,�b) (A, B)F = �ij AijBij (A,B)F = � AijkBijk (1)
i ijk
</equation>
<bodyText confidence="0.9850625">
A simple property that relates the dot product between two vectors and the
Frobenius product between two general tensors is the following:
</bodyText>
<equation confidence="0.904749333333334">
(a,�b) = (I,�a�bT)F (2)
where I is the identity matrix. The dot product of Ax and By can be rewritten as:
(A�x, B�y) = (ATB,4T)F (3)
</equation>
<page confidence="0.984857">
166
</page>
<bodyText confidence="0.8236915">
Zanzotto, Ferrone, and Baroni When the Whole Is Not Greater Than the Combination of Its Parts
Let A and B be two third-order tensors and x, y,ta, c~ four vectors. It can be shown that:
</bodyText>
<equation confidence="0.987628">
~��xA�y,�aB�c � = � (A ⊗ B)j,�x ⊗ y~ ⊗�a ⊗ c~ ~F (4)
j
</equation>
<bodyText confidence="0.99984875">
where C = ~j(A ⊗B)j is a non-standard way to indicate the tensor contraction of the
tensor product between two third-order tensors. In this particular tensor contraction,
the elements Ciknm of the resulting fourth-order tensor C are Ciknm = ~j AijkBnjm. The
elements Diknm of the tensor D = x� ⊗ y�⊗d ⊗ c�are Diknm = xiykancm.
</bodyText>
<sectionHeader confidence="0.93962" genericHeader="method">
3. Formalizing the Convolution Conjecture
</sectionHeader>
<bodyText confidence="0.999739181818182">
Structured Objects. In line with Haussler (1999), a structured object x ∈ X is either
a terminal object that cannot be furthermore decomposed, or a non-terminal object
that can be decomposed into n subparts. We indicate with x = (x1, ... , xn) one such
decomposition, where the subparts xi ∈ X are structured objects themselves. The set
X is the set of the structured objects and TX ⊆ X is the set of the terminal objects. A
structured object x can be anything according to the representational needs. Here, x is
a representation of a text fragment, and so it can be a sequence of words, a sequence
of words along with their part of speech, a tree structure, and so on. The set R(x) is
the set of decompositions of x relevant to define a specific CDSM. Note that a given
decomposition of a structured object x does not need to contain all the subparts of the
original object. For example, let us consider the phrase x = tall boy. We can then define
</bodyText>
<equation confidence="0.6758896">
R(x) = {(tall, boy), (tall), (boy)}. This set contains the three possible decompositions of
the phrase: (tall ���� , boy), ( tall
���� ���� ), and (boy
����
x1 x2 x1 x1
</equation>
<bodyText confidence="0.9948005">
Recursive formulation of CDSM. A CDSM can be viewed as a function f that acts recur-
sively on a structured object x. If x is a non-terminal object
</bodyText>
<equation confidence="0.996662">
f(x) = ~ γ(f (x1),f (x2),... ,f (xn)) (5)
x∈R(x)
</equation>
<bodyText confidence="0.993491714285714">
where R(x) is the set of relevant decompositions, is a repeated operation on this
set, γ is a function defined on f (xi) where xi are the subparts of a decomposition of
x. If x is a terminal object, f (x) is directly mapped to a tensor. The function f may
operate differently on different kinds of structured objects, with tensor degree varying
accordingly. The set R(x) and the functions f, γ, and depend on the specific CDSM,
and the same CDSM might be susceptible to alternative analyses satisfying the form in
Equation (5). As an example, under Additive, x is a sequence of words and f is
</bodyText>
<equation confidence="0.938407">
).
f(y) if x ∈/ TX
x~ if x ∈ TX
f(x) =
{
~
y∈R(x)
(6)
</equation>
<page confidence="0.972691">
167
</page>
<note confidence="0.335706">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999896">
where R((w1,..., wn)) = {(w1),..., (wn)}. The repeated operation (D corresponds to
summing and -y is identity. For Multiplicative we have
</bodyText>
<equation confidence="0.999691666666667">
f(x) = { � f (y) if x ∈/ TX (7)
yER(x)
x� if x ∈ TX
</equation>
<bodyText confidence="0.9996462">
where R(x) = {(w1, ... , wn)} (a single trivial decomposition including all subparts).
With a single decomposition, the repeated operation reduces to a single term; and here -y
is the product (it will be clear subsequently, when we apply the Convolution Conjecture
to these models, why we are assuming different decomposition sets for Additive and
Multiplicative).
</bodyText>
<subsectionHeader confidence="0.447738">
Definition 1 (Convolution Conjecture)
</subsectionHeader>
<bodyText confidence="0.9239855">
For every CDSM f along with its R(x) set, there exist functions K, Ki and a function g
such that:
</bodyText>
<equation confidence="0.989643333333333">
K( f (x), f (y)) = E g(K1(f(x1),f(y1)), K2(f(x2),f(y2)),..., Kn(f(xn),f(yn))) (8)
xER(x)
yER(y)
</equation>
<bodyText confidence="0.99984">
The Convolution Conjecture postulates that the similarity K( f (x),f (y)) between the
tensors f (x) and f (y) is computed by combining operations on the subparts, that is,
Ki(f (xi),f (yi)), using the function g. This is exactly what happens in convolution kernels
(Haussler 1999). K is usually the dot product, but this is not necessary: We will show
that for the dual-space model of Turney (2012) K turns out to be the fourth root of the
Frobenius tensor.
</bodyText>
<sectionHeader confidence="0.556468" genericHeader="method">
4. Comparing Composed Phrases
</sectionHeader>
<bodyText confidence="0.937347666666667">
We illustrate now how the Convolution Conjecture (CC) applies to the considered
CDSMs, exemplifying with adjective–noun and subject–verb–object phrases. Without
loss of generality we use tall boy and red cat for adjective–noun phrases and goats eat
grass and cows drink water for subject–verb–object phrases.
Additive Model. K and Ki are dot products, g is the identity function, and f is as in
Equation (6). The structure of the input is a word sequence (i.e., x = (w1 w2)) and the
relevant decompositions consist of these single words, R(x) = {(w1), (w2)}. Then
K( f (tall boy), f (red cat)) = ( tall + bay, red + cdts) =
= ( tall, red) + ( tall, cdt) + ( bay, red) + ( bay, cdt) =
</bodyText>
<equation confidence="0.895458666666667">
K( f(x),f(y)) (9)
xE{tall,boy}
yE{red,cat}
</equation>
<bodyText confidence="0.99967725">
The CC form of Additive shows that the overall dot product can be decomposed
into dot products of the vectors of the single words. Composition does not add any
further information. These results can be easily extended to longer phrases and to
phrases of different length.
</bodyText>
<equation confidence="0.57204">
E=
E
�f(x),f(y)� =
xE{tall,boy}
yE{red,cat}
</equation>
<page confidence="0.966995">
168
</page>
<note confidence="0.583872">
Zanzotto, Ferrone, and Baroni When the Whole Is Not Greater Than the Combination of Its Parts
</note>
<bodyText confidence="0.919651625">
Multiplicative Model. K, g are dot products, Ki the component-wise product, and
f is as in Equation (7). The structure of the input is x = (w1 w2), and we use the
trivial single decomposition consisting of all subparts (thus summation reduces to a
single term):
K( f (tall boy),f (red cat)) = ( tall 0 bay, red 0 cat) = ( tall 0 red 0 bay 0 cat,f) = (10)
= ( tall 0 red, bay 0 cat) = g(K1( tall, red), K2( bay, cat))
This is the dot product between an indistinct chain of element-wise products and a
vector 1� of all ones or the product of two separate element-wise products, one on
adjectives tall 0 red, and one on nouns bay 0 cat. In this latter CC form, the final dot
product is obtained in two steps: first separately operating on the adjectives and on the
nouns; then taking the dot product of the resulting vectors. The comparison operations
are thus reflecting the input syntactic structure. The results can be easily extended to
longer phrases and to phrases of different lengths.
Full Additive Model. The input consists of a sequence of (label,word) pairs x =
((L1 w1),..., (Ln wn)) and the relevant decomposition set includes the single tuples,
that is, TZ(x) = {(L1 w1), ..., (Ln wn)}. The CDSM f is defined as
</bodyText>
<equation confidence="0.998138777777778">
E f(L)f(w) if x E/ TX
(L w)ETZ(x)
X if x E TX is a label L
w� if x E TX is a word w
(11)
⎧
⎨⎪⎪⎪
⎪⎪⎪⎩
f(x) =
</equation>
<bodyText confidence="0.99971175">
The repeated operation 0 here is summation, and Y the matrix-by-vector product.
In the CC form, K is the dot product, g the Frobenius product, K1(f (x), f (y)) = f (x)Tf (y),
and K2( f (x),f (y)) = f(x)f(y)T. We have then for adjective–noun composition (by
using the property in Equation (3)):
</bodyText>
<equation confidence="0.784393166666667">
K(f ((A tall) (N boy)), f ((A red) (N cat))) = (A toll + N bay, A red + N cat) =
= (A tall, A red) + (A tall, N cat) + (N b4, A red) + (N b4, N cat) =
= (ATA, tall redT)F + (NTA, bay redT)F + (ATN, tall �catT)F + (NTN, boy �catT)F = (12)
g(K1(f (lx),f (ly)), K2(f (wx),f (wy))
(lx wx)E{(A tall),(N boy)}
(ly wy)E{(A red),(N cat)}
</equation>
<bodyText confidence="0.94796675">
The CC form shows how Full Additive factorizes into a more structural and a
more lexical part: Each element of the sum is the Frobenius product between the
product of two matrices representing syntactic labels and the tensor product between
= �
</bodyText>
<page confidence="0.98078">
169
</page>
<note confidence="0.460085">
Computational Linguistics Volume 41, Number 1
</note>
<equation confidence="0.9626934">
two vectors representing the corresponding words. For subject–verb–object phrases
((S w1) (V w2) (O w3)) we have
K(f (((S goats) (V eat) (O grass))), f (((S cows) (V drink) (O water)))) =
= (S gaats + V edt + O grdss, S cows + V drank + O water) =
= (STS, �goats COZUsT)F + (STV,�goats �drinkT)F + (STO, goats �waterT)F
+(VTS, eat COZUsT)F + (VTV, eat �drinkT)F + (VTO, eat �waterT)F
+(OTS, grRss COZUsT)F + (OTV, grdss �drinkT)F + (OTO, grass �waterT)F
= � g(K1(f (lx),f(ly)), K2(f (wx),f(wy))
(lx wx)E{(S goats),(V eat),(Ograss)}
(ly wy)E{(S cows),(V drink),(O water)}
</equation>
<bodyText confidence="0.998245461538461">
Again, we observe the factoring into products of syntactic and lexical representations.
By looking at Full Additive in the CC form, we observe that when XTY Z:� I for all
matrix pairs, it degenerates to Additive. Interestingly, Full Additive can also approx-
imate a semantic convolution kernel (Mehdad, Moschitti, and Zanzotto 2010), which
combines dot products of elements in the same slot. In the adjective–noun case, we
obtain this approximation by choosing two nearly orthonormal matrices A and N such
that AAT = NNT zz� I and ANT zz� 0 and applying Equation (2): (A tall + N bay, A red +
N cdt) zz� (tall, red) + (boy, cat).
This approximation is valid also for three-word phrases. When the matrices S, V,
and O are such that XXT � I with X one of the three matrices and YXT � 0 with X and
Y two different matrices, Full Additive approximates a semantic convolution kernel
comparing two sentences by summing the dot products of the words in the same role,
that is,
</bodyText>
<equation confidence="0.853352">
(S gaats + V edt + O grdss, S cows + V drank + O water) ~ (14)
� (goats, cows) + (eat, drink) + (grass, water)
Results can again be easily extended to longer and different-length phrases.
</equation>
<bodyText confidence="0.99930675">
Lexical Function Model. We distinguish composition with one- vs. two argument pred-
icates. We illustrate the first through adjective–noun composition, where the adjective
acts as the predicate, and the second with transitive verb constructions. Although we
use the relevant syntactic labels, the formulas generalize to any construction with the
same argument count. For adjective–noun phrases, the input is a sequence of (label,
word) pairs (x = ((A,w1),(N,w2))) and the relevant decomposition set again includes
only the single trivial decomposition into all the subparts: IZ(x) = {((A, w1), (N, w2))�.
The method itself is recursively defined as
</bodyText>
<equation confidence="0.944853">
⎧ (15)
⎨⎪f((A, w1))f((N, w2)) if x E/ TX = ((A, w1), (N, w2))
f (x) = W1 if x E Tx = (A, w1)
⎩ ⎪w2 if x E Tx = (N, w2)
(13)
</equation>
<page confidence="0.972703">
170
</page>
<bodyText confidence="0.818446">
Zanzotto, Ferrone, and Baroni When the Whole Is Not Greater Than the Combination of Its Parts
Here, K and g are, respectively, the dot and Frobenius product, K1(f (x), f (y)) = f (x)Tf (y),
and K2(f (x),f (y)) = f(x)f(y)T. Using Equation (3), we have then
K( f (tall boy)),f (red cat)) = (TALL bay, RED cat) =
</bodyText>
<equation confidence="0.90309">
(16)
</equation>
<bodyText confidence="0.930776833333333">
= �TALLTRED, �boy �catT�F = g(K1( f (tall), f (red)), K2( f (boy), f (cat)))
The role of predicate and argument words in the final dot product is clearly
separated, showing again the structure-sensitive nature of the decomposition of the
comparison operations. In the two-place predicate case, again, the input is a set of
(label, word) tuples, and the relevant decomposition set only includes the single trivial
decomposition into all subparts. The CDSM f is defined as
</bodyText>
<equation confidence="0.99685075">
f(x) = w� if x ∈ TX = (l w) and l is S or O
⎨⎪f((S w1)) ⊗ f((V w2)) ⊗ f((O w3)) if x ∈/TX = ((S w1) (V w2) (O w3))
⎧
⎩ ⎪W if x ∈ TX = (V w) (17)
</equation>
<bodyText confidence="0.991140461538462">
K is the dot product and g(x, y, z) = (x, y ⊗ z)F, K1(f (x), f (y)) = Ej(f (x) ⊗ f (y))j—
that is, the tensor contraction2 along the second index of the tensor product between
f (x) and f (y)—and K2( f (x),f (y)) = K3(f (x), f (y)) = f(x) ⊗ f(y) are tensor products. The
dot product of goats EAT grass and cows DRINK water is (by using Equation (4))
K( f (((S goats) (V eat) (O grass))),f (((S cows) (V drink) (O water)))) =
= ( goats EAT grass, cows DRINK water) =
= (Ej(EAT ⊗ DRINK)j, goats ⊗ grass ⊗ cows ⊗ water)F = (18)
= g(K1(f ((V eat)), f ((V drink))),
K2(f ((S goats)),f ((S cows))) ⊗ K3(f ((O grass)), f ((O water))))
We rewrote the equation as a Frobenius product between two fourth-order ten-
sors. The first combines the two third-order tensors of the verbs Ej(EAT ⊗ DRINK)j
and the second combines the vectors representing the arguments of the verb, that is:
goats ⊗ grass ⊗ cows ⊗ water. In this case as well we can separate the role of predicate
and argument types in the comparison computation.
Extension of the Lexical Function to structured objects of different lengths is treated
by using the identity element c for missing parts. As an example, we show here the
comparison between tall boy and cat where the identity element is the identity matrix I:
K( f (tall boy)),f (cat)) = (TALL bay, cat) = (TALL bay, I cat) =
(19)
= �TALLTI, �boy �catT�F = g(K1(f(tall),f(�)), K2(f(boy),f(cat)))
Dual Space Model. We have until now applied the CC to linear CDSMs with the dot
product as the final comparison operator (what we called K). The CC also holds for
the effective Dual Space model of Turney (2012), which assumes that each word has
two distributional representations, wd in “domain” space and wf in “function” space.
The similarity of two phrases is directly computed as the geometric average of the
separate similarities between the first and second words in both spaces. Even though
</bodyText>
<note confidence="0.411002">
2 Grefenstette et al. (2013) first framed the Lexical Function in terms of tensor contraction.
</note>
<page confidence="0.99019">
171
</page>
<note confidence="0.656244">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999333333333333">
there is no explicit composition step, it is still possible to put the model in CC form.
Take x = (x1, x2) and its trivial decomposition. Define, for a word w with vector rep-
resentations wd and wf: f (w) = �wd �wfT . Define also K1(f (x1),f (y1)) = \/ (f (x1),f(y1))F,
</bodyText>
<equation confidence="0.9961504">
\/ -\/
K2(f(x2),f(y2)) = ( f(x2), f (y2))F and g(a, b) to be ab. Then
g(K1(f(x1),f(y1)), K2(f(x2),f(y2))) =
= 1/ ✓(xd1�xf1T,�yd1�yF1T)F · ✓(�xd2�xf2T,�yd2�yf2T )F =
= YYYVVV� ( xd1, yd1) · (�xf1, yf1) · ( xd2, yd2) · ( xf2, yf2) =
</equation>
<bodyText confidence="0.933714142857143">
= geo(sim(xd1, yd1), sim(xd2, yd2), sim(xf1, yf1), sim(xf2, yf2))
A Neural-network-like Model. Consider the phrase (w1, w2,. . . , wn) and the model defined
by f (x) = 6( w1 + w2 + ... + an), where 6(·) is a component-wise logistic function. Here
we have a single trivial decomposition that includes all the subparts, and γ(x1, ..., xn)
is defined as 6(x1 + ... + xn). To see that for this model the CC cannot hold, consider
two two-word phrases (a b) and (c d)
K( f ((a, b)),f ((c, d))) = ( f ((a, b)),f ((c, d))) = Ei [6(d + b)]i · [6(c + d)] i
</bodyText>
<equation confidence="0.7153685">
= E(1 + e−ai−bi + e−ci−di + e−ai−bi−ci−di)−1 (21)
i
</equation>
<bodyText confidence="0.998635833333333">
We need to rewrite this as
g(K1(d,), K2(�b,�d)) (22)
But there is no possible choice of g, K1, and K2 that allows Equation (21) to be written
as Equation (22). This example can be regarded as a simplified version of the neural-
network model of Socher et al. (2011). The fact that the CC does not apply to it suggests
that it will not apply to other models in this family.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="method">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.9994825">
The Convolution Conjecture offers a general way to rewrite the phrase similarity com-
putations of CDSMs by highlighting the role played by the subparts of a composed
representation. This perspective allows for a better understanding of the exact op-
erations that a composition model applies to its input. The Convolution Conjecture
also suggests a strong connection between CDSMs and semantic convolution kernels.
This link suggests that insights from the CDSM literature could be directly integrated
in the development of convolution kernels, with all the benefits offered by this well-
understood general machine-learning framework.
</bodyText>
<sectionHeader confidence="0.657065" genericHeader="method">
Acknowledgments References
</sectionHeader>
<bodyText confidence="0.8035628">
We thank the reviewers for Clark, Stephen. 2015. Vector space models
helpful comments. Marco Baroni of lexical meaning. In Shalom Lappin
acknowledges ERC 2011 Starting and Chris Fox, editors, Handbook of
Independent Research Grant Contemporary Semantics, 2nd ed.
n. 283554 (COMPOSES). Blackwell, Malden, MA. In press.
</bodyText>
<figure confidence="0.605031">
(20)
</figure>
<page confidence="0.979891">
172
</page>
<reference confidence="0.998570350877193">
Zanzotto, Ferrone, and Baroni When the Whole Is Not Greater Than the Combination of Its Parts
Coecke, Bob, Mehrnoosh Sadrzadeh, and
Stephen Clark. 2010. Mathematical
foundations for a compositional
distributional model of meaning.
Linguistic Analysis, 36:345–384.
Ganesalingam, Mohan and Aur´elie Herbelot.
2013. Composing distributions:
Mathematical structures and their
linguistic interpretation. Working paper,
Computer Laboratory, University
of Cambridge. Available at
www.cl.cam.ac.uk/∼ah433/.
Grefenstette, Edward, Georgiana Dinu,
Yao-Zhong Zhang, Mehrnoosh Sadrzadeh,
and Marco Baroni. 2013. Multi-step
regression learning for compositional
distributional semantics. Proceedings
of IWCS, pages 131–142, Potsdam.
Guevara, Emiliano. 2010. A regression
model of adjective-noun compositionality
in distributional semantics. In Proceedings
of GEMS, pages 33–37, Uppsala.
Haussler, David. 1999. Convolution kernels
on discrete structures. Technical report
USCS-CL-99-10, University of California
at Santa Cruz.
Mehdad, Yashar, Alessandro Moschitti,
and Fabio Massimo Zanzotto. 2010.
Syntactic/semantic structures for
textual entailment recognition. In
Proceedings of NAACL, pages 1,020–1,028,
Los Angeles, CA.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL,
pages 236–244, Columbus, OH.
Socher, Richard, Eric Huang, Jeffrey Pennin,
Andrew Ng, and Christopher Manning.
2011. Dynamic pooling and unfolding
recursive autoencoders for paraphrase
detection. In Proceedings of NIPS,
pages 801–809, Granada.
Turney, Peter. 2012. Domain and function:
A dual-space model of semantic relations
and compositions. Journal of Artificial
Intelligence Research, 44:533–585.
Turney, Peter and Patrick Pantel. 2010.
From frequency to meaning: Vector
space models of semantics. Journal of
Artificial Intelligence Research, 37:141–188.
Zanzotto, Fabio Massimo, Ioannis
Korkontzelos, Francesca Falucchi, and
Suresh Manandhar. 2010. Estimating linear
models for compositional distributional
semantics. In Proceedings of COLING,
pages 1,263–1,271, Beijing.
</reference>
<page confidence="0.999106">
173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.472848">
<title confidence="0.9971995">When the Whole Is Not Greater Than the Combination of Its Parts: A “Decompositional” Look at Compositional Distributional Semantics</title>
<author confidence="0.996545">Massimo</author>
<affiliation confidence="0.991747">University of Rome “Tor Vergata”</affiliation>
<author confidence="0.993634">Lorenzo Ferrone</author>
<affiliation confidence="0.993112">University of Rome “Tor Vergata”</affiliation>
<author confidence="0.999232">Marco Baroni</author>
<affiliation confidence="0.998664">University of Trento</affiliation>
<abstract confidence="0.897496">Distributional semantics has been extended to phrases and sentences by means of composition operations. We look at how these operations affect similarity measurements, showing that similarity equations of an important class of composition methods can be decomposed into operations performed on the subparts of the input phrases. This establishes a strong link between these models and convolution kernels.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Ferrone Zanzotto</author>
</authors>
<title>and Baroni When the Whole Is Not Greater Than the Combination of Its Parts</title>
<marker>Zanzotto, </marker>
<rawString>Zanzotto, Ferrone, and Baroni When the Whole Is Not Greater Than the Combination of Its Parts</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Coecke, Bob, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohan Ganesalingam</author>
<author>Aur´elie Herbelot</author>
</authors>
<title>Composing distributions: Mathematical structures and their linguistic interpretation. Working paper,</title>
<date>2013</date>
<institution>Computer Laboratory, University of Cambridge.</institution>
<note>Available at www.cl.cam.ac.uk/∼ah433/.</note>
<contexts>
<context position="1799" citStr="Ganesalingam and Herbelot (2013)" startWordPosition="253" endWordPosition="256">or tasks such as paraphrasing. We take a mathematical look1 at how the composition operations postulated by CDSMs affect similarity measurements involving the vectors they produce for phrases or sentences. We show that, for an important class of composition methods, encompassing at least those based on linear transformations, the similarity equations can be decomposed into operations performed on the subparts of the input phrases, * Department of Enterprise Engineering, University of Rome “Tor Vergata,” Viale del Politecnico, 1, 00133 Rome, Italy. E-mail: fabio.massimo.zanzotto@uniroma2.it. 1 Ganesalingam and Herbelot (2013) also present a mathematical investigation of CDSMs. However, except for the tensor product (a composition method we do not consider here as it is not empirically effective), they do not look at how composition strategies affect similarity comparisons. Original submission received: 10 December 2013; revision received: 26 May 2014; accepted for publication: 1 August 2014. doi:10.1162/COLI a 00215 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 Table 1 Compositional Distributional Semantic Models: it, b, and c�are distributional vectors representing</context>
</contexts>
<marker>Ganesalingam, Herbelot, 2013</marker>
<rawString>Ganesalingam, Mohan and Aur´elie Herbelot. 2013. Composing distributions: Mathematical structures and their linguistic interpretation. Working paper, Computer Laboratory, University of Cambridge. Available at www.cl.cam.ac.uk/∼ah433/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>Proceedings of IWCS,</booktitle>
<pages>131--142</pages>
<location>Potsdam.</location>
<contexts>
<context position="17325" citStr="Grefenstette et al. (2013)" startWordPosition="3042" endWordPosition="3045">y, cat) = (TALL bay, I cat) = (19) = �TALLTI, �boy �catT�F = g(K1(f(tall),f(�)), K2(f(boy),f(cat))) Dual Space Model. We have until now applied the CC to linear CDSMs with the dot product as the final comparison operator (what we called K). The CC also holds for the effective Dual Space model of Turney (2012), which assumes that each word has two distributional representations, wd in “domain” space and wf in “function” space. The similarity of two phrases is directly computed as the geometric average of the separate similarities between the first and second words in both spaces. Even though 2 Grefenstette et al. (2013) first framed the Lexical Function in terms of tensor contraction. 171 Computational Linguistics Volume 41, Number 1 there is no explicit composition step, it is still possible to put the model in CC form. Take x = (x1, x2) and its trivial decomposition. Define, for a word w with vector representations wd and wf: f (w) = �wd �wfT . Define also K1(f (x1),f (y1)) = \/ (f (x1),f(y1))F, \/ -\/ K2(f(x2),f(y2)) = ( f(x2), f (y2))F and g(a, b) to be ab. Then g(K1(f(x1),f(y1)), K2(f(x2),f(y2))) = = 1/ ✓(xd1�xf1T,�yd1�yF1T)F · ✓(�xd2�xf2T,�yd2�yf2T )F = = YYYVVV� ( xd1, yd1) · (�xf1, yf1) · ( xd2, yd2)</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Grefenstette, Edward, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. Proceedings of IWCS, pages 131–142, Potsdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>33--37</pages>
<location>Uppsala.</location>
<contexts>
<context position="2863" citStr="Guevara (2010)" startWordPosition="424" endWordPosition="425">mputational Linguistics Volume 41, Number 1 Table 1 Compositional Distributional Semantic Models: it, b, and c�are distributional vectors representing the words a, b, and c, respectively; matrices X, Y, and Z are constant across a phrase type, corresponding to syntactic slots; the matrix A and the third-order tensor B represent the predicate words a in the first phrase and b in the second phrase, respectively. 2-word phrase 3-word phrase reference Additive a�+ b a�+ 6 +c� Mitchell and Lapata (2008) Multiplicative a� 0 b a�0 9 0 c� Mitchell and Lapata (2008) Full Additive Xa + Y b Ya + Xb + Zc Guevara (2010), Zanzotto et al. (2010) Lexical Function A b a�B c� Coecke, Sadrzadeh, and Clark (2010) and typically factorized into terms that reflect the linguistic structure of the input. This establishes a strong link between CDSMs and convolution kernels (Haussler 1999), which act in the same way. We thus refer to our claim as the “Convolution Conjecture.” We focus on the models in Table 1. These CDSMs all apply linear methods, and we suspect that linearity is a sufficient (but not necessary) condition to ensure that the Convolution Conjecture holds. We will first illustrate the conjecture for linear m</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Guevara, Emiliano. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of GEMS, pages 33–37, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report USCS-CL-99-10,</tech>
<institution>University of California at Santa Cruz.</institution>
<contexts>
<context position="3124" citStr="Haussler 1999" startWordPosition="464" endWordPosition="465">syntactic slots; the matrix A and the third-order tensor B represent the predicate words a in the first phrase and b in the second phrase, respectively. 2-word phrase 3-word phrase reference Additive a�+ b a�+ 6 +c� Mitchell and Lapata (2008) Multiplicative a� 0 b a�0 9 0 c� Mitchell and Lapata (2008) Full Additive Xa + Y b Ya + Xb + Zc Guevara (2010), Zanzotto et al. (2010) Lexical Function A b a�B c� Coecke, Sadrzadeh, and Clark (2010) and typically factorized into terms that reflect the linguistic structure of the input. This establishes a strong link between CDSMs and convolution kernels (Haussler 1999), which act in the same way. We thus refer to our claim as the “Convolution Conjecture.” We focus on the models in Table 1. These CDSMs all apply linear methods, and we suspect that linearity is a sufficient (but not necessary) condition to ensure that the Convolution Conjecture holds. We will first illustrate the conjecture for linear methods, and then briefly consider two nonlinear approaches: the dual space model of Turney (2012), for which it does, and a representative of the recent strand of work on neuralnetwork models of composition, for which it does not. 2. Mathematical Preliminaries </context>
<context position="5331" citStr="Haussler (1999)" startWordPosition="847" endWordPosition="848">le Is Not Greater Than the Combination of Its Parts Let A and B be two third-order tensors and x, y,ta, c~ four vectors. It can be shown that: ~��xA�y,�aB�c � = � (A ⊗ B)j,�x ⊗ y~ ⊗�a ⊗ c~ ~F (4) j where C = ~j(A ⊗B)j is a non-standard way to indicate the tensor contraction of the tensor product between two third-order tensors. In this particular tensor contraction, the elements Ciknm of the resulting fourth-order tensor C are Ciknm = ~j AijkBnjm. The elements Diknm of the tensor D = x� ⊗ y�⊗d ⊗ c�are Diknm = xiykancm. 3. Formalizing the Convolution Conjecture Structured Objects. In line with Haussler (1999), a structured object x ∈ X is either a terminal object that cannot be furthermore decomposed, or a non-terminal object that can be decomposed into n subparts. We indicate with x = (x1, ... , xn) one such decomposition, where the subparts xi ∈ X are structured objects themselves. The set X is the set of the structured objects and TX ⊆ X is the set of the terminal objects. A structured object x can be anything according to the representational needs. Here, x is a representation of a text fragment, and so it can be a sequence of words, a sequence of words along with their part of speech, a tree </context>
<context position="8360" citStr="Haussler 1999" startWordPosition="1396" endWordPosition="1397">e to these models, why we are assuming different decomposition sets for Additive and Multiplicative). Definition 1 (Convolution Conjecture) For every CDSM f along with its R(x) set, there exist functions K, Ki and a function g such that: K( f (x), f (y)) = E g(K1(f(x1),f(y1)), K2(f(x2),f(y2)),..., Kn(f(xn),f(yn))) (8) xER(x) yER(y) The Convolution Conjecture postulates that the similarity K( f (x),f (y)) between the tensors f (x) and f (y) is computed by combining operations on the subparts, that is, Ki(f (xi),f (yi)), using the function g. This is exactly what happens in convolution kernels (Haussler 1999). K is usually the dot product, but this is not necessary: We will show that for the dual-space model of Turney (2012) K turns out to be the fourth root of the Frobenius tensor. 4. Comparing Composed Phrases We illustrate now how the Convolution Conjecture (CC) applies to the considered CDSMs, exemplifying with adjective–noun and subject–verb–object phrases. Without loss of generality we use tall boy and red cat for adjective–noun phrases and goats eat grass and cows drink water for subject–verb–object phrases. Additive Model. K and Ki are dot products, g is the identity function, and f is as </context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>Haussler, David. 1999. Convolution kernels on discrete structures. Technical report USCS-CL-99-10, University of California at Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Syntactic/semantic structures for textual entailment recognition.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>1--020</pages>
<location>Los Angeles, CA.</location>
<marker>Mehdad, Moschitti, Zanzotto, 2010</marker>
<rawString>Mehdad, Yashar, Alessandro Moschitti, and Fabio Massimo Zanzotto. 2010. Syntactic/semantic structures for textual entailment recognition. In Proceedings of NAACL, pages 1,020–1,028, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="2752" citStr="Mitchell and Lapata (2008)" startWordPosition="397" endWordPosition="400">2014; accepted for publication: 1 August 2014. doi:10.1162/COLI a 00215 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 Table 1 Compositional Distributional Semantic Models: it, b, and c�are distributional vectors representing the words a, b, and c, respectively; matrices X, Y, and Z are constant across a phrase type, corresponding to syntactic slots; the matrix A and the third-order tensor B represent the predicate words a in the first phrase and b in the second phrase, respectively. 2-word phrase 3-word phrase reference Additive a�+ b a�+ 6 +c� Mitchell and Lapata (2008) Multiplicative a� 0 b a�0 9 0 c� Mitchell and Lapata (2008) Full Additive Xa + Y b Ya + Xb + Zc Guevara (2010), Zanzotto et al. (2010) Lexical Function A b a�B c� Coecke, Sadrzadeh, and Clark (2010) and typically factorized into terms that reflect the linguistic structure of the input. This establishes a strong link between CDSMs and convolution kernels (Haussler 1999), which act in the same way. We thus refer to our claim as the “Convolution Conjecture.” We focus on the models in Table 1. These CDSMs all apply linear methods, and we suspect that linearity is a sufficient (but not necessary) </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Mitchell, Jeff and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric Huang</author>
<author>Jeffrey Pennin</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>801--809</pages>
<location>Granada.</location>
<contexts>
<context position="18804" citStr="Socher et al. (2011)" startWordPosition="3323" endWordPosition="3326">Here we have a single trivial decomposition that includes all the subparts, and γ(x1, ..., xn) is defined as 6(x1 + ... + xn). To see that for this model the CC cannot hold, consider two two-word phrases (a b) and (c d) K( f ((a, b)),f ((c, d))) = ( f ((a, b)),f ((c, d))) = Ei [6(d + b)]i · [6(c + d)] i = E(1 + e−ai−bi + e−ci−di + e−ai−bi−ci−di)−1 (21) i We need to rewrite this as g(K1(d,), K2(�b,�d)) (22) But there is no possible choice of g, K1, and K2 that allows Equation (21) to be written as Equation (22). This example can be regarded as a simplified version of the neuralnetwork model of Socher et al. (2011). The fact that the CC does not apply to it suggests that it will not apply to other models in this family. 5. Conclusion The Convolution Conjecture offers a general way to rewrite the phrase similarity computations of CDSMs by highlighting the role played by the subparts of a composed representation. This perspective allows for a better understanding of the exact operations that a composition model applies to its input. The Convolution Conjecture also suggests a strong connection between CDSMs and semantic convolution kernels. This link suggests that insights from the CDSM literature could be</context>
</contexts>
<marker>Socher, Huang, Pennin, Ng, Manning, 2011</marker>
<rawString>Socher, Richard, Eric Huang, Jeffrey Pennin, Andrew Ng, and Christopher Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of NIPS, pages 801–809, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Domain and function: A dual-space model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>44--533</pages>
<contexts>
<context position="3560" citStr="Turney (2012)" startWordPosition="536" endWordPosition="537">010) and typically factorized into terms that reflect the linguistic structure of the input. This establishes a strong link between CDSMs and convolution kernels (Haussler 1999), which act in the same way. We thus refer to our claim as the “Convolution Conjecture.” We focus on the models in Table 1. These CDSMs all apply linear methods, and we suspect that linearity is a sufficient (but not necessary) condition to ensure that the Convolution Conjecture holds. We will first illustrate the conjecture for linear methods, and then briefly consider two nonlinear approaches: the dual space model of Turney (2012), for which it does, and a representative of the recent strand of work on neuralnetwork models of composition, for which it does not. 2. Mathematical Preliminaries Vectors are represented as small letters with an arrow a� and their elements are ai, matrices as capital letters in bold A and their elements are Aij, and third-order or fourth-order tensors as capital letters in the form A and their elements are Aijk or Aijkh. The symbol 0 represents the element-wise product and ® is the tensor product. The dot product is (a,b) and the Frobenius product—that is, the generalization of the dot produc</context>
<context position="8478" citStr="Turney (2012)" startWordPosition="1418" endWordPosition="1419">volution Conjecture) For every CDSM f along with its R(x) set, there exist functions K, Ki and a function g such that: K( f (x), f (y)) = E g(K1(f(x1),f(y1)), K2(f(x2),f(y2)),..., Kn(f(xn),f(yn))) (8) xER(x) yER(y) The Convolution Conjecture postulates that the similarity K( f (x),f (y)) between the tensors f (x) and f (y) is computed by combining operations on the subparts, that is, Ki(f (xi),f (yi)), using the function g. This is exactly what happens in convolution kernels (Haussler 1999). K is usually the dot product, but this is not necessary: We will show that for the dual-space model of Turney (2012) K turns out to be the fourth root of the Frobenius tensor. 4. Comparing Composed Phrases We illustrate now how the Convolution Conjecture (CC) applies to the considered CDSMs, exemplifying with adjective–noun and subject–verb–object phrases. Without loss of generality we use tall boy and red cat for adjective–noun phrases and goats eat grass and cows drink water for subject–verb–object phrases. Additive Model. K and Ki are dot products, g is the identity function, and f is as in Equation (6). The structure of the input is a word sequence (i.e., x = (w1 w2)) and the relevant decompositions con</context>
<context position="17009" citStr="Turney (2012)" startWordPosition="2994" endWordPosition="2995">ation. Extension of the Lexical Function to structured objects of different lengths is treated by using the identity element c for missing parts. As an example, we show here the comparison between tall boy and cat where the identity element is the identity matrix I: K( f (tall boy)),f (cat)) = (TALL bay, cat) = (TALL bay, I cat) = (19) = �TALLTI, �boy �catT�F = g(K1(f(tall),f(�)), K2(f(boy),f(cat))) Dual Space Model. We have until now applied the CC to linear CDSMs with the dot product as the final comparison operator (what we called K). The CC also holds for the effective Dual Space model of Turney (2012), which assumes that each word has two distributional representations, wd in “domain” space and wf in “function” space. The similarity of two phrases is directly computed as the geometric average of the separate similarities between the first and second words in both spaces. Even though 2 Grefenstette et al. (2013) first framed the Lexical Function in terms of tensor contraction. 171 Computational Linguistics Volume 41, Number 1 there is no explicit composition step, it is still possible to put the model in CC form. Take x = (x1, x2) and its trivial decomposition. Define, for a word w with vec</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Turney, Peter. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="816" citStr="Turney and Pantel 2010" startWordPosition="112" endWordPosition="115">nzo Ferrone University of Rome “Tor Vergata” Marco Baroni University of Trento Distributional semantics has been extended to phrases and sentences by means of composition operations. We look at how these operations affect similarity measurements, showing that similarity equations of an important class of composition methods can be decomposed into operations performed on the subparts of the input phrases. This establishes a strong link between these models and convolution kernels. 1. Introduction Distributional semantics approximates word meanings with vectors tracking cooccurrence in corpora (Turney and Pantel 2010). Recent work has extended this approach to phrases and sentences through vector composition (Clark 2015). Resulting compositional distributional semantic models (CDSMs) estimate degrees of semantic similarity (or, more generally, relatedness) between two phrases: A good CDSM might tell us that green bird is closer to parrot than to pigeon, useful for tasks such as paraphrasing. We take a mathematical look1 at how the composition operations postulated by CDSMs affect similarity measurements involving the vectors they produce for phrases or sentences. We show that, for an important class of com</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, Peter and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Falucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1--263</pages>
<location>Beijing.</location>
<contexts>
<context position="2887" citStr="Zanzotto et al. (2010)" startWordPosition="426" endWordPosition="429">uistics Volume 41, Number 1 Table 1 Compositional Distributional Semantic Models: it, b, and c�are distributional vectors representing the words a, b, and c, respectively; matrices X, Y, and Z are constant across a phrase type, corresponding to syntactic slots; the matrix A and the third-order tensor B represent the predicate words a in the first phrase and b in the second phrase, respectively. 2-word phrase 3-word phrase reference Additive a�+ b a�+ 6 +c� Mitchell and Lapata (2008) Multiplicative a� 0 b a�0 9 0 c� Mitchell and Lapata (2008) Full Additive Xa + Y b Ya + Xb + Zc Guevara (2010), Zanzotto et al. (2010) Lexical Function A b a�B c� Coecke, Sadrzadeh, and Clark (2010) and typically factorized into terms that reflect the linguistic structure of the input. This establishes a strong link between CDSMs and convolution kernels (Haussler 1999), which act in the same way. We thus refer to our claim as the “Convolution Conjecture.” We focus on the models in Table 1. These CDSMs all apply linear methods, and we suspect that linearity is a sufficient (but not necessary) condition to ensure that the Convolution Conjecture holds. We will first illustrate the conjecture for linear methods, and then briefly</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Falucchi, Manandhar, 2010</marker>
<rawString>Zanzotto, Fabio Massimo, Ioannis Korkontzelos, Francesca Falucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of COLING, pages 1,263–1,271, Beijing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>